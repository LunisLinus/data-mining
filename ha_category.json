[
    {
        "title": "Reinhardt: Real-time Reconfigurable Hardware Architecture for Regular Expression Matching in DPI",
        "authors": "['Taejune Park', 'Jaehyun Nam', 'Seung Ho Na', 'Jaewoong Chung', 'Seungwon Shin']",
        "date": "December 2021",
        "source": "ACSAC '21: Proceedings of the 37th Annual Computer Security Applications Conference",
        "abstract": "Regular expression (regex) matching is an integral part of deep packet inspection (DPI) but a major bottleneck due to its low performance. For regex matching (REM) acceleration, FPGA-based studies have emerged and exploited parallelism by matching multiple regex patterns concurrently. However, even though guaranteeing high-performance, existing FPGA-based regex solutions do not still support dynamic updates in run time. Hence, it was inappropriate as a DPI function due to frequently altered malicious signatures. In this work, we introduce Reinhardt, a real-time reconfigurable hardware architecture for REM. Reinhardt represents regex patterns as a combination of reconfigurable cells in hardware and updates regex patterns in real-time while providing high performance. We implement the prototype using NetFPGA-SUME, and our evaluation demonstrates that Reinhardt updates hundreds of patterns within a second and achieves up to 10 Gbps throughput (max. hardware bandwidth). Our case studies show that Reinhardt can operate as NIDS/NIPS and as the REM accelerator for them.",
        "link": "https://dl.acm.org/doi/10.1145/3485832.3485878",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "FLASH: Fast Neural Architecture Search with Hardware Optimization",
        "authors": "['Guihong Li', 'Sumit K. Mandal', 'Umit Y. Ogras', 'Radu Marculescu']",
        "date": "None",
        "source": "ACM Transactions on Embedded Computing Systems",
        "abstract": "Neural architecture search (NAS) is a promising technique to design efficient and high-performance deep neural networks (DNNs). As the performance requirements of ML applications grow continuously, the hardware accelerators start playing a central role in DNN design. This trend makes NAS even more complicated and time-consuming for most real applications. This paper proposes FLASH, a very fast NAS methodology that co-optimizes the DNN accuracy and performance on a real hardware platform. As the main theoretical contribution, we first propose the NN-Degree, an analytical metric to quantify the topological characteristics of DNNs with skip connections (e.g., DenseNets, ResNets, Wide-ResNets, and MobileNets). The newly proposed NN-Degree allows us to do training-free NAS within one second and build an accuracy predictor by training as few as 25 samples out of a vast search space with more than 63 billion configurations. Second, by performing inference on the target hardware, we fine-tune and validate our analytical models to estimate the latency, area, and energy consumption of various DNN architectures while executing standard ML datasets. Third, we construct a hierarchical algorithm based on simplicial homology global optimization (SHGO) to optimize the model-architecture co-design process, while considering the area, latency, and energy consumption of the target hardware. We demonstrate that, compared to the state-of-the-art NAS approaches, our proposed hierarchical SHGO-based algorithm enables more than four orders of magnitude speedup (specifically, the execution time of the proposed algorithm is about 0.1 seconds). Finally, our experimental evaluations show that FLASH is easily transferable to different hardware architectures, thus enabling us to do NAS on a Raspberry Pi-3B processor in less than 3 seconds.",
        "link": "https://dl.acm.org/doi/10.1145/3476994",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "When Massive GPU Parallelism Ain’t Enough: A Novel Hardware Architecture of 2D-LSTM Neural Network",
        "authors": "['Vladimir Rybalkin', 'Jonas Ney', 'Menbere Kina Tekleyohannes', 'Norbert Wehn']",
        "date": "None",
        "source": "ACM Transactions on Reconfigurable Technology and Systems",
        "abstract": "Multidimensional Long Short-Term Memory (MD-LSTM) neural network is an extension of one-dimensional LSTM for data with more than one dimension. MD-LSTM achieves state-of-the-art results in various applications, including handwritten text recognition, medical imaging, and many more. However, its implementation suffers from the inherently sequential execution that tremendously slows down both training and inference compared to other neural networks.The main goal of the current research is to provide acceleration for inference of MD-LSTM. We advocate that Field-Programmable Gate Array (FPGA) is an alternative platform for deep learning that can offer a solution when the massive parallelism of GPUs does not provide the necessary performance required by the application.In this article, we present the first hardware architecture for MD-LSTM. We conduct a systematic exploration to analyze a tradeoff between precision and accuracy. We use a challenging dataset for semantic segmentation, namely historical document image binarization from the DIBCO 2017 contest and a well-known MNIST dataset for handwritten digit recognition. Based on our new architecture, we implement FPGA-based accelerators that outperform Nvidia Geforce RTX 2080 Ti with respect to throughput by up to 9.9 and Nvidia Jetson AGX Xavier with respect to energy efficiency by up to 48. Our accelerators achieve higher throughput, energy efficiency, and resource efficiency than FPGA-based implementations of convolutional neural networks (CNNs) for semantic segmentation tasks. For the handwritten digit recognition task, our FPGA implementations provide higher accuracy and can be considered as a solution when accuracy is a priority. Furthermore, they outperform earlier FPGA implementations of one-dimensional LSTMs with respect to throughput, energy efficiency, and resource efficiency.",
        "link": "https://dl.acm.org/doi/10.1145/3469661",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "One Proxy Device Is Enough for Hardware-Aware Neural Architecture Search",
        "authors": "['Bingqian Lu', 'Jianyi Yang', 'Weiwen Jiang', 'Yiyu Shi', 'Shaolei Ren']",
        "date": "None",
        "source": "Proceedings of the ACM on Measurement and Analysis of Computing Systems",
        "abstract": "Convolutional neural networks (CNNs) are used in numerous real-world applications such as vision-based autonomous driving and video content analysis. To run CNN inference on various target devices, hardware-aware neural architecture search (NAS) is crucial. A key requirement of efficient hardware-aware NAS is the fast evaluation of inference latencies in order to rank different architectures. While building a latency predictor for each target device has been commonly used in state of the art, this is a very time-consuming process, lacking scalability in the presence of extremely diverse devices. In this work, we address the scalability challenge by exploiting latency monotonicity --- the architecture latency rankings on different devices are often correlated. When strong latency monotonicity exists, we can re-use architectures searched for one proxy device on new target devices, without losing optimality. In the absence of strong latency monotonicity, we propose an efficient proxy adaptation technique to significantly boost the latency monotonicity. Finally, we validate our approach and conduct experiments with devices of different platforms on multiple mainstream search spaces, including MobileNet-V2, MobileNet-V3, NAS-Bench-201, ProxylessNAS and FBNet. Our results highlight that, by using just one proxy device, we can find almost the same Pareto-optimal architectures as the existing per-device NAS, while avoiding the prohibitive cost of building a latency predictor for each device.",
        "link": "https://dl.acm.org/doi/10.1145/3491046",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Hardware architecture and software stack for PIM based on commercial DRAM technology",
        "authors": "['Sukhan Lee', 'Shin-haeng Kang', 'Jaehoon Lee', 'Hyeonsu Kim', 'Eojin Lee', 'Seungwoo Seo', 'Hosang Yoon', 'Seungwon Lee', 'Kyounghwan Lim', 'Hyunsung Shin', 'Jinhyun Kim', 'Seongil O', 'Anand Iyer', 'David Wang', 'Kyomin Sohn', 'Nam Sung Kim']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "Emerging applications such as deep neural network demand high off-chip memory bandwidth. However, under stringent physical constraints of chip packages and system boards, it becomes very expensive to further increase the bandwidth of off-chip memory. Besides, transferring data across the memory hierarchy constitutes a large fraction of total energy consumption of systems, and the fraction has steadily increased with the stagnant technology scaling and poor data reuse characteristics of such emerging applications. To cost-effectively increase the bandwidth and energy efficiency, researchers began to reconsider the past processing-in-memory (PIM) architectures and advance them further, especially exploiting recent integration technologies such as 2.5D/3D stacking. Albeit the recent advances, no major memory manufacturer has developed even a proof-of-concept silicon yet, not to mention a product. This is because the past PIM architectures often require changes in host processors and/or application code which memory manufacturers cannot easily govern. In this paper, elegantly tackling the aforementioned challenges, we propose an innovative yet practical PIM architecture. To demonstrate its practicality and effectiveness at the system level, we implement it with a 20nm DRAM technology, integrate it with an unmodified commercial processor, develop the necessary software stack, and run existing applications without changing their source code. Our evaluation at the system level shows that our PIM improves the performance of memory-bound neural network kernels and applications by 11.2X and 3.5X, respectively. Atop the performance improvement, PIM also reduces the energy per bit transfer by 3.5X, and the overall energy efficiency of the system running the applications by 3.2X.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00013",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A Lightweight Architecture for Hardware-Based Security in the Emerging Era of Systems of Systems",
        "authors": "['Nico Mexis', 'Nikolaos Athanasios Anagnostopoulos', 'Shuai Chen', 'Jan Bambach', 'Tolga Arul', 'Stefan Katzenbeisser']",
        "date": "None",
        "source": "ACM Journal on Emerging Technologies in Computing Systems",
        "abstract": "In recent years, a new generation of the Internet of Things (IoT 2.0) is emerging, based on artificial intelligence, the blockchain technology, machine learning, and the constant consolidation of pre-existing systems and subsystems into larger systems. In this work, we construct and examine a proof-of-concept prototype of such a system of systems, which consists of heterogeneous commercial off-the-shelf components, and utilises diverse communication protocols. We recognise the inherent need for lightweight security in this context, and address it by employing a low-cost state-of-the-art security solution. Our solution is based on a novel hardware and software co-engineering paradigm, utilising well-known software-based cryptographic algorithms, in order to maximise the security potential of the hardware security primitive (a Physical Unclonable Function) that is used as a security anchor. The performance of the proposed security solution is evaluated, proving its suitability even for real-time applications. Additionally, the Dolev-Yao attacker model is considered in order to assess the resilience of our solution towards attacks against the confidentiality, integrity, and availability of the examined system of systems. In this way, it is confirmed that the proposed solution is able to address the emerging security challenges of the oncoming era of systems of systems.",
        "link": "https://dl.acm.org/doi/10.1145/3458824",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Algorithm-hardware co-design of a discontinuous Galerkin shallow-water model for a dataflow architecture on FPGA",
        "authors": "['Tobias Kenter', 'Adesh Shambhu', 'Sara Faghih-Naini', 'Vadym Aizinger']",
        "date": "July 2021",
        "source": "PASC '21: Proceedings of the Platform for Advanced Scientific Computing Conference",
        "abstract": "We present the first FPGA implementation of the full simulation pipeline of a shallow water code based on the discontinuous Galerkin method. Using OpenCL and following an algorithm-hardware codesign approach, the software reference is transformed into a dataflow architecture that can process a full mesh element per clock cycle. The novel projection approach on the algorithmic level complements the pipeline and memory optimizations in the hardware design. With this, the FPGA kernels for different polynomial orders outperform the CPU reference by 43x -- 144x in a strong scaling benchmark scenario. A performance model can explain the measured FPGA performance of up to 717 GFLOPs accurately.",
        "link": "https://dl.acm.org/doi/10.1145/3468267.3470617",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Solver-Aided Constant-Time Hardware Verification",
        "authors": "['Klaus v. Gleissenthall', 'Rami Gökhan Kıcı', 'Deian Stefan', 'Ranjit Jhala']",
        "date": "November 2021",
        "source": "CCS '21: Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security",
        "abstract": "We present Xenon, a solver-aided, interactive method for formally verifying that Verilog hardware executes in constant-time. Xenon scales to realistic hardware designs by drastically reducing the effort needed to localize the root cause of verification failures via a new notion of constant-time counterexamples, which Xenon uses to synthesize a minimal set of secrecy assumptions in an interactive verification loop. To reduce verification time Xenon exploits modularity in Verilog code via module summaries, thereby avoiding duplicate work across multiple module instantiations. We show how Xenon's assumption synthesis and summaries enable us to verify different kinds of circuits, including a highly modular AES- 256 implementation where modularity cuts verification from six hours to under three seconds, and the ScarVside-channel hardened RISC-V micro-controller whose size exceeds previously verified designs by an order of magnitude. In a small study, we also find that Xenon helps non-expert users complete verification tasks correctly and faster than previous state-of-art tools.",
        "link": "https://dl.acm.org/doi/10.1145/3460120.3484810",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "On the Co-Design of Quantum Software and Hardware",
        "authors": "['Gushu Li', 'Anbang Wu', 'Yunong Shi', 'Ali Javadi-Abhari', 'Yufei Ding', 'Yuan Xie']",
        "date": "September 2021",
        "source": "NANOCOM '21: Proceedings of the Eight Annual ACM International Conference on Nanoscale Computing and Communication",
        "abstract": "A quantum computing system naturally consists of two components, the software system and the hardware system. Quantum applications are programmed using the quantum software and then executed on the quantum hardware. However, the performance of existing quantum computing system is still limited. Solving a practical problem that is beyond the capability of classical computers on a quantum computer has not yet been demonstrated. In this review, we point out that the quantum software and hardware systems should be designed collaboratively to fully exploit the potential of quantum computing. We first review three related works, including one hardware-aware quantum compiler optimization, one application-aware quantum hardware architecture design flow, and one co-design approach for the emerging quantum computational chemistry. Then we discuss some potential future directions following the co-design principle.",
        "link": "https://dl.acm.org/doi/10.1145/3477206.3477464",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Algorithm-hardware Co-design of Attention Mechanism on FPGA Devices",
        "authors": "['Xinyi Zhang', 'Yawen Wu', 'Peipei Zhou', 'Xulong Tang', 'Jingtong Hu']",
        "date": "None",
        "source": "ACM Transactions on Embedded Computing Systems",
        "abstract": "Multi-head self-attention (attention mechanism) has been employed in a variety of fields such as machine translation, language modeling, and image processing due to its superiority in feature extraction and sequential data analysis. This is benefited from a large number of parameters and sophisticated model architecture behind the attention mechanism. To efficiently deploy attention mechanism on resource-constrained devices, existing works propose to reduce the model size by building a customized smaller model or compressing a big standard model. A customized smaller model is usually optimized for the specific task and needs effort in model parameters exploration. Model compression reduces model size without hurting the model architecture robustness, which can be efficiently applied to different tasks. The compressed weights in the model are usually regularly shaped (e.g. rectangle) but the dimension sizes vary (e.g. differs in rectangle height and width). Such compressed attention mechanism can be efficiently deployed on CPU/GPU platforms as their memory and computing resources can be flexibly assigned with demand. However, for Field Programmable Gate Arrays (FPGAs), the data buffer allocation and computing kernel are fixed at run time to achieve maximum energy efficiency. After compression, weights are much smaller and different in size, which leads to inefficient utilization of FPGA on-chip buffer. Moreover, the different weight heights and widths may lead to inefficient FPGA computing kernel execution. Due to the large number of weights in the attention mechanism, building a unique buffer and computing kernel for each compressed weight on FPGA is not feasible. In this work, we jointly consider the compression impact on buffer allocation and the required computing kernel during the attention mechanism compressing. A novel structural pruning method with memory footprint awareness is proposed and the associated accelerator on FPGA is designed. The experimental results show that our work can compress Transformer (an attention mechanism based model) by 95x. The developed accelerator can fully utilize the FPGA resource, processing the sparse attention mechanism with the run-time throughput performance of 1.87 Tops in ZCU102 FPGA.",
        "link": "https://dl.acm.org/doi/10.1145/3477002",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Co-designing hardware and models for efficient on-device ML inference",
        "authors": "['Matthew Mattina']",
        "date": "July 2021",
        "source": "ISLPED '21: Proceedings of the ACM/IEEE International Symposium on Low Power Electronics and Design",
        "abstract": "Deep learning inference at the edge continues to deliver state of the art results on real-world applications involving images, video, speech, and human activity. The workhorse behind these advances---increasingly complex neural network models---continue to grow in size and computational requirements. These advances place significant demand on the energy-constrained hardware platforms responsible for executing such models and are driving trends like low-precision number formats, network pruning, and complexity-reducing network transforms. This talk will discuss emerging research aimed at co-designing neural networks and hardware to enable even larger, more complex models to operate on highly-constrained hardware platforms.",
        "link": "https://dl.acm.org/doi/10.1109/ISLPED52811.2021.9502470",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "BitX: Empower Versatile Inference with Hardware Runtime Pruning",
        "authors": "['Hongyan Li', 'Hang Lu', 'Jiawen Huang', 'Wenxu Wang', 'Mingzhe Zhang', 'Wei Chen', 'Liang Chang', 'Xiaowei Li']",
        "date": "August 2021",
        "source": "ICPP '21: Proceedings of the 50th International Conference on Parallel Processing",
        "abstract": "Classic DNN pruning mostly leverages software-based methodologies to tackle the accuracy/speed tradeoff, which involves complicated procedures like critical parameter searching, fine-tuning and sparse training to find the best plan. In this paper, we explore the opportunities of hardware runtime pruning and propose a hardware runtime pruning methodology, termed as “BitX” to empower versatile DNN inference. It targets the abundant useless bits in the parameters, pinpoints and prunes these bits on-the-fly in the proposed BitX accelerator. The versatility of BitX lies in: (1) software effortless; (2) orthogonal to the software-based pruning; and (3) multi-precision support (including both floating point and fixed point). Empirical studies on image classification and object detection models highlight the following results: (1) up to 4.82x speedup over the original non-pruned DNN and 14.76x speedup collaborated with the software-pruned DNN; (2) up to 0.07% and 0.9% higher accuracy for the floating-point and fixed-point DNN, respectively; (3) 2.00x and 3.79x performance improvement over the state-of-the-art accelerators, with 0.039 mm2 and 68.62 mW (floating-point 32), 36.41 mW(16-bit fixed point) power consumption under TSMC 28 nm technology library.",
        "link": "https://dl.acm.org/doi/10.1145/3472456.3472513",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Side-channel Resistant Implementations of a Novel Lightweight Authenticated Cipher with Application to Hardware Security",
        "authors": "['Abubakr Abdulgadir', 'Sammy Lin', 'Farnoud Farahmand', 'Jens-Peter Kaps', 'Kris Gaj']",
        "date": "June 2021",
        "source": "GLSVLSI '21: Proceedings of the 2021 on Great Lakes Symposium on VLSI",
        "abstract": "Lightweight authenticated ciphers are crucial in many resource-constrained applications, including hardware security. To protect Intellectual Property (IPs) from theft and reverse-engineering, multiple obfuscation methods have been developed. An essential component of such schemes is the need for secrecy and authenticity of the obfuscation keys. Such keys may need to be exchanged through the unprotected channels, and their recovery attempted using side-channel attacks. However, the use of the current AES-GCM standard to protected key exchange requires a substantial area and power overhead. NIST is currently coordinating a standardization process to select lightweight algorithms for resource-constrained applications. Although security against cryptanalysis is paramount, cost, performance, and resistance to side-channel attacks are among the most important selection criteria. Since the cost of protection against side-channel attacks is a function of the algorithm, quantifying this cost is necessary for estimating its cost and performance in real-world applications. In this work, we investigate side-channel resistant lightweight implementations of an authenticated cipher TinyJAMBU, one of ten finalists in the current NIST LWC standardization process. Our results demonstrate that these implementations achieve robust security against side-channel attacks while keeping the area and power consumption significantly lower than it is possible using the current standards.",
        "link": "https://dl.acm.org/doi/10.1145/3453688.3461761",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Software-hardware co-optimization for computational chemistry on superconducting quantum processors",
        "authors": "['Gushu Li', 'Yunong Shi', 'Ali Javadi-Abhari']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "Computational chemistry is the leading application to demonstrate the advantage of quantum computing in the near term. However, large-scale simulation of chemical systems on quantum computers is currently hindered due to a mismatch between the computational resource needs of the program and those available in today's technology. In this paper we argue that significant new optimizations can be discovered by co-designing the application, compiler, and hardware. We show that multiple optimization objectives can be coordinated through the key abstraction layer of Pauli strings, which are the basic building blocks of computational chemistry programs. In particular, we leverage Pauli strings to identify critical program components that can be used to compress program size with minimal loss of accuracy. We also leverage the structure of Pauli string simulation circuits to tailor a novel hardware architecture and compiler, leading to significant execution overhead reduction by up to 99%. While exploiting the high-level domain knowledge reveals significant optimization opportunities, our hardware/software framework is not tied to a particular program instance and can accommodate the full family of computational chemistry problems with such structure. We believe the co-design lessons of this study can be extended to other domains and hardware technologies to hasten the onset of quantum advantage.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00070",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Clio: a hardware-software co-designed disaggregated memory system",
        "authors": "['Zhiyuan Guo', 'Yizhou Shan', 'Xuhao Luo', 'Yutong Huang', 'Yiying Zhang']",
        "date": "February 2022",
        "source": "ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "Memory disaggregation has attracted great attention recently because of its benefits in efficient memory utilization and ease of management. So far, memory disaggregation research has all taken one of two approaches: building/emulating memory nodes using regular servers or building them using raw memory devices with no processing power. The former incurs higher monetary cost and faces tail latency and scalability limitations, while the latter introduces performance, security, and management problems.  Server-based memory nodes and memory nodes with no processing power are two extreme approaches. We seek a sweet spot in the middle by proposing a hardware-based memory disaggregation solution that has the right amount of processing power at memory nodes. Furthermore, we take a clean-slate approach by starting from the requirements of memory disaggregation and designing a memory-disaggregation-native system.  We built Clio, a disaggregated memory system that virtualizes, protects, and manages disaggregated memory at hardware-based memory nodes. The Clio hardware includes a new virtual memory system, a customized network system, and a framework for computation offloading. In building Clio, we not only co-design OS functionalities, hardware architecture, and the network system, but also co-design compute nodes and memory nodes. Our FPGA prototype of Clio demonstrates that each memory node can achieve 100 Gbps throughput and an end-to-end latency of 2.5 µ s at median and 3.2 µ s at the 99th percentile. Clio also scales much better and has orders of magnitude lower tail latency than RDMA. It has 1.1× to 3.4× energy saving compared to CPU-based and SmartNIC-based disaggregated memory systems and is 2.7× faster than software-based SmartNIC solutions.",
        "link": "https://dl.acm.org/doi/10.1145/3503222.3507762",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "CirFix: automatically repairing defects in hardware design code",
        "authors": "['Hammad Ahmad', 'Yu Huang', 'Westley Weimer']",
        "date": "February 2022",
        "source": "ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "This paper presents CirFix, a framework for automatically repairing defects in hardware designs implemented in languages like Verilog. We propose a novel fault localization approach based on assignments to wires and registers, and a fitness function tailored to the hardware domain to bridge the gap between software-level automated program repair and hardware descriptions. We also present a benchmark suite of 32 defect scenarios corresponding to a variety of hardware projects. Overall, CirFix produces plausible repairs for 21/32 and correct repairs for 16/32 of the defect scenarios. This repair rate is comparable to that of successful program repair approaches for software, indicating CirFix is effective at bringing over the benefits of automated program repair to the hardware domain for the first time.",
        "link": "https://dl.acm.org/doi/10.1145/3503222.3507763",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Hardware Information Flow Tracking",
        "authors": "['Wei Hu', 'Armaiti Ardeshiricham', 'Ryan Kastner']",
        "date": "None",
        "source": "ACM Computing Surveys",
        "abstract": "Information flow tracking (IFT) is a fundamental computer security technique used to understand how information moves through a computing system. Hardware IFT techniques specifically target security vulnerabilities related to the design, verification, testing, manufacturing, and deployment of hardware circuits. Hardware IFT can detect unintentional design flaws, malicious circuit modifications, timing side channels, access control violations, and other insecure hardware behaviors. This article surveys the area of hardware IFT. We start with a discussion on the basics of IFT, whose foundations were introduced by Denning in the 1970s. Building upon this, we develop a taxonomy for hardware IFT. We use this to classify and differentiate hardware IFT tools and techniques. Finally, we discuss the challenges yet to be resolved. The survey shows that hardware IFT provides a powerful technique for identifying hardware security vulnerabilities, as well as verifying and enforcing hardware security properties.",
        "link": "https://dl.acm.org/doi/10.1145/3447867",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Debugging in the brave new world of reconfigurable hardware",
        "authors": "['Jiacheng Ma', 'Gefei Zuo', 'Kevin Loughlin', 'Haoyang Zhang', 'Andrew Quinn', 'Baris Kasikci']",
        "date": "February 2022",
        "source": "ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "Software and hardware development cycles have traditionally been quite distinct. Software allows post-deployment patches, which leads to a rapid development cycle. In contrast, hardware bugs that are found after fabrication are extremely costly to fix (and sometimes even unfixable), so the traditional hardware development cycle involves massive investment in extensive simulation and formal verification. Reconfigurable hardware, such as a Field Programmable Gate Array (FPGA), promises to propel hardware development towards an agile software-like development approach, since it enables a hardware developer to patch bugs that are detected during on-chip testing or in production. Unfortunately, FPGA programmers lack bug localization tools amenable to this rapid development cycle, since past tools mainly find bugs via simulation and verification. To develop hardware bug localization tools for a rapid development cycle, a thorough understanding of the symptoms, root causes, and fixes of hardware bugs is needed.   In this paper, we first study bugs in existing FPGA designs and produce a testbed of reliably-reproducible bugs. We classify the bugs according to their intrinsic properties, symptoms, and root causes. We demonstrate that many hardware bugs are comparable to software bug counterparts, and would benefit from similar techniques for bug diagnosis and repair. Based upon our findings, we build a novel collection of hybrid static/dynamic program analysis and monitoring tools for debugging FPGA designs, showing that our tools enable a software-like development cycle by effectively reducing developers' manual efforts for bug localization.",
        "link": "https://dl.acm.org/doi/10.1145/3503222.3507701",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Enhancements for Hardware-based IEEE802.1CB embedded in Automotive Gateway System-on-Chip",
        "authors": "['Angela Gonzalez Mariño', 'Abdoul Aziz Kane', 'Francesc Fons', 'Juan Manuel Moreno Arostegui']",
        "date": "December 2021",
        "source": "ANCS '21: Proceedings of the Symposium on Architectures for Networking and Communications Systems",
        "abstract": "In this work, authors present a Hardware based strategy for IEEE802.1CB Network Reliability embedded in Automotive Gateways (GW). It is a new approach for HW efficient and cost-effective integration of Frame Replication and Elimination for Reliability (FRER) algorithm within automotive Network-on-Chip / System-on-Chip. In essence, it is a HW architecture that permits to manage the complex integration of IEEE802.1CB within In-Vehicle Networks. The FRER algorithm is split into smaller functionalities which are allocated across the different processing stages of the GW, maximizing device and network performance. The proposed architecture has a strong focus on Functional Safety, introducing important enhancements to overcome IEEE802.1CB limitations identified in the state of the art: data content verification, meaningful network diagnosability and performance verification. It moves a step forward towards fail operational systems and the compliance with ISO 26262, contributing to future autonomous driving networking solutions.",
        "link": "https://dl.acm.org/doi/10.1145/3493425.3502754",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "PRISM: Strong Hardware Isolation-based Soft-Error Resilient Multicore Architecture with High Performance and Availability at Low Hardware Overheads",
        "authors": "['Hamza Omar', 'Omer Khan']",
        "date": "None",
        "source": "ACM Transactions on Architecture and Code Optimization",
        "abstract": "Multicores increasingly deploy safety-critical parallel applications that demand resiliency against soft-errors to satisfy the safety standards. However, protection against these errors is challenging due to complex communication and data access protocols that aggressively share on-chip hardware resources. Research has explored various temporal and spatial redundancy-based resiliency schemes that provide multicores with high soft-error coverage. However, redundant execution incurs performance overheads due to interference effects induced by aggressive resource sharing. Moreover, these schemes require intrusive hardware modifications and fall short in providing efficient system availability guarantees. This article proposes PRISM, a resilient multicore architecture that incorporates strong hardware isolation to form redundant clusters of cores, ensuring a non-interference-based redundant execution environment. A soft error in one cluster does not effect the execution of the other cluster, resulting in high system availability. Implementing strong isolation for shared hardware resources, such as queues, caches, and networks requires logic for partitioning. However, it is less intrusive as complex hardware modifications to protocols, such as hardware cache coherence, are avoided. The PRISM approach is prototyped on a real Tilera Tile-Gx72 processor that enables primitives to implement the proposed cluster-level hardware resource isolation. The evaluation shows performance benefits from avoiding destructive hardware interference effects with redundant execution, while delivering superior system availability.",
        "link": "https://dl.acm.org/doi/10.1145/3450523",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "HeapCheck: Low-cost Hardware Support for Memory Safety",
        "authors": "['Gururaj Saileshwar', 'Rick Boivie', 'Tong Chen', 'Benjamin Segal', 'Alper Buyuktosunoglu']",
        "date": "None",
        "source": "ACM Transactions on Architecture and Code Optimization",
        "abstract": "Programs written in C/C++ are vulnerable to memory-safety errors like buffer-overflows and use-after-free. While several mechanisms to detect such errors have been previously proposed, they suffer from a variety of drawbacks, including poor performance, imprecise or probabilistic detection of errors, or requiring invasive changes to the ISA, binary-layout, or source-code that results in compatibility issues. As a result, memory-safety errors continue to be hard to detect and a principal cause of security problems.In this work, we present a minimally invasive and low-cost hardware-based memory-safety checking framework for detecting out-of-bounds accesses and use-after-free errors. The key idea of our mechanism is to re-purpose some of the “unused bits” in a pointer in 64-bit architectures to store an index into a bounds information table that can be used to catch out-bounds errors and use-after-free errors without any change to the binary layout. Using this memory-safety checking framework, we enable HeapCheck, a design for detecting Out-of-bounds and Use-after-free accesses for heap-objects, that are responsible for the majority of memory-safety errors in the wild. Our evaluations using C/C++ SPEC CPU 2017 workloads on Gem5 show that our solution incurs 1.5% slowdown on average, using an 8 KB on-chip SRAM cache for caching bounds-information. Our mechanism allows detection of out-of-bounds errors in user-code as well as in unmodified shared-library functions. Our mechanism has detected out-of-bounds accesses in 87 lines of code in the SPEC CPU 2017 benchmarks, primarily in Glibc v2.27 functions, that, to our knowledge, have not been previously detected even with popular tools like Address Sanitizer.",
        "link": "https://dl.acm.org/doi/10.1145/3495152",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Conware: Automated Modeling of Hardware Peripherals",
        "authors": "['Chad Spensky', 'Aravind Machiry', 'Nilo Redini', 'Colin Unger', 'Graham Foster', 'Evan Blasband', 'Hamed Okhravi', 'Christopher Kruegel', 'Giovanni Vigna']",
        "date": "May 2021",
        "source": "ASIA CCS '21: Proceedings of the 2021 ACM Asia Conference on Computer and Communications Security",
        "abstract": "Emulation is at the core of many security analyses. However, emulating embedded systems is still not possible in most cases. To facilitate this critical analysis, we present Conware, a hardware emulation framework that can automatically generate models for hardware peripherals, which alleviates one of the major challenges currently hindering embedded systems emulation. Conware enables individual peripherals to be modeled, exported, and combined with other peripherals in a pluggable fashion. Conware achieves this by first obtaining a recording of the low-level hardware interactions between the firmware and the peripheral, using either existing methods or our source-code instrumentation technique. These recordings are then used to create high-fidelity automata representations of the peripheral using novel automata-generation techniques. The various models can then be merged to facilitate full-system emulation of any embedded firmware that uses any of the modeled peripherals, even if that specific firmware or its target hardware was never directly instrumented. Indeed, we demonstrate that Conware is able to successfully emulate a peripheral-heavy firmware binary that was never instrumented, by merging the models of six unique peripherals that were trained on a development board using only the vendor-provided example code.",
        "link": "https://dl.acm.org/doi/10.1145/3433210.3437532",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Hardware Security for and beyond CMOS Technology",
        "authors": "['Johann Knechtel']",
        "date": "March 2021",
        "source": "ISPD '21: Proceedings of the 2021 International Symposium on Physical Design",
        "abstract": "As with most aspects of electronic systems and integrated circuits, hardware security has traditionally evolved around the dominant CMOS technology. However, with the rise of various emerging technologies, whose main purpose is to overcome the fundamental limitations for scaling and power consumption of CMOS technology, unique opportunities arise to advance the notion of hardware security. In this paper, I first provide an overview on hardware security in general. Next, I review selected emerging technologies, namely (i) spintronics, (ii) memristors, (iii) carbon nanotubes and related transistors, (iv) nanowires and related transistors, and (v) 3D and 2.5D integration. I then discuss their application to advance hardware security and also outline related challenges.",
        "link": "https://dl.acm.org/doi/10.1145/3439706.3446902",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Hardware Performance Counters: Ready-Made vs Tailor-Made",
        "authors": "['Abraham Peedikayil Kuruvila', 'Anushree Mahapatra', 'Ramesh Karri', 'Kanad Basu']",
        "date": "None",
        "source": "ACM Transactions on Embedded Computing Systems",
        "abstract": "Micro-architectural footprints can be used to distinguish one application from another. Most modern processors feature hardware performance counters to monitor the various micro-architectural events when an application is executing. These ready-made hardware performance counters can be used to create program fingerprints and have been shown to successfully differentiate between individual applications. In this paper, we demonstrate how ready-made hardware performance counters, due to their coarse-grain nature (low sampling rate and bundling of similar events, e.g., number of instructions instead of number of add instructions), are insufficient to this end. This observation motivates exploration of tailor-made hardware performance counters to capture fine-grain characteristics of the programs. As a case study, we evaluate both ready-made and tailor-made hardware performance counters using post-quantum cryptographic key encapsulation mechanism implementations. Machine learning models trained on tailor-made hardwareperformance counter streams demonstrate that they can uniquely identify the behavior of every post-quantum cryptographic key encapsulation mechanism algorithm with at least 98.99% accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3476996",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Hardware Acceleration for Embedded Keyword Spotting: Tutorial and Survey",
        "authors": "['J. S. P. Giraldo', 'Marian Verhelst']",
        "date": "None",
        "source": "ACM Transactions on Embedded Computing Systems",
        "abstract": "In recent years, Keyword Spotting (KWS) has become a crucial human–machine interface for mobile devices, allowing users to interact more naturally with their gadgets by leveraging their own voice. Due to privacy, latency and energy requirements, the execution of KWS tasks on the embedded device itself instead of in the cloud, has attracted significant attention from the research community. However, the constraints associated with embedded systems, including limited energy, memory, and computational capacity, represent a real challenge for the embedded deployment of such interfaces. In this article, we explore and guide the reader through the design of KWS systems. To support this overview, we extensively survey the different approaches taken by the recent state-of-the-art (SotA) at the algorithmic, architectural, and circuit level to enable KWS tasks in edge, devices. A quantitative and qualitative comparison between relevant SotA hardware platforms is carried out, highlighting the current design trends, as well as pointing out future research directions in the development of this technology.",
        "link": "https://dl.acm.org/doi/10.1145/3474365",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Integrated Hardware Garbage Collection",
        "authors": "['Andrés Amaya García', 'David May', 'Ed Nutting']",
        "date": "None",
        "source": "ACM Transactions on Embedded Computing Systems",
        "abstract": "Garbage collected programming languages, such as Python and C#, have accelerated software development. These modern languages increase productivity and software reliability as they provide high-level data representation and control structures. Modern languages are widely used in software development for mobile, desktop, and server devices, but their adoption is limited in real-time embedded systems.There is clear interest in supporting modern languages in embedded devices as emerging markets, like the Internet of Things, demand ever smarter and more reliable products. Multiple commercial and open-source projects, such as Zerynth and MicroPython, are attempting to provide support. But these projects rely on software garbage collectors that impose high overheads and introduce unpredictable pauses, preventing their use in many embedded applications. These limitations arise from the unsuitability of conventional processors for performing efficient, predictable garbage collection.We propose the Integrated Hardware Garbage Collector (IHGC); a garbage collector tightly coupled with the processor that runs continuously in the background. Further, we introduce a static analysis technique to guarantee that real-time programs are never paused by the collector. Our design allocates a memory cycle to the collector when the processor is not using the memory. The IHGC achieves this by careful division of collection work into single-memory-access steps that are interleaved with the processor’s memory accesses. As a result, our collector eliminates run-time overheads and enables real-time program analysis.The principles behind the IHGC can be used in conjunction with existing architectures. For example, we simulated the IHGC alongside the ARMv6-M architecture. Compared to a conventional processor, our experiments indicate that the IHGC offers 1.5–7 times better performance for programs that rely on garbage collection. The IHGC delivers the benefits of garbage-collected languages with real-time performance but without the complexity and overheads inherent in software collectors.",
        "link": "https://dl.acm.org/doi/10.1145/3450147",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Hardware Context Switch-based Cryptographic Accelerator for Handling Multiple Streams",
        "authors": "['Arif Sasongko', 'I. M. Narendra Kumara', 'Arief Wicaksana', 'Frédéric Rousseau', 'Olivier Muller']",
        "date": "None",
        "source": "ACM Transactions on Reconfigurable Technology and Systems",
        "abstract": "The confidentiality and integrity of a stream has become one of the biggest issues in telecommunication. The best available algorithm handling the confidentiality of a data stream is the symmetric key block cipher combined with a chaining mode of operation such as cipher block chaining (CBC) or counter mode (CTR). This scheme is difficult to accelerate using hardware when multiple streams coexist. This is caused by the computation time requirement and mainly by management of the streams. In most accelerators, computation is treated at the block-level rather than as a stream, making the management of multiple streams complex. This article presents a solution combining CBC and CTR modes of operation with a hardware context switching. The hardware context switching allows the accelerator to treat the data as a stream. Each stream can have different parameters: key, initialization value, state of counter. Stream switching was managed by the hardware context switching mechanism. A high-level synthesis tool was used to generate the context switching circuit. The scheme was tested on three cryptographic algorithms: AES, DES, and BC3. The hardware context switching allowed the software to manage multiple streams easily, efficiently, and rapidly. The software was freed of the task of managing the stream state. Compared to the original algorithm, about 18%–38% additional logic elements were required to implement the CBC or CTR mode and the additional circuits to support context switching. Using this method, the performance overhead when treating multiple streams was low, and the performance was comparable to that of existing hardware accelerators not supporting multiple streams.",
        "link": "https://dl.acm.org/doi/10.1145/3460941",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A Runtime Reconfigurable Design of Compute-in-Memory–Based Hardware Accelerator for Deep Learning Inference",
        "authors": "['Anni Lu', 'Xiaochen Peng', 'Yandong Luo', 'Shanshi Huang', 'Shimeng Yu']",
        "date": "None",
        "source": "ACM Transactions on Design Automation of Electronic Systems",
        "abstract": "Compute-in-memory (CIM) is an attractive solution to address the “memory wall” challenges for the extensive computation in deep learning hardware accelerators. For custom ASIC design, a specific chip instance is restricted to a specific network during runtime. However, the development cycle of the hardware is normally far behind the emergence of new algorithms. Although some of the reported CIM-based architectures can adapt to different deep neural network (DNN) models, few details about the dataflow or control were disclosed to enable such an assumption. Instruction set architecture (ISA) could support high flexibility, but its complexity would be an obstacle to efficiency. In this article, a runtime reconfigurable design methodology of CIM-based accelerators is proposed to support a class of convolutional neural networks running on one prefabricated chip instance with ASIC-like efficiency. First, several design aspects are investigated: (1) the reconfigurable weight mapping method; (2) the input side of data transmission, mainly about the weight reloading; and (3) the output side of data processing, mainly about the reconfigurable accumulation. Then, a system-level performance benchmark is performed for the inference of different DNN models, such as VGG-8 on a CIFAR-10 dataset and AlexNet GoogLeNet, ResNet-18, and DenseNet-121 on an ImageNet dataset to measure the trade-offs between runtime reconfigurability, chip area, memory utilization, throughput, and energy efficiency.",
        "link": "https://dl.acm.org/doi/10.1145/3460436",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Effective simulation and debugging for a high-level hardware language using software compilers",
        "authors": "['Clément Pit-Claudel', 'Thomas Bourgeat', 'Stella Lau', 'Arvind', 'Adam Chlipala']",
        "date": "April 2021",
        "source": "ASPLOS '21: Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "Rule-based hardware-design languages (RHDLs) promise to enhance developer productivity by offering convenient abstractions. Advanced compiler technology keeps the cost of these abstractions low, generating circuits with excellent area and timing properties.   Unfortunately, comparatively little effort has been spent on building simulators and debuggers for these languages, so users often simulate and debug their designs at the RTL level. This is problematic because generated circuits typically suffer from poor readability, as compiler optimizations can break high-level abstractions. Worse, optimizations that operate under the assumption that concurrency is essentially free yield faster circuits but often actively hurt simulation performance on platforms with limited concurrency, like desktop computers or servers.   This paper demonstrates the benefits of completely separating the simulation and synthesis pipelines. We propose a new approach, yielding the first compiler designed for effective simulation and debugging of a language in the Bluespec family. We generate cycle-accurate C++ models that are readable, compatible with a wide range of traditional software-debugging tools, and fast (often two to three times faster than circuit-level simulation). We achieve these results by optimizing for sequential performance and using static analysis to minimize redundant work. The result is a vastly improved hardware-design experience, which we demonstrate on embedded processor designs and DSP building blocks using performance benchmarks and debugging case studies.",
        "link": "https://dl.acm.org/doi/10.1145/3445814.3446720",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "The hardware lottery",
        "authors": "['Sara Hooker']",
        "date": "December 2021",
        "source": "Communications of the ACM",
        "abstract": "After decades of incentivizing the isolation of hardware, software, and algorithm development, the catalysts for closer collaboration are changing the paradigm.",
        "link": "https://dl.acm.org/doi/10.1145/3467017",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Hardware Support to Improve Fuzzing Performance and Precision",
        "authors": "['Ren Ding', 'Yonghae Kim', 'Fan Sang', 'Wen Xu', 'Gururaj Saileshwar', 'Taesoo Kim']",
        "date": "November 2021",
        "source": "CCS '21: Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security",
        "abstract": "Coverage-guided fuzzing is considered one of the most efficient bug-finding techniques, given its number of bugs reported. However, coverage tracing provided by existing software-based approaches, such as source instrumentation and dynamic binary translation, can incur large overhead. Hindered by the significantly lowered execution speed, it also becomes less beneficial to improve coverage feedback by incorporating additional execution states. In this paper, we propose SNAP, a customized hardware platform that implements hardware primitives to enhance the performance and precision of coverage-guided fuzzing. By sitting at the bottom of the computer stack, SNAP leverages the existing CPU pipeline and micro-architectural features to provide coverage tracing and rich execution semantics with near-zero cost regardless of source code availability. Prototyped as a synthesized RISC-V BOOM processor on FPGA, SNAP incurs a barely 3.1% tracing overhead on the SPEC benchmarks while achieving a 228x higher fuzzing throughput than the existing software-based solution. Posing only a 4.8% area and 6.5% power overhead, SNAP is highly practical and can be adopted by existing CPU architectures with minimal changes.",
        "link": "https://dl.acm.org/doi/10.1145/3460120.3484573",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A Software/Hardware Co-Design of Crystals-Dilithium Signature Scheme",
        "authors": "['Zhen Zhou', 'Debiao He', 'Zhe Liu', 'Min Luo', 'Kim-Kwang Raymond Choo']",
        "date": "None",
        "source": "ACM Transactions on Reconfigurable Technology and Systems",
        "abstract": "As quantum computers become more affordable and commonplace, existing security systems that are based on classical cryptographic primitives, such as RSA and Elliptic Curve Cryptography (ECC), will no longer be secure. Hence, there has been interest in designing post-quantum cryptographic (PQC) schemes, such as those based on lattice-based cryptography (LBC). The potential of LBC schemes is evidenced by the number of such schemes passing the selection of NIST PQC Standardization Process Round-3. One such scheme is the Crystals-Dilithium signature scheme, which is based on the hard module-lattice problem. However, there is no efficient implementation of the Crystals-Dilithium signature scheme. Hence, in this article, we present a compact hardware architecture containing elaborate modular multiplication units using the Karatsuba algorithm along with smart generators of address sequence and twiddle factors for NTT, which can complete polynomial addition/multiplication with the parameter setting of Dilithium in a short clock period. Also, we propose a fast software/hardware co-design implementation on Field Programmable Gate Array (FPGA) for the Dilithium scheme with a tradeoff between speed and resource utilization. Our co-design implementation outperforms a pure C implementation on a Nios-II processor of the platform Altera DE2-115, in the sense that our implementation is 11.2 and 7.4 times faster for signature and verification, respectively. In addition, we also achieve approximately 51% and 31% speed improvement for signature and verification, in comparison to the pure C implementation on processor ARM Cortex-A9 of ZYNQ-7020 platform.",
        "link": "https://dl.acm.org/doi/10.1145/3447812",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Hardware-accelerated Simulation-based Inference of Stochastic Epidemiology Models for COVID-19",
        "authors": "['Sourabh Kulkarni', 'Mario Michael Krell', 'Seth Nabarro', 'Csaba Andras Moritz']",
        "date": "None",
        "source": "ACM Journal on Emerging Technologies in Computing Systems",
        "abstract": "Epidemiology models are central to understanding and controlling large-scale pandemics. Several epidemiology models require simulation-based inference such as Approximate Bayesian Computation (ABC) to fit their parameters to observations. ABC inference is highly amenable to efficient hardware acceleration. In this work, we develop parallel ABC inference of a stochastic epidemiology model for COVID-19. The statistical inference framework is implemented and compared on Intel’s Xeon CPU, NVIDIA’s Tesla V100 GPU, Google’s V2 Tensor Processing Unit (TPU), and the Graphcore’s Mk1 Intelligence Processing Unit (IPU), and the results are discussed in the context of their computational architectures. Results show that TPUs are 3×, GPUs are 4×, and IPUs are 30× faster than Xeon CPUs. Extensive performance analysis indicates that the difference between IPU and GPU can be attributed to higher communication bandwidth, closeness of memory to compute, and higher compute power in the IPU. The proposed framework scales across 16 IPUs, with scaling overhead not exceeding 8% for the experiments performed. We present an example of our framework in practice, performing inference on the epidemiology model across three countries and giving a brief overview of the results.",
        "link": "https://dl.acm.org/doi/10.1145/3471188",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Understanding and utilizing hardware transactional memory capacity",
        "authors": "['Zixian Cai', 'Stephen M. Blackburn', 'Michael D. Bond']",
        "date": "June 2021",
        "source": "ISMM 2021: Proceedings of the 2021 ACM SIGPLAN International Symposium on Memory Management",
        "abstract": "Hardware transactional memory (HTM) provides a simpler programming model than lock-based synchronization. However, HTM has limits that mean that transactions may suffer costly capacity aborts. Understanding HTM capacity is therefore critical. Unfortunately, crucial implementation details are undisclosed. In practice HTM capacity can manifest in puzzling ways. It is therefore unsurprising that the literature reports results that appear to be highly contradictory, reporting capacities that vary by nearly three orders of magnitude. We conduct an in-depth study into the causes of HTM capacity aborts using four generations of Intel's Transactional Synchronization Extensions (TSX). We identify the apparent contradictions among prior work, and shed new light on the causes of HTM capacity aborts. In doing so, we reconcile the apparent contradictions. We focus on how replacement policies and the status of the cache can affect HTM capacity.   One source of surprising behavior appears to be the cache replacement policies used by the processors we evaluated. Both invalidating the cache and warming it up with the transactional working set can significantly improve the read capacity of transactions across the microarchitectures we tested. A further complication is that a physically indexed LLC will typically yield only half the total LLC capacity. We found that methodological differences in the prior work led to different warmup states and thus to their apparently contradictory findings. This paper deepens our understanding of how the underlying implementation and cache behavior affect the apparent capacity of HTM. Our insights on how to increase the read capacity of transactions can be used to optimize HTM applications, particularly those with large read-mostly transactions, which are common in the context of optimistic parallelization.",
        "link": "https://dl.acm.org/doi/10.1145/3459898.3463901",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A Hardware Accelerator for Protocol Buffers",
        "authors": "['Sagar Karandikar', 'Chris Leary', 'Chris Kennelly', 'Jerry Zhao', 'Dinesh Parimi', 'Borivoje Nikolic', 'Krste Asanovic', 'Parthasarathy Ranganathan']",
        "date": "October 2021",
        "source": "MICRO '21: MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture",
        "abstract": "Serialization frameworks are a fundamental component of scale-out systems, but introduce significant compute overheads. However, they are amenable to acceleration with specialized hardware. To understand the trade-offs involved in architecting such an accelerator, we present the first in-depth study of serialization framework usage at scale by profiling Protocol Buffers (“protobuf”) usage across Google’s datacenter fleet. We use this data to build HyperProtoBench, an open-source benchmark representative of key serialization-framework user services at scale. In doing so, we identify key insights that challenge prevailing assumptions about serialization framework usage.  We use these insights to develop a novel hardware accelerator for protobufs, implemented in RTL and integrated into a RISC-V SoC. Applications can easily harness the accelerator, as it integrates with a modified version of the open-source protobuf library and is wire-compatible with standard protobufs. We have fully open-sourced our RTL, which, to the best of our knowledge, is the only such implementation currently available to the community.  We also present a first-of-its-kind, end-to-end evaluation of our entire RTL-based system running hyperscale-derived benchmarks and microbenchmarks. We boot Linux on the system using FireSim to run these benchmarks and implement the design in a commercial 22nm FinFET process to obtain area and frequency metrics. We demonstrate an average 6.2 × to 11.2 × performance improvement vs. our baseline RISC-V SoC with BOOM OoO cores and despite the RISC-V SoC’s weaker uncore/supporting components, an average 3.8 × improvement vs. a Xeon-based server.",
        "link": "https://dl.acm.org/doi/10.1145/3466752.3480051",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Dead flit attack on NoC by hardware trojan and its impact analysis",
        "authors": "['Mohammad Humam Khan', 'Ruchika Gupta', 'John Jose', 'Sukumar Nandi']",
        "date": "October 2021",
        "source": "NoCArc '21: Proceedings of the 14th International Workshop on Network on Chip Architectures",
        "abstract": "With the advancement in VLSI technology, Tiled Chip Multicore Processors (TCMPs) with packet switched Network-on-Chip (NoC) have emerged as the most popular design choice for compute and data intensive embedded and parallel systems. Tight time-to-market constraints and budget limitations have forced the designers to explore the possibilities of using several third party Intellectual Property (IP) cores. Use of such unsecured inexpensive third party IPs may pose severe security challenges that are not detected at manufacturing and testing phases. Recent research shows that manipulation of the NoC packet content by Hardware Trojan (HT) has the potential to disrupt the on-chip communication resulting in application level stalling. We model a novel HT that alters the common prefix field of NoC packets leading to the creation of dead flits in router buffers. We introduce two variants of this proposed HT: one that modifies head flit to body flit and another one that modifies the body flit to head flit. We analyze the HT impact at core level, cache level, and NoC level. The experimental analysis on a 16-core TCMP demonstrates that the proposed HT significantly reduces IPC, increases the average cache miss penalty, and increases the average buffer occupancy of selected packets in NoC.",
        "link": "https://dl.acm.org/doi/10.1145/3477231.3490425",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Packet header attack by hardware trojan in NoC based TCMP and its impact analysis",
        "authors": "['Vedika J. Kulkarni', 'R. Manju', 'Ruchika Gupta', 'John Jose', 'Sukumar Nandi']",
        "date": "October 2021",
        "source": "NOCS '21: Proceedings of the 15th IEEE/ACM International Symposium on Networks-on-Chip",
        "abstract": "With the advancement of VLSI technology, Tiled Chip Multicore Processors (TCMP) with packet switched Network-on-Chip (NoC) have been emerged as the backbone of the modern data intensive parallel systems. Due to tight time-to-market constraints, manufacturers are exploring the possibility of integrating several third-party Intellectual Property (IP) cores in their TCMP designs. Presence of malicious Hardware Trojan (HT) in the NoC routers can adversely affect communication between tiles leading to degradation of overall system performance. In this paper, we model an HT mounted on the input buffers of NoC routers that can alter the destination address field of selected NoC packets. We study the impact of such HTs and analyse its first and second order impacts at the core level, cache level, and NoC level both quantitatively and qualitatively. Our experimental study shows that the proposed HT can bring application to a complete halt by stalling instruction issue and can significantly impact the miss penalty of L1 caches. The impact of re-transmission techniques in the context of HT impacted packets getting discarded is also studied. We also expose the unrealistic assumptions and unacceptable latency overheads of existing mitigation techniques for packet header attacks and emphasise the need for alternative cost effective HT management techniques for the same.",
        "link": "https://dl.acm.org/doi/10.1145/3479876.3481597",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Hardware Trojan Horse Detection through Improved Switching of Dormant Nets",
        "authors": "['Tapobrata Dhar', 'Surajit Kumar Roy', 'Chandan Giri']",
        "date": "None",
        "source": "ACM Journal on Emerging Technologies in Computing Systems",
        "abstract": "Covert Hardware Trojan Horses (HTH) introduced by malicious attackers during the fabless manufacturing process of integrated circuits (IC) have the potential to cause malignant functions within the circuit. This article employs a Design-for-Security technique to detect any HTHs present in the circuit by inserting tri-state buffers (TSB) in the ICs that inject the internal nets with weighted logic values during the test phase. This increases the transitions in the logic values of the nets within the IC, thereby stimulating any inserted HTH circuits. The TSBs are efficiently inserted in the IC considering various circuit parameters and testability measures to bolster the transitions in logic values of the nets throughout the IC while minimising the area overhead. Simulation results show a significant increase in transitions in logic values within HTH triggers using this method, thus aiding in their detection through side-channel analysis or direct activation of the payload.",
        "link": "https://dl.acm.org/doi/10.1145/3439951",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Better Keep Cash in Your Boots - Hardware Wallets are the New Single Point of Failure",
        "authors": "['Adrian Dabrowski', 'Katharina Pfeffer', 'Markus Reichel', 'Alexandra Mai', 'Edgar R. Weippl', 'Michael Franz']",
        "date": "November 2021",
        "source": "DeFi '21: Proceedings of the 2021 ACM CCS Workshop on Decentralized Finance and Security",
        "abstract": "Hardware wallets are currently considered the most secure way to manage cryptocurrency keys and sign transactions. However, previous publications show that such tokens can be replaced or manipulated in a number of hard-to-detect ways pre- or post-delivery to the user and that implemented (remote) attestation and authenticity checks fail their purpose for multiple reasons. We analyzed the architecture of current products by examining their initialization procedure and attestation methods. Unlike previous publications, we found that tightened attestation and communications encryption will not solve the fundamental architectural flaws sustainably. We conclude that the architecture of current-generation cryptocurrency hardware wallets missed the opportunity for a resilient design by copying the PC's wallet architecture and thus merely shifting the single point of trust from the PC to the hardware wallet. We advocate a mutually verified architecture through changes to BIP32/BIP44 wallet architectures to incorporate collaborative signatures and key generation. This way, neither a compromised wallet nor a compromised PC can meaningfully manipulate keys or transactions.",
        "link": "https://dl.acm.org/doi/10.1145/3464967.3488588",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Revamping storage class memory with hardware automated memory-over-storage solution",
        "authors": "['Jie Zhang', 'Miryeong Kwon', 'Donghyun Gouk', 'Sungjoon Koh', 'Nam Sung Kim', 'Mahmut Taylan Kandemir', 'Myoungsoo Jung']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "Large persistent memories such as NVDIMM have been perceived as a disruptive memory technology, because they can maintain the state of a system even after a power failure and allow the system to recover quickly. However, overheads incurred by a heavy software-stack intervention seriously negate the benefits of such memories. First, to significantly reduce the software stack overheads, we propose HAMS, a hardware automated Memory-over-Storage (MoS) solution. Specifically, HAMS aggregates the capacity of NVDIMM and ultra-low latency flash archives (ULL-Flash) into a single large memory space, which can be used as a working memory expansion or persistent memory expansion, in an OS-transparent manner. HAMS resides in the memory controller hub and manages its MoS address pool over conventional DDR and NVMe interfaces; it employs a simple hardware cache to serve all the memory requests from the host MMU after mapping the storage space of ULL-Flash to the memory space of NVDIMM. Second, to make HAMS more energy-efficient and reliable, we propose an \"advanced HAMS\" which removes unnecessary data transfers between NVDIMM and ULL-Flash after optimizing the datapath and hardware modules of HAMS. This approach unleashes the ULL-Flash and its NVMe controller from the storage box and directly connects the HAMS datapath to NVDIMM over the conventional DDR4 interface. Our evaluations show that HAMS and advanced HAMS can offer 97% and 119% higher system performance than a software-based NVDIMM design, while costing 41% and 45% lower energy, respectively.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00065",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A Pipelined Multi-Tiered Hardware Acceleration Approach Towards Content Addressable Binary Arithmetic",
        "authors": "['Shunjun Qian', 'Yendo Hu', 'Yiliang Wu', 'Weijie Yang', 'Xue Bai', 'Rui Shao']",
        "date": "October 2021",
        "source": "EITCE '21: Proceedings of the 2021 5th International Conference on Electronic Information Technology and Computer Engineering",
        "abstract": "This paper proposes a low-latency multi-layer pipelined hardware implementation scheme of context based adaptive binary arithmetic coding (CABAC) for H.265/HEVC, which improves the coding performance and reduces the encoding latency with less hardware overhead. It is verified, debugged and optimized on the FPGA hardware platform. From the synthesis results on Xilinx K7 series FPGA, it can be seen that at 200MHz clock, the CABAC encoding of this scheme only uses 12K LUT resources, supports 1080P60Hz video encoding, and the encoding latency is as low as 6.7us. Experiments and calculations show that the proposed scheme can complete 8K UHD real-time video encoding using 59.71K logic gates at 800MHz clock on ASIC chip.",
        "link": "https://dl.acm.org/doi/10.1145/3501409.3501638",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "InTrust-IoT: Intelligent Ecosystem based on Power Profiling of Trusted device(s) in IoT for Hardware Trojan Detection",
        "authors": "['Hawzhin Mohammed', 'Faiq Khalid', 'Paul Sawyer', 'Gabriella Cataloni', 'Syed Rafay Hasan']",
        "date": "October 2021",
        "source": "HASP '21: Proceedings of the 10th International Workshop on Hardware and Architectural Support for Security and Privacy",
        "abstract": "Modern Resource-Constrained (RC) Internet of Things (IoT) devices are subject to several types of attacks, including hardware-level attacks. Most of the existing state-of-the-art solutions are invasive, require expensive design time interventions, or need dataset generation from non-trusted RC-IoT devices or both. We argue that the health of modern RC-IoT devices requires a final line of defense against possible hardware attacks that go undetected during the IC design and test process. Hence, in this paper, we propose a defense methodology against non-zero-day and zero-day attacks, leveraging machine learning techniques trained on the dataset obtained without design time intervention and using ‘only’ trusted IoT devices. In the process, a complete eco-system is developed where data is generated through a trusted group of devices, and machine learning is done on these trusted datasets. Next, this trusted trained model is deployed in regular IoT systems that contain untrusted devices, where the attack on untrusted devices can be detected in real-time. Our results indicate that for non-zero-day attacks, the proposed technique can concurrently detect DoS and power depletion attacks with an accuracy of about 80%. Similarly, zero-day attack experiments are able to detect the attack without fail as well.",
        "link": "https://dl.acm.org/doi/10.1145/3505253.3505262",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "MNEMOSENE: Tile Architecture and Simulator for Memristor-based Computation-in-memory",
        "authors": "['Mahdi Zahedi', 'Muah Abu Lebdeh', 'Christopher Bengel', 'Dirk Wouters', 'Stephan Menzel', 'Manuel Le Gallo', 'Abu Sebastian', 'Stephan Wong', 'Said Hamdioui']",
        "date": "None",
        "source": "ACM Journal on Emerging Technologies in Computing Systems",
        "abstract": "In recent years, we are witnessing a trend toward in-memory computing for future generations of computers that differs from traditional von-Neumann architecture in which there is a clear distinction between computing and memory units. Considering that data movements between the central processing unit (CPU) and memory consume several orders of magnitude more energy compared to simple arithmetic operations in the CPU, in-memory computing will lead to huge energy savings as data no longer needs to be moved around between these units. In an initial step toward this goal, new non-volatile memory technologies, e.g., resistive RAM (ReRAM) and phase-change memory (PCM), are being explored. This has led to a large body of research that mainly focuses on the design of the memory array and its peripheral circuitry. In this article, we mainly focus on the tile architecture (comprising a memory array and peripheral circuitry) in which storage and compute operations are performed in the (analog) memory array and the results are produced in the (digital) periphery. Such an architecture is termed compute-in-memory-periphery (CIM-P). More precisely, we derive an abstract CIM-tile architecture and define its main building blocks. To bridge the gap between higher-level programming languages and the underlying (analog) circuit designs, an instruction-set architecture is defined that is intended to control and, in turn, sequence the operations within this CIM tile to perform higher-level more complex operations. Moreover, we define a procedure to pipeline the CIM-tile operations to further improve the performance. To simulate the tile and perform design space exploration considering different technologies and parameters, we introduce the fully parameterized first-of-its-kind CIM tile simulator and compiler. Furthermore, the compiler is technology-aware when scheduling the CIM-tile instructions. Finally, using the simulator, we perform several preliminary design space explorations regarding the three competing technologies, ReRAM, PCM, and STT-MRAM concerning CIM-tile parameters, e.g., the number of ADCs. Additionally, we investigate the effect of pipelining in relation to the clock speeds of the digital periphery assuming the three technologies. In the end, we demonstrate that our simulator is also capable of reporting energy consumption for each building block within the CIM tile after the execution of in-memory kernels considering the data-dependency on the energy consumption of the memory array. All the source codes are publicly available.",
        "link": "https://dl.acm.org/doi/10.1145/3485824",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Tapeout of a RISC-V crypto chip with hardware trojans: a case-study on trojan design and pre-silicon detectability",
        "authors": "['Alexander Hepp', 'Georg Sigl']",
        "date": "May 2021",
        "source": "CF '21: Proceedings of the 18th ACM International Conference on Computing Frontiers",
        "abstract": "This paper presents design and integration of four hardware Trojans (HTs) into a post-quantum-crypto-enhanced RISC-V micro-controller, which was taped-out in September 2020. We cover multiple HTs ranging from a simple denial-of-service HT to a side-channel HT transmitting arbitrary information to external observers. For each HT, we give estimations of the detectability by the microcontroller-integration team using design tools or by simulation. We conclude that some HTs are easily detected by design-tool warnings. Other powerful HTs, modifying software control flow, cause little disturbance, but require covert executable code modifications. With this work, we strengthen awareness for HT risks and present a realistic testing device for HT detection tools.",
        "link": "https://dl.acm.org/doi/10.1145/3457388.3458869",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Attack Mitigation of Hardware Trojans for Thermal Sensing via Micro-ring Resonator in Optical NoCs",
        "authors": "['Jun Zhou', 'Mengquan Li', 'Pengxing Guo', 'Weichen Liu']",
        "date": "None",
        "source": "ACM Journal on Emerging Technologies in Computing Systems",
        "abstract": "As an emerging role in new-generation on-chip communication, optical networks-on-chip (ONoCs) provide ultra-high bandwidth, low latency, and low power dissipation for data transfers. However, the thermo-optic effects of the photonic devices have a great impact on the operating performance and reliability of ONoCs, where the thermal-aware control with accurate measurements, e.g., thermal sensing, is typically applied to alleviate it. Besides, the temperature-sensitive ONoCs are prone to be attacked by the hardware Trojans (HTs) covertly embedded in the counterfeit integrated circuits (ICs) from the malicious third-party vendors, leading to performance degradation, denial-of-service (DoS), or even permanent damages. In this article, we focus on the tampering and snooping attacks during the thermal sensing via micro-ring resonator (MR) in ONoCs. Based on the provided workflow and attack model, a new structure of the anti-HT module is proposed to verify and protect the obtained data from the thermal sensor for attacks in its optical sampling and electronic transmission processes. In addition, we present the detection scheme based on the spiking neural networks (SNNs) to implement an accurate classification of the network security statuses for further high-level control. Evaluation results indicate that, with less than 1% extra area of a tile, our approach can significantly enhance the hardware security of thermal sensing for ONoC with trivial costs of up to 8.73%, 5.32%, and 6.14% in average latency, execution time, and energy consumption, respectively.",
        "link": "https://dl.acm.org/doi/10.1145/3433676",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "DirectNVM: Hardware-accelerated NVMe SSDs for High-performance Embedded Computing",
        "authors": "['Yu Zou', 'Amro Awad', 'Mingjie Lin']",
        "date": "None",
        "source": "ACM Transactions on Embedded Computing Systems",
        "abstract": "With data-intensive artificial intelligence (AI) and machine learning (ML) applications rapidly surging, modern high-performance embedded systems, with heterogeneous computing resources, critically demand low-latency and high-bandwidth data communication. As such, the newly emerging NVMe (Non-Volatile Memory Express) protocol, with parallel queuing, access prioritization, and optimized I/O arbitration, starts to be widely adopted as a de facto fast I/O communication interface. However, effectively leveraging the potential of modern NVMe storage proves to be nontrivial and demands fine-grained control, high processing concurrency, and application-specific optimization. Fortunately, modern FPGA devices, capable of efficient parallel processing and application-specific programmability, readily meet the underlying physical layer requirements of the NVMe protocol, therefore providing unprecedented opportunities to implementing a rich-featured NVMe middleware to benefit modern high-performance embedded computing.In this article, we present how to rethink existing accessing mechanisms of NVMe storage and devise innovative hardware-assisted solutions to accelerating NVMe data access performance for the high-performance embedded computing system. Our key idea is to exploit the massively parallel I/O queuing capability, provided by the NVMe storage system, through leveraging FPGAs’ reconfigurability and native hardware computing power to operate transparently to the main processor. Specifically, our DirectNVM system aims at providing effective hardware constructs for facilitating high-performance and scalable userspace storage applications through (1) hardening all the essential NVMe driver functionalities, therefore avoiding expensive OS syscalls and enabling zero-copy data access from the application, (2) relying on hardware for the I/O communication control instead of relying on OS-level interrupts that can significantly reduce both total I/O latency and its variance, and (3) exposing cutting-edge and application-specific weighted-round-robin I/O traffic scheduling to the userspace.To validate our design methodology, we developed a complete DirectNVM system utilizing the Xilinx Zynq MPSoC architecture that incorporates a high-performance application processor (APU) equipped with DDR4 system memory and a hardened configurable PCIe Gen3 block in its programmable logic part. We then measured the storage bandwidth and I/O latency of both our DirectNVM system and a conventional OS-based system when executing the standard FIO benchmark suite [2]. Specifically, compared against the PetaLinux built-in kernel driver code running on a Zynq MPSoC, our DirectNVM has shown to achieve up to 18.4× higher throughput and up to 4.5× lower latency. To ensure the fairness of our performance comparison, we also measured our DirectNVM system against the Intel SPDK [26], a highly optimized userspace asynchronous NVMe I/O framework running on a X86 PC system. Our experiment results have shown that our DirectNVM, even running on a considerably less powerful embedded ARM processor than a full-scale AMD processor, achieved up to 2.2× higher throughput and 1.3× lower latency. Furthermore, by experimenting with a multi-threading test case, we have demonstrated that our DirectNVM’s weighted-round-robin scheduling can significantly optimize the bandwidth allocation between latency-constraint frontend applications and other backend applications in real-time systems. Finally, we have developed a theoretical framework of performance modeling with classic queuing theory that can quantitatively define the relationship between a system’s I/O performance and its I/O implementation.",
        "link": "https://dl.acm.org/doi/10.1145/3463911",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Reliability analysis of a spiking neural network hardware accelerator",
        "authors": "['Theofilos Spyrou', 'Sarah A. El-Sayed', 'Engin Afacan', 'Luis A. Camuñas-Mesa', 'Bernabé Linares-Barranco', 'Haralampos-G. Stratigopoulos']",
        "date": "March 2022",
        "source": "DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe",
        "abstract": "Despite the parallelism and sparsity in neural network models, their transfer into hardware unavoidably makes them susceptible to hardware-level faults. Hardware-level faults can occur either during manufacturing, such as physical defects and process-induced variations, or in the field due to environmental factors and aging. The performance under fault scenarios needs to be assessed so as to develop cost-effective fault-tolerance schemes. In this work, we assess the resilience characteristics of a hardware accelerator for Spiking Neural Networks (SNNs) designed in VHDL and implemented on an FPGA. The fault injection experiments pinpoint the parts of the design that need to be protected against faults, as well as the parts that are inherently fault-tolerant.",
        "link": "https://dl.acm.org/doi/10.5555/3539845.3539935",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "FTT-NAS: Discovering Fault-tolerant Convolutional Neural Architecture",
        "authors": "['Xuefei Ning', 'Guangjun Ge', 'Wenshuo Li', 'Zhenhua Zhu', 'Yin Zheng', 'Xiaoming Chen', 'Zhen Gao', 'Yu Wang', 'Huazhong Yang']",
        "date": "None",
        "source": "ACM Transactions on Design Automation of Electronic Systems",
        "abstract": "With the fast evolvement of embedded deep-learning computing systems, applications powered by deep learning are moving from the cloud to the edge. When deploying neural networks (NNs) onto the devices under complex environments, there are various types of possible faults: soft errors caused by cosmic radiation and radioactive impurities, voltage instability, aging, temperature variations, malicious attackers, and so on. Thus, the safety risk of deploying NNs is now drawing much attention. In this article, after the analysis of the possible faults in various types of NN accelerators, we formalize and implement various fault models from the algorithmic perspective. We propose Fault-Tolerant Neural Architecture Search (FT-NAS) to automatically discover convolutional neural network (CNN) architectures that are reliable to various faults in nowadays devices. Then, we incorporate fault-tolerant training (FTT) in the search process to achieve better results, which is referred to as FTT-NAS. Experiments on CIFAR-10 show that the discovered architectures outperform other manually designed baseline architectures significantly, with comparable or fewer floating-point operations (FLOPs) and parameters. Specifically, with the same fault settings, F-FTT-Net discovered under the feature fault model achieves an accuracy of 86.2% (VS. 68.1% achieved by MobileNet-V2), and W-FTT-Net discovered under the weight fault model achieves an accuracy of 69.6% (VS. 60.8% achieved by ResNet-18). By inspecting the discovered architectures, we find that the operation primitives, the weight quantization range, the capacity of the model, and the connection pattern have influences on the fault resilience capability of NN models.",
        "link": "https://dl.acm.org/doi/10.1145/3460288",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Inferring DNN layer-types through a Hardware Performance Counters based Side Channel Attack",
        "authors": "['Bhargav Achary Dandpati Kumar', 'Sai Chandra Teja R', 'Sparsh Mittal', 'Biswabandan Panda', 'C. Krishna Mohan']",
        "date": "October 2021",
        "source": "AIMLSystems '21: Proceedings of the First International Conference on AI-ML Systems",
        "abstract": "Recent trends of the use of deep neural networks (DNNs) in mission-critical applications have increased the threats of microarchitectural attacks on DNN models. Recently, researchers have proposed techniques for inferring the DNN model based on microarchitecture-level clues. However, existing techniques require prior knowledge of victim models, lack generality, or provide incomplete information of the victim model architecture. This paper proposes an attack that leaks the layer-type of DNNs using hardware performance monitoring counters (PMCs).  Our attack works by profiling low-level hardware events and then analyzes this data using machine learning algorithms. We also apply techniques for removing the class imbalance in the PMC traces and for removing the noise. We present microarchitectural insights (hardware PMCs such as cache accesses/misses, branch instructions, and total instructions) that correlate with the characteristics of DNN layers. The extracted models are also helpful for crafting adversarial inputs. Our attack does not require any prior knowledge of the DNN architecture and still infers the layer-types of the DNN with high accuracy (above 90%). We have released the traces for public use at https://github.com/bhargavarch/DNN_RevEngg_PMC_Dataset.",
        "link": "https://dl.acm.org/doi/10.1145/3486001.3486224",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A Comprehensive Survey of Attacks without Physical Access Targeting Hardware Vulnerabilities in IoT/IIoT Devices, and Their Detection Mechanisms",
        "authors": "['Nikolaos-Foivos Polychronou', 'Pierre-Henri Thevenon', 'Maxime Puys', 'Vincent Beroulle']",
        "date": "None",
        "source": "ACM Transactions on Design Automation of Electronic Systems",
        "abstract": "With the advances in the field of the Internet of Things (IoT) and Industrial IoT (IIoT), these devices are increasingly used in daily life or industry. To reduce costs related to the time required to develop these devices, security features are usually not considered. This situation creates a major security concern. Many solutions have been proposed to protect IoT/IIoT against various attacks, most of which are based on attacks involving physical access. However, a new class of attacks has emerged targeting hardware vulnerabilities in the micro-architecture that do not require physical access. We present attacks based on micro-architectural hardware vulnerabilities and the side effects they produce in the system. In addition, we present security mechanisms that can be implemented to address some of these attacks. Most of the security mechanisms target a small set of attack vectors or a single specific attack vector. As many attack vectors exist, solutions must be found to protect against a wide variety of threats. This survey aims to inform designers about the side effects related to attacks and detection mechanisms that have been described in the literature. For this purpose, we present two tables listing and classifying the side effects and detection mechanisms based on the given criteria.",
        "link": "https://dl.acm.org/doi/10.1145/3471936",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Security Threat Analyses and Attack Models for Approximate Computing Systems: From Hardware and Micro-architecture Perspectives",
        "authors": "['Pruthvy Yellu', 'Landon Buell', 'Miguel Mark', 'Michel A. Kinsy', 'Dongpeng Xu', 'Qiaoyan Yu']",
        "date": "None",
        "source": "ACM Transactions on Design Automation of Electronic Systems",
        "abstract": "Approximate computing (AC) represents a paradigm shift from conventional precise processing to inexact computation but still satisfying the system requirement on accuracy. The rapid progress on the development of diverse AC techniques allows us to apply approximate computing to many computation-intensive applications. However, the utilization of AC techniques could bring in new unique security threats to computing systems. This work does a survey on existing circuit-, architecture-, and compiler-level approximate mechanisms/algorithms, with special emphasis on potential security vulnerabilities. Qualitative and quantitative analyses are performed to assess the impact of the new security threats on AC systems. Moreover, this work proposes four unique visionary attack models, which systematically cover the attacks that build covert channels, compensate approximation errors, terminate normal error resilience mechanisms, and propagate additional errors. To thwart those attacks, this work further offers the guideline of countermeasure designs. Several case studies are provided to illustrate the implementation of the suggested countermeasures.",
        "link": "https://dl.acm.org/doi/10.1145/3442380",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A Quality-assured Approximate Hardware Accelerators–based on Machine Learning and Dynamic Partial Reconfiguration",
        "authors": "['Mahmoud Masadeh', 'Yassmeen Elderhalli', 'Osman Hasan', 'Sofiene Tahar']",
        "date": "None",
        "source": "ACM Journal on Emerging Technologies in Computing Systems",
        "abstract": "Machine learning is widely used these days to extract meaningful information out of the Zettabytes of sensors data collected daily. All applications require analyzing and understanding the data to identify trends, e.g., surveillance, exhibit some error tolerance. Approximate computing has emerged as an energy-efficient design paradigm aiming to take advantage of the intrinsic error resilience in a wide set of error-tolerant applications. Thus, inexact results could reduce power consumption, delay, area, and execution time. To increase the energy-efficiency of machine learning on FPGA, we consider approximation at the hardware level, e.g., approximate multipliers. However, errors in approximate computing heavily depend on the application, the applied inputs, and user preferences. However, dynamic partial reconfiguration has been introduced, as a key differentiating capability in recent FPGAs, to significantly reduce design area, power consumption, and reconfiguration time by adaptively changing a selective part of the FPGA design without interrupting the remaining system. Thus, integrating “Dynamic Partial Reconfiguration” (DPR) with “Approximate Computing” (AC) will significantly ameliorate the efficiency of FPGA-based design approximation. In this article, we propose hardware-efficient quality-controlled approximate accelerators, which are suitable to be implemented in FPGA-based machine learning algorithms as well as any error-resilient applications. Experimental results using three case studies of image blending, audio blending, and image filtering applications demonstrate that the proposed adaptive approximate accelerator satisfies the required quality with an accuracy of 81.82%, 80.4%, and 89.4%, respectively. On average, the partial bitstream was found to be 28.6\\(\\) smaller than the full bitstream.",
        "link": "https://dl.acm.org/doi/10.1145/3462329",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "ARES: Persistently Secure Non-Volatile Memory with Processor-transparent and Hardware-friendly Integrity Verification and Metadata Recovery",
        "authors": "['Yu Zou', 'Kazi Abu Zubair', 'Mazen Alwadi', 'Rakin Muhammad Shadab', 'Sanjay Gandham', 'Amro Awad', 'Mingjie Lin']",
        "date": "None",
        "source": "ACM Transactions on Embedded Computing Systems",
        "abstract": "Emerging byte-addressable Non-Volatile Memory (NVM) technology, although promising superior memory density and ultra-low energy consumption, poses unique challenges to achieving persistent data privacy and computing security, both of which are critically important to the embedded and IoT applications. Specifically, to successfully restore NVMs to their working states after unexpected system crashes or power failure, maintaining and recovering all the necessary security-related metadata can severely increase memory traffic, degrade runtime performance, exacerbate write endurance problem, and demand costly hardware changes to off-the-shelf processors.In this article, we designed and implemented ARES, a new FPGA-assisted processor-transparent security mechanism that aims at efficiently and effectively achieving all three aspects of a security triad—confidentiality, integrity, and recoverability—in modern embedded computing. Given the growing prominence of CPU-FPGA heterogeneous computing architectures, ARES leverages FPGA’s hardware reconfigurability to offload performance-critical and security-related functions to the programmable hardware without microprocessors’ involvement. In particular, recognizing that the traditional Merkle tree caching scheme cannot fully exploit FPGA’s parallelism due to its sequential and recursive function calls, we (1) proposed a Merkle tree cache architecture that partitions a unified cache into multiple levels with parallel accesses and (2) further designed a novel Merkle tree scheme that flattened and reorganized the computation in the traditional Merkle tree verification and update processes to fully exploit the parallel cache ports and to fully pipeline time-consuming hashing operations. Beyond that, to accelerate the metadata recovery process, multiple parallel recovery units are instantiated to recover counter metadata and multiple Merkle sub-trees.Our hardware prototype of the ARES system on a Xilinx U200 platform shows that ARES achieved up to 1.4× lower latency and 2.6× higher throughput against the baseline implementation, while metadata recovery time was shortened by 1.8 times. When integrated with an embedded processor, neither hardware changes nor software changes are required. We also developed a theoretical framework to analytically model and explain experimental results.",
        "link": "https://dl.acm.org/doi/10.1145/3492735",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Scalable hardware acceleration of non-maximum suppression",
        "authors": "['Chunyun Chen', 'Tianyi Zhang', 'Zehui Yu', 'Adithi Raghuraman', 'Shwetalaxmi Udayan', 'Jie Lin', 'Mohamed M. Sabry Aly']",
        "date": "March 2022",
        "source": "DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe",
        "abstract": "Non-maximum Suppression (NMS) in one- and two-stage object detection deep neural networks (e.g., SSD and Faster-RCNN) is becoming the computation bottleneck. In this paper, we introduce a hardware acceleration for the scalable PSRR-MaxpoolNMS algorithm. Our architecture shows 75.0X and 305X speedups compared to the software implementation of the PSRR-MaxpoolNMS as well as the hardware implementations of GreedyNMS, respectively, while simultaneously achieving comparable Mean Average Precision (mAP) to software-based floating-point implementations. Our architecture is 13.4X faster than the state-of-the-art NMS one. Our accelerator supports both one- and two-stage detectors, while supporting very high input resolutions (i.e., FHD)---essential input size for better detection accuracy.",
        "link": "https://dl.acm.org/doi/10.5555/3539845.3539874",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "REMOT: A Hardware-Software Architecture for Attention-Guided Multi-Object Tracking with Dynamic Vision Sensors on FPGAs",
        "authors": "['Yizhao Gao', 'Song Wang', 'Hayden Kwok-Hay So']",
        "date": "February 2022",
        "source": "FPGA '22: Proceedings of the 2022 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays",
        "abstract": "In contrast to conventional vision sensors that produce images of the entire field-of-view at a fixed frame rate, dynamic vision sensors (DVS) are neuromorphic devices that only produce sparse events in response to changes in light intensity local to each pixel, making them promising technologies for use in demanding edge scenarios where energy-efficient intelligent computations are needed. While several early research have demonstrated promising results in performing high-level machine vision tasks using vision events only, these algorithms are often too complex for real-time deployments in edge systems with limited processing and storage capabilities. In this work, a novel hardware-software architecture, called REMOT, is proposed to leverage the unique properties of DVS to perform real-time multi-object tracking (MOT) on FPGAs. REMOT incorporates a parallel set of reconfigurable hardware attention units (AUs) that work in tandem with a modular attention-guided software framework running in the attached processor. Each hardware AU autonomously adjusts its region of attention by processing each vision event as they are produced by the DVS. Using information aggregated by the AUs, high-level analyses are performed in software. To demonstrate the flexibility and modularity of REMOT, a family of MOT algorithms with different hardware-software configurations and tradeoffs have been implemented on 2 different edge reconfigurable systems. Experimental results show that REMOT is capable of processing 0.43-2.22 million events per second at 1.75-5.68 watts, making them suitable for real-time operations while maintaining good MOT accuracy in our target datasets. When compared with a software-only implementation using the same edge platforms, our HW-SW implementation results in up to 33.6 times higher event processing throughput and 25.9 times higher power efficiency.",
        "link": "https://dl.acm.org/doi/10.1145/3490422.3502365",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Crumbs: Utilizing Functional Programming for Hardware Trace Data Analysis",
        "authors": "['Max Brand', 'Albrecht Mayer', 'Frank Slomka']",
        "date": "April 2021",
        "source": "RTNS '21: Proceedings of the 29th International Conference on Real-Time Networks and Systems",
        "abstract": "As modern system-on-chip devices are getting more complex, so does their software. Luckily, the observational capabilities of such devices has also increased by providing interfaces for non-intrusive hardware tracing. With these tracing facilities, it is possible to record the device’s state over time. The acquired trace data is very helpful for debugging and system validation. However, it is tedious to analyze trace data by hand as it consists of numerous trace messages without high-level information of the system. Due to this hardware-related information, we need better tools and descriptions for its analysis. Therefore, we propose Crumbs, a formalism that can be used to specify trace data and algorithms for trace data analysis. Moreover, it can easily be translated to a functional programming language, providing an easy execution of the formally designed algorithm. The paper is accompanied by three different use cases designed with Crumbs and implemented in Haskell.",
        "link": "https://dl.acm.org/doi/10.1145/3453417.3453418",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "ShuntFlowPlus: An Efficient and Scalable Dataflow Accelerator Architecture for Stream Applications",
        "authors": "['Shijun Gong', 'Jiajun Li', 'Wenyan Lu', 'Guihai Yan', 'Xiaowei Li']",
        "date": "None",
        "source": "ACM Journal on Emerging Technologies in Computing Systems",
        "abstract": "Streaming processing is an important and growing class of applications for analyzing continuous streams in real time. In such applications, sliding-window aggregation (SWAG) is a widely used approach, and general-purpose processors cannot efficiently handle SWAG because of the specific computation patterns. This article proposes an efficient dataflow accelerator architecture for ubiquitous SWAGs, called ShuntFlowPlus. ShuntFlowPlus supports two main categories of SWAGs that are widely used in streaming processing. Meanwhile, we propose a shunt rule to enable ShuntFlowPlus to efficiently handle SWAGs with arbitrary parameters. Furthermore, we propose a novel realization scheme of SWAG kernels based on buffer sharing to maximize buffer utilization. As a case study, we implemented ShuntFlowPlus on an Altera Arria 10 AX115N FPGA board at 150 MHz and compared it to previous approaches. The experimental results show that ShuntFlowPlus provides a tremendous throughput and latency advantage over CPU and GPU implementations on both reduce-like and index-like SWAGs. Compare to ShuntFlow, 41% of buffer resources are saved.",
        "link": "https://dl.acm.org/doi/10.1145/3453164",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "CeUPF: Offloading 5G User Plane Function to Programmable Hardware Base on Co-existence Architecture",
        "authors": "['Zhou Cong', 'Zhao Baokang', 'Wang Baosheng', 'Yuan Yulei']",
        "date": "December 2021",
        "source": "ACM ICEA '21: Proceedings of the 2021 ACM International Conference on Intelligent Computing and its Emerging Applications",
        "abstract": "Growing 5G1 applications require a user plane that has a high forward throughput and low packets loss. To meet these requirements, 5G acceleration means emerge, and User Plane Function (UPF) even more. The state of the art UPF acceleration is divided to software acceleration and hardware offloading. Overcoming these limitations between them, we design coexistence architecture for UPF, named CeUPF. We design and describe CeUPF's rule, which enables the CeUPF to reuse existing 5G user plane software function and provides high-performance hardware forwarding. UPF's transmitting function is implemented by offloading to smart NIC and P4 switch. We evaluate and compare the performance of the architecture. Our results show that offloading to P4 switch is better. Comparing with benchmark, CeUPF's bandwidth is promoted 10-33 times, and throughput is promoted 2.12-2.67 times. Our work also presents an open source platform to validate CeUPF, which is consistent with UPF.",
        "link": "https://dl.acm.org/doi/10.1145/3491396.3506526",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Hardware Trust and Assurance through Reverse Engineering: A Tutorial and Outlook from Image Analysis and Machine Learning Perspectives",
        "authors": "['Ulbert J. Botero', 'Ronald Wilson', 'Hangwei Lu', 'Mir Tanjidur Rahman', 'Mukhil A. Mallaiyan', 'Fatemeh Ganji', 'Navid Asadizanjani', 'Mark M. Tehranipoor', 'Damon L. Woodard', 'Domenic Forte']",
        "date": "None",
        "source": "ACM Journal on Emerging Technologies in Computing Systems",
        "abstract": "In the context of hardware trust and assurance, reverse engineering has been often considered as an illegal action. Generally speaking, reverse engineering aims to retrieve information from a product, i.e., integrated circuits (ICs) and printed circuit boards (PCBs) in hardware security-related scenarios, in the hope of understanding the functionality of the device and determining its constituent components. Hence, it can raise serious issues concerning Intellectual Property (IP) infringement, the (in)effectiveness of security-related measures, and even new opportunities for injecting hardware Trojans. Ironically, reverse engineering can enable IP owners to verify and validate the design. Nevertheless, this cannot be achieved without overcoming numerous obstacles that limit successful outcomes of the reverse engineering process. This article surveys these challenges from two complementary perspectives: image processing and machine learning. These two fields of study form a firm basis for the enhancement of efficiency and accuracy of reverse engineering processes for both PCBs and ICs. In summary, therefore, this article presents a roadmap indicating clearly the actions to be taken to fulfill hardware trust and assurance objectives.",
        "link": "https://dl.acm.org/doi/10.1145/3464959",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Generating correct initial page tables from formal hardware descriptions",
        "authors": "['Reto Achermann', 'David Cock', 'Roni Haecki', 'Nora Hossle', 'Lukas Humbel', 'Timothy Roscoe', 'Daniel Schwyn']",
        "date": "October 2021",
        "source": "PLOS '21: Proceedings of the 11th Workshop on Programming Languages and Operating Systems",
        "abstract": "Modern hardware platforms are increasingly complex and heterogeneous. System software uses a hodgepodge of different mechanisms and representations to express the memory topology of the target platform. Considerable maintenance effort is required to keep them in sync while often sharing is impossible due to hard-coded values. Incorrect platform-specific values in the hardware initialization sequence can lead to security critical and hard-to-find bugs because of misconfigured translation hardware, inaccessible devices, or the use of bad pointers. We present a better way for system software to express and initialize memory hardware. We adopt an existing, powerful hardware description language, and efficiently compile it to generate correct initial page tables and memory maps for OS kernels and firmware from a single system description. We evaluate our system on multiple architectures and platforms, and demonstrate that we can use the generated data structures to successfully initialize translation hardware, devices, memory maps, and allocators enabling easy support of new hardware platforms.",
        "link": "https://dl.acm.org/doi/10.1145/3477113.3487270",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Hardware acceleration of explainable machine learning",
        "authors": "['Zhixin Pan', 'Prabhat Mishra']",
        "date": "March 2022",
        "source": "DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe",
        "abstract": "Machine learning (ML) is successful in achieving human-level performance in various fields. However, it lacks the ability to explain an outcome due to its black-box nature. While recent efforts on explainable ML has received significant attention, the existing solutions are not applicable in real-time systems since they map interpretability as an optimization problem, which leads to numerous iterations of time-consuming complex computations. To make matters worse, existing implementations are not amenable for hardware-based acceleration. In this paper, we propose an efficient framework to enable acceleration of explainable ML procedure with hardware accelerators. We explore the effectiveness of both Tensor Processing Unit (TPU) and Graphics Processing Unit (GPU) based architectures in accelerating explainable ML. Specifically, this paper makes three important contributions. (1) To the best of our knowledge, our proposed work is the first attempt in enabling hardware acceleration of explainable ML. (2) Our proposed solution exploits the synergy between matrix convolution and Fourier transform, and therefore, it takes full advantage of TPU's inherent ability in accelerating matrix computations. (3) Our proposed approach can lead to real-time outcome interpretation. Extensive experimental evaluation demonstrates that proposed approach deployed on TPU can provide drastic improvement in interpretation time (39x on average) as well as energy efficiency (69x on average) compared to existing acceleration techniques.",
        "link": "https://dl.acm.org/doi/10.5555/3539845.3540108",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Comparative Analysis and Enhancement of CFG-based Hardware-Assisted CFI Schemes",
        "authors": "['Stefan Tauner', 'Mario Telesklav']",
        "date": "None",
        "source": "ACM Transactions on Embedded Computing Systems",
        "abstract": "Subverting the flow of instructions (e.g., by use of code-reuse attacks) still poses a serious threat to the security of today’s systems. Various control flow integrity (CFI) schemes have been proposed as a powerful technique to detect and mitigate such attacks. In recent years, many hardware-assisted implementations of CFI enforcement based on control flow graphs (CFGs) have been presented by academia. Such approaches check whether control flow transfers follow the intended CFG by limiting the valid target addresses. However, these papers all target different platforms and were evaluated with different sets of benchmark applications, which makes quantitative comparisons hardly possible.For this paper, we have implemented multiple promising CFG-based CFI schemes on a common platform comprising a RISC-V within FPGA. By porting almost 40 benchmark applications to this system we can present a meaningful comparison of the various techniques in terms of run-time performance, hardware utilization, and binary size. In addition, we present an enhanced CFI approach that is inspired by what we consider the best concepts and ideas of previously proposed mechanisms. We have made this approach more practical and feature-complete by tackling some problems largely ignored previously. We show with this fine-grained scheme that CFI can be achieved with even less overheads than previously demonstrated.",
        "link": "https://dl.acm.org/doi/10.1145/3476989",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Transitioning Spiking Neural Network Simulators to Heterogeneous Hardware",
        "authors": "['Quang Anh Pham Nguyen', 'Philipp Andelfinger', 'Wen Jun Tan', 'Wentong Cai', 'Alois Knoll']",
        "date": "None",
        "source": "ACM Transactions on Modeling and Computer Simulation",
        "abstract": "Spiking neural networks (SNN) are among the most computationally intensive types of simulation models, with node counts on the order of up to 1011. Currently, there is intensive research into hardware platforms suitable to support large-scale SNN simulations, whereas several of the most widely used simulators still rely purely on the execution on CPUs. Enabling the execution of these established simulators on heterogeneous hardware allows new studies to exploit the many-core hardware prevalent in modern supercomputing environments, while still being able to reproduce and compare with results from a vast body of existing literature. In this article, we propose a transition approach for CPU-based SNN simulators to enable the execution on heterogeneous hardware (e.g., CPUs, GPUs, and FPGAs), with only limited modifications to an existing simulator code base and without changes to model code. Our approach relies on manual porting of a small number of core simulator functionalities as found in common SNN simulators, whereas the unmodified model code is analyzed and transformed automatically. We apply our approach to the well-known simulator NEST and make a version executable on heterogeneous hardware available to the community. Our measurements show that at full utilization, a single GPU achieves the performance of about 9 CPU cores. A CPU-GPU co-execution with load balancing is also demonstrated, which shows better performance compared to CPU-only or GPU-only execution. Finally, an analytical performance model is proposed to heuristically determine the optimal parameters to execute the heterogeneous NEST.",
        "link": "https://dl.acm.org/doi/10.1145/3422389",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "QIHE: Quantifying the Importance of Hardware Events with Respect to Performance of Mobile Processors",
        "authors": "['Chenghao Ouyang', 'Qiannan Wang', 'Zhibin Yu']",
        "date": "May 2021",
        "source": "ICBDC '21: Proceedings of the 6th International Conference on Big Data and Computing",
        "abstract": "Nowadays, an increasing number of applications run on mobile smartphones, making people's life much more convenient than ever before. In particular, the number of mobile phones running Android operating systems equipped with ARM processors is growing steadily, accounting for more than 50% of the global mobile phone market share. Therefore, the performance of these mobile phones still needs to be improved. Hardware (microarchitecture) events of the mobile processors contain the fundamental causes of their performance bottlenecks. However, it is challenging to clearly understand the details of the impact of the micro-architecture on the processor due to: 1) the difficulty of obtaining values of micro-architecture events, and 2) the large number (more than 200) of micro-architecture events. This paper proposes QIHE, a hybrid methodology which encompasses not only a way of collecting micro-architecture events, but also quantifying the importance of them with respect to performance. This method first collect 126 micro-architecture events for each of 70 applications on two mobile phones. Subsequently, it need quantify the importance of the events with respect to performance by using a machine learning algorithm — SGBRT (Stochastic Gradient Boosted Regression Tree). Finally, the 13 most important microarchitecture events are identified for all the applications running on the two mobile phones. These events can be used to optimize the processor microarchitecture as well as the performance of the applications.",
        "link": "https://dl.acm.org/doi/10.1145/3469968.3469999",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Integration verification across software and hardware for a simple embedded system",
        "authors": "['Andres Erbsen', 'Samuel Gruetter', 'Joonwon Choi', 'Clark Wood', 'Adam Chlipala']",
        "date": "June 2021",
        "source": "PLDI 2021: Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation",
        "abstract": "The interfaces between layers of a system are susceptible to bugs if developers of adjacent layers proceed under subtly different assumptions. Formal verification of two layers against the same formal model of the interface between them can be used to shake out these bugs. Doing so for every interface in the system can, in principle, yield unparalleled assurance of the correctness and security of the system as a whole. However, there have been remarkably few efforts that carry out this exercise, and all of them have simplified the task by restricting interactivity of the application, inventing new simplified instruction sets, and using unrealistic input and output mechanisms. We report on the first verification of a realistic embedded system, with its application software, device drivers, compiler, and RISC-V processor represented inside the Coq proof assistant as one mathematical object, with a machine-checked proof of functional correctness. A key challenge is structuring the proof modularly, so that further refinement of the components or expansion of the system can proceed without revisiting the rest of the system.",
        "link": "https://dl.acm.org/doi/10.1145/3453483.3454065",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Weaving Schematics and Code: Interactive Visual Editing for Hardware Description Languages",
        "authors": "['Richard Lin', 'Rohit Ramesh', 'Nikhil Jain', 'Josephine Koe', 'Ryan Nuqui', 'Prabal Dutta', 'Bjoern Hartmann']",
        "date": "October 2021",
        "source": "UIST '21: The 34th Annual ACM Symposium on User Interface Software and Technology",
        "abstract": "In many engineering disciplines such as circuit board, chip, and mechanical design, a hardware description language (HDL) approach provides important benefits over direct manipulation interfaces by supporting concepts like abstraction and generator meta-programming. While several such HDLs have emerged recently and promised power and flexibility, they also present challenges – especially to designers familiar with current graphical workflows. In this work, we investigate an IDE approach to provide a graphical editor for a board-level circuit design HDL. Unlike GUI builders which convert an entire diagram to code, we instead propose generating equivalent HDL from individual graphical edit actions. By keeping code as the primary design input, we preserve the full power of the underlying HDL, while remaining useful even to advanced users. We discuss our concept, design considerations such as performance, system implementation, and report on the results of an exploratory remote user study with four experienced hardware designers.",
        "link": "https://dl.acm.org/doi/10.1145/3472749.3474804",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "RecPipe: Co-designing Models and Hardware to Jointly Optimize Recommendation Quality and Performance",
        "authors": "['Udit Gupta', 'Samuel Hsia', 'Jeff Zhang', 'Mark Wilkening', 'Javin Pombra', 'Hsien-Hsin Sean Lee', 'Gu-Yeon Wei', 'Carole-Jean Wu', 'David Brooks']",
        "date": "October 2021",
        "source": "MICRO '21: MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture",
        "abstract": "Deep learning recommendation systems must provide high quality, personalized content under strict tail-latency targets and high system loads. This paper presents RecPipe, a system to jointly optimize recommendation quality and inference performance. Central to RecPipe is decomposing recommendation models into multi-stage pipelines to maintain quality while reducing compute complexity and exposing distinct parallelism opportunities. RecPipe implements an inference scheduler to map multi-stage recommendation engines onto commodity, heterogeneous platforms (e.g., CPUs, GPUs). While the hardware-aware scheduling improves ranking efficiency, the commodity platforms suffer from many limitations requiring specialized hardware. Thus, we design RecPipeAccel (RPAccel), a custom accelerator that jointly optimizes quality, tail-latency, and system throughput. RPAccel is designed specifically to exploit the distinct design space opened via RecPipe. In particular, RPAccel processes queries in sub-batches to pipeline recommendation stages, implements dual static and dynamic embedding caches, a set of top-k filtering units, and a reconfigurable systolic array. Compared to previously proposed specialized recommendation accelerators and at iso-quality, we demonstrate that RPAccel improves latency and throughput by 3 × and 6 ×.",
        "link": "https://dl.acm.org/doi/10.1145/3466752.3480127",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Hardware Secure Execution and Simulation Model Correlation using IFT on RISC-V",
        "authors": "['Geraldine Shirley Nicholas', 'Bhavin Thakar', 'Fareena Saqib']",
        "date": "June 2021",
        "source": "GLSVLSI '21: Proceedings of the 2021 on Great Lakes Symposium on VLSI",
        "abstract": "With Heterogeneous architectures and IoT devices connecting to billions of devices in the network, securing the application and tracking the data flow from different untrusted communication channels during run time and protecting the return address is an essential aspect of system integrity. In this work, we propose a correlated hardware and software-based information flow tracking mechanism to track the data using tagged logic. This scheme leverages the open-source benefits of RISC V by extending the architecture with security policies providing precise coarse grain management along with a simulation model with minimal overhead.",
        "link": "https://dl.acm.org/doi/10.1145/3453688.3461517",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "NeuroEngine: a hardware-based event-driven simulation system for advanced brain-inspired computing",
        "authors": "['Hunjun Lee', 'Chanmyeong Kim', 'Yujin Chung', 'Jangwoo Kim']",
        "date": "April 2021",
        "source": "ASPLOS '21: Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "Brain-inspired computing aims to understand the cognitive mechanisms of a brain and apply them to advance various areas in computer science. Deep learning is an example to greatly improve the field of pattern recognition and classification by utilizing an artificial neural network (ANN). To exploit advanced mechanisms of a brain and thus make more great advances, researchers need a methodology that can simulate neural networks with higher computational capabilities such as advanced spiking neural networks (SNNs) with two-stage neurons and synaptic delays. However, existing SNN simulation methodologies are too slow and energy-inefficient due to their software-based simulation or hardware-based but time-driven execution mechanisms.  In this paper, we present NeuroEngine, a fast and energy-efficient hardware-based system to efficiently simulate advanced SNNs. The key idea is to design an accelerator to enable event-driven simulations of the SNNs at a minimum cost. NeuroEngine achieves high speed and energy efficiency by carefully architecting its datapath and memory units to take the best advantage of the event-driven mechanism while satisfying all the important requirements to simulate our target SNNs. For high performance and energy efficiency, NeuroEngine applies a simpler datapath, multi-queue scheduler, and lazy update to minimize its neuron computation and event scheduling overhead. Then, we build an end-to-end simulation system by implementing a programming interface and a compilation toolchain for NeuroEngine hardware. Our evaluations show that NeuroEngine greatly improves the harmonic mean performance and energy efficiency by 4.30× and 2.60×, respectively, over the state-of-the-art time-driven simulator.",
        "link": "https://dl.acm.org/doi/10.1145/3445814.3446738",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A Scalable Cluster-based Hierarchical Hardware Accelerator for a Cortically Inspired Algorithm",
        "authors": "['Sumon Dey', 'Lee Baker', 'Joshua Schabel', 'Weifu Li', 'Paul D. Franzon']",
        "date": "None",
        "source": "ACM Journal on Emerging Technologies in Computing Systems",
        "abstract": "This article describes a scalable, configurable and cluster-based hierarchical hardware accelerator through custom hardware architecture for Sparsey, a cortical learning algorithm. Sparsey is inspired by the operation of the human cortex and uses a Sparse Distributed Representation to enable unsupervised learning and inference in the same algorithm. A distributed on-chip memory organization is designed and implemented in custom hardware to improve memory bandwidth and accelerate the memory read/write operations for synaptic weight matrices. Bit-level data are processed from distributed on-chip memory and custom multiply-accumulate hardware is implemented for binary and fixed-point multiply-accumulation operations. The fixed-point arithmetic and fixed-point storage are also adapted in this implementation. At 16 nm, the custom hardware of Sparsey achieved an overall 24.39×  speedup, 353.12× energy efficiency per frame, and 1.43× reduction in silicon area against a state-of-the-art GPU.",
        "link": "https://dl.acm.org/doi/10.1145/3447777",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Low-overhead Hardware Supervision for Securing an IoT Bluetooth-enabled Device: Monitoring Radio Frequency and Supply Voltage",
        "authors": "['Abdelrahman Elkanishy', 'Paul M. Furth', 'Derrick T. Rivera', 'Abdel-Hameed A. Badawy']",
        "date": "None",
        "source": "ACM Journal on Emerging Technologies in Computing Systems",
        "abstract": "Over the past decade, the number of Internet of Things (IoT) devices increased tremendously. In particular, the Internet of Medical Things (IoMT) and the Industrial Internet of Things (IIoT) expanded dramatically. Resource restrictions on IoT devices and the insufficiency of software security solutions raise the need for smart Hardware-Assisted Security (HAS) solutions. These solutions target one or more of the three C’s of IoT devices: Communication, Control, and Computation. Communication is an essential technology in the development of IoT. Bluetooth is a widely-used wireless communication protocol in small portable devices due to its low energy consumption and high transfer rates. In this work, we propose a supervisory framework to monitor and verify the operation of a Bluetooth system-on-chip (SoC) in real-time. To verify the operation of the Bluetooth SoC, we classify its transmission state in real-time to ensure a secure connection. Our overall classification accuracy is measured as 98.7%. We study both power supply current (IVDD) and RF domains to maximize the classification performance and minimize the overhead of our proposed supervisory system.",
        "link": "https://dl.acm.org/doi/10.1145/3468064",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "CICERO: A Domain-Specific Architecture for Efficient Regular Expression Matching",
        "authors": "['Daniele Parravicini', 'Davide Conficconi', 'Emanuele Del Sozzo', 'Christian Pilato', 'Marco D. Santambrogio']",
        "date": "None",
        "source": "ACM Transactions on Embedded Computing Systems",
        "abstract": "Regular Expression (RE) matching is a computational kernel used in several applications. Since RE complexity and data volumes are steadily increasing, hardware acceleration is gaining attention also for this problem. Existing approaches have limited flexibility as they require a different implementation for each RE. On the other hand, it is complex to map efficient RE representations like non-deterministic finite-state automata onto software-programmable engines or parallel architectures. In this work, we present CICERO , an end-to-end framework composed of a domain-specific architecture and a companion compilation framework for RE matching. Our solution is suitable for many applications, such as genomics/proteomics and natural language processing. CICERO aims at exploiting the intrinsic parallelism of non-deterministic representations of the REs. CICERO can trade-off accelerators’ efficiency and processors’ flexibility thanks to its programmable architecture and the compilation framework. We implemented CICERO prototypes on embedded FPGA achieving up to 28.6× and 20.8× more energy efficiency than embedded and mainstream processors, respectively. Since it is a programmable architecture, it can be implemented as a custom ASIC that is orders of magnitude more energy-efficient than mainstream processors.",
        "link": "https://dl.acm.org/doi/10.1145/3476982",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "DiAG: a dataflow-inspired architecture for general-purpose processors",
        "authors": "['Dong Kai Wang', 'Nam Sung Kim']",
        "date": "April 2021",
        "source": "ASPLOS '21: Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "The end of Dennard scaling and decline of Moore's law has prompted the proliferation of hardware accelerators for a wide range of application domains. Yet, at the dawn of an era of specialized computing, left behind the trend is the general-purpose processor that is still most easily programmed and widely used but has seen incremental changes for decades. This work uses an accelerator-inspired approach to rethink CPU microarchitecture to improve its energy efficiency while retaining its generality. We propose DiAG, a dataflow-based general-purpose processor architecture that can minimize latency by exploiting instruction-level parallelism or maximize throughput by exploiting data-level parallelism. DiAG is designed to support any RISC-like instruction set without explicitly requiring specialized languages, libraries, or compilers. Central to this architecture is the abstraction of the register file as register 'lanes' that allow implicit construction of the program's dataflow graph in hardware. At the cost of increased area, DiAG offers three main benefits over conventional out-of-order microarchitectures: reduced front-end overhead, efficient instruction reuse, and thread-level pipelining. We implement a DiAG prototype that supports the RISC-V ISA in SystemVerilog and evaluate its performance, power consumption, and area with EDA tools. In the tested Rodinia and SPEC CPU2017 benchmarks, DiAG configured with 512 PEs achieves a 1.18x speedup and 1.63x improvement in energy efficiency against an aggressive out-of-order CPU baseline.",
        "link": "https://dl.acm.org/doi/10.1145/3445814.3446703",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Novel Reconfigurable Hardware Systems for Tumor Growth Prediction",
        "authors": "['Konstantinos Malavazos', 'Maria Papadogiorgaki', 'Pavlos Malakonakis', 'Ioannis Papaefstathiou']",
        "date": "None",
        "source": "ACM Transactions on Computing for Healthcare",
        "abstract": "An emerging trend in biomedical systems research is the development of models that take full advantage of the increasing available computational power to manage and analyze new biological data as well as to model complex biological processes. Such biomedical models require significant computational resources, since they process and analyze large amounts of data, such as medical image sequences. We present a family of advanced computational models for the prediction of the spatio-temporal evolution of glioma and their novel implementation in state-of-the-art FPGA devices. Glioma is a rapidly evolving type of brain cancer, well known for its aggressive and diffusive behavior. The developed system simulates the glioma tumor growth in the brain tissue, which consists of different anatomic structures, by utilizing MRI slices. The presented models have been proved highly accurate in predicting the growth of the tumor, whereas the developed innovative hardware system, when implemented on a low-end, low-cost FPGA, is up to 85% faster than a high-end server consisting of 20 physical cores (and 40 virtual ones) and more than 28× more energy-efficient than it; the energy efficiency grows up to 50× and the speedup up to 14× if the presented designs are implemented in a high-end FPGA. Moreover, the proposed reconfigurable system, when implemented in a large FPGA, is significantly faster than a high-end GPU (i.e., from 80% and up to 250% faster), for the majority of the models, while it is also significantly better (i.e., from 80% to over 1,600%) in terms of power efficiency, for all the implemented models.",
        "link": "https://dl.acm.org/doi/10.1145/3454126",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Sparsity-aware and re-configurable NPU architecture for samsung flagship mobile SoC",
        "authors": "['Jun-Woo Jang', 'Sehwan Lee', 'Dongyoung Kim', 'Hyunsun Park', 'Ali Shafiee Ardestani', 'Yeongjae Choi', 'Channoh Kim', 'Yoojin Kim', 'Hyeongseok Yu', 'Hamzah Abdel-Aziz', 'Jun-Seok Park', 'Heonsoo Lee', 'Dongwoo Lee', 'Myeong Woo Kim', 'Hanwoong Jung', 'Heewoo Nam', 'Dongguen Lim', 'Seungwon Lee', 'Joon-Ho Song', 'Suknam Kwon', 'Joseph Hassoun', 'SukHwan Lim', 'Changkyu Choi']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "Of late, deep neural networks have become ubiquitous in mobile applications. As mobile devices generally require immediate response while maintaining user privacy, the demand for on-device machine learning technology is on the increase. Nevertheless, mobile devices suffer from restricted hardware resources, whereas deep neural networks involve considerable computation and communication. Therefore, the implementation of a neural-network specialized hardware accelerator, generally called neural processing unit (NPU), has started to gain attention for the mobile application processor (AP). However, NPUs for commercial mobile AP face two challenges that are difficult to realize simultaneously: execution of a wide range of applications and efficient performance. In this paper, we propose a flexible but efficient NPU architecture for a Samsung flagship mobile system-on-chip (SoC). To implement an efficient NPU, we design an energy-efficient inner-product engine that utilizes the input feature map sparsity. We propose a re-configurable MAC array to enhance the flexibility of the proposed NPU, dynamic internal memory port assignment to maximize on-chip memory bandwidth utilization, and efficient architecture to support mixed-precision arithmetic. We implement the proposed NPU using the Samsung 5nm library. Our silicon measurement experiments demonstrate that the proposed NPU achieves 290.7 FPS and 13.6 TOPS/W, when executing an 8-bit quantized Inception-v3 model [1] with a single NPU core. In addition, we analyze the proposed zero-skipping architecture in detail. Finally, we present the findings and lessons learned when implementing the commercial mobile NPU and interesting avenues for future work.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00011",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Sanger: A Co-Design Framework for Enabling Sparse Attention using Reconfigurable Architecture",
        "authors": "['Liqiang Lu', 'Yicheng Jin', 'Hangrui Bi', 'Zizhang Luo', 'Peng Li', 'Tao Wang', 'Yun Liang']",
        "date": "October 2021",
        "source": "MICRO '21: MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture",
        "abstract": "In recent years, attention-based models have achieved impressive performance in natural language processing and computer vision applications by effectively capturing contextual knowledge from the entire sequence. However, the attention mechanism inherently contains a large number of redundant connections, imposing a heavy computational burden on model deployment. To this end, sparse attention has emerged as an attractive approach to reduce the computation and memory footprint, which involves the sampled dense-dense matrix multiplication (SDDMM) and sparse-dense matrix multiplication (SpMM) at the same time, thus requiring the hardware to eliminate zero-valued operations effectively. Existing techniques based on irregular sparse patterns or regular but coarse-grained patterns lead to low hardware efficiency or less computation saving.  This paper proposes Sanger, a framework that harvests sparsity in the attention mechanism through synergistic hardware and software co-design. The software part prunes the attention matrix into a dynamic structured pattern, and the hardware part features a reconfigurable architecture that exploits such patterns. Specifically, we dynamically sparsify vanilla attention based on a quantized prediction of the attention matrix. Then, the sparse mask is re-arranged into structured blocks that are more amenable to hardware implementation. The hardware design of Sanger features a score-stationary dataflow that keeps sparse scores stationary in the PE to avoid decoding overhead. Using this dataflow and a reconfigurable systolic array design, we can unify the computation of SDDMM and SpMM operations. Typically, the PEs can be configured during runtime to support different data access and partial sum accumulation schemes. Experiments on BERT show that Sanger can prune the model to 0.08 - 0.27 sparsity without accuracy loss, achieving 4.64X, 22.7X, 2.39X, and 1.47X speedup compared to V100 GPU, AMD Ryzen Threadripper 3970X CPU, as well as the state-of-the-art attention accelerators A3 and SpAtten.",
        "link": "https://dl.acm.org/doi/10.1145/3466752.3480125",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Configurable Multi-directional Systolic Array Architecture for Convolutional Neural Networks",
        "authors": "['Rui Xu', 'Sheng Ma', 'Yaohua Wang', 'Xinhai Chen', 'Yang Guo']",
        "date": "None",
        "source": "ACM Transactions on Architecture and Code Optimization",
        "abstract": "The systolic array architecture is one of the most popular choices for convolutional neural network hardware accelerators. The biggest advantage of the systolic array architecture is its simple and efficient design principle. Without complicated control and dataflow, hardware accelerators with the systolic array can calculate traditional convolution very efficiently. However, this advantage also brings new challenges to the systolic array. When computing special types of convolution, such as the small-scale convolution or depthwise convolution, the processing element (PE) utilization rate of the array decreases sharply. The main reason is that the simple architecture design limits the flexibility of the systolic array.In this article, we design a configurable multi-directional systolic array (CMSA) to address these issues. First, we added a data path to the systolic array. It allows users to split the systolic array through configuration to speed up the calculation of small-scale convolution. Second, we redesigned the PE unit so that the array has multiple data transmission modes and dataflow strategies. This allows users to switch the dataflow of the PE array to speed up the calculation of depthwise convolution. In addition, unlike other works, we only make a few changes and modifications to the existing systolic array architecture. It avoids additional hardware overheads and can be easily deployed in application scenarios that require small systolic arrays such as mobile terminals. Based on our evaluation, CMSA can increase the PE utilization rate by up to 1.6 times compared to the typical systolic array when running the last layers of ResNet-18. When running depthwise convolution in MobileNet, CMSA can increase the utilization rate by up to 14.8 times. At the same time, CMSA and the traditional systolic arrays are similar in area and energy consumption.",
        "link": "https://dl.acm.org/doi/10.1145/3460776",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "BlastFunction: A Full-stack Framework Bringing FPGA Hardware Acceleration to Cloud-native Applications",
        "authors": "['Andrea Damiani', 'Giorgia Fiscaletti', 'Marco Bacis', 'Rolando Brondolin', 'Marco D. Santambrogio']",
        "date": "None",
        "source": "ACM Transactions on Reconfigurable Technology and Systems",
        "abstract": "“Cloud-native” is the umbrella adjective describing the standard approach for developing applications that exploit cloud infrastructures’ scalability and elasticity at their best. As the application complexity and user-bases grow, designing for performance becomes a first-class engineering concern. As an answer to these needs, heterogeneous computing platforms gained widespread attention as powerful tools to continue meeting SLAs for compute-intensive cloud-native workloads. We propose BlastFunction, an FPGA-as-a-Service full-stack framework to ease FPGAs’ adoption for cloud-native workloads, integrating with the vast spectrum of fundamental cloud models. At the IaaS level, BlastFunction time-shares FPGA-based accelerators to provide multi-tenant access to accelerated resources without any code rewriting. At the PaaS level, BlastFunction accelerates functionalities leveraging the serverless model and scales functions proactively, depending on the workload’s performance. Further lowering the FPGAs’ adoption barrier, an accelerators’ registry hosts accelerated functions ready to be used within cloud-native applications, bringing the simplicity of a SaaS-like approach to the developers. After an extensive experimental campaign against state-of-the-art cloud scenarios, we show how BlastFunction leads to higher performance metrics (utilization and throughput) against native execution, with minimal latency and overhead differences. Moreover, the scaling scheme we propose outperforms the main serverless autoscaling algorithms in workload performance and scaling operation amount.",
        "link": "https://dl.acm.org/doi/10.1145/3472958",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "ProSE: the architecture and design of a protein discovery engine",
        "authors": "['Eyes Robson', 'Ceyu Xu', 'Lisa Wu Wills']",
        "date": "February 2022",
        "source": "ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "Protein language models have enabled breakthrough approaches to protein structure prediction, function annotation, and drug discovery. A primary limitation to the widespread adoption of these powerful models is the high computational cost associated with the training and inference of these models, especially at longer sequence lengths. We present the architecture, microarchitecture, and hardware implementation of a protein design and discovery accelerator, ProSE (Protein Systolic Engine). ProSE has a collection of custom heterogeneous systolic arrays and special functions that process transfer learning model inferences efficiently. The architecture marries SIMD-style computations with systolic array architectures, optimizing coarse-grained operation sequences across model layers to achieve efficiency without sacrificing generality. ProSE performs Protein BERT inference at up to 6.9× speedup and 48× power efficiency (performance/Watt) compared to one NVIDIA A100 GPU. ProSE achieves up to 5.5 × (12.7×) speedup and 173× (249×) power efficiency compared to TPUv3 (TPUv2).",
        "link": "https://dl.acm.org/doi/10.1145/3503222.3507722",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Communication algorithm-architecture co-design for distributed deep learning",
        "authors": "['Jiayi Huang', 'Pritam Majumder', 'Sungkeun Kim', 'Abdullah Muzahid', 'Ki Hwan Yum', 'Eun Jung Kim']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "Large-scale distributed deep learning training has enabled developments of more complex deep neural network models to learn from larger datasets for sophisticated tasks. In particular, distributed stochastic gradient descent intensively invokes all-reduce operations for gradient update, which dominates communication time during iterative training epochs. In this work, we identify the inefficiency in widely used all-reduce algorithms, and the opportunity of algorithm-architecture co-design. We propose MULTITREE all-reduce algorithm with topology and resource utilization awareness for efficient and scalable all-reduce operations, which is applicable to different interconnect topologies. Moreover, we co-design the network interface to schedule and coordinate the all-reduce messages for contention-free communications, working in synergy with the algorithm. The flow control is also simplified to exploit the bulk data transfer of big gradient exchange. We evaluate the co-design using different all-reduce data sizes for synthetic study, demonstrating its effectiveness on various interconnection network topologies, in addition to state-of-the-art deep neural networks for real workload experiments. The results show that MULTITREE achieves 2.3X and 1.56X communication speedup, as well as up to 81% and 30% training time reduction compared to ring all-reduce and state-of-the-art approaches, respectively.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00023",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Design of Picture Classification System Based on Embedded PYNQ Architecture",
        "authors": "['Shengkai Wang', 'Jun Li', 'Yuan Meng']",
        "date": "October 2021",
        "source": "EITCE '21: Proceedings of the 2021 5th International Conference on Electronic Information Technology and Computer Engineering",
        "abstract": "With the rapid development of deep learning, traditional convolutional neural networks have achieved success in various fields. In order to meet the situation that FPGAs and other devices have less available resources, a picture classification system that satisfies convolutional neural network inference with less resources, small size, and high throughput is designed. This design is based on the PYNQ development board, using the design idea of software and hardware coordination, quantified convolutional neural network is applied on the development board, reducing computational redundancy, taking advantage of FPGA parallel computing, and constructing a picture classification system based on the PYNQ architecture. Experiments show that the quantified network model has low power consumption on the PYNQ-Z1 development board with limited resources, and can ideally complete the classification task. This paper realizes the rapid deployment of quantitative neural network, which has higher efficiency and good acceleration performance.",
        "link": "https://dl.acm.org/doi/10.1145/3501409.3501486",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "The Role of Edge Offload for Hardware - Accelerated Mobile Devices",
        "authors": "['Mahadev Satyanarayanan', 'Nathan Beckmann', 'Grace A. Lewis', 'Brandon Lucia']",
        "date": "June 2021",
        "source": "GetMobile: Mobile Computing and Communications",
        "abstract": "This position paper examines a spectrum of approaches to overcoming the limited computing power of mobile devices caused by their need to be small, lightweight and energy efficient. At one extreme is offloading of compute-intensive operations to a cloudlet nearby. At the other extreme is the use of fixed-function hardware accelerators on mobile devices. Between these endpoints lie various configurations of programmable hardware accelerators. We explore the strengths and weaknesses of these approaches and conclude that they are, in fact, complementary. Based on this insight, we advocate a softwarehardware co-evolution path that combines their strengths.",
        "link": "https://dl.acm.org/doi/10.1145/3486880.3486882",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "The Business Case for Media Architecture: Modelling Project Benefits to Justify Investment",
        "authors": "['Niels Wouters', 'Franz Wohlgezogen', 'Kim Halskov']",
        "date": "June 2021",
        "source": "MAB20: Media Architecture Biennale 20",
        "abstract": "A growing portfolio of global media architecture projects and sustained research interest in the domain suggest that the discipline is here to stay. With practical knowledge becoming easily accessible to clients, architects and urban planners, we notice a shift from traditional advertising screens to integrated context-aware installations. The challenge now becomes to understand investment return of media architecture in order to ensure ongoing support by clients and funders. In this paper we study The Digital Bricks, a 208 megapixel media façade integrated within a university building. We describe the project vision, engagement strategy and design outcome, and analyse in detail the business case for the project. We share considerations to support development of business cases for media architecture projects that favour engagement, cultural and innovation capacity over financial returns. As the discipline matures, our insights will help in the endeavour to convince clients to invest in media architecture that inspires and engages audiences.",
        "link": "https://dl.acm.org/doi/10.1145/3469410.3469416",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Software-driven Security Attacks: From Vulnerability Sources to Durable Hardware Defenses",
        "authors": "['Lauren Biernacki', 'Mark Gallagher', 'Zhixing Xu', 'Misiker Tadesse Aga', 'Austin Harris', 'Shijia Wei', 'Mohit Tiwari', 'Baris Kasikci', 'Sharad Malik', 'Todd Austin']",
        "date": "None",
        "source": "ACM Journal on Emerging Technologies in Computing Systems",
        "abstract": "There is an increasing body of work in the area of hardware defenses for software-driven security attacks. A significant challenge in developing these defenses is that the space of security vulnerabilities and exploits is large and not fully understood. This results in specific point defenses that aim to patch particular vulnerabilities. While these defenses are valuable, they are often blindsided by fresh attacks that exploit new vulnerabilities. This article aims to address this issue by suggesting ways to make future defenses more durable based on an organization of security vulnerabilities as they arise throughout the program life cycle. We classify these vulnerability sources through programming, compilation, and hardware realization, and we show how each source introduces unintended states and transitions into the implementation. Further, we show how security exploits gain control by moving the implementation to an unintended state using knowledge of these sources and how defenses work to prevent these transitions. This framework of analyzing vulnerability sources, exploits, and defenses provides insights into developing durable defenses that could defend against broader categories of exploits. We present illustrative case studies of four important attack genealogies—showing how they fit into the presented framework and how the sophistication of the exploits and defenses have evolved over time, providing us insights for the future.",
        "link": "https://dl.acm.org/doi/10.1145/3456299",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Automatic generation of architecture-level models from RTL designs for processors and accelerators",
        "authors": "['Yu Zeng', 'Aarti Gupta', 'Sharad Malik']",
        "date": "March 2022",
        "source": "DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe",
        "abstract": "Hardware platforms comprise general-purpose processors and application-specific accelerators. Unlike processors, application-specific accelerators often do not have clearly specified architecture-level models/specifications (the instruction set architecture or ISA). This poses challenges to the development and verification/validation of firmware/software for these accelerators. Manually writing architecture-level models takes great effort and is error-prone. When Register-Transfer Level (RTL) designs are available, they can be a source from which to automatically derive the architecture-level models. In this work, we propose an approach for automatically generating architecture-level models for processors as well as accelerators from their RTL designs. In previous work we showed how to automatically extract the architectural state variables (ASVs) from RTL designs. (These are the state variables that are persistent across instructions.) In this work we present an algorithm for generating the update functions of the model: how the ASVs and outputs are updated by each instruction. Experiments on several processors and accelerators demonstrate that our approach can cover a wide range of hardware features and generate high-quality architecture-level models within reasonable time.",
        "link": "https://dl.acm.org/doi/10.5555/3539845.3539954",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Timing-Optimized Hardware Implementation to Accelerate Polynomial Multiplication in the NTRU Algorithm",
        "authors": "['Eros Camacho-Ruiz', 'Santiago Sánchez-Solano', 'Piedad Brox', 'Macarena C. Martínez-Rodríguez']",
        "date": "None",
        "source": "ACM Journal on Emerging Technologies in Computing Systems",
        "abstract": "Post-quantum cryptographic algorithms have emerged to secure communication channels between electronic devices faced with the advent of quantum computers. The performance of post-quantum cryptographic algorithms on embedded systems has to be evaluated to achieve a good trade-off between required resources (area) and timing. This work presents two optimized implementations to speed up the NTRUEncrypt algorithm on a system-on-chip. The strategy is based on accelerating the most time-consuming operation that is the truncated polynomial multiplication. Hardware dedicated modules for multiplication are designed by exploiting the presence of consecutive zeros in the coefficients of the blinding polynomial. The results are validated on a PYNQ-Z2 platform that includes a Zynq-7000 SoC from Xilinx and supports a Python-based programming environment. The optimized version that exploits the presence of double, triple, and quadruple consecutive zeros offers the best performance in timing, in addition to considerably reducing the possibility of an information leakage against an eventual attack on the device, making it practically negligible.",
        "link": "https://dl.acm.org/doi/10.1145/3445979",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "In-fat pointer: hardware-assisted tagged-pointer spatial memory safety defense with subobject granularity protection",
        "authors": "['Shengjie Xu', 'Wei Huang', 'David Lie']",
        "date": "April 2021",
        "source": "ASPLOS '21: Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "Programming languages like C and C++ are not memory-safe because they provide programmers with low-level pointer manipulation primitives. The incorrect use of these primitives can result in bugs and security vulnerabilities: for example, spatial memory safety errors can be caused by dereferencing pointers outside the legitimate address range belonging to the corresponding object. While a range of schemes to provide protection against these vulnerabilities have been proposed, they all suffer from the lack of one or more of low performance overhead, compatibility with legacy code, or comprehensive protection for all objects and subobjects.   We present In-Fat Pointer, the first hardware-assisted defense that can achieve spatial memory safety at subobject granularity while maintaining compatibility with legacy code and low overhead. In-Fat Pointer improves the protection granularity of tagged-pointer schemes using object metadata, which is efficient and binary-compatible for object-bound spatial safety. Unlike previous work that devotes all pointer tag bits to object metadata lookup, In-Fat Pointer uses three complementary object metadata schemes to reduce the number pointer tag bits needed for metadata lookup, allowing it to use the left-over bits, along with in-memory type metadata, to refine the object bounds to subobject granularity. We show that this approach provides practical protection of fine-grained spatial memory safety.",
        "link": "https://dl.acm.org/doi/10.1145/3445814.3446761",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Fundamentals of Physical Computing: Determining Key Concepts in Embedded Systems and Hardware/Software Co-Design",
        "authors": "['Mareen Przybylla', 'Andreas Grillenberger']",
        "date": "October 2021",
        "source": "WiPSCE '21: Proceedings of the 16th Workshop in Primary and Secondary Computing Education",
        "abstract": "Studies have shown that teachers find it difficult to prepare contents in the area of embedded systems and hardware/software co-design for school students. The goal of this paper is to support them by obtaining a clearly structured representation of the key concepts from this area in order to be able to derive concrete competence goals based on them later on. We apply a method for identifying the key concepts of a subject area within computer science, which has already been tested in the field of data management, to embedded systems and the related hardware/software co-design. Here, we present the procedure (literature selection, content analysis, concept clustering, and structuring) and the results of this process.",
        "link": "https://dl.acm.org/doi/10.1145/3481312.3481352",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Joint Program and Layout Transformations to Enable Convolutional Operators on Specialized Hardware Based on Constraint Programming",
        "authors": "['Dennis Rieber', 'Axel Acosta', 'Holger Fröning']",
        "date": "None",
        "source": "ACM Transactions on Architecture and Code Optimization",
        "abstract": "The success of Deep Artificial Neural Networks (DNNs) in many domains created a rich body of research concerned with hardware accelerators for compute-intensive DNN operators. However, implementing such operators efficiently with complex hardware intrinsics such as matrix multiply is a task not yet automated gracefully. Solving this task often requires joint program and data layout transformations. First solutions to this problem have been proposed, such as TVM, UNIT, or ISAMIR, which work on a loop-level representation of operators and specify data layout and possible program transformations before the embedding into the operator is performed. This top-down approach creates a tension between exploration range and search space complexity, especially when also exploring data layout transformations such as im2col, channel packing, or padding.In this work, we propose a new approach to this problem. We created a bottom-up method that allows the joint transformation of both computation and data layout based on the found embedding. By formulating the embedding as a constraint satisfaction problem over the scalar dataflow, every possible embedding solution is contained in the search space. Adding additional constraints and optimization targets to the solver generates the subset of preferable solutions.An evaluation using the VTA hardware accelerator with the Baidu DeepBench inference benchmark shows that our approach can automatically generate code competitive to reference implementations. Further, we show that dynamically determining the data layout based on intrinsic and workload is beneficial for hardware utilization and performance. In cases where the reference implementation has low hardware utilization due to its fixed deployment strategy, we achieve a geomean speedup of up to × 2.813, while individual operators can improve as much as × 170.",
        "link": "https://dl.acm.org/doi/10.1145/3487922",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "MERCI: efficient embedding reduction on commodity hardware via sub-query memoization",
        "authors": "['Yejin Lee', 'Seong Hoon Seo', 'Hyunji Choi', 'Hyoung Uk Sul', 'Soosung Kim', 'Jae W. Lee', 'Tae Jun Ham']",
        "date": "April 2021",
        "source": "ASPLOS '21: Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "Deep neural networks (DNNs) with embedding layers are widely adopted to capture complex relationships among entities within a dataset. Embedding layers aggregate multiple embeddings — a dense vector used to represent the complicated nature of a data feature— into a single embedding; such operation is called embedding reduction. Embedding reduction spends a significant portion of its runtime on reading embeddings from memory and thus is known to be heavily memory-bandwidth-bound. Recent works attempt to accelerate this critical operation, but they often require either hardware modifications or emerging memory technologies, which makes it hardly deployable on commodity hardware. Thus, we propose MERCI, Memoization for Embedding Reduction with ClusterIng, a novel memoization framework for efficient embedding reduction. MERCI provides a mechanism for memoizing partial aggregation of correlated embeddings and retrieving the memoized partial result at a low cost. MERCI substantially reduces the number of memory accesses by 44% (29%), leading to 102% (74%) throughput improvement on real machines and 40.2% (28.6%) energy savings at the expense of 8×(1×) additional memory usage.",
        "link": "https://dl.acm.org/doi/10.1145/3445814.3446717",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A Genetic-algorithm-based Approach to the Design of DCT Hardware Accelerators",
        "authors": "['Mario Barbareschi', 'Salvatore Barone', 'Alberto Bosio', 'Jie Han', 'Marcello Traiola']",
        "date": "None",
        "source": "ACM Journal on Emerging Technologies in Computing Systems",
        "abstract": "As modern applications demand an unprecedented level of computational resources, traditional computing system design paradigms are no longer adequate to guarantee significant performance enhancement at an affordable cost. Approximate Computing (AxC) has been introduced as a potential candidate to achieve better computational performances by relaxing non-critical functional system specifications. In this article, we propose a systematic and high-abstraction-level approach allowing the automatic generation of near Pareto-optimal approximate configurations for a Discrete Cosine Transform (DCT) hardware accelerator. We obtain the approximate variants by using approximate operations, having configurable approximation degree, rather than full-precise ones. We use a genetic searching algorithm to find the appropriate tuning of the approximation degree, leading to optimal tradeoffs between accuracy and gains. Finally, to evaluate the actual HW gains, we synthesize non-dominated approximate DCT variants for two different target technologies, namely, Field Programmable Gate Arrays (FPGAs) and Application Specific Integrated Circuits (ASICs). Experimental results show that the proposed approach allows performing a meaningful exploration of the design space to find the best tradeoffs in a reasonable time. Indeed, compared to the state-of-the-art work on approximate DCT, the proposed approach allows an 18% average energy improvement while providing at the same time image quality improvement.",
        "link": "https://dl.acm.org/doi/10.1145/3501772",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Hardware Implementation of Hierarchical Temporal Memory Algorithm",
        "authors": "['Weifu Li', 'Paul Franzon', 'Sumon Dey', 'Joshua Schabel']",
        "date": "None",
        "source": "ACM Journal on Emerging Technologies in Computing Systems",
        "abstract": "Hierarchical temporal memory (HTM) is an un-supervised machine learning algorithm that can learn both spatial and temporal information of input. It has been successfully applied to multiple areas. In this paper, we propose a multi-level hierarchical ASIC implementation of HTM, referred to as processor core, to support both spatial and temporal pooling. To improve the unbalanced workload in HTM, the proposed design provides different mapping methods for the spatial and temporal pooling, respectively. In the proposed design, we implement a distributed memory system by assigning one dedicated memory bank to each level of hierarchy to improve the memory bandwidth utilization efficiency. Finally, the hot-spot operations are optimized using a series of customized units. Regarding scalability, we propose a ring-based network consisting of multiple processor cores to support a larger HTM network. To evaluate the performance of our proposed design, we map an HTM network that includes 2,048 columns and 65,536 cells on both the proposed design and NVIDIA Tesla K40c GPU using the KTH database as input. The latency and power of the proposed design is 6.04 ms and 4.1 W using GP 65 nm technology. Compared to the equivalent GPU implementation, the latency and power is improved 12.45× and 57.32×, respectively.",
        "link": "https://dl.acm.org/doi/10.1145/3479430",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "FPGAs in Client Compute Hardware: Despite certain challenges, FPGAs provide security and performance benefits over ASICs.",
        "authors": "['Michael Mattioli']",
        "date": "November-December 2021",
        "source": "Queue",
        "abstract": "FPGAs (field-programmable gate arrays) are remarkably versatile. They are used in a wide variety of applications and industries where use of ASICs (application-specific integrated circuits) is less economically feasible. Despite the area, cost, and power challenges designers face when integrating FPGAs into devices, they provide significant security and performance benefits. Many of these benefits can be realized in client compute hardware such as laptops, tablets, and smartphones.",
        "link": "https://dl.acm.org/doi/10.1145/3512327",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Intermittent-Aware Neural Architecture Search",
        "authors": "['Hashan Roshantha Mendis', 'Chih-Kai Kang', 'Pi-cheng Hsiu']",
        "date": "None",
        "source": "ACM Transactions on Embedded Computing Systems",
        "abstract": "The increasing paradigm shift towards intermittent computing has made it possible to intermittently execute deep neural network (DNN) inference on edge devices powered by ambient energy. Recently, neural architecture search (NAS) techniques have achieved great success in automatically finding DNNs with high accuracy and low inference latency on the deployed hardware. We make a key observation, where NAS attempts to improve inference latency by primarily maximizing data reuse, but the derived solutions when deployed on intermittently-powered systems may be inefficient, such that the inference may not satisfy an end-to-end latency requirement and, more seriously, they may be unsafe given an insufficient energy budget. This work proposes iNAS, which introduces intermittent execution behavior into NAS to find accurate network architectures with corresponding execution designs, which can safely and efficiently execute under intermittent power. An intermittent-aware execution design explorer is presented, which finds the right balance between data reuse and the costs related to intermittent inference, and incorporates a preservation design search space into NAS, while ensuring the power-cycle energy budget is not exceeded. To assess an intermittent execution design, an intermittent-aware abstract performance model is presented, which formulates the key costs related to progress preservation and recovery during intermittent inference. We implement iNAS on top of an existing NAS framework and evaluate their respective solutions found for various datasets, energy budgets and latency requirements, on a Texas Instruments device. Compared to those NAS solutions that can safely complete the inference, the iNAS solutions reduce the intermittent inference latency by 60% on average while achieving comparable accuracy, with an average 7% increase in search overhead.",
        "link": "https://dl.acm.org/doi/10.1145/3476995",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Relevance and Applicability of Hardware-independent Pointing Transfer Functions",
        "authors": "['Raiza Hanada', 'Damien Masson', 'Géry Casiez', 'Mathieu Nancel', 'Sylvain Malacria']",
        "date": "October 2021",
        "source": "UIST '21: The 34th Annual ACM Symposium on User Interface Software and Technology",
        "abstract": "Pointing transfer functions remain predominantly expressed in pixels per input counts, which can generate different visual pointer behaviors with different input and output devices; we show in a first controlled experiment that even small hardware differences impact pointing performance with functions defined in this manner. We also demonstrate the applicability of “hardware-independent” transfer functions defined in physical units. We explore two methods to maintain hardware-independent pointer performance in operating systems that require hardware-dependent definitions: scaling them to the resolutions of the input and output devices, or selecting the OS acceleration setting that produces the closest visual behavior. In a second controlled experiment, we adapted a baseline function to different screen and mouse resolutions using both methods, and the resulting functions provided equivalent performance. Lastly, we provide a tool to calculate equivalent transfer functions between hardware setups, allowing users to match pointer behavior with different devices, and researchers to tune and replicate experiment conditions. Our work emphasizes, and hopefully facilitates, the idea that operating systems should have the capability to formulate pointing transfer functions in physical units, and to adjust them automatically to hardware setups.",
        "link": "https://dl.acm.org/doi/10.1145/3472749.3474767",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "RSSD: defend against ransomware with hardware-isolated network-storage codesign and post-attack analysis",
        "authors": "['Benjamin Reidys', 'Peng Liu', 'Jian Huang']",
        "date": "February 2022",
        "source": "ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "Encryption ransomware has become a notorious malware. It encrypts user data on storage devices like solid-state drives (SSDs) and demands a ransom to restore data for users. To bypass existing defenses, ransomware would keep evolving and performing new attack models. For instance, we identify and validate three new attacks, including (1) garbage-collection (GC) attack that exploits storage capacity and keeps writing data to trigger GC and force SSDs to release the retained data; (2) timing attack that intentionally slows down the pace of encrypting data and hides its I/O patterns to escape existing defense; (3) trimming attack that utilizes the trim command available in SSDs to physically erase data.   To enhance the robustness of SSDs against these attacks, we propose RSSD, a ransomware-aware SSD. It redesigns the flash management of SSDs for enabling the hardware-assisted logging, which can conservatively retain older versions of user data and received storage operations in time order with low overhead. It also employs hardware-isolated NVMe over Ethernet to expand local storage capacity by transparently offloading the logs to remote cloud/servers in a secure manner. RSSD enables post-attack analysis by building a trusted evidence chain of storage operations to assist the investigation of ransomware attacks. We develop RSSD with a real-world SSD FPGA board. Our evaluation shows that RSSD can defend against new and future ransomware attacks, while introducing negligible performance overhead.",
        "link": "https://dl.acm.org/doi/10.1145/3503222.3507773",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Developing a Hardware Approach to Simulating Quantum Computing Using an Optimization Algorithm",
        "authors": "['Valery Pukhovsky', 'Sergey Gushanskiy', 'Viktor Potapov']",
        "date": "November 2021",
        "source": "PCI '21: Proceedings of the 25th Pan-Hellenic Conference on Informatics",
        "abstract": "In the past few decades, there has been an acute problem of creating a quantum computer that uses quantum mechanical effects such as quantum parallelism and quantum entanglement for computations. Using these mechanisms, the quantum computer is able to solve some of the NP-class problems in polynomial time. A hardware approach to modeling quantum computing is considered, and a general mathematical model of the operation of a quantum computer is described, and a technique for mathematical modeling of quantum computing is presented. At the stage of consideration of the method of mathematical modeling, the most resource-intensive parts of the model are indicated. Also, issues related to data parallelization on a hardware accelerator, which are planned to be used to simulate quantum computing, were considered, and algorithms for the operation of this type of accelerator were given. A more compact data format is proposed, which is recommended to be used when implementing the accelerator. Using the proposed format, it is possible to reduce the resources spent on elementary arithmetic operations. The possibility of introducing an optimization algorithm to minimize the time spent in computations, as well as to speed up the execution of quantum algorithms in simulators of a quantum computer at the stage of the effect of quantum gates on the quantum register model, is proposed. The results of the optimization algorithm and its comparison with the classical mathematical approach using a software model are presented.",
        "link": "https://dl.acm.org/doi/10.1145/3503823.3503894",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A Task-Parallel and Reconfigurable FPGA-Based Hardware Implementation of Extreme Learning Machine",
        "authors": "['Hui Huang', 'Hai-Jun Rong', 'Zhao-Xu Yang']",
        "date": "February 2022",
        "source": "ASSE' 22: 2022 3rd Asia Service Sciences and Software Engineering Conference",
        "abstract": "Extreme learning machine (ELM) is an emerging machine learning algorithm and widely used in various real-world applications due to its extremely fast training speed, good generalization and universal approximation capability. In order to further explore the ELM to be used in practical embedded systems, a task-parallel and reconfigurable FPGA-based hardware architecture of ELM algorithm is presented in this paper. The proposed architecture performs the on-chip machine learning for both training and prediction phases which are implemented parameterizably based on the reconfigurable parameters. Meanwhile, the task-parallel efforts are focused on the training phase to improve the computational efficiency by resolving the serial computations into subtasks for task-parallel computations. In addition, the on-chip block RAMs reuse scheme is also applied in proposed architecture for saving on-chip resource consumption. The experimental results show that the proposed ELM architecture can achieve similar accuracy compared with floating-point implementation on Matlab and outperform the recently published ELM implementations in terms of hardware performance, power consumption and resource utilization.",
        "link": "https://dl.acm.org/doi/10.1145/3523181.3523209",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Improving Power of DSP and CNN Hardware Accelerators Using Approximate Floating-point Multipliers",
        "authors": "['Vasileios Leon', 'Theodora Paparouni', 'Evangelos Petrongonas', 'Dimitrios Soudris', 'Kiamal Pekmestzi']",
        "date": "None",
        "source": "ACM Transactions on Embedded Computing Systems",
        "abstract": "Approximate computing has emerged as a promising design alternative for delivering power-efficient systems and circuits by exploiting the inherent error resiliency of numerous applications. The current article aims to tackle the increased hardware cost of floating-point multiplication units, which prohibits their usage in embedded computing. We introduce AFMU (Approximate Floating-point MUltiplier), an area/power-efficient family of multipliers, which apply two approximation techniques in the resource-hungry mantissa multiplication and can be seamlessly extended to support dynamic configuration of the approximation levels via gating signals. AFMU offers large accuracy configuration margins, provides negligible logic overhead for dynamic configuration, and detects unexpected results that may arise due to the approximations. Our evaluation shows that AFMU delivers energy gains in the range 3.6%–53.5% for half-precision and 37.2%–82.4% for single-precision, in exchange for mean relative error around 0.05%–3.33% and 0.01%–2.20%, respectively. In comparison with state-of-the-art multipliers, AFMU exhibits up to 4–6× smaller error on average while delivering more energy-efficient computing. The evaluation in image processing shows that AFMU provides sufficient quality of service, i.e., more than 50db PSNR and near 1 SSIM values, and up to 57.4% power reduction. When used in floating-point CNNs, the accuracy loss is small (or zero), i.e., up to 5.4% for MNIST and CIFAR-10, in exchange for up to 63.8% power gain.",
        "link": "https://dl.acm.org/doi/10.1145/3448980",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "The Open Domain-Specific Architecture: Next Steps to Production",
        "authors": "['Bapiraju Vinnakota']",
        "date": "September 2021",
        "source": "NANOCOM '21: Proceedings of the Eight Annual ACM International Conference on Nanoscale Computing and Communication",
        "abstract": "Domain-specific architectures (DSAs) are expected to play an increasing role in hyperscale data centers and other applications. The Open Domain-Specific Architecture is an industry consortium that aims to enable chiplet-based designs for DSAs and a marketplace to source chiplets. The ODSA aims to build on recent growth in commercial chiplet-based products from major vendors. This paper reviews the significant activities in the ODSA - an open chiplet interface, open prototypes and workflows and open data for community learning. The paper concludes with a discussion on the readiness of open efforts to enable chiplet-based product development.",
        "link": "https://dl.acm.org/doi/10.1145/3477206.3477462",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Data-flow-sensitive fault-space pruning for the injection of transient hardware faults",
        "authors": "['Oskar Pusz', 'Christian Dietrich', 'Daniel Lohmann']",
        "date": "June 2021",
        "source": "LCTES 2021: Proceedings of the 22nd ACM SIGPLAN/SIGBED International Conference on Languages, Compilers, and Tools for Embedded Systems",
        "abstract": "In the domain of safety-critical systems, fault injection campaigns on ISA-level have become a widespread approach to systematically assess the resilience of a system with respect to transient hardware faults. However, experimentally injecting all possible faults to achieve full fault-space coverage is infeasible in practice. Hence, pruning techniques, such as def/use pruning are commonly applied to reduce the campaign size by grouping injections that surely provoke the same erroneous behavior. We describe data-flow pruning, a new data-flow sensitive fault-space pruning method that extends on def/use-pruning by also considering the instructions’ semantics when deriving fault-equivalence sets. By tracking the information flow for each bit individually across the respective instructions and considering their fault-masking capability, data-flow pruning (DFP) has to plan fewer pilot injections as it derives larger fault-equivalence sets. Like def/use pruning, DFP is precise and complete and it can be used as a direct replacement/alternative in existing software-based fault-injection tools. Our prototypical implementation so far considers local fault equivalence for five types of instructions. In our experimental evaluation, this already reduces the number of necessary injections by up to 18 percent compared to def/use pruning.",
        "link": "https://dl.acm.org/doi/10.1145/3461648.3463851",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Capability Boehm: challenges and opportunities for garbage collection with capability hardware",
        "authors": "['Dejice Jacob', 'Jeremy Singer']",
        "date": "February 2022",
        "source": "VEE 2022: Proceedings of the 18th ACM SIGPLAN/SIGOPS International Conference on Virtual Execution Environments",
        "abstract": "The Boehm-Demers-Weiser Garbage Collector (BDWGC) is a widely used, production-quality memory management framework for C and C++ applications. In this work, we describe our experiences in adapting BDWGC for modern capability hardware, in particular the CHERI system, which provides guarantees about memory safety due to runtime enforcement of fine-grained pointer bounds and permissions. Although many libraries and applications have been ported to CHERI already, to the best of our knowledge this is the first analysis of the complexities of transferring a garbage collector to CHERI. We describe various challenges presented by the CHERI micro-architectural constraints, along with some significant opportunities for runtime optimization. Since we do not yet have access to capability hardware, we present a limited study of software event counts on emulated micro-benchmarks. This experience report should be helpful to other systems implementors as they attempt to support the ongoing CHERI initiative.",
        "link": "https://dl.acm.org/doi/10.1145/3516807.3516823",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A reference architecture for functional interoperability in robotics",
        "authors": "['László Németh', 'Gustavo Quiros Araya', 'William Regli', 'András Varró']",
        "date": "May 2021",
        "source": "Destion '21: Proceedings of the Workshop on Design Automation for CPS and IoT",
        "abstract": "The ability of robotic platforms to be resilient to changes in demand, changes in the environment, changes in the dimensions or weight of the workpiece, replacement of a robot, in other words to be functionally interoperable, is a vexing issue for the entire industry, and it is particularly important for small and medium enterprises (SMEs) where these kinds of changes are the norm. We propose the concept of functional interoperability, propose interoperability scenarios as a way to measuring it and based on our experiences in an ARM funded project we propose an architecture, and a few variations on it, which exhibit the much desired behaviour.",
        "link": "https://dl.acm.org/doi/10.1145/3445034.3460506",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "You don't need a Microservices Architecture (yet): Monoliths may do the trick",
        "authors": "['Dimitrios Gravanis', 'George Kakarontzas', 'Vassilis Gerogiannis']",
        "date": "November 2021",
        "source": "ESSE '21: Proceedings of the 2021 European Symposium on Software Engineering",
        "abstract": "Within the past decade, the advent of cloud computing in terms of infrastructure, technology stacks, availability of services and tooling, along with the gradual improvement of its market environment, has driven many organizations to either consider or migrate many existing software systems to the cloud, either fully or partially. A common predicament in most cases, is the existence of a complex, monolithic application, potentially considered legacy at the time, that was not designed to be cloud-native and therefore requires a degree of redesign/reimplementation in order to benefit from cloud deployment. In such cases, the decomposition of the monolith to a set of loosely coupled, highly cohesive and self-contained microservices is a valid recommendation, provided that the organization is prepared to withstand the additional cost, in terms of human and financial resources, along with the unavoidable development overhead, which is inevitable during the early stages. However, the tendency of the tech world to embrace new trends and jump on hype trains for fear of obsoletion, has led to an excessive adoption of the microservices architecture (MSA), even in cases where such an architecture is not viable for the organization, or does not derive from any business requirements. This research focuses on establishing the position of a traditional monolith in the modern software architecture landscape and determine use cases that can still benefit from this paradigm, as well as use cases that could benefit from a partial or full transition to microservices architectures instead.",
        "link": "https://dl.acm.org/doi/10.1145/3501774.3501780",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "\"The Network Is an Excuse\": Hardware Maintenance Supporting Community",
        "authors": "['Philip Garrison', 'Esther Han Beol Jang', 'Michael A. Lithgow', 'Nicolás Andrés Pace']",
        "date": "None",
        "source": "Proceedings of the ACM on Human-Computer Interaction",
        "abstract": "The global community networking movement promotes locally-managed network infrastructure as a strategy for affordable Internet connectivity. This case study investigates a group of collectively managed WiFi Internet networks in Argentina and the technologists who design the networking hardware and software. Members of these community networks collaborate on maintenance and repair and practice new forms of collective work. Drawing on Actor-Network Theory, we show that the networking technologies play a role in the social relations of their maintenance and that they are intentionally configured to do so. For technology designers and deployers, we suggest a path beyond designing for easy repair: since every breakdown is an opportunity to learn, we should design for accessible repair experiences that enable effective collaborative learning.",
        "link": "https://dl.acm.org/doi/10.1145/3479608",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Formal Verification of a Multiprocessor Hypervisor on Arm Relaxed Memory Hardware",
        "authors": "['Runzhou Tao', 'Jianan Yao', 'Xupeng Li', 'Shih-Wei Li', 'Jason Nieh', 'Ronghui Gu']",
        "date": "October 2021",
        "source": "SOSP '21: Proceedings of the ACM SIGOPS 28th Symposium on Operating Systems Principles",
        "abstract": "Concurrent systems software is widely-used, complex, and error-prone, posing a significant security risk. We introduce VRM, a new framework that makes it possible for the first time to verify concurrent systems software, such as operating systems and hypervisors, on Arm relaxed memory hardware. VRM defines a set of synchronization and memory access conditions such that a program that satisfies these conditions can be mostly verified on a sequentially consistent hardware model and the proofs will automatically hold on relaxed memory hardware. VRM can be used to verify concurrent kernel code that is not data race free, including code responsible for managing shared page tables in the presence of relaxed MMU hardware. Using VRM, we verify the security guarantees of a retrofitted implementation of the Linux KVM hypervisor on Arm. For multiple versions of KVM, we prove KVM's security properties on a sequentially consistent model, then prove that KVM satisfies VRM's required program conditions such that its security proofs hold on Arm relaxed memory hardware. Our experimental results show that the retrofit and VRM conditions do not adversely affect the scalability of verified KVM, as it performs similar to unmodified KVM when concurrently running many multiprocessor virtual machines with real application workloads on Arm multiprocessor server hardware. Our work is the first machine-checked proof for concurrent systems software on Arm relaxed memory hardware.",
        "link": "https://dl.acm.org/doi/10.1145/3477132.3483560",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "HECTOR-V: A Heterogeneous CPU Architecture for a Secure RISC-V Execution Environment",
        "authors": "['Pascal Nasahl', 'Robert Schilling', 'Mario Werner', 'Stefan Mangard']",
        "date": "May 2021",
        "source": "ASIA CCS '21: Proceedings of the 2021 ACM Asia Conference on Computer and Communications Security",
        "abstract": "To ensure secure and trustworthy execution of applications in potentially insecure environments, vendors frequently embed trusted execution environments (TEE) into their systems. Applications executed in this safe, isolated space are protected from adversaries, including a malicious operating system. TEEs are usually build by integrating protection mechanisms directly into the processor or by using dedicated external secure elements. However, both of these approaches only cover a narrow threat model resulting in limited security guarantees. Enclaves nested into the application processor typically provide weak isolation between the secure and non-secure domain, especially when considering side-channel attacks. Although external secure elements do provide strong isolation, the slow communication interface to the application processor is exposed to adversaries and restricts the use cases. Independently of the used approach, TEEs often lack the possibility to establish secure communication to peripherals, and most operating systems executed inside TEEs do not provide state-of-the-art defense strategies, making them vulnerable to various attacks. We argue that TEEs, such as Intel SGX or ARM TrustZone, implemented on the main application processor, are insecure, especially when considering side-channel attacks. In this paper, we demonstrate how a heterogeneous multicore architecture can be utilized to realize a secure TEE design. We directly embed a secure processor into our HECTOR-V architecture to provide strong isolation between the secure and non-secure domain. The tight coupling of the TEE and the application processor enables HECTOR-V to provide mechanisms for establishing secure communication channels between different devices. We further introduce RISC-V Secure Co-Processor (RVSCP), a security-hardened processor tailored for TEEs. To secure applications executed inside the TEE, RVSCP provides hardware enforced control-flow integrity and rigorously restricts I/O accesses to certain execution states. RVSCP reduces the trusted computing base to a minimum by providing operating system services directly in hardware.",
        "link": "https://dl.acm.org/doi/10.1145/3433210.3453112",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Authenticated key-value stores with hardware enclaves",
        "authors": "['Kai Li', 'Yuzhe Tang', 'Qi Zhang', 'Jianliang Xu', 'Ju Chen']",
        "date": "December 2021",
        "source": "Middleware '21: Proceedings of the 22nd International Middleware Conference: Industrial Track",
        "abstract": "Authenticated data storage on an untrusted platform is an important computing paradigm for cloud applications ranging from data outsourcing, to cryptocurrency and general transparency logs. These modern applications increasingly feature update-intensive workloads, whereas existing authenticated data structures (ADSs) designed with in-place updates are inefficient to handle such workloads. This work addresses the issue and presents a novel authenticated log-structured merge tree (eLSM) based key-value store built on Intel SGX. We present a system design that runs the code of eLSM store inside enclave. To circumvent the limited enclave memory (128 MB with the latest Intel CPUs), we propose to place the memory buffer of the eLSM store outside the enclave and protect the buffer using a new authenticated data structure by digesting individual LSM-tree levels. We design protocols to support data integrity, (range) query completeness, and freshness. Our protocol causes small proofs by including the Merkle proofs at selected levels. We implement eLSM on top of Google LevelDB and Facebook RocksDB with minimal code change and performance interference. We evaluate the performance of eLSM under the YCSB workload benchmark and show a performance advantage of up to 4.5X speedup.",
        "link": "https://dl.acm.org/doi/10.1145/3491084.3491425",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "ECCNAS: Efficient Crowd Counting Neural Architecture Search",
        "authors": "['Yabin Wang', 'Zhiheng Ma', 'Xing Wei', 'Shuai Zheng', 'Yaowei Wang', 'Xiaopeng Hong']",
        "date": "None",
        "source": "ACM Transactions on Multimedia Computing, Communications, and Applications",
        "abstract": "Recent solutions to crowd counting problems have already achieved promising performance across various benchmarks. However, applying these approaches to real-world applications is still challenging, because they are computation intensive and lack the flexibility to meet various resource budgets. In this article, we propose an efficient crowd counting neural architecture search (ECCNAS) framework to search efficient crowd counting network structures, which can fill this research gap. A novel search from pre-trained strategy enables our cross-task NAS to explore the significantly large and flexible search space with less search time and get more proper network structures. Moreover, our well-designed search space can intrinsically provide candidate neural network structures with high performance and efficiency. In order to search network structures according to hardwares with different computational performance, we develop a novel latency cost estimation algorithm in our ECCNAS. Experiments show our searched models get an excellent trade-off between computational complexity and accuracy and have the potential to deploy in practical scenarios with various resource budgets. We reduce the computational cost, in terms of multiply-and-accumulate (MACs), by up to 96% with comparable accuracy. And we further designed experiments to validate the efficiency and the stability improvement of our proposed search from pre-trained strategy.",
        "link": "https://dl.acm.org/doi/10.1145/3465455",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Pythia: A Customizable Hardware Prefetching Framework Using Online Reinforcement Learning",
        "authors": "['Rahul Bera', 'Konstantinos Kanellopoulos', 'Anant Nori', 'Taha Shahroodi', 'Sreenivas Subramoney', 'Onur Mutlu']",
        "date": "October 2021",
        "source": "MICRO '21: MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture",
        "abstract": "Past research has proposed numerous hardware prefetching techniques, most of which rely on exploiting one specific type of program context information (e.g., program counter, cacheline address, or delta between cacheline addresses) to predict future memory accesses. These techniques either completely neglect a prefetcher’s undesirable effects (e.g., memory bandwidth usage) on the overall system, or incorporate system-level feedback as an afterthought to a system-unaware prefetch algorithm. We show that prior prefetchers often lose their performance benefit over a wide range of workloads and system configurations due to their inherent inability to take multiple different types of program context and system-level feedback information into account while prefetching. In this paper, we make a case for designing a holistic prefetch algorithm that learns to prefetch using multiple different types of program context and system-level feedback information inherent to its design.  To this end, we propose Pythia, which formulates the prefetcher as a reinforcement learning agent. For every demand request, Pythia observes multiple different types of program context information to make a prefetch decision. For every prefetch decision, Pythia receives a numerical reward that evaluates prefetch quality under the current memory bandwidth usage. Pythia uses this reward to reinforce the correlation between program context information and prefetch decision to generate highly accurate, timely, and system-aware prefetch requests in the future. Our extensive evaluations using simulation and hardware synthesis show that Pythia outperforms two state-of-the-art prefetchers (MLOP and Bingo) by 3.4% and 3.8% in single-core, 7.7% and 9.6% in twelve-core, and 16.9% and 20.2% in bandwidth-constrained core configurations, while incurring only 1.03% area overhead over a desktop-class processor and no software changes in workloads. The source code of Pythia can be freely downloaded from https://github.com/CMU-SAFARI/Pythia.",
        "link": "https://dl.acm.org/doi/10.1145/3466752.3480114",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Taurus: a data plane architecture for per-packet ML",
        "authors": "['Tushar Swamy', 'Alexander Rucker', 'Muhammad Shahbaz', 'Ishan Gaur', 'Kunle Olukotun']",
        "date": "February 2022",
        "source": "ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "Emerging applications---cloud computing, the internet of things, and augmented/virtual reality---demand responsive, secure, and scalable datacenter networks. These networks currently implement simple, per-packet, data-plane heuristics (e.g., ECMP and sketches) under a slow, millisecond-latency control plane that runs data-driven performance and security policies. However, to meet applications' service-level objectives (SLOs) in a modern data center, networks must bridge the gap between line-rate, per-packet execution and complex decision making.   In this work, we present the design and implementation of Taurus, a data plane for line-rate inference. Taurus adds custom hardware based on a flexible, parallel-patterns (MapReduce) abstraction to programmable network devices, such as switches and NICs; this new hardware uses pipelined SIMD parallelism to enable per-packet MapReduce operations (e.g., inference). Our evaluation of a Taurus switch ASIC---supporting several real-world models---shows that Taurus operates orders of magnitude faster than a server-based control plane while increasing area by 3.8% and latency for line-rate ML models by up to 221 ns. Furthermore, our Taurus FPGA prototype achieves full model accuracy and detects two orders of magnitude more events than a state-of-the-art control-plane anomaly-detection system.",
        "link": "https://dl.acm.org/doi/10.1145/3503222.3507726",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Profiling Neural Blocks and Design Spaces for Mobile Neural Architecture Search",
        "authors": "['Keith G. Mills', 'Fred X. Han', 'Jialin Zhang', 'Seyed Saeed Changiz Rezaei', 'Fabian Chudak', 'Wei Lu', 'Shuo Lian', 'Shangling Jui', 'Di Niu']",
        "date": "October 2021",
        "source": "CIKM '21: Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management",
        "abstract": "Neural architecture search automates neural network design and has achieved state-of-the-art results in many deep learning applications. While recent literature has focused on designing networks to maximize accuracy, little work has been conducted to understand the compatibility of architecture design spaces to varying hardware. In this paper, we analyze the neural blocks used to build Once-for-All (MobileNetV3), ProxylessNAS and ResNet families, in order to understand their predictive power and inference latency on various devices, including Huawei Kirin 9000 NPU, RTX 2080 Ti, AMD Threadripper 2990WX, and Samsung Note10. We introduce a methodology to quantify the friendliness of neural blocks to hardware and the impact of their placement in a macro network on overall network performance via only end-to-end measurements. Based on extensive profiling results, we derive design insights and apply them to hardware-specific search space reduction. We show that searching in the reduced search space generates better accuracy-latency Pareto frontiers than searching in the original search spaces, customizing architecture search according to the hardware. Moreover, insights derived from measurements lead to notably higher ImageNet top-1 scores on all search spaces investigated.",
        "link": "https://dl.acm.org/doi/10.1145/3459637.3481944",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Neural architecture search as program transformation exploration",
        "authors": "['Jack Turner', 'Elliot J. Crowley', \"Michael F. P. O'Boyle\"]",
        "date": "April 2021",
        "source": "ASPLOS '21: Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "Improving the performance of deep neural networks (DNNs) is important to both the compiler and neural architecture search (NAS) communities. Compilers apply program transformations in order to exploit hardware parallelism and memory hierarchy. However, legality concerns mean they fail to exploit the natural robustness of neural networks. In contrast, NAS techniques mutate networks by operations such as the grouping or bottlenecking of convolutions, exploiting the resilience of DNNs. In this work, we express such neural architecture operations as program transformations whose legality depends on a notion of representational capacity. This allows them to be combined with existing transformations into a unified optimization framework. This unification allows us to express existing NAS operations as combinations of simpler transformations. Crucially, it allows us to generate and explore new tensor convolutions. We prototyped the combined framework in TVM and were able to find optimizations across different DNNs, that significantly reduce inference time - over 3× in the majority of cases. Furthermore, our scheme dramatically reduces NAS search time.",
        "link": "https://dl.acm.org/doi/10.1145/3445814.3446753",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Noema: Hardware-Efficient Template Matching for Neural Population Pattern Detection",
        "authors": "['Ameer M. S. Abdelhadi', 'Eugene Sha', 'Ciaran Bannon', 'Hendrik Steenland', 'Andreas Moshovos']",
        "date": "October 2021",
        "source": "MICRO '21: MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture",
        "abstract": "Repeating patterns of activity across neurons is thought to be key to understanding how the brain represents, reacts, and learns. Advances in imaging and electrophysiology allow us to observe activities of groups of neurons in real-time, with ever increasing detail. Detecting patterns over these activity streams is an effective means to explore the brain, and to detect memories, decisions, and perceptions in real-time while driving effectors such as robotic arms, or augmenting and repairing brain function. Template matching is a popular algorithm for detecting recurring patterns in neural populations and has primarily been implemented on commodity systems. Unfortunately, template matching is memory intensive and computationally expensive. This has prevented its use in portable applications, such as neuroprosthetics, which are constrained by latency, form-factor, and energy. We present Noema a dedicated template matching hardware accelerator that overcomes these limitations. Noema is designed to overcome the key bottlenecks of existing implementations: binning that converts the incoming bit-serial neuron activity streams into a stream of aggregate counts, memory storage and traffic for the templates and the binned stream, and the extensive use of floating-point arithmetic. The key innovation in Noema is a reformulation of template matching that enables computations to proceed progressively as data is received without binning while generating numerically identical results. This drastically reduces latency when most computations can now use simple, area- and energy efficient bit- and integer-arithmetic units. Furthermore, Noema implements template encoding to greatly reduce template memory storage and traffic. Noema is a hierarchical and scalable design where the bulk of its units are low-cost and can be readily replicated and their frequency can be adjusted to meet a variety of energy, area, and computation constraints.",
        "link": "https://dl.acm.org/doi/10.1145/3466752.3480121",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Controllable Person Image Synthesis GAN and Its Reconfigurable Energy-efficient Hardware Implementation",
        "authors": "['Shaoyue Lin', 'Yanjun Zhang']",
        "date": "March 2022",
        "source": "ICIAI '22: Proceedings of the 2022 6th International Conference on Innovation in Artificial Intelligence",
        "abstract": "At this stage, how to controllably generate higher quality person image is still the challenge of person image synthesis. At the same time, the update of image synthesis network is far ahead of its hardware implementation. Therefore, this paper proposes a GAN network for person image synthesis that can generate high quality person image with controllable pose and attributes. The newly designed network is more convenient for hardware implementation while ensuring that the generated image is controllable. This paper also designs a synthesizable library for GAN to pursue faster hardware reconfiguration. We completed the new model proposed in this paper based on this library. Finally, the proposed network achieves better results both quantitatively and qualitatively compared with previous work. Compared with GPU and CPU, the hardware implementation based on FPGA can achieve the highest energy efficient of 73.67 GOPS / W.",
        "link": "https://dl.acm.org/doi/10.1145/3529466.3529500",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "JetStream: Graph Analytics on Streaming Data with Event-Driven Hardware Accelerator",
        "authors": "['Shafiur Rahman', 'Mahbod Afarin', 'Nael Abu-Ghazaleh', 'Rajiv Gupta']",
        "date": "October 2021",
        "source": "MICRO '21: MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture",
        "abstract": "Graph Processing is at the core of many critical emerging workloads operating on unstructured data, including social network analysis, bioinformatics, and many others. Many applications operate on graphs that are constantly changing, i.e., new nodes and edges are added or removed over time. In this paper, we present JetStream, a hardware accelerator for evaluating queries over streaming graphs and capable of handling additions, deletions, and updates of edges. JetStream extends a recently proposed event-based accelerator for graph workloads to support streaming updates. It handles both accumulative and monotonic graph algorithms via an event-driven computation model that limits accesses to a smaller subset of the graph vertices, efficiently reuses the prior query results to eliminate redundancy, and optimizes the memory access pattern for enhanced memory bandwidth utilization. To the best of our knowledge, JetStream is the first graph accelerator that supports streaming graphs, reducing the computation time by 90% compared with cold-start computation using an existing accelerator. In addition, JetStream achieves about 18 × speedup over KickStarter and GraphBolt software frameworks at the large baseline batch sizes that these systems use with significantly higher speedup at smaller batch sizes.",
        "link": "https://dl.acm.org/doi/10.1145/3466752.3480126",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Hardware-In-The-Loop Labs for SCADA Cybersecurity Awareness and Training",
        "authors": "['Maxime Puys', 'Pierre-Henri Thevenon', 'Stéphane Mocanu']",
        "date": "August 2021",
        "source": "ARES '21: Proceedings of the 16th International Conference on Availability, Reliability and Security",
        "abstract": "In this paper, we present a SCADA cybersecurity awareness and training program based on a Hands-On training using two twin cyber-ranges named WonderICS and G-ICS. These labs are built using a Hardware-In-the-Loop simulation system of the physical process developed by the two partners. The cyber-ranges allow replication of realistic Advanced Persistent Threat (APT) attacks and demonstration of known vulnerabilities, as they rely on real industrial control devices and softwares. In this work, we present both the demonstration scenarios used for awareness on WonderICS and the training programs developed for graduate students on G-ICS.",
        "link": "https://dl.acm.org/doi/10.1145/3465481.3469185",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Omegaflow: a high-performance dependency-based architecture",
        "authors": "['Yaoyang Zhou', 'Zihao Yu', 'Chuanqi Zhang', 'Yinan Xu', 'Huizhe Wang', 'Sa Wang', 'Ninghui Sun', 'Yungang Bao']",
        "date": "June 2021",
        "source": "ICS '21: Proceedings of the ACM International Conference on Supercomputing",
        "abstract": "This paper investigates how to better track and deliver dependency in dependency-based cores to exploit instruction-level parallelism (ILP) as much as possible. To this end, we first propose an analytical performance model for the state-of-art dependency-based core, Forwardflow, and figure out two vital factors affecting its upper bound of performance. Then we propose Omegaflow,a dependency-based architecture adopting three new techniques, which respond to the discovered factors. Experimental results show that Omegaflow improves IPC by 24.6% compared to the state-of-the-art design, approaching the performance of the OoO architecture with an ideal scheduler (94.4%) without increasing the clock cycle and consumes only 8.82% more energy than Forwardflow.",
        "link": "https://dl.acm.org/doi/10.1145/3447818.3460367",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Searching a Hierarchically Aggregated Fusion Architecture for Fast Multi-Modality Image Fusion",
        "authors": "['Risheng Liu', 'Zhu Liu', 'Jinyuan Liu', 'Xin Fan']",
        "date": "October 2021",
        "source": "MM '21: Proceedings of the 29th ACM International Conference on Multimedia",
        "abstract": "Multi-modality image fusion refers to generating a complementary image that integrates typical characteristics from source images. In recent years, we have witnessed the remarkable progress of deep learning models for multi-modality fusion. Existing CNN-based approaches strain every nerve to design various architectures for realizing these tasks in an end-to-end manner. However, these handcrafted designs are unable to cope with the high demanding fusion tasks, resulting in blurred targets and lost textural details. To alleviate these issues, in this paper, we propose a novel approach, aiming at searching effective architectures according to various modality principles and fusion mechanisms. Specifically, we construct a hierarchically aggregated fusion architecture to extract and refine fused features from feature-level and object-level fusion perspectives, which is responsible for obtaining complementary target/detail representations. Then by investigating diverse effective practices, we composite a more flexible fusion-specific search space. Motivated by the collaborative principle, we employ a new search strategy with different principled losses and hardware constraints for sufficient discovery of components. As a result, we can obtain a task-specific architecture with fast inference time. Extensive quantitative and qualitative results demonstrate the superiority and versatility of our method against state-of-the-art methods.",
        "link": "https://dl.acm.org/doi/10.1145/3474085.3475299",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Supporting Autonomous Vehicle Applications on the Heterogeneous System Architecture",
        "authors": "['Nandinbaatar Tsog', 'Marielle Gallardo', 'Sweta Chakraborty', 'Torbjörn Martinson', 'Alexandra Hengl', 'Magnus Moberg', 'Adem Sen', 'Mobyen Uddin Ahmed', 'Shahina Begum', 'Moris Behnam', 'Mikael Sjödin', 'Saad Mubeen']",
        "date": "May 2021",
        "source": "ECBS 2021: 7th Conference on the Engineering of Computer Based Systems",
        "abstract": "The contemporary processors are unable to meet the increasing data-intensive and computation-demanding requirements in autonomous vehicle software applications. Recently, the new Heterogeneous System Architecture (HSA) has emerged as a promising solution to meet these requirements. The HSA reduces the latency of data exchange between the compute units and cache-coherent shared memory, which is not supported by the non-HSA compliant heterogeneous platforms with acceleration support. The main goal of the paper is to investigate the performance gain by the HSA and conduct a comparative evaluation of the HSA and non-HSA compliant heterogeneous platforms. The paper aims at evaluating these platforms by using two computation-intensive software functions in autonomous vehicles, namely the object detection and vehicle movement. In order to achieve this goal, the CUDA-accelerated source code of the functions is ported from a non-HSA compliant heterogeneous platform to the HSA platform. In this regard, the paper presents the architecture of a proof-of-concept prototype and provides evaluation using the prototype.",
        "link": "https://dl.acm.org/doi/10.1145/3459960.3459970",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Algorithm-hardware co-design for efficient brain-inspired hyperdimensional learning on edge",
        "authors": "['Yang Ni', 'Yeseong Kim', 'Tajana Rosing', 'Mohsen Imani']",
        "date": "March 2022",
        "source": "DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe",
        "abstract": "Machine learning methods have been widely utilized to provide high quality for many cognitive tasks. Running sophisticated learning tasks requires high computational costs to process a large amount of learning data. Brain-inspired Hyperdimensional Computing (HDC) is introduced as an alternative solution for lightweight learning on edge devices. However, HDC models still rely on accelerators to ensure real-time and efficient learning. These hardware designs are not commercially available and need a relatively long period to synthesize and fabricate after deriving the new applications. In this paper, we propose an efficient framework for accelerating the HDC at the edge by fully utilizing the available computing power. We optimize the HDC through algorithm-hardware co-design of the host CPU and existing low-power machine learning accelerators, such as Edge TPU. We interpret the lightweight HDC learning model as a hyper-wide neural network to take advantage of the accelerator and machine learning platform. We further improve the runtime cost of training by employing a bootstrap aggregating algorithm called bagging while maintaining the learning quality. We evaluate the performance of the proposed framework with several applications. Joint experiments on mobile CPU and the Edge TPU show that our framework achieves 4.5X faster training and 4.2X faster inference compared to the baseline platform. In addition, our framework achieves 19.4X faster training and 8.9X faster inference as compared to embedded ARM CPU, Raspberry Pi, that consumes similar power consumption.",
        "link": "https://dl.acm.org/doi/10.5555/3539845.3539920",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Design and Evaluation of a Tunable PUF Architecture for FPGAs",
        "authors": "['Franz-Josef Streit', 'Paul Krüger', 'Andreas Becher', 'Stefan Wildermann', 'Jürgen Teich']",
        "date": "None",
        "source": "ACM Transactions on Reconfigurable Technology and Systems",
        "abstract": "FPGA-based Physical Unclonable Functions (PUF) have emerged as a viable alternative to permanent key storage by turning effects of inaccuracies during the manufacturing process of a chip into a unique, FPGA-intrinsic secret. However, many fixed PUF designs may suffer from unsatisfactory statistical properties in terms of uniqueness, uniformity, and robustness. Moreover, a PUF signature may alter over time due to aging or changing operating conditions, rendering a PUF insecure in the worst case. As a remedy, we propose CHOICE, a novel class of FPGA-based PUF designs with tunable uniqueness and reliability characteristics. By the use of addressable shift registers available on an FPGA, we show that a wide configuration space for adjusting a device-specific PUF response is obtained without any sacrifice of randomness. In particular, we demonstrate the concept of address-tunable propagation delays, whereby we are able to increase or decrease the probability of obtaining “1”s in the PUF response. Experimental evaluations on a group of six 28 nm Xilinx Artix-7 FPGAs show that CHOICE PUFs provide a large range of configurations to allow a fine-tuning to an average uniqueness between 49% and 51%, while simultaneously achieving bit error rates below 1.5%, thus outperforming state-of-the-art PUF designs. Moreover, with only a single FPGA slice per PUF bit, CHOICE is one of the smallest PUF designs currently available for FPGAs. It is well-known that signal propagation delays are affected by temperature, as the operating temperature impacts the internal currents of transistors that ultimately make up the circuit. We therefore comprehensively investigate how temperature variations affect the PUF response and demonstrate how the tunability of CHOICE enables us to determine configurations that show a high robustness to such variations. As a case study, we present a cryptographic key generation scheme based on CHOICE PUF responses as device-intrinsic secret and investigate the design objectives resource costs, performance, and temperature robustness to show the practicability of our approach.",
        "link": "https://dl.acm.org/doi/10.1145/3491237",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Revamping hardware persistency models: view-based and axiomatic persistency models for Intel-x86 and Armv8",
        "authors": "['Kyeongmin Cho', 'Sung-Hwan Lee', 'Azalea Raad', 'Jeehoon Kang']",
        "date": "June 2021",
        "source": "PLDI 2021: Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation",
        "abstract": "Non-volatile memory (NVM) is a cutting-edge storage technology that promises the performance of DRAM with the durability of SSD. Recent work has proposed several persistency models for mainstream architectures such as Intel-x86 and Armv8, describing the order in which writes are propagated to NVM. However, these models have several limitations; most notably, they either lack operational models or do not support persistent synchronization patterns.  We close this gap by revamping the existing persistency models. First, inspired by the recent work on promising semantics, we propose a unified operational style for describing persistency using views, and develop view-based operational persistency models for Intel-x86 and Armv8, thus presenting the first operational model for Armv8 persistency. Next, we propose a unified axiomatic style for describing hardware persistency, allowing us to recast and repair the existing axiomatic models of Intel-x86 and Armv8 persistency. We prove that our axiomatic models are equivalent to the authoritative semantics reviewed by Intel and Arm engineers. We further prove that each axiomatic hardware persistency model is equivalent to its operational counterpart. Finally, we develop a persistent model checking algorithm and tool, and use it to verify several representative examples.",
        "link": "https://dl.acm.org/doi/10.1145/3453483.3454027",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A high performance architecture for object detection in drones",
        "authors": "['Ioannis Kwnsantinos Galanakis', 'Athanasios Milidonis', 'Ioannis Voyiatzis']",
        "date": "November 2021",
        "source": "PCI '21: Proceedings of the 25th Pan-Hellenic Conference on Informatics",
        "abstract": "Nowadays the use of drones in daily life is becoming more and more frequent. One important service of drones is object detection which is used in surveillance and search-and-rescue missions. High performance is a critical requirement for object detection, since drones travel at high flying speeds. Current systems mainly use software platforms which have limited performance. In this paper a hardware architecture is presented for object detection in drones. The ability of the proposed architecture to exploit parallelization in object detection tasks more efficiently than in software platforms, offers lower execution time and more accurate and faster results. The proposed architecture is implemented using an FPGA platform. Experimental results show the benefits of this architecture compared to a software platform in terms of performance and detection-precision.",
        "link": "https://dl.acm.org/doi/10.1145/3503823.3503868",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "All-Optical Neural Network Tanh Architecture with MZI",
        "authors": "['Ruizhen Wu', 'Jingjing Chen', 'Ping Huang', 'Lin Wang']",
        "date": "September 2021",
        "source": "MLMI '21: Proceedings of the 2021 4th International Conference on Machine Learning and Machine Intelligence",
        "abstract": "Artificial neural networks (ANNs) have been widely used for industrial applications and have played a more and more important role in fundamental research but the electronic-based integrated circuits are limited by Moore's law. Optical neural networks (ONNs) can process information in parallel and have low energy advantages. The MZI with Gridnet or FFTnet can realize the convolution calculation is already proved by lots of researches. But the activation functions still have to use the DA/ADC to do the photoelectric conversion and then be calculated in electronic-based hardware systems. So an optical Tanh architecture with MZI is proposed in this paper for the all-optical neural network's activation function. Which can achieve the accuracy of Lenet-5 with MNIST dataset to 86.49%.",
        "link": "https://dl.acm.org/doi/10.1145/3490725.3490740",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "STAP: An Architecture and Design Tool for Automata Processing on Memristor TCAMs",
        "authors": "['João Paulo Cardoso de Lima', 'Marcelo Brandalero', 'Michael Hübner', 'Luigi Carro']",
        "date": "None",
        "source": "ACM Journal on Emerging Technologies in Computing Systems",
        "abstract": "Accelerating finite-state automata benefits several emerging application domains that are built on pattern matching. In-memory architectures, such as the Automata Processor (AP), are efficient to speed them up, at least for outperforming traditional von-Neumann architectures. In spite of the AP’s massive parallelism, current APs suffer from poor memory density, inefficient routing architectures, and limited capabilities. Although these limitations can be lessened by emerging memory technologies, its architecture is still the major source of huge communication demands and lack of scalability. To address these issues, we present STAP, a Scalable TCAM-based architecture for Automata Processing. STAP adopts a reconfigurable array of processing elements, which are based on memristive Ternary CAMs (TCAMs), to efficiently implement Non-deterministic finite automata (NFAs) through proper encoding and mapping methods. The CAD tool for STAP integrates the design flow of automata applications, a specific mapping algorithm, and place and route tools for connecting processing elements by RRAM-based programmable interconnects. Results showed 1.47× higher throughput when processing 16-bit input symbols, and improvements of 3.9× and 25× on state and routing densities over the state-of-the-art AP, while preserving 104 programming cycles.",
        "link": "https://dl.acm.org/doi/10.1145/3450769",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Hardware Acceleration of High-Performance Computational Flow Dynamics Using High-Bandwidth Memory-Enabled Field-Programmable Gate Arrays",
        "authors": "['Tom Hogervorst', 'Răzvan Nane', 'Giacomo Marchiori', 'Tong Dong Qiu', 'Markus Blatt', 'Alf Birger Rustad']",
        "date": "None",
        "source": "ACM Transactions on Reconfigurable Technology and Systems",
        "abstract": "Scientific computing is at the core of many High-Performance Computing applications, including computational flow dynamics. Because of the utmost importance to simulate increasingly larger computational models, hardware acceleration is receiving increased attention due to its potential to maximize the performance of scientific computing. Field-Programmable Gate Arrays could accelerate scientific computing because of the possibility to fully customize the memory hierarchy important in irregular applications such as iterative linear solvers. In this article, we study the potential of using Field-Programmable Gate Arrays in High-Performance Computing because of the rapid advances in reconfigurable hardware, such as the increase in on-chip memory size, increasing number of logic cells, and the integration of High-Bandwidth Memories on board. To perform this study, we propose a novel Sparse Matrix-Vector multiplication unit and an ILU0 preconditioner tightly integrated with a BiCGStab solver kernel. We integrate the developed preconditioned iterative solver in Flow from the Open Porous Media project, a state-of-the-art open source reservoir simulator. Finally, we perform a thorough evaluation of the FPGA solver kernel in both stand-alone mode and integrated in the reservoir simulator, using the NORNE field, a real-world case reservoir model using a grid with more than 105 cells and using three unknowns per cell.",
        "link": "https://dl.acm.org/doi/10.1145/3476229",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Neuromorphic Design Using Reward-based STDP Learning on Event-Based Reconfigurable Cluster Architecture",
        "authors": "['Mahyar Shahsavari', 'David Thomas', 'Andrew Brown', 'Wayne Luk']",
        "date": "July 2021",
        "source": "ICONS 2021: International Conference on Neuromorphic Systems 2021",
        "abstract": "Neuromorphic computing systems simulate spiking neural networks that are used for research into how biological neural networks function, as well as for applied engineering such as robotics, pattern recognition, and machine learning. In this paper, we present a neuromorphic system based on an asynchronous event-based hardware platform. We represent three algorithms for implementing spiking networks on our asynchronous hardware platform. We also discuss different trade-offs between synchronisation and messaging costs. A reinforcement learning method known as Reward-modulated STDP is presented as an online learning algorithm in the network. We evaluate the system performance in a single box of our designed architecture using 6000 concurrent hardware threads and demonstrate scaling to networks with up to 2 million neurons and 400 million synapses. The performance of our architecture is also compared to existing neuromorphic platforms, showing a 20 times speed-up over the Brian simulator on an x86 machine, and a 16 times speed-up over a 48-chip SpiNNaker node.",
        "link": "https://dl.acm.org/doi/10.1145/3477145.3477151",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "TrustCross: Enabling Confidential Interoperability across Blockchains Using Trusted Hardware",
        "authors": "['Ying Lan', 'Jianbo Gao', 'Yue Li', 'Ke Wang', 'Yuesheng Zhu', 'Zhong Chen']",
        "date": "December 2021",
        "source": "ICBTA '21: Proceedings of the 2021 4th International Conference on Blockchain Technology and Applications",
        "abstract": "With the rapid development of blockchain technology, different types of blockchains are adopted and interoperability across blockchains has received widespread attention. There have been many cross-chain solutions proposed in recent years, including notary scheme, sidechain, and relay chain. However, most of the existing platforms do not take confidentiality into account, although privacy has become an important concern for blockchain. In this paper, we present TrustCross, a privacy- preserving cross-chain platform to enable confidential interoperability across blockchains. The key insight behind TrustCross is to encrypt cross-chain communication data on the relay chain with the assistance of trusted execution environment and employ fine-grained access control to protect user privacy. Our experimental results show that TrustCross achieves reasonable latency and high scalability on the contract calls across heterogeneous blockchains.",
        "link": "https://dl.acm.org/doi/10.1145/3510487.3510491",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Accelerating Neural Architecture Search with Rank-Preserving Surrogate Models",
        "authors": "['Hadjer Benmeziane', 'Hamza Ouarnoughi', 'Kaoutar El Maghraoui', 'Smail Niar']",
        "date": "August 2021",
        "source": "ArabWIC 2021: The 7th Annual International Conference on Arab Women in Computing in Conjunction with the 2nd Forum of Women in Research",
        "abstract": "Over the past years, deep learning has enabled significant progress in several tasks, such as image recognition, speech recognition and language modelling. Novel Neural architectures are behind this achievement. However, manually designing these architectures by human experts is time-consuming and error-prone. Neural architecture search (NAS) automates the design process by searching for the best architecture in a huge search space. This search process requires evaluating each sampled architecture via time-consuming training. To speed up NAS algorithms, several existing approaches use surrogate models that predict the neural architectures’ precision instead of training each sampled one. In this paper, we propose RS-NAS for Rank-preserving Surrogate model in NAS, a surrogate model trained with a rank-preserving loss function. We posit that the search algorithm doesn’t need to know the exact accuracy of a candidate architecture but instead needs to know if it is better or worse than others. We thoroughly experiment and validate our surrogate models with state-of-the-art search algorithms. Using the rank-preserving surrogate models, local search in DARTS finds a 2% more accurate architecture than using the NAS-Bench-301 surrogate model on the same search time. The code and models are available: https://github.com/IHIaadj/ranked_nas",
        "link": "https://dl.acm.org/doi/10.1145/3485557.3485579",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Design of a Robust Memristive Spiking Neuromorphic System with Unsupervised Learning in Hardware",
        "authors": "['Md Musabbir Adnan', 'Sagarvarma Sayyaparaju', 'Samuel D. Brown', 'Mst Shamim Ara Shawkat', 'Catherine D. Schuman', 'Garrett S. Rose']",
        "date": "None",
        "source": "ACM Journal on Emerging Technologies in Computing Systems",
        "abstract": "Spiking neural networks (SNN) offer a power efficient, biologically plausible learning paradigm by encoding information into spikes. The discovery of the memristor has accelerated the progress of spiking neuromorphic systems, as the intrinsic plasticity of the device makes it an ideal candidate to mimic a biological synapse. Despite providing a nanoscale form factor, non-volatility, and low-power operation, memristors suffer from device-level non-idealities, which impact system-level performance. To address these issues, this article presents a memristive crossbar-based neuromorphic system using unsupervised learning with twin-memristor synapses, fully digital pulse width modulated spike-timing-dependent plasticity, and homeostasis neurons. The implemented single-layer SNN was applied to a pattern-recognition task of classifying handwritten-digits. The performance of the system was analyzed by varying design parameters such as number of training epochs, neurons, and capacitors. Furthermore, the impact of memristor device non-idealities, such as device-switching mismatch, aging, failure, and process variations, were investigated and the resilience of the proposed system was demonstrated.",
        "link": "https://dl.acm.org/doi/10.1145/3451210",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Accelerate hardware logging for efficient crash consistency in persistent memory",
        "authors": "['Zhiyuan Lu', 'Jianhui Yue', 'Yifu Deng', 'Yifeng Zhu']",
        "date": "March 2022",
        "source": "DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe",
        "abstract": "While logging has been adopted in persistent memory (PM) to support crash consistency, logging incurs severe performance overhead. This paper discovers two common factors that contribute to the inefficiency of logging: (1) load imbalance among memory banks, and (2) constraints of intra-record ordering. Overloaded memory banks may significantly prolong the waiting time of log requests targeting these banks. To address this issue, we propose a novel log entry allocation scheme (LALEA) that reshapes the traffic distribution over PM banks. In addition, the intra-record ordering between a header and its log entries decreases the degree of parallelism in log operations. We design a log metadata buffering scheme (BLOM) that eliminates the intra-record ordering constraints. These two proposed log optimizations are general and can be applied to many existing designs. We evaluate our designs using both micro-benchmarks and real PM applications. Our experimental results show that LALEA and BLOM can achieve 54.04% and 17.16% higher transaction throughput on average, compared to two state-of-the-art designs, respectively.",
        "link": "https://dl.acm.org/doi/10.5555/3539845.3539939",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Synthesizing Formal Models of Hardware from RTL for Efficient Verification of Memory Model Implementations",
        "authors": "['Yao Hsiao', 'Dominic P. Mulligan', 'Nikos Nikoleris', 'Gustavo Petri', 'Caroline Trippel']",
        "date": "October 2021",
        "source": "MICRO '21: MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture",
        "abstract": "Modern hardware complexity makes it challenging to determine if a given microarchitecture adheres to a particular memory consistency model (MCM). This observation inspired the Check tools, which formally check that a specific microarchitecture correctly implements an MCM with respect to a suite of litmus test programs. Unfortunately, despite their effectiveness and efficiency, the Check tools must be supplied a microarchitecture in the guise of a manually constructed axiomatic specification, called a μspec model.  To facilitate MCM verification—and enable the Check tools to consume processor RTL directly—we introduce a methodology and associated tool, rtl2μspec, for automatically synthesizing μspec models from processor designs written in Verilog or SystemVerilog, with the help of modest user-provided design metadata. As a case study, we use rtl2μspec to facilitate the Check-based verification of the four-core RISC-V V-scale (multi-V-scale) processor’s MCM implementation. We show that rtl2μspec can synthesize a complete, and proven correct by construction, μspec model from the SystemVerilog design of the multi-V-scale processor in 6.84 minutes. Subsequent Check-based MCM verification of the synthesized μspec model takes less than one second per litmus test.",
        "link": "https://dl.acm.org/doi/10.1145/3466752.3480087",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A Cost-Efficient Digital ESN Architecture on FPGA for OFDM Symbol Detection",
        "authors": "['Victor M. Gan', 'Yibin Liang', 'Lianjun Li', 'Lingjia Liu', 'Yang Yi']",
        "date": "None",
        "source": "ACM Journal on Emerging Technologies in Computing Systems",
        "abstract": "The echo state network (ESN) is a recently developed machine-learning paradigm whose processing capabilities rely on the dynamical behavior of recurrent neural networks. Its performance outperforms traditional recurrent neural networks in nonlinear system identification and temporal information processing applications. We design and implement a cost-efficient ESN architecture on field-programmable gate array (FPGA) that explores the full capacity of digital signal processor blocks on low-cost and low-power FPGA hardware. Specifically, our scalable ESN architecture on FPGA exploits Xilinx DSP48E1 units to cut down the need of configurable logic blocks. The proposed architecture includes a linear combination processor with negligible deployment of configurable logic blocks and a high-accuracy nonlinear function approximator. Our work is verified with the prediction task on the classical NARMA dataset and a symbol detection task for orthogonal frequency division multiplexing systems using a wireless communication testbed built on a software-defined radio platform. Experiments and performance measurement show that the new ESN architecture is capable of processing real-world data efficiently for low-cost and low-power applications.",
        "link": "https://dl.acm.org/doi/10.1145/3440017",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "DECA: DoD Enterprise Cloud Architecture Concept for Cloud-Based Cross Domain Solutions",
        "authors": "['Leo Aguilera', 'Doug Jacobson']",
        "date": "December 2021",
        "source": "AICCC '21: Proceedings of the 2021 4th Artificial Intelligence and Cloud Computing Conference",
        "abstract": "The Department of Defense (DoD) battlefield exists both in cyber and the physical world. Information sharing is a top priority for the DoD in support of our warfighters and allies. To maintain military technological advantage and superiority, access to information and the capacity to process it are critical components for empowering the warfighter for mission success. The volume of information shared has increased exponentially, necessitating the development of a DoD enterprise cloud capable of sustaining, and supporting strategic worldwide DoD missions through effective information sharing. However, the existing U.S. Government cloud design does not support enterprise use, and legacy software and hardware applications such as a Cross Domain Solution (CDS) will need to be re-architected, certified, accredited, and authorized for future enterprise cloud use. A CDS is a requirement for information sharing in both unclassified and classified systems and information transmission from one system to another, but there must also be a DoD enterprise cloud structure to leverage the CDS in the U.S. Government cloud. The purpose of this research is to explore the future possibilities of using an enterprise cloud CDS and to present a conceptual design for a DoD enterprise cloud architecture that will save the DoD time and money in the certification process while also allowing efficient information sharing across multiple DoD Command and Control (C2) systems. To have this architecture design approved and accredited by the DoD for future use, we adhere to the Federal Risk and Authorization Management Program (FedRAMP), a process required for federal agency cloud deployments and the National Institute of Standards and Technology (NIST) standards. We use existing systems from across the DoD and allies found in the open literature as a baseline.",
        "link": "https://dl.acm.org/doi/10.1145/3508259.3508283",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Reducing network latency in industrial IoT systems using hardware-software complex based on injection",
        "authors": "['Alexey Slepnev', 'Danila Matveev', 'Evgenii Karelin', 'Ivan Tumanov', 'Ivan Vnukov', 'Ruslan Kirichek', 'Igor Kandakov']",
        "date": "December 2021",
        "source": "ICFNDS 2021: The 5th International Conference on Future Networks &amp; Distributed Systems",
        "abstract": "Over the past couple of years, the Internet and data networks have evolved noticeably. The number of IoT devices is increasing every year, and according to forecasts, it will continue to grow. Because of this, the shortcomings of today’s network architecture are becoming more and more apparent. In this article, we present a universal system for optimizing networks, in which it will be built.",
        "link": "https://dl.acm.org/doi/10.1145/3508072.3508227",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Automating the design flow under dynamic partial reconfiguration for hardware-software co-design in FPGA SoC",
        "authors": "['Biruk Seyoum', 'Marco Pagani', 'Alessandro Biondi', 'Giorgio Buttazzo']",
        "date": "March 2021",
        "source": "SAC '21: Proceedings of the 36th Annual ACM Symposium on Applied Computing",
        "abstract": "Despite its benefits, hardware acceleration under dynamic partial reconfiguration (DPR) has not been fully leveraged by many system designers, mostly due to the complexities of the DPR design flow and the lack of efficient design tools to automate the design process. Furthermore, making such a design approach suitable for real-time embedded systems requires the need for extending the standard DPR design flow with additional design steps, which have to accurately account for the timing behavior of the software and hardware components of the design, as well as of the components of the computing platform (e.g., the reconfiguration interface). To address this problem, this paper presents DART, a tool that fully automates the design flow in a real-time DPR-based system that comprises both software and hardware components. The tool targets the Zynq 7-series and Ultrascale+ FPGA-based SoCs by Xilinx. It aims at alleviating the manual effort required by state-of-the-art tools while not expecting high expertise in the design of programmable logic components under DPR. To this purpose, it fully automates the partitioning, floorplanning, and implementation (routing and bitstream generation) phases, generating a set of bitstreams starting from a set of tasks annotated with high-level timing requirements. The tool leverages mathematical optimization to solve the partitioning and floorplanning problems, and relies on a set of auto-generated scripts that interact with the vendor tools to mobilize the synthesis and implementation stages. DART has been experimentally evaluated with a case study application from an accelerated image processing system.",
        "link": "https://dl.acm.org/doi/10.1145/3412841.3441928",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A Spiking Neuromorphic Architecture Using Gated-RRAM for Associative Memory",
        "authors": "['Alexander Jones', 'Aaron Ruen', 'Rashmi Jha']",
        "date": "None",
        "source": "ACM Journal on Emerging Technologies in Computing Systems",
        "abstract": "This work reports a spiking neuromorphic architecture for associative memory simulated in a SPICE environment using recently reported gated-RRAM (resistive random-access memory) devices as synapses alongside neurons based on complementary metal-oxide semiconductors (CMOSs). The network utilizes a Verilog A model to capture the behavior of the gated-RRAM devices within the architecture. The model uses parameters obtained from experimental gated-RRAM devices that were fabricated and tested in this work. Using these devices in tandem with CMOS neuron circuitry, our results indicate that the proposed architecture can learn an association in real time and retrieve the learned association when incomplete information is provided. These results show the promise for gated-RRAM devices for associative memory tasks within a spiking neuromorphic architecture framework.",
        "link": "https://dl.acm.org/doi/10.1145/3461667",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Multi-task Command and Control System based on Cloud Architecture",
        "authors": "['Guang Hu', 'Mingmei Zhang', 'Ming Ni', 'Wanzeng Cai', 'Junjie Wang']",
        "date": "October 2021",
        "source": "SSIP '21: Proceedings of the 2021 4th International Conference on Sensors, Signal and Image Processing",
        "abstract": "This paper designs a multi-task command and control system based on cloud architecture. It can greatly facilitate the deployment of multiple command and control tasks. The cloud architecture in this system includes virtualized cloud, container cloud, and desktop cloud. The management nodes in the container cloud and the underlying virtual machines in the desktop cloud are deployed on the virtualized cloud. In order to make full use of and ensure the reliability of data, the data of virtualized cloud, container cloud, and desktop cloud are all stored on distributed storage systems. The system can greatly reduce the operational complexity of operation and maintenance personnel through introducing unified cloud management and comprehensive operation and maintenance.",
        "link": "https://dl.acm.org/doi/10.1145/3502814.3502821",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "An urban sensing architecture as essential infrastructure for future cities",
        "authors": "['Vijay Kumar', 'George Oikonomou', 'Theo Tryfonas']",
        "date": "December 2021",
        "source": "UCC '21: Proceedings of the 14th IEEE/ACM International Conference on Utility and Cloud Computing Companion",
        "abstract": "Climate change and migration have become one of the most challenging problems for our civilization. In this context, city councils work hard to manage essential services for citizens such as waste collection, street lamp lighting, and water supply. Increasingly, digitalization and the Internet of Things (IoT) help cities improve services, increase productivity and reduce costs. However, to understand how this may happen, we explore the urban sensing capabilities from citizen- to city-scale, how sensing at different levels is interlinked, and the challenges of managing innovations based on IoT data and devices. Local authorities collaborate with researchers and deploy testbeds as a part of demonstration and research projects to perform the above data collection, improve city services, and support innovation. The data gathered is about indoor and outdoor environmental conditions, energy usage, built environment, structural health monitoring. Such monitoring requires IT infrastructure at three different tiers: at the endpoint, edge, and cloud. Managing infrastructure at all tiers with provisioning, connectivity, security updates of devices, user data privacy controls, visualization of data, multi-tenancy of applications, and network resilience, is challenging. So, in turn, we focus on performing a systematic study of the technical and non-technical challenges faced during the implementation, management, and deployment of devices into citizens' homes and public spaces. Our third piece of work explores IoT edge applications' resiliency and reliability requirements that vary from non-critical (best delivery efforts) to safety-critical with time-bounded guarantees. We investigate how to meet IoT application mixed-criticality QoS requirements in multi-communication networks. Finally, to demonstrate the principles of our framework in the real world, we implement an open-source air quality platform Open City Air Quality Platform (OpenCAQP), that merges a wide range of data sources and air pollution parameters into a single platform. The OpenCAQP allows citizens, environmentalists, data analysts, and developers to access and visualize that data.",
        "link": "https://dl.acm.org/doi/10.1145/3492323.3503507",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Leveraging Targeted Value Prediction to Unlock New Hardware Strength Reduction Potential",
        "authors": "['Arthur Perais']",
        "date": "October 2021",
        "source": "MICRO '21: MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture",
        "abstract": "Value Prediction (VP) is a microarchitectural technique that speculatively breaks data dependencies to increase the available Instruction Level Parallelism (ILP) in general purpose processors. Despite recent proposals, VP remains expensive and has intricate interactions with several stages of the classical superscalar pipeline. In this paper, we revisit and simplify VP by leveraging the irregular distribution of the values produced during the execution of common programs.  First, we demonstrate that a reasonable fraction of the performance uplift brought by a full VP infrastructure can be obtained by predicting only a few ”usual suspects” values. Furthermore, we show that doing so allows to greatly simplify VP operation as well as reduce the value predictor footprint. Lastly, we show that these Minimal and Targeted VP infrastructures conceptually enable Speculative Strength Reduction (SpSR), a rename-time optimization whereby instructions can disappear at rename in the presence of specific operand values.",
        "link": "https://dl.acm.org/doi/10.1145/3466752.3480050",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Hardware acceleration of tensor-structured multilevel ewald summation method on MDGRAPE-4A, a special-purpose computer system for molecular dynamics simulations",
        "authors": "['Gentaro Morimoto', 'Yohei M. Koyama', 'Hao Zhang', 'Teruhisa S. Komatsu', 'Yousuke Ohno', 'Keigo Nishida', 'Itta Ohmura', 'Hiroshi Koyama', 'Makoto Taiji']",
        "date": "November 2021",
        "source": "SC '21: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",
        "abstract": "We developed MDGRAPE-4A, a special-purpose computer system for molecular dynamics simulations, consisting of 512 nodes of custom system-on-a-chip LSIs with dedicated processor cores and interconnects designed to achieve strong scalability for biomolecular simulations. To reduce the global communications required for the evaluation of Coulomb interactions, we conducted a co-design of the MDGRAPE-4A and the novel algorithm, tensor-structured multilevel Ewald summation method (TME), which produced hardware modules on the custom LSI circuit for particle-grid operations and for grid-grid separable convolutions on a 3D torus network. We implemented the convolution for the top-level grid potentials by using 3D FFTs on an FPGA, along with an FPGA-based octree network to gather grid charges. The elapsed time for the long-range part of Coulomb is 50 μs, which can mostly overlap with those for the short-range part, and the additional cost is approximately 10 μs/step, which is only a 5% performance loss.",
        "link": "https://dl.acm.org/doi/10.1145/3458817.3476190",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "GPU Domain Specialization via Composable On-Package Architecture",
        "authors": "['Yaosheng Fu', 'Evgeny Bolotin', 'Niladrish Chatterjee', 'David Nellans', 'Stephen W. Keckler']",
        "date": "None",
        "source": "ACM Transactions on Architecture and Code Optimization",
        "abstract": "As GPUs scale their low-precision matrix math throughput to boost deep learning (DL) performance, they upset the balance between math throughput and memory system capabilities. We demonstrate that a converged GPU design trying to address diverging architectural requirements between FP32 (or larger)-based HPC and FP16 (or smaller)-based DL workloads results in sub-optimal configurations for either of the application domains. We argue that a Composable On-PAckage GPU (COPA-GPU) architecture to provide domain-specialized GPU products is the most practical solution to these diverging requirements. A COPA-GPU leverages multi-chip-module disaggregation to support maximal design reuse, along with memory system specialization per application domain. We show how a COPA-GPU enables DL-specialized products by modular augmentation of the baseline GPU architecture with up to 4× higher off-die bandwidth, 32× larger on-package cache, and 2.3× higher DRAM bandwidth and capacity, while conveniently supporting scaled-down HPC-oriented designs. This work explores the microarchitectural design necessary to enable composable GPUs and evaluates the benefits composability can provide to HPC, DL training, and DL inference. We show that when compared to a converged GPU design, a DL-optimized COPA-GPU featuring a combination of 16× larger cache capacity and 1.6× higher DRAM bandwidth scales per-GPU training and inference performance by 31% and 35%, respectively, and reduces the number of GPU instances by 50% in scale-out training scenarios.",
        "link": "https://dl.acm.org/doi/10.1145/3484505",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Study of the Utility of Text Classification Based Software Architecture Recovery Method RELAX for Maintenance",
        "authors": "['Daniel Link', 'Kamonphop Srisopha', 'Barry Boehm']",
        "date": "October 2021",
        "source": "ESEM '21: Proceedings of the 15th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)",
        "abstract": "Background. The software architecture recovery method RELAX produces a concern-based architectural view of a software system graphically and textually from that system's source code. The method has been implemented in software which can recover the architecture of systems whose source code is written in Java. Aims. Our aim was to find out whether the availability of architectural views produced by RELAX can help maintainers who are new to a project in becoming productive with development tasks sooner, and how they felt about working in such an environment. Method. We conducted a user study with nine participants. They were subjected to a controlled experiment in which maintenance success and speed with and without access to RELAX recovery results were compared to each other. Results. We have observed that employing architecture views produced by RELAX helped participants reduce time to get started on maintenance tasks by a factor of 5.38 or more. While most participants were unable to finish their tasks within the allotted time when they did not have recovery results available, all of them finished them successfully when they did. Additionally, participants reported that these views were easy to understand, helped them to learn the system's structure and enabled them to compare different versions of the system. Conclusions. Through the speedup to the start of maintenance experienced by the participants as well as in their formed opinions, RELAX has shown itself to be a valuable help that could provide the basis of further tools that specifically support the development process with a focus on maintenance.",
        "link": "https://dl.acm.org/doi/10.1145/3475716.3484194",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Research on Network Architecture Design Based on Artificial Intelligence Application Technology",
        "authors": "['Jing Guo']",
        "date": "January 2022",
        "source": "icWCSN '22: Proceedings of the 2022 9th International Conference on Wireless Communication and Sensor Networks",
        "abstract": "Abstract: With the continuous development of AI technology, the training of massive data and the emergence of large-scale models have made stand-alone model training increasingly unable to meet the needs of AI applications. Distributed machine learning technologies (such as data parallelism and model parallelism) have appeared at historic moments and will have extreme large-scale application scenarios. At present, the training speed of distributed machine learning models is slow, and the scale of model parameters is still the main problem in this field. From the perspective of model parallelism, this article aims to design the optimal division method for different models under model parallelism by analyzing the structure of the existing AI application model. According to the framework structure of artificial intelligence application model, design the model optimization partition strategy and model based on parallelism. A network architecture suitable for accelerating AI application training, focusing on solving technical problems, such as network architecture design based on AI applications and model optimization and partitioning under model parallelization.",
        "link": "https://dl.acm.org/doi/10.1145/3514105.3514124",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Neural Cascade Architecture With Triple-Domain Loss for Speech Enhancement",
        "authors": "['Heming Wang', 'DeLiang Wang']",
        "date": "None",
        "source": "IEEE/ACM Transactions on Audio, Speech and Language Processing",
        "abstract": "This paper proposes a neural cascade architecture to address the monaural speech enhancement problem. The cascade architecture is composed of three modules which optimize in turn enhanced speech with respect to the magnitude spectrogram, the time-domain signal and the complex spectrogram. Each module takes as input the noisy speech and the output obtained from the previous module, and generates a prediction of the respective target. Our model is trained in an end-to-end manner, using a triple-domain loss function that accounts for three domains of signal representation. Experimental results on the WSJ0 SI-84 corpus show that the proposed model outperforms other strong speech enhancement baselines in terms of objective speech quality and intelligibility.",
        "link": "https://dl.acm.org/doi/10.1109/TASLP.2021.3138716",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Timing-Driven X-architecture Steiner Minimum Tree Construction Based on Social Learning Multi-Objective Particle Swarm Optimization",
        "authors": "['Xiaohua Chen', 'Ruping Zhou', 'Genggeng Liu', 'Zhen Chen', 'Wenzhong Guo']",
        "date": "April 2021",
        "source": "WWW '21: Companion Proceedings of the Web Conference 2021",
        "abstract": "The construction of timing-driven Steiner minimum tree is a critical issue in VLSI routing design. Meanwhile, since the interconnection model of X-architecture can make full use of routing resources compared to the traditional Manhattan architecture, constructing a Timing-Driven X-architecture Steiner Minimum Tree (TDXSMT) is of great significance to improving routing performance. In this paper, an efficient algorithm based on Social Learning Multi-Objective Particle Swarm Optimization (SLMOPSO) is proposed to construct a TDXSMT with minimizing the maximum source-to-sink pathlength. An X-architecture Prim-Dijkstra model is presented to construct an initial Steiner tree which can optimize both the wirelength and the maximum source-to-sink pathlength. In order to find a better solution, an SLMOPSO method based on the nearest and best select strategy is presented to improve the global exploration capability of the algorithm. Besides, the mutation and crossover operators are utilized to achieve the discrete particle update process, thereby better solving the discrete TDXSMT problem. The experimental results indicate that the proposed algorithm has an excellent trade-off between the wirelength and maximum source-to-sink pathlength of the routing tree and can greatly optimize the timing delay.",
        "link": "https://dl.acm.org/doi/10.1145/3442442.3451143",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Break dancing: low overhead, architecture neutral software branch tracing",
        "authors": "['Gabriel Marin', 'Alexey Alexandrov', 'Tipp Moseley']",
        "date": "June 2021",
        "source": "LCTES 2021: Proceedings of the 22nd ACM SIGPLAN/SIGBED International Conference on Languages, Compilers, and Tools for Embedded Systems",
        "abstract": "Sampling-based Feedback Directed Optimization (FDO) methods like AutoFDO and BOLT that employ profiles collected in live production environments, are commonly used in datacenter applications to attain significant performance benefits without the toil of maintaining representative load tests. Sampled profiles rely on hardware facilities like Intel’s Last Branch Record (LBR) which are not currently available even on popular CPUs from ARM or AMD. Since not all architectures include a hardware LBR feature, we present an architecture neutral approach to collect LBR-like data. We use sampling and limited program tracing to capture LBR-like data from optimized and unmodified applications binaries. Since the implementation is in user space, we can collect arbitrarily long LBR buffers, and by varying the sampling rate, we can adjust the runtime overhead to arbitrarily low values. We target runtime overheads of <2% when the profiler is on and zero when it’s off. This amortizes to negligible fleet-wide collection cost given the size of a modern production fleet. We implemented a profiler that uses this method of software branch tracing. We also analyzed its overhead and the similarity of the data it collects to the Intel LBR hardware using the SPEC2006 benchmarks. Results demonstrate profile quality and optimization efficacy at parity with LBR-based AutoFDO and the target profiling overhead being achievable even without implementing any advanced tuning.",
        "link": "https://dl.acm.org/doi/10.1145/3461648.3463853",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Research on the information System architecture design framework and reference resources of American Army",
        "authors": "['Ping Jian']",
        "date": "November 2021",
        "source": "AISS '21: Proceedings of the 3rd International Conference on Advanced Information Science and System",
        "abstract": "The concept of architecture is put forward by the US Army, which refers to the composition structure of the system and its mutual relationship, as well as the principles and guidelines guiding the design and development of the system. Architecture design technology and its reference resources have important theoretical and technical support for the top-level design of information system. Based on the analysis of architecture technology and function, this paper systematically studies the connotation and development history of the architecture design framework of the US army, and sorts out the main reference resources of the architecture design of the US army, such as the common joint task list, information system interoperability level model, joint technology architecture. It provides useful reference for the theoretical and technical research of information system top-level design.",
        "link": "https://dl.acm.org/doi/10.1145/3503047.3503094",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Neural Cascade Architecture for Multi-Channel Acoustic Echo Suppression",
        "authors": "['Hao Zhang', 'DeLiang Wang']",
        "date": "None",
        "source": "IEEE/ACM Transactions on Audio, Speech and Language Processing",
        "abstract": "Traditional acoustic echo cancellation (AEC) works by identifying an acoustic impulse response using adaptive algorithms. This paper proposes a neural cascade architecture for joint acoustic echo and noise suppression to address both single-channel and multi-channel AEC (MCAEC) problems. The proposed cascade architecture consists of two modules. A convolutional recurrent network (CRN) is employed in the first module for complex spectral mapping. Its output is fed as an additional input to the second module, where a long short-term memory network (LSTM) is utilized for magnitude mask estimation. The entire architecture is trained in an end-to-end manner with the two modules optimized jointly using a single loss function. The final output is generated using the enhanced phase and magnitude obtained from the first and the second module, respectively. The cascade architecture enables the proposed method to obtain robust magnitude estimation as well as phase enhancement. The proposed method is investigated under different AEC setups. We find that the deep learning based approach avoids the no-uniqueness problem in traditional MCAEC. For MCAEC setups with multiple microphones, combining deep MCAEC with supervised beamforming further improves the system performance. Evaluation results show that the proposed approach effectively suppresses acoustic echo and noise while preserving speech quality, and consistently outperforms related methods under different setups.",
        "link": "https://dl.acm.org/doi/10.1109/TASLP.2022.3192104",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A Novelty-Centric Agent Architecture for Changing Worlds",
        "authors": "['Faizan Muhammad', 'Vasanth Sarathy', 'Gyan Tatiya', 'Shivam Goel', 'Saurav Gyawali', 'Mateo Guaman', 'Jivko Sinapov', 'Matthias Scheutz']",
        "date": "May 2021",
        "source": "AAMAS '21: Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems",
        "abstract": "Open-world AI requires artificial agents to cope with novelties that arise during task performance, i.e., they must (1) detect novelties, (2) characterize them, in order to (3) accommodate them, especially in cases where sudden changes to the environment make task accomplishment impossible without utilizing the novelty. We present a formal framework and implementation thereof in a cognitive agent for novelty handling and demonstrate the efficacy of the proposed methods for detecting and handling a large set of novelties in a crafting task in a simulated environment. We discuss the success of the proposed knowledge-based methods and propose heuristic extensions that will further improve novelty handling in open-worlds tasks.",
        "link": "https://dl.acm.org/doi/10.5555/3463952.3464062",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Pareto-optimal progressive neural architecture search",
        "authors": "['Eugenio Lomurno', 'Stefano Samele', 'Matteo Matteucci', 'Danilo Ardagna']",
        "date": "July 2021",
        "source": "GECCO '21: Proceedings of the Genetic and Evolutionary Computation Conference Companion",
        "abstract": "Neural Architecture Search (NAS) is the process of automating architecture engineering, searching for the best deep learning configuration. One of the main NAS approaches proposed in the literature, Progressive Neural Architecture Search (PNAS), seeks for the architectures with a sequential model-based optimization strategy: it defines a common recursive structure to generate the networks, whose number of building blocks rises through iterations. However, NAS algorithms are generally designed for an ideal setting without considering the needs and the technical constraints imposed by practical applications. In this paper, we propose a new architecture search named Pareto-Optimal Progressive Neural Architecture Search (POPNAS) that combines the benefits of PNAS to a time-accuracy Pareto optimization problem. POPNAS adds a new time predictor to the existing approach to carry out a joint prediction of time and accuracy for each candidate neural network, searching through the Pareto front. This allows us to reach a trade-off between accuracy and training time, identifying neural network architectures with competitive accuracy in the face of a drastically reduced training time.",
        "link": "https://dl.acm.org/doi/10.1145/3449726.3463146",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Rapid Detection Solution for Fabric Printing Based on Dual Architecture Design of FPGA and Neural Network",
        "authors": "['Feng Li', 'Qinggang Xi', 'Yiqing Feng']",
        "date": "May 2021",
        "source": "ICFEICT 2021: International Conference on Frontiers of Electronics, Information and Computation Technologies",
        "abstract": "A rapid detection solution for fabric printing based on dual architecture design of FPGA and neural network is proposed to address the problems of low accuracy and slow speed of fabric printing positioning in modern textile fabric manufacturing companies. The detection solution combines the popular deep neural networks with FPGA(Field Programmable Logic Gate Arrays) by selecting a lightweight deep neural network that is compatible with fabric print localization and using an improved method in the network training process. Then we design a hardware architecture solution that is compatible with the network to accelerate the convolutional pooling and other operations of the network, and finally building a complete fabric printing detection solution. The experimental results show that the combination of the lightweight deep neural network and the designed hardware architecture solution can effectively localize the fabric prints while maximizing the hardware resources and meeting the basic real-time requirements. The designed scheme is less powerful compared to GPU and faster compared to CPU.",
        "link": "https://dl.acm.org/doi/10.1145/3474198.3478157",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Emotional AI in Healthcare: a pilot architecture proposal to merge emotion recognition tools",
        "authors": "['Samuel Marcos', 'Francisco José García Peñalvo', 'Andrea Vázquez Ingelmo']",
        "date": "October 2021",
        "source": "TEEM'21: Ninth International Conference on Technological Ecosystems for Enhancing Multiculturality (TEEM&apos;21)",
        "abstract": "The use of emotional artificial intelligence (EAI) looks promising and is continuing to improve during the last years. However, in order to effectively use EAI to help in the diagnose and treat health conditions there are still significant challenges to be tackled. Because EAI is still under development, one of the most important challenges is to integrate the technology into the health provision process. In this sense, it is important to complement EAI technologies with expert supervision, and to provide health professionals with the necessary tools to make the best of EAI without a deep knowledge of the technology. The present work aims to provide an initial architecture proposal for making use of different available technologies for emotion recognition, where their combination could enhance emotion detection. The proposed architecture is based on an evolutionary approach so to be integrated in digital health ecosystems, so new modules can be easily integrated. In addition, internal data exchange utilizes Robot Operating System (ROS) syntax, so it can also be suitable for physical agents.",
        "link": "https://dl.acm.org/doi/10.1145/3486011.3486472",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A Real-Time Effective Fusion-Based Image Defogging Architecture on FPGA",
        "authors": "['Gaoming Du', 'Jiting Wu', 'Hongfang Cao', 'Kun Xing', 'Zhenmin Li', 'Duoli Zhang', 'Xiaolei Wang']",
        "date": "None",
        "source": "ACM Transactions on Multimedia Computing, Communications, and Applications",
        "abstract": "Foggy weather reduces the visibility of photographed objects, causing image distortion and decreasing overall image quality. Many approaches (e.g., image restoration, image enhancement, and fusion-based methods) have been proposed to work out the problem. However, most of these defogging algorithms are facing challenges such as algorithm complexity or real-time processing requirements. To simplify the defogging process, we propose a fusional defogging algorithm on the linear transmission of gray single-channel. This method combines gray single-channel linear transform with high-boost filtering according to different proportions. To enhance the visibility of the defogging image more effectively, we convert the RGB channel into a gray-scale single channel without decreasing the defogging results. After gray-scale fusion, the data in the gray-scale domain should be linearly transmitted. With the increasing real-time requirements for clear images, we also propose an efficient real-time FPGA defogging architecture. The architecture optimizes the data path of the guided filtering to speed up the defogging speed and save area and resources. Because the pixel reading order of mean and square value calculations are identical, the shift register in the box filter after the average and the computation of the square values is separated from the box filter and put on the input terminal for sharing, saving the storage area. What’s more, using LUTs instead of the multiplier can decrease the time delays of the square value calculation module and increase efficiency. Experimental results show that the linear transmission can save 66.7% of the total time. The architecture we proposed can defog efficiently and accurately, meeting the real-time defogging requirements on 1920 × 1080 image size.",
        "link": "https://dl.acm.org/doi/10.1145/3446241",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Minimally Invasive HW/SW Co-debug Live Visualization on Architecture Level",
        "authors": "['Pascal Pieper', 'Ralf Wimmer', 'Gerhard Angst', 'Rolf Drechsler']",
        "date": "June 2021",
        "source": "GLSVLSI '21: Proceedings of the 2021 on Great Lakes Symposium on VLSI",
        "abstract": "We present a tool that allows developers to debug hard- and software and their interaction in an early design stage. We combine a SystemC virtual prototype (VP) with an easily configurable and interactive graphical user interface and a standard software debugger. The graphical user interface visualizes the internal state of the hardware. At the same time, the software debugger monitors and allows to manipulate the state of the software. This co-visualization supports design understanding and live debugging of the HW/SW interaction. We demonstrate its usefulness with a case-study where we debug an OLED display driver running on a RISC-V VP.",
        "link": "https://dl.acm.org/doi/10.1145/3453688.3461524",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "SAGE: A Split-Architecture Methodology for Efficient End-to-End Autonomous Vehicle Control",
        "authors": "['Arnav Malawade', 'Mohanad Odema', 'Sebastien Lajeunesse-degroot', 'Mohammad Abdullah Al Faruque']",
        "date": "None",
        "source": "ACM Transactions on Embedded Computing Systems",
        "abstract": "Autonomous vehicles (AV) are expected to revolutionize transportation and improve road safety significantly. However, these benefits do not come without cost; AVs require large Deep-Learning (DL) models and powerful hardware platforms to operate reliably in real-time, requiring between several hundred watts to one kilowatt of power. This power consumption can dramatically reduce vehicles’ driving range and affect emissions. To address this problem, we propose SAGE: a methodology for selectively offloading the key energy-consuming modules of DL architectures to the cloud to optimize edge, energy usage while meeting real-time latency constraints. Furthermore, we leverage Head Network Distillation (HND) to introduce efficient bottlenecks within the DL architecture in order to minimize the network overhead costs of offloading with almost no degradation in the model’s performance. We evaluate SAGE using an Nvidia Jetson TX2 and an industry-standard Nvidia Drive PX2 as the AV edge, devices and demonstrate that our offloading strategy is practical for a wide range of DL models and internet connection bandwidths on 3G, 4G LTE, and WiFi technologies. Compared to edge-only computation, SAGE reduces energy consumption by an average of 36.13%, 47.07%, and 55.66% for an AV with one low-resolution camera, one high-resolution camera, and three high-resolution cameras, respectively. SAGE also reduces upload data size by up to 98.40% compared to direct camera offloading.",
        "link": "https://dl.acm.org/doi/10.1145/3477006",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "ELSA: hardware-software co-design for efficient, lightweight self-attention mechanism in neural networks",
        "authors": "['Tae Jun Ham', 'Yejin Lee', 'Seong Hoon Seo', 'Soosung Kim', 'Hyunji Choi', 'Sung Jun Jung', 'Jae W. Lee']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "The self-attention mechanism is rapidly emerging as one of the most important key primitives in neural networks (NNs) for its ability to identify the relations within input entities. The self-attention-oriented NN models such as Google Transformer and its variants have established the state-of-the-art on a very wide range of natural language processing tasks, and many other self-attention-oriented models are achieving competitive results in computer vision and recommender systems as well. Unfortunately, despite its great benefits, the self-attention mechanism is an expensive operation whose cost increases quadratically with the number of input entities that it processes, and thus accounts for a significant portion of the inference runtime. Thus, this paper presents ELSA (Efficient, Lightweight Self-Attention), a hardware-software co-designed solution to substantially reduce the runtime as well as energy spent on the self-attention mechanism. Specifically, based on the intuition that not all relations are equal, we devise a novel approximation scheme that significantly reduces the amount of computation by efficiently filtering out relations that are unlikely to affect the final output. With the specialized hardware for this approximate self-attention mechanism, ELSA achieves a geomean speedup of 58.1X as well as over three orders of magnitude improvements in energy efficiency compared to GPU on self-attention computation in modern NN models while maintaining less than 1% loss in the accuracy metric.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00060",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "High-Performance and Energy-Efficient 3D Manycore GPU Architecture for Accelerating Graph Analytics",
        "authors": "['Dwaipayan Choudhury', 'Aravind Sukumaran Rajam', 'Ananth Kalyanaraman', 'Partha Pratim Pande']",
        "date": "None",
        "source": "ACM Journal on Emerging Technologies in Computing Systems",
        "abstract": "Recent advances in GPU-based manycore accelerators provide the opportunity to efficiently process large-scale graphs on chip. However, real world graphs have a diverse range of topology and connectivity patterns (e.g., degree distributions) that make the design of input-agnostic hardware architectures a challenge. Network-on-Chip (NoC)-based architectures provide a way to overcome this challenge as the architectural topology can be used to approximately model the expected traffic patterns that emerge from graph application workloads. In this paper, we first study the mix of long- and short-range traffic patterns generated on-chip using graph workloads, and subsequently use the findings to adapt the design of an optimal NoC-based architecture. In particular, by leveraging emerging three-dimensional (3D) integration technology, we propose design of a small-world NoC (SWNoC)-enabled manycore GPU architecture, where the placement of the links connecting the streaming multiprocessors (SM) and the memory controllers (MC) follow a power-law distribution. The proposed 3D manycore GPU architecture outperforms the traditional planar (2D) counterparts in both performance and energy consumption. Moreover, by adopting a joint performance-thermal optimization strategy, we address the thermal concerns in a 3D design without noticeably compromising the achievable performance. The 3D integration technology is also leveraged to incorporate Near Data Processing (NDP) to complement the performance benefits introduced by the SWNoC architecture. As graph applications are inherently memory intensive, off-chip data movement gives rise to latency and energy overheads in the presence of external DRAM. In conventional GPU architectures, as the main memory layer is not integrated with the logic, off-chip data movement negatively impacts overall performance and energy consumption. We demonstrate that NDP significantly reduces the overheads associated with such frequent and irregular memory accesses in graph-based applications. The proposed SWNoC-enabled NDP framework that integrates 3D memory (like Micron's HMC) with a massive number of GPU cores achieves 29.5% performance improvement and 30.03% less energy consumption on average compared to a conventional planar Mesh-based design with external DRAM.",
        "link": "https://dl.acm.org/doi/10.1145/3482880",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A Digital Twin Architecture for Wireless Networked Adaptive Active Noise Control",
        "authors": "['Chuang Shi', 'Feiyu Du', 'Qianyang Wu']",
        "date": "None",
        "source": "IEEE/ACM Transactions on Audio, Speech and Language Processing",
        "abstract": "The active noise control (ANC) is a complementary technique to the passive noise control (PNC) to reduce the low frequency noise. The ANC controller can be implemented by pre-trained filters or adaptive filters. The adaptive ANC controller is advantageous in its adaptation to environmental changes. However, the algorithm complexity of the adaptive ANC controller increases with the scale of ANC applications, making it difficult to be carried out on low-cost processors. To resolve this problem, cloud computing should be utilized in ANC systems, and thus the wireless networked ANC system is proposed. Since it is crucial for ANC controllers to generate the anti-noise wave in real time, this paper formulates a digital twin architecture that implements the control filter adaptation in the cloud and the anti-noise signal generation on the local controller, respectively. A digital twin filtered-reference least mean squares (DT-FxLMS) algorithm is proposed to coordinate the digital twin with the local controller. Simulation and experiment results demonstrate the effectiveness and efficiency of the wireless networked ANC system based on the digital twin architecture.",
        "link": "https://dl.acm.org/doi/10.1109/TASLP.2022.3199992",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "EfficientTDNN: Efficient Architecture Search for Speaker Recognition",
        "authors": "['Rui Wang', 'Zhihua Wei', 'Haoran Duan', 'Shouling Ji', 'Yang Long', 'Zhen Hong']",
        "date": "None",
        "source": "IEEE/ACM Transactions on Audio, Speech and Language Processing",
        "abstract": "Convolutional neural networks (CNNs), such as the time-delay neural network (TDNN), have shown their remarkable capability in learning speaker embedding. However, they meanwhile bring a huge computational cost in storage size, processing, and memory. Discovering the specialized CNN that meets a specific constraint requires a substantial effort of human experts. Compared with hand-designed approaches, neural architecture search (NAS) appears as a practical technique in automating the manual architecture design process and has attracted increasing interest in spoken language processing tasks such as speaker recognition. In this paper, we propose EfficientTDNN, an efficient architecture search framework consisting of a TDNN-based supernet and a TDNN-NAS algorithm. The proposed supernet introduces temporal convolution of different ranges of the receptive field and feature aggregation of various resolutions from different layers to TDNN. On top of it, the TDNN-NAS algorithm quickly searches for the desired TDNN architecture via weight-sharing subnets, which surprisingly reduces computation while handling the vast number of devices with various resources requirements. Experimental results on the VoxCeleb dataset show the proposed EfficientTDNN enables approximate <inline-formula><tex-math notation=\"LaTeX\">$10^{13}$</tex-math></inline-formula> architectures concerning depth, kernel, and width. Considering different computation constraints, it achieves a 2.20&#x0025; equal error rate (EER) with 204 M multiply-accumulate operations (MACs), 1.41&#x0025; EER with 571 M MACs as well as 0.94&#x0025; EER with 1.45 G MACs. Comprehensive investigations suggest that the trained supernet generalizes subnets not sampled during training and obtains a favorable trade-off between accuracy and efficiency.",
        "link": "https://dl.acm.org/doi/10.1109/TASLP.2022.3182856",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Neural Architecture Search for LF-MMI Trained Time Delay Neural Networks",
        "authors": "['Shoukang Hu', 'Xurong Xie', 'Mingyu Cui', 'Jiajun Deng', 'Shansong Liu', 'Jianwei Yu', 'Mengzhe Geng', 'Xunying Liu', 'Helen Meng']",
        "date": "None",
        "source": "IEEE/ACM Transactions on Audio, Speech and Language Processing",
        "abstract": "State-of-the-art automatic speech recognition (ASR) system development is data and computation intensive. The optimal design of deep neural networks (DNNs) for these systems often require expert knowledge and empirical evaluation. In this paper, a range of neural architecture search (NAS) techniques are used to automatically learn two types of hyper-parameters of factored time delay neural networks (TDNN-Fs): i) the left and right splicing context offsets; and ii) the dimensionality of the bottleneck linear projection at each hidden layer. These techniques include the differentiable neural architecture search (DARTS) method integrating architecture learning with lattice-free MMI training; Gumbel-Softmax and pipelined DARTS methods reducing the confusion over candidate architectures and improving the generalization of architecture selection; and Penalized DARTS incorporating resource constraints to balance the trade-off between performance and system complexity. Parameter sharing among TDNN-F architectures allows an efficient search over up to <inline-formula><tex-math notation=\"LaTeX\">$7^{28}$</tex-math></inline-formula> different systems. Statistically significant word error rate (WER) reductions of up to 1.2&#x0025; absolute and relative model size reduction of 31&#x0025; were obtained over a state-of-the-art 300-hour Switchboard corpus trained baseline LF-MMI TDNN-F system featuring speed perturbation, i-Vector and learning hidden unit contribution (LHUC) based speaker adaptation as well as RNNLM rescoring. Performance contrasts on the same task against recent end-to-end systems reported in the literature suggest the best NAS auto-configured system achieves state-of-the-art WERs of 9.9&#x0025; and 11.1&#x0025; on the NIST Hub5&#x2019; 00 and Rt03 s test sets respectively with up to 96&#x0025; model size reduction. Further analysis using Bayesian learning shows that the proposed NAS approaches can effectively minimize the structural redundancy in the TDNN-F systems and reduce their model parameter uncertainty. Consistent performance improvements were also obtained on a UASpeech dysarthric speech recognition task.",
        "link": "https://dl.acm.org/doi/10.1109/TASLP.2022.3153253",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Trojan Awakener: Detecting Dormant Malicious Hardware Using Laser Logic State Imaging",
        "authors": "['Thilo Krachenfels', 'Jean-Pierre Seifert', 'Shahin Tajik']",
        "date": "November 2021",
        "source": "ASHES '21: Proceedings of the 5th Workshop on Attacks and Solutions in Hardware Security",
        "abstract": "The threat of hardware Trojans (HTs) and their detection is a widely studied field. While the effort for inserting a Trojan into an application-specific integrated circuit (ASIC) can be considered relatively high, especially when trusting the chip manufacturer, programmable hardware is vulnerable to Trojan insertion even after the product has been shipped or during usage. At the same time, detecting dormant HTs with small or zero-overhead triggers and payloads on these platforms is still a challenging task, as the Trojan might not get activated during the chip verification using logical testing or physical measurements. In this work, we present a novel Trojan detection approach based on a technique known from integrated circuit (IC) failure analysis, capable of detecting virtually all classes of dormant Trojans. Using laser logic state imaging (LLSI), we show how supply voltage modulations can awaken inactive Trojans, making them detectable using laser voltage imaging techniques. Therefore, our technique does not require triggering the Trojan. To support our claims, we present two case studies on 28 nm SRAM- and flash-based field-programmable gate arrays (FPGAs). We demonstrate how to detect with high confidence small changes in sequential and combinatorial logic as well as in the routing configuration of FPGAs in a non-invasive manner. Finally, we discuss the practical applicability of our approach on dormant analog Trojans in ASICs.",
        "link": "https://dl.acm.org/doi/10.1145/3474376.3487282",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Power delivery and thermal-aware arm-based multi-tier 3D architecture",
        "authors": "['Lingjun Zhu', 'Tuan Ta', 'Rossana Liu', 'Rahul Mathur', 'Xiaoqing Xu', 'Shidhartha Das', 'Ankit Kaul', 'Alejandro Rico', 'Doug Joseph', 'Brian Cline', 'Sung Kyu Lim']",
        "date": "July 2021",
        "source": "ISLPED '21: Proceedings of the ACM/IEEE International Symposium on Low Power Electronics and Design",
        "abstract": "3D integration is becoming a cost-effective way to incorporate more CPU cores and memory to improve the performance of computing systems. Meanwhile, due to the higher power density, power delivery and thermal issues become more significant in multi-tier 3D ICs. In this paper, we explore and evaluate multiple design options for an Arm Neoverse-based 3D architecture focusing on power and thermals at 7nm process and sub-10μm pitch. Using a rapid voltage-drop and thermal analysis methodology, we model a system with a 32-core CPU layer and up to 4 layers of system-level caches, and quantify the trade-offs between performance, cost, voltage-drop, and temperature. A 3-layer configuration shows a good balance with 17% IPC gain and 17% lower cost, while incurring 15mV worse voltage drop and 8.5°C higher temperature compared with 2D. Our studies suggest that the co-optimization of system architecture, technology, and physical design is key for high-performance 3D systems.",
        "link": "https://dl.acm.org/doi/10.1109/ISLPED52811.2021.9502481",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "SISA: Set-Centric Instruction Set Architecture for Graph Mining on Processing-in-Memory Systems",
        "authors": "['Maciej Besta', 'Raghavendra Kanakagiri', 'Grzegorz Kwasniewski', 'Rachata Ausavarungnirun', 'Jakub Beránek', 'Konstantinos Kanellopoulos', 'Kacper Janda', 'Zur Vonarburg-Shmaria', 'Lukas Gianinazzi', 'Ioana Stefan', 'Juan Gómez Luna', 'Jakub Golinowski', 'Marcin Copik', 'Lukas Kapp-Schwoerer', 'Salvatore Di Girolamo', 'Nils Blach', 'Marek Konieczny', 'Onur Mutlu', 'Torsten Hoefler']",
        "date": "October 2021",
        "source": "MICRO '21: MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture",
        "abstract": "Simple graph algorithms such as PageRank have been the target of numerous hardware accelerators. Yet, there also exist much more complex graph mining algorithms for problems such as clustering or maximal clique listing. These algorithms are memory-bound and thus could be accelerated by hardware techniques such as Processing-in-Memory (PIM). However, they also come with non-straightforward parallelism and complicated memory access patterns. In this work, we address this problem with a simple yet surprisingly powerful observation: operations on sets of vertices, such as intersection or union, form a large part of many complex graph mining algorithms, and can offer rich and simple parallelism at multiple levels. This observation drives our cross-layer design, in which we (1) expose set operations using a novel programming paradigm, (2) express and execute these operations efficiently with carefully designed set-centric ISA extensions called SISA, and (3) use PIM to accelerate SISA instructions. The key design idea is to alleviate the bandwidth needs of SISA instructions by mapping set operations to two types of PIM: in-DRAM bulk bitwise computing for bitvectors representing high-degree vertices, and near-memory logic layers for integer arrays representing low-degree vertices. Set-centric SISA-enhanced algorithms are efficient and outperform hand-tuned baselines, offering more than 10 × speedup over the established Bron-Kerbosch algorithm for listing maximal cliques. We deliver more than 10 SISA set-centric algorithm formulations, illustrating SISA’s wide applicability.",
        "link": "https://dl.acm.org/doi/10.1145/3466752.3480133",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "ETEH: Unified Attention-Based End-to-End ASR and KWS Architecture",
        "authors": "['Gaofeng Cheng', 'Haoran Miao', 'Runyan Yang', 'Keqi Deng', 'Yonghong Yan']",
        "date": "None",
        "source": "IEEE/ACM Transactions on Audio, Speech and Language Processing",
        "abstract": "Even though attention-based end-to-end (E2E) automatic speech recognition (ASR) models have been yielding state-of-the-art recognition accuracy, they still fall behind many of the ASR models deployed in the industry in some crucial functionalities such as online processing and precise timestamps generating. This weakness prevents attention-based E2E ASR models from being applied in several essential speech tasks, such as online speech recognition and keyword searching (KWS). In this paper, we describe our proposed unified attention-based E2E ASR and KWS architecture&#x2013;ETEH, which supports, in one model, both online and offline ASR decoding modes, thus allowing for precise and reliable KWS. &#x201C;ETE&#x201D; stands for attention-based E2E modeling, whereas &#x201C;H&#x201D; represents the hybrid gaussian mixture model and hidden Markov model (GMM-HMM). As a combination of both, ETEH is an attention-based E2E ASR architecture which utilizes the frame-wise time alignment (FTA) generated by GMM-HMM ASR models. This FTA is used to better the model in two ways: first, it helps the monotonic attentions of ETEH models to capture more accurate word time stamps, thus resulting in lower latency for online decoding; second, it helps ETEH models to provide accurate and reliable KWS results. Furthermore, we are able to combine both offline and online modes in one ETEH model and establish a concise system by adopt the universal training strategy. ETEH is functional and unique, and to the best of our knowledge, we can hardly find a comparable single attention-based E2E ASR system as the baseline. To evaluate ASR accuracy and latency for ETEH, we use our previously proposed monotonic truncated attention (MTA) based online CTC/attention (OCA) ASR models as baselines. Experimental results show that ETEH ASR models gain significant improvement in ASR latency compared to the baseline. To evaluate KWS performance, we compare ETEH models with CTC-based KWS models. Results demonstrate that our ETEH models achieve significantly better KWS performance compared to the CTC baselines.",
        "link": "https://dl.acm.org/doi/10.1109/TASLP.2022.3161159",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "NASGuard: a novel accelerator architecture for robust neural architecture search (NAS) networks",
        "authors": "['Xingbin Wang', 'Boyan Zhao', 'Rui Hou', 'Amro Awad', 'Zhihong Tian', 'Dan Meng']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "Due to the wide deployment of deep learning applications in safety-critical systems, robust and secure execution of deep learning workloads is imperative. Adversarial examples, where the inputs are carefully designed to mislead the machine learning model is among the most challenging attacks to detect and defeat. The most dominant approach for defending against adversarial examples is to systematically create a network architecture that is sufficiently robust. Neural Architecture Search (NAS) has been heavily used as the de facto approach to design robust neural network models, by using the accuracy of detecting adversarial examples as a key metric of the neural network's robustness. While NAS has been proven effective in improving the robustness (and accuracy in general), the NAS-generated network models run noticeably slower on typical DNN accelerators than the hand-crafted networks, mainly because DNN accelerators are not optimized for robust NAS-generated models. In particular, the inherent multi-branch nature of NAS-generated networks causes unacceptable performance and energy overheads. To bridge the gap between the robustness and performance efficiency of deep learning applications, we need to rethink the design of AI accelerators to enable efficient execution of robust (auto-generated) neural networks. In this paper, we propose a novel hardware architecture, NASGuard, which enables efficient inference of robust NAS networks. NASGuard leverages a heuristic multi-branch mapping model to improve the efficiency of the underlying computing resources. Moreover, NASGuard addresses the load imbalance problem between the computation and memory-access tasks from multi-branch parallel computing. Finally, we propose a topology-aware performance prediction model for data prefetching, to fully exploit the temporal and spatial localities of robust NAS-generated architectures. We have implemented NASGuard with Verilog RTL. The evaluation results show that NASGuard achieves an average speedup of 1.74X over the baseline DNN accelerator.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00066",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Towards low-cost high-accuracy stochastic computing architecture for univariate functions: design and design space exploration",
        "authors": "['Kuncai Zhong', 'Zexi Li', 'Weikang Qian']",
        "date": "March 2022",
        "source": "DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe",
        "abstract": "Univariate functions are widely used. Several recent works propose to implement them by an unconventional computing paradigm, stochastic computing (SC). However, existing SC designs either have a high hardware cost due to the area-consuming randomizer or a low accuracy. In this work, we propose a low-cost high-accuracy SC architecture for univariate functions. It consists of only a single stochastic number generator and a minimum number of D flip-flops. We also apply three methods, random number source (RNS) negating, RNS scrambling, and input scrambling, to improve the accuracy of the architecture. To efficiently configure the architecture to achieve a high accuracy, we further propose a design space exploration algorithm. The experimental results show that compared to the conventional architecture, the area of the proposed architecture is reduced by up to 76%, while its accuracy is close to or sometimes even higher than that of the conventional architecture.",
        "link": "https://dl.acm.org/doi/10.5555/3539845.3539930",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Weaving a Faster Tor: A Multi-Threaded Relay Architecture for Improved Throughput",
        "authors": "['Steven Engler', 'Ian Goldberg']",
        "date": "August 2021",
        "source": "ARES '21: Proceedings of the 16th International Conference on Availability, Reliability and Security",
        "abstract": "The Tor anonymity network has millions of daily users and thousands of volunteer-run relays. Increasing the number of Tor users will enhance the privacy of not just new users, but also existing users by increasing their anonymity sets. However, growing the network further has several research and deployment challenges. One such challenge is supporting the increase in bandwidth required by additional users joining the network. While adding more Tor relays to the network would increase the total available bandwidth, it requires network architecture changes to reduce the impact of Tor’s growing directory documents. In order to increase the total available network bandwidth without needing to grow Tor’s directory documents, this work provides a multi-threaded relay architecture designed to improve the throughput of individual multi-core relays with available network capacity. We built an implementation of a subset of this new design on top of the standard Tor code base to demonstrate the potential throughput improvements of this architecture on both high- and low-performance hardware.",
        "link": "https://dl.acm.org/doi/10.1145/3465481.3465745",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Multi-Property Molecular Optimization using an Integrated Poly-Cycle Architecture",
        "authors": "['Guy Barshatski', 'Galia Nordon', 'Kira Radinsky']",
        "date": "October 2021",
        "source": "CIKM '21: Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management",
        "abstract": "Molecular lead optimization is an important task of drug discovery focusing on generating molecules similar to a drug candidate but with enhanced properties. Most prior works focused on optimizing a single property. However, in real settings, we wish to find molecules that satisfy multiple constraints, e.g., potency and safety. Simultaneously optimizing these constraints was shown to be difficult, mostly due to the lack of training examples satisfying all constraints. In this work, we present a novel approach for multi-property optimization. Unlike prior approaches, that require a large training set of pairs of a lead molecule and an enhanced molecule, our approach is unpaired. Our architecture learns a transformation for each property optimization separately, while constraining the latent embedding space between all transformations. This allows generating a molecule which optimizes multiple properties simultaneously. We present a novel adaptive loss which balances the separate transformations and stabilizes the optimization process. We evaluate our method on optimizing for two properties: dopamine receptor (DRD2) and drug likeness (QED), and show our method outperforms previous state-of-the-art, especially when training examples satisfying all constraints are sparse.",
        "link": "https://dl.acm.org/doi/10.1145/3459637.3481938",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A Reference Architecture for Validating Security Across Multi-Cloud Computing Systems",
        "authors": "['Henry Edet']",
        "date": "June 2021",
        "source": "EASE '21: Proceedings of the 25th International Conference on Evaluation and Assessment in Software Engineering",
        "abstract": "Correlative studies carried out by different experts have cited a number of impediments to the growth of cloud computing technology; at the top of the list is security and data privacy issues. More so, a systematic mapping study which was conducted at the start of this research, revealed that the most prevalent cloud security issues are a consequence of poor architecture. 73 percent of literature surveyed also revealed that frameworks and reference architectures are one of the most effective ways of preventing security and data privacy breaches within a cloud computing environment as these issues are addressed during the requirements phase, prior to deployment. [1], [14] This research seeks to explore a preventative approach to cloud security breaches through the use of reference architectures. Firstly, we investigate the main causes of cloud security breaches and then, we analyse existing reference architectures with the aim of designing a universal security framework usable across multi-cloud computing platforms.",
        "link": "https://dl.acm.org/doi/10.1145/3463274.3463345",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Ideological and political mobile learning system of computer professional courses based on J2EE architecture",
        "authors": "['Jing Luo', 'Mengyao Peng']",
        "date": "September 2021",
        "source": "ICISCAE 2021: 2021 4th International Conference on Information Systems and Computer Aided Education",
        "abstract": "The core of the system is to feed back all kinds of search data according to the needs of users. However, the demand of Ideological and political learning obtained by the existing system is not clear enough, which makes the feedback logic of the system confused, which leads to the continuous decline of the stability of the system. In view of this problem, a mobile learning system of Ideological and political learning for computer specialty courses based on J2EE architecture is designed. In hardware design, the control chip is re selected to design carrier communication circuit. In software design, the demand for ideological and political learning is clarified, and the feedback logic of Ideological and political content of computer major course is established based on J2EE architecture. Through setting up data exchange function, all knowledge contents of Ideological and political course can be learned. The experimental results show that the hardware and software of the design system have high compatibility, and the feedback time and the number of crashes meet the design requirements. Thus, J2EE architecture can enhance the stability of mobile learning system.",
        "link": "https://dl.acm.org/doi/10.1145/3482632.3482665",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Architecture-Based Semantic Description Framework for Model Transformation",
        "authors": "['Jinkui Hou', 'Cong Xu', 'Yuyan Zhang']",
        "date": "December 2021",
        "source": "NLPIR '21: Proceedings of the 2021 5th International Conference on Natural Language Processing and Information Retrieval",
        "abstract": "In order to solve the problems of description and verification of semantic properties in model driven development, process algebra is introduced on the basis of extending typed category theory. A unified semantic description framework is established for the description and transformation of component-based software models, as well as the maintenance and verification of semantic properties in the process of model transformation. Category diagram is used to describe the semantics of architecture model, and typed morphism implies the dependency relationship between component objects, and typed functor is used to describe the mapping mechanism before and after model transformation. Application research shows that the framework well follows the essence and process requirements of model-driven development, and provides a new guidance framework for understanding, cognitive learning and promotion of software development research on the basis of model transformation.",
        "link": "https://dl.acm.org/doi/10.1145/3508230.3508241",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Acoustic Source Localization in the Circular Harmonic Domain Using Deep Learning Architecture",
        "authors": "['Kunkun SongGong', 'Wenwu Wang', 'Huawei Chen']",
        "date": "None",
        "source": "IEEE/ACM Transactions on Audio, Speech and Language Processing",
        "abstract": "The problem of direction of arrival (DOA) estimation with a circular microphone array has been addressed with classical source localization methods, such as the model-based methods and the param etric methods. These methods have an advantage in estimating the DOAs in a blind manner, i.e. with no (or limited) prior knowledge about the sound sources. However, their performance tends to degrade rapidly in noisy and reverberant environments or in the presence of sensor array limitations, such as sensor gain and phase errors. In this paper, we present a new approach by leveraging the strength of a convolutional neural network (CNN)-based deep learning approach. In particular, we design new circular harmonic features that are frequency-invariant as inputs to the CNN architecture, so as to offer improvements in DOA estimation in unseen adverse environments and obtain good adaptation to array imperfections. To our knowledge, such a deep learning approach has not been used in the circular harmonic domain. Experiments performed on both simulated and real-data show that our method gives significantly better performance, than the recent baseline methods, in a variety of noise and reverberation levels, in terms of the accuracy of the DOA estimation.",
        "link": "https://dl.acm.org/doi/10.1109/TASLP.2022.3190723",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Video Analytics Architecture with Metadata Event-Engine for Urban Safe Cities",
        "authors": "['David Eneko Ruiz de Gauna', 'Eider Irigoyen', 'Inaki Cejudo', 'Harbil Arregui', 'Peter Leskovsky', 'Oihana Otaegui']",
        "date": "July 2021",
        "source": "ICCTA '21: Proceedings of the 2021 7th International Conference on Computer Technology Applications",
        "abstract": "Intelligent video analysis from sources such as urban surveillance cameras is a prolific research area today. Multiple types of computer architectures offer a wide range of possibilities when addressing the needs of computer vision technologies. When it comes to real time processing for high level and complex event detections, however, some limitations may arise, such as the computing power in the edge or the cost of sending real time video to the cloud for running advanced algorithms. In this paper, we present a functional architecture of a complete video surveillance solution and we focus on the metadata-processing event engine which takes care of the high-level video processing that is decoupled from a low-level video analysis. The low-level video analysis running in the edge generates and publishes a flow of JSON messages structure containing the details of bounding boxes detected in each frame into an asynchronous messaging service. The metadata event engine is running in a remote cloud, far from the camera locations. We present the performance evaluation of this event engine under different circumstances simulating data coming simultaneously from multiple cameras, in order to study the best strategies when deploying and partitioning distributed processing tasks.",
        "link": "https://dl.acm.org/doi/10.1145/3477911.3477919",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Isadora: Automated Information Flow Property Generation for Hardware Designs",
        "authors": "['Calvin Deutschbein', 'Andres Meza', 'Francesco Restuccia', 'Ryan Kastner', 'Cynthia Sturton']",
        "date": "November 2021",
        "source": "ASHES '21: Proceedings of the 5th Workshop on Attacks and Solutions in Hardware Security",
        "abstract": "Isadora is a methodology for creating information flow specifications of hardware designs. The methodology combines information flow tracking and specification mining to produce a set of information flow properties that are suitable for use during the security validation process, and which support a better understanding of the security posture of the design. Isadora is fully automated; the user provides only the design under consideration and a testbench and need not supply a threat model nor security specifications. We evaluate Isadora on a RISC-V processor plus two designs related to SoC access control. Isadora generates security properties that align with those suggested by the Common Weakness Enumerations (CWEs), and in the case of the SoC designs, align with the properties written manually by security experts.",
        "link": "https://dl.acm.org/doi/10.1145/3474376.3487286",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Automatic mapping and code optimization for OpenCL kernels on FT-matrix architecture (WIP paper)",
        "authors": "['Xiaolei Zhao', 'Mei Wen', 'Zhaoyun Chen', 'Yang Shi', 'Chunyuan Zhang']",
        "date": "June 2021",
        "source": "LCTES 2021: Proceedings of the 22nd ACM SIGPLAN/SIGBED International Conference on Languages, Compilers, and Tools for Embedded Systems",
        "abstract": "FT-Matrix is a typical vector-SIMD architecture that refines the cooperation between scalar and vector units. This approach is widely used in digital signal processing, high-performance computing, and artificial intelligence, among other fields. FT-Matrix currently adopts C vector extension as the main programming model, improving the utilization efficiency of SIMD by providing explicit vector extension API. Moreover, it is difficult to efficiently transplant parallel programs (OpenCL, CUDA) adopted by users. This paper proposes an automatic mapping and code optimization method for OpenCL kernels on FT-Matrix architecture. The proposed approach solves these challenges by means of work item coalescing, slicing and rotation, and instruction-level code optimization. Preliminary results show that our method can achieve high performance and good hardware utilization for OpenCL kernels, as well as decreasing the programming difficulty on FT-Matrix.",
        "link": "https://dl.acm.org/doi/10.1145/3461648.3463845",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "HyperParser: A High-Performance Parser Architecture for Next Generation Programmable Switch and SmartNIC",
        "authors": "['Huan Liu', 'Zhiliang Qiu', 'Weitao Pan', 'Jiajun Li', 'Jinjian Huang']",
        "date": "June 2021",
        "source": "APNet '21: Proceedings of the 5th Asia-Pacific Workshop on Networking",
        "abstract": "Programmable switches and SmartNICs motivate the programmable network. ASIC is adopted in programmable switches to achieve high throughput, and FPGA-based SmartNIC is becoming increasingly popular. The programmable parser is a key element in programmable switches and SmartNICs, which can identify the protocol types and extract the relevant fields. The programmable parser for the next generation programmable switches and SmartNICs requires a significant improvement in PPAL (performance, power, area, and latency), which is quite challenging. According to the Ethernet roadmap, 800 Gbps and 1.6 Tbps are expected to be the future switch interface speeds after 2022, which leads to higher throughput of the parser. Meanwhile, the end of Dennard scaling and the slowdown of Moore’s Law result in limited power and area. Besides, the need for low-latency and low-jitter operations at the datacenter scale continues to grow.  Aforementioned requirements on PPAL inspire us to propose HyperParser, a high-performance parser architecture for next generation programmable switches and FPGA-based SmartNICs. The key innovation of HyperParser is the adoption of the butterfly network, which is widely used in cryptographic circuits. HyperParser supports ASIC and FPGA implementations, with low and deterministic latency. The PPAL of the ASIC implementation are 3.2-6.8 Tbps, 0.55 W, 2M gates, and 11.7 ns, and the PPAL of the FPGA implementation are 1.3-2.8 Tbps, 16.2 W, 43K LUTs, and 40 ns. The source code of HyperParser has been released on Github.",
        "link": "https://dl.acm.org/doi/10.1145/3469393.3469399",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Formal Modelling and Automated Trade-off Analysis of Enforcement Architectures for Cryptographic Access Control in the Cloud",
        "authors": "['Stefano Berlato', 'Roberto Carbone', 'Adam J. Lee', 'Silvio Ranise']",
        "date": "None",
        "source": "ACM Transactions on Privacy and Security",
        "abstract": "To facilitate the adoption of cloud by organizations, Cryptographic Access Control (CAC) is the obvious solution to control data sharing among users while preventing partially trusted Cloud Service Providers (CSP) from accessing sensitive data. Indeed, several CAC schemes have been proposed in the literature. Despite their differences, available solutions are based on a common set of entities—e.g., a data storage service or a proxy mediating the access of users to encrypted data—that operate in different (security) domains—e.g., on-premise or the CSP. However, the majority of these CAC schemes assumes a fixed assignment of entities to domains; this has security and usability implications that are not made explicit and can make inappropriate the use of a CAC scheme in certain scenarios with specific trust assumptions and requirements. For instance, assuming that the proxy runs at the premises of the organization avoids the vendor lock-in effect but may give rise to other security concerns (e.g., malicious insiders attackers).To the best of our knowledge, no previous work considers how to select the best possible architecture (i.e., the assignment of entities to domains) to deploy a CAC scheme for the trust assumptions and requirements of a given scenario. In this article, we propose a methodology to assist administrators in exploring different architectures for the enforcement of CAC schemes in a given scenario. We do this by identifying the possible architectures underlying the CAC schemes available in the literature and formalizing them in simple set theory. This allows us to reduce the problem of selecting the most suitable architectures satisfying a heterogeneous set of trust assumptions and requirements arising from the considered scenario to a decidable Multi-objective Combinatorial Optimization Problem (MOCOP) for which state-of-the-art solvers can be invoked. Finally, we show how we use the capability of solving the MOCOP to build a prototype tool assisting administrators to preliminarily perform a “What-if” analysis to explore the trade-offs among the various architectures and then use available standards and tools (such as TOSCA and Cloudify) for automated deployment in multiple CSPs.",
        "link": "https://dl.acm.org/doi/10.1145/3474056",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "StarFL: Hybrid Federated Learning Architecture for Smart Urban Computing",
        "authors": "['Anbu Huang', 'Yang Liu', 'Tianjian Chen', 'Yongkai Zhou', 'Quan Sun', 'Hongfeng Chai', 'Qiang Yang']",
        "date": "None",
        "source": "ACM Transactions on Intelligent Systems and Technology",
        "abstract": "From facial recognition to autonomous driving, Artificial Intelligence (AI) will transform the way we live and work over the next couple of decades. Existing AI approaches for urban computing suffer from various challenges, including dealing with synchronization and processing of vast amount of data generated from the edge devices, as well as the privacy and security of individual users, including their bio-metrics, locations, and itineraries. Traditional centralized-based approaches require data in each organization be uploaded to the central database, which may be prohibited by data protection acts, such as GDPR and CCPA. To decouple model training from the need to store the data in the cloud, a new training paradigm called Federated Learning (FL) is proposed. FL enables multiple devices to collaboratively learn a shared model while keeping the training data on devices locally, which can significantly mitigate privacy leakage risk. However, under urban computing scenarios, data are often communication-heavy, high-frequent, and asynchronized, posing new challenges to FL implementation. To handle these challenges, we propose a new hybrid federated learning architecture called StarFL. By combining with Trusted Execution Environment (TEE), Secure Multi-Party Computation (MPC), and (Beidou) satellites, StarFL enables safe key distribution, encryption, and decryption, and provides a verification mechanism for each participant to ensure the security of the local data. In addition, StarFL can provide accurate timestamp matching to facilitate synchronization of multiple clients. All these improvements make StarFL more applicable to the security-sensitive scenarios for the next generation of urban computing.",
        "link": "https://dl.acm.org/doi/10.1145/3467956",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Research on Distributed Edge Computing Architecture Scheme for Cable Status Awareness",
        "authors": "['Zehua Pan', 'Zhigang Ren', 'Wei Guo', 'Yekun Men', 'Zhao Yu', 'Ping Chen', 'Ming Ren', 'Ran Duan']",
        "date": "May 2021",
        "source": "ICAIIS 2021: 2021 2nd International Conference on Artificial Intelligence and Information Systems",
        "abstract": "Power cables are important equipment in the power system. Comprehensive condition monitoring of the cables is the key to maintaining the safety and stability of the power system. Through the partial discharge (PD) multi-state perception method, various faults of the cable insulation can be detected in time, and mutual verification and supplementation can be achieved. The distributed sensing architecture can realize the comprehensive perception of the cable status while exerting the edge computing capabilities of the sensors, reducing the pressure of data transmission and processing. This paper proposes a distributed edge computing architecture scheme for cable state awareness, and on this basis, studies the architecture operation and data transmission methods.",
        "link": "https://dl.acm.org/doi/10.1145/3469213.3470663",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "The Demikernel Datapath OS Architecture for Microsecond-scale Datacenter Systems",
        "authors": "['Irene Zhang', 'Amanda Raybuck', 'Pratyush Patel', 'Kirk Olynyk', 'Jacob Nelson', 'Omar S. Navarro Leija', 'Ashlie Martinez', 'Jing Liu', 'Anna Kornfeld Simpson', 'Sujay Jayakar', 'Pedro Henrique Penna', 'Max Demoulin', 'Piali Choudhury', 'Anirudh Badam']",
        "date": "October 2021",
        "source": "SOSP '21: Proceedings of the ACM SIGOPS 28th Symposium on Operating Systems Principles",
        "abstract": "Datacenter systems and I/O devices now run at single-digit microsecond latencies, requiring ns-scale operating systems. Traditional kernel-based operating systems impose an unaffordable overhead, so recent kernel-bypass OSes [73] and libraries [23] eliminate the OS kernel from the I/O datapath. However, none of these systems offer a general-purpose datapath OS replacement that meet the needs of μs-scale systems.' AB@This paper proposes Demikernel, a flexible datapath OS and architecture designed for heterogenous kernel-bypass devices and μs-scale datacenter systems. We build two prototype Demikernel OSes and show that minimal effort is needed to port existing μs-scale systems. Once ported, Demikernel lets applications run across heterogenous kernel-bypass devices with ns-scale overheads and no code changes.",
        "link": "https://dl.acm.org/doi/10.1145/3477132.3483569",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Design and Research of Web Crawler Based on Distributed Architecture",
        "authors": "['Lili Wang', 'Haoliang Wang']",
        "date": "October 2021",
        "source": "AIAM2021: 2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture",
        "abstract": "Internet data is abundance in content and diverse in organization. In order to automatically complete the process of collecting, analyzing, and storing large amounts of data and information on the web, a web crawler technology based on Hadoop distributed clusters is proposed. Using Nutch crawler framework, Hadoop distributed technology, and Zookeeper distributed coordination service framework, through the construction of distributed clusters, and high-performance Key-Value database Redis to store data, it verifies the feasibility of distributed crawlers. Through the analysis of data collection experiments, the comparison of multiple sets of experimental data between the distributed crawler based on Nutch and the traditional crawler shows that the crawler design of the distributed architecture is superior to the traditional crawler in terms of collection speed and efficiency.",
        "link": "https://dl.acm.org/doi/10.1145/3495018.3495061",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Discovering enterprise architecture developing a course with enterprise application software tools",
        "authors": "['Maria Weber', 'Kyle Chapman', 'John Buerck']",
        "date": "None",
        "source": "Journal of Computing Sciences in Colleges",
        "abstract": "Information Technology (IT) and Digital Transformation (Dx) are shifting how companies function and embrace emerging technologies. Companies are reimagining how work can be done by adopting new technologies to existing or new infrastructure. Organizations need skilled IT professionals who can drive this transformation, leading to the establishment of a robust enterprise. Industry trends indicate a need to shift academic curricula in Information System (IS) programs. Enterprise Architecture (EA) is a course in a graduate IS program at Saint Louis University. The EA curriculum focuses on the alignment between IT and business. The course covers designing, planning, and implementing organizational changes to corporate architecture through standardization of principles, models, and procedures. The use of EA open-source tools in this course is innovative and ideal for building students' marketable skills and improving their university experience. This paper provides an overview of designing and delivering a gateway course in Enterprise Architecture and Infrastructure Systems using enterprise-class and open-source software tools as part of the Master of Information Systems.",
        "link": "https://dl.acm.org/doi/10.5555/3469567.3469569",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "IOT Data Storage Solution Based on Hybrid Blockchain Edge Architecture",
        "authors": "['Li Zhou', 'Jianhua Liu']",
        "date": "September 2021",
        "source": "AIPR '21: Proceedings of the 2021 4th International Conference on Artificial Intelligence and Pattern Recognition",
        "abstract": "In order to solve the problem of inefficient data storage and high latency in the case of a large number of IoT devices networked, for this purpose, this paper implements a data storage scheme for IoT devices based on blockchain and edge computing architecture, using an interstellar file system network formed by edge computing servers as nodes to achieve distributed localized storage of data and storing data hash addresses into the blockchain. Combined with smart contracts and an attribute-based access control framework, secure dynamic fine-grained access control of data and behavior tracking of visitors are achieved. Experiments show that the proposed scheme can effectively reduce data transmission latency, as well as lower latency request response and higher throughput, which also makes it more compatible with the current multi-user and multi-request access requirements in the IoT environment.",
        "link": "https://dl.acm.org/doi/10.1145/3488933.3489012",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A low-cost AR assistant component architecture for Warehouse Management Systems",
        "authors": "['Iakovos Stratigakis', 'Theodoros Amanatidis', 'Christina Volioti', 'George Kakarontzas', 'Thrasyvoulos Tsiatsos', 'Ioannis Stamelos', 'Charalampos Avratoglou', 'Apostolos Ampatzoglou', 'Alexander Chatzigeorgiou', 'Dimitris Folinas']",
        "date": "November 2021",
        "source": "PCI '21: Proceedings of the 25th Pan-Hellenic Conference on Informatics",
        "abstract": "This work describes a research collaboration between universities and industry with the aim to provide a low-cost prototype based on Augmented Reality technologies, that assists with maintaining correct information in Warehouse Management Systems. The component interacts with the central server of an existing commercial WMS to provide up-to-date information on the actual state of the warehouse. The low-cost requirement restricts the solution to smartphones and other inexpensive equipment readily available, such as drones, as well as mostly Open Source Software. This requirement also introduces several interesting architectural issues that we discuss in this work. A prototype was built for the proposed architecture and several tests were carried out.",
        "link": "https://dl.acm.org/doi/10.1145/3503823.3503854",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Large graph convolutional network training with GPU-oriented data communication architecture",
        "authors": "['Seung Won Min', 'Kun Wu', 'Sitao Huang', 'Mert Hidayetoğlu', 'Jinjun Xiong', 'Eiman Ebrahimi', 'Deming Chen', 'Wen-mei Hwu']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Graph Convolutional Networks (GCNs) are increasingly adopted in large-scale graph-based recommender systems. Training GCN requires the minibatch generator traversing graphs and sampling the sparsely located neighboring nodes to obtain their features. Since real-world graphs often exceed the capacity of GPU memory, current GCN training systems keep the feature table in host memory and rely on the CPU to collect sparse features before sending them to the GPUs. This approach, however, puts tremendous pressure on host memory bandwidth and the CPU. This is because the CPU needs to (1) read sparse features from memory, (2) write features into memory as a dense format, and (3) transfer the features from memory to the GPUs.In this work, we propose a novel GPU-oriented data communication approach for GCN training, where GPU threads directly access sparse features in host memory through zero-copy accesses without much CPU help. By removing the CPU gathering stage, our method significantly reduces the consumption of the host resources and data access latency. We further present two important techniques to achieve high host memory access efficiency by the GPU: (1) automatic data access address alignment to maximize PCIe packet efficiency, and (2) asynchronous zero-copy access and kernel execution to fully overlap data transfer with training. We incorporate our method into PyTorch and evaluate its effectiveness using several graphs with sizes up to 111 million nodes and 1.6 billion edges. In a multi-GPU training setup, our method is 65--92% faster than the conventional data transfer method, and can even match the performance of all-in-GPU-memory training for some graphs that fit in GPU memory.",
        "link": "https://dl.acm.org/doi/10.14778/3476249.3476264",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "SecTULab: A Moodle-Integrated Secure Remote Access Architecture for Cyber Security Laboratories",
        "authors": "['Joachim Fabini', 'Alexander Hartl', 'Fares Meghdouri', 'Claudia Breitenfellner', 'Tanja Zseby']",
        "date": "August 2021",
        "source": "ARES '21: Proceedings of the 16th International Conference on Availability, Reliability and Security",
        "abstract": "The Covid-19 crisis has challenged cyber security teaching by creating the need for secure remote access to existing cyber security laboratory infrastructure. In this paper, we present requirements, architecture and key functionalities of a secure remote laboratory access solution that has been instantiated successfully for two existing laboratories at TU Wien. The proposed design prioritizes security and privacy aspects while integrating with existing Moodle eLearning platforms to leverage available authentication and group collaboration features. Performance evaluations of the prototype implementation for real cyber security classes support a first estimate of dimensioning and resources that must be provisioned when implementing the proposed secure remote laboratory access.",
        "link": "https://dl.acm.org/doi/10.1145/3465481.3470034",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Design of inpatient medical insurance management system based on three-tier architecture mode",
        "authors": "['Lianying Ge']",
        "date": "September 2021",
        "source": "ICISCAE 2021: 2021 4th International Conference on Information Systems and Computer Aided Education",
        "abstract": "The informatization of medical insurance (medical insurance) plays a very important role in the reform of medical insurance. The field of medical insurance has its particularity. The regional difference and variability of medical insurance policy make the medical insurance system have strong scalability and maintainability. In the past, most medical insurance systems used C / S architecture. In this architecture, the client integrated user interface and business logic, and the business logic is the most easily changed. Three-tier architecture is a further extension of C/S architecture, which adopts layered idea and component technology. The three-tier architecture is logically divided into user layer, business layer and data layer. The user layer is responsible for interacting with users, the business layer is responsible for business logic, and the data layer is responsible for information access. This paper introduces the standard requirements of medical insurance management system, and adopts the three-tier structure model for system statistics.",
        "link": "https://dl.acm.org/doi/10.1145/3482632.3487551",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Failure sentinels: ubiquitous just-in-time intermittent computation via low-cost hardware support for voltage monitoring",
        "authors": "['Harrison Williams', 'Michael Moukarzel', 'Matthew Hicks']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "Energy harvesting systems support the deployment of low-power microcontrollers untethered by constant power sources or batteries, enabling long-lived deployments in a variety of applications previously limited by power or size constraints. However, the limitations of harvested energy mean that even the lowest-power microcontrollers operate intermittently---waiting for the harvester to slowly charge a buffer capacitor and rapidly discharging the capacitor to support a brief burst of computation. The challenges of the intermittent operation brought on by harvested energy drive a variety of hardware and software techniques that first enabled long-running computation, then focused on improving performance. Many of the most promising systems demand dynamic updates of available energy to inform checkpointing and mode decisions. Unfortunately, existing energy monitoring solutions based on analog circuits (e.g., analog-to-digital converters) are ill-matched for the task because their signal processing focus sacrifices power efficiency for increased performance---performance not required by current or future intermittent computation systems. This results in existing solutions consuming as much energy as the microcontroller, stealing energy from useful computation. To create a low-power energy monitoring solution that provides just enough performance for intermittent computation use cases, we design and implement Failure Sentinels, an on-chip, fully-digital energy monitor. Failure Sentinels leverages the predictable propagation delay response of digital logic gates to supply voltage fluctuations to measure available energy. Our design space exploration shows that Failure Sentinels provides 30--50mV of resolution at sample rates up to 10kHz, while consuming less than 2μA of current. Experiments show that Failure Sentinels increases the energy available for software computation by up to 77%, compared to current solutions. We also implement a RISC-V-based FPGA prototype that validates our design space exploration and shows the overheads of incorporating Failure Sentinels into a system-on-chip.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00058",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Design and Implementation of Smart Archives Information Service Architecture",
        "authors": "['Ying Luo']",
        "date": "September 2021",
        "source": "ICISCAE 2021: 2021 4th International Conference on Information Systems and Computer Aided Education",
        "abstract": "None",
        "link": "https://dl.acm.org/doi/10.1145/3482632.3484063",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "CGRA-EAM—Rapid Energy and Area Estimation for Coarse-grained Reconfigurable Architectures",
        "authors": "['Mark Wijtvliet', 'Henk Corporaal', 'Akash Kumar']",
        "date": "None",
        "source": "ACM Transactions on Reconfigurable Technology and Systems",
        "abstract": "Reconfigurable architectures are quickly gaining in popularity due to their flexibility and ability to provide high energy efficiency. However, reconfigurable systems allow for a huge design space. Iterative design space exploration (DSE) is often required to achieve good Pareto points with respect to some combination of performance, area, and/or energy. DSE tools depend on information about hardware characteristics in these aspects. These characteristics can be obtained from hardware synthesis and net-list simulation, but this is very time-consuming. Therefore, architecture models are common. This work introduces CGRA-EAM (Coarse-Grained Reconfigurable Architecture - Energy & Area Model), a model for energy and area estimation framework for coarse-grained reconfigurable architectures. The model is evaluated for the Blocks CGRA. The results demonstrate that the mean absolute percentage error is 15.5% and 2.1% for energy and area, respectively, while the model achieves a speedup of close to three orders of magnitude compared to synthesis.",
        "link": "https://dl.acm.org/doi/10.1145/3468874",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Exploiting architecture advances for sparse solvers in circuit simulation",
        "authors": "['Zhiyuan Yan', 'Biwei Xie', 'Xingquan Li', 'Yungang Bao']",
        "date": "March 2022",
        "source": "DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe",
        "abstract": "Sparse direct solvers provide vital functionality for a wide variety of scientific applications. The dominated part of the sparse direct solver, LU factorization, suffers a lot from the irregularity of sparse matrices. Meanwhile, the specific characteristics of sparse solvers in circuit simulation and unique sparse pattern of circuit matrices provide more design spaces and also great challenges. In this paper, we propose a sparse solver named FLU and re-examine the performance of LU factorization from the perspectives of vectorization, parallelization, and data locality. To improve vectorization efficiency and data locality, FLU introduces a register-level supernode computation method by delicately manipulating data movement. With alternating multiple columns computation, FLU further reduces the off-chip memory accesses greatly. Furthermore, we implement a fine-grained elimination tree based parallelization scheme to fully exploit task-level parallelism. Compared with PARDISO and NICSLU, experimental results show that FLU achieves a speedup up to 19.51× (3.86× on average) and 2.56× (1.66× on average) on Intel Xeon respectively.",
        "link": "https://dl.acm.org/doi/10.5555/3539845.3540043",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Towards a Blockchain-SDN Architecture for Secure and Trustworthy 5G Massive IoT Networks",
        "authors": "['Akram Hakiri', 'Behnam Dezfouli']",
        "date": "April 2021",
        "source": "SDN-NFV Sec'21: Proceedings of the 2021 ACM International Workshop on Software Defined Networks &amp; Network Function Virtualization Security",
        "abstract": "The emerging 5G mobile network is a prominent technology for addressing networking related challenges of Internet of Things (IoT). The forthcoming 5G is expected to allow low-power massive IoT devices to produce high volumes of data that can be transmitted over ultra-reliable, low-latency wireless communication services. However, IoT systems encounter several security and privacy issues to prevent unauthorized access to IoT nodes. To address these challenges, this paper introduces a novel blockchain-based architecture that leverages Software Defined Network (SDN) and Network Function Virtualization (NFV) for securing IoT transactions. A novel security appliance is introduced in a form of Virtualized Network Functions (VNFs) for improving the scalability and performance of IoT networks. Then, we introduce a novel consensus algorithm to detect and report suspected IoT nodes and mitigate malicious traffic. We evaluate and compare our proposed solution against three well-known consensus algorithms, i.e., Proof of Work (PoW), Proof of Elapsed Time (PoET), and Proof of Stake (PoS). We demonstrate that the proposed solution provides substantially lower latency and higher throughput as well as trustworthy IoT communication.",
        "link": "https://dl.acm.org/doi/10.1145/3445968.3452090",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "SMART: A Heterogeneous Scratchpad Memory Architecture for Superconductor SFQ-based Systolic CNN Accelerators",
        "authors": "['Farzaneh Zokaee', 'Lei Jiang']",
        "date": "October 2021",
        "source": "MICRO '21: MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture",
        "abstract": "Ultra-fast & low-power superconductor single-flux-quantum (SFQ)-based CNN systolic accelerators are built to enhance the CNN inference throughput. However, shift-register (SHIFT)-based scratchpad memory (SPM) arrays prevent a SFQ CNN accelerator from exceeding 40% of its peak throughput, due to the lack of random access capability. This paper first documents our study of a variety of cryogenic memory technologies, including Vortex Transition Memory (VTM), Josephson-CMOS SRAM, MRAM, and Superconducting Nanowire Memory, during which we found that none of the aforementioned technologies made a SFQ CNN accelerator achieve high throughput, small area, and low power simultaneously. Second, we present a heterogeneous SPM architecture, SMART, composed of SHIFT arrays and a random access array to improve the inference throughput of a SFQ CNN systolic accelerator. Third, we propose a fast, low-power and dense pipelined random access CMOS-SFQ array by building SFQ passive-transmission-line-based H-Trees that connect CMOS sub-banks. Finally, we create an ILP-based compiler to deploy CNN models on SMART. Experimental results show that, with the same chip area overhead, compared to the latest SHIFT-based SFQ CNN accelerator, SMART improves the inference throughput by 3.9 × (2.2 ×), and reduces the inference energy by 86% (71%) when inferring a single image (a batch of images).",
        "link": "https://dl.acm.org/doi/10.1145/3466752.3480041",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "SIEM Architecture for the Internet of Things and Smart City",
        "authors": "['Abdalrahman Hwoij', \"As'har Khamaiseh\", 'Mohammad Ababneh']",
        "date": "April 2021",
        "source": "DATA'21: International Conference on Data Science, E-learning and Information Systems 2021",
        "abstract": "The Internet of things (IoT) is a new technology that shapes the future of a world that is rapidly being invaded by smart devices connected to the Internet. Such technology has a great role in developing the idea of a smart city. A smart city is a city that takes advantage of existing infrastructure and integrates it with the Internet of things technology to improve the quality of life. Internet of Things (IoT) sensors are distributed geographically around the city to collect data from the environment (i.e.: streets, cars, traffic lights...etc.), process, and manage it to provide intelligent actionable information to citizens. All data transferred through networks of a smart city may be threatened and susceptible to illegal actions such as violation, stealing, and inappropriate use. These security threats affect the privacy and security of users; where hackers can get access to user's data and gain control of their smart homes, cars, medical devices and might even gain control over city traffic lights. All the above enforce the need to have a security system that continuously monitors and tracks all data logs to detect any suspicious activity. In this paper, we propose a Security Information and Event Management (SIEM) approach for smart cities by forwarding event logs generated by smart devices to a security operation center that works around the clock to detect security incidents and handle them. Such an approach aims to create a safe smart living environment.",
        "link": "https://dl.acm.org/doi/10.1145/3460620.3460747",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Analysis and Optimization Discussion on Control System Architecture of Electrochemical energy storage Power Station",
        "authors": "['Quan Hong', 'Jinbo Wu', 'Li Li', 'Shangfeng Xiong', 'Yusheng Gong', 'Zhihao Liu']",
        "date": "October 2021",
        "source": "ICITEE '21: Proceedings of the 4th International Conference on Information Technologies and Electrical Engineering",
        "abstract": "With the continuous expansion of the scale of electrochemical energy storage power station connected to the grid, the demand for its unified dispatching control to participate in multi-scenario applications such as peak shaving, frequency modulation and dynamic reactive power support is also increasing. In this paper, combined with the above requirements, the shortcomings of the current mainstream technical route are analyzed, and the improvement direction is discussed. The system improvement scheme based on 4S architecture is proposed, which can be used as a reference for the improvement of the subsequent energy storage control system.",
        "link": "https://dl.acm.org/doi/10.1145/3513142.3513164",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Research on Financial Intelligence System Architecture and Metadata Management Based on Support Vector Machine",
        "authors": "['Yifei Bai']",
        "date": "September 2021",
        "source": "ICISCAE 2021: 2021 4th International Conference on Information Systems and Computer Aided Education",
        "abstract": "As far as the present situation of financial market supervision system is concerned, there are some problems in financial statistics, such as lack of information and distortion of information, which have greatly affected the stable development of China's financial market system and caused some damage to the national economy. Aiming at the business and technical problems of bank financial risk management, this paper constructs the basic framework of financial intelligence system, and designs the corresponding data warehouse system architecture. Through the storage and analysis of financial system monitoring information, using distributed file system and parallel programming model, the management scheme of application cluster and virtual resources is proposed, and metadata is automatically extracted by using support vector machine (SVM) method. At the same time, through the difference analysis and importance analysis of metadata management, we can clearly know the importance of data dictionary and the impact of data dictionary changes.",
        "link": "https://dl.acm.org/doi/10.1145/3482632.3487532",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Research on the Overall Architecture Design of Project Management Information System Based on SOA",
        "authors": "['Kun Zhang', 'Xia Zhang']",
        "date": "September 2021",
        "source": "ICISCAE 2021: 2021 4th International Conference on Information Systems and Computer Aided Education",
        "abstract": "Engineering project management system is an indispensable part of enterprises, and it is a key issue for the survival and development of construction enterprises. In this paper, the overall structure of the system is analyzed and designed in detail on the basis of following SOA architecture and its layered thought, and the design of each level is analyzed and explained in detail. Different from traditional management information systems, parallel management information systems interconnect virtual systems with real information systems, and integrate social factors such as human behavior and management system into overall consideration. The system has the characteristics of openness and flexible reconfiguration. To ensure the effective implementation of project construction, in order to comprehensively improve the project management ability.",
        "link": "https://dl.acm.org/doi/10.1145/3482632.3483996",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Cambricon-Q: a hybrid architecture for efficient training",
        "authors": "['Yongwei Zhao', 'Chang Liu', 'Zidong Du', 'Qi Guo', 'Xing Hu', 'Yimin Zhuang', 'Zhenxing Zhang', 'Xinkai Song', 'Wei Li', 'Xishan Zhang', 'Ling Li', 'Zhiwei Xu', 'Tianshi Chen']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "Deep neural network (DNN) training is notoriously time-consuming, and quantization is promising to improve the training efficiency with reduced bandwidth/storage requirements and computation costs. However, state-of-the-art quantized algorithms with negligible training accuracy loss, which require on-the-fly statistic-based quantization over a great amount of data (e.g., neurons and weights) and high-precision weight update, cannot be effectively deployed on existing DNN accelerators. To address this problem, we propose the first customized architecture for efficient quantized training with negligible accuracy loss, which is named as Cambricon-Q. Cambricon-Q features a hybrid architecture consisting of an ASIC acceleration core and a near-data-processing (NDP) engine. The acceleration core mainly targets at improving the efficiency of statistic-based quantization with specialized computing units for both statistical analysis (e.g., determining maximum) and data reformating, while the NDP engine avoids transferring the high-precision weights from the off-chip memory to the acceleration core. Experimental results show that on the evaluated benchmarks, Cambricon-Q improves the energy efficiency of DNN training by 6.41X and 1.62X, performance by 4.20X and 1.70X compared to GPU and TPU, respectively, with only ⩽ 0.4% accuracy degradation compared with full precision training.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00061",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Research on the Overall Architecture Design of Project Management Information System Based on SOA",
        "authors": "['Kun Zhang', 'Xia Zhang']",
        "date": "September 2021",
        "source": "ICISCAE 2021: 2021 4th International Conference on Information Systems and Computer Aided Education",
        "abstract": "Engineering project management system is an indispensable part of enterprises, and it is a key issue for the survival and development of construction enterprises. In this paper, the overall structure of the system is analyzed and designed in detail on the basis of following SOA architecture and its layered thought, and the design of each level is analyzed and explained in detail. Different from traditional management information systems, parallel management information systems interconnect virtual systems with real information systems, and integrate social factors such as human behavior and management system into overall consideration. The system has the characteristics of openness and flexible reconfiguration. To ensure the effective implementation of project construction, in order to comprehensively improve the project management ability.",
        "link": "https://dl.acm.org/doi/10.1145/3482632.3483996",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A Flow-based Multi-agent Data Exfiltration Detection Architecture for Ultra-low Latency Networks",
        "authors": "['Rafael Salema Marques', 'Gregory Epiphaniou', 'Haider Al-Khateeb', 'Carsten Maple', 'Mohammad Hammoudeh', 'Paulo André Lima De Castro', 'Ali Dehghantanha', 'Kim Kwang Raymond Choo']",
        "date": "None",
        "source": "ACM Transactions on Internet Technology",
        "abstract": "Modern network infrastructures host converged applications that demand rapid elasticity of services, increased security, and ultra-fast reaction times. The Tactile Internet promises to facilitate the delivery of these services while enabling new economies of scale for high fidelity of machine-to-machine and human-to-machine interactions. Unavoidably, critical mission systems served by the Tactile Internet manifest high demands not only for high speed and reliable communications but equally, the ability to rapidly identify and mitigate threats and vulnerabilities. This article proposes a novel Multi-Agent Data Exfiltration Detector Architecture (MADEX), inspired by the mechanisms and features present in the human immune system. MADEX seeks to identify data exfiltration activities performed by evasive and stealthy malware that hides malicious traffic from an infected host in low-latency networks. Our approach uses cross-network traffic information collected by agents to effectively identify unknown illicit connections by an operating system subverted. MADEX does not require prior knowledge of the characteristics or behavior of the malicious code or a dedicated access to a knowledge repository. We tested the performance of MADEX in terms of its capacity to handle real-time data and the sensitivity of our algorithm’s classification when exposed to malicious traffic. Experimental evaluation results show that MADEX achieved 99.97% sensitivity, 98.78% accuracy, and an error rate of 1.21% when compared to its best rivals. We created a second version of MADEX, called MADEX level 2, that further improves its overall performance with a slight increase in computational complexity. We argue for the suitability of MADEX level 1 in non-critical environments, while MADEX level 2 can be used to avoid data exfiltration in critical mission systems. To the best of our knowledge, this is the first article in the literature that addresses the detection of rootkits real-time in an agnostic way using an artificial immune system approach while it satisfies strict latency requirements.",
        "link": "https://dl.acm.org/doi/10.1145/3419103",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Distributed Graph Processing System and Processing-in-memory Architecture with Precise Loop-carried Dependency Guarantee",
        "authors": "['Youwei Zhuo', 'Jingji Chen', 'Gengyu Rao', 'Qinyi Luo', 'Yanzhi Wang', 'Hailong Yang', 'Depei Qian', 'Xuehai Qian']",
        "date": "None",
        "source": "ACM Transactions on Computer Systems",
        "abstract": "To hide the complexity of the underlying system, graph processing frameworks ask programmers to specify graph computations in user-defined functions (UDFs) of graph-oriented programming model. Due to the nature of distributed execution, current frameworks cannot precisely enforce the semantics of UDFs, leading to unnecessary computation and communication. It exemplifies a gap between programming model and runtime execution. This article proposes novel graph processing frameworks for distributed system and Processing-in-memory (PIM) architecture that precisely enforces loop-carried dependency; i.e., when a condition is satisfied by a neighbor, all following neighbors can be skipped. Our approach instruments the UDFs to express the loop-carried dependency, then the distributed execution framework enforces the precise semantics by performing dependency propagation dynamically. Enforcing loop-carried dependency requires the sequential processing of the neighbors of each vertex distributed in different nodes. We propose to circulant scheduling in the framework to allow different nodes to process disjoint sets of edges/vertices in parallel while satisfying the sequential requirement. The technique achieves an excellent trade-off between precise semantics and parallelism—the benefits of eliminating unnecessary computation and communication offset the reduced parallelism. We implement a new distributed graph processing framework SympleGraph, and two variants of runtime systems—GraphS and GraphSR—for PIM-based graph processing architecture, which significantly outperform the state-of-the-art.",
        "link": "https://dl.acm.org/doi/10.1145/3453681",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Dynamic Scheduling Algorithm in Cyber Mimic Defense Architecture of Volunteer Computing",
        "authors": "['Qianmu Li', 'Shunmei Meng', 'Xiaonan Sang', 'Hanrui Zhang', 'Shoujin Wang', 'Ali Kashif Bashir', 'Keping Yu', 'Usman Tariq']",
        "date": "None",
        "source": "ACM Transactions on Internet Technology",
        "abstract": "Volunteer computing uses computers volunteered by the general public to do distributed scientific computing. Volunteer computing is being used in high-energy physics, molecular biology, medicine, astrophysics, climate study, and other areas. These projects have attained unprecedented computing power. However, with the development of information technology, the traditional defense system cannot deal with the unknown security problems of volunteer computing. At the same time, Cyber Mimic Defense (CMD) can defend the unknown attack behavior through its three characteristics: dynamic, heterogeneous, and redundant. As an important part of the CMD, the dynamic scheduling algorithm realizes the dynamic change of the service centralized executor, which can enusre the security and reliability of CMD of volunteer computing. Aiming at the problems of passive scheduling and large scheduling granularity existing in the existing scheduling algorithms, this article first proposes a scheduling algorithm based on time threshold and task threshold and realizes the dynamic randomness of mimic defense from two different dimensions; finally, combining time threshold and random threshold, a dynamic scheduling algorithm based on multi-level queue is proposed. The experiment shows that the dynamic scheduling algorithm based on multi-level queue can take both security and reliability into account, has better dynamic heterogeneous redundancy characteristics, and can effectively prevent the transformation rule of heterogeneous executors from being mastered by attackers.",
        "link": "https://dl.acm.org/doi/10.1145/3408291",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Point-X: A Spatial-Locality-Aware Architecture for Energy-Efficient Graph-Based Point-Cloud Deep Learning",
        "authors": "['Jie-Fang Zhang', 'Zhengya Zhang']",
        "date": "October 2021",
        "source": "MICRO '21: MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture",
        "abstract": "Deep learning on point clouds has attracted increasing attention in the fields of 3D computer vision and robotics. In particular, graph-based point-cloud deep neural networks (DNNs) have demonstrated promising performance in 3D object classification and scene segmentation tasks. However, the scattered and irregular graph-structured data in a graph-based point-cloud DNN cannot be computed efficiently by existing SIMD architectures and accelerators. We present Point-X, an energy-efficient accelerator architecture that extracts and exploits the spatial locality in point cloud data for efficient processing. Point-X uses a clustering method to extract fine-grained and coarse-grained spatial locality from the input point cloud. The clustering maps the point cloud into distributed compute tiles to maximize intra-tile computational parallelism and minimize inter-tile data movement. Point-X employs a chain network-on-chip (NoC) to further reduce the NoC traffic and achieve up to 3.2 × speedup over a traditional mesh NoC. Point-X’s multi-mode dataflow can support all common operations in a graph-based point-cloud DNN, i.e., edge convolution, shared multi-layer perceptron, and fully-connected layers. Point-X is synthesized in a 28nm technology and it demonstrates a throughput of 1307.1 inference/s and an energy efficiency of 604.5 inference/J on the DGCNN workload. Compared to the Nvidia GTX-1080Ti GPU, Point-X shows 4.5 × and 342.9 × improvement in throughput and efficiency, respectively.",
        "link": "https://dl.acm.org/doi/10.1145/3466752.3480081",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A fresh look at the architecture and performance of contemporary isolation platforms",
        "authors": "['Vincent van Rijn', 'Jan S. Rellermeyer']",
        "date": "December 2021",
        "source": "Middleware '21: Proceedings of the 22nd International Middleware Conference",
        "abstract": "With the ever-increasing pervasiveness of the cloud computing paradigm, strong isolation guarantees and low performance overhead from isolation platforms are paramount. An ideal isolation platform offers both: an impermeable isolation boundary while imposing a negligible performance overhead. In this paper, we examine various isolation platforms (containers, secure containers, hypervisors, unikernels), and conduct a wide array of experiments to measure the performance overhead and degree of isolation offered by the platforms. We find that container platforms have the best, near-native, performance while the newly emerging secure containers suffer from various overheads. The highest degree of isolation is achieved by unikernels, closely followed by traditional containers.",
        "link": "https://dl.acm.org/doi/10.1145/3464298.3493404",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "PipeZK: accelerating zero-knowledge proof with a pipelined architecture",
        "authors": "['Ye Zhang', 'Shuo Wang', 'Xian Zhang', 'Jiangbin Dong', 'Xingzhong Mao', 'Fan Long', 'Cong Wang', 'Dong Zhou', 'Mingyu Gao', 'Guangyu Sun']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "Zero-knowledge proof (ZKP) is a promising cryptographic protocol for both computation integrity and privacy. It can be used in many privacy-preserving applications including verifiable cloud outsourcing and blockchains. The major obstacle of using ZKP in practice is its time-consuming step for proof generation, which consists of large-size polynomial computations and multi-scalar multiplications on elliptic curves. To efficiently and practically support ZKP in real-world applications, we propose PipeZK, a pipelined accelerator with two subsystems to handle the aforementioned two intensive compute tasks, respectively. The first subsystem uses a novel dataflow to decompose large kernels into smaller ones that execute on bandwidth-efficient hardware modules, with optimized off-chip memory accesses and on-chip compute resources. The second subsystem adopts a lightweight dynamic work dispatch mechanism to share the heavy processing units, with minimized resource underutilization and load imbalance. When evaluated in 28 nm, PipeZK can achieve 10x speedup on standard cryptographic benchmarks, and 5x on a widely-used cryptocurrency application, Zcash.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00040",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Research on the Architecture and Key Technologies of the Ubiquitous Customer Service Operating System for State Grid",
        "authors": "['Bin Xu', 'Xusheng Liu', 'Wei Zhao', 'Ziqian Li', 'Chenfei Wang', 'Qing Zhu']",
        "date": "May 2021",
        "source": "ICAIIS 2021: 2021 2nd International Conference on Artificial Intelligence and Information Systems",
        "abstract": "Traditional customer service systems can hardly satisfy the high quality and efficiency requirements in the digital and intelligent era, to tackle this issue, advanced data and artificial intelligence technology are required to be integrated. Based on the ubiquitous operating system theory and take the State Grid customer service as a business scenario, this paper proposes an intelligent customer service platform. The platform takes the ubiquitous power customer service operating system as its core, manages and information multiplexes the massive data resources systematically to support customer service in a high-efficiency and intelligent manner. This paper studies the architecture of the platform and the key technologies involved, and implemented core applications at the industrial level. The paper also discusses the reliability of the system and proposed a reliability guarantee mechanism for customer service operating system. The demonstration system was verified in the actual production environment of the State Grid, and finally proved that this system can effectively improve the work efficiency of the customer service team and reduce operation and maintenance costs.",
        "link": "https://dl.acm.org/doi/10.1145/3469213.3470417",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Plant(e)tecture: Towards a Multispecies Media Architecture Framework for amplifying Plant Agencies",
        "authors": "['Hira Sheikh', 'Kavita Gonsalves', 'Marcus Foth']",
        "date": "June 2021",
        "source": "MAB20: Media Architecture Biennale 20",
        "abstract": "More-than-human media architecture is gaining increased attention as a response to the planet's environmental emergency and in turn allowing us to envision a more desirable future compared to tech-utopian smart cities. New types of digital technologies have emerged into the human mindscape, and with that a new potential for bridging human understanding and multispecies geographies. Emphasising the agency of plants, this paper attempts to answer the following research questions: how can twenty-first-century media technologies, such as media architecture, be used to better incorporate plant/flora perspectives? This paper begins by providing a review of multispecies ontology, exploration of plant agency and how media architecture can visualise and amplify plant agency based on Whitehead's Process Philosophy, Mark Hansen's Feed-Forward concept and Feenberg's technical agency framework. Hortum Machina B, terra0, and Elowan are explored as three examples of Plant(e)tecture where the multispecies actors “plants'' are coupled with technologies and analysed for agency and autonomy. Synthesising the three case studies, the paper discusses the role of media architecture in (i) enabling plant agency; (ii) engaging multispecies actors in autonomous decision-making, and; (iii) the creation of technology to amplify the agency of plants in a design process that transcends human sensibilities. The paper closes on the prospect of the multispecies-techno agency framework to enable human designers, makers, decision-makers and thinkers to move beyond human-centric determinism prevalent in media architecture. The framework offers new ways of thinking of the purpose of technical agency in multispecies assemblages.",
        "link": "https://dl.acm.org/doi/10.1145/3469410.3469419",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "ExRec: Experimental Framework for Reconfigurable Networks Based on Off-the-Shelf Hardware",
        "authors": "['Johannes Zerwas', 'Chen Avin', 'Stefan Schmid', 'Andreas Blenk']",
        "date": "December 2021",
        "source": "ANCS '21: Proceedings of the Symposium on Architectures for Networking and Communications Systems",
        "abstract": "In order to meet the increasingly stringent throughput and latency requirements in datacenter networks, several innovative network architectures based on reconfigurable optical topologies have been proposed. Examples include demand-oblivious reconfigurable topologies such as RotorNet (SIGCOMM 2017), Opera (NSDI 2020), and Sirius (SIGCOMM 2021), as well as demand-aware topologies such as ProjecToR (SIGCOMM 2016). All these architectures feature attractive performance properties using specific prototypes. However, reproducing these experiments is often difficult due to missing hardware and publicly available software. This paper presents a flexible framework for reconfigurable networks based on off-the-shelf hardware, which supports experimentation and reproducibility at a small scale. We describe how our framework, ExReC, can be instantiated with different configurations, allowing us to emulate existing architectures and to study their trade-offs. Finally, we demonstrate the application of our approach to different use cases and workloads, including distributed machine learning training.",
        "link": "https://dl.acm.org/doi/10.1145/3493425.3502748",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Practical and Scalable Security Verification of Secure Architectures",
        "authors": "['Tianwei Zhang', 'Jakub Szefer', 'Ruby B. Lee']",
        "date": "October 2021",
        "source": "HASP '21: Proceedings of the 10th International Workshop on Hardware and Architectural Support for Security and Privacy",
        "abstract": "We present a new and practical framework for security verification of secure architectures. Specifically, we break the verification task into external verification and internal verification. External verification considers the external protocols, i.e. interactions between users, compute servers, network entities, etc. Meanwhile, internal verification considers the interactions between hardware and software components within each server. This verification framework is general-purpose and can be applied to a stand-alone server, or a large-scale distributed system. We evaluate our verification method on the CloudMonatt and HyperWall architectures as examples.",
        "link": "https://dl.acm.org/doi/10.1145/3505253.3505256",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "NeuroXplorer 1.0: An Extensible Framework for Architectural Exploration with Spiking Neural Networks",
        "authors": "['Adarsha Balaji', 'Shihao Song', 'Twisha Titirsha', 'Anup Das', 'Jeffrey Krichmar', 'Nikil Dutt', 'James Shackleford', 'Nagarajan Kandasamy', 'Francky Catthoor']",
        "date": "July 2021",
        "source": "ICONS 2021: International Conference on Neuromorphic Systems 2021",
        "abstract": "Recently, both industry and academia have proposed many different neuromorphic architectures to execute applications that are designed with Spiking Neural Network (SNN). Consequently, there is a growing need for an extensible simulation framework that can perform architectural explorations with SNNs, including both platform-based design of today’s hardware, and hardware-software co-design and design-technology co-optimization of the future. We present NeuroXplorer, a fast and extensible framework that is based on a generalized template for modeling a neuromorphic architecture that can be infused with the specific details of a given hardware and/or technology. NeuroXplorer can perform both low-level cycle-accurate architectural simulations and high-level analysis with data-flow abstractions. NeuroXplorer’s optimization engine can incorporate hardware-oriented metrics such as energy, throughput, and latency, as well as SNN-oriented metrics such as inter-spike interval distortion and spike disorder, which directly impact SNN performance. We demonstrate the architectural exploration capabilities of NeuroXplorer through case studies with many state-of-the-art machine learning models.",
        "link": "https://dl.acm.org/doi/10.1145/3477145.3477156",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Architecture Design and Key Technology Research of Urban Intelligent Transportation Integrated Control System Based on Edge Cloud Cooperation Technology",
        "authors": "['Fenxin Zhang', 'Keshuang Tang', 'Huixin Zhang', 'Peng Zhang']",
        "date": "October 2021",
        "source": "CSAE '21: Proceedings of the 5th International Conference on Computer Science and Application Engineering",
        "abstract": "With the development of vehicle-road collaborative technology and the popularity of unmanned vehicles, smart cars and smart roads have brought a wide range of impacts on urban intelligent transportation systems, which has made great changes in front-end equipment perception, front-end and back-end information interaction, and traffic collaborative command and control. This paper mainly studies the new application scenarios of urban intelligent transportation vehicle-road collaboration. With edge-cloud collaboration technology as the core, the container cloud technology is used to construct the system, which supports edge intelligent analysis, cloud big data fusion and multi-level deployment of system design theory, implementation methods and related collaborative technologies. Finally, the integrated management and control system of cloud, edge and end for urban intelligent transportation management is developed.",
        "link": "https://dl.acm.org/doi/10.1145/3487075.3487182",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Exploiting long-distance interactions and tolerating atom loss in neutral atom quantum architectures",
        "authors": "['Jonathan M. Baker', 'Andrew Litteken', 'Casey Duckering', 'Henry Hoffmann', 'Hannes Bernien', 'Frederic T. Chong']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "Quantum technologies currently struggle to scale beyond moderate scale prototypes and are unable to execute even reasonably sized programs due to prohibitive gate error rates or coherence times. Many software approaches rely on heavy compiler optimization to squeeze extra value from noisy machines but are fundamentally limited by hardware. Alone, these software approaches help to maximize the use of available hardware but cannot overcome the inherent limitations posed by the underlying technology. An alternative approach is to explore the use of new, though potentially less developed, technology as a path towards scalability. In this work we evaluate the advantages and disadvantages of a Neutral Atom (NA) architecture. NA systems offer several promising advantages such as long range interactions and native multiqubit gates which reduce communication overhead, overall gate count, and depth for compiled programs. Long range interactions, however, impede parallelism with restriction zones surrounding interacting qubit pairs. We extend current compiler methods to maximize the benefit of these advantages and minimize the cost. Furthermore, atoms in an NA device have the possibility to randomly be lost over the course of program execution which is extremely detrimental to total program execution time as atom arrays are slow to load. When the compiled program is no longer compatible with the underlying topology, we need a fast and efficient coping mechanism. We propose hardware and compiler methods to increase system resilience to atom loss dramatically reducing total computation time by circumventing complete reloads or full recompilation every cycle.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00069",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Design and practice of website group architecture and security based on data mining technology",
        "authors": "['Wei Zhao']",
        "date": "December 2021",
        "source": "ICASIT 2021: 2021 International Conference on Aviation Safety and Information Technology",
        "abstract": "None",
        "link": "https://dl.acm.org/doi/10.1145/3510858.3511423",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "AIoT Platform Design Based on Front and Rear End Separation Architecture for Smart Agricultural",
        "authors": "['Hang Li', 'Sufang Li', 'Jiguo Yu', 'Yubing Han', 'Anming Dong']",
        "date": "January 2022",
        "source": "APIT '22: Proceedings of the 2022 4th Asia Pacific Information Technology Conference",
        "abstract": "With the development of informatization, intelligence and precision of modern agriculture, i.e. there is a need for the integration of artificial intelligence (AI) with Internet of Things (IoT) systems, which is called AIoT (AI + IoT) systems. In this paper, we design an AIoT system for the smart agriculture based on the concept of front-rear end separation and the framework of MVVM (Model-View-View Model), through which it is possible to handle complex business logic and makes the integrating the AI algorithms much easier. Specifically, the system consists of a remote data service platform, the data collection terminals build on Raspberry Pi and the wireless data transmission using narrow-band Internet of Things (NB-IoT) modules. The data service platform is designed with the separated front-end and rear-end. The front-end is a web page constructed by the Vue.js and Element, while the rear-end business logic processing is constructed using the Python Django framework. The data interaction between the front and rear ends is realized through Axios. In such a way, the data in the front-end and the rear-end are decoupled, which makes it possible to improve the capability in dealing with complex data and makes it easy to carry out add-on development and extend new functions. Based on the data service platform, a series of basic application functions are integrated, including real-time data monitoring, historical data query, data visualization and abnormal data alerting, etc. Moreover, we integrate a deep-learning-based plant disease and pest detection algorithm in the propose system to show its scalability. In addition, the system also combines edge computing technology to improve the overall response efficiency of the system. The system has a convenient expansion interface and can be used as a basic development platform for various agricultural IoT applications, such as the soil environmental monitoring system and the intelligent disease and pest monitoring system, etc.",
        "link": "https://dl.acm.org/doi/10.1145/3512353.3512384",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Architecture and Implementation of Focus Crawler for Big Data Collection in Functional Sports Training",
        "authors": "['Weiwei Wang']",
        "date": "September 2021",
        "source": "ICISCAE 2021: 2021 4th International Conference on Information Systems and Computer Aided Education",
        "abstract": "None",
        "link": "https://dl.acm.org/doi/10.1145/3482632.3487513",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Research on the Evolution and Key Technologies of Mobile Communication Network Architecture in the Future",
        "authors": "['Li Pengcheng']",
        "date": "May 2021",
        "source": "ICAIIS 2021: 2021 2nd International Conference on Artificial Intelligence and Information Systems",
        "abstract": "None",
        "link": "https://dl.acm.org/doi/10.1145/3469213.3470341",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Flodam: cross-layer reliability analysis flow for complex hardware designs",
        "authors": "['Angeliki Kritikakou', 'Olivier Sentieys', 'Guillaume Hubert', 'Youri Helen', 'Jean-Francois Coulon', 'Patrice Deroux-Dauphin']",
        "date": "March 2022",
        "source": "DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe",
        "abstract": "Modern technologies make hardware designs more and more sensitive to radiation particles and related faults. As a result, analysing the behavior of a system under radiation-induced faults has become an essential part of the system design process. Existing approaches either focus on analysing the radiation impact at the lower hardware design layers, without further propagating any radiation-induced fault to the system execution, or analyse system reliability at higher hardware or application layers, based on fault models that are agnostic of the fabrication technology and the radiation environment. Flodam combines the benefits of existing approaches by providing a novel cross-layer reliability analysis from the semiconductor layer up to the application layer, able to quantify the risks of faults under a given context, taking into account the environmental conditions, the physical hardware design and the application under study.",
        "link": "https://dl.acm.org/doi/10.5555/3539845.3540039",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Snafu: an ultra-low-power, energy-minimal CGRA-generation framework and architecture",
        "authors": "['Graham Gobieski', 'Ahmet Oguz Atli', 'Kenneth Mai', 'Brandon Lucia', 'Nathan Beckmann']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "Ultra-low-power (ULP) devices are becoming pervasive, enabling many emerging sensing applications. Energy-efficiency is paramount in these applications, as efficiency determines device lifetime in battery-powered deployments and performance in energy-harvesting deployments. Unfortunately, existing designs fall short because ASICs' upfront costs are too high and prior ULP architectures are too inefficient or inflexible. We present SNAFU, the first framework to flexibly generate ULP coarse-grain reconfigurable arrays (CGRAs). SNAFU provides a standard interface for processing elements (PE), making it easy to integrate new types of PEs for new applications. Unlike prior high-performance, high-power CGRAs, SNAFU is designed from the ground up to minimize energy consumption while maximizing flexibility. SNAFU saves energy by configuring PEs and routers for a single operation to minimize switching activity; by minimizing buffering within the fabric; by implementing a statically routed, bufferless, multi-hop network; and by executing operations in-order to avoid expensive tag-token matching. We further present SNAFU-ARCH, a complete ULP system that integrates an instantiation of the SNAFU fabric alongside a scalar RISC-V core and memory. We implement SNAFU in RTL and evaluate it on an industrial sub-28 nm FinFET process across a suite of common sensing benchmarks. SNAFU-ARCH operates at <1mW, orders-of-magnitude less power than most prior CGRAs. SNAFU-ARCH uses 41% less energy and runs 4.4X faster than the prior state-of-the-art general-purpose ULP architecture. Moreover, we conduct three comprehensive case-studies to quantify the cost of programmability in SNAFU. We find that SNAFU-ARCH is close to ASIC designs built in the same technology, using just 2.6X more energy on average.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00084",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Design and Development Multi-Verification Protocols of A Digital Signed Document System Using Component-Based Architecture",
        "authors": "['Paniti Netinant', 'Siwaphat Boonbangyang', 'Meennapa Rukhiran']",
        "date": "April 2021",
        "source": "ICEEG '21: Proceedings of the 5th International Conference on E-Commerce, E-Business and E-Government",
        "abstract": "In the digital age, a transformation of a traditional business to an electronic organization becomes widely general. One of the resolutions is an adopted information technology to design and develop software supporting the whole process of organizations. End users enable to access and use digital services anywhere and anytime. Covid-19 has directed us to face a dramatic death of human life. Social distancing is an announcement of avoiding and touching unknown people, including colleagues, lecturers, and students. In this article, we aim to propose the design and development of a digitally signed document system. The system can handle documents of university processes to advance online forms significantly. By providing the digitally signed document system, a documented strategy can integrate users (e.g., general employees, executives, directors, and lecturers) to send electronic documents with proportional high security. The digital signature of user identification can perform via user identification and verification protocols in many methods. The design and development of software models are approached using rapid application development (RAD). RAD is the most compatible with project factors. The software functionaries of the digitally signed document system are decomposed into components for improving software quality. The interface components deliver indicators of a friendly environment of green information technology as well.",
        "link": "https://dl.acm.org/doi/10.1145/3466029.3466039",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "KallaxDB: A Table-less Hash-based Key-Value Store on Storage Hardware with Built-in Transparent Compression",
        "authors": "['Xubin Chen', 'Ning Zheng', 'Shukun Xu', 'Yifan Qiao', 'Yang Liu', 'Jiangpeng Li', 'Tong Zhang']",
        "date": "June 2021",
        "source": "DAMON '21: Proceedings of the 17th International Workshop on Data Management on New Hardware",
        "abstract": "This paper studies the design of a key-value (KV) store that can take full advantage of modern storage hardware with built-in transparent compression capability. Many modern storage appliances/drives implement hardware-based data compression, transparent to OS and applications. Moreover, the growing deployment of hardware-based compression in Cloud infrastructure leads to the imminent arrival of Cloud-based storage hardware with built-in transparent compression. By decoupling the logical storage space utilization efficiency from the true physical storage usage, transparent compression allows data management software to purposely waste logical storage space in return for simpler data structures and algorithms, leading to lower implementation complexity and higher performance. This work proposes a table-less hash-based KV store, where the basic idea is to hash the key space directly onto the logical storage space without using a hash table at all. With a substantially simplified data structure, this approach is subject to significant logical storage space under-utilization, which can be seamlessly mitigated by storage hardware with transparent compression. This paper presents the basic KV store architecture, and develops mathematical formulations to assist its configuration and analysis. We implemented such a KV store KallaxDB and carried out experiments on a commercial SSD with built-in transparent compression. The results show that, while consuming very little memory resource, it compares favorably with the other modern KV stores in terms of throughput, latency, and CPU usage.",
        "link": "https://dl.acm.org/doi/10.1145/3465998.3466004",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "The Architecture and Security Design of Big Data Platform of the Physical Teaching Information System in Colleges and Universities",
        "authors": "['Caibao Han']",
        "date": "September 2021",
        "source": "ICISCAE 2021: 2021 4th International Conference on Information Systems and Computer Aided Education",
        "abstract": "This paper constructs the logic structure of big data platform of physical teaching information system in Colleges and universities through Hadoop distributed system, which ensures the realization of management and monitoring function, data collection function, data storage and query function, data calculation function, security and management function. The security design is carried out from access control, data authorization and management, Hadoop security configuration to deal with the possible security problems of big data platform.",
        "link": "https://dl.acm.org/doi/10.1145/3482632.3483061",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "DEDD: Deep Encoder with Dual Decoder Architecture for Stability and Specificity Preserving Encoding and Translation of Embedding between Domains",
        "authors": "['Rajesh Ranjan', 'Debasmita Das', 'Ram Ganesh V', 'Yatin Katyal', 'Tanmoy Bhowmik']",
        "date": "May 2021",
        "source": "CNIOT '21: Proceedings of the 2021 2nd International Conference on Computing, Networks and Internet of Things",
        "abstract": "We propose a deep learning-based encoder with a dual decoder system to enrich the expressive power of embeddings pre-trained on two different corpora along with switching representation between domains. There are two scenarios: (a) Each of the corpora is pertaining to the different subject matter or topic of interests and (b) One corpus is a vast super-domain with generic and non-specific embeddings while the second one pertains to one specific sub-domain. In either case, the criterion for high-quality training would be to have enough common words between them. The mapping of contextual embeddings from both the corpus into the common latent space blends the semantic richness of both the corpus-specific learning while maintaining embedding stability. Furthermore, there is one dedicated decoder for either of the domains for generating the representation from common latent space. We evaluated our method for cross-learning between generalized GLOVE embedding and a very specialized skill-embedding developed by random-walk on a graph-based Skills Hierarchy. We demonstrate that our method preserves the stability of the generic embedding, the specificity of the skill domain as well as enriches the semantic representation of either domain through switching enabled by the encoder-to-duel-decoder path.",
        "link": "https://dl.acm.org/doi/10.1145/3468691.3468711",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Full-stack quantum computing systems in the NISQ era: algorithm-driven and hardware-aware compilation techniques",
        "authors": "['Medina Bandic', 'Sebastian Feld', 'Carmen G. Almudever']",
        "date": "March 2022",
        "source": "DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe",
        "abstract": "The progress in developing quantum hardware with functional quantum processors integrating tens of noisy qubits, together with the availability of near-term quantum algorithms has led to the release of the first quantum computers. These quantum computing systems already integrate different software and hardware components of the so-called \"full-stack\", bridging quantum applications to quantum devices. In this paper, we will provide an overview on current full-stack quantum computing systems. We will emphasize the need for tight co-design among adjacent layers as well as vertical cross-layer design to extract the most from noisy intermediate-scale quantum (NISQ) processors which are both error-prone and severely constrained in resources. As an example of co-design, we will focus on the development of hardware-aware and algorithm-driven compilation techniques.",
        "link": "https://dl.acm.org/doi/10.5555/3539845.3539847",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "SafeDM: a hardware diversity monitor for redundant execution on non-lockstepped cores",
        "authors": "['Francisco Bas', 'Pedro Benedicte', 'Sergi Alcaide', 'Guillem Cabo', 'Fabio Mazzocchetti', 'Jaume Abella']",
        "date": "March 2022",
        "source": "DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe",
        "abstract": "Computing systems in the safety domain, such as those in avionics or space, require specific safety measures related to the criticality of the deployment. A problem these systems face is that of transient failures in hardware. A solution commonly used to tackle potential failures is to introduce redundancy in these systems, for example 2 cores that execute the same program at the same time. However, redundancy does not solve all potential failures, such as Common Cause Failures (CCF), where a single fault affects both cores identically (e.g. a voltage droop). If both redundant cores have identical state when the fault occurs, then there may be a CCF since the fault can affect both cores in the same way. To avoid CCF it is critical to know that there is diversity in the execution amongst the redundant cores. In this paper we introduce SafeDM, a hardware Diversity Monitor that quantifies the diversity of each redundant processor to guarantee that CCF will not go unnoticed, and without needing to deploy lockstepped cores. SafeDM computes data and instruction diversity separately, using different techniques appropriate for each case. We integrate SafeDM in a RISC-V FPGA space MPSoC from Cobham Gaisler where SafeDM is proven effective with a large benchmark suite, incurring low area and power overheads. Overall, SafeDM is an effective hardware solution to quantify diversity in cores performing redundant execution.",
        "link": "https://dl.acm.org/doi/10.5555/3539845.3539933",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Odyssey: the impact of modern hardware on strongly-consistent replication protocols",
        "authors": "['Vasilis Gavrielatos', 'Antonios Katsarakis', 'Vijay Nagarajan']",
        "date": "April 2021",
        "source": "EuroSys '21: Proceedings of the Sixteenth European Conference on Computer Systems",
        "abstract": "Get/Put Key-Value Stores (KVSes) rely on replication protocols to enforce consistency and guarantee availability. Today's modern hardware, with manycore servers and RDMA-capable networks, challenges the conventional wisdom on protocol design. In this paper, we investigate the impact of modern hardware on the performance of strongly-consistent replication protocols. First, we create an informal taxonomy of replication protocols, based on which we carefully select 10 protocols for analysis. Secondly, we present Odyssey, a framework tailored towards protocol implementation for multi-threaded, RDMA-enabled, in-memory, replicated KVSes. We implement all 10 protocols over Odyssey, and perform the first apples-to-apples comparison of replication protocols over modern hardware. Our comparison characterizes the protocol design space, revealing the performance capabilities of different classes of protocols on modern hardware. Among other things, our results demonstrate that some of the protocols that were efficient in yesterday's hardware are not so today because they cannot take advantage of the abundant parallelism and fast networking present in modern hardware. Conversely, some protocols that were inefficient in yesterday's hardware are very attractive today. We distill our findings in a concise set of general guidelines and recommendations for protocol selection and design in the era of modern hardware.",
        "link": "https://dl.acm.org/doi/10.1145/3447786.3456240",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "ArchViMP – a Framework for Automatic Extraction of Concurrency-related Software Architectural Properties",
        "authors": "['Monireh Pourjafarian', 'Jasmin Jahic']",
        "date": "August 2021",
        "source": "ICPP Workshops '21: 50th International Conference on Parallel Processing Workshop",
        "abstract": "Concurrent multithreaded programs are more complex than sequential ones due to inter-dependencies of threads over shared memory. Because of these, software architects and developers quickly become overwhelmed when trying to design and manage concurrent software. Existing approaches that try to support architecture efforts in this domain rely on the visualization of concurrency-related properties of software to ease its understanding, but they fail because i) the abstractions they use do not capture information of architectural significance, and because ii) raw visualization of the interdependencies does not scale.  In this paper, we suggest a scalable solution that focuses on the abstraction of concurrency properties and their visualization using architectural views. Our framework for automatic extraction of concurrency-related architectural properties (ArchViMP) proposes i) a set of logical rules that abstract concurrency-related architecturally significant software properties and ii) a set of architectural views suitable for showing these concurrency properties.",
        "link": "https://dl.acm.org/doi/10.1145/3458744.3473349",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "On the Exploration and Optimization of High-Dimensional Architectural Design Space",
        "authors": "['Vincent Bode', 'Fariz Huseynli', 'Matrtin Schreiber', 'Carsten Trinitis', 'Martin Schulz']",
        "date": "June 2021",
        "source": "PERMAVOST '21: Proceedings of the 2021 on Performance EngineeRing, Modelling, Analysis, and VisualizatiOn STrategy",
        "abstract": "The rise of heterogeneity in High-Performance Computing (HPC) architectures has caused a spike in the number of viable hardware solutions for different workloads. In order to take advantage of the increasing possibilities to influence how hardware can be tailored to boost software performance, collaboration between hardware manufacturers, computing centers and application developers must intensify with the goal of hardware-software co-design. To support the co-design effort, we need efficient methods to compare the performance of the many potential architectures running user-supplied applications. We present the High-Dimensional Exploration and Optimization Tool (HOT), a tool for visualizing and comparing software performance on CPU/GPU hybrid architectures. HOT is currently based on data acquired from Intel's Offload Advisor (I-OA) to model application performance, allowing us to extract performance predictions for existing/custom accelerator architectures. This eliminates the necessity of porting applications to different (parallel) programming models and also avoids benchmarking the application on target hardware. However, tools like I-OA allow users to tweak many hardware parameters, making it tedious to evaluate and compare results. HOT, therefore, focuses on visualizing these high-dimensional design spaces and assists the user in identifying suitable hardware configurations for given applications. Thus, users can gain rapid insights into how hardware/software influence each other in heterogeneous environments. We show the usage of HOT on several case studies. To determine the accuracy of collected performance data with I-OA, we analyze LULESH on different architectures. Next, we apply HOT to the synthetic benchmarks STREAM and 2MM to demonstrate the tool's visualization under these well-defined and known workloads, validating both the tool and its usage. Finally, we apply HOT to the real world code Gadget and the proxy application LULESH allowing us to easily identify their bottlenecks and optimize the choice of compute architecture for them.",
        "link": "https://dl.acm.org/doi/10.1145/3452412.3462754",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "LCCG: a locality-centric hardware accelerator for high throughput of concurrent graph processing",
        "authors": "['Jin Zhao', 'Yu Zhang', 'Xiaofei Liao', 'Ligang He', 'Bingsheng He', 'Hai Jin', 'Haikun Liu']",
        "date": "November 2021",
        "source": "SC '21: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",
        "abstract": "In modern data centers, massive concurrent graph processing jobs are being processed on large graphs. However, existing hardware/-software solutions suffer from irregular graph traversal and intense resource contention. In this paper, we propose LCCG, a <u>L</u>ocality-<u>C</u>entric programmable accelerator that augments the many-core processor for achieving higher throughput of <u>C</u>oncurrent <u>G</u>raph processing jobs. Specifically, we develop a novel topology-aware execution approach into the accelerator design to regularize the graph traversals for multiple jobs on-the-fly according to the graph topology, which is able to fully consolidate the graph data accesses from concurrent jobs. By reusing the same graph data among more jobs and coalescing the accesses of the vertices' states for these jobs, LCCG can improve the core utilization. We conduct extensive experiments on a simulated 64-core processor. The results show that LCCG improves the throughput of the cutting-edge software system by 11.3~23.9 times with only 0.5% additional area cost. Moreover, LCCG gains the speedups of 4.7~10.3, 5.5~13.2, and 3.8~8.4 times over state-of-the-art hardware graph processing accelerators (namely, HATS, Minnow, and PHI, respectively).",
        "link": "https://dl.acm.org/doi/10.1145/3458817.3480854",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "TwinVisor: Hardware-isolated Confidential Virtual Machines for ARM",
        "authors": "['Dingji Li', 'Zeyu Mi', 'Yubin Xia', 'Binyu Zang', 'Haibo Chen', 'Haibing Guan']",
        "date": "October 2021",
        "source": "SOSP '21: Proceedings of the ACM SIGOPS 28th Symposium on Operating Systems Principles",
        "abstract": "Confidential VM, which offers an isolated execution environment for cloud tenants with limited trust in the cloud provider, has recently been deployed in major clouds such as AWS and Azure. However, while ARM has become increasingly popular in cloud data centers, existing confidential VM designs mainly leverage specialized x86 hardware extensions (e.g., AMD SEV and Intel TDX) to isolate VMs upon a shared hypervisor. This paper proposes TwinVisor, the first system that enables the hardware-enforced isolation of confidential VMs on ARM platforms. TwinVisor takes advantage of the mature ARM TrustZone to run two isolated hypervisors, one in the secure world (called S-visor in this paper) and the other in the normal world (called N-visor), to support normal VMs and confidential VMs respectively. Instead of building a new S-visor from scratch, our design decouples protection from resource management, and reuses most functionalities of a full-fledged N-visor to minimize the size of S-visor. We have built two prototypes of TwinVisor: one on an official ARM simulator with S-EL2 enabled to validate functional correctness and the other on an ARM development board to evaluate performance. The S-visor comprises 5.8K LoCs while the N-visor introduces 906 LoC changes to KVM. According to our evaluation, TwinVisor can run unmodified VM images as confidential VMs while incurring less than 5% performance overhead for various real-world workloads on SMP VMs.",
        "link": "https://dl.acm.org/doi/10.1145/3477132.3483554",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Towards Extraction of Message-Based Communication in Mixed-Technology Architectures for Performance Model",
        "authors": "['Snigdha Singh', 'Yves Richard Kirschner', 'Anne Koziolek']",
        "date": "April 2021",
        "source": "ICPE '21: Companion of the ACM/SPEC International Conference on Performance Engineering",
        "abstract": "Software systems architected using multiple technologies are becoming popular. Many developers use these technologies as it offers high service quality which has often been optimized in terms of performance. In spite of the fact that performance is a key to the technology-mixed software applications, still there a little research on performance evaluation approaches explicitly considering the extraction of architecture for modelling and predicting performance. In this paper, we discuss the opportunities and challenges in applying existing architecture extraction approaches to support model-driven performance prediction for technology-mixed software. Further, we discuss how it can be extended to support a message-based system. We describe how various technologies deriving the architecture can be transformed to create the performance model. In order to realise the work, we used a case study from the energy system domain as an running example to support our arguments and observations throughout the paper.",
        "link": "https://dl.acm.org/doi/10.1145/3447545.3451201",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Tenet: A Neural Network Model Extraction Attack in Multi-core Architecture",
        "authors": "['Chengsi Gao', 'Bing Li', 'Ying Wang', 'Weiwei Chen', 'Lei Zhang']",
        "date": "June 2021",
        "source": "GLSVLSI '21: Proceedings of the 2021 on Great Lakes Symposium on VLSI",
        "abstract": "As neural networks (NNs) are being widely deployed in many cloud-oriented systems for safety-critical tasks, the privacy and security of NNs become significant concerns to users in the cloud platform that shares the computation infrastructure such as memory resource. In this work, we observed that the memory timing channel in the shared memory of cloud multi-core architecture poses the risk of network model information leakage. Based on the observation, we propose a learning-based method to steal the model architecture of the NNs by exploiting the memory timing channel without any high-level privilege or physical access. We first trained an end-to-end measurement network offline to learn the relation between memory timing information and NNs model architecture. Then, we performed an online attack and reconstructed the target model using the prediction from the measurement network. We evaluated the proposed attack method on a multi-core architecture simulator. The experimental results show that our learning-based attack method can reconstruct the target model with high accuracy and improve the adversarial attack success rate by 42.4%.",
        "link": "https://dl.acm.org/doi/10.1145/3453688.3461512",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Towards trustworthy AI: safe-visor architecture for uncertified controllers in stochastic cyber-physical systems",
        "authors": "['Abolfazl Lavaei', 'Bingzhuo Zhong', 'Marco Caccamo', 'Majid Zamani']",
        "date": "May 2021",
        "source": "CAADCPS '21: Proceedings of the Workshop on Computation-Aware Algorithmic Design for Cyber-Physical Systems",
        "abstract": "Artificial intelligence-based (a.k.a. AI-based) controllers have received significant attentions in the past few years due to their broad applications in cyber-physical systems (CPSs) to accomplish complex control missions. However, guaranteeing safety and reliability of CPSs equipped with this kind of (uncertified) controllers is currently very challenging, which is of vital importance in many real-life safety-critical applications. To cope with this difficulty, we propose a Safe-visor architecture for sandboxing AI-based controllers in stochastic CPSs. The proposed framework contains (i) a history-based supervisor which checks inputs from the AI-based controller and makes compromise between functionality and safety of the system, and (ii) a safety advisor that provides fallback when the AI-based controller endangers the safety of the system. By employing this architecture, we provide formal probabilistic guarantees on the satisfaction of those classes of safety specifications which can be represented by the accepting languages of deterministic finite automata (DFA), while AI-based controllers can still be employed in the control loop even though they are not reliable.",
        "link": "https://dl.acm.org/doi/10.1145/3457335.3461705",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "FPRA: a fine-grained parallel RRAM architecture",
        "authors": "['Xiao Liu', 'Minxuan Zhou', 'Rachata Ausavarungnirun', 'Sean Eilert', 'Ameen Akel', 'Tajana Rosing', 'Vijaykrishnan Narayanan', 'Jishen Zhao']",
        "date": "July 2021",
        "source": "ISLPED '21: Proceedings of the ACM/IEEE International Symposium on Low Power Electronics and Design",
        "abstract": "Emerging resistive memory (RRAM) based crossbar array is a promising technology to accelerate neural network applications. RRAM-based CNN accelerators support a high-degree of intra-layer and inter-layer parallelism. The intra-layer parallelism duplicates kernels for each network layer while the inter-layer parallelism allows execution of each layer when a portion of input data is available. However, previously proposed RRAM-based accelerators do not leverage data sharing between duplicate kernels leading to significant idleness of crossbar arrays during inference. This shared data creates data dependencies that stall the processing of the next layer in the pipeline. To address these issues, we propose Fine-grained Parallel RRAM Architecture (FPRA), a novel architectural design, to improve parallelism for pipeline-enabled RRAM-based accelerators. FPRA addresses the data sharing issue with kernel batching and data sharing aware memory. Kernel batching rearranges the layout of the kernels and minimizes the data dependencies created by the input shared data. The data sharing aware memory uniformly buffers the input and output data for each layer, efficiently dispatching data to duplicate kernels while reducing the amount of data transferred between layers. We evaluate FPRA on eight popular image recognition CNN models with various configurations in a cycle-accurate simulator. We find that FPRA manages to achieve 2.0x average latency speedup, and 2.1x average throughput increase, as compared to the state-of-the-art RRAM-based accelerators.",
        "link": "https://dl.acm.org/doi/10.1109/ISLPED52811.2021.9502474",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Taming the zoo: the unified GraphIt compiler framework for novel architectures",
        "authors": "['Ajay Brahmakshatriya', 'Emily Furst', 'Victor A. Ying', 'Claire Hsu', 'Changwan Hong', 'Max Ruttenberg', 'Yunming Zhang', 'Dai Cheol Jung', 'Dustin Richmond', 'Michael B. Taylor', 'Julian Shun', 'Mark Oskin', 'Daniel Sanchez', 'Saman Amarasinghe']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "We live in a new Cambrian Explosion of hardware devices. The end of conventional processor scaling has driven research and industry practice to explore a new generation of approaches. The old DNA of architecture design, including vectors, threads, shared or private memories, coherence or message passing, dataflow or von Neumann execution, are hybridized together in new and exciting ways. Each new architecture exposes a unique hardware-level API. Performance and energy efficiency are critically dependent on how well programs can use these APIs. One approach is to implement custom libraries for each new hardware architecture and application domain. A more scalable approach is to utilize a portable compiler infrastructure tailored to the application domain that makes it easy to generate efficient code for a diverse set of architectures with minimal porting effort. We propose the Unified GraphIt Compiler framework (UGC), which does exactly this for graph applications. UGC achieves portability with reasonable effort by decoupling the architecture-independent algorithm from the architecture-specific schedules and backends. We introduce a new domain-specific intermediate representation, GraphIR, that is key to this decoupling. GraphIR encodes high-level algorithm and optimization information needed for hardware-specific code generation, making it easy to develop different backends (GraphVMs) for diverse architectures, including CPUs, GPUs, and next-generation hardware such as Swarm and the HammerBlade manycore. We also build scheduling language extensions that make it easy to expose optimization decisions like load balancing strategies, blocking for locality, and other data structure choices. We evaluate UGC on five algorithms and 10 input graphs on these 4 distinct architectures and show that UGC enables implementing optimizations that can provide up to 53X speedup over programmer-generated straightforward implementations.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00041",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Technology-aware Router Architectures for On-Chip-Networks in Heterogeneous Technologies",
        "authors": "['Lennart Bamberg', 'Tushar Krishna', 'Jan Moritz Joseph']",
        "date": "September 2021",
        "source": "NANOCOM '21: Proceedings of the Eight Annual ACM International Conference on Nanoscale Computing and Communication",
        "abstract": "Heterogeneous 3D/2.5D stacking allows to tightly couple components that are ideally integrated into different technologies yielding advantages in nearly all design metrics. Massively parallel and scalable communication architectures between the components in such 3D ICs are commonly implemented through Networks-on-Chip (NoCs). This paper contributes a systematic approach to improve the efficiency of NoCs for these heterogeneous 3D ICs. The core idea is a heterogeneous co-design of the NoC routing algorithm and router micro-architecture. Thereby, the level of heterogeneity is derived from the physical implications of the different technologies. The proposed systematic approach enables a simultaneous improvement in the NoC power consumption, silicon footprint, and performance by 17 %, 45 %, and 52 %, respectively.",
        "link": "https://dl.acm.org/doi/10.1145/3477206.3477457",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Projecting Robot Navigation Paths: Hardware and Software for Projected AR",
        "authors": "['Zhao Han', 'Jenna Parrillo', 'Alexander Wilkinson', 'Holly A. Yanco', 'Tom Williams']",
        "date": "March 2022",
        "source": "HRI '22: Proceedings of the 2022 ACM/IEEE International Conference on Human-Robot Interaction",
        "abstract": "For mobile robots, mobile manipulators, and autonomous vehicles to safely navigate around populous places such as streets and warehouses, human observers must be able to understand their navigation intent. One way to enable such understanding is by visualizing this intent through projections onto the surrounding environment. But despite the demonstrated effectiveness of such projections, no open codebase with an integrated hardware setup exists. In this work, we detail the empirical evidence for the effectiveness of such directional projections, and share a robot-agnostic implementation of such projections, coded in C++ using the widely-used Robot Operating System (ROS) and rviz. Additionally, we demonstrate a hardware configuration for deploying this software, using a Fetch robot, and briefly summarize a full-scale user study that motivates this configuration. The code, configuration files (roslaunch and rviz files), and documentation are freely available on GitHub at https://github.com/umhan35/arrow_projection.",
        "link": "https://dl.acm.org/doi/10.5555/3523760.3523843",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Adversarial Attack Mitigation Approaches Using RRAM-Neuromorphic Architectures",
        "authors": "['Siddharth Barve', 'Sanket Shukla', 'Sai Manoj Pudukotai Dinakarrao', 'Rashmi Jha']",
        "date": "June 2021",
        "source": "GLSVLSI '21: Proceedings of the 2021 on Great Lakes Symposium on VLSI",
        "abstract": "The rising trend and advancements in machine learning has resulted into its numerous applications in the field of computer vision, pattern recognition to providing security to hardware devices. Eventhough the proven achievements showcased by advancement in machine learning, one can exploit the vulnerabilities in those techniques by feeding adversaries. Adversarial samples are generated by well crafting and adding perturbations to the normal input samples. There exists majority of the software based adversarial attacks and defenses. In this paper, we demonstrate the effects of adversarial attacks on a reconfigurable RRAM-neuromorphic architecture with different learning algorithms and device characteristics. We also propose an integrated solution for mitigating the effects of the adversarial attack using the reconfigurable RRAM architecture.",
        "link": "https://dl.acm.org/doi/10.1145/3453688.3461757",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Online model swapping for architectural simulation",
        "authors": "['Patrick Lavin', 'Jeffrey Young', 'Richard Vuduc', 'Jonathan Beard']",
        "date": "May 2021",
        "source": "CF '21: Proceedings of the 18th ACM International Conference on Computing Frontiers",
        "abstract": "As systems and applications grow more complex, detailed computer architecture simulation takes an ever increasing amount of time. Longer simulation times result in slower design iterations which then force architects to use simpler models, such as spreadsheets, when they want to iterate quickly on a design. Simple models are not easy to work with though, as architects must rely on intuition to choose representative models, and the path from the simple models to a detailed hardware simulation is not always clear. In this work, we present a method of bridging the gap between simple and detailed simulation by monitoring simulation behavior online and automatically swapping out detailed models with simpler statistical approximations. We demonstrate the potential of our methodology by implementing it in the open-source simulator SVE-Cachesim to swap out the level one data cache (L1D) within a memory hierarchy. This proof of concept demonstrates that our technique can train simple models to match real program behavior in the L1D and can swap them in without destructive side-effects for the performance of downstream models. Our models introduce only 8% error in the overall cycle count, while being used for over 90% of the simulation and using models that require two to eight times less computation per cache access.",
        "link": "https://dl.acm.org/doi/10.1145/3457388.3458670",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "SwingNet: Ubiquitous Fine-Grained Swing Tracking Framework via Stochastic Neural Architecture Search and Adversarial Learning",
        "authors": "['Hong Jia', 'Jiawei Hu', 'Wen Hu']",
        "date": "None",
        "source": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies",
        "abstract": "Sports analytics in the wild (i.e., ubiquitously) is a thriving industry. Swing tracking is a key feature in sports analytics. Therefore, a centimeter-level tracking resolution solution is required. Recent research has explored deep neural networks for sensor fusion to produce consistent swing-tracking performance. This is achieved by combining the advantages of two sensor modalities (IMUs and depth sensors) for golf swing tracking. Here, the IMUs are not affected by occlusion and can support high sampling rates. Meanwhile, depth sensors produce significantly more accurate motion measurements than those produced by IMUs. Nevertheless, this method can be further improved in terms of accuracy and lacking information for different domains (e.g., subjects, sports, and devices). Unfortunately, designing a deep neural network with good performance is time consuming and labor intensive, which is challenging when a network model is deployed to be used in new settings. To this end, we propose a network based on Neural Architecture Search (NAS), called SwingNet, which is a regression-based automatic generated deep neural network via stochastic neural network search. The proposed network aims to learn the swing tracking feature for better prediction automatically. Furthermore, SwingNet features a domain discriminator by using unsupervised learning and adversarial learning to ensure that it can be adaptive to unobserved domains. We implemented SwingNet prototypes with a smart wristband (IMU) and smartphone (depth sensor), which are ubiquitously available. They enable accurate sports analytics (e.g., coaching, tracking, analysis and assessment) in the wild.Our comprehensive experiment shows that SwingNet achieves less than 10 cm errors of swing tracking with a subject-independent model covering multiple sports (e.g., golf and tennis) and depth sensor hardware, which outperforms state-of-the-art approaches.",
        "link": "https://dl.acm.org/doi/10.1145/3478082",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Architectural strategies for interoperability of software-intensive systems: practitioners' perspective",
        "authors": "['Pedro Henrique Dias Valle', 'Lina Garcés', 'Elisa Yumi Nakagawa']",
        "date": "March 2021",
        "source": "SAC '21: Proceedings of the 36th Annual ACM Symposium on Applied Computing",
        "abstract": "Background: Currently, it is becoming increasingly common in the construction of more complex systems through the integration of existing and operational systems. To correctly construct these systems they must address interoperability requirements at different levels, i.e., technical, semantic, syntactic, and organizational. Several architectural strategies (i.e., architectural styles, patterns, and tactics) can be reused during such systems integration. However, there is a lack of evidence to orient which strategies and how they could be used during software integration practice. Objective: To investigate how architectural strategies have been used in practice to promote the interoperability of software-intensive systems. Method: We planned and executed an online survey with practitioners from different parts of the world, most of them with more than five years of experience in software integration. Results: We identified: (i) the main architectural strategies used in practice to promote different interoperability types in software-intensive systems; (ii) the difficulty level perceived by practitioners for using these strategies in real projects; (iii) the quality attributes more negatively impacted by these strategies; (iv) practitioners' suggestions to improve integration processes of software-intensive systems; (vi) common technologies used to integrate systems; (vii) and challenges perceived by practitioners during software-intensive systems integration. Conclusions: It is important to develop guidelines to assist practitioners in systematically selecting and applying the most suitable architectural strategies to their integration projects, analyzing the benefits and drawbacks of each strategy regarding other important quality attributes, such as security, reliability, and performance.",
        "link": "https://dl.acm.org/doi/10.1145/3412841.3442015",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Applying Architectural Patterns for Parallel Programming: Solving a Matrix Multiplication",
        "authors": "['Jorge L. Ortega-Arjona']",
        "date": "July 2021",
        "source": "EuroPLoP'21: 26th European Conference on Pattern Languages of Programs",
        "abstract": "The Architectural Patterns for Parallel Programming is a set of patterns along with a method for designing the coordination of parallel software systems. Their application takes as input: (a) the available parallel hardware platform, (b) the available parallel programming language, and (c) the analysis of the problem as an algorithm and data. This paper presents the application of the architectural patterns within the method for solving the Matrix Multiplication. The method takes information from the problem analysis, selects an architectural pattern for the coordination, and provides some elements about its implementation.",
        "link": "https://dl.acm.org/doi/10.1145/3489449.3490011",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Tangled: A Conventional Processor Integrating A Quantum-Inspired Coprocessor",
        "authors": "['Henry Dietz']",
        "date": "August 2021",
        "source": "ICPP Workshops '21: 50th International Conference on Parallel Processing Workshop",
        "abstract": "Quantum computers use quantum physics phenomena to create specialized hardware that can efficiently execute algorithms operating on entangled superposed data. That hardware must be attached to and controlled by a conventional host computer. However, it can be argued that the main benefit thus far has been from reformulating problems to make use of entangled superpositions rather than from use of exotic physics mechanisms to perform the computation – such reformulations often have produced more efficient algorithms for conventional computers. Parallel bit pattern computing does not simulate quantum computing, but provides a way to use non-quantum, bit-level, massively-parallel, SIMD hardware to efficiently execute a broad class of algorithms leveraging superposition and entanglement.  Just as quantum hardware needs a conventional host, so to does parallel bit pattern hardware. Thus, the current work presents Tangled: a simple proof-of-concept conventional processor design incorporating a tightly-coupled interface to an integrated parallel bit pattern co-processor (Qat). The feasibility of this type of interface between conventional and quantum-inspired computation was investigated by construction of an instruction set, building complete Verilog designs for pipelined implementations, and by observing the effectiveness of the interface in executing simple quantum-inspired algorithms involving operations on entangled, superposed, values.",
        "link": "https://dl.acm.org/doi/10.1145/3458744.3474044",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "DTQAtten: leveraging <u>d</u>ynamic <u>t</u>oken-based quantization for efficient <u>atten</u>tion architecture",
        "authors": "['Tao Yang', 'Dongyue Li', 'Zhuoran Song', 'Yilong Zhao', 'Fangxin Liu', 'Zongwu Wang', 'Zhezhi He', 'Li Jiang']",
        "date": "March 2022",
        "source": "DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe",
        "abstract": "Models based on the attention mechanism, i.e. transformers, have shown extraordinary performance in Natural Language Processing (NLP) tasks. However, their memory footprint, inference latency, and power consumption are still prohibitive for efficient inference at edge devices, even at data centers. To tackle this issue, we present an algorithm-architecture co-design with dynamic and mixed-precision quantization, DTQAtten. We present empirically that the tolerance to the noise varies from token to token in attention-based NLP models. This finding leads us to quantize different tokens with mixed levels of bits. Thus, we design a compression framework that (i) dynamically quantizes tokens while they are forwarded in the models and (ii) jointly determines the ratio of each precision. Moreover, due to the dynamic mixed-precision tokens caused by our framework, previous matrix-multiplication accelerators (e.g. systolic array) cannot effectively exploit the benefit of the compressed attention computation. We thus design our accelerator with the variable-speed systolic array (VSSA) and propose an effective optimization strategy to alleviate the pipeline-stall problem in VSSA without hardware overhead. We conduct experiments with existing attention-based NLP models, including BERT and GPT-2 on various language tasks. Our results show that DTQAtten outperforms the previous neural network accelerator Eyeriss by 13.12× in terms of speedup and 3.8× in terms of energy-saving. Compared with the state-of-the-art attention accelerator SpAtten, our DTQAtten achieves at least 2.65× speedup and 3.38× energy efficiency improvement.",
        "link": "https://dl.acm.org/doi/10.5555/3539845.3540016",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "BasicBlocker: ISA Redesign to Make Spectre-Immune CPUs Faster",
        "authors": "['Jan Philipp Thoma', 'Jakob Feldtkeller', 'Markus Krausz', 'Tim Güneysu', 'Daniel J. Bernstein']",
        "date": "October 2021",
        "source": "RAID '21: Proceedings of the 24th International Symposium on Research in Attacks, Intrusions and Defenses",
        "abstract": "Recent research has revealed an ever-growing class of microarchitectural attacks that exploit speculative execution, a standard feature in modern processors. Proposed and deployed countermeasures involve a variety of compiler updates, firmware updates, and hardware updates. None of the deployed countermeasures have convincing security arguments, and many of them have already been broken.  The obvious way to simplify the analysis of speculative-execution attacks is to eliminate speculative execution. This is normally dismissed as being unacceptably expensive, but the underlying cost analyses consider only software written for current instruction-set architectures, so they do not rule out the possibility of a new instruction-set architecture providing acceptable performance without speculative execution. A new ISA requires compiler and hardware updates, but these are happening in any case.  This paper introduces BasicBlocker, a generic ISA modification that works for all common ISAs and that allows non-speculative CPUs to obtain most of the performance benefit that would have been provided by speculative execution. To demonstrate the feasibility of BasicBlocker, this paper defines a variant of the RISC-V ISA called BBRISC-V and provides a thorough evaluation on both a 5-stage in-order soft core and a superscalar out-of-order processor using an associated compiler and a variety of benchmarks.",
        "link": "https://dl.acm.org/doi/10.1145/3471621.3471857",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "BTMMU: an efficient and versatile cross-ISA memory virtualization",
        "authors": "['Kele Huang', 'Fuxin Zhang', 'Cun Li', 'Gen Niu', 'Junrong Wu', 'Tianyi Liu']",
        "date": "April 2021",
        "source": "VEE 2021: Proceedings of the 17th ACM SIGPLAN/SIGOPS International Conference on Virtual Execution Environments",
        "abstract": "Full system dynamic binary translation (DBT) has many important applications, but it is typically much slower than the native host. One major overhead in full system DBT comes from cross-ISA memory virtualization, where multi-level memory address translation is needed to map guest virtual address into host physical address. Like the SoftMMU used in the popular open-source emulator QEMU, software-based memory virtualization solutions are not efficient. Meanwhile, mature techniques for same-ISA virtualization such as shadow page table or second level address translation are not directly applicable due to cross-ISA difficulties. Some previous studies achieved significant speedup by utilizing existing hardware (TLB or virtualization hardware) of the host. However, since the hardware is not designed with cross-ISA in mind, those solutions had some limitations that were hard to overcome. Most of them only supported guests with smaller virtual address space than the host. Some supported only guests with the same page size. And some did not support privileged memory accesses.  This paper proposes a new solution named BTMMU (Binary Translation Memory Management Unit). BTMMU composes of a low-cost hardware extension of host MMU, a kernel module and a patched QEMU version. BTMMU is able to solve most known limitations of previous hardware-assisted solutions and thus versatile enough for real deployments. Meanwhile, BTMMU achieves high efficiency by directly accessing guest address space, implementing shadow page table in kernel module, utilizing dedicated entrance for guest-related MMU exceptions and various software optimizations. Evaluations on SPEC CINT2006 benchmark suite and some real-world applications show that BTMMU achieves 1.40x and 1.36x speedup on IA32-to-MIPS64 and X86_64-to-MIPS64 configurations respectively when comparing with the base QEMU version. The result is compared to a representative previous work and shows its advantage.",
        "link": "https://dl.acm.org/doi/10.1145/3453933.3454015",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "TAMA: Turn-aware Mapping and Architecture – A Power-efficient Network-on-Chip Approach",
        "authors": "['Rashid Aligholipour', 'Mohammad Baharloo', 'Behnam Farzaneh', 'Meisam Abdollahi', 'Ahmad Khonsari']",
        "date": "None",
        "source": "ACM Transactions on Embedded Computing Systems",
        "abstract": "Nowadays, static power consumption in chip multiprocessor (CMP) is the most crucial concern of chip designers. Power-gating is an effective approach to mitigate static power consumption particularly in low utilization. Network-on-Chip (NoC) as the backbone of multi- and many-core chips has no exception. Previous state-of-the-art techniques in power-gating desire to decrease static power consumption alongside the lack of diminution in performance of NoC. However, maintaining the performance and utilization of the power-gating approach has not yet been addressed very well. In this article, we propose TAMA (Turn-Aware Mapping & Architecture) as an effective method to boost the performance of the TooT method that was only powering on a router during turning pass or packet injection. In other words, in the TooT method, straight and eject packets pass the router via a bypass route without powering on the router. By employing meta-heuristic approaches (Genetic and Ant Colony algorithms), we develop a specific application mapping that attempts to decrease the number of turns through interconnection networks. Accordingly, the average latency of packet transmission decreases due to fewer turns. Also, by powering on turn routers in advance with lightweight hardware, the latency of sending packets diminishes. The experimental results demonstrate that our proposed approach, i.e., TAMA achieves more than 13% reduction in packet latency of NoC in comparison with TooT. Besides the packet latency, the power consumption of TAMA is reduced by about 87% compared to the traditional approach.",
        "link": "https://dl.acm.org/doi/10.1145/3462700",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "RECOIN: A Low-Power Processing-in-ReRAM Architecture for Deformable Convolution",
        "authors": "['Cheng Chu', 'Fan Chen', 'Dawen Xu', 'Ying Wang']",
        "date": "June 2021",
        "source": "GLSVLSI '21: Proceedings of the 2021 on Great Lakes Symposium on VLSI",
        "abstract": "The recent proposed Deformable Convolutional Networks (DCNs)greatly enhance the performance of conventional Convolutional Neural Networks (CNNs) on vision recognition tasks by allowing flexible input sampling during inference runtime. DCNs introduce an additional convolutional layer for adaptive sampling offset generation, followed by a bilinear interpolation (BLI) algorithm to integerize the generated non-integer offset values. Finally, a regular convolution is performed on the loaded input pixels. Compared with conventional CNNs, DCN demonstrated significantly increased computational complexity and irregular input-dependentmemory access patterns, making it a great challenge for deploying DCNs onto edge devices for real-time computer vision tasks. In this work, we propose RECOIN, a processing-in-memory (PIM) architecture, which supports DCN inference on resistive memory (ReRAM)crossbars, thus making the first DCN inference accelerator possible. We present a novel BLI processing engine that leverage both row-and column-oriented computation for in-situ BLI calculation. Amapping scheme and an address converter are particular designed to accommodate the intensive computation and irregular data access. We implement the DCN inference in a 4-stage pipeline and evaluate the effectiveness of RECOIN on six DCN models. Experimental results show RECOIN achieves respectively 225×and 17.4×improvement in energy efficiency compared to general-purpose CPU and GPU. Compared to two state-of-the-art ASIC accelerators, RECOIN achieve 26.8× and 20.4× speedup respectively.",
        "link": "https://dl.acm.org/doi/10.1145/3453688.3461480",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "AgEBO-tabular: joint neural architecture and hyperparameter search with autotuned data-parallel training for tabular data",
        "authors": "['Romain Égelé', 'Prasanna Balaprakash', 'Isabelle Guyon', 'Venkatram Vishwanath', 'Fangfang Xia', 'Rick Stevens', 'Zhengying Liu']",
        "date": "November 2021",
        "source": "SC '21: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",
        "abstract": "Developing high-performing predictive models for large tabular data sets is a challenging task. Neural architecture search (NAS) is an AutoML approach that generates and evaluates multiple neural networks with different architectures concurrently to automatically discover an high performing model. A key issue in NAS, particularly for large data sets, is the large computation time required to evaluate each generated architecture. While data-parallel training has the potential to address this issue, a straightforward approach can result in significant loss of accuracy. To that end, we develop AgEBO-Tabular, which combines Aging Evolution (AE) to search over neural architectures and asynchronous Bayesian optimization (BO) to search over hyperparameters to adapt data-parallel training. We evaluate the efficacy of our approach on two large predictive modeling tabular data sets from the Exascale Computing Project-CANcer Distributed Learning Environment (ECP-CANDLE).",
        "link": "https://dl.acm.org/doi/10.1145/3458817.3476203",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "AgEBO-tabular: joint neural architecture and hyperparameter search with autotuned data-parallel training for tabular data",
        "authors": "['Romain Égelé', 'Prasanna Balaprakash', 'Isabelle Guyon', 'Venkatram Vishwanath', 'Fangfang Xia', 'Rick Stevens', 'Zhengying Liu']",
        "date": "November 2021",
        "source": "SC '21: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",
        "abstract": "Developing high-performing predictive models for large tabular data sets is a challenging task. Neural architecture search (NAS) is an AutoML approach that generates and evaluates multiple neural networks with different architectures concurrently to automatically discover an high performing model. A key issue in NAS, particularly for large data sets, is the large computation time required to evaluate each generated architecture. While data-parallel training has the potential to address this issue, a straightforward approach can result in significant loss of accuracy. To that end, we develop AgEBO-Tabular, which combines Aging Evolution (AE) to search over neural architectures and asynchronous Bayesian optimization (BO) to search over hyperparameters to adapt data-parallel training. We evaluate the efficacy of our approach on two large predictive modeling tabular data sets from the Exascale Computing Project-CANcer Distributed Learning Environment (ECP-CANDLE).",
        "link": "https://dl.acm.org/doi/10.1145/3458817.3476203",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Gibbon: efficient co-exploration of NN model and processing-in-memory architecture",
        "authors": "['Hanbo Sun', 'Chenyu Wang', 'Zhenhua Zhu', 'Xuefei Ning', 'Guohao Dai', 'Huazhong Yang', 'Yu Wang']",
        "date": "March 2022",
        "source": "DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe",
        "abstract": "The memristor-based Processing-In-Memory (PIM) architectures have shown great potential to boost the computing energy efficiency of Neural Networks (NNs). Existing work concentrates on hardware architecture design and algorithm-hardware co-optimization, but neglects the non-negligible impact of the correlation between NN models and PIM architectures. To ensure high accuracy and energy efficiency, it is important to co-design the NN model and PIM architecture. However, on the one hand, the co-exploration space of NN model and PIM architecture is extremely tremendous, making searching for the optimal results difficult. On the other hand, during the co-exploration process, PIM simulators pose a heavy computational burden and runtime overhead for evaluation. To address these problems, in this paper, we propose an efficient co-exploration framework for the NN model and PIM architecture, named Gibbon. In Gibbon, we propose an evolutionary search algorithm with adaptive parameter priority, which focuses on subspace of high priority parameters and alleviates the problem of vast co-design space. Besides, we design a Recurrent Neural Network (RNN) based predictor for accuracy and hardware performances. It substitutes for a large part of the PIM simulator workload and reduces the long simulation time. Experimental results show that the proposed co-exploration framework can find better NN models and PIM architectures than existing studies in only seven GPU hours (8.4~41.3 × speedup). At the same time, Gibbon can improve the accuracy of co-design results by 10.7% and reduce the energy-delay-product by 6.48 × compared with existing work.",
        "link": "https://dl.acm.org/doi/10.5555/3539845.3540049",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "On resilience of security-oriented error detecting architectures against power attacks: a theoretical analysis",
        "authors": "['Osnat Keren', 'Ilia Polian']",
        "date": "May 2021",
        "source": "CF '21: Proceedings of the 18th ACM International Conference on Computing Frontiers",
        "abstract": "It has been previously shown that hardware implementation of fault attack countermeasures based on error-detecting codes (EDCs) can make the circuit more vulnerable to power analysis attacks. We revisit this finding and show that the hypothesis space can grow significantly when a state-of-the-art security-oriented robust EDC is properly crafted. We use the Roth-Karp decomposition as an analytical tool to prove that by a simple re-ordering of the EDC's bits, the number of extra bits needed to formulate the hypotheses becomes so large that power analysis (that tries to exploit additional information from the redundant bits) is rendered infeasible.",
        "link": "https://dl.acm.org/doi/10.1145/3457388.3458867",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Jupiter: a networked computing architecture",
        "authors": "['Pradipta Ghosh', 'Quynh Nguyen', 'Pranav K Sakulkar', 'Jason A. Tran', 'Aleksandra Knezevic', 'Jiatong Wang', 'Zhifeng Lin', 'Bhaskar Krishnamachari', 'Murali Annavaram', 'Salman Avestimehr']",
        "date": "December 2021",
        "source": "UCC '21: Proceedings of the 14th IEEE/ACM International Conference on Utility and Cloud Computing Companion",
        "abstract": "Modern latency-sensitive applications such as real-time multi-camera video analytics require networked computing to meet the time constraints. We present Jupiter, an open-source networked computing system that inputs a Directed Acyclic Graph (DAG)-based computational task graph to efficiently distribute the tasks among a set of networked compute nodes and orchestrates the execution thereafter. This Kubernetes container-orchestration-based system includes a range of profilers: network profilers, resource profilers, and execution time profilers; to support both centralized and decentralized scheduling algorithms. While centralized scheduling algorithms with global knowledge have been popular among the grid/cloud computing community, we argue that a distributed scheduling approach is better suited for networked computing due to lower communication and computation overhead in the face of network dynamics. We propose a new class of distributed scheduling algorithms called WAVE and show that despite using more localized knowledge, the WAVE algorithm can match the performance of a well-known centralized scheduling algorithm called Heterogeneous Earliest Finish Time (HEFT). To this, we present a set of real-world experiments on two separate testbeds: (1) a worldwide network of 90 cloud computers across eight cities and (2) a cluster of 30 Raspberry pi nodes.",
        "link": "https://dl.acm.org/doi/10.1145/3492323.3495630",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "η-LSTM: co-designing highly-efficient large LSTM training via exploiting memory-saving and architectural design opportunities",
        "authors": "['Xingyao Zhang', 'Haojun Xia', 'Donglin Zhuang', 'Hao Sun', 'Xin Fu', 'Michael B. Taylor', 'Shuaiwen Leon Song']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "Recently, the recurrent neural network, or its most popular type---the Long Short Term Memory (LSTM) network---has achieved great success in a broad spectrum of real-world application domains, such as autonomous driving, natural language processing, sentiment analysis, and epidemiology. Due to the complex features of the real-world tasks, current LSTM models become increasingly bigger and more complicated for enhancing the learning ability and prediction accuracy. However, through our in-depth characterization on the state-of-the-art general-purpose deep-learning accelerators, we observe that the LSTM training execution grows inefficient in terms of storage, performance, and energy consumption, under an increasing model size. With further algorithmic and architectural analysis, we identify the root cause for large LSTM training inefficiency: massive intermediate variables. To enable a highly-efficient LSTM training solution for the ever-growing model size, we exploit some unique memory-saving and performance improvement opportunities from the LSTM training procedure, and leverage them to propose the first cross-stack training solution, η-LSTM, for large LSTM models. η-LSTM comprises both software-level and hardware-level innovations that effectively lower the memory footprint upper-bound and excessive data movements during large LSTM training, while also drastically improving training performance and energy efficiency. Experimental results on six real-world large LSTM training benchmarks demonstrate that η-LSTM reduces the required memory footprint by an average of 57.5% (up to 75.8%) and brings down the data movements for weight matrices, activation data, and intermediate variables by 40.9%, 32.9%, and 80.0%, respectively. Furthermore, it outperforms the state-of-the-art GPU implementation for LSTM training by an average of 3.99× (up to 5.73×) on performance and 2.75× (up to 4.25x ) on energy. We hope this work can shed some light on how to design high logic utilization for future NPUs.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00051",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Toward systematic architectural design of near-term trapped ion quantum computers",
        "authors": "['Prakash Murali', 'Dripto M. Debroy', 'Kenneth R. Brown', 'Margaret Martonosi']",
        "date": "March 2022",
        "source": "Communications of the ACM",
        "abstract": "Trapped ions (TIs) are a leading candidate for building Noisy Intermediate-Scale Quantum (NISQ) hardware. TI qubits have fundamental advantages over other technologies, featuring high qubit quality, coherence time, and qubit connectivity. However, current TI systems are small in size and typically use a single trap architecture, which has fundamental scalability limitations. To progress toward the next major milestone of 50--100 qubit TI devices, a modular architecture termed the Quantum Charge Coupled Device (QCCD) has been proposed. In a QCCD-based TI device, small traps are connected through ion shuttling. While the basic hardware components for such devices have been demonstrated, building a 50--100 qubit system is challenging because of a wide range of design possibilities for trap sizing, communication topology, and gate implementations and the need to match diverse application resource requirements.Toward realizing QCCD-based TI systems with 50--100 qubits, we perform an extensive application-driven architectural study evaluating the key design choices of trap sizing, communication topology, and operation implementation methods. To enable our study, we built a design toolflow, which takes a QCCD architecture's parameters as input, along with a set of applications and realistic hardware performance models. Our toolflow maps the applications onto the target device and simulates their execution to compute metrics such as application run time, reliability, and device noise rates. Using six applications and several hardware design points, we show that trap sizing and communication topology choices can impact application reliability by up to three orders of magnitude. Microarchitectural gate implementation choices influence reliability by another order of magnitude. From these studies, we provide concrete recommendations to tune these choices to achieve highly reliable and performant application executions. With industry and academic efforts underway to build TI devices with 50-100 qubits, our insights have the potential to influence QC hardware in the near future and accelerate the progress toward practical QC systems.",
        "link": "https://dl.acm.org/doi/10.1145/3511064",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Voltage-Based Covert Channels Using FPGAs",
        "authors": "['Dennis R. E. Gnad', 'Cong Dang Khoa Nguyen', 'Syed Hashim Gillani', 'Mehdi B. Tahoori']",
        "date": "None",
        "source": "ACM Transactions on Design Automation of Electronic Systems",
        "abstract": "Field Programmable Gate Arrays (FPGAs) are increasingly used in cloud applications and being integrated into Systems-on-Chip. For these systems, various side-channel attacks on cryptographic implementations have been reported, motivating one to apply proper countermeasures. Beyond cryptographic implementations, maliciously introduced covert channel receivers and transmitters can allow one to exfiltrate other secret information from the FPGA. In this article, we present a fast covert channel on FPGAs, which exploits the on-chip power distribution network. This can be achieved without any logical connection between the transmitter and receiver blocks. Compared to a recently published covert channel with an estimated 4.8 Mbit/s transmission speed, we show 8 Mbit/s transmission and reduced errors from around 3% to less than 0.003%. Furthermore, we demonstrate proper transmissions of word-size messages and test the channel in the presence of noise generated from other residing tenants’ modules in the FPGA. When we place and operate other co-tenant modules that require 85% of the total FPGA area, the error rate increases to 0.02%, depending on the platform and setup. This error rate is still reasonably low for a covert channel. Overall, the transmitter and receiver work with less than 3–5% FPGA LUT resources together. We also show the feasibility of other types of covert channel transmitters, in the form of synchronous circuits within the FPGA.",
        "link": "https://dl.acm.org/doi/10.1145/3460229",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Towards Trusted IoT Sensing Systems: Implementing PUF as Secure Key Generator for Root of Trust and Message Authentication Code",
        "authors": "['Kota Yoshida', 'Kuniyasu Suzaki', 'Takeshi Fujino']",
        "date": "October 2021",
        "source": "HASP '21: Proceedings of the 10th International Workshop on Hardware and Architectural Support for Security and Privacy",
        "abstract": "Trustworthy sensor data is important for IoT sensing systems. As such, these systems need to guarantee that the sensor data is acquired by the correct device and has not been tampered with. However, IoT sensing systems can be quite complex and are often composed of multiple components, i.e., a main device and subordinate sensors. The main device is responsible for gathering and processing the data from the subordinate sensor and reports the result to a server. In order to guarantee data correctness, we introduce two types of physically unclonable function (PUF): one for the main device and one for the subordinate sensor. The main device has a trusted execution environment (TEE) for critical processing, and the correctness of the TEE is guaranteed by remote attestation based on a PUF. The subordinate sensor sends the sensor data to the main device with a message authentication code (MAC) based on a PUF. We implemented a trusted IoT sensing system using a RISC-V Keystone with a PRINCE Glitch PUF for the main device and a Raspberry Pi that simulates a CMOS image sensor PUF for the subordinate sensor.",
        "link": "https://dl.acm.org/doi/10.1145/3505253.3505258",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "SPARROW: a low-cost hardware/software co-designed SIMD microarchitecture for AI operations in space processors",
        "authors": "['Marc Solé Bonet', 'Leonidas Kosmidis']",
        "date": "March 2022",
        "source": "DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe",
        "abstract": "Recently there is an increasing interest in the use of artificial intelligence for on-board processing as indicated by the latest space missions, which cannot be satisfied by existing low-performance space-qualified processors. Although COTS AI accelerators can provide the required performance, they are not designed to meet space requirements. In this work, we co-design a low-cost SIMD micro-architecture integrated in a space qualified processor, which can significantly increase its performance. Our solution has no impact on the processor's 100 MHz frequency and consumes minimal area thanks to its innovative design compared to conventional vector micro-architectures. For the minimum configuration of our baseline space processor, our results indicate a performance boost of up to 9.3× for commonly used AI-related and image processing algorithms and 5.5× faster for a complex, space-relevant inference application with just 30% area increase.",
        "link": "https://dl.acm.org/doi/10.5555/3539845.3540112",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "No-FAT: architectural support for low overhead memory safety checks",
        "authors": "['Mohamed Tarek Ibn Ziad', 'Miguel A. Arroyo', 'Evgeny Manzhosov', 'Ryan Piersma', 'Simha Sethumadhavan']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "Memory safety continues to be a significant software reliability and security problem, and low overhead and low complexity hardware solutions have eluded computer designers. In this paper, we explore a pathway to deployable memory safety defenses. Our technique builds on a recent trend in software: the usage of binning memory allocators. We observe that if memory allocation sizes (e.g., malloc sizes) are made an architectural feature, then it is possible to overcome many of the thorny issues with traditional approaches to memory safety such as compatibility with unsecured software and significant performance degradation. We show that our architecture, No-FAT, incurs an overhead of 8% on SPEC CPU2017 benchmarks, and our VLSI measurements show low power and area overheads. Finally, as No-FAT's hardware is aware of the memory allocation sizes, it effectively mitigates certain speculative attacks (e.g., Spectre-V1) with no additional cost. When our solution is used for pre-deployment fuzz testing it can improve fuzz testing bandwidth by an order of magnitude compared to state-of-the-art approaches.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00076",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "How to exploit sparsity in RNNs on event-driven architectures",
        "authors": "['Jarno Brils', 'Luc Waeijen', 'Arash Pourtaherian']",
        "date": "November 2021",
        "source": "SCOPES '21: Proceedings of the 24th International Workshop on Software and Compilers for Embedded Systems",
        "abstract": "Event-driven architectures have been shown to provide low-power, low-latency artificial neural network (ANN) inference. This is especially beneficial on Edge devices, particularly when combined with sparse execution. Recurrent neural networks (RNNs) are ANNs that emulate memory. Their recurrent connection enables the reuse of previous output for the generation of new output. However, when trying to use RNNs in a sparse context on event-driven architectures, novel challenges in synchronization and the usage of sparse data are encountered. In this work, these challenges are systematically analyzed, and mechanisms to overcome them are proposed. Experimental results of a monocular depth estimation use case on the NeuronFlow architecture show that sparsity in RNNs can be exploited effectively on event-driven architectures.",
        "link": "https://dl.acm.org/doi/10.1145/3493229.3493302",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "GraphAttack: Optimizing Data Supply for Graph Applications on In-Order Multicore Architectures",
        "authors": "['Aninda Manocha', 'Tyler Sorensen', 'Esin Tureci', 'Opeoluwa Matthews', 'Juan L. Aragón', 'Margaret Martonosi']",
        "date": "None",
        "source": "ACM Transactions on Architecture and Code Optimization",
        "abstract": "Graph structures are a natural representation of important and pervasive data. While graph applications have significant parallelism, their characteristic pointer indirect loads to neighbor data hinder scalability to large datasets on multicore systems. A scalable and efficient system must tolerate latency while leveraging data parallelism across millions of vertices. Modern Out-of-Order (OoO) cores inherently tolerate a fraction of long latencies, but become clogged when running severely memory-bound applications. Combined with large power/area footprints, this limits their parallel scaling potential and, consequently, the gains that existing software frameworks can achieve. Conversely, accelerator and memory hierarchy designs provide performant hardware specializations, but cannot support diverse application demands.To address these shortcomings, we present GraphAttack, a hardware-software data supply approach that accelerates graph applications on in-order multicore architectures. GraphAttack proposes compiler passes to (1) identify idiomatic long-latency loads and (2) slice programs along these loads into data Producer/ Consumer threads to map onto pairs of parallel cores. Each pair shares a communication queue; the Producer asynchronously issues long-latency loads, whose results are buffered in the queue and used by the Consumer. This scheme drastically increases memory-level parallelism (MLP) to mitigate latency bottlenecks. In equal-area comparisons, GraphAttack outperforms OoO cores, do-all parallelism, prefetching, and prior decoupling approaches, achieving a 2.87× speedup and 8.61× gain in energy efficiency across a range of graph applications. These improvements scale; GraphAttack achieves a 3× speedup over 64 parallel cores. Lastly, it has pragmatic design principles; it enhances in-order architectures that are gaining increasing open-source support.",
        "link": "https://dl.acm.org/doi/10.1145/3469846",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "DeepDive: An Integrative Algorithm/Architecture Co-Design for Deep Separable Convolutional Neural Networks",
        "authors": "['Mohammadreza Baharani', 'Ushma Sunil', 'Kaustubh Manohar', 'Steven Furgurson', 'Hamed Tabkhi']",
        "date": "June 2021",
        "source": "GLSVLSI '21: Proceedings of the 2021 on Great Lakes Symposium on VLSI",
        "abstract": "Deep Separable Convolutional Neural Network (DSCNN) has become the emerging paradigm by offering modular networks with structural sparsity to achieve higher accuracy with relatively lower operations and parameters. However, there is a lack of customized architectures that can provide flexible solutions that fit the sparsity of the DSCNNs. This paper introduces DeepDive, a fully-functional vertical co-design framework, for power-efficient implementation of DSCNNs on edge FPGAs. DeepDive's architecture supports crucial heterogeneous Compute Units (CUs) to fully support DSCNNs with various convolutional operators interconnected with structural sparsity. It offers FPGA-aware training and online quantization combined with modular synthesizable C++ CUs, customized for DSCNNs. The execution results on Xilinx's ZCU102 FPGA board demonstrate 47.4 and 233.3 FPS/Watt for MobileNet-V2 and a compact version of EfficientNet, respectively, as two state-of-the-art depthwise separable CNNs. These comparisons showcase how DeepDive improves FPS/Watt by 2.2× and 1.51× over Jetson Nano high and low power modes, respectively. It also enhances FPS/Watt by about 2.27× and 37.25× over two other FPGA implementations.",
        "link": "https://dl.acm.org/doi/10.1145/3453688.3461485",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A uniform latency model for DNN accelerators with diverse architectures and dataflows",
        "authors": "['Linyan Mei', 'Huichu Liu', 'Tony Wu', 'H. Ekin Sumbul', 'Marian Verhelst', 'Edith Beigne']",
        "date": "March 2022",
        "source": "DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe",
        "abstract": "In the early design phase of a Deep Neural Network (DNN) acceleration system, fast energy and latency estimation are important to evaluate the optimality of different design candidates on algorithm, hardware, and algorithm-to-hardware mapping, given the gigantic design space. This work proposes a uniform intra-layer analytical latency model for DNN accelerators that can be used to evaluate diverse architectures and dataflows. It employs a 3-step approach to systematically estimate the latency breakdown of different system components, capture the operation state of each memory component, and identify stall-induced performance bottlenecks. To achieve high accuracy, different memory attributes, operands' memory sharing scenarios, as well as dataflow implications have been taken into account. Validation against an in-house taped-out accelerator across various DNN layers has shown an average latency model accuracy of 94.3%. To showcase the capability of the proposed model, we carry out 3 case studies to assess respectively the impact of mapping, workloads, and diverse hardware architectures on latency, driving design insights for algorithm-hardware-mapping co-optimization.",
        "link": "https://dl.acm.org/doi/10.5555/3539845.3539904",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "MG-Join: A Scalable Join for Massively Parallel Multi-GPU Architectures",
        "authors": "['Johns Paul', 'Shengliang Lu', 'Bingsheng He', 'Chiew Tong Lau']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "The recent scale-up of GPU hardware through the integration of multiple GPUs into a single machine and the introduction of higher bandwidth interconnects like NVLink 2.0 has enabled new opportunities of relational query processing on multiple GPUs. However, due to the unique characteristics of GPUs and the interconnects, existing hash join implementations spend up to 66% of their execution time moving the data between the GPUs and achieve lower than 50% utilization of the newer high bandwidth interconnects. This leads to extremely poor scalablity of hash join performance on multiple GPUs, which can be slower than the performance on a single GPU. In this paper, we propose MG-Join, a scalable partitioned hash join implementation on multiple GPUs of a single machine. In order to effectively improve the bandwidth utilization, we develop a novel multi-hop routing for cross-GPU communication that adaptively chooses the efficient route for each data flow to minimize congestion. Our experiments on the DGX-1 machine show that MG-Join helps significantly reduce the communication overhead and achieves up to 97% utilization of the bisection bandwidth of the interconnects, resulting in significantly better scalability. Overall, MG-Join outperforms the state-of-the-art hash join implementations by up to 2.5x. MG-Join further helps improve the overall performance of TPC-H queries by up to 4.5x over multi-GPU version of an open-source commercial GPU database Omnisci.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3457254",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Fifer: Practical Acceleration of Irregular Applications on Reconfigurable Architectures",
        "authors": "['Quan M. Nguyen', 'Daniel Sanchez']",
        "date": "October 2021",
        "source": "MICRO '21: MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture",
        "abstract": "Coarse-grain reconfigurable arrays (CGRAs) can achieve much higher performance and efficiency than general-purpose cores, approaching the performance of a specialized design while retaining programmability. Unfortunately, CGRAs have so far only been effective on applications with regular compute patterns. However, many important workloads like graph analytics, sparse linear algebra, and databases, are irregular applications with unpredictable access patterns and control flow. Since CGRAs map computation statically to a spatial fabric of functional units, irregular memory accesses and control flow cause frequent stalls and load imbalance.  We present Fifer, an architecture and compilation technique that makes irregular applications efficient on CGRAs. Fifer first decouples irregular applications into a feed-forward network of pipeline stages. Each resulting stage is regular and can efficiently use the CGRA fabric. However, irregularity causes stages to have widely varying loads, resulting in high load imbalance if they execute spatially in a conventional CGRA. Fifer solves this by introducing dynamic temporal pipelining: it time-multiplexes multiple stages onto the same CGRA, and dynamically schedules stages to avoid load imbalance. Fifer makes time-multiplexing fast and cheap to quickly respond to load imbalance while retaining the efficiency and simplicity of a CGRA design. We show that Fifer improves performance by gmean 2.8 × (and up to 5.5 ×) over a conventional CGRA architecture (and by gmean 17 × over an out-of-order multicore) on a variety of challenging irregular applications.",
        "link": "https://dl.acm.org/doi/10.1145/3466752.3480048",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Albireo: energy-efficient acceleration of convolutional neural networks via silicon photonics",
        "authors": "['Kyle Shiflett', 'Avinash Karanth', 'Razvan Bunescu', 'Ahmed Louri']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "With the end of Dennard scaling, highly-parallel and specialized hardware accelerators have been proposed to improve the throughput and energy-efficiency of deep neural network (DNN) models for various applications. However, collective data movement primitives such as multicast and broadcast that are required for multiply-and-accumulate (MAC) computation in DNN models are expensive, and require excessive energy and latency when implemented with electrical networks. This consequently limits the scalability and performance of electronic hardware accelerators. Emerging technology such as silicon photonics can inherently provide efficient implementation of multicast and broadcast operations, making photonics more amenable to exploit parallelism within DNN models. Moreover, when coupled with other unique features such as low energy consumption, high channel capacity with wavelength-division multiplexing (WDM), and high speed, silicon photonics could potentially provide a viable technology for scaling DNN acceleration. In this paper, we propose Albireo, an analog photonic architecture for scaling DNN acceleration. By characterizing photonic devices such as microring resonators (MRRs) and Mach-Zehnder modulators (MZM) using photonic simulators, we develop realistic device models and outline their capability for system level acceleration. Using the device models, we develop an efficient broadcast combined with multicast data distribution by leveraging parameter sharing through unique WDM dot product processing. We evaluate the energy and throughput performance of Albireo on DNN models such as ResNet18, MobileNet and VGG16. When compared to current state-of-the-art electronic accelerators, Albireo increases throughput by 110 X, and improves energy-delay product (EDP) by an average of 74 X with current photonic devices. Furthermore, by considering moderate and aggressive photonic scaling, the proposed Albireo design shows that EDP can be reduced by at least 229 X.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00072",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Metaprogramming with combinators",
        "authors": "['Mahshid Shahmohammadian', 'Geoffrey Mainland']",
        "date": "October 2021",
        "source": "GPCE 2021: Proceedings of the 20th ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences",
        "abstract": "There are a wide array of methods for writing code generators. We advocate for a point in the design space, which we call metaprogramming with combinators, where programmers use (and write) combinator libraries that directly manipulate object language terms. The key language feature that makes this style of programming palatable is quasiquotation. Our approach leverages quasiquotation and other host language features to provide what is essentially a rich, well-typed macro language. Unlike other approaches, metaprogramming with combinators allows full control over generated code, thereby also providing full control over performance and resource usage. This control does not require sacrificing the ability to write high-level abstractions. We demonstrate metaprogramming with combinators through several code generators written in Haskell that produce VHDL targeted to Xilinx FPGAs.",
        "link": "https://dl.acm.org/doi/10.1145/3486609.3487198",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "CoPIM: a concurrency-aware PIM workload offloading architecture for graph applications",
        "authors": "['Liang Yan', 'Mingzhe Zhang', 'Rujia Wang', 'Xiaoming Chen', 'Xingqi Zou', 'Xiaoyang Lu', 'Yinhe Han', 'Xian-He Sun']",
        "date": "July 2021",
        "source": "ISLPED '21: Proceedings of the ACM/IEEE International Symposium on Low Power Electronics and Design",
        "abstract": "Processing-in-Memory (PIM) is considered a promising solution to improve the performance of graph-computing applications by minimizing the data movement between the host and memory. Which workload to offload and how to offload it to PIM logic determine whether the PIM architecture is well utilized. Offloading too much or too little workload from the host processor to the PIM side could hurt overall performance. On the other hand, the offloading granularity needs to be representative without losing generality. In this paper, we present CoPIM, a novel PIM workload offloading architecture that can dynamically determine which portion of the graph workload can benefit more from PIM-side computation. CoPIM focuses on the loop code blocks of graph applications and evaluates the necessity of offloading based on a concurrent memory access model. We also provide detailed architectural designs to support the offloading. In this way, CoPIM reduces the size of offloading instructions and also improves the overall performance with less energy consumption. The experimental results show that compared with other state-of-the-art PIM workload offloading frameworks, CoPIM achieves a speedup by the geometric mean of 19.5% and 11.4% than PEI and GraphPIM, respectively. On the other hand, CoPIM also reduces the un-core energy consumption by 6.8% and 6.5% on average over PEI and GraphPIM, respectively.",
        "link": "https://dl.acm.org/doi/10.1109/ISLPED52811.2021.9502483",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Scaling of multi-core quantum architectures: a communications-aware structured gap analysis",
        "authors": "['Santiago Rodrigo', 'Medina Bandic', 'Sergi Abadal', 'Hans van Someren', 'Eduard Alarcón', 'Carmen G. Almudéver']",
        "date": "May 2021",
        "source": "CF '21: Proceedings of the 18th ACM International Conference on Computing Frontiers",
        "abstract": "In the quest of large-scale quantum computers, multi-core distributed architectures are considered a compelling alternative to be explored. A crucial aspect in such approach is the stringent demand on communication among cores when qubits need to interact, which conditions the scalability potential of these architectures. In this work, we address the question of how the cost of the communication among cores impacts on the viability of the quantum multi-core approach. Methodologically, we consider a design space in which architectural variables (number of cores, number of qubits per core), application variables for several quantum benchmarks (number of qubits, number of gates, percentage of two-qubit gates) and inter-core communication latency are swept along with the definition of a figure of merit. This approach yields both a qualitative understanding of trends in the design space and companion dimensioning guidelines for the architecture, including optimal points, as well as quantitative answers to the question of beyond which communication performance levels the multi-core architecture pays off. Our results allow to determine the thresholds for inter-core communication latency in order for multi-core architectures to outperform single-core quantum processors.",
        "link": "https://dl.acm.org/doi/10.1145/3457388.3458674",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "multiPULPly: A Multiplication Engine for Accelerating Neural Networks on Ultra-low-power Architectures",
        "authors": "['Adi Eliahu', 'Ronny Ronen', 'Pierre-Emmanuel Gaillardon', 'Shahar Kvatinsky']",
        "date": "None",
        "source": "ACM Journal on Emerging Technologies in Computing Systems",
        "abstract": "Computationally intensive neural network applications often need to run on resource-limited low-power devices. Numerous hardware accelerators have been developed to speed up the performance of neural network applications and reduce power consumption; however, most focus on data centers and full-fledged systems. Acceleration in ultra-low-power systems has been only partially addressed. In this article, we present multiPULPly, an accelerator that integrates memristive technologies within standard low-power CMOS technology, to accelerate multiplication in neural network inference on ultra-low-power systems. This accelerator was designated for PULP, an open-source microcontroller system that uses low-power RISC-V processors. Memristors were integrated into the accelerator to enable power consumption only when the memory is active, to continue the task with no context-restoring overhead, and to enable highly parallel analog multiplication. To reduce the energy consumption, we propose novel dataflows that handle common multiplication scenarios and are tailored for our architecture. The accelerator was tested on FPGA and achieved a peak energy efficiency of 19.5 TOPS/W, outperforming state-of-the-art accelerators by 1.5× to 4.5×.",
        "link": "https://dl.acm.org/doi/10.1145/3432815",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Eva-CAM: a circuit/architecture-level evaluation tool for general content addressable memories",
        "authors": "['Liu Liu', 'Mohammad Mehdi Sharifi', 'Ramin Rajaei', 'Arman Kazemi', 'Kai Ni', 'Xunzhao Yin', 'Michael Niemier', 'Xiaobo Sharon Hu']",
        "date": "March 2022",
        "source": "DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe",
        "abstract": "Content addressable memories (CAMs), a special-purpose in-memory computing (IMC) unit, support parallel searches directly in memory. There are growing interests in CAMs for data-intensive applications such as machine learning and bioinformatics. The design space for CAMs is rapidly expanding. In addition to traditional ternary CAMs (TCAMs), analog CAM (ACAM) and multi-bit CAM (MCAM) designs based on various non-volatile memory (NVM) devices have been recently introduced and may offer higher density, better energy efficiency, and non-volatility. Furthermore, aside from the widely-used exact match based search, CAM-based approximate matches have been proposed to further extend the utility of CAMs to new application spaces. For this memory architecture, evaluating different CAM design options for a given application is becoming more challenging. This paper presents Eva-CAM, a circuit/architecture-level modeling and evaluation tool for CAMs. Eva-CAM supports TCAM, ACAM, and MCAM designs implemented in non-volatile memories, for both exact and approximate match types. It also allows for the exploration of CAM array structures and sensing circuits. Eva-CAM has been validated with HSPICE simulation results and chip measurements. A comprehensive case study is described for FeFET CAM design space exploration.",
        "link": "https://dl.acm.org/doi/10.5555/3539845.3540123",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "BOSS: bandwidth-optimized search accelerator for storage-class memory",
        "authors": "['Jun Heo', 'Seung Yul Lee', 'Sunhong Min', 'Yeonhong Park', 'Sung Jun Jung', 'Tae Jun Ham', 'Jae W. Lee']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "Search is one of the most popular and important web services. The inverted index is the standard data structure adopted by most full-text search engines. Recently, custom hardware accelerators for inverted index search have emerged to demonstrate much higher throughput than the conventional CPU or GPU. However, less attention has been paid to addressing the memory capacity pressure with inverted index. The conventional DDRx DRAM memory system significantly increases the system cost to make a terabyte-scale main memory. Instead, a shared memory pool composed of storage-class memory (SCM) devices is a promising alternative for scaling memory capacity at a much lower cost. However, this SCM-based pooled memory poses new challenges caused by the limited bandwidth of both SCM devices and the shared interconnect to the host CPU. Thus, we propose BOSS, the first near-data processing (NDP) architecture for inverted index search on SCM-based pooled memory, which maintains high throughput of query processing in this bandwidth-constrained environment. BOSS mitigates the impact of low bandwidth of SCM devices by employing early-termination search algorithms, reducing the footprint of intermediate data, and introducing a programmable decompression module that can select the best compression scheme for a given inverted index. Furthermore, BOSS includes a top-k selection module in hardware to substantially reduce the host-accelerator bandwidth consumption. Compared to Apache Lucene, a production-grade search engine library, running on 8 CPU cores, BOSS achieves a geomean speedup of 8.1x on various complex query types, while reducing the average energy consumption by 189x.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00030",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "ClickTrain: efficient and accurate end-to-end deep learning training via fine-grained architecture-preserving pruning",
        "authors": "['Chengming Zhang', 'Geng Yuan', 'Wei Niu', 'Jiannan Tian', 'Sian Jin', 'Donglin Zhuang', 'Zhe Jiang', 'Yanzhi Wang', 'Bin Ren', 'Shuaiwen Leon Song', 'Dingwen Tao']",
        "date": "June 2021",
        "source": "ICS '21: Proceedings of the ACM International Conference on Supercomputing",
        "abstract": "Convolutional neural networks (CNNs) are becoming increasingly deeper, wider, and non-linear because of the growing demand on prediction accuracy and analysis quality. The wide and deep CNNs, however, require a large amount of computing resources and processing time. Many previous works have studied model pruning to improve inference performance, but little work has been done for effectively reducing training cost. In this paper, we propose ClickTrain: an efficient and accurate end-to-end training and pruning framework for CNNs. Different from the existing pruning-during-training work, ClickTrain provides higher model accuracy and compression ratio via fine-grained architecture-preserving pruning. By leveraging pattern-based pruning with our proposed novel accurate weight importance estimation, dynamic pattern generation and selection, and compiler-assisted computation optimizations, ClickTrain generates highly accurate and fast pruned CNN models for direct deployment without any time overhead, compared with the baseline training. ClickTrain also reduces the end-to-end time cost of the state-of-the-art pruning-after-training method by up to 2.3x with comparable accuracy and compression ratio. Moreover, compared with the state-of-the-art pruning-during-training approach, ClickTrain provides significant improvements both accuracy and compression ratio on the tested CNN models and datasets, under similar limited training time.",
        "link": "https://dl.acm.org/doi/10.1145/3447818.3459988",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "coxHE: a software-hardware co-design framework for FPGA acceleration of homomorphic computation",
        "authors": "['Mingqin Han', 'Yilan Zhu', 'Qian Lou', 'Zimeng Zhou', 'Shanqing Guo', 'Lei Ju']",
        "date": "March 2022",
        "source": "DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe",
        "abstract": "Data privacy becomes a crucial concern in the AI and big data era. Fully homomorphic encryption (FHE) is a promising data privacy protection technique where the entire computation is performed on encrypted data. However, the dramatic increase of the computation workload restrains the usage of FHE for the real-world applications. In this paper, we propose an FPFA accelerator design framework for CKKS-based HE. While the KeySwitch operations are the primary performance bottleneck of FHE computation, we propose a low latency design of KeySwitch module with reduced intra-operation data dependency. Compared with the state-of-the-art FPGA based key-switch implementation that is based on Verilog, the proposed high-level synthesis (HLS) based design reduces the operation latency by 40%. Furthermore, we propose an automated design space exploration framework which generates optimal encryption parameters and accelerators for a given application kernel and the target FPGA device. Experimental results for a set of real HE application kernels on different FPGA devices show that our HLS-based flexible design framework produces substantially better accelerator design compared with a fixed-parameter HE accelerator in terms of security, approximation error, and overall performance.",
        "link": "https://dl.acm.org/doi/10.5555/3539845.3540161",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "μNAS: Constrained Neural Architecture Search for Microcontrollers",
        "authors": "['Edgar Liberis', 'Łukasz Dudziak', 'Nicholas D. Lane']",
        "date": "April 2021",
        "source": "EuroMLSys '21: Proceedings of the 1st Workshop on Machine Learning and Systems",
        "abstract": "IoT devices are powered by microcontroller units (MCUs) which are extremely resource-scarce: a typical MCU may have an underpowered processor and around 64 KB of memory and persistent storage. Designing neural networks for such a platform requires an intricate balance between keeping high predictive performance (accuracy) while achieving low memory and storage usage and inference latency. This is extremely challenging to achieve manually, so in this work, we build a neural architecture search (NAS) system, called μNAS, to automate the design of such small-yet-powerful MCU-level networks. μNAS explicitly targets the three primary aspects of resource scarcity of MCUs: the size of RAM, persistent storage and processor speed. μNAS represents a significant advance in resource-efficient models, especially for \"mid-tier\" MCUs with memory requirements ranging from 0.5 KB to 64 KB. We show that on a variety of image classification datasets μNAS is able to (a) improve top-1 classification accuracy by up to 4.8%, or (b) reduce memory footprint by 4-13×, or (c) reduce the number of multiply-accumulate operations by at least 2×, compared to existing MCU specialist literature and resource-efficient models. μNAS is freely available for download at https://github.com/eliberis/uNAS",
        "link": "https://dl.acm.org/doi/10.1145/3437984.3458836",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "AStitch: enabling a new multi-dimensional optimization space for memory-intensive ML training and inference on modern SIMT architectures",
        "authors": "['Zhen Zheng', 'Xuanda Yang', 'Pengzhan Zhao', 'Guoping Long', 'Kai Zhu', 'Feiwen Zhu', 'Wenyi Zhao', 'Xiaoyong Liu', 'Jun Yang', 'Jidong Zhai', 'Shuaiwen Leon Song', 'Wei Lin']",
        "date": "February 2022",
        "source": "ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "This work reveals that memory-intensive computation is a rising performance-critical factor in recent machine learning models. Due to a unique set of new challenges, existing ML optimizing compilers cannot perform efficient fusion under complex two-level dependencies combined with just-in-time demand. They face the dilemma of either performing costly fusion due to heavy redundant computation, or skipping fusion which results in massive number of kernels. Furthermore, they often suffer from low parallelism due to the lack of support for real-world production workloads with irregular tensor shapes. To address these rising challenges, we propose AStitch, a machine learning optimizing compiler that opens a new multi-dimensional optimization space for memory-intensive ML computations. It systematically abstracts four operator-stitching schemes while considering multi-dimensional optimization objectives, tackles complex computation graph dependencies with novel hierarchical data reuse, and efficiently processes various tensor shapes via adaptive thread mapping. Finally, AStitch provides just-in-time support incorporating our proposed optimizations for both ML training and inference. Although AStitch serves as a stand-alone compiler engine that is portable to any version of TensorFlow, its basic ideas can be generally applied to other ML frameworks and optimization compilers. Experimental results show that AStitch can achieve an average of 1.84x speedup (up to 2.73x) over the state-of-the-art Google's XLA solution across five production workloads. We also deploy AStitch onto a production cluster for ML workloads with thousands of GPUs. The system has been in operation for more than 10 months and saves about 20,000 GPU hours for 70,000 tasks per week.",
        "link": "https://dl.acm.org/doi/10.1145/3503222.3507723",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Trident: Harnessing Architectural Resources for All Page Sizes in x86 Processors",
        "authors": "['Venkat Sri Sai Ram', 'Ashish Panwar', 'Arkaprava Basu']",
        "date": "October 2021",
        "source": "MICRO '21: MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture",
        "abstract": "Intel and AMD processors have long supported more than one large page sizes – 1GB and 2MB, to reduce address translation overheads for applications with large memory footprints. However, previous works on large pages have primarily focused on 2MB pages, partly due to a lack of evidence on the usefulness of 1GB pages to real-world applications. Consequently, micro-architectural resources devoted to 1GB pages have gone underutilized for a decade.  We quantitatively demonstrate where 1GB pages can be valuable, especially when employed in conjunction with 2MB pages. Unfortunately, the lack of application-transparent dynamic allocation of 1GB pages is to blame for the under-utilization of 1GB pages on today’s systems. Toward this, we design and implement Trident in Linux to fully harness micro-architectural resources devoted for all page sizes in the current x86 hardware by transparently allocating 1GB, 2MB, and 4KB pages as suitable at runtime. Trident speeds up eight memory-intensive applications by 18%, on average, over Linux’s use of 2MB pages. We then propose Tridentpv, an extension to Trident that virtualizes 1GB pages via copy-less promotion and compaction in the guest OS. Overall, this paper shows that adequate software enablement brings practical relevance to even GB-sized pages, and motivates micro-architects to continue enhancing hardware support for all large page sizes.",
        "link": "https://dl.acm.org/doi/10.1145/3466752.3480062",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Improving on the Experience of Hand-Assembling Programs for Application-Specific Architectures",
        "authors": "['Ian Piumarta']",
        "date": "March 2021",
        "source": "Programming '21: Companion Proceedings of the 5th International Conference on the Art, Science, and Engineering of Programming",
        "abstract": "Creating an application-specific processor is an effective and popular way to solve many problems in embedded hardware design using FPGAs, ASICs, or custom silicon. Programming these processors is complicated by the lack of toolchain support for creating the necessary binary code as part of hardware design, implementation, and evaluation. Hardware developers who cannot create their own ad-hoc assembler are left to hand-assemble their code into binary instructions which is both painful and error prone. We present a tool that supports the rapid creation of assemblers for application-specific processors. A single language is used to specify both instruction formats as collections of bit fields and the instantiation of those formats into sequences of binary instructions as a single, homogeneous activity that is designed to be as familiar and accessible to hardware designers as possible. The output from the tool can be used directly by hardware synthesis tools to initialise the program memory of an application-specific processor.",
        "link": "https://dl.acm.org/doi/10.1145/3464432.3464434",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "SpZip: architectural support for effective data compression in irregular applications",
        "authors": "['Yifan Yang', 'Joel S. Emer', 'Daniel Sanchez']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "Irregular applications, such as graph analytics and sparse linear algebra, exhibit frequent indirect, data-dependent accesses to single or short sequences of elements that cause high main memory traffic and limit performance. Data compression is a promising way to accelerate irregular applications by reducing memory traffic. However, software compression adds substantial overheads, and prior hardware compression techniques work poorly on the complex access patterns of irregular applications. We present SpZip, an architectural approach that makes data compression practical for irregular algorithms. SpZip accelerates the traversal, decompression, and compression of the data structures used by irregular applications. In addition, these activities run in a decoupled fashion, hiding both memory access and decompression latencies. To support the wide range of access patterns in these applications, SpZip is programmable, and uses a novel Dataflow Configuration Language to specify programs that traverse and generate compressed data. Our SpZip implementation leverages dataflow execution and time-multiplexing to implement programmability cheaply. We evaluate SpZip on a simulated multicore system running a broad set of graph and linear algebra algorithms. SpZip outperforms prior state-of-the art software-only (hardware-accelerated) systems by gmean 3.0X (1.5X) and reduces memory traffic by 1.7X (1.4X). These benefits stem from both reducing data movement due to compression, and offloading expensive traversal and (de)compression operations.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00087",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Architectural Patterns for Integrating AI Technology into Safety-Critical Systems",
        "authors": "['Georg Macher', 'Matthias Seidl', 'Maid Dzambic', 'Jürgen Dobaj']",
        "date": "July 2021",
        "source": "EuroPLoP'21: 26th European Conference on Pattern Languages of Programs",
        "abstract": "Artificial Intelligence (AI) is widely acknowledged as one of the most disruptive technologies driving the digital transformation of industries, enterprises, and societies in the 21st century. Advances in computing speed, algorithmic improvements, and access to a vast amount of data contributed to the adaption of AI in many different domains. Due to the outstanding performance, AI technology is increasingly integrated into safety-critical applications. However, the established safety engineering processes and practices have been only successfully applied in conventional model-based system development and no commonly agreed approaches for integrating AI technology are available yet. This work presents two architectural patterns that can support designers and engineers in the conception of safety-critical AI-enhanced cyber-physical system (CPS) applications. The first pattern addresses the problem of integrating AI capabilities into safety-critical functions. The second pattern deals with architectural approaches to integrate AI technologies for monitoring and learning system-specific behavior at runtime.",
        "link": "https://dl.acm.org/doi/10.1145/3489449.3490014",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Analysing and extending privacy patterns with architectural context",
        "authors": "['Su Yen Chia', 'Xiwei Xu', 'Hye-Young Paik', 'Liming Zhu']",
        "date": "March 2021",
        "source": "SAC '21: Proceedings of the 36th Annual ACM Symposium on Applied Computing",
        "abstract": "Privacy is now an increasingly important software quality. Software architects and developers should consider privacy from the early stages of system design to prevent privacy breaches. Both industry and academia have proposed privacy patterns as reusable design solutions to address common privacy problems. However, from the system development perspective, the existing privacy patterns do not provide architectural context to assist software design for privacy. More specifically, the current privacy patterns lack proper analysis with regards to privacy properties - the well-established software traits relating to privacy (e.g., unlinkability, identifiability). Furthermore, the impacts of privacy patterns on other quality attributes such as performance are yet to be investigated. Our paper aims to provide guidance to software architects and developers for considering privacy patterns, by adding new perspectives to the existing privacy patterns. First, we provide a new structural and interaction view of the patterns by relating privacy regulation contexts. Then, we analyse the patterns in architectural contexts and map available privacy-preserving techniques for implementing each privacy pattern. We also give an analysis of privacy patterns with regard to their impact on privacy properties, and the trade-off between privacy and other quality attributes.",
        "link": "https://dl.acm.org/doi/10.1145/3412841.3442014",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Efficient Application of Tensor Core Units for Convolving Images",
        "authors": "['Stefan Groth', 'Jürgen Teich', 'Frank Hannig']",
        "date": "November 2021",
        "source": "SCOPES '21: Proceedings of the 24th International Workshop on Software and Compilers for Embedded Systems",
        "abstract": "Tensor Core Units (TCUs) are a domain-specific architecture capable of executing small Matrix Multiply-Accumulates (MMAs) in a single clock cycle, showing significant performance improvements over other optimized implementations. When Convolutional Neural Networks (CNNs) are accelerated using TCUs, the layout of the input image is transformed to allow large amounts of filters to be applied to an image using a single large matrix-matrix multiplication. However, there are applications in other domains that only require a small number of filters. To accommodate such applications, we first show the inappropriateness of this standard technique of transforming the data layout. Subsequently, we propose an approach that uses TCUs to convolve one filter with an image. Further, we introduce several optimizations of this method. Finally, we evaluate the performance of our approach and its optimizations by comparing it to code generated using a state-of-the-art image processing language.",
        "link": "https://dl.acm.org/doi/10.1145/3493229.3493305",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "FSA: fronthaul slicing architecture for 5G using dataplane programmable switches",
        "authors": "['Nishant Budhdev', 'Raj Joshi', 'Pravein Govindan Kannan', 'Mun Choon Chan', 'Tulika Mitra']",
        "date": "October 2021",
        "source": "MobiCom '21: Proceedings of the 27th Annual International Conference on Mobile Computing and Networking",
        "abstract": "5G networks are gaining pace in development and deployment in recent years. One of 5G's key objective is to support a variety of use cases with different Service Level Objectives (SLOs). Slicing is a key part of 5G that allows operators to provide a tailored set of resources to different use cases in order to meet their SLOs. Existing works focus on slicing in the frontend or the C-RAN. However, slicing is missing in the fronthaul network that connects the frontend to the C-RAN. This leads to over-provisioning in the fronthaul and the C-RAN, and also limits the scalability of the network. In this paper, we design and implement Fronthaul Slicing Architecture (FSA), which to the best of our knowledge, is the first slicing architecture for the fronthaul network. FSA runs in the switch dataplane and uses information from the wireless schedule to identify the slice of a fronthaul data packet at line-rate. It enables multipoint-to-multipoint routing as well as packet prioritization to provide multiplexing gains in the fronthaul and the C-RAN, making the system more scalable. Our testbed evaluation using scaled-up LTE traces shows that FSA can support accurate multipoint-to-multipoint routing for 80 Gbps of fronthaul traffic. Further, the slice-aware packet scheduling enabled by FSA's packet prioritization reduces the 95th percentile Flowlet Completion Times (FCT) of latency-sensitive traffic by up to 4 times.",
        "link": "https://dl.acm.org/doi/10.1145/3447993.3483247",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Simba: scaling deep-learning inference with chiplet-based architecture",
        "authors": "['Yakun Sophia Shao', 'Jason Cemons', 'Rangharajan Venkatesan', 'Brian Zimmer', 'Matthew Fojtik', 'Nan Jiang', 'Ben Keller', 'Alicia Klinefelter', 'Nathaniel Pinckney', 'Priyanka Raina', 'Stephen G. Tell', 'Yanqing Zhang', 'William J. Dally', 'Joel Emer', 'C. Thomas Gray', 'Brucek Khailany', 'Stephen W. Keckler']",
        "date": "June 2021",
        "source": "Communications of the ACM",
        "abstract": "Package-level integration using multi-chip-modules (MCMs) is a promising approach for building large-scale systems. Compared to a large monolithic die, an MCM combines many smaller chiplets into a larger system, substantially reducing fabrication and design costs. Current MCMs typically only contain a handful of coarse-grained large chiplets due to the high area, performance, and energy overheads associated with inter-chiplet communication. This work investigates and quantifies the costs and benefits of using MCMs with finegrained chiplets for deep learning inference, an application domain with large compute and on-chip storage requirements. To evaluate the approach, we architected, implemented, fabricated, and tested Simba, a 36-chiplet prototype MCM system for deep-learning inference. Each chiplet achieves 4 TOPS peak performance, and the 36-chiplet MCM package achieves up to 128 TOPS and up to 6.1 TOPS/W. The MCM is configurable to support a flexible mapping of DNN layers to the distributed compute and storage units. To mitigate inter-chiplet communication overheads, we introduce three tiling optimizations that improve data locality. These optimizations achieve up to 16% speedup compared to the baseline layer mapping. Our evaluation shows that Simba can process 1988 images/s running ResNet-50 with a batch size of one, delivering an inference latency of 0.50 ms.",
        "link": "https://dl.acm.org/doi/10.1145/3460227",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "GoSPA: an energy-efficient high-performance globally optimized sparse convolutional neural network accelerator",
        "authors": "['Chunhua Deng', 'Yang Sui', 'Siyu Liao', 'Xuehai Qian', 'Bo Yuan']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "The co-existence of activation sparsity and model sparsity in convolutional neural network (CNN) models makes sparsity-aware CNN hardware designs very attractive. The existing sparse CNN accelerators utilize intersection operation to search and identify the key positions of the matched entries between two sparse vectors, and hence avoid unnecessary computations. However, these state-of-the-art designs still suffer from three major architecture-level drawbacks, including 1) hardware cost for the intersection operation is high; 2) frequent stalls of computation phase due to strong data dependency between intersection and computation phases; and 3) unnecessary data transfer incurred by the explicit intersection operation. By leveraging the knowledge of the complete sparse 2-D convolution, this paper proposes two key ideas that overcome all of the three drawbacks. First, an implicit on-the-fly intersection is proposed to realize the optimal solution for intersection between one static stream and one dynamic stream, which is the case for sparse neural network inference. Second, by leveraging the global computation structure of 2--D convolution, we propose a specialized computation reordering to ensure that the activation is only transferred if necessary and only once. Based on these two key ideas, we develop GoSPA, an energy-efficient high-performance Globally Optimized SParse CNN Accelerator. GoSPA is implemented with CMOS 28nm technology. Compared with the state-of-the-art sparse CNN architecture, GoSPA achieves average 1.38X, 1.28X, 1.23X, 1.17X, 1.21X and 1.28X speedup on AlexNet, VGG, GoogLeNet, MobileNet, ResNet and ResNeXt workloads, respectively. Also, GoSPA achieves 5.38X, 4.96X, 4.79X, 5.02X, 4.86X and 2.06X energy efficiency improvement on AlexNet, VGG, GoogLeNet, MobileNet, ResNet and ResNeXt, respectively. In more comprehensive comparison including DRAM access, GoSPA also shows significant performance improvement over the existing designs.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00090",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A Novel Hybrid Cache Coherence with Global Snooping for Many-core Architectures",
        "authors": "['Sri Harsha Gade', 'Sujay Deb']",
        "date": "None",
        "source": "ACM Transactions on Design Automation of Electronic Systems",
        "abstract": "Cache coherence ensures correctness of cached data in multi-core processors. Traditional implementations of existing protocols make them unscalable for many core architectures. While snoopy coherence requires unscalable ordered networks, directory coherence is weighed down by high area and energy overheads. In this work, we propose Wireless-enabled Share-aware Hybrid (WiSH) to provide scalable coherence in many core processors. WiSH implements a novel Snoopy over Directory protocol using on-chip wireless links and hierarchical, clustered Network-on-Chip to achieve low-overhead and highly efficient coherence. A local directory protocol maintains coherence within a cluster of cores, while coherence among such clusters is achieved through global snoopy protocol. The ordered network for global snooping is provided through low-latency and low-energy broadcast wireless links. The overheads are further reduced through share-aware cache segmentation to eliminate coherence for private blocks. Evaluations show that WiSH reduces traffic by \\(\\) and runtime by \\(\\), while requiring \\(\\) smaller storage and \\(\\) lower energy as compared to existing hierarchical and hybrid coherence protocols. Owing to its modularity, WiSH provides highly efficient and scalable coherence for many core processors.",
        "link": "https://dl.acm.org/doi/10.1145/3462775",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "On the Application of Computer CAD Drawing Software in Civil Engineering Architectural Design and Structural Design",
        "authors": "['Xu Yao']",
        "date": "August 2021",
        "source": "ICIMTECH 21: <italic toggle='yes'>Retracted on September 15, 2021</italic>The Sixth International Conference on Information Management and Technology",
        "abstract": "NOTICE OF RETRACTION: While investigating potential publication-related misconduct in connection with the ICIMTech 2021 Conference Proceedings, serious concerns were raised that cast doubt on the integrity of the peer-review process and all papers published in the Proceedings of this Conference. The integrity of the entire Conference has been called into question. As a result, of its investigation, ACM has decided to retract the Entire Conference Proceedings and all related papers from the ACM Digital Library.None of the papers from this Proceeding should be cited in the literature because of the questionable integrity of the peer review process for this Conference.",
        "link": "https://dl.acm.org/doi/10.1145/3465631.3465816",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Towards Learning-Based Architectures for Sensor Impact Evaluation in Building Controls",
        "authors": "['Saptarshi Bhattacharya', 'Himanshu Sharma', 'Veronica Adetola']",
        "date": "June 2021",
        "source": "e-Energy '21: Proceedings of the Twelfth ACM International Conference on Future Energy Systems",
        "abstract": "Advanced control algorithms for building systems have significant potential in reducing energy consumption while optimizing thermal comfort. Success of such algorithms is critically contingent on several different types of sensor systems, which are deployed for continuous monitoring, identification and estimation of several important building states, such as temperatures, humidity, air quality, power consumption and occupancy. Nonidealities in these sensors can lead to significant performance degradation of the control functionalities, thereby causing unwanted sub-optimal building operation. In this paper, we provide a simulation example with a high-fidelity building model, for a particular use-case of advanced optimization-based control in buildings, i.e., occupancy-based controls. We show how imperfections in occupancy sensing can degrade performance. Subsequently, we discuss a novel learning-based framework to efficiently evaluate the impact of sensor nonidealities for building systems, in context of advanced control algorithms.",
        "link": "https://dl.acm.org/doi/10.1145/3447555.3466591",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Architectural Jewels of Lublin: A Modern Computerized Board Game in Cultural Heritage Education",
        "authors": "['Jerzy Montusiewicz', 'Marek Milosz']",
        "date": "None",
        "source": "Journal on Computing and Cultural Heritage ",
        "abstract": "Lublin is a city located in the eastern part of Poland, which is an important place of cultural heritage, being the venue where the Polish-Lithuanian Union was signed 450 years ago in 2019. This article presents “Architectural Jewels of Lublin,” a computerized serious board game for two players. The aim of the game is to collect points for recognizing models of the city's historic architectural objects and their correct position on the board. They represent the landmarks of the historic Old Town quarter. Another point of the game is to answer questions about the cultural heritage of Lublin. 3D models of historic buildings were initially designed manually and then 3D printed in FFF (Fused Filament Fabrication) technology. The correct location of the object on the board is identified by sensors working in the RFID (Radio-Frequency IDentification) technology supported by two microcontrollers of an Arduino platform, which were connected to the software managing the whole game shown on a tablet monitor. The game is used both to promote Lublin at numerous cyclical cultural and science popularization events, and during conferences and seminars organized for circles representing cultural heritage from Poland and abroad. It is aimed at presenting a way to integrate many different contemporary digital technologies that can serve education in the area of cultural heritage. The game, in contrast to popular games using VR and AR technologies, combines in an interesting way physical and digital space using modern computer technologies. The research carried out on the participants of the game has shown its high effectiveness in raising the historical awareness of its participants, as well as the players’ positive attitude toward the game.",
        "link": "https://dl.acm.org/doi/10.1145/3446978",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Exploring Efficient Architectures on Remote In-Memory NVM over RDMA",
        "authors": "['Qingfeng Zhuge', 'Hao Zhang', 'Edwin Hsing-Mean Sha', 'Rui Xu', 'Jun Liu', 'Shengyu Zhang']",
        "date": "None",
        "source": "ACM Transactions on Embedded Computing Systems",
        "abstract": "Efficiently accessing remote file data remains a challenging problem for data processing systems. Development of technologies in non-volatile dual in-line memory modules (NVDIMMs), in-memory file systems, and RDMA networks provide new opportunities towards solving the problem of remote data access. A general understanding about NVDIMMs, such as Intel Optane DC Persistent Memory (DCPM), is that they expand main memory capacity with a cost of multiple times lower performance than DRAM. With an in-depth exploration presented in this paper, however, we show an interesting finding that the potential of NVDIMMs for high-performance, remote in-memory accesses can be revealed through careful design. We explore multiple architectural structures for accessing remote NVDIMMs in a real system using Optane DCPM, and compare the performance of various structures. Experiments are conducted to show significant performance gaps among different ways of using NVDIMMs as memory address space accessible through RDMA interface. Furthermore, we design and implement a prototype of user-level, in-memory file system, RIMFS, in the device DAX mode on Optane DCPM. By comparing against the DAX-supported Linux file system, Ext4-DAX, we show that the performance of remote reads on RIMFS over RDMA is 11.44 \\(\\) higher than that on a remote Ext4-DAX on average. The experimental results also show that the performance of remote accesses on RIMFS is maintained on a heavily loaded data server with CPU utilization as high as 90%, while the performance of remote reads on Ext4-DAX is significantly reduced by 49.3%, and the performance of local reads on Ext4-DAX is even more significantly reduced by 90.1%. The performance comparisons of writes exhibit the same trends.",
        "link": "https://dl.acm.org/doi/10.1145/3477004",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "RaPiD: AI accelerator for ultra-low precision training and inference",
        "authors": "['Swagath Venkataramani', 'Vijayalakshmi Srinivasan', 'Wei Wang', 'Sanchari Sen', 'Jintao Zhang', 'Ankur Agrawal', 'Monodeep Kar', 'Shubham Jain', 'Alberto Mannari', 'Hoang Tran', 'Yulong Li', 'Eri Ogawa', 'Kazuaki Ishizaki', 'Hiroshi Inoue', 'Marcel Schaal', 'Mauricio Serrano', 'Jungwook Choi', 'Xiao Sun', 'Naigang Wang', 'Chia-Yu Chen', 'Allison Allain', 'James Bonano', 'Nianzheng Cao', 'Robert Casatuta', 'Matthew Cohen', 'Bruce Fleischer', 'Michael Guillorn', 'Howard Haynie', 'Jinwook Jung', 'Mingu Kang', 'Kyu-hyoun Kim', 'Siyu Koswatta', 'Saekyu Lee', 'Martin Lutz', 'Silvia Mueller', 'Jinwook Oh', 'Ashish Ranjan', 'Zhibin Ren', 'Scot Rider', 'Kerstin Schelm', 'Michael Scheuermann', 'Joel Silberman', 'Jie Yang', 'Vidhi Zalani', 'Xin Zhang', 'Ching Zhou', 'Matt Ziegler', 'Vinay Shah', 'Moriyoshi Ohara', 'Pong-Fei Lu', 'Brian Curran', 'Sunil Shukla', 'Leland Chang', 'Kailash Gopalakrishnan']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "The growing prevalence and computational demands of Artificial Intelligence (AI) workloads has led to widespread use of hardware accelerators in their execution. Scaling the performance of AI accelerators across generations is pivotal to their success in commercial deployments. The intrinsic error-resilient nature of AI workloads present a unique opportunity for performance/energy improvement through precision scaling. Motivated by the recent algorithmic advances in precision scaling for inference and training, we designed RAPID1, a 4-core AI accelerator chip supporting a spectrum of precisions, namely, 16 and 8-bit floating-point and 4 and 2-bit fixed-point. The 36mm2 RAPID chip fabricated in 7nm EUV technology delivers a peak 3.5 TFLOPS/W in HFP8 mode and 16.5 TOPS/W in INT4 mode at nominal voltage. Using a performance model calibrated to within 1% of the measurement results, we evaluated DNN inference using 4-bit fixed-point representation for a 4-core 1 RAPID chip system and DNN training using 8-bit floating point representation for a 768 TFLOPs AI system comprising 4 32-core RAPID chips. Our results show INT4 inference for batch size of 1 achieves 3 - 13.5 (average 7) TOPS/W and FP8 training for a mini-batch of 512 achieves a sustained 102 - 588 (average 203) TFLOPS across a wide range of applications.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00021",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "DANA: Dimension-Adaptive Neural Architecture for Multivariate Sensor Data",
        "authors": "['Mohammad Malekzadeh', 'Richard Clegg', 'Andrea Cavallaro', 'Hamed Haddadi']",
        "date": "None",
        "source": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies",
        "abstract": "Motion sensors embedded in wearable and mobile devices allow for dynamic selection of sensor streams and sampling rates, enabling several applications, such as power management and data-sharing control. While deep neural networks (DNNs) achieve competitive accuracy in sensor data classification, DNN architectures generally process incoming data from a fixed set of sensors with a fixed sampling rate, and changes in the dimensions of their inputs cause considerable accuracy loss, unnecessary computations, or failure in operation. To address these limitations, we introduce a dimension-adaptive pooling (DAP) layer that makes DNNs flexible and more robust to changes in sensor availability and in sampling rate. DAP operates on convolutional filter maps of variable dimensions and produces an input of fixed dimensions suitable for feedforward and recurrent layers. Further, we propose a dimension-adaptive training (DAT) procedure for enabling DNNs that use DAP to better generalize over the set of feasible data dimensions at inference time. DAT comprises the random selection of dimensions during the forward passes and optimization with accumulated gradients of several backward passes. Combining DAP and DAT, we show how to transform existing non-adaptive DNNs into a Dimension-Adaptive Neural Architecture (DANA), while keeping the same number of parameters. Compared to existing approaches, our solution provides better average classification accuracy over the range of possible data dimensions at inference time and does not require up-sampling or imputation, thus reducing unnecessary computations. Experimental results on seven datasets (four benchmark real-world datasets for human activity recognition and three synthetic datasets) show that DANA prevents significant losses in classification accuracy of the state-of-the-art DNNs and, compared to baselines, it better captures correlated patterns in sensor data under dynamic sensor availability and varying sampling rates.",
        "link": "https://dl.acm.org/doi/10.1145/3478074",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Nap: Persistent Memory Indexes for NUMA Architectures",
        "authors": "['Qing Wang', 'Youyou Lu', 'Junru Li', 'Minhui Xie', 'Jiwu Shu']",
        "date": "None",
        "source": "ACM Transactions on Storage",
        "abstract": "We present Nap, a black-box approach that converts concurrent persistent memory (PM) indexes into non-uniform memory access (NUMA)-aware counterparts. Based on the observation that real-world workloads always feature skewed access patterns, Nap introduces a NUMA-aware layer (NAL) on the top of existing concurrent PM indexes, and steers accesses to hot items to this layer. The NAL maintains (1) per-node partial views in PM for serving insert/update/delete operations with failure atomicity and (2) a global view in DRAM for serving lookup operations. The NAL eliminates remote PM accesses to hot items without inducing extra local PM accesses. Moreover, to handle dynamic workloads, Nap adopts a fast NAL switch mechanism. We convert five state-of-the-art PM indexes using Nap. Evaluation on a four-node machine with Optane DC Persistent Memory shows that Nap can improve the throughput by up to 2.3× and 1.56× under write-intensive and read-intensive workloads, respectively.",
        "link": "https://dl.acm.org/doi/10.1145/3507922",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Network Performance Influences of Software-defined Networks on Micro-service Architectures",
        "authors": "['Axel Busch', 'Martin Kammerer']",
        "date": "April 2021",
        "source": "ICPE '21: Proceedings of the ACM/SPEC International Conference on Performance Engineering",
        "abstract": "Modern business applications are increasingly developed as micro-services and deployed in the cloud. Due to many components involved micro-services need a flexible and high-performance network infrastructure. To ensure highly available and high performance applications, operators are increasingly relying on cloud service platforms such as the OpenShift Container Platform on Z. In such environments modern software-defined network technologies such as Open vSwitch (OVS) are used. However, the impact of their architecture on network performance has not yet been sufficiently researched although networking performance is particularly critical for the quality of the service. In this paper, we analyse the impact of the OVS pipeline and selected OVS operations in detail. We define different scenarios used in the industry and analyse the performance of different OVS configurations using an IBM z14 mainframe system. Our analysis showed the OVS pipeline and its operations can affect network performance by up to factor 3. Our results show that even the use of virtual switches such as OVS, network performance can be significantly improved by optimizing the OVS pipeline architecture.",
        "link": "https://dl.acm.org/doi/10.1145/3427921.3450236",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "On Using Blockchains for Beyond Visual Line of Sight (BVLOS) Drones Operation: An Architectural Study",
        "authors": "['Tahina Ralitera', 'Onder Gurcan']",
        "date": "January 2022",
        "source": "DroneSE and RAPIDO: System Engineering for constrained embedded systems",
        "abstract": "Beyond Visual Line of Sight operation enables drones to surpass the limits imposed by the reach and constraints of the eyes of their operator. It extends their range and, as such, productivity and profitability. Drones operating BVLOS include a variety of highly sensitive assets and information that could be subject to unintentional or intentional security vulnerabilities. As a solution, blockchain-based services could enable secure and trustworthy exchange and storage of related data. They also allow for traceability of exchanges and synchronization with other nodes in the network. In this context, we develop simulation models to evaluate different blockchain-based solutions before implementing them in real systems. However, most of these blockchain-based approaches focus on the network and the protocol aspects of drone systems. Few studies focus on the architectural level of on-chip compute platforms of drones. Based on this observation, the contribution of this paper is twofold : (1) a generic blockchain-based service architecture for on-chip compute platforms of drones in a simulated environment, and (2) an illustration of the proposed generic architecture in a simulated environment for authentication.",
        "link": "https://dl.acm.org/doi/10.1145/3522784.3522794",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "An Archive of Interfaces: Exploring the Potential of Emulation for Software Research, Pedagogy, and Design",
        "authors": "['Daniel Cardoso-Llach', 'Eric Kaltman', 'Emek Erdolu', 'Zachary Furste']",
        "date": "None",
        "source": "Proceedings of the ACM on Human-Computer Interaction",
        "abstract": "This paper explores the potential of distributed emulation networks to support research and pedagogy into historical and sociotechnical aspects of software. Emulation is a type of virtualization that re-creates the conditions for a piece of legacy software to operate on a modern system. The paper first offers a review of Computer-Supported Cooperative Work (CSCW), Human-Computer Interaction (HCI), and Science and Technology Studies (STS) literature engaging with software as historical and sociotechnical artifacts, and with emulation as a vehicle of scholarly inquiry. It then documents the novel use of software emulations as a pedagogical resource and research tool for legacy software systems analysis. This is accomplished through the integration of the Emulation as a Service Infrastructure (EaaSI) distributed emulation network into a university-level course focusing on computer-aided design (CAD). The paper offers a detailed case study of a pedagogical experience oriented to incorporate emulations into software research and learning. It shows how emulations allow for close, user-centered analyses of software systems that highlight both their historical evolution and core interaction concepts, and how they shape the work practices of their users.",
        "link": "https://dl.acm.org/doi/10.1145/3476035",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Improving Barnes-Hut t-SNE Algorithm in Modern GPU Architectures with Random Forest KNN and Simulated Wide-Warp",
        "authors": "['Bruno Henrique Meyer', 'Aurora Trinidad Ramirez Pozo', 'Wagner M. Nunan Zola']",
        "date": "None",
        "source": "ACM Journal on Emerging Technologies in Computing Systems",
        "abstract": "The t-Distributed Stochastic Neighbor Embedding (t-SNE) is a widely used technique for dimensionality reduction but is limited by its scalability when applied to large datasets. Recently, BH-tSNE was proposed; this is a successful approximation that transforms a step of the original algorithm into an N-Body simulation problem that can be solved by a modified Barnes-Hut algorithm. However, this improvement still has limitations to process large data volumes (millions of records). Late studies, such as t-SNE-CUDA, have used GPUs to implement highly parallel BH-tSNE. In this research we have developed a new GPU BH-tSNE implementation that produces the embedding of multidimensional data points into three-dimensional space. We examine scalability issues in two of the most expensive steps of GPU BH-tSNE by using efficient memory access strategies , recent acceleration techniques , and a new approach to compute the KNN graph structure used in BH-tSNE with GPU. Our design allows up to 460% faster execution when compared to the t-SNE-CUDA implementation. Although our SIMD acceleration techniques were used in a modern GPU setup, we have also verified a potential for applications in the context of multi-core processors.",
        "link": "https://dl.acm.org/doi/10.1145/3447779",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Sustainable Security for the Internet of Things Using Artificial Intelligence Architectures",
        "authors": "['Celestine Iwendi', 'Saif Ur Rehman', 'Abdul Rehman Javed', 'Suleman Khan', 'Gautam Srivastava']",
        "date": "None",
        "source": "ACM Transactions on Internet Technology",
        "abstract": "In this digital age, human dependency on technology in various fields has been increasing tremendously. Torrential amounts of different electronic products are being manufactured daily for everyday use. With this advancement in the world of Internet technology, cybersecurity of software and hardware systems are now prerequisites for major business’ operations. Every technology on the market has multiple vulnerabilities that are exploited by hackers and cyber-criminals daily to manipulate data sometimes for malicious purposes. In any system, the Intrusion Detection System (IDS) is a fundamental component for ensuring the security of devices from digital attacks. Recognition of new developing digital threats is getting harder for existing IDS. Furthermore, advanced frameworks are required for IDS to function both efficiently and effectively. The commonly observed cyber-attacks in the business domain include minor attacks used for stealing private data. This article presents a deep learning methodology for detecting cyber-attacks on the Internet of Things using a Long Short Term Networks classifier. Our extensive experimental testing show an Accuracy of 99.09%, F1-score of 99.46%, and Recall of 99.51%, respectively. A detailed metric representing our results in tabular form was used to compare how our model was better than other state-of-the-art models in detecting cyber-attacks with proficiency.",
        "link": "https://dl.acm.org/doi/10.1145/3448614",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Review on architectures of aquaponic systems based on the Internet of Things and artificial intelligence: Comparative study.",
        "authors": "['Khaoula Taji', 'Rachida Ait Abdelouahid', 'Ibtissame Ezzahoui', 'Abdelaziz Marzak']",
        "date": "April 2021",
        "source": "NISS '21: Proceedings of the 4th International Conference on Networking, Information Systems &amp; Security",
        "abstract": "The aquaponics system is an agricultural production technique that combines aquaculture with hydroponics and reduces the environmental impact by reducing the need for water and nutrients. New technologies such as the Internet of Things and artificial intelligence can be an efficient and reliable application for this purpose. They can help to implement this system in urban areas and to produce organic food. The main objective of this article is to present a comparative study between different existing aquaponic architectures according to different factors such as Security, Interoperability, Renewable energy usability, Cost, and so one, and various technologies used based on Internet of Things and artificial intelligence approaches such as sensors, actuators, gateway, communication technology, storage system, user interface etc., as well as the advantages and limitations of each proposed solution. This paper is an opening towards a new contribution that will be based on an interoperable, secure, scalable, low-cost, fully self-powered, flexible, reliable, intelligent and generic IoT architecture that meets the requirements of aquaponics. This new contribution will also make it possible to optimize the critical parameters in aquaponics, namely water quality, pH, air temperature, humidity, CO2, etc., and predict the state of the system's health to improve its productivity, thanks to artificial intelligence.",
        "link": "https://dl.acm.org/doi/10.1145/3454127.3457625",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Dynamic Partitioned Scheduling of Real-Time DAG Tasks on ARM big.LITTLE Architectures*",
        "authors": "['Agostino Mascitti', 'Tommaso Cucinotta']",
        "date": "April 2021",
        "source": "RTNS '21: Proceedings of the 29th International Conference on Real-Time Networks and Systems",
        "abstract": "This paper evaluates the combination of a Directed Acyclic Graph (DAG) task splitting technique already proposed in the literature and the state-of-the-art, energy-aware version of the well-known CBS server (BL-CBS), which dynamically partitions and schedules real-time task sets in an energy-efficient way on multi-core platforms based on the ARM big.LITTLE architecture. The approach is designed to be used with any DAG in a transparent way as an on-line and adaptive scheduler supporting “open” systems. The approach is validated and evaluated through the open-source RTSim simulator, which has been extended integrating an energy model of the ODROID-XU3 board and the code-base needed to perform the DAG task decomposition and scheduling. Simulations on randomly generated DAGs show that the approach leads to promising results.",
        "link": "https://dl.acm.org/doi/10.1145/3453417.3453442",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Gem5-X: A Many-core Heterogeneous Simulation Platform for Architectural Exploration and Optimization",
        "authors": "['Yasir Mahmood Qureshi', 'William Andrew Simon', 'Marina Zapater', 'Katzalin Olcoz', 'David Atienza']",
        "date": "None",
        "source": "ACM Transactions on Architecture and Code Optimization",
        "abstract": "The increasing adoption of smart systems in our daily life has led to the development of new applications with varying performance and energy constraints, and suitable computing architectures need to be developed for these new applications. In this article, we present gem5-X, a system-level simulation framework, based on gem-5, for architectural exploration of heterogeneous many-core systems. To demonstrate the capabilities of gem5-X, real-time video analytics is used as a case-study. It is composed of two kernels, namely, video encoding and image classification using convolutional neural networks (CNNs). First, we explore through gem5-X the benefits of latest 3D high bandwidth memory (HBM2) in different architectural configurations. Then, using a two-step exploration methodology, we develop a new optimized clustered-heterogeneous architecture with HBM2 in gem5-X for video analytics application. In this proposed clustered-heterogeneous architecture, ARMv8 in-order cluster with in-cache computing engine executes the video encoding kernel, giving 20% performance and 54% energy benefits compared to baseline ARM in-order and Out-of-Order systems, respectively. Furthermore, thanks to gem5-X, we conclude that ARM Out-of-Order clusters with HBM2 are the best choice to run visual recognition using CNNs, as they outperform DDR4-based system by up to 30% both in terms of performance and energy savings.",
        "link": "https://dl.acm.org/doi/10.1145/3461662",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Fast simulation of future 128-bit architectures",
        "authors": "['Fabien Portas', 'Frédéric Pétrot']",
        "date": "March 2022",
        "source": "DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe",
        "abstract": "Whether 128-bit architectures will some day hit the market or not is an open question. There is however a trend towards that direction: virtual addresses grew from 34 to 48 bits in 1999 and then to 57 bits in 2019. The impact of a virtually infinite addressable space on software is hard to predict, but it will most likely be major. Simulation tools are therefore needed to support research and experimentation for tooling and software. In this paper, we present the implementation of the 128-bit extension of the RISC-V architecture in the QEMU functional simulator and report first performance evaluations. On our limited set of programs, simulation is slowed down by a factor of at worst 5 compared to 64-bit simulation, making the tool still usable for executing large software codes.",
        "link": "https://dl.acm.org/doi/10.5555/3539845.3540109",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Designing calibration and expressivity-efficient instruction sets for quantum computing",
        "authors": "['Lingling Lao', 'Prakash Murali', 'Margaret Martonosi', 'Dan Browne']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "Near-term quantum computing (QC) systems have limited qubit counts, high gate (instruction) error rates, and typically support a minimal instruction set having one type of two-qubit gate (2Q). To reduce program instruction counts and improve application expressivity, vendors have proposed, and shown proof-of-concept demonstrations of richer instruction sets such as XY gates (Rigetti) and fSim gates (Google). These instruction sets comprise of families of 2Q gate types parameterized by continuous qubit rotation angles. That is, it allows a large set of different physical operations to be realized on the qubits, based on the input angles. However, having such a large number of gate types is problematic because each gate type has to be calibrated periodically, across the full system, to obtain high fidelity implementations. This results in substantial recurring calibration overheads even on current systems which use only a few gate types. Our work aims to navigate this tradeoff between application expressivity and calibration overhead, and identify what instructions vendors should implement to get the best expressivity with acceptable calibration time. Studying this tradeoff is challenging because of the diversity in QC application requirements, the need to optimize applications for widely different hardware gate types and noise variations across gate types. Therefore, our work develops NuOp, a flexible compilation pass based on numerical optimization, to efficiently decompose application operations into arbitrary hardware gate types. Using NuOp and four important quantum applications, we study the instruction set proposals of Rigetti and Google, with realistic noise simulations and a calibration model. Our experiments show that implementing 4--8 types of 2Q gates is sufficient to attain nearly the same expressivity as a full continuous gate family, while reducing the calibration overhead by two orders of magnitude. With several vendors proposing rich gate families as means to higher fidelity, our work has potential to provide valuable instruction set design guidance for near-term QC systems.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00071",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "On the Performance of Deep Learning in the Full Edge and the Full Cloud Architectures",
        "authors": "['Tajeddine Benbarrad', 'Marouane Salhaoui', 'Mounir Arioua']",
        "date": "April 2021",
        "source": "NISS '21: Proceedings of the 4th International Conference on Networking, Information Systems &amp; Security",
        "abstract": "Deep learning today surpasses various machine learning approaches in performance and is widely used for variety of different tasks. Deep learning has increased accuracy compared to other approaches for tasks like language translation and image recognition. However, training a deep learning model on a large dataset is a challenging and expensive task that can be time consuming and require large computational resources. Therefore, Different architectures have been proposed for the implementation of deep learning models in machine vision systems to deal with this problem. Currently, the application of deep learning in the cloud is the most common and typical method. Nevertheless, the challenge of having to move the data from where it is generated to a cloud data center so that it can be used to prepare and develop machine learning models represents a major limitation of this approach. As a result, it is becoming increasingly important to consider moving aspects of deep learning to the edge, instead of the cloud, especially with the rapid increase in data volumes and the growing need to act in real time. From this perspective, a comparative study between the full edge and the full cloud architectures based on the performance of the deep learning models implemented in both architectures is elaborated. The results of this study lead us to specify the strengths of both the cloud and the edge for deploying deep learning models, and to choose the optimal architecture to deal with the rapid increase in data volumes and the growing need for real-time action.",
        "link": "https://dl.acm.org/doi/10.1145/3454127.3457632",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "RingCNN: exploiting algebraically-sparse ring tensors for energy-efficient CNN-based computational imaging",
        "authors": "['Chao-Tsung Huang']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "In the era of artificial intelligence, convolutional neural networks (CNNs) are emerging as a powerful technique for computational imaging. They have shown superior quality for reconstructing fine textures from badly-distorted images and have potential to bring next-generation cameras and displays to our daily life. However, CNNs demand intensive computing power for generating high-resolution videos and defy conventional sparsity techniques when rendering dense details. Therefore, finding new possibilities in regular sparsity is crucial to enable large-scale deployment of CNN-based computational imaging. In this paper, we consider a fundamental but yet well-explored approach---algebraic sparsity---for energy-efficient CNN acceleration. We propose to build CNN models based on ring algebra that defines multiplication, addition, and non-linearity for n-tuples properly. Then the essential sparsity will immediately follow, e.g. n-times reduction for the number of real-valued weights. We define and unify several variants of ring algebras into a modeling framework, RingCNN, and make comparisons in terms of image quality and hardware complexity. On top of that, we further devise a novel ring algebra which minimizes complexity with component-wise product and achieves the best quality using directional ReLU. Finally, we design an accelerator, eRingCNN, to accommodate to the proposed ring algebra, in particular with regular ring-convolution arrays for efficient inference and on-the-fly directional ReLU blocks for fixed-point computation. We implement two configurations, n = 2 and 4 (50% and 75% sparsity), with 40 nm technology to support advanced denoising and super-resolution at up to 4K UHD 30 fps. Layout results show that they can deliver equivalent 41 TOPS using 3.76 W and 2.22 W, respectively. Compared to the real-valued counterpart, our ring convolution engines for n = 2 achieve 2.00 x energy efficiency and 2.08 x area efficiency with similar or even better image quality. With n = 4, the efficiency gains of energy and area are further increased to 3.84X and 3.77X with only 0.11 dB drop of peak signal-to-noise ratio (PSNR). The results show that RingCNN exhibits great architectural advantages for providing near-maximum hardware efficiencies and graceful quality degradation simultaneously.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00089",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Accelerating Transform Algorithm Implementation for Efficient Intra Coding of 8K UHD Videos",
        "authors": "['Yang Guo', 'Wei Gao', 'Siwei Ma', 'Ge Li']",
        "date": "None",
        "source": "ACM Transactions on Multimedia Computing, Communications, and Applications",
        "abstract": "Real-time ultra-high-definition (UHD) video applications have attracted much attention, where the encoder side urgently demands the high-throughput two-dimensional (2D) transform hardware implementation for the latest video coding standards. This article proposes an effective acceleration method for transform algorithm in UHD intra coding based on the third generation of audio video coding standard (AVS3). First, by conducting detailed statistical analysis, we devise an efficient hardware-friendly transform algorithm that can reduce running cycles and resource consumption remarkably. Second, to implement multiplierless computation for saving resources and power, a series of shift-and-add unit (SAU) hardwares are investigated to have much less adoptions of shifters and adders than the existing methods. Third, different types of hardware acceleration methods, including calculation pipelining, logical-loop unrolling, and module-level parallelism, are designed to efficaciously support the data-intensive high frame-rate 8K UHD video coding. Finally, due to the scarcity of 8K video sources, we also provide a new dataset for the performance verification. Experimental results demonstrate that our proposed method can effectively fulfill the real-time 8K intra encoding at beyond 60 fps, with very negligible loss on rate-distortion (R-D) performance, which is averagely 0.98% Bjontegaard-Delta Bit-Rate (BD-BR).",
        "link": "https://dl.acm.org/doi/10.1145/3507970",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A Pattern for a Secure Actuator Node",
        "authors": "['Cristian Orellana', 'Hernán Astudillo', 'Eduardo B. Fernandez']",
        "date": "July 2021",
        "source": "EuroPLoP'21: 26th European Conference on Pattern Languages of Programs",
        "abstract": "IoT devices are increasingly present in our lives, and today constitute a strong pillar for designing smart systems that perceive the environment and intervene in it according to predefined rules. This ability to interact with the environment has historically been the target of cyberattacks, causing erratic behavior in these systems, which brings serious consequences. This work introduces Secure Actuator Node, a pattern that addresses the design of a secure actuator that considers the identification of threats and their respective countermeasures to mitigate or eliminate these types of security issues.",
        "link": "https://dl.acm.org/doi/10.1145/3489449.3490007",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Efficient-Grad: Efficient Training Deep Convolutional Neural Networks on Edge Devices with Gradient Optimizations",
        "authors": "['Ziyang Hong', 'C. Patrick Yue']",
        "date": "None",
        "source": "ACM Transactions on Embedded Computing Systems",
        "abstract": "With the prospering of mobile devices, the distributed learning approach, enabling model training with decentralized data, has attracted great interest from researchers. However, the lack of training capability for edge devices significantly limits the energy efficiency of distributed learning in real life. This article describes Efficient-Grad, an algorithm-hardware co-design approach for training deep convolutional neural networks, which improves both throughput and energy saving during model training, with negligible validation accuracy loss.The key to Efficient-Grad is its exploitation of two observations. Firstly, the sparsity has potential for not only activation and weight, but gradients and the asymmetry residing in the gradients for the conventional back propagation (BP). Secondly, a dedicated hardware architecture for sparsity utilization and efficient data movement can be optimized to support the Efficient-Grad algorithm in a scalable manner. To the best of our knowledge, Efficient-Grad is the first approach that successfully adopts a feedback-alignment (FA)-based gradient optimization scheme for deep convolutional neural network training, which leads to its superiority in terms of energy efficiency. We present case studies to demonstrate that the Efficient-Grad design outperforms the prior arts by 3.72x in terms of energy efficiency.",
        "link": "https://dl.acm.org/doi/10.1145/3504034",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "SparseP: Towards Efficient Sparse Matrix Vector Multiplication on Real Processing-In-Memory Architectures",
        "authors": "['Christina Giannoula', 'Ivan Fernandez', 'Juan Gómez Luna', 'Nectarios Koziris', 'Georgios Goumas', 'Onur Mutlu']",
        "date": "None",
        "source": "Proceedings of the ACM on Measurement and Analysis of Computing Systems",
        "abstract": "Several manufacturers have already started to commercialize near-bank Processing-In-Memory (PIM) architectures, after decades of research efforts. Near-bank PIM architectures place simple cores close to DRAM banks. Recent research demonstrates that they can yield significant performance and energy improvements in parallel applications by alleviating data access costs. Real PIM systems can provide high levels of parallelism, large aggregate memory bandwidth and low memory access latency, thereby being a good fit to accelerate the Sparse Matrix Vector Multiplication (SpMV) kernel. SpMV has been characterized as one of the most significant and thoroughly studied scientific computation kernels. It is primarily a memory-bound kernel with intensive memory accesses due its algorithmic nature, the compressed matrix format used, and the sparsity patterns of the input matrices given. This paper provides the first comprehensive analysis of SpMV on a real-world PIM architecture, and presents SparseP, the first SpMV library for real PIM architectures. We make three key contributions. First, we implement a wide variety of software strategies on SpMV for a multithreaded PIM core, including (1) various compressed matrix formats, (2) load balancing schemes across parallel threads and (3) synchronization approaches, and characterize the computational limits of a single multithreaded PIM core. Second, we design various load balancing schemes across multiple PIM cores, and two types of data partitioning techniques to execute SpMV on thousands of PIM cores: (1) 1D-partitioned kernels to perform the complete SpMV computation only using PIM cores, and (2) 2D-partitioned kernels to strive a balance between computation and data transfer costs to PIM-enabled memory. Third, we compare SpMV execution on a real-world PIM system with 2528 PIM cores to an Intel Xeon CPU and an NVIDIA Tesla V100 GPU to study the performance and energy efficiency of various devices, i.e., both memory-centric PIM systems and conventional processor-centric CPU/GPU systems, for the SpMV kernel. SparseP software package provides 25 SpMV kernels for real PIM systems supporting the four most widely used compressed matrix formats, i.e., CSR, COO, BCSR and BCOO, and a wide range of data types. SparseP is publicly and freely available at https://github.com/CMU-SAFARI/SparseP. Our extensive evaluation using 26 matrices with various sparsity patterns provides new insights and recommendations for software designers and hardware architects to efficiently accelerate the SpMV kernel on real PIM systems.",
        "link": "https://dl.acm.org/doi/10.1145/3508041",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Branchless Code Generation for Modern Processor Architectures",
        "authors": "['Alexandros Angelou', 'Antonios Dadaliaris', 'Michael Dossis', 'Georgios Dimitriou']",
        "date": "November 2021",
        "source": "PCI '21: Proceedings of the 25th Pan-Hellenic Conference on Informatics",
        "abstract": "Compilers apply transformations to the code they compile in order to make it run faster without changing its behavior. This process is called code optimization. Modern compilers apply many different passes of code optimization to ensure maximum runtime performance and efficiency, at the rather negligible expense of larger compilation times. This study focuses on a particular optimization, called branchless optimization, which eliminates code branches by utilizing different data transformation techniques that have the same effect. Such techniques are explored on their implementation on the LLVM IR and MIPS and partly ARM assembly, and ranked based on their runtime efficiency. Moreover, the stages of implementing the optimization transformation are explored, as well as different instruction set features that some CPU architectures provide that can be used to increase the efficiency of the optimization.",
        "link": "https://dl.acm.org/doi/10.1145/3503823.3503879",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "NanoTransport: A Low-Latency, Programmable Transport Layer for NICs",
        "authors": "['Serhat Arslan', 'Stephen Ibanez', 'Alex Mallery', 'Changhoon Kim', 'Nick McKeown']",
        "date": "October 2021",
        "source": "SOSR '21: Proceedings of the ACM SIGCOMM Symposium on SDN Research (SOSR)",
        "abstract": "Transport protocols can be implemented in NIC (Network Interface Card) hardware to increase throughput, reduce latency and free up CPU cycles. If the ideal transport protocol were known, the optimal implementation would be simple: bake it into fixed-function hardware. But transport layer protocols are still evolving, with innovative new algorithms proposed every year. A recent study proposed Tonic, a Verilog-programmable transport layer in hardware. We build on this work to propose a new programmable hardware transport layer architecture, called nanoTransport, optimized for the extremely low-latency message-based RPCs (Remote Procedure Calls) that dominate large, modern distributed data center applications. NanoTransport is programmed using the P4 language, making it easy to modify existing (or create entirely new) transport protocols in hardware. We identify common events and primitive operations, allowing for a streamlined, modular, programmable pipeline, including packetization, reassembly, timeouts and packet generation, all to be expressed by the programmer. We evaluate our nanoTransport prototype by programming it to run the reliable message-based transport protocols NDP and Homa, as well as a hybrid variant. Our FPGA prototype - implemented in Chisel and running on the Firesim simulator - exposes P4-programmable pipelines and is designed to run in an ASIC at 200Gb/s with each packet processed end-to-end in less than 10ns (including message reassembly).",
        "link": "https://dl.acm.org/doi/10.1145/3482898.3483365",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Domain Decomposition Preconditioners for Unstructured Network Problems in Parallel Vector Architectures",
        "authors": "['Daniel Adrian Maldonado', 'Michel Schanen', 'François Pacaud', 'Mihai Anitescu']",
        "date": "August 2021",
        "source": "ICPP Workshops '21: 50th International Conference on Parallel Processing Workshop",
        "abstract": "In this paper we present our experience implementing domain decomposition preconditioners on vector architectures. In particular, we will focus on the solution of unstructured network equations arising from electrical power systems by preconditioning iterative algorithms with the Additive Schwarz Method (ASM). The implementation will be carried out using the Julia programming language, which allows for easy prototyping and interfacing with GPU architectures thanks to its multiple dispatch features. In our experiments, we will show the trade-off between device throughput and convergence of the iterative algorithm as the size of the domain varies, and determine optimal fronts of computational performance.",
        "link": "https://dl.acm.org/doi/10.1145/3458744.3473363",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Fast and Scalable Sparse Triangular Solver for Multi-GPU Based HPC Architectures",
        "authors": "['CHENHAO XIE', 'Jieyang Chen', 'Jesun Firoz', 'Jiajia Li', 'Shuaiwen Leon Song', 'Kevin Barker', 'Mark Raugas', 'Ang Li']",
        "date": "August 2021",
        "source": "ICPP '21: Proceedings of the 50th International Conference on Parallel Processing",
        "abstract": "Designing efficient and scalable sparse linear algebra kernels on modern multi-GPU based HPC systems is a challenging task due to significant irregular memory references and workload imbalance across GPUs. These challenges are particularly compounded in the case of Sparse Triangular Solver (SpTRSV), which introduces additional complexity of two-dimensional computation dependencies among subsequent computation steps. Dependency information may need to be exchanged and shared among GPUs, thus warranting for efficient memory allocation, data partitioning, and workload distribution as well as fine-grained communication and synchronization support. In this work, we focus on designing algorithm for SpTRSV in a single-node, multi-GPU setting. We demonstrate that directly adopting unified memory can adversely affect the performance of SpTRSV on multi-GPU architectures, despite linking via fast interconnect like NVLinks and NVSwitches. Alternatively, we employ the latest NVSHMEM technology based on Partitioned Global Address Space programming model to enable efficient fine-grained communication and drastic synchronization overhead reduction. Furthermore, to handle workload imbalance, we propose a malleable task-pool execution model which can further enhance the utilization of GPUs. By applying these techniques, our experiments on the NVIDIA multi-GPU supernode V100-DGX-1 and DGX-2 systems demonstrate that our design can achieve an average of 3.53 × (up to 9.86 ×) speedup on a DGX-1 system and 3.66 × (up to 9.64 ×) speedup on a DGX-2 system with four GPUs over the Unified-Memory design. The comprehensive sensitivity and scalability studies also show that the proposed zero-copy SpTRSV is able to fully utilize the computing and communication resources of the multi-GPU systems.",
        "link": "https://dl.acm.org/doi/10.1145/3472456.3472478",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Application of VR technology in architectural decoration engineering technology",
        "authors": "['Ran Yin']",
        "date": "October 2021",
        "source": "AIAM2021: 2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture",
        "abstract": "With the rapid development of social economy and the increasing complexity of architectural decoration projects, there is an urgent need to introduce new technologies to improve work efficiency and solve practical problems. Nowadays, the emerging VR technology, a computer-aided technology, has been widely used in actual engineering projects and has proven to greatly improve work efficiency. This article will discuss the specific application and operation of VR technology in architectural decoration engineering technology, analyze the application advantages of VR technology and look forward to its application prospects in architectural decoration engineering technology.",
        "link": "https://dl.acm.org/doi/10.1145/3495018.3501213",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "FlexMiner: a pattern-aware accelerator for graph pattern mining",
        "authors": "['Xuhao Chen', 'Tianhao Huang', 'Shuotao Xu', 'Thomas Bourgeat', 'Chanwoo Chung', 'Arvind']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "Graph pattern mining (GPM) is a class of algorithms widely used in many real-world applications in bio-medicine, e-commerce, security, social sciences, etc. GPM is a computationally intensive problem with an enormous amount of coarse-grain parallelism and therefore, attractive for hardware acceleration. Unfortunately, existing GPM accelerators have not used the best known algorithms and optimizations, and thus offer questionable benefits over software implementations. We present FlexMiner, a software/hardware co-designed GPM accelerator that improves the efficiency without compromising the generality or productivity of state-of-the-art software GPM frameworks. FlexMiner exploits massive amount of coarse-grain parallelism in GPM by deploying a large number of specialized processing elements. For efficient searches, the FlexMiner hardware accepts pattern-specific execution plans, which are generated automatically by the FlexMiner compiler from the given pat-tern(s). To avoid repetitive computation on neighborhood connectivity, we provide dedicated on-chip storage to memoize reusable connectivity information in a connectivity map (c-map) which is implemented with low-cost yet high-throughput hardware. The on-chip memories in FlexMiner are managed dynamically using heuristics derived by the compiler, and thus are fully utilized. We have evaluated FlexMiner with 4 GPM applications on a wide range of real-world graphs. Our cycle-accurate simulation shows that FlexMiner with 64 PEs achieves 10.6× speedup on average over the state-of-the-art software system executing 20 threads on a 10-core Intel CPU.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00052",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Audio-Frequency Induction Loops (AFILs) as a Design Materialfor Architectural Interactivity: An Illustrated Guide",
        "authors": "['Eleni Economidou', 'Bart Hengeveld', 'Moritz Kubesch', 'Alina Krischkowsky', 'Martin Murer', 'Manfred Tscheligi']",
        "date": "June 2021",
        "source": "DIS '21: Proceedings of the 2021 ACM Designing Interactive Systems Conference",
        "abstract": "Audio-frequency induction loops (AFILs) are commonly used as an assistive listening technology for hard-of-hearing individuals. They generate an electromagnetic field proportional to a sound source receivable by hearing aids. Our interactive system, the Sound of Space, is based on AFILs that generate a multi-dimensional soundscape in space. Cochlear implant (CI) listeners and hearing-aids wearers can experience the soundscape through bodily movement, whereas hearing individuals can experience it via a corresponding tangible device. While typical AFIL installations transmit a single sound source, in our interactive system we implement overlapping loops and their interference to locate multiple synchronised audio sources (i.e., corresponding electromagnetic fields) in space. The designed system is installed permanently in an integrative school for deaf, hard-of-hearing and hearing students and teachers. In this pictorial, we illustrate our design and implementation process and contribute our learnings of using AFILs as a design material for architectural interactivity.",
        "link": "https://dl.acm.org/doi/10.1145/3461778.3462070",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A study of work distribution and contention in database primitives on heterogeneous CPU/GPU architectures",
        "authors": "['Michael Gowanlock', 'Zane Fink', 'Ben Karsin', 'Jordan Wright']",
        "date": "March 2021",
        "source": "SAC '21: Proceedings of the 36th Annual ACM Symposium on Applied Computing",
        "abstract": "Graphics Processing Units (GPUs) provide very high on-card memory bandwidth which can be exploited to address data-intensive workloads. To maximize algorithm throughput, it is important to concurrently utilize both the CPU and GPU to carry out database queries. We select data-intensive algorithms that are common in databases and data analytic applications including: (i) scan; (ii) batched predecessor searches; (iii) multiway merging; and, (iv) partitioning. For each algorithm, we examine the performance of parallel CPU/GPU-only, and hybrid CPU/GPU approaches. There are several challenges to combining the CPU and GPU for query processing, including distributing work between architectures. We demonstrate that despite being able to accurately split the work between the CPU and GPU, contention for memory bandwidth is a major limiting factor for hybrid CPU/GPU data-intensive algorithms. We employ performance models that allow us to explore several research questions. We find that while hybrid data-intensive algorithms may be limited by contention, these algorithms are more robust to workload characteristics; therefore, they are preferable to CPU/GPU-only approaches. We also find that hybrid algorithms achieve good performance when there is low memory contention between the CPU and GPU, such that the GPU can perform its operations without significantly reducing CPU throughput.",
        "link": "https://dl.acm.org/doi/10.1145/3412841.3441913",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Blockchain Technology as A Guarantee of Transparency in The Supply Chain of Commercial Enterprises",
        "authors": "['Jonathan Lenge', 'Kaninda Musumbu', 'Geisla Wanuku', 'Pascal Sungu']",
        "date": "February 2022",
        "source": "ASSE' 22: 2022 3rd Asia Service Sciences and Software Engineering Conference",
        "abstract": "Traceability has become an essential element of supply chain management, especially in safety-sensitive sectors such as food, pharmaceuticals, etc. Upstream (manufacturers, producers, etc.) and downstream (distributors, wholesalers, etc.) supply chain actors need to store and process traceability information in order to provide proof of regulatory compliance to national authorities as well as the most demanding customers. Studies have shown that developing countries, especially on the African continent, are particularly prone to the commercialization of counterfeit products and the use of diverted consumables, thus reducing the confidence in the various products and services offered by commercial enterprises, not only towards consumers but also towards companies and other actors of the concerned sectors. Therefore, the main objective of this paper is to examine the characteristics and functionalities of blockchain technology and to identify blockchain-based solutions to address consumer product traceability issues, as well as to highlight the benefits of implementing blockchain-based traceability systems by proposing a blockchain architecture to ensure traceability and transparency.",
        "link": "https://dl.acm.org/doi/10.1145/3523181.3523182",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Achieving High In Situ Training Accuracy and Energy Efficiency with Analog Non-Volatile Synaptic Devices",
        "authors": "['Shanshi Huang', 'Xiaoyu Sun', 'Xiaochen Peng', 'Hongwu Jiang', 'Shimeng Yu']",
        "date": "None",
        "source": "ACM Transactions on Design Automation of Electronic Systems",
        "abstract": "On-device embedded artificial intelligence prefers the adaptive learning capability when deployed in the field, and thus in situ training is required. The compute-in-memory approach, which exploits the analog computation within the memory array, is a promising solution for deep neural network (DNN) on-chip acceleration. Emerging non-volatile memories are of great interest, serving as analog synapses due to their multilevel programmability. However, the asymmetry and nonlinearity in the conductance tuning remain grand challenges for achieving high in situ training accuracy. In addition, analog-to-digital converters at the edge of the memory array introduce quantization errors. In this work, we present an algorithm-hardware co-optimization to overcome these challenges. We incorporate the device/circuit non-ideal effects into the DNN propagation and weight update steps. By introducing the adaptive “momentum” in the weight update rule, in situ training accuracy on CIFAR-10 could approach its software baseline even under severe asymmetry/nonlinearity and analog-to-digital converter quantization error. The hardware performance of the on-chip training architecture and the overhead for adding “momentum” are also evaluated. By optimizing the backpropagation dataflow, 23.59 TOPS/W training energy efficiency (12× improvement compared to naïve dataflow) is achieved. The circuits that handle “momentum” introduce only 4.2% energy overhead. Our results show great potential and more relaxed requirements that enable emerging non-volatile memories for DNN acceleration on the embedded artificial intelligence platforms.",
        "link": "https://dl.acm.org/doi/10.1145/3500929",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "First Step Towards μNet: Open-Access Aquatic Testbeds and Robotic Ecosystem",
        "authors": "['Scott Mayberry', 'Junkai Wang', 'Qiuyang Tao', 'Fumin Zhang', 'Aijun Song', 'Xiaoyan Hong', 'Shuai Dong', 'Connor Webb', 'Dmitrii Dugaev', 'Zheng Peng']",
        "date": "November 2021",
        "source": "WUWNet '21: Proceedings of the 15th International Conference on Underwater Networks &amp; Systems",
        "abstract": "Aquatic-based research requires a special set of facilities, expertise, and supporting personnel, all of which are costly and often not accessible to small research groups. A shared research infrastructure, particularly testbeds and open-access software, would lower the barrier to entry, decrease implementation time, and mitigate the risk of failure in harsh underwater environments. We have made the first step towards developing μNet, an open-access aquatic testbed and robotic ecosystem addressing the need for shared infrastructure. Our contributions include the initial steps towards an indoor testbed, an outdoor testbed, and an open-access software suite.",
        "link": "https://dl.acm.org/doi/10.1145/3491315.3491322",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "FCOSMask: Fully Convolutional One-Stage Face Mask Wearing Detection Based on MobileNetV3",
        "authors": "['Yang Yu', 'Jie Lu', 'Chao Huang', 'Bo Xiao']",
        "date": "October 2021",
        "source": "CSAE '21: Proceedings of the 5th International Conference on Computer Science and Application Engineering",
        "abstract": "Wearing masks correctly in public is one major self-prevention method against the worldwide Coronavirus disease 2019 (COVID-19). This paper proposes FCOSMask, a fully convolutional one-stage face mask wearing detector based on the lightweight network, for emergency epidemic control and long-term epidemic prevention work. MobileNetV3 is applied as the backbone network to reduce computational overhead. Thus, complex calculation related to anchor boxes is avoided in the anchor-free method, and Complete Intersection over Union (CIoU) loss is selected as the bounding box regression loss function to speed up model convergence. Experiments show that compared to other anchor-based methods, detection speed of FCOSMask is improved around 3 to 4 times on self-established datasets and mean average precision (mAP) achieves 92.4%, which meets the accuracy and real-time requirements of the face mask wearing detection task in most public areas. Finally, a Web-based face mask wearing system is developed that can support public epidemic prevention and control management.",
        "link": "https://dl.acm.org/doi/10.1145/3487075.3487078",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "FINGERS: exploiting fine-grained parallelism in graph mining accelerators",
        "authors": "['Qihang Chen', 'Boyu Tian', 'Mingyu Gao']",
        "date": "February 2022",
        "source": "ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "Graph mining is an emerging application of high importance and also with high complexity, thus requiring efficient hardware acceleration. Current accelerator designs only utilize coarse-grained parallelism, leaving large room for further optimizations. Our key insight is to fully exploit fine-grained parallelism to overcome the existing issues of hardware underutilization, inefficient resource provision, and limited single-thread performance under imbalanced loads. Targeting pattern-aware graph mining algorithms, we first comprehensively identify and analyze the abundant fine-grained parallelism at the branch, set, and segment levels during search tree exploration and set operations. We then propose a novel graph mining accelerator, FINGERS, which effectively exploits these multiple levels of fine-grained parallelism to achieve significant performance improvements. FINGERS mainly enhances the design of each single processing element with parallel compute units for set operations, and efficient techniques for task scheduling, load balancing, and data aggregation. FINGERS outperforms the state-of-the-art design by 2.8× on average and up to 8.9× with the same chip area. We also demonstrate that different patterns and different graphs exhibit drastically different parallelism opportunities, justifying the necessity of exploiting all levels of fine-grained parallelism in FINGERS.",
        "link": "https://dl.acm.org/doi/10.1145/3503222.3507730",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "MT-DLA: An Efficient Multi-Task Deep Learning Accelerator Design",
        "authors": "['Mengdi Wang', 'Bing Li', 'Ying Wang', 'Cheng Liu', 'Xiaohan Ma', 'Xiandong Zhao', 'Lei Zhang']",
        "date": "June 2021",
        "source": "GLSVLSI '21: Proceedings of the 2021 on Great Lakes Symposium on VLSI",
        "abstract": "Multi-task learning systems are commonly adopted in many real-world AI applications such as intelligent robots and self-driving vehicles. Instead of improving single-network performance, this work proposes a specialized Multi-Task Deep Learning Accelerator architecture, MT-DLA, to improve the performance of concurrent networks by exploiting the shared feature and parameters across these models. It is shown in our evaluation with realistic multi-task workloads, MT-DLA dramatically eliminates the memory and computation overhead caused by the shared parameters, activations and computation result. In the experiments with real-world multi-task learning workloads, MT-DLA brings about 1.4x-7.0x energy efficiency boost when compared to the baseline neural network accelerator without multi-task support.",
        "link": "https://dl.acm.org/doi/10.1145/3453688.3461514",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A full-stack search technique for domain optimized deep learning accelerators",
        "authors": "['Dan Zhang', 'Safeen Huda', 'Ebrahim Songhori', 'Kartik Prabhu', 'Quoc Le', 'Anna Goldie', 'Azalia Mirhoseini']",
        "date": "February 2022",
        "source": "ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "The rapidly-changing deep learning landscape presents a unique opportunity for building inference accelerators optimized for specific datacenter-scale workloads. We propose Full-stack Accelerator Search Technique (FAST), a hardware accelerator search framework that defines a broad optimization environment covering key design decisions within the hardware-software stack, including hardware datapath, software scheduling, and compiler passes such as operation fusion and tensor padding. In this paper, we analyze bottlenecks in state-of-the-art vision and natural language processing (NLP) models, including EfficientNet and BERT, and use FAST to design accelerators capable of addressing these bottlenecks. FAST-generated accelerators optimized for single workloads improve Perf/TDP by 3.7× on average across all benchmarks compared to TPU-v3. A FAST-generated accelerator optimized for serving a suite of workloads improves Perf/TDP by 2.4× on average compared to TPU-v3. Our return on investment analysis shows that FAST-generated accelerators can potentially be practical for moderate-sized datacenter deployments.",
        "link": "https://dl.acm.org/doi/10.1145/3503222.3507767",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A Flexible Multichannel EEG Artifact Identification Processor using Depthwise-Separable Convolutional Neural Networks",
        "authors": "['Mohit Khatwani', 'Hasib-Al Rashid', 'Hirenkumar Paneliya', 'Mark Horton', 'Nicholas Waytowich', 'W. David Hairston', 'Tinoosh Mohsenin']",
        "date": "None",
        "source": "ACM Journal on Emerging Technologies in Computing Systems",
        "abstract": "This article presents an energy-efficient and flexible multichannel Electroencephalogram (EEG) artifact identification network and its hardware using depthwise and separable convolutional neural networks. EEG signals are recordings of the brain activities. EEG recordings that are not originated from cerebral activities are termed artifacts. Our proposed model does not need expert knowledge for feature extraction or pre-processing of EEG data and has a very efficient architecture implementable on mobile devices. The proposed network can be reconfigured for any number of EEG channel and artifact classes. Experiments were done with the proposed model with the goal of maximizing the identification accuracy while minimizing the weight parameters and required number of operations. Our proposed network achieves 93.14% classification accuracy using an EEG dataset collected by 64-channel BioSemi ActiveTwo headsets, averaged across 17 patients and 10 artifact classes. Our hardware architecture is fully parameterized with number of input channels, filters, depth, and data bit-width. The number of processing engines (PE) in the proposed hardware can vary between 1 to 16, providing different latency, throughput, power, and energy efficiency measurements. We implement our custom hardware architecture on Xilinx FPGA (Artix-7), which on average consumes 1.4 to 4.7 mJ dynamic energy with different PE configurations. Energy consumption is further reduced by 16.7× implementing on application-specified integrated circuit at the post layout level in 65-nm CMOS technology. Our FPGA implementation is 1.7 × to 5.15 × higher in energy efficiency than some previous works. Moreover, our Application-Specified Integrated Circuit implementation is also 8.47 × to 25.79 × higher in energy efficiency compared to previous works. We also demonstrated that the proposed network is reconfigurable to detect artifacts from another EEG dataset collected in our lab by a 14-channel Emotiv EPOC+ headset and achieved 93.5% accuracy for eye blink artifact detection.",
        "link": "https://dl.acm.org/doi/10.1145/3427471",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Efficient and scalable core multiplexing with M³v",
        "authors": "['Nils Asmussen', 'Sebastian Haas', 'Carsten Weinhold', 'Till Miemietz', 'Michael Roitzsch']",
        "date": "February 2022",
        "source": "ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "The M³ system (ASPLOS ’16) proposed a hardware/software co-design that simplifies integration between general-purpose cores and special-purpose accelerators, allowing users to easily utilize them in a unified manner. M³ is a tiled architecture, whose tiles (cores and accelerators) are partitioned between applications, such that each tile is dedicated to its own application. The M³x system (ATC ’19) extended M³ by trading off some isolation to enable coarse-grained multiplexing of tiles among multiple applications. With M³x, if source tile t₁ runs code of application p and sends a message m to destination tile t₂ while t₂ is currently not associated with p, then m is forwarded to the right place through a “slow path”, via some special OS tile. In this paper, we present M³v, which extends M³x by further trading off some isolation between applications to support “fast path” communication that does not require the said OS tile’s involvement. Thus, with M³v, a tile can be efficiently multiplexed between applications provided it is a general-purpose core. M³v achieves this goal by 1) adding a local multiplexer to each such core, and by 2) virtualizing the core’s hardware component responsible for cross-tile communications. We prototype M³v using RISC-V cores on an FPGA platform and show that it significantly outperforms M³x and may achieve competitive performance to Linux.",
        "link": "https://dl.acm.org/doi/10.1145/3503222.3507741",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "XJoin: Portable, parallel hash join across diverse XPU architectures with oneAPI",
        "authors": "['Eugenio Marinelli', 'Raja Appuswamy']",
        "date": "June 2021",
        "source": "DAMON '21: Proceedings of the 17th International Workshop on Data Management on New Hardware",
        "abstract": "Modern server hardware is increasingly heterogeneous with a diverse mix of XPU architectures deployed across CPU, GPU, and FPGAs. However, till date, database developers have had to rely on either proprietary, architecture-specific solutions (like CUDA), or low-level, cross-architecture solutions that complicate development (like OpenCL). The lack of portable parallelism caused by the absence of a common high-level programming framework is one of the main reasons preventing a wider adoption of XPUs by database systems. In this paper, we take the first steps towards solving this problem using oneAPI-a cross-industry effort for developing an open, standards-based unified programming model that extends standard C++ to provide portable parallelism across diverse processor architectures. In particular, we port a recently-proposed, highly-optimized, GPU-based hash join algorithm from CUDA to Data Parallel C++ (DPC++). We then execute the hash join on multicore CPUs, integrated GPUs (Intel GEN9), and discrete GPUs (Intel DG1 and NVIDIA GeForce) without changing a single line of kernel code to demonstrate that DPC++ enables portable parallelism. We compare the performance of DPC++ kernels with hand-optimized CUDA kernels and model-based theoretical performance bounds to demonstrate the performance-portability trade off in using DPC++.",
        "link": "https://dl.acm.org/doi/10.1145/3465998.3466012",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Warehouse-scale video acceleration: co-design and deployment in the wild",
        "authors": "['Parthasarathy Ranganathan', 'Daniel Stodolsky', 'Jeff Calow', 'Jeremy Dorfman', 'Marisabel Guevara', 'Clinton Wills Smullen IV', 'Aki Kuusela', 'Raghu Balasubramanian', 'Sandeep Bhatia', 'Prakash Chauhan', 'Anna Cheung', 'In Suk Chong', 'Niranjani Dasharathi', 'Jia Feng', 'Brian Fosco', 'Samuel Foss', 'Ben Gelb', 'Sara J. Gwin', 'Yoshiaki Hase', 'Da-ke He', 'C. Richard Ho', 'Roy W. Huffman Jr.', 'Elisha Indupalli', 'Indira Jayaram', 'Poonacha Kongetira', 'Cho Mon Kyaw', 'Aaron Laursen', 'Yuan Li', 'Fong Lou', 'Kyle A. Lucke', 'JP Maaninen', 'Ramon Macias', 'Maire Mahony', 'David Alexander Munday', 'Srikanth Muroor', 'Narayana Penukonda', 'Eric Perkins-Argueta', 'Devin Persaud', 'Alex Ramirez', 'Ville-Mikko Rautio', 'Yolanda Ripley', 'Amir Salek', 'Sathish Sekar', 'Sergey N. Sokolov', 'Rob Springer', 'Don Stark', 'Mercedes Tan', 'Mark S. Wachsler', 'Andrew C. Walton', 'David A. Wickeraad', 'Alvin Wijaya', 'Hon Kwan Wu']",
        "date": "April 2021",
        "source": "ASPLOS '21: Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "Video sharing (e.g., YouTube, Vimeo, Facebook, TikTok) accounts for the majority of internet traffic, and video processing is also foundational to several other key workloads (video conferencing, virtual/augmented reality, cloud gaming, video in Internet-of-Things devices, etc.). The importance of these workloads motivates larger video processing infrastructures and – with the slowing of Moore’s law – specialized hardware accelerators to deliver more computing at higher efficiencies. This paper describes the design and deployment, at scale, of a new accelerator targeted at warehouse-scale video transcoding. We present our hardware design including a new accelerator building block – the video coding unit (VCU) – and discuss key design trade-offs for balanced systems at data center scale and co-designing accelerators with large-scale distributed software systems. We evaluate these accelerators “in the wild\" serving live data center jobs, demonstrating 20-33x improved efficiency over our prior well-tuned non-accelerated baseline. Our design also enables effective adaptation to changing bottlenecks and improved failure management, and new workload capabilities not otherwise possible with prior systems. To the best of our knowledge, this is the first work to discuss video acceleration at scale in large warehouse-scale environments.",
        "link": "https://dl.acm.org/doi/10.1145/3445814.3446723",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "E-BATCH: Energy-Efficient and High-Throughput RNN Batching",
        "authors": "['Franyell Silfa', 'Jose Maria Arnau', 'Antonio González']",
        "date": "None",
        "source": "ACM Transactions on Architecture and Code Optimization",
        "abstract": "Recurrent Neural Network (RNN) inference exhibits low hardware utilization due to the strict data dependencies across time-steps. Batching multiple requests can increase throughput. However, RNN batching requires a large amount of padding since the batched input sequences may vastly differ in length. Schemes that dynamically update the batch every few time-steps avoid padding. However, they require executing different RNN layers in a short time span, decreasing energy efficiency. Hence, we propose E-BATCH, a low-latency and energy-efficient batching scheme tailored to RNN accelerators. It consists of a runtime system and effective hardware support. The runtime concatenates multiple sequences to create large batches, resulting in substantial energy savings. Furthermore, the accelerator notifies it when the evaluation of an input sequence is done. Hence, a new input sequence can be immediately added to a batch, thus largely reducing the amount of padding. E-BATCH dynamically controls the number of time-steps evaluated per batch to achieve the best trade-off between latency and energy efficiency for the given hardware platform. We evaluate E-BATCH on top of E-PUR and TPU. E-BATCH improves throughput by 1.8× and energy efficiency by 3.6× in E-PUR, whereas in TPU, it improves throughput by 2.1× and energy efficiency by 1.6×, over the state-of-the-art.",
        "link": "https://dl.acm.org/doi/10.1145/3499757",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Instruments of Vision: Eye-Tracking and Robotics as an Embodied Interface",
        "authors": "['Irem Bugdayci', 'Anne-Heloise Dautel', 'Robert Wuss', 'Ruairi Glynn']",
        "date": "None",
        "source": "Proceedings of the ACM on Computer Graphics and Interactive Techniques",
        "abstract": "In the age of ubiquitous visual technologies and systems, our perceptive apparatuses are constantly challenged, adapted, and shaped by instruments and machines, rendering the observing body as an active site of knowledge. Your Eye's Motion by Luna is an interactive installation that uses real-time eye-tracking to control a robotic creature named Luna (Figure 1). Materializing eye movements through a wondrous spectacle of light, motion, and color, the observer becomes conscious of her gaze enacted and extended by a robotic counterpart. Building on a diverse set of theories and understandings of vision from the fields of cybernetics, visual studies, embodied mind, and more, the project explores how our perceptual apparatuses and bodies are reconfigured in relation to machines and the environment to afford new ways of seeing. Once we see how observing bodies accommodate feedback from actions to cognition, we can uncover the embodied and affective potential of eye movement as an interface for robotics. The curiosity of Luna invests in this potential, articulating a unity between our embodied percepts and machinic environments to create a \"vision machine.\"",
        "link": "https://dl.acm.org/doi/10.1145/3465618",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Deploying Multi-tenant FPGAs within Linux-based Cloud Infrastructure",
        "authors": "['Joel Mandebi Mbongue', 'Danielle Tchuinkou Kwadjo', 'Alex Shuping', 'Christophe Bobda']",
        "date": "None",
        "source": "ACM Transactions on Reconfigurable Technology and Systems",
        "abstract": "Cloud deployments now increasingly exploit Field-Programmable Gate Array (FPGA) accelerators as part of virtual instances. While cloud FPGAs are still essentially single-tenant, the growing demand for efficient hardware acceleration paves the way to FPGA multi-tenancy. It then becomes necessary to explore architectures, design flows, and resource management features that aim at exposing multi-tenant FPGAs to the cloud users. In this article, we discuss a hardware/software architecture that supports provisioning space-shared FPGAs in Kernel-based Virtual Machine (KVM) clouds. The proposed hardware/software architecture introduces an FPGA organization that improves hardware consolidation and support hardware elasticity with minimal data movement overhead. It also relies on VirtIO to decrease communication latency between hardware and software domains. Prototyping the proposed architecture with a Virtex UltraScale+ FPGA demonstrated near specification maximum frequency for on-chip data movement and high throughput in virtual instance access to hardware accelerators. We demonstrate similar performance compared to single-tenant deployment while increasing FPGA utilization, which is one of the goals of virtualization. Overall, our FPGA design achieved about 2× higher maximum frequency than the state of the art and a bandwidth reaching up to 28 Gbps on 32-bit data width.",
        "link": "https://dl.acm.org/doi/10.1145/3474058",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Demystifying the system vulnerability stack: transient fault effects across the layers",
        "authors": "['George Papadimitriou', 'Dimitris Gizopoulos']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "In this paper, we revisit the system vulnerability stack for transient faults. We reveal severe pitfalls in widely used vulnerability measurement approaches, which separate the hardware and the software layers. We rely on microarchitecture level fault injection to derive very tight full-system vulnerability measurements. For our architectural and microarchitectural measurements, we employ GeFIN, a state-of-the-art fault injector built on top of the gem5 simulator, while for software level measurements we employ the LLFI fault injector. Analyzing two different Arm ISAs and two different microarchitectures for each ISA, we quantify the sources and the magnitude of error of architecture and software level vulnerability evaluation methods, which aim to reproduce the effects of hardware faults. We show that widely applied methodologies for system resilience evaluation fail to capture important fault manifestation and propagation aspects and lead to misleading findings, which report opposite vulnerability results than a comprehensive cross-layer analysis. To justify the validity of our findings we employ a state-of-the-art software-based fault tolerance technique and evaluate its impact at all layers through a case study. Our evaluation shows that although higher-level methods can report significant vulnerability improvements (up to 3.8x vulnerability reduction), the actual cross-layer vulnerability of the protected system can be degraded (increased) by up to 30% for the selected benchmarks. Our analysis firmly suggests that only accurate methodologies for full-system vulnerability evaluation of a microprocessor can guide informed transient faults protection decisions either at the hardware or at the software layer.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00075",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "TENET: a framework for modeling tensor dataflow based on relation-centric notation",
        "authors": "['Liqiang Lu', 'Naiqing Guan', 'Yuyue Wang', 'Liancheng Jia', 'Zizhang Luo', 'Jieming Yin', 'Jason Cong', 'Yun Liang']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "Accelerating tensor applications on spatial architectures provides high performance and energy-efficiency, but requires accurate performance models for evaluating various dataflow alternatives. Such modeling relies on the notation of tensor dataflow and the formulation of performance metrics. Recent proposed compute-centric and data-centric notations describe the dataflow using imperative directives. However, these two notations are less expressive and thus lead to limited optimization opportunities and inaccurate performance models. In this paper, we propose a framework TENET that models hardware dataflow of tensor applications. We start by introducing a relation-centric notation, which formally describes the hardware dataflow for tensor computation. The relation-centric notation specifies the hardware dataflow, PE interconnection, and data assignment in a uniform manner using relations. The relation-centric notation is more expressive than the compute-centric and data-centric notations by using more sophisticated affine transformations. Another advantage of relation-centric notation is that it inherently supports accurate metrics estimation, including data reuse, bandwidth, latency, and energy. TENET computes each performance metric by counting the relations using integer set structures and operators. Overall, TENET achieves 37.4% and 51.4% latency reduction for CONV and GEMM kernels compared with the state-of-the-art data-centric notation by identifying more sophisticated hardware dataflows.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00062",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A novel network fabric for efficient spatio-temporal reduction in flexible DNN accelerators",
        "authors": "['Francisco Muñoz-Martínez', 'José L. Abellán', 'Manuel E. Acacio', 'Tushar Krishna']",
        "date": "October 2021",
        "source": "NOCS '21: Proceedings of the 15th IEEE/ACM International Symposium on Networks-on-Chip",
        "abstract": "Increasing deployment of Deep Neural Networks (DNNs) in a myriad of applications, has recently fueled interest in the development of specific accelerator architectures capable of meeting their stringent performance and energy consumption requirements. DNN accelerators use three separate NoCs within the accelerator, namely distribution, multiplier and reduction networks (or DN, MN and RN, respectively) between the global buffer(s) and compute units (multipliers/adders). These NoCs enable data delivery, and more importantly, on-chip reuse of operands and outputs to minimize the expensive off-chip memory accesses. Among them, the RN, used to generate and reduce the partial sums produced during DNN processing, is what implies the largest fraction of chip area (25% of the total chip area in some cases) and power dissipation (38% of the total chip power budget), thus representing a first-order driver of the energy efficiency of the accelerator. RNs can be orchestrated to exploit a Temporal, Spatial or Spatio-Temporal reduction dataflow. Among these, the latter is the one that has shown superior performance. However, as we demonstrate in this work, a state-of-the-art implementation of the Spatio-Temporal reduction dataflow, based on the addition of Accumulators (Ac) to the RN (i.e. RN+Ac strategy), can result into significant area and energy expenses. To cope with this important issue, we propose STIFT (that stands for Spatio-Temporal Integrated Folding Tree) that implements the Spatio-Temporal reduction dataflow entirely on the RN hardware substrate (i.e. without the need of the extra accumulators). STIFT results into significant area and power savings regarding the more complex RN+Ac strategy, at the same time its performance advantage is preserved.",
        "link": "https://dl.acm.org/doi/10.1145/3479876.3481602",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "FORMS: fine-grained polarized ReRAM-based in-situ computation for mixed-signal DNN accelerator",
        "authors": "['Geng Yuan', 'Payman Behnam', 'Zhengang Li', 'Ali Shafiee', 'Sheng Lin', 'Xiaolong Ma', 'Hang Liu', 'Xuehai Qian', 'Mahdi Nazm Bojnordi', 'Yanzhi Wang', 'Caiwen Ding']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "Recent work demonstrated the promise of using resistive random access memory (ReRAM) as an emerging technology to perform inherently parallel analog domain in-situ matrix-vector multiplication---the intensive and key computation in deep neural networks (DNNs). One key problem is the weights that are signed values. However, in a ReRAM crossbar, weights are stored as conductance of the crossbar cells, and the in-situ computation assumes all cells on each crossbar column are of the same sign. The current architectures either use two ReRAM crossbars for positive and negative weights (PRIME), or add an offset to weights so that all values become positive (ISAAC). Neither solution is ideal: they either double the cost of crossbars, or incur extra offset circuity. To better address this problem, we propose FORMS, a fine-grained ReRAM-based DNN accelerator with algorithm/hardware co-design. Instead of trying to represent the positive/negative weights, our key design principle is to enforce exactly what is assumed in the in-situ computation---ensuring that all weights in the same column of a crossbar have the same sign. It naturally avoids the cost of an additional crossbar. Such polarized weights can be nicely generated using alternating direction method of multipliers (ADMM) regularized optimization during the DNN training, which can exactly enforce certain patterns in DNN weights. To achieve high accuracy, we divide the crossbar into logical sub-arrays and only enforce this property within the fine-grained sub-array columns. Crucially, the small sub-arrays provides a unique opportunity for input zero-skipping, which can significantly avoid unnecessary computations and reduce computation time. At the same time, it also makes the hardware much easier to implement and is less susceptible to non-idealities and noise than coarse-grained architectures. Putting all together, with the same optimized DNN models, FORMS achieves 1.50× and 1.93× throughput improvement in terms of and GOPs/sxmm2 and GOPs/W compared to ISAAC, and 1.12× ~ 2.4× speed up in terms of frame per second over optimized ISAAC with almost the same power/area cost. Interestingly, FORMS optimization framework can even speed up the original ISAAC from 10.7× up to 377.9×, reflecting the importance of software/hardware co-design optimizations.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00029",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "S2Dedup: SGX-enabled secure deduplication",
        "authors": "['Mariana Miranda', 'Tânia Esteves', 'Bernardo Portela', 'João Paulo']",
        "date": "June 2021",
        "source": "SYSTOR '21: Proceedings of the 14th ACM International Conference on Systems and Storage",
        "abstract": "Secure deduplication allows removing duplicate content at third-party storage services while preserving the privacy of users' data. However, current solutions are built with strict designs that cannot be adapted to storage service and applications with different security and performance requirements. We present S2Dedup, a trusted hardware-based privacy-preserving deduplication system designed to support multiple security schemes that enable different levels of performance, security guarantees and space savings. An in-depth evaluation shows these trade-offs for the distinct Intel SGX-based secure schemes supported by our prototype. Moreover, we propose a novel Epoch and Exact Frequency scheme that prevents frequency analysis leakage attacks present in current deterministic approaches for secure deduplication while maintaining similar performance and space savings to state-of-the-art approaches.",
        "link": "https://dl.acm.org/doi/10.1145/3456727.3463773",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Sextans: A Streaming Accelerator for General-Purpose Sparse-Matrix Dense-Matrix Multiplication",
        "authors": "['Linghao Song', 'Yuze Chi', 'Atefeh Sohrabizadeh', 'Young-kyu Choi', 'Jason Lau', 'Jason Cong']",
        "date": "February 2022",
        "source": "FPGA '22: Proceedings of the 2022 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays",
        "abstract": "Sparse-Matrix Dense-Matrix multiplication (SpMM) is the key operator for a wide range of applications including scientific computing, graph processing, and deep learning. Architecting accelerators for SpMM is faced with three challenges - (1) the random memory accessing and unbalanced load in processing because of random distribution of elements in sparse matrices, (2) inefficient data handling of the large matrices which can not be fit on-chip, and (3) a non-general-purpose accelerator design where one accelerator can only process a fixed-size problem. In this paper, we present Sextans, an accelerator for general-purpose SpMM processing. Sextans accelerator features (1) fast random access using on-chip memory, (2) streaming access to off-chip large matrices, (3) PE-aware non-zero scheduling for balanced workload with an II=1 pipeline, and (4) hardware flexibility to enable prototyping the hardware once to support SpMMs of different size as a general-purpose accelerator. We leverage high bandwidth memory (HBM) for the efficient accessing of both sparse and dense matrices. In the evaluation, we present an FPGA prototype Sextans which is executable on a Xilinx U280 HBM FPGA board and a projected prototype Sextans-P with higher bandwidth competitive to V100 and more frequency optimization. We conduct a comprehensive evaluation on 1,400 SpMMs on a wide range of sparse matrices including 50 matrices from SNAP and 150 from SuiteSparse. We compare Sextans with NVIDIA K80 and V100 GPUs. Sextans achieves a 2.50x geomean speedup over K80 GPU, and Sextans-P achieves a 1.14x geomean speedup over V100 GPU (4.94x over K80). The code is available at https://github.com/linghaosong/Sextans.",
        "link": "https://dl.acm.org/doi/10.1145/3490422.3502357",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "NN-baton: DNN workload orchestration and chiplet granularity exploration for multichip accelerators",
        "authors": "['Zhanhong Tan', 'Hongyu Cai', 'Runpei Dong', 'Kaisheng Ma']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "The revolution of machine learning poses an unprecedented demand for computation resources, urging more transistors on a single monolithic chip, which is not sustainable in the Post-Moore era. The multichip integration with small functional dies, called chiplets, can reduce the manufacturing cost, improve the fabrication yield, and achieve die-level reuse for different system scales. DNN workload mapping and hardware design space exploration on such multichip systems are critical, but missing in the current stage. This work provides a hierarchical and analytical framework to describe the DNN mapping on a multichip accelerator and analyze the communication overhead. Based on this framework, we propose an automatic tool called NN-Baton with a pre-design flow and a post-design flow. The pre-design flow aims to guide the chiplet granularity exploration with given area and performance budgets for the target workload. The post-design flow focuses on the workload orchestration on different computation levels - package, chiplet, and core - in the hierarchy. Compared to Simba, NN-Baton generates mapping strategies that save 22.5%~44% energy under the same computation and memory configurations. The architecture exploration demonstrates that area is a decisive factor for the chiplet granularity. For a 2048-MAC system under a 2 mm2 chiplet area constraint, the 4-chiplet implementation with 4 cores and 16 lanes of 8-size vector-MAC is always the top-pick computation allocation across several benchmarks. In contrast, the optimal memory allocation policy in the hierarchy typically depends on the neural network models.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00083",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "DOTA: detect and omit weak attentions for scalable transformer acceleration",
        "authors": "['Zheng Qu', 'Liu Liu', 'Fengbin Tu', 'Zhaodong Chen', 'Yufei Ding', 'Yuan Xie']",
        "date": "February 2022",
        "source": "ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "Transformer Neural Networks have demonstrated leading performance in many applications spanning over language understanding, image processing, and generative modeling. Despite the impressive performance, long-sequence Transformer processing is expensive due to quadratic computation complexity and memory consumption of self-attention. In this paper, we present DOTA, an algorithm-architecture co-design that effectively addresses the challenges of scalable Transformer inference. Based on the insight that not all connections in an attention graph are equally important, we propose to jointly optimize a lightweight Detector with the Transformer model to accurately detect and omit weak connections during runtime. Furthermore, we design a specialized system architecture for end-to-end Transformer acceleration using the proposed attention detection mechanism. Experiments on a wide range of benchmarks demonstrate the superior performance of DOTA over other solutions. In summary, DOTA achieves 152.6x and 4.5x performance speedup and orders of magnitude energy-efficiency improvements over GPU and customized hardware, respectively.",
        "link": "https://dl.acm.org/doi/10.1145/3503222.3507738",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Accelerating On-Chip Training with Ferroelectric-Based Hybrid Precision Synapse",
        "authors": "['Yandong Luo', 'Panni Wang', 'Shimeng Yu']",
        "date": "None",
        "source": "ACM Journal on Emerging Technologies in Computing Systems",
        "abstract": "In this article, we propose a hardware accelerator design using ferroelectric transistor (FeFET)-based hybrid precision synapse (HPS) for deep neural network (DNN) on-chip training. The drain erase scheme for FeFET programming is incorporated for both FeFET HPS design and FeFET buffer design. By using drain erase, high-density FeFET buffers can be integrated onchip to store the intermediate input-output activations and gradients, which reduces the energy consuming off-chip DRAM access. Architectural evaluation results show that the energy efficiency could be improved by 1.2× ∼ 2.1×, 3.9× ∼ 6.0× compared to the other HPS-based designs and emerging non-volatile memory baselines, respectively. The chip area is reduced by 19% ∼ 36% compared with designs using SRAM on-chip buffer even though the capacity of FeFET buffer is increased. Besides, by utilizing drain erase scheme for FeFET programming, the chip area is reduced by 11% ∼ 28.5% compared with the designs using body erase scheme.",
        "link": "https://dl.acm.org/doi/10.1145/3473461",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A RISC-V in-network accelerator for flexible high-performance low-power packet processing",
        "authors": "['Salvatore Di Girolamo', 'Andreas Kurth', 'Alexandru Calotoiu', 'Thomas Benz', 'Timo Schneider', 'Jakub Beránek', 'Luca Benini', 'Torsten Hoefler']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "The capacity of offloading data and control tasks to the network is becoming increasingly important, especially if we consider the faster growth of network speed when compared to CPU frequencies. In-network compute alleviates the host CPU load by running tasks directly in the network, enabling additional computation/communication overlap and potentially improving overall application performance. However, sustaining bandwidths provided by next-generation networks, e.g., 400 Gbit/s, can become a challenge. sPIN is a programming model for in-NIC compute, where users specify handler functions that are executed on the NIC, for each incoming packet belonging to a given message or flow. It enables a CUDA-like acceleration, where the NIC is equipped with lightweight processing elements that process network packets in parallel. We investigate the architectural specialties that a sPIN NIC should provide to enable high-performance, low-power, and flexible packet processing. We introduce PsPIN, a first open-source sPIN implementation, based on a multi-cluster RISC-V architecture and designed according to the identified architectural specialties. We investigate the performance of PsPIN with cycle-accurate simulations, showing that it can process packets at 400 Gbit/s for several use cases, introducing minimal latencies (26 ns for 64 B packets) and occupying a total area of 18.5 mm2 (22 nm FDSOI).",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00079",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Patterns for Blockchain-Based Payment Applications",
        "authors": "['Qinghua Lu', 'Xiwei Xu', 'H.M.N. Dilum Bandara', 'Shiping Chen', 'Liming Zhu']",
        "date": "July 2021",
        "source": "EuroPLoP'21: 26th European Conference on Pattern Languages of Programs",
        "abstract": "As the killer application of blockchain technology, blockchain-based payments have attracted extensive attention ranging from hobbyists to corporates to regulatory bodies. Blockchain facilitates fast, secure, and cross-border payments without the need for intermediaries such as banks. Because blockchain technology is still emerging, systematically organised knowledge providing a holistic and comprehensive view on designing payment applications that use blockchain is yet to be established. If such knowledge could be established in the form of a set of blockchain-specific patterns, architects could use those patterns in designing a payment application that leverages blockchain. Therefore, in this paper, we first identify a token’s lifecycle and then present 12 patterns that cover critical aspects in enabling the state transitions of a token in blockchain-based payment applications. The lifecycle and the annotated patterns provide a payment-focused systematic view of system interactions and a guide to effective use of the patterns.",
        "link": "https://dl.acm.org/doi/10.1145/3489449.3490006",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Enhancing the Security of FPGA-SoCs via the Usage of ARM TrustZone and a Hybrid-TPM",
        "authors": "['Mathieu Gross', 'Konrad Hohentanner', 'Stefan Wiehler', 'Georg Sigl']",
        "date": "None",
        "source": "ACM Transactions on Reconfigurable Technology and Systems",
        "abstract": "Isolated execution is a concept commonly used for increasing the security of a computer system. In the embedded world, ARM TrustZone technology enables this goal and is currently used on mobile devices for applications such as secure payment or biometric authentication. In this work, we investigate the security benefits achievable through the usage of ARM TrustZone on FPGA-SoCs. We first adapt Microsoft’s implementation of a firmware Trusted Platform Module (fTPM) running inside ARM TrustZone for the Zynq UltraScale+ platform. This adaptation consists in integrating hardware accelerators available on the device to fTPM’s implementation and to enhance fTPM with an entropy source derived from on-chip SRAM start-up patterns. With our approach, we transform a software implementation of a TPM into a hybrid hardware/software design that could address some of the security drawbacks of the original implementation while keeping its flexibility. To demonstrate the security gains obtained via the usage of ARM TrustZone and our hybrid-TPM on FPGA-SoCs, we propose a framework that combines them for enabling a secure remote bitstream loading. The approach consists in preventing the insecure usages of a bitstream reconfiguration interface that are made possible by the manufacturer and to integrate the interface inside a Trusted Execution Environment.",
        "link": "https://dl.acm.org/doi/10.1145/3472959",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "FleetRec: Large-Scale Recommendation Inference on Hybrid GPU-FPGA Clusters",
        "authors": "['Wenqi Jiang', 'Zhenhao He', 'Shuai Zhang', 'Kai Zeng', 'Liang Feng', 'Jiansong Zhang', 'Tongxuan Liu', 'Yong Li', 'Jingren Zhou', 'Ce Zhang', 'Gustavo Alonso']",
        "date": "August 2021",
        "source": "KDD '21: Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "We present FleetRec, a high-performance and scalable recommendation inference system within tight latency constraints. FleetRec takes advantage of heterogeneous hardware including GPUs and the latest FPGAs equipped with high-bandwidth memory. By disaggregating computation and memory to different types of hardware and bridging their connections by high-speed network, FleetRec gains the best of both worlds, and can naturally scale out by adding nodes to the cluster. Experiments on three production models up to 114 GB show that FleetRec outperforms optimized CPU baseline by more than one order of magnitude in terms of throughput while achieving significantly lower latency.",
        "link": "https://dl.acm.org/doi/10.1145/3447548.3467139",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "When climate meets machine learning: edge to cloud ML energy efficiency",
        "authors": "['Diana Marculescu']",
        "date": "July 2021",
        "source": "ISLPED '21: Proceedings of the ACM/IEEE International Symposium on Low Power Electronics and Design",
        "abstract": "A large portion of current cloud and edge workloads feature Machine Learning (ML) tasks, thereby requiring a deep understanding of their energy efficiency. While the holy grail for judging the quality of a ML model has largely been testing accuracy, and only recently its resource usage, neither of these metrics translate directly to energy efficiency, runtime, or mobile device battery lifetime. This work uncovers the need for building accurate, platform-specific power and latency models for ML and efficient hardware-aware ML design methodologies, thus allowing machine learners and hardware designers to identify not just the best accuracy ML model configuration, but also those that satisfy given hardware constraints.",
        "link": "https://dl.acm.org/doi/10.1109/ISLPED52811.2021.9502472",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Mapping Computations in Heterogeneous Multicore Systems with Statistical Regression on Program Inputs",
        "authors": "['Junio Cezar Ribeiro Da Silva', 'Lorena Leão', 'Vinicius Petrucci', 'Abdoulaye Gamatié', 'Fernando Magno Quintão Pereira']",
        "date": "None",
        "source": "ACM Transactions on Embedded Computing Systems",
        "abstract": "A hardware configuration is a set of processors and their frequency levels in a multicore heterogeneous system. This article presents a compiler-based technique to match functions with hardware configurations. Such a technique consists of using multivariate linear regression to associate function arguments with particular hardware configurations. By showing that this classification space tends to be convex in practice, this article demonstrates that linear regression is not only an efficient tool to map computations to heterogeneous hardware, but also an effective one. To demonstrate the viability of multivariate linear regression as a way to perform adaptive compilation for heterogeneous architectures, we have implemented our ideas onto the Soot Java bytecode analyzer. Code that we produce can predict the best configuration for a large class of Java and Scala benchmarks running on an Odroid XU4 big.LITTLE board; hence, outperforming prior techniques such as ARM’s GTS and CHOAMP, a recently released static program scheduler.",
        "link": "https://dl.acm.org/doi/10.1145/3478288",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Programming and Synthesis for Software-defined FPGA Acceleration: Status and Future Prospects",
        "authors": "['Yi-Hsiang Lai', 'Ecenur Ustun', 'Shaojie Xiang', 'Zhenman Fang', 'Hongbo Rong', 'Zhiru Zhang']",
        "date": "None",
        "source": "ACM Transactions on Reconfigurable Technology and Systems",
        "abstract": "FPGA-based accelerators are increasingly popular across a broad range of applications, because they offer massive parallelism, high energy efficiency, and great flexibility for customizations. However, difficulties in programming and integrating FPGAs have hindered their widespread adoption. Since the mid 2000s, there has been extensive research and development toward making FPGAs accessible to software-inclined developers, besides hardware specialists. Many programming models and automated synthesis tools, such as high-level synthesis, have been proposed to tackle this grand challenge. In this survey, we describe the progression and future prospects of the ongoing journey in significantly improving the software programmability of FPGAs. We first provide a taxonomy of the essential techniques for building a high-performance FPGA accelerator, which requires customizations of the compute engines, memory hierarchy, and data representations. We then summarize a rich spectrum of work on programming abstractions and optimizing compilers that provide different trade-offs between performance and productivity. Finally, we highlight several additional challenges and opportunities that deserve extra attention by the community to bring FPGA-based computing to the masses.",
        "link": "https://dl.acm.org/doi/10.1145/3469660",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Rethinking Embedded Blocks for Machine Learning Applications",
        "authors": "['Seyedramin Rasoulinezhad', 'Esther Roorda', 'Steve Wilton', 'Philip H. W. Leong', 'David Boland']",
        "date": "None",
        "source": "ACM Transactions on Reconfigurable Technology and Systems",
        "abstract": "The underlying goal of FPGA architecture research is to devise flexible substrates that implement a wide variety of circuits efficiently. Contemporary FPGA architectures have been optimized to support networking, signal processing, and image processing applications through high-precision digital signal processing (DSP) blocks. The recent emergence of machine learning has created a new set of demands characterized by: (1) higher computational density and (2) low precision arithmetic requirements. With the goal of exploring this new design space in a methodical manner, we first propose a problem formulation involving computing nested loops over multiply-accumulate (MAC) operations, which covers many basic linear algebra primitives and standard deep neural network (DNN) kernels. A quantitative methodology for deriving efficient coarse-grained compute block architectures from benchmarks is then proposed together with a family of new embedded blocks, called MLBlocks. An MLBlock instance includes several multiply-accumulate units connected via a flexible routing, where each configuration performs a few parallel dot-products in a systolic array fashion. This architecture is parameterized with support for different data movements, reuse, and precisions, utilizing a columnar arrangement that is compatible with existing FPGA architectures. On synthetic benchmarks, we demonstrate that for 8-bit arithmetic, MLBlocks offer 6× improved performance over the commercial Xilinx DSP48E2 architecture with smaller area and delay; and for time-multiplexed 16-bit arithmetic, achieves 2× higher performance per area with the same area and frequency. All source codes and data, along with documents to reproduce all the results in this article, are available at  http://github.com/raminrasoulinezhad/MLBlocks.",
        "link": "https://dl.acm.org/doi/10.1145/3491234",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "The Programmable Data Plane: Abstractions, Architectures, Algorithms, and Applications",
        "authors": "['Oliver Michel', 'Roberto Bifulco', 'Gábor Rétvári', 'Stefan Schmid']",
        "date": "None",
        "source": "ACM Computing Surveys",
        "abstract": "Programmable data plane technologies enable the systematic reconfiguration of the low-level processing steps applied to network packets and are key drivers toward realizing the next generation of network services and applications. This survey presents recent trends and issues in the design and implementation of programmable network devices, focusing on prominent abstractions, architectures, algorithms, and applications proposed, debated, and realized over the past years. We elaborate on the trends that led to the emergence of this technology and highlight the most important pointers from the literature, casting different taxonomies for the field, and identifying avenues for future research.",
        "link": "https://dl.acm.org/doi/10.1145/3447868",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "BISWSRBS: A Winograd-based CNN Accelerator with a Fine-grained Regular Sparsity Pattern and Mixed Precision Quantization",
        "authors": "['Tao Yang', 'Zhezhi He', 'Tengchuan Kou', 'Qingzheng Li', 'Qi Han', 'Haibao Yu', 'Fangxin Liu', 'Yun Liang', 'Li Jiang']",
        "date": "None",
        "source": "ACM Transactions on Reconfigurable Technology and Systems",
        "abstract": "Field-programmable Gate Array (FPGA) is a high-performance computing platform for Convolution Neural Networks (CNNs) inference. Winograd algorithm, weight pruning, and quantization are widely adopted to reduce the storage and arithmetic overhead of CNNs on FPGAs. Recent studies strive to prune the weights in the Winograd domain, however, resulting in irregular sparse patterns and leading to low parallelism and reduced utilization of resources. Besides, there are few works to discuss a suitable quantization scheme for Winograd.In this article, we propose a regular sparse pruning pattern in the Winograd-based CNN, namely, Sub-row-balanced Sparsity (SRBS) pattern, to overcome the challenge of the irregular sparse pattern. Then, we develop a two-step hardware co-optimization approach to improve the model accuracy using the SRBS pattern. Based on the pruned model, we implement a mixed precision quantization to further reduce the computational complexity of bit operations. Finally, we design an FPGA accelerator that takes both the advantage of the SRBS pattern to eliminate low-parallelism computation and the irregular memory accesses, as well as the mixed precision quantization to get a layer-wise bit width. Experimental results on VGG16/VGG-nagadomi with CIFAR-10 and ResNet-18/34/50 with ImageNet show up to 11.8×/8.67× and 8.17×/8.31×/10.6× speedup, 12.74×/9.19× and 8.75×/8.81×/11.1× energy efficiency improvement, respectively, compared with the state-of-the-art dense Winograd accelerator [20] with negligible loss of model accuracy. We also show that our design has 4.11× speedup compared with the state-of-the-art sparse Winograd accelerator [19] on VGG16.",
        "link": "https://dl.acm.org/doi/10.1145/3467476",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "EffiCSense: an architectural pathfinding framework for energy-constrained sensor applications",
        "authors": "['Jonah Van Assche', 'Ruben Helsen', 'Georges Gielen']",
        "date": "March 2022",
        "source": "DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe",
        "abstract": "This paper introduces EffiCSense, an architectural pathfinding framework for mixed-signal sensor front-ends for both regular and compressive sensing systems. Since sensing systems are often energy constrained, finding a suitable architecture can be a long iterative process between high-level modeling and circuit design. We present a Simulink-based framework that allows for architectural pathfinding with high-level functional models while also including power consumption models of the different circuit blocks. This allows to directly model the impact of design specifications on power consumption and speeds up the overall design process significantly. Both architectures with and without compressive sensing can be handled. The framework is demonstrated for the processing of EEG signals for epilepsy detection, comparing solutions with and without analog compressive sensing. Simulations show that using the compression, an optimal design can be found that is estimated to be 3.6 times more power-efficient compared to a system without compression, consuming 2.44μW for a detection accuracy of 99.3%.",
        "link": "https://dl.acm.org/doi/10.5555/3539845.3539888",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "PHiLIP on the HiL: Automated Multi-Platform OS Testing With External Reference Devices",
        "authors": "['Kevin Weiss', 'Michel Rottleuthner', 'Thomas C. Schmidt', 'Matthias Wählisch']",
        "date": "None",
        "source": "ACM Transactions on Embedded Computing Systems",
        "abstract": "Developing an operating systems (OSs) for low-end embedded devices requires continuous adaptation to new hardware architectures and components, while serviceability of features needs to be assured for each individual platform under tight resource constraints. It is challenging to design a versatile and accurate heterogeneous test environment that is agile enough to cover a continuous evolution of the code base and platforms. This mission is even more challenging when organized in an agile open-source community process with many contributors such as for the RIOT OS. Hardware in the Loop (HiL) testing and Continuous Integration (CI) are automatable approaches to verify functionality, prevent regressions, and improve the overall quality at development speed in large community projects.In this paper, we present PHiLIP (Primitive Hardware in the Loop Integration Product), an open-source external reference device together with tools that validate the system software while it controls hardware and interprets physical signals. Instead of focusing on a specific test setting, PHiLIP takes the approach of a tool-assisted agile HiL test process, designed for continuous evolution and deployment cycles. We explain its design, describe how it supports HiL tests, evaluate performance metrics, and report on practical experiences of employing PHiLIP in an automated CI test infrastructure. Our initial deployment comprises 22 unique platforms, each of which executes 98 peripheral tests every night. PHiLIP allows for easy extension of low-cost, adaptive testing infrastructures but serves testing techniques and tools to a much wider range of applications.",
        "link": "https://dl.acm.org/doi/10.1145/3477040",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Don't forget the I/O when allocating your LLC",
        "authors": "['Yifan Yuan', 'Mohammad Alian', 'Yipeng Wang', 'Ren Wang', 'Ilia Kurakin', 'Charlie Tai', 'Nam Sung Kim']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "In modern server CPUs, last-level cache (LLC) is a critical hardware resource that exerts significant influence on the performance of the workloads, and how to manage LLC is a key to the performance isolation and QoS in the cloud with multi-tenancy. In this paper, we argue that in addition to CPU cores, high-speed I/O is also important for LLC management. This is because of an Intel architectural innovation - Data Direct I/O (DDIO) - that directly injects the inbound I/O traffic to (part of) the LLC instead of the main memory. We summarize two problems caused by DDIO and show that (1) the default DDIO configuration may not always achieve optimal performance, (2) DDIO can decrease the performance of non-I/O workloads that share LLC with it by as high as 32%. We then present IAT, the first LLC management mechanism that treats the I/O as the first-class citizen. IAT monitors and analyzes the performance of the core/LLC/DDIO using CPU's hardware performance counters and adaptively adjusts the number of LLC ways for DDIO or the tenants that demand more LLC capacity. In addition, IAT dynamically chooses the tenants that share its LLC resource with DDIO to minimize the performance interference by both the tenants and the I/O. Our experiments with multiple microbenchmarks and real-world applications demonstrate that with minimal overhead, IAT can effectively and stably reduce the performance degradation caused by DDIO.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00018",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Parallel Computing of Graph-based Functions in ReRAM",
        "authors": "['Saman Froehlich', 'Saeideh Shirinzadeh', 'Rolf Drechsler']",
        "date": "None",
        "source": "ACM Journal on Emerging Technologies in Computing Systems",
        "abstract": "Resistive Random Access Memory (ReRAM) is an emerging non-volatile memory technology. Besides its low power consumption and its high scalability, its inherent computation capabilities make ReRAM especially interesting for future computer architectures. Merging computations into the memory is a promising solution for overcoming the memory bottleneck. To perform computations in ReRAM, efficient synthesis strategies for Boolean functions have to be developed. In this article, we give a thorough presentation of how to employ parallel computing capabilities of ReRAM for the synthesis of functions given state-of-the-art graph-based representations AIGs or BDDs. Additionally, we introduce a new graph-based representation called m-And-Inverter Graph (m-AIGs), which allows us to fully exploit the computing capabilities of ReRAM. In the simulations, we show that our proposed approaches outperform state-of-the art synthesis strategies, and we show the superiority of m-AIGs over the standard AIG representation for ReRAM-based synthesis.",
        "link": "https://dl.acm.org/doi/10.1145/3453163",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Opening pandora's box: a systematic study of new ways microarchitecture can leak private data",
        "authors": "['Jose Rodrigo Sanchez Vicarte', 'Pradyumna Shome', 'Nandeeka Nayak', 'Caroline Trippel', 'Adam Morrison', 'David Kohlbrenner', 'Christopher W. Fletcher']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "Microarchitectural attacks have plunged Computer Architecture into a security crisis. Yet, as the slowing of Moore's law justifies the use of ever more exotic microarchitecture, it is likely we have only seen the tip of the iceberg. To better anticipate this security crisis, this paper performs a systematic security-centric analysis of the Computer Architecture literature. Our rationale is that when implementing current and future processors, microarchitects will (quite reasonably) look to previously-proposed ideas. Our study uncovers seven classes of microarchitectural optimization with novel security implications, proposes a conceptual framework through which to study them and demonstrates several proofs-of-concept to show their efficacy. The optimizations we study range from those that leak as much privacy as Spectre/Meltdown (but without exploiting speculative execution) to those that otherwise undermine security-critical programs in a variety of ways. Many have storied histories---ranging from industry patents to media/3rd party speculation regarding current implementation status to recent renewed interest in the academic community. This paper's goal is to perform an early (hopefully not too late) analysis to inform their development moving forward.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00035",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "ZeRØ: <u>ze</u>ro-overhead <u>r</u>esilient <u>o</u>peration under pointer integrity attacks",
        "authors": "['Mohamed Tarek Ibn Ziad', 'Miguel A. Arroyo', 'Evgeny Manzhosov', 'Simha Sethumadhavan']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "A large class of today's systems require high levels of availability and security. Unfortunately, state-of-the-art security solutions tend to induce crashes and raise exceptions when under attack, trading off availability for security. In this work, we propose ZeRØ, a pointer integrity mechanism that can continue program execution even when under attack. ZeRØ proposes unique memory instructions and a novel metadata encoding scheme to protect code and data pointers. The combination of instructions and metadata allows ZeRØ to avoid explicitly tagging every word in memory, eliminating performance overheads. Moreover, ZeRØ is a deterministic security primitive that requires minor microarchitectural changes. We show that ZeRØ is better than commercially available state-of-the-art hardware primitives, e.g., ARM's Pointer Authentication (PAC), by a significant margin. ZeRØ incurs zero performance overheads on the SPEC CPU2017 benchmarks, and our VLSI measurements show low power and area overheads.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00082",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Energy Efficient Error Resilient Multiplier Using Low-power Compressors",
        "authors": "['Skandha Deepsita S', 'Dhayala Kumar M', 'Noor Mahammad SK']",
        "date": "None",
        "source": "ACM Transactions on Design Automation of Electronic Systems",
        "abstract": "The approximate hardware design can save huge energy at the cost of errors incurred in the design. This article proposes the approximate algorithm for low-power compressors, utilized to build approximate multiplier with low energy and acceptable error profiles. This article presents two design approaches (DA1 and DA2) for higher bit size approximate multipliers. The proposed multiplier of DA1 have no propagation of carry signal from LSB to MSB, resulted in a very high-speed design. The increment in delay, power, and energy are not exponential with increment of multiplier size (n) for DA1 multiplier. It can be observed that the maximum combinations lie in the threshold Error Distance of 5% of the maximum value possible for any particular multiplier of size n. The proposed 4-bit DA1 multiplier consumes only 1.3 fJ of energy, which is 87.9%, 78%, 94%, 67.5%, and 58.9% less when compared to M1, M2, LxA, MxA, accurate designs respectively. The DA2 approach is recursive method, i.e., n-bit multiplier built with n/2-bit sub-multipliers. The proposed 8-bit multiplication has 92% energy savings with Mean Relative Error Distance (MRED) of 0.3 for the DA1 approach and at least 11% to 40% of energy savings with MRED of 0.08 for the DA2 approach. The proposed multipliers are employed in the image processing algorithm of DCT, and the quality is evaluated. The standard PSNR metric is 55 dB for less approximation and 35 dB for maximum approximation.",
        "link": "https://dl.acm.org/doi/10.1145/3488837",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "ABC-DIMM: alleviating the bottleneck of communication in DIMM-based near-memory processing with inter-DIMM broadcast",
        "authors": "['Weiyi Sun', 'Zhaoshi Li', 'Shouyi Yin', 'Shaojun Wei', 'Leibo Liu']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "Near-Memory Processing (NMP) systems that integrate accelerators within DIMM (Dual-Inline Memory Module) buffer chips potentially provide high performance with relatively low design and manufacturing costs. However, an inevitable communication bottleneck arises when considering the main memory bus among peer DIMMs and the host CPU. This communication bottleneck roots in the bus-based nature and the limited point-to-point communication pattern of the main memory system. The aggregated memory bandwidth of DIMM-based NMP scales with the number of DIMMs. When the number of DIMMs in a channel scales up, the per-DIMM point-to-point communication bandwidth scales down, whereas the computation resources and local memory bandwidth per DIMM stay the same. For many important sparse data-intensive workloads like graph applications and sparse tensor algebra, we identify that communication among DIMMs and the host CPU easily dominates their processing procedure in previous DIMM-based NMP systems, which severely bottlenecks their performance. To tackle this challenge, we propose that inter-DIMM broadcast should be implemented and utilized in the main memory system of DIMM-based NMP. On the hardware side, the main memory bus naturally scales out with broadcast, where per-DIMM effective bandwidth of broadcast remains the same as the number of DIMMs grows. On the software side, many sparse applications can be implemented in a form such that broadcasts dominate their communication. Based on these ideas, we design <u>ABC-DIMM</u>, which Alleviates the Bottleneck of Communication in DIMM-based NMP, consisting of integral broadcast mechanisms and Broadcast-Process programming framework, with minimized modifications to commodity software-hardware stack. Our evaluation shows that ABC-DIMM offers an 8.33X geo-mean speedup over a 16-core CPU baseline, and outperforms two NMP baselines by 2.59X and 2.93X on average.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00027",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "HP-GNN: Generating High Throughput GNN Training Implementation on CPU-FPGA Heterogeneous Platform",
        "authors": "['Yi-Chien Lin', 'Bingyi Zhang', 'Viktor Prasanna']",
        "date": "February 2022",
        "source": "FPGA '22: Proceedings of the 2022 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays",
        "abstract": "Graph Neural Networks (GNNs) have shown great success in many applications such as recommendation systems, molecular property prediction, traffic prediction, etc. Recently, CPU-FPGA heterogeneous platforms have been used to accelerate many applications by exploiting customizable data path and abundant user-controllable on-chip memory resources of FPGAs. Yet, accelerating and deploying GNN training on such platforms requires not only expertise in hardware design but also substantial development efforts. We propose HP-GNN, a novel framework that generates high throughput GNN training implementations on a given CPU-FPGA platform that can benefit both application developers and machine learning researchers. HP-GNN takes GNN training algorithms, GNN models as the inputs, and automatically performs hardware mapping onto the target CPU-FPGA platform. HP-GNN consists of: (1) data layout and internal representation that reduce the memory traffic and random memory accesses; (2) optimized hardware templates that support various GNN models; (3) a design space exploration engine for automatic hardware mapping; (4) high-level application programming interfaces (APIs) that allow users to specify GNN training with only a handful of lines of code. To evaluate HP-GNN, we experiment with two well-known sampling-based GNN training algorithms and two GNN models. For each training algorithm and model, HP-GNN generates implementation on a state-of-the-art CPU-FPGA platform. Compared with CPU-only and CPU-GPU platforms, experimental results show that the generated implementations achieve 55.67x and 2.17x speedup on the average, respectively. Compared with the state-of-the-art GNN training implementations, HP-GNN achieves up to 4.45x speedup.",
        "link": "https://dl.acm.org/doi/10.1145/3490422.3502359",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "High-Performance Match-Action Table Updates from within Programmable Software Data Planes",
        "authors": "['Manuel Simon', 'Henning Stubbe', 'Dominik Scholz', 'Sebastian Gallenmüller', 'Georg Carle']",
        "date": "December 2021",
        "source": "ANCS '21: Proceedings of the Symposium on Architectures for Networking and Communications Systems",
        "abstract": "For long, P4's mantra was that table entries could only be updated by the control plane. With the ongoing Portable NIC Architecture (PNA) standardization efforts, this is changing. In fact, PNA presumably includes explicit methods for table updates from within the data planes. Now, it is onto manufacturers and developers to integrate and use this mechanism in future P4 data planes. This would enable novel and improved applications, e.g., requiring means for maintaining state. We present our implementation of flexible match-action tables for the DPDK-based t4p4s target. We discuss different approaches for table updates from within the data plane and challenges that arise when operating at line rate. Further, we analyze the data consistency of our enhanced table structures in a multi-core scenario and model the memory overhead for state management purposes.",
        "link": "https://dl.acm.org/doi/10.1145/3493425.3502759",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "FILM-QNN: Efficient FPGA Acceleration of Deep Neural Networks with Intra-Layer, Mixed-Precision Quantization",
        "authors": "['Mengshu Sun', 'Zhengang Li', 'Alec Lu', 'Yanyu Li', 'Sung-En Chang', 'Xiaolong Ma', 'Xue Lin', 'Zhenman Fang']",
        "date": "February 2022",
        "source": "FPGA '22: Proceedings of the 2022 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays",
        "abstract": "With the trend to deploy Deep Neural Network (DNN) inference models on edge devices with limited resources, quantization techniques have been widely used to reduce on-chip storage and improve computation throughput. However, existing DNN quantization work deploying quantization below 8-bit may be either suffering from evident accuracy loss or facing a big gap between the theoretical improvement of computation throughput and the practical inference speedup. In this work, we propose a general framework, called FILM-QNN, to quantize and accelerate multiple DNN models across different embedded FPGA devices. First, we propose the novel intra-layer, mixed-precision quantization algorithm that assigns different precisions onto the filters of each layer. The candidate precision levels and assignment granularity are determined from our empirical study with the capability of preserving accuracy and improving hardware parallelism. Second, we apply multiple optimization techniques for the FPGA accelerator architecture in support of quantized computations, including DSP packing, weight reordering, and data packing, to enhance the overall throughput with the available resources. Moreover, a comprehensive resource model is developed to balance the allocation of FPGA computation resources (LUTs and DSPs) as well as data transfer and on-chip storage resources (BRAMs) to accelerate the computations in mixed precisions within each layer. Finally, to improve the portability of FILM-QNN, we implement it using Vivado High-Level Synthesis (HLS) on Xilinx PYNQ-Z2 and ZCU102 FPGA boards. Our experimental results of ResNet-18, ResNet-50, and MobileNet-V2 demonstrate that the implementations with intra-layer, mixed-precision (95% of 4-bit weights and 5% of 8-bit weights, and all 5-bit activations) can achieve comparable accuracy (70.47%, 77.25%, and 65.67% for the three models) as the 8-bit (and 32-bit) versions and comparable throughput (214.8 FPS, 109.1 FPS, and 537.9 FPS on ZCU102) as the 4-bit designs.",
        "link": "https://dl.acm.org/doi/10.1145/3490422.3502364",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "IChannels: exploiting current management mechanisms to create covert channels in modern processors",
        "authors": "['Jawad Haj-Yahya', 'Jeremie S. Kim', 'A. Giray Yağlikçi', 'Ivan Puddu', 'Lois Orosa', 'Juan Gómez Luna', 'Mohammed Alser', 'Onur Mutlu']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "To operate efficiently across a wide range of workloads with varying power requirements, a modern processor applies different current management mechanisms, which briefly throttle instruction execution while they adjust voltage and frequency to accommodate for power-hungry instructions (PHIs) in the instruction stream. Doing so 1) reduces the power consumption of non-PHI instructions in typical workloads and 2) optimizes system voltage regulators' cost and area for the common use case while limiting current consumption when executing PHIs. However, these mechanisms may compromise a system's confidentiality guarantees. In particular, we observe that multi-level side-effects of throttling mechanisms, due to PHI-related current management mechanisms, can be detected by two different software contexts (i.e., sender and receiver) running on 1) the same hardware thread, 2) co-located Simultaneous Multi-Threading (SMT) threads, and 3) different physical cores. Based on these new observations on current management mechanisms, we develop a new set of covert channels, IChannels, and demonstrate them in real modern Intel processors (which span more than 70% of the entire client and server processor market). Our analysis shows that IChannels provides more than 24X the channel capacity of state-of-the-art power management covert channels. We propose practical and effective mitigations to each covert channel in IChannels by leveraging the insights we gain through a rigorous characterization of real systems.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00081",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "AIgean: An Open Framework for Deploying Machine Learning on Heterogeneous Clusters",
        "authors": "['Naif Tarafdar', 'Giuseppe Di Guglielmo', 'Philip C. Harris', 'Jeffrey D. Krupa', 'Vladimir Loncar', 'Dylan S. Rankin', 'Nhan Tran', 'Zhenbin Wu', 'Qianfeng Shen', 'Paul Chow']",
        "date": "None",
        "source": "ACM Transactions on Reconfigurable Technology and Systems",
        "abstract": "AIgean, pronounced like the sea, is an open framework to build and deploy machine learning (ML) algorithms on a heterogeneous cluster of devices (CPUs and FPGAs). We leverage two open source projects: Galapagos, for multi-FPGA deployment, and hls4ml, for generating ML kernels synthesizable using Vivado HLS. AIgean provides a full end-to-end multi-FPGA/CPU implementation of a neural network. The user supplies a high-level neural network description, and our tool flow is responsible for the synthesizing of the individual layers, partitioning layers across different nodes, as well as the bridging and routing required for these layers to communicate. If the user is an expert in a particular domain and would like to tinker with the implementation details of the neural network, we define a flexible implementation stack for ML that includes the layers of Algorithms, Cluster Deployment & Communication, and Hardware. This allows the user to modify specific layers of abstraction without having to worry about components outside of their area of expertise, highlighting the modularity of AIgean. We demonstrate the effectiveness of AIgean with two use cases: an autoencoder, and ResNet-50 running across 10 and 12 FPGAs. AIgean leverages the FPGA’s strength in low-latency computing, as our implementations target batch-1 implementations.",
        "link": "https://dl.acm.org/doi/10.1145/3482854",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Accelerated seeding for genome sequence alignment with enumerated radix trees",
        "authors": "['Arun Subramaniyan', 'Jack Wadden', 'Kush Goliya', 'Nathan Ozog', 'Xiao Wu', 'Satish Narayanasamy', 'David Blaauw', 'Reetuparna Das']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "Read alignment is a time-consuming step in genome sequencing analysis. The most widely used software for read alignment, BWA-MEM, and the recently published faster version BWA-MEM2 are based on the seed-and-extend paradigm for read alignment. The seeding step of read alignment is a major bottleneck contributing ~40% to the overall execution time of BWA-MEM2 when aligning whole human genome reads from the Platinum Genomes dataset. This is because both BWA-MEM and BWA-MEM2 use a compressed index structure called the FMD-Index, which results in high bandwidth requirements, primarily due to its character-by-character processing of reads. For instance, to seed each read (101 DNA base-pairs stored in 37.8 bytes), the FMD-Index solution in BWA-MEM2 requires ~68.5 KB of index data. We propose a novel indexing data structure named Enumerated Radix Tree (ERT) and design a custom seeding accelerator based on it. ERT improves bandwidth efficiency of BWA-MEM2 by 4.5X while guaranteeing 100% identical output to the original software, and still fitting in 64 GB DRAM. Overall, the proposed seeding accelerator implemented on AWS F1 FPGA (f1.4xlarge) improves seeding throughput of BWA-MEM2 by 3.3X. When combined with seed-extension accelerators, we observe a 2.1X improvement in overall read alignment throughput over BWA-MEM2. The software implementation of ERT is integrated into BWA-MEM2 (ert branch: https://github.com/bwa-mem2/bwa-mem2/tree/ert) and is open sourced for the benefit of the research community.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00038",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "QUAC-TRNG: high-throughput true random number generation using quadruple row activation in commodity DRAM chips",
        "authors": "['Ataberk Olgun', 'Minesh Patel', 'A. Giray Yağlikçi', 'Haocong Luo', 'Jeremie S. Kim', 'F. Nisa Bostanci', 'Nandita Vijaykumar', 'Oğuz Ergin', 'Onur Mutlu']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "True random number generators (TRNG) sample random physical processes to create large amounts of random numbers for various use cases, including security-critical cryptographic primitives, scientific simulations, machine learning applications, and even recreational entertainment. Unfortunately, not every computing system is equipped with dedicated TRNG hardware, limiting the application space and security guarantees for such systems. To open the application space and enable security guarantees for the overwhelming majority of computing systems that do not necessarily have dedicated TRNG hardware (e.g., processing-in-memory systems), we develop QUAC-TRNG, a new high-throughput TRNG that can be fully implemented in commodity DRAM chips, which are key components in most modern systems. QUAC-TRNG exploits the new observation that a carefully-engineered sequence of DRAM commands activates four consecutive DRAM rows in rapid succession. This QUadruple ACtivation (QUAC) causes the bitline sense amplifiers to non-deterministically converge to random values when we activate four rows that store conflicting data because the net deviation in bitline voltage fails to meet reliable sensing margins. We experimentally demonstrate that QUAC reliably generates random values across 136 commodity DDR4 DRAM chips from one major DRAM manufacturer. We describe how to develop an effective TRNG (QUAC-TRNG) based on QUAC. We evaluate the quality of our TRNG using the commonly-used NIST statistical test suite for randomness and find that QUAC-TRNG successfully passes each test. Our experimental evaluations show that QUAC-TRNG reliably generates true random numbers with a throughput of 3.44 Gb/s (per DRAM channel), outperforming the state-of-the-art DRAM-based TRNG by 15.08X and 1.41X for basic and throughput-optimized versions, respectively. We show that QUAC-TRNG utilizes DRAM bandwidth better than the state-of-the-art, achieving up to 2.03X the throughput of a throughput-optimized baseline when scaling bus frequencies to 12 GT/s.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00078",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "The Future of FPGA Acceleration in Datacenters and the Cloud",
        "authors": "['Christophe Bobda', 'Joel Mandebi Mbongue', 'Paul Chow', 'Mohammad Ewais', 'Naif Tarafdar', 'Juan Camilo Vega', 'Ken Eguro', 'Dirk Koch', 'Suranga Handagala', 'Miriam Leeser', 'Martin Herbordt', 'Hafsah Shahzad', 'Peter Hofste', 'Burkhard Ringlein', 'Jakub Szefer', 'Ahmed Sanaullah', 'Russell Tessier']",
        "date": "None",
        "source": "ACM Transactions on Reconfigurable Technology and Systems",
        "abstract": "In this article, we survey existing academic and commercial efforts to provide Field-Programmable Gate Array (FPGA) acceleration in datacenters and the cloud. The goal is a critical review of existing systems and a discussion of their evolution from single workstations with PCI-attached FPGAs in the early days of reconfigurable computing to the integration of FPGA farms in large-scale computing infrastructures. From the lessons learned, we discuss the future of FPGAs in datacenters and the cloud and assess the challenges likely to be encountered along the way. The article explores current architectures and discusses scalability and abstractions supported by operating systems, middleware, and virtualization. Hardware and software security becomes critical when infrastructure is shared among tenants with disparate backgrounds. We review the vulnerabilities of current systems and possible attack scenarios and discuss mitigation strategies, some of which impact FPGA architecture and technology. The viability of these architectures for popular applications is reviewed, with a particular focus on deep learning and scientific computing. This work draws from workshop discussions, panel sessions including the participation of experts in the reconfigurable computing field, and private discussions among these experts. These interactions have harmonized the terminology, taxonomy, and the important topics covered in this manuscript.",
        "link": "https://dl.acm.org/doi/10.1145/3506713",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Understanding host network stack overheads",
        "authors": "['Qizhe Cai', 'Shubham Chaudhary', 'Midhul Vuppalapati', 'Jaehyun Hwang', 'Rachit Agarwal']",
        "date": "August 2021",
        "source": "SIGCOMM '21: Proceedings of the 2021 ACM SIGCOMM 2021 Conference",
        "abstract": "Traditional end-host network stacks are struggling to keep up with rapidly increasing datacenter access link bandwidths due to their unsustainable CPU overheads. Motivated by this, our community is exploring a multitude of solutions for future network stacks: from Linux kernel optimizations to partial hardware offload to clean-slate userspace stacks to specialized host network hardware. The design space explored by these solutions would benefit from a detailed understanding of CPU inefficiencies in existing network stacks. This paper presents measurement and insights for Linux kernel network stack performance for 100Gbps access link bandwidths. Our study reveals that such high bandwidth links, coupled with relatively stagnant technology trends for other host resources (e.g., CPU speeds and capacity, cache sizes, NIC buffer sizes, etc.), mark a fundamental shift in host network stack bottlenecks. For instance, we find that a single core is no longer able to process packets at line rate, with data copy from kernel to application buffers at the receiver becoming the core performance bottleneck. In addition, increase in bandwidth-delay products have outpaced the increase in cache sizes, resulting in inefficient DMA pipeline between the NIC and the CPU. Finally, we find that traditional loosely-coupled design of network stack and CPU schedulers in existing operating systems becomes a limiting factor in scaling network stack performance across cores. Based on insights from our study, we discuss implications to design of future operating systems, network protocols, and host hardware.",
        "link": "https://dl.acm.org/doi/10.1145/3452296.3472888",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "HW-FlowQ: A Multi-Abstraction Level HW-CNN Co-design Quantization Methodology",
        "authors": "['Nael Fasfous', 'Manoj Rohit Vemparala', 'Alexander Frickenstein', 'Emanuele Valpreda', 'Driton Salihu', 'Nguyen Anh Vu Doan', 'Christian Unger', 'Naveen Shankar Nagaraja', 'Maurizio Martina', 'Walter Stechele']",
        "date": "None",
        "source": "ACM Transactions on Embedded Computing Systems",
        "abstract": "Model compression through quantization is commonly applied to convolutional neural networks (CNNs) deployed on compute and memory-constrained embedded platforms. Different layers of the CNN can have varying degrees of numerical precision for both weights and activations, resulting in a large search space. Together with the hardware (HW) design space, the challenge of finding the globally optimal HW-CNN combination for a given application becomes daunting. To this end, we propose HW-FlowQ, a systematic approach that enables the co-design of the target hardware platform and the compressed CNN model through quantization. The search space is viewed at three levels of abstraction, allowing for an iterative approach for narrowing down the solution space before reaching a high-fidelity CNN hardware modeling tool, capable of capturing the effects of mixed-precision quantization strategies on different hardware architectures (processing unit counts, memory levels, cost models, dataflows) and two types of computation engines (bit-parallel vectorized, bit-serial). To combine both worlds, a multi-objective non-dominated sorting genetic algorithm (NSGA-II) is leveraged to establish a Pareto-optimal set of quantization strategies for the target HW-metrics at each abstraction level. HW-FlowQ detects optima in a discrete search space and maximizes the task-related accuracy of the underlying CNN while minimizing hardware-related costs. The Pareto-front approach keeps the design space open to a range of non-dominated solutions before refining the design to a more detailed level of abstraction. With equivalent prediction accuracy, we improve the energy and latency by 20% and 45% respectively for ResNet56 compared to existing mixed-precision search methods.",
        "link": "https://dl.acm.org/doi/10.1145/3476997",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A Novel Memory Management for RISC-V Enclaves",
        "authors": "['Haonan Li', 'Weijie Huang', 'Mingde Ren', 'Hongyi Lu', 'Zhenyu Ning', 'Heming Cui', 'Fengwei Zhang']",
        "date": "October 2021",
        "source": "HASP '21: Proceedings of the 10th International Workshop on Hardware and Architectural Support for Security and Privacy",
        "abstract": "Trusted Execution Environment (TEE) is a popular technology to protect sensitive data and programs. Recent TEEs have proposed the concept of enclaves to execute code processing sensitive data, which cannot be tampered with even by a malicious OS. However, due to hardware limitations and security requirements, existing TEE architectures usually offer limited memory management, such as dynamic memory allocation, defragmentation, etc. In this paper, we present Ashman—a novel software-based memory management extension of TEE on RISC-V, including dynamic memory allocation, migration, and defragmentation. We integrate Ashman into a self-designed TEE and evaluate the performance on a real-world development board. Experimental results have shown that Ashman provides memory management functions similar to native user applications while ensuring enclave security without modifying hardware.",
        "link": "https://dl.acm.org/doi/10.1145/3505253.3505257",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Hetero-ViTAL: a virtualization stack for heterogeneous FPGA clusters",
        "authors": "['Yue Zha', 'Jing Li']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "With field-programmable gate arrays (FPGAs) being widely deployed into data centers, an efficient virtualization support is required to fully unleash the potential of cloud FPGAs. Nevertheless, existing FPGA virtualization solutions only support a homogeneous FPGA cluster comprising identical FPGA devices. Representative work such as ViTAL provides sufficient system support for scale-out acceleration and improves the overall resource utilization through a fine-grained spatial sharing. While these existing solutions (including ViTAL) can efficiently virtualize a homogeneous cluster, it is hard to extend them to virtualizing a heterogeneous cluster which comprises multiple types of FPGAs. We expect the future cloud FPGAs are likely to be more heterogeneous due to hardware rolling upgrade. In this paper, we rethink FPGA virtualization from ground up and propose HETERO-VITAL to virtualize heterogeneous FPGA clusters. We identify the conflicting requirements of runtime management and offline compilation when designing the abstraction for a heterogeneous cluster, which is also the fundamental reason why the single-level abstraction as proposed in ViTAL (and other prior works) cannot be trivially extended to the heterogeneous case. To decouple these conflicting requirements, we provide a two-level system abstraction in HETERO-VITAL. Specifically, the high-level abstraction is FPGA-agnostic and provides a simple and homogeneous view of the FPGA resources to simplify the runtime management. On the contrary, the low-level abstraction is FPGA-specific and exposes sufficient spatial resource constraints to the compilation framework to ensure the mapping quality. Rather than simply adding a layer on top of the single-level abstraction as proposed in ViTAL and other prior work, we judiciously determine how much hardware details should be exposed at each level to balance the management complexity, mapping quality and compilation cost. We then develop a compilation framework to map applications onto this two-level abstraction with several optimization techniques to further improve the mapping quality. We also provide a runtime management policy to alleviate the fragmentation issue, which becomes more severe in a heterogeneous cluster due to the distinct resource capacities of diverse FPGAs. We evaluate HETERO-VITAL on a custom-built FPGA cluster and demonstrate its effectiveness using machine learning and image processing applications. Results show that HETERO-VITAL reduces the average response time (a critical metric for QoS) by 79.2% for a heterogeneous cluster compared to the non-virtualized baseline. When virtualizing a homogeneous cluster, HETERO-VITAL also reduces the average response time by 42.0% compared with ViTAL due to a better system design.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00044",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A Survey of Network-on-Chip Security Attacks and Countermeasures",
        "authors": "['Subodha Charles', 'Prabhat Mishra']",
        "date": "None",
        "source": "ACM Computing Surveys",
        "abstract": "With the advances of chip manufacturing technologies, computer architects have been able to integrate an increasing number of processors and other heterogeneous components on the same chip. Network-on-Chip (NoC) is widely employed by multicore System-on-Chip (SoC) architectures to cater to their communication requirements. NoC has received significant attention from both attackers and defenders. The increased usage of NoC and its distributed nature across the chip has made it a focal point of potential security attacks. Due to its prime location in the SoC coupled with connectivity with various components, NoC can be effectively utilized to implement security countermeasures to protect the SoC from potential attacks. There is a wide variety of existing literature on NoC security attacks and countermeasures. In this article, we provide a comprehensive survey of security vulnerabilities in NoC-based SoC architectures and discuss relevant countermeasures.",
        "link": "https://dl.acm.org/doi/10.1145/3450964",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "ACE-GCN: A Fast Data-driven FPGA Accelerator for GCN Embedding",
        "authors": "['José Romero Hung', 'Chao Li', 'Pengyu Wang', 'Chuanming Shao', 'Jinyang Guo', 'Jing Wang', 'Guoyong Shi']",
        "date": "None",
        "source": "ACM Transactions on Reconfigurable Technology and Systems",
        "abstract": "ACE-GCN is a fast and resource/energy-efficient FPGA accelerator for graph convolutional embedding under data-driven and in-place processing conditions. Our accelerator exploits the inherent power law distribution and high sparsity commonly exhibited by real-world graphs datasets. Contrary to other hardware implementations of GCN, on which traditional optimization techniques are employed to bypass the problem of dataset sparsity, our architecture is designed to take advantage of this very same situation. We propose and implement an innovative acceleration approach supported by our “implicit-processing-by-association” concept, in conjunction with a dataset-customized convolutional operator. The computational relief and consequential acceleration effect arise from the possibility of replacing rather complex convolutional operations for a faster embedding result estimation. Based on a computationally inexpensive and super-expedited similarity calculation, our accelerator is able to decide from the automatic embedding estimation or the unavoidable direct convolution operation. Evaluations demonstrate that our approach presents excellent applicability and competitive acceleration value. Depending on the dataset and efficiency level at the target, between 23× and 4,930× PyG baseline, coming close to AWB-GCN by 46% to 81% on smaller datasets and noticeable surpassing AWB-GCN for larger datasets and with controllable accuracy loss levels. We further demonstrate the unique hardware optimization characteristics of our approach and discuss its multi-processing potentiality.",
        "link": "https://dl.acm.org/doi/10.1145/3470536",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A Forward Error Compensation Approach for Fault Resilient Deep Neural Network Accelerator Design",
        "authors": "['Wenye Liu', 'Chip-Hong Chang']",
        "date": "November 2021",
        "source": "ASHES '21: Proceedings of the 5th Workshop on Attacks and Solutions in Hardware Security",
        "abstract": "Deep learning accelerator is a key enabler of a variety of safety-critical applications such as self-driving car and video surveillance. However, recently reported hardware-oriented attack vectors, e.g., fault injection attacks, have extended the threats on deployed deep neural network (DNN) systems beyond the software attack boundary by input data perturbation. Existing fault mitigation schemes including data masking, zeroing-on-error and circuit level time-borrowing techniques exploit the noise-tolerance of neural network models to resist random and sparse errors. Such noise tolerant-based schemes are not sufficiently effective to suppress intensive transient errors if a DNN accelerator is blasted with malicious and deliberate faults. In this paper, we conduct comprehensive investigations on reported resilient designs and propose a more robust countermeasure to fault injection attacks. The proposed design utilizes shadow flip flops for error detection and lightweight circuit for timely error correction. Our forward error compensation scheme rectifies the incorrect partial sum of the multiply-accumulation operation by estimating the difference between the correct and error-inflicted computation. The difference is added back to the final accumulated result at a later cycle without stalling the execution pipeline. We implemented our proposed design and the existing fault-mitigation schemes on the same Intel FPGA-based DNN accelerator to demonstrate its substantially enhanced resiliency against deliberate fault attacks on two popular DNN models, ResNet50 and VGG16, trained with ImageNet.",
        "link": "https://dl.acm.org/doi/10.1145/3474376.3487281",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Accelerating DNNs inference with predictive layer fusion",
        "authors": "['MohammadHossein Olyaiy', 'Christopher Ng', 'Mieszko Lis']",
        "date": "June 2021",
        "source": "ICS '21: Proceedings of the ACM International Conference on Supercomputing",
        "abstract": "Many modern convolutional neural neworks (CNNs) rely on bottleneck block structures where the activation tensor is mapped between higher dimensions using an intermediate low dimension, and convolved with depthwise feature filters rather than multi-channel filters. Because most of the computation lies in computing the large dimensional tensors, however, such networks cannot be scaled without significant computation costs. In this paper, we show how \\emph{fusing} the layers inside these blocks can dramatically reduce the multiplication count (by 6--20x) at the cost of extra additions. ReLU nonlinearities are predicted dynamically, and only the activations that survive ReLU contribute to directly compute the output of the block. We also propose FusioNet, a CNN architecture optimized for fusion, as well as ARCHON, a novel accelerator design with a dataflow optimized for fused networks. When FusioNet is executed on the proposed accelerator, it yields up to 5.8x faster inference compared to compact networks executed on a dense DNN accelerator, and 2.1x faster inference compared to the same networks when pruned and executed on a sparse DNN accelerator.",
        "link": "https://dl.acm.org/doi/10.1145/3447818.3460378",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Maya: using formal control to obfuscate power side channels",
        "authors": "['Raghavendra Pradyumna Pothukuchi', 'Sweta Yamini Pothukuchi', 'Petros G. Voulgaris', 'Alexander Schwing', 'Josep Torrellas']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "The security of computers is at risk because of information leaking through their power consumption. Attackers can use advanced signal measurement and analysis to recover sensitive data from this side channel. To address this problem, this paper presents Maya, a simple and effective defense against power side channels. The idea is to use formal control to re-shape the power dissipated by a computer in an application-transparent manner---preventing attackers from learning any information about the applications that are running. With formal control, a controller can reliably keep power close to a desired target function even when runtime conditions change unpredictably. By selecting the target function intelligently, the controller can make power to follow any desired shape, appearing to carry activity information which, in reality, is unrelated to the application. Maya can be implemented in privileged software, firmware, or simple hardware. In this paper, we implement Maya on three machines using privileged threads only, and show its effectiveness and ease of deployment. Maya has already thwarted a newly-developed remote power attack.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00074",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "N3H-Core: Neuron-designed Neural Network Accelerator via FPGA-based Heterogeneous Computing Cores",
        "authors": "['Yu Gong', 'Zhihan Xu', 'Zhezhi He', 'Weifeng Zhang', 'Xiaobing Tu', 'Xiaoyao Liang', 'Li Jiang']",
        "date": "February 2022",
        "source": "FPGA '22: Proceedings of the 2022 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays",
        "abstract": "Accelerating the neural network inference by FPGA has emerged as a popular option, since the reconfigurability and high performance computing capability of FPGA intrinsically satisfies the computation demand of the fast-evolving neural algorithms. However, the popular neural accelerators on FPGA (e.g., Xilinx DPU) mainly utilize the DSP resources for constructing their processing units, while the rich LUT resources are not well exploited. Via the software-hardware co-design approach, in this work, we develop an FPGA-based heterogeneous computing system for neural network acceleration. From the hardware perspective, the proposed accelerator consists of DSP- and LUT-based GEneral Matrix-Multiplication (GEMM) computing cores, which forms the entire computing system in a heterogeneous fashion. The DSP- and LUT-based GEMM cores are computed w.r.t a unified Instruction Set Architecture (ISA) and unified buffers. Along the data flow of the neural network inference path, the computation of the convolution/fully-connected layer is split into two portions, handled by the DSP- and LUT-based GEMM cores asynchronously. From the software perspective, we mathematically and systematically model the latency and resource utilization of the proposed heterogeneous accelerator, regarding varying system design configurations. Through leveraging the reinforcement learning technique, we construct a framework to achieve end-to-end selection and optimization of the design specification of target heterogeneous accelerator, including workload split strategy, mixed-precision quantization scheme, and resource allocation of DSP- and LUT-core. In virtue of the proposed design framework and heterogeneous computing system, our design outperforms the state-of-the-art Mix&Match design with latency reduced by 1.12-1.32x with higher inference accuracy. The N3H-core is open-sourced at: https://github.com/elliothe/N3H_Core.",
        "link": "https://dl.acm.org/doi/10.1145/3490422.3502367",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A Modular End-to-End Framework for Secure Firmware Updates on Embedded Systems",
        "authors": "['Solon Falas', 'Charalambos Konstantinou', 'Maria K. Michael']",
        "date": "None",
        "source": "ACM Journal on Emerging Technologies in Computing Systems",
        "abstract": "Firmware refers to device read-only resident code which includes microcode and macro-instruction-level routines. For Internet-of-Things (IoT) devices without an operating system, firmware includes all the necessary instructions on how such embedded systems operate and communicate. Thus, firmware updates are essential parts of device functionality. They provide the ability to patch vulnerabilities, address operational issues, and improve device reliability and performance during the lifetime of the system. This process, however, is often exploited by attackers in order to inject malicious firmware code into the embedded device. In this article, we present a framework for secure firmware updates on embedded systems. This approach is based on hardware primitives and cryptographic modules, and it can be deployed in environments where communication channels might be insecure. The implementation of the framework is flexible, as it can be adapted in regards to the IoT device’s available hardware resources and constraints. Our security analysis shows that our framework is resilient to a variety of attack vectors. The experimental setup demonstrates the feasibility of the approach. By implementing a variety of test cases on FPGA, we demonstrate the adaptability and performance of the framework. Experiments indicate that the update procedure for a 1183-kB firmware image could be achieved, in a secure manner, under 1.73 seconds.",
        "link": "https://dl.acm.org/doi/10.1145/3460234",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Design of Distributed Reconfigurable Robotics Systems with ReconROS",
        "authors": "['Christian Lienen', 'Marco Platzner']",
        "date": "None",
        "source": "ACM Transactions on Reconfigurable Technology and Systems",
        "abstract": "Robotics applications process large amounts of data in real time and require compute platforms that provide high performance and energy efficiency. FPGAs are well suited for many of these applications, but there is a reluctance in the robotics community to use hardware acceleration due to increased design complexity and a lack of consistent programming models across the software/hardware boundary. In this article, we present ReconROS, a framework that integrates the widely used robot operating system (ROS) with ReconOS, which features multithreaded programming of hardware and software threads for reconfigurable computers. This unique combination gives ROS 2 developers the flexibility to transparently accelerate parts of their robotics applications in hardware. We elaborate on the architecture and the design flow for ReconROS and report on a set of experiments that underline the feasibility and flexibility of our approach.",
        "link": "https://dl.acm.org/doi/10.1145/3494571",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A Conditionally Chaotic Physically Unclonable Function Design Framework with High Reliability",
        "authors": "['Saranyu Chattopadhyay', 'Pranesh Santikellur', 'Rajat Subhra Chakraborty', 'Jimson Mathew', 'Marco Ottavi']",
        "date": "None",
        "source": "ACM Transactions on Design Automation of Electronic Systems",
        "abstract": "Physically Unclonable Function (PUF) circuits are promising low-overhead hardware security primitives, but are often gravely susceptible to machine learning–based modeling attacks. Recently, chaotic PUF circuits have been proposed that show greater robustness to modeling attacks. However, they often suffer from unacceptable overhead, and their analog components are susceptible to low reliability. In this article, we propose the concept of a conditionally chaotic PUF that enhances the reliability of the analog components of a chaotic PUF circuit to a level at par with their digital counterparts. A conditionally chaotic PUF has two modes of operation: bistable and chaotic, and switching between these two modes is conveniently achieved by setting a mode-control bit (at a secret position) in an applied input challenge. We exemplify our PUF design framework for two different PUF variants—the CMOS Arbiter PUF and a previously proposed hybrid CMOS-memristor PUF, combined with a hardware realization of the Lorenz system as the chaotic component. Through detailed circuit simulation and modeling attack experiments, we demonstrate that the proposed PUF circuits are highly robust to modeling and cryptanalytic attacks, without degrading the reliability of the original PUF that was combined with the chaotic circuit, and incurs acceptable hardware footprint.",
        "link": "https://dl.acm.org/doi/10.1145/3460004",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Birds of a Feather Flock Together: Scaling RDMA RPCs with Flock",
        "authors": "['Sumit Kumar Monga', 'Sanidhya Kashyap', 'Changwoo Min']",
        "date": "October 2021",
        "source": "SOSP '21: Proceedings of the ACM SIGOPS 28th Symposium on Operating Systems Principles",
        "abstract": "RDMA-capable networks are gaining traction with datacenter deployments due to their high throughput, low latency, CPU efficiency, and advanced features, such as remote memory operations. However, efficiently utilizing RDMA capability in a common setting of high fan-in, fan-out asymmetric network topology is challenging. For instance, using RDMA programming features comes at the cost of connection scalability, which does not scale with increasing cluster size. To address that, several works forgo some RDMA features by only focusing on conventional RPC APIs. In this work, we strive to exploit the full capability of RDMA, while scaling the number of connections regardless of the cluster size. We present Flock, a communication framework for RDMA networks that uses hardware provided reliable connection. Using a partially shared model, Flock departs from the conventional RDMA design by enabling connection sharing among threads, which provides significant performance improvements contrary to the widely held belief that connection sharing deteriorates performance. At its core, Flock uses a connection handle abstraction for connection multiplexing; a new coalescing-based synchronization approach for efficient network utilization; and a load-control mechanism for connections with symbiotic send-recv scheduling, which reduces the synchronization overheads associated with connection sharing along with ensuring fair utilization of network connections. We demonstrate the benefits for a distributed transaction processing system and an in-memory index, where it outperforms other RPC systems by up to 88% and 50%, respectively, with significant reductions in median and tail latency.",
        "link": "https://dl.acm.org/doi/10.1145/3477132.3483576",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "The benefits of general-purpose on-NIC memory",
        "authors": "['Boris Pismenny', 'Liran Liss', 'Adam Morrison', 'Dan Tsafrir']",
        "date": "February 2022",
        "source": "ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "We propose to use the small, newly available on-NIC memory (\"nicmem\") to keep pace with the rapidly increasing performance of NICs. We motivate our proposal by accelerating two types of workload classes: NFV and key-value stores. As NFV workloads frequently operate on headers---rather than data---of incoming packets, we introduce a new packet-processing architecture that splits between the two, keeping the data on nicmem when possible and thus reducing PCIe traffic, memory bandwidth, and CPU processing time. Our approach consequently shortens NFV latency by up to 23% and increases its throughput by up to 19%. Similarly, because key-value stores commonly exhibit skewed distributions, we introduce a new network stack mechanism that lets applications keep frequently accessed items on nicmem. Our design shortens memcached latency by up to 43% and increases its throughput by up to 80%.",
        "link": "https://dl.acm.org/doi/10.1145/3503222.3507711",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "PolyGraph: exposing the value of flexibility for graph processing accelerators",
        "authors": "['Vidushi Dadu', 'Sihao Liu', 'Tony Nowatzki']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "Because of the importance of graph workloads and the limitations of CPUs/GPUs, many graph processing accelerators have been proposed. The basic approach of prior accelerators is to focus on a single graph algorithm variant (eg. bulk-synchronous + slicing). While helpful for specialization, this leaves performance potential from flexibility on the table and also complicates understanding the relationship between graph types, workloads, algorithms, and specialization. In this work, we explore the value of flexibility in graph processing accelerators. First, we identify a taxonomy of key algorithm variants. Then we develop a template architecture (PolyGraph) that is flexible across these variants while being able to modularly integrate specialization features for each. Overall we find that flexibility in graph acceleration is critical. If only one variant can be supported, asynchronous-updates/priority-vertex-scheduling/graph-slicing is the best design, achieving 1.93X speedup over the best-performing accelerator, GraphPulse. However, static flexibility per-workload can further improve performance by 2.71X. With dynamic flexibility per-phase, performance further improves by up to 50%.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00053",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Designing and Building communIT",
        "authors": "['Carlos Henrique Araujo de Aguiar', 'Keith Green', 'Trevor Pinch', 'Gilly Leshed', 'Kevin Guo', 'Yeolim Jo']",
        "date": "June 2021",
        "source": "MAB20: Media Architecture Biennale 20",
        "abstract": "Many subgroups in the US remain marginalized from, misunderstood by, or invisible to the larger communities they reside in. Technologies supporting community building, more generally, have focused on apps, but these apps can fall short of making visible and heard subgroups such as the LGTBQ+, immigrant, and black populations. In response to this shortcoming, we report on the design iterations and an early evaluation of communIT—an interactive artifact for making visible and heard subgroups towards building community. To inform the design of communIT, we conducted in our lab a design studio study (N=57), a co-design activity with a to-scale prototype (N= 12), and a co-design activity with a full-scale prototype (N=28). This paper offers a design exemplar of a large-scale, cyber-physical artifact that might support groups in shaping their identities, practices, and roles in the larger community.",
        "link": "https://dl.acm.org/doi/10.1145/3469410.3469411",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Design of AID and Monitoring Function Based on Intelligent Vision",
        "authors": "['Yang Yang', 'Jianglong Fu', 'Jianguang Zhao', 'Juan Hao', 'Haoyue Sun', 'Xiaohui Qin']",
        "date": "December 2021",
        "source": "ICASIT 2021: 2021 International Conference on Aviation Safety and Information Technology",
        "abstract": "Event detection system is more and more used in road monitoring. This paper proposes a traffic multi state recognition system based on intelligent vision recognition technology. The gray change interval of frame difference is used to update the image background, and the pixel change control rule is introduced as the core software algorithm. Combined with traffic state monitoring data source, multi state traffic events such as congestion, pedestrian and parking are detected. The actual system test shows that the system can timely and accurately detect related traffic events, and has high detection accuracy. As an important part of intelligent transportation system, traffic incident automatic detection (AID) system plays an important role in avoiding traffic accidents, handling and controlling.",
        "link": "https://dl.acm.org/doi/10.1145/3510858.3511360",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Lessons Learned from Blockchain Applications of Trusted Execution Environments and Implications for Future Research",
        "authors": "['Karanjai Rabimba', 'Lei Xu', 'Lin Chen', 'Fengwei Zhang', 'Zhimin Gao', 'Weidong Shi']",
        "date": "October 2021",
        "source": "HASP '21: Proceedings of the 10th International Workshop on Hardware and Architectural Support for Security and Privacy",
        "abstract": "Modern computer systems tend to rely on large trusted computing bases (TCBs) for operations. To address the TCB bloating problem, hardware vendors have developed mechanisms to enable or facilitate the creation of a trusted execution environment (TEE) in which critical software applications can execute securely in an isolated environment. Even under the circumstance that a host OS is compromised by an adversary, key security properties such as confidentiality and integrity of the software inside the TEEs can be guaranteed. The promise of integrity and security has driven developers to adopt it for use cases involving access control, PKS, IoT among other things. Among these applications include blockchain-related use cases. The usage of the TEEs doesn’t come without its own implementation challenges and potential pitfalls. In this paper, we examine the assumptions, security models, and operational environments of the proposed TEE use cases of blockchain-based applications. The exercise and analysis help the hardware TEE research community to identify some open challenges and opportunities for research and rethink the design of hardware TEEs in general.",
        "link": "https://dl.acm.org/doi/10.1145/3505253.3505259",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Jetstream2: Accelerating cloud computing via Jetstream",
        "authors": "['David Y. Hancock', 'Jeremy Fischer', 'John Michael Lowe', 'Winona Snapp-Childs', 'Marlon Pierce', 'Suresh Marru', 'J. Eric Coulter', 'Matthew Vaughn', 'Brian Beck', 'Nirav Merchant', 'Edwin Skidmore', 'Gwen Jacobs']",
        "date": "July 2021",
        "source": "PEARC '21: Practice and Experience in Advanced Research Computing",
        "abstract": "Jetstream2 will be a category I production cloud resource that is part of the National Science Foundation’s Innovative HPC Program. The project’s aim is to accelerate science and engineering by providing “on-demand” programmable infrastructure built around a core system at Indiana University and four regional sites. Jetstream2 is an evolution of the Jetstream platform, which functions primarily as an Infrastructure-as-a-Service cloud. The lessons learned in cloud architecture, distributed storage, and container orchestration have inspired changes in both hardware and software for Jetstream2. These lessons have wide implications as institutions converge HPC and cloud technology while building on prior work when deploying their own cloud environments. Jetstream2’s next-generation hardware, robust open-source software, and enhanced virtualization will provide a significant platform to further cloud adoption within the US research and education communities.",
        "link": "https://dl.acm.org/doi/10.1145/3437359.3465565",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "BiSon-e: a lightweight and high-performance accelerator for narrow integer linear algebra computing on the edge",
        "authors": "['Enrico Reggiani', 'Cristóbal Ramírez Lazo', 'Roger Figueras Bagué', 'Adrián Cristal', 'Mauro Olivieri', 'Osman Sabri Unsal']",
        "date": "February 2022",
        "source": "ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "Linear algebra computational kernels based on byte and sub-byte integer data formats are at the base of many classes of applications, ranging from Deep Learning to Pattern Matching. Porting the computation of these applications from cloud to edge and mobile devices would enable significant improvements in terms of security, safety, and energy efficiency. However, despite their low memory and energy demands, their intrinsically high computational intensity makes the execution of these workloads challenging on highly resource-constrained devices. In this paper, we present BiSon-e, a novel RISC-V based architecture that accelerates linear algebra kernels based on narrow integer computations on edge processors by performing Single Instruction Multiple Data (SIMD) operations on off-the-shelf scalar Functional Units (FUs). Our novel architecture is built upon the binary segmentation technique, which allows to significantly reduce the memory footprint and the arithmetic intensity of linear algebra kernels requiring narrow data sizes. We integrate BiSon-e into a complete System-on-Chip (SoC) based on RISC-V, synthesized and Place&Routed in 65nm and 22nm technologies, introducing a negligible 0.07% area overhead with respect to the baseline architecture. Our experimental evaluation shows that, when computing the Convolution and Fully-Connected layers of the AlexNet and VGG-16 Convolutional Neural Networks (CNNs) with 8-, 4-, and 2-bit, our solution gains up to 5.6×, 13.9× and 24× in execution time compared to the scalar implementation of a single RISC-V core, and improves the energy efficiency of string matching tasks by 5× when compared to a RISC-V-based Vector Processing Unit (VPU).",
        "link": "https://dl.acm.org/doi/10.1145/3503222.3507746",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Dual-side sparse tensor core",
        "authors": "['Yang Wang', 'Chen Zhang', 'Zhiqiang Xie', 'Cong Guo', 'Yunxin Liu', 'Jingwen Leng']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "Leveraging sparsity in deep neural network (DNN) models is promising for accelerating model inference. Yet existing GPUs can only leverage the sparsity from weights but not activations, which are dynamic, unpredictable, and hence challenging to exploit. In this work, we propose a novel architecture to efficiently harness the dual-side sparsity (i.e., weight and activation sparsity). We take a systematic approach to understand the (dis)advantages of previous sparsity-related architectures and propose a novel, unexplored paradigm that combines outer-product computation primitive and bitmap-based encoding format. We demonstrate the feasibility of our design with minimal changes to the existing production-scale inner-product-based Tensor Core. We propose a set of novel ISA extensions and co-design the matrix-matrix multiplication and convolution algorithms, which are the two dominant computation patterns in today's DNN models, to exploit our new dual-side sparse Tensor Core. Our evaluation shows that our design can fully unleash the dualside DNN sparsity and improve the performance by up to one order of magnitude with small hardware overhead.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00088",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Design Cognition while using digital tools: A Distributed Cognition Approach",
        "authors": "['Vriddhi Vriddhi', 'Joy Sen', 'Aneesha Sharma']",
        "date": "September 2021",
        "source": "ICDTE '21: Proceedings of the 5th International Conference on Digital Technology in Education",
        "abstract": "The use of digital tools in the conventional architecture design thinking process which derives its basis from sketching is followed in many colleges in India. Various shortcomings due to the integration of digital tools to the manual design process have been enumerated during the past 30 years. Digital tools provide affordances different from the manual sketching design process, the effects of which can be understood by adopting a distributed cognition approach. The paper builds on design cognition research while using externalization tools in the design process. It does so by developing a theoretical framework derived from distributed cognition and an understanding of visual thinking processes from design literature. The paper utilizes the distributed cognition framework by Zhang and Norman, to arrive at resultant affordances of externalization tools in design. The same is then utilized for a protocol study which was coded for its visual thinking components and other relevant codes. The same protocol study was also coded for ideation flow analysis. The findings pointed towards compromised visual thinking and reduced ideation while utilizing digital tools in quick conceptualization.",
        "link": "https://dl.acm.org/doi/10.1145/3488466.3488491",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Software Hint-Driven Data Management for Hybrid Memory in Mobile Systems",
        "authors": "['Fei Wen', 'Mian Qin', 'Paul Gratz', 'Narasimha Reddy']",
        "date": "None",
        "source": "ACM Transactions on Embedded Computing Systems",
        "abstract": "Hybrid memory systems, comprised of emerging non-volatile memory (NVM) and DRAM, have been proposed to address the growing memory demand of current mobile applications. Recently emerging NVM technologies, such as phase-change memories (PCM), memristor, and 3D XPoint, have higher capacity density, minimal static power consumption and lower cost per GB. However, NVM has longer access latency and limited write endurance as opposed to DRAM. The different characteristics of distinct memory classes render a new challenge for memory system design.Ideally, pages should be placed or migrated between the two types of memories according to the data objects’ access properties. Prior system software approaches exploit the program information from OS but at the cost of high software latency incurred by related kernel processes. Hardware approaches can avoid these latencies, however, hardware’s vision is constrained to a short time window of recent memory requests, due to the limited on-chip resources.In this work, we propose OpenMem: a hardware-software cooperative approach that combines the execution time advantages of pure hardware approaches with the data object properties in a global scope. First, we built a hardware-based memory manager unit (HMMU) that can learn the short-term access patterns by online profiling, and execute data migration efficiently. Then, we built a heap memory manager for the heterogeneous memory systems that allows the programmer to directly customize each data object’s allocation to a favorable memory device within the presumed object life cycle. With the programmer’s hints guiding the data placement at allocation time, data objects with similar properties will be congregated to reduce unnecessary page migrations.We implemented the whole system on the FPGA board with embedded ARM processors. In testing under a set of benchmark applications from SPEC 2017 and PARSEC, experimental results show that OpenMem reduces 44.6% energy consumption with only a 16% performance degradation compared to the all-DRAM memory system. The amount of writes to the NVM is reduced by 14% versus the HMMU-only, extending the NVM device lifetime.",
        "link": "https://dl.acm.org/doi/10.1145/3494536",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Approximate Constant-Coefficient Multiplication Using Hybrid Binary-Unary Computing for FPGAs",
        "authors": "['S. Rasoul Faraji', 'Pierre Abillama', 'Kia Bazargan']",
        "date": "None",
        "source": "ACM Transactions on Reconfigurable Technology and Systems",
        "abstract": "Multipliers are used in virtually all Digital Signal Processing (DSP) applications such as image and video processing. Multiplier efficiency has a direct impact on the overall performance of such applications, especially when real-time processing is needed, as in 4K video processing, or where hardware resources are limited, as in mobile and IoT devices. We propose a novel, low-cost, low energy, and high-speed approximate constant coefficient multiplier (CCM) using a hybrid binary-unary encoding method. The proposed method implements a CCM using simple routing networks with no logic gates in the unary domain, which results in more efficient multipliers compared to Xilinx LogiCORE IP CCMs and table-based KCM CCMs (Flopoco) on average. We evaluate the proposed multipliers on 2-D discrete cosine transform algorithm as a common DSP module. Post-routing FPGA results show that the proposed multipliers can improve the {area, area × delay, power consumption, and energy-delay product} of a 2-D discrete cosine transform on average by {30%, 33%, 30%, 31%}. Moreover, the throughput of the proposed 2-D discrete cosine transform is on average 5% more than that of the binary architecture implemented using table-based KCM CCMs. We will show that our method has fewer routability issues compared to binary implementations when implementing a DCT core.",
        "link": "https://dl.acm.org/doi/10.1145/3494570",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "DTA-PUF: Dynamic Timing-aware Physical Unclonable Function for Resource-constrained Devices",
        "authors": "['Ioannis Tsiokanos', 'Jack Miskelly', 'Chongyan Gu', 'Maire O’neill', 'Georgios Karakonstantis']",
        "date": "None",
        "source": "ACM Journal on Emerging Technologies in Computing Systems",
        "abstract": "In recent years, physical unclonable functions (PUFs) have gained a lot of attention as mechanisms for hardware-rooted device authentication. While the majority of the previously proposed PUFs derive entropy using dedicated circuitry, software PUFs achieve this from existing circuitry in a system. Such software-derived designs are highly desirable for low-power embedded systems as they require no hardware overhead. However, these software PUFs induce considerable processing overheads that hinder their adoption in resource-constrained devices. In this article, we propose DTA-PUF, a novel, software PUF design that exploits the instruction- and data-dependent dynamic timing behaviour of pipelined cores to provide a reliable challenge-response mechanism without requiring any extra hardware. DTA-PUF accepts sequences of instructions as an input challenge and produces an output response based on the manifested timing errors under specific over-clocked settings. To lower the required processing effort, we systematically select instruction sequences that maximise error-rate. The application to a post-layout pipelined floating-point unit, which is implemented in 45 nm process technology, demonstrates the effectiveness and practicability of our PUF design. Finally, DTA-PUF requires up to 50× fewer instructions than existing software processor PUF designs, limiting processing costs and resulting in up to 26% power savings.",
        "link": "https://dl.acm.org/doi/10.1145/3434281",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A cost-effective entangling prefetcher for instructions",
        "authors": "['Alberto Ros', 'Alexandra Jimborean']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "Prefetching instructions in the instruction cache is a fundamental technique for designing high-performance computers. There are three key properties to consider when designing an efficient and effective prefetcher: timeliness, coverage, and accuracy. Timeliness is essential, as bringing instructions too early increases the risk of the instructions being evicted from the cache before their use and requesting them too late can lead to the instructions arriving after they are demanded. Coverage is important to reduce the number of instruction cache misses and accuracy to ensure that the prefetcher does not pollute the cache or interacts negatively with the other hardware mechanisms. This paper presents the Entangling Prefetcher for Instructions that entangles instructions to maximize timeliness. The prefetcher works by finding which instruction should trigger the prefetch for a subsequent instruction, accounting for the latency of each cache miss. The prefetcher is carefully adjusted to account for both coverage and accuracy. Our evaluation shows that with 40KB of storage, Entangling can increase performance up to 23%, outperforming state-of-the-art prefetchers.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00017",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Dvé: improving DRAM reliability and performance on-demand via coherent replication",
        "authors": "['Adarsh Patil', 'Vijay Nagarajan', 'Rajeev Balasubramonian', 'Nicolai Oswald']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "As technologies continue to shrink, memory system failure rates have increased, demanding support for stronger forms of reliability. In this work, we take inspiration from the two-tier approach that decouples correction from detection and explore a novel extrapolation. We propose Dvé, a hardware-driven replication mechanism where data blocks are replicated in 2 different sockets across a cache-coherent NUMA system. Each data block is also accompanied by a code with strong error detection capabilities so that when an error is detected, correction is performed using the replica. Such an organization has the advantage of offering two independent points of access to data which enables: (a) strong error correction that can recover from a range of faults affecting any of the components in the memory, upto and including the memory controller, and (b) higher performance by providing another nearer point of memory access. Dvé realizes both of these benefits via Coherent Replication, a technique that builds on top of existing cache coherence protocols for not only keeping the replicas in sync for reliability, but also to provide coherent access to the replicas during fault-free operation for performance. Dvé can flexibly provide these benefits on-demand by simply using the provisioned memory capacity which, as reported in recent studies, is often underutilized in today's systems. Thus, Dvé introduces a unique design point that offers higher reliability and performance for workloads that do not require the entire memory capacity.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00048",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Rebooting virtual memory with midgard",
        "authors": "['Siddharth Gupta', 'Atri Bhattacharyya', 'Yunho Oh', 'Abhishek Bhattacharjee', 'Babak Falsafi', 'Mathias Payer']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "Computer systems designers are building cache hierarchies with higher capacity to capture the ever-increasing working sets of modern workloads. Cache hierarchies with higher capacity improve system performance but shift the performance bottleneck to address translation. We propose Midgard, an intermediate address space between the virtual and the physical address spaces, to mitigate address translation overheads without program-level changes. Midgard leverages the operating system concept of virtual memory areas (VMAs) to realize a single Midgard address space where VMAs of all processes can be uniquely mapped. The Midgard address space serves as the namespace for all data in a coherence domain and the cache hierarchy. Because real-world workloads use far fewer VMAs than pages to represent their virtual address space, virtual to Midgard translation is achieved with hardware structures that are much smaller than TLB hierarchies. Costlier Midgard to physical address translations are needed only on LLC misses, which become much less frequent with larger caches. As a consequence, Midgard shows that instead of amplifying address translation overheads, memory hierarchies with large caches can reduce address translation overheads. Our evaluation shows that Midgard achieves only 5% higher address translation overhead as compared to traditional TLB hierarchies for 4KB pages when using a 16MB aggregate LLC. Midgard also breaks even with traditional TLB hierarchies for 2MB pages when using a 256MB aggregate LLC. For cache hierarchies with higher capacity, Midgard's address translation overhead drops to near zero as secondary and tertiary data working sets fit in the LLC, while traditional TLBs suffer even higher degrees of address translation overhead.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00047",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "SPACE: locality-aware processing in heterogeneous memory for personalized recommendations",
        "authors": "['Hongju Kal', 'Seokmin Lee', 'Gun Ko', 'Won Woo Ro']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "Personalized recommendation systems have become a major AI application in modern data centers. The main challenges in processing personalized recommendation inferences are the large memory footprint and high bandwidth requirement of embedding layers. To overcome the capacity limit and bandwidth congestion of on-chip memory, near memory processing (NMP) can be a promising solution. Recent work on accelerating personalized recommendations proposes a DIMM-based NMP design to solve the bandwidth problem and increases memory capacity. The performance of NMP is determined by the internal bandwidth and the prior DIMM-based approach utilizes more DIMMs to achieve higher operation throughput. However, extending the number of DIMMs could eventually lead to significant power consumption due to inefficient scaling. We propose SPACE, a novel heterogeneous memory architecture, which is efficient in terms of performance and energy. SPACE exploits a compute-capable 3D-stacked DRAM with DIMMs for personalized recommendations. Prior to designing the proposed system, we give a quantitative analysis of the user/item interactions and define the two localities: gather locality and reduction locality. In gather operations, we find only a small proportion of items are highly-accessed by users, and we call this gather locality. Also, we define reduction locality as the reusability of the gathered items in reduction operations. Based on the gather locality, SPACE allocates highly-accessed embedding items to the 3D-stacked DRAM to achieve the maximum bandwidth. Subsequently, by exploiting reduction locality, we utilize the remaining space of the 3D-stacked DRAM to store and reuse repeated partial sums, thereby minimizing the required number of element-wise reduction operations. As a result, the evaluation shows that SPACE achieves 3.2X performance improvement and 56% energy saving over the previous DIMM-based NMPs leveraging 3D-stacked DRAM with a 1/8 size of DIMMs. Also, compared to the state-of-the-art DRAM cache designs with the same NMP configuration, SPACE achieves an average 32.7% of performance improvement.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00059",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Porting Deep Spiking Q-Networks to neuromorphic chip Loihi",
        "authors": "['Mahmoud Akl', 'Yulia Sandamirskaya', 'Florian Walter', 'Alois Knoll']",
        "date": "July 2021",
        "source": "ICONS 2021: International Conference on Neuromorphic Systems 2021",
        "abstract": "Deep neural networks (DNNs) set the benchmark in many tasks in perception and control. Spiking versions of DNNs, implemented on neuromorphic hardware can enable orders of magnitude lower power consumption and low latency during network use. In this paper, we explore behavior and generalization capability of spiking, quantized spiking, and hardware implementation of deep Q-networks in two classical reinforcement learning tasks. We found that spiking neural networks have slightly decreased performance compared to non-spiking network, but we can avoid performance degradation from quantization and in-chip implementation. We conclude that since hardware implementation leads to lower power consumption and low latency, neuromorphic approach is a promising avenue for deep Q-learning. Furthermore, online learning, enabled in neuromorphic chips, can be used to compensate for the performance decrease in environments with parameter variations.",
        "link": "https://dl.acm.org/doi/10.1145/3477145.3477159",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "TimeCache: using time to eliminate cache side channels when sharing software",
        "authors": "['Divya Ojha', 'Sandhya Dwarkadas']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "Timing side channels have been used to extract cryptographic keys and sensitive documents even from trusted enclaves. Specifically, cache side channels created by reuse of shared code or data in the memory hierarchy have been exploited by several known attacks, e.g., evict+reload for recovering an RSA key and Spectre variants for leaking speculatively loaded data. In this paper, we present TimeCache, a cache design that incorporates knowledge of prior cache line access to eliminate cache side channels due to reuse of shared software (code and data). Our goal is to retain the benefits of a shared cache of allowing each process access to the entire cache and of cache occupancy by a single copy of shared software. We achieve our goal by implementing per-process cache line visibility so that the processes do not benefit from cached data brought in by another process until they have incurred a corresponding miss penalty. Our design achieves low overhead by using a novel combination of timestamps and a hardware design to allow efficient parallel comparisons of the timestamps. The solution works at all the cache levels without the need to limit the number of security domains, and defends against an attacker process running on the same core, on a another hyperthread, or on another core. Our implementation in the gem5 simulator demonstrates that the system is able to defend against RSA key extraction. We evaluate performance using SPEC2006 and PARSEC and observe the overhead of TimeCache to be 1.13% on average. Delay due to first access misses adds the majority of the overhead, with the security context bookkeeping incurred at the time of a context switch contributing 0.02% of the 1.13%.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00037",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "GraphPEG: Accelerating Graph Processing on GPUs",
        "authors": "['Yashuai Lü', 'Hui Guo', 'Libo Huang', 'Qi Yu', 'Li Shen', 'Nong Xiao', 'Zhiying Wang']",
        "date": "None",
        "source": "ACM Transactions on Architecture and Code Optimization",
        "abstract": "Due to massive thread-level parallelism, GPUs have become an attractive platform for accelerating large-scale data parallel computations, such as graph processing. However, achieving high performance for graph processing with GPUs is non-trivial. Processing graphs on GPUs introduces several problems, such as load imbalance, low utilization of hardware unit, and memory divergence. Although previous work has proposed several software strategies to optimize graph processing on GPUs, there are several issues beyond the capability of software techniques to address.In this article, we present GraphPEG, a graph processing engine for efficient graph processing on GPUs. Inspired by the observation that many graph algorithms have a common pattern on graph traversal, GraphPEG improves the performance of graph processing by coupling automatic edge gathering with fine-grain work distribution. GraphPEG can also adapt to various input graph datasets and simplify the software design of graph processing with hardware-assisted graph traversal. Simulation results show that, in comparison with two representative highly efficient GPU graph processing software framework Gunrock and SEP-Graph, GraphPEG improves graph processing throughput by 2.8× and 2.5× on average, and up to 7.3× and 7.0× for six graph algorithm benchmarks on six graph datasets, with marginal hardware cost.",
        "link": "https://dl.acm.org/doi/10.1145/3450440",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Scalable Phylogeny Reconstruction with Disaggregated Near-memory Processing",
        "authors": "['Nikolaos Alachiotis', 'Panagiotis Skrimponis', 'Manolis Pissadakis', 'Dionisios Pnevmatikatos']",
        "date": "None",
        "source": "ACM Transactions on Reconfigurable Technology and Systems",
        "abstract": "Disaggregated computer architectures eliminate resource fragmentation in next-generation datacenters by enabling virtual machines to employ resources such as CPUs, memory, and accelerators that are physically located on different servers. While this paves the way for highly compute- and/or memory-intensive applications to potentially deploy all CPUs and/or memory resources in a datacenter, it poses a major challenge to the efficient deployment of hardware accelerators: input/output data can reside on different servers than the ones hosting accelerator resources, thereby requiring time- and energy-consuming remote data transfers that diminish the gains of hardware acceleration. Targeting a disaggregated datacenter architecture similar to the IBM dReDBox disaggregated datacenter prototype, the present work explores the potential of deploying custom acceleration units adjacently to the disaggregated-memory controller on memory bricks (in dReDBox terminology), which is implemented on FPGA technology, to reduce data movement and improve performance and energy efficiency when reconstructing large phylogenies (evolutionary relationships among organisms). A fundamental computational kernel is the Phylogenetic Likelihood Function (PLF), which dominates the total execution time (up to 95%) of widely used maximum-likelihood methods. Numerous efforts to boost PLF performance over the years focused on accelerating computation; since the PLF is a data-intensive, memory-bound operation, performance remains limited by data movement, and memory disaggregation only exacerbates the problem. We describe two near-memory processing models, one that addresses the problem of workload distribution to memory bricks, which is particularly tailored toward larger genomes (e.g., plants and mammals), and one that reduces overall memory requirements through memory-side data interpolation transparently to the application, thereby allowing the phylogeny size to scale to a larger number of organisms without requiring additional memory.",
        "link": "https://dl.acm.org/doi/10.1145/3484983",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "IntroSpectre: a pre-silicon framework for discovery and analysis of transient execution vulnerabilities",
        "authors": "['Moein Ghaniyoun', 'Kristin Barber', 'Yinqian Zhang', 'Radu Teodorescu']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "Transient execution vulnerabilities originate in the extensive speculation implemented in modern high-performance microprocessors. Identifying all possible vulnerabilities in complex designs is very challenging. One of the challenges stems from the lack of visibility into the transient micro-architectural state of the processor. Prior work has used covert channels to identify data leakage from transient state, which limits the systematic discovery of all potential leakage sources. This paper presents INTROSPECTRE, a pre-silicon framework for early discovery of transient execution vulnerabilities. INTROSPECTRE addresses the lack of visibility into the micro-architectural processor state by integrating into the register transfer level (RTL) design flow, gaining full access to the internal state of the processor. Full visibility into the processor state enables INTROSPECTRE to perform a systematic leakage analysis that includes all micro-architectural structures, allowing it to identify potential leakage that may not be reachable with known side channels. We implement INTROSPECTRE on an RTL simulator and use it to perform transient leakage analysis on the RISC-V BOOM processor. We identify multiple transient leakage scenarios, most of which had not been highlighted on this processor design before.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00073",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "SIAM: Chiplet-based Scalable In-Memory Acceleration with Mesh for Deep Neural Networks",
        "authors": "['Gokul Krishnan', 'Sumit K. Mandal', 'Manvitha Pannala', 'Chaitali Chakrabarti', 'Jae-Sun Seo', 'Umit Y. Ogras', 'Yu Cao']",
        "date": "None",
        "source": "ACM Transactions on Embedded Computing Systems",
        "abstract": "In-memory computing (IMC) on a monolithic chip for deep learning faces dramatic challenges on area, yield, and on-chip interconnection cost due to the ever-increasing model sizes. 2.5D integration or chiplet-based architectures interconnect multiple small chips (i.e., chiplets) to form a large computing system, presenting a feasible solution beyond a monolithic IMC architecture to accelerate large deep learning models. This paper presents a new benchmarking simulator, SIAM, to evaluate the performance of chiplet-based IMC architectures and explore the potential of such a paradigm shift in IMC architecture design. SIAM integrates device, circuit, architecture, network-on-chip (NoC), network-on-package (NoP), and DRAM access models to realize an end-to-end system. SIAM is scalable in its support of a wide range of deep neural networks (DNNs), customizable to various network structures and configurations, and capable of efficient design space exploration. We demonstrate the flexibility, scalability, and simulation speed of SIAM by benchmarking different state-of-the-art DNNs with CIFAR-10, CIFAR-100, and ImageNet datasets. We further calibrate the simulation results with a published silicon result, SIMBA. The chiplet-based IMC architecture obtained through SIAM shows 130\\(\\) and 72\\(\\) improvement in energy-efficiency for ResNet-50 on the ImageNet dataset compared to Nvidia V100 and T4 GPUs.",
        "link": "https://dl.acm.org/doi/10.1145/3476999",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Ripple: profile-guided instruction cache replacement for data center applications",
        "authors": "['Tanvir Ahmed Khan', 'Dexin Zhang', 'Akshitha Sriraman', 'Joseph Devietti', 'Gilles Pokam', 'Heiner Litz', 'Baris Kasikci']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "Modern data center applications exhibit deep software stacks, resulting in large instruction footprints that frequently cause instruction cache misses degrading performance, cost, and energy efficiency. Although numerous mechanisms have been proposed to mitigate instruction cache misses, they still fall short of ideal cache behavior, and furthermore, introduce significant hardware overheads. We first investigate why existing I-cache miss mitigation mechanisms achieve sub-optimal performance for data center applications. We find that widely-studied instruction prefetchers fall short due to wasteful prefetch-induced cache line evictions that are not handled by existing replacement policies. Existing replacement policies are unable to mitigate wasteful evictions since they lack complete knowledge of a data center application's complex program behavior. To make existing replacement policies aware of these eviction-inducing program behaviors, we propose Ripple, a novel software-only technique that profiles programs and uses program context to inform the underlying replacement policy about efficient replacement decisions. Ripple carefully identifies program contexts that lead to I-cache misses and sparingly injects \"cache line eviction\" instructions in suitable program locations at link time. We evaluate Ripple using nine popular data center applications and demonstrate that Ripple enables any replacement policy to achieve speedup that is closer to that of an ideal I-cache. Specifically, Ripple achieves an average performance improvement of 1.6% (up to 2.13%) over prior work due to a mean 19% (up to 28.6%) I-cache miss reduction.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00063",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Confidential serverless made efficient with <u>p</u>lug-<u>in</u> <u>e</u>nclaves",
        "authors": "['Mingyu Li', 'Yubin Xia', 'Haibo Chen']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "Serverless computing has become a fact of life on modern clouds. A serverless function may process sensitive data from clients. Protecting such a function against untrusted clouds using hardware enclave is attractive for user privacy. In this work, we run existing serverless applications in SGX enclave, and observe that the performance degradation can be as high as 5.6X to even 422.6X. Our investigation identifies these slowdowns are related to architectural features, mainly from page-wise enclave initialization. Leveraging insights from our overhead analysis, we revisit SGX hardware design and make minimal modification to its enclave model. We extend SGX with a new primitive---region-wise plugin enclaves that can be mapped into existing enclaves to reuse attested common states amongst functions. By remapping plugin enclaves, an enclave allows in-situ processing to avoid expensive data movement in a function chain. Experiments show that our design reduces the enclave function latency by 94.74--99.57%, and boosts the autoscaling throughput by 19-179X.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00032",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Sunder: Enabling Low-Overhead and Scalable Near-Data Pattern Matching Acceleration",
        "authors": "['Elaheh Sadredini', 'Reza Rahimi', 'Mohsen Imani', 'Kevin Skadron']",
        "date": "October 2021",
        "source": "MICRO '21: MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture",
        "abstract": "Automata processing is an efficient computation model for regular expressions and other forms of sophisticated pattern matching. The demand for high-throughput and real-time pattern matching in many applications, including network intrusion detection and spam filters, has motivated several in-memory architectures for automata processing. Existing in-memory architectures focus on accelerating the pattern-matching kernel, but either fail to support a practical reporting solution or optimistically assume that the reporting stage is not the performance bottleneck. However, gathering and processing the reports can be the major bottleneck, especially when the reporting frequency is high. Moreover, all the existing in-memory architectures work with a fixed processing rate (mostly 8-bit/cycle), and they do not adjust the input consumption rate based on the properties of the applications, which can lead to throughput and capacity loss.  To address these issues, we present Sunder, an in-SRAM pattern matching architecture, to processes a reconfigurable number of nibbles (4-bit symbols) in parallel, instead of fixed-rate processing, by adopting an algorithm/architecture methodology to perform hardware-aware transformations. Inspired by prior work, we transform the commonly-used 8-bit processing to nibble-processing (4-bit processing) to reduce hardware requirements exponentially and achieve higher information density. This frees up space for storing reporting data in place, which significantly eliminates host communication and reporting overhead. Our proposed reporting architecture supports in-place report summarization and provides an easy access mechanism to read the reporting data. As a result, Sunder enables a low-overhead, high-performance, and flexible in-memory pattern-matching and reporting solution. Our results confirm that Sunder reporting architecture has zero performance overhead for 95% of the applications and incurs only 2% additional hardware overhead.",
        "link": "https://dl.acm.org/doi/10.1145/3466752.3480934",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "HASCO: towards agile <u>ha</u>rdware and <u>s</u>oftware <u>co</u>-design for tensor computation",
        "authors": "['Qingcheng Xiao', 'Size Zheng', 'Bingzhe Wu', 'Pengcheng Xu', 'Xuehai Qian', 'Yun Liang']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "Tensor computations overwhelm traditional general-purpose computing devices due to the large amounts of data and operations of the computations. They call for a holistic solution composed of both hardware acceleration and software mapping. Hardware/software (HW/SW) co-design optimizes the hardware and software in concert and produces high-quality solutions. There are two main challenges in the co-design flow. First, multiple methods exist to partition tensor computation and have different impacts on performance and energy efficiency. Besides, the hardware part must be implemented by the intrinsic functions of spatial accelerators. It is hard for programmers to identify and analyze the partitioning methods manually. Second, the overall design space composed of HW/SW partitioning, hardware optimization, and software optimization is huge. The design space needs to be efficiently explored. To this end, we propose an agile co-design approach HASCO that provides an efficient HW/SW solution to dense tensor computation. We use tensor syntax trees as the unified IR, based on which we develop a two-step approach to identify partitioning methods. For each method, HASCO explores the hardware and software design spaces. We propose different algorithms for the explorations, as they have distinct objectives and evaluation costs. Concretely, we develop a multi-objective Bayesian optimization algorithm to explore hardware optimization. For software optimization, we use heuristic and Q-learning algorithms. Experiments demonstrate that HASCO achieves a 1.25X to 1.44X latency reduction through HW/SW co-design compared with developing the hardware and software separately.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00086",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "On the TOCTOU Problem in Remote Attestation",
        "authors": "['Ivan De Oliveira Nunes', 'Sashidhar Jakkamsetti', 'Norrathep Rattanavipanon', 'Gene Tsudik']",
        "date": "November 2021",
        "source": "CCS '21: Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security",
        "abstract": "Much attention has been devoted to verifying software integrity of remote embedded (IoT) devices. Many techniques, with different assumptions and security guarantees, have been proposed under the common umbrella of so-called Remote Attestation (RA). Aside from executable's integrity verification, RA serves as a foundation for many security services, such as proofs of memory erasure, system reset, software update, and verification of runtime properties. Prior RA techniques verify the remote device's binary at the time when RA functionality is executed, thus providing no information about the device's binary before current RA execution or between consecutive RA executions. This implies that presence of transient malware (in the form of modified binary) may be undetected. In other words, if transient malware infects a device (by modifying its binary), performs its nefarious tasks, and erases itself before the next attestation, its temporary presence will not be detected. This important problem, called Time-Of-Check-Time-Of-Use ( TOCTOU ), is well-known in the research literature and remains unaddressed in the context of hybrid RA. In this work, we propose Remote Attestation with TOCTOU Avoidance (RATA): a provably secure approach to address the RA TOCTOU problem. With RATA, even malware that erases itself before execution of the next RA, can not hide its ephemeral presence. RATA targets hybrid RA architectures, which are aimed at low-end embedded devices. We present two alternative techniques - RATA A and RATA B - suitable for devices with and without real-time clocks, respectively. Each is shown to be secure and accompanied by a publicly available and formally verified implementation. Our evaluation demonstrates low hardware overhead of both techniques. Compared with current hybrid RA architectures - that offer no TOCTOU protection - RATA incurs no extra runtime overhead. In fact, it substantially reduces the time complexity of RA computations: from linear to constant time.",
        "link": "https://dl.acm.org/doi/10.1145/3460120.3484532",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Implementation and acceleration scheme of Heart sound classification Algorithm based on SOC-FPGA",
        "authors": "['Guozheng Li', 'Hongbo Yang', 'Tao Guo', 'Weilain Wang']",
        "date": "January 2022",
        "source": "BIC 2022: 2022 2nd International Conference on Bioinformatics and Intelligent Computing",
        "abstract": "ABSTRACT-Widespread screening of congenital heart disease is of time-consuming, labor-consuming, and difficult for rural doctors to master the skill of cardiac auscultation. A kind of machine-assisted diagnosis method was put forwarded in this paper to solve the above problems. In which a heart sound classification algorithm and acceleration plan of CNN was implemented on a small scale SoC-FPGA chip with fewer resources. In this method, heart sounds were denoised and segmented into cardio cycles first. Then STFT transformation was done for time-frequency feature extraction. The time-frequency features were used to train the CNN model to extract network model parameters. In hardware implementation, the parallelism of CNN was corresponding to FPGA parallel hardware. In order to make acceleration of the algorithm, loop unrolling, fixed-point of model parameter, and reducing global memory access were done. The experimental results show that the classification speed is 3.13 times as much as one of CPU at the same conditions with the classification accuracy not any dropping significantly. It provides an offline solution for the machine-assisted screening of congenital heart disease.",
        "link": "https://dl.acm.org/doi/10.1145/3523286.3524551",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "FlexDriver: a network driver for your accelerator",
        "authors": "['Haggai Eran', 'Maxim Fudim', 'Gabi Malka', 'Gal Shalom', 'Noam Cohen', 'Amit Hermony', 'Dotan Levi', 'Liran Liss', 'Mark Silberstein']",
        "date": "February 2022",
        "source": "ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "We propose a new system design for connecting hardware and FPGA accelerators to the network, allowing the accelerator to directly control commodity Network Interface Cards (NICs) without using the CPU. This enables us to solve the key challenge of leveraging existing NIC hardware offloads such as virtualization, tunneling, and RDMA for accelerator networking. Our approach supports a diverse set of use cases, from direct network access for disaggregated accelerators to inline-acceleration of the network stack, all without the complex networking logic in the accelerator.  To demonstrate the feasibility of this approach, we build FlexDriver (FLD), an on-accelerator hardware module that implements a NIC data-plane driver. Our main technical contribution is a mechanism that compresses the NIC control structures by two orders of magnitude, allowing FLD to achieve high networking scalability with low die area cost and no bandwidth interference with the accelerator logic.  The prototype for NVIDIA Innova-2 FPGA SmartNICs showcases our design’s utility for three different accelerators: a disaggregated LTE cipher, an IP-defragmentation inline accelerator, and an IoT cryptographic-token authentication offload. These accelerators reach 25 Gbps line rate and leverage the NIC for RDMA processing, VXLAN tunneling, and traffic shaping without CPU involvement.",
        "link": "https://dl.acm.org/doi/10.1145/3503222.3507776",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Research on High Performance Transmission Technology of DC Based on Network Awareness",
        "authors": "['Yanwei Wang', 'Cheng Huang', 'Jiaheng Fan', 'Le Yang', 'Hongwei Kan', 'Gaoming Cao']",
        "date": "October 2021",
        "source": "CSAE '21: Proceedings of the 5th International Conference on Computer Science and Application Engineering",
        "abstract": "In recent years, RDMA over Converged Ethernet (RoCE) provides high performance data transmission for Data center (DC). RoCE has excellent performance in lossless network, but when the network environment is unstable, the transmission performance of RoCE will decline rapidly. This paper proposes a DC network transmission technology based on network awareness. By monitoring the network status, the network status can be updated in real time. The transmission mechanism can be adjusted dynamically. In order to support the transmission in harsh environment, this paper constructs DC-TCP transmission based on Data Plane Development Kit (DPDK) TCP and DC-RoCE transmission based on RoCE, and designs a high-performance DC-TCP / DC-RoCE fusion technology framework. The simulation results show that the DC network transmission technology based on network awareness significantly improves the transmission capacity of the DC in complex environment.",
        "link": "https://dl.acm.org/doi/10.1145/3487075.3487153",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Domain Isolation in FPGA-Accelerated Cloud and Data Center Applications",
        "authors": "['Joel Mandebi Mbongue', 'Sujan Kumar Saha', 'Christophe Bobda']",
        "date": "June 2021",
        "source": "GLSVLSI '21: Proceedings of the 2021 on Great Lakes Symposium on VLSI",
        "abstract": "Cloud and data center applications increasingly leverage FPGAs because of their performance/watt benefits and flexibility advantages over traditional processing cores such as CPUs and GPUs. As the rising demand for hardware acceleration gradually leads to FPGA multi-tenancy in the cloud, there are rising concerns about the security challenges posed by FPGA virtualization. Exposing space-shared FPGAs to multiple cloud tenants may compromise the confidentiality, integrity, and availability of FPGA-accelerated applications. In this work, we present a hardware/software architecture for domain isolation in FPGA-accelerated clouds and data centers with a focus on software-based attacks aiming at unauthorized access and information leakage. Our proposed architecture implements Mandatory Access Control security policies from software down to the hardware accelerators on FPGA. Our experiments demonstrate that the proposed architecture protects against such attacks with minimal area and communication overhead.",
        "link": "https://dl.acm.org/doi/10.1145/3453688.3461527",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Energy efficiency boost in the AI-infused POWER10 processor",
        "authors": "['Brian W. Thompto', 'Dung Q. Nguyen', 'José E. Moreira', 'Ramon Bertran', 'Hans Jacobson', 'Richard J. Eickemeyer', 'Rahul M. Rao', 'Michael Goulet', 'Marcy Byers', 'Christopher J. Gonzalez', 'Karthik Swaminathan', 'Nagu R. Dhanwada', 'Silvia M. Müller', 'Andreas Wagner', 'Satish Kumar Sadasivam', 'Robert K. Montoye', 'William J. Starke', 'Christian G. Zoellin', 'Michael S. Floyd', 'Jeffrey Stuecheli', 'Nandhini Chandramoorthy', 'John-David Wellman', 'Alper Buyuktosunoglu', 'Matthias Pflanz', 'Balaram Sinharoy', 'Pradip Bose']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "We present the novel micro-architectural features, supported by an innovative and novel pre-silicon methodology in the design of POWER10. The resulting projected energy efficiency boost over POWER9 is 2.6x at core level (for SPECint) and up to 3x at socket level. In addition, a new feature supporting inline AI acceleration was added to the POWER ISA and incorporated into the POWER10 processor core design. The resulting boost in SIMD/AI socket performance is projected to be up to 10x for FP32 and 21x for INT8 models of ResNet-50 and BERT-Large. In this paper, we describe the novel methodology deployed and used not only to obtain these efficiency boosts for traditional workloads, but also to infuse AI/ML/HPC capability directly into the POWER10 core.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00012",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Design and Implementation of FPGA-based High-speed Printing Printhead Controller",
        "authors": "['Haoran Wen', 'Feng Li', 'Shijie Zhou']",
        "date": "May 2021",
        "source": "ICFEICT 2021: International Conference on Frontiers of Electronics, Information and Computation Technologies",
        "abstract": "This paper presents a new FPGA-based control system for high-speed digital printing printheads. System we proposed is dedicated to freeing general-purpose processors from the heavy burden of controlling printhead, and instead using FPGA hardware acceleration to achieve a more efficient, stable control system. The main work of this paper includes: (1) changing the traditional printing image processing algorithm suitable for general-purpose processors to make it a hardware-oriented algorithm that can be executed by FPGA in parallel; (2) Splitting data processing unit and timing control unit of the system, reducing their coupling, increasing the reliability, and also bringing higher maintainability and expandability to the system. After simulation tests, our system is able to achieve the expected functions, and the upper limit of system throughput can be improved by about 8 times compared with traditional methods.",
        "link": "https://dl.acm.org/doi/10.1145/3474198.3478155",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Research on DC Network Transmission Handover Technology Based on User Mode Sharing",
        "authors": "['Cheng Huang', 'Yanwei Wang', 'Jiaheng Fan', 'Le Yang', 'Junkai Liu', 'Hongwei Kan']",
        "date": "October 2021",
        "source": "CSAE '21: Proceedings of the 5th International Conference on Computer Science and Application Engineering",
        "abstract": "RDMA over Converged Ethernet (RoCE) and Data Plane Development Kit (DPDK) TCP are both high-performance transmission technologies for Data center (DC), and they are often mixed due to different characteristics. When the application scenario or network status changes, the application needs to be switched from one communication mode to another. The current method is to stop one mode of communication and then re-establish another mode of network connection, which is less efficient. This paper constructs a DC network transmission handover technology based on user mode sharing, which includes: user mode integrated driver, end-to-end transmission handover, and multi-mode synchronous caching technology. User mode integration driver transfers the lower-level authority of the Ethernet card and RoCE upwards. On this basis, technologies such as end-to-end transmission handover and multi-mode synchronous cache can be established. The end-to-end transmission handover improves the overall performance and reduces the performance loss of switching streams one by one. Multi-mode synchronous cache technology applies synchronous processing technology to the user-mode driven technology, which reduces the performance loss caused by cache retransmission during handover. The simulation experiment verifies that the new technology has the characteristics of low latency in various switching scenarios.",
        "link": "https://dl.acm.org/doi/10.1145/3487075.3487154",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Networked Answer to \"Life, The Universe, and Everything\"",
        "authors": "['Giles Babich', 'Keith Bengston', 'Andrew Bolin', 'John Bunton', 'Yuqing Chen', 'Grant Hampson', 'David Humphrey', 'Guillaume Jourjon']",
        "date": "December 2021",
        "source": "ANCS '21: Proceedings of the Symposium on Architectures for Networking and Communications Systems",
        "abstract": "In the last few years, Input/Output (I/O) bandwidth limitation of legacy computer architectures forced us to reconsider where and how to store and compute data across a large range of applications. This shift has been made possible with the concurrent development of both smartNICs and programmable switches with a common programming language (P4), and the advent of attached High Bandwidth Memory within smartNICs/FPGAs. Recently, proposals to use this kind of technology have emerged to tackle computer science related issues such as fast consensus algorithm in the network, network accelerated key-value stores, machine learning, or data-center data aggregation. In this paper, we introduce a novel architecture that leverages these advancements to potentially accelerate and improve the processing of radio-astronomy Digital Signal Processing (DSP), such as correlators or beamformers, at unprecedented continuous rates in what we have called the \"Atomic COTS\" design. We give an overview of this new type of architecture to accelerate digital signal processing, leveraging programmable switches and HBM capable FPGAs. We also discuss how to handle radio astronomy data streams to pre-process this stream of data for astronomy science products such as pulsar timing and search. Finally, we illustrate, using a proof of concept, how we can process emulated data from the Square Kilometer Array (SKA) project to time pulsars.",
        "link": "https://dl.acm.org/doi/10.1145/3493425.3502770",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Cache abstraction for data race detection in heterogeneous systems with non-coherent accelerators",
        "authors": "['May Young', 'Alan J. Hu', 'Guy G. F. Lemieux']",
        "date": "June 2021",
        "source": "LCTES 2021: Proceedings of the 22nd ACM SIGPLAN/SIGBED International Conference on Languages, Compilers, and Tools for Embedded Systems",
        "abstract": "Embedded systems are becoming increasingly complex and heterogeneous, featuring multiple processor cores (which might themselves be heterogeneous) as well as specialized hardware accelerators, all accessing shared memory. Many accelerators are non-coherent (i.e., do not support hardware cache coherence) because it reduces hardware complexity, cost, and power consumption, while potentially offering superior performance. However, the disadvantage of non-coherence is that the software must explicitly synchronize between accelerators and processors, and this synchronization is notoriously error-prone.   We propose an analysis technique to find data races in software for heterogeneous systems that include non-coherent accelerators. Our approach builds on classical results for data race detection, but the challenge turns out to be analyzing cache behavior rather than the behavior of the non-coherent accelerators. Accordingly, our central contribution is a novel, sound (data-race-preserving) abstraction of cache behavior. We prove our abstraction sound, and then to demonstrate the precision of our abstraction, we implement it in a simple dynamic race detector for a system with a processor and a massively parallel accelerator provided by a commercial FPGA-based accelerator vendor. On eleven software examples provided by the vendor, the tool had zero false positives and was able to detect previously unknown data races in 2 of the 11 examples.",
        "link": "https://dl.acm.org/doi/10.1145/3461648.3463856",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "NDS: N-Dimensional Storage",
        "authors": "['Yu-Chia Liu', 'Hung-Wei Tseng']",
        "date": "October 2021",
        "source": "MICRO '21: MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture",
        "abstract": "Demands for efficient computing among applications that use high-dimensional datasets have led to multi-dimensional computers—computers that leverage heterogeneous processors/accelerators offering various processing models to support multi-dimensional compute kernels. Yet the front-end for these processors/accelerators is inefficient, as memory/storage systems often expose only entrenched linear-space abstractions to an application, and they often ignore the benefits of modern memory/storage systems, such as support for multi-dimensionality through different types of parallel access.  This paper presents N-Dimensional Storage (NDS), a novel, multi-dimensional memory/storage system that fulfills the demands of modern hardware accelerators and applications. NDS abstracts memory arrays as native storage that applications can use to describe data locations and uses coordinates in any application-defined multi-dimensional space, thereby avoiding the software overhead associated with data-object transformations. NDS gauges the application demand underlying memory-device architectures in order to intelligently determine the physical data layout that maximizes access bandwidth and minimizes the overhead of presenting objects for arbitrary applications.  This paper demonstrates an efficient architecture in supporting NDS. We evaluate a set of linear/tensor algebra workloads along with graph and data-mining algorithms on custom-built systems using each architecture. Our result shows a 5.73 × speedup with appropriate architectural support.",
        "link": "https://dl.acm.org/doi/10.1145/3466752.3480122",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "In-sensor classification with boosted race trees",
        "authors": "['Georgios Tzimpragos', 'Advait Madhavan', 'Dilip Vasudevan', 'Dmitri Strukov', 'Timothy Sherwood']",
        "date": "June 2021",
        "source": "Communications of the ACM",
        "abstract": "When extremely low-energy processing is required, the choice of data representation makes a tremendous difference. Each representation (e.g., frequency domain, residue coded, and log-scale) embodies a different set of tradeoffs based on the algebraic operations that are either easy or hard to perform in that domain. We demonstrate the potential of a novel form of encoding, race logic, in which information is represented as the delay in the arrival of a signal. Under this encoding, the ways in which signal delays interact and interfere with one another define the operation of the system. Observations of the relative delays (for example, the outcome of races between signals) define the output of the computation. Interestingly, completely standard hardware logic elements can be repurposed to this end and the resulting embedded systems have the potential to be extremely energy efficient. To realize this potential in a practical design, we demonstrate two different approaches to the creation of programmable tree-based ensemble classifiers in an extended set of race logic primitives; we explore the trade-offs inherent to their operation across sensor, hardware architecture, and algorithm; and we compare the resulting designs against traditional state-of-the-art hardware techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3460223",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Cloud Building Block Chip for Creating FPGA and ASIC Clouds",
        "authors": "['Atakan Doğan', 'Kemal Ebcioğlu']",
        "date": "None",
        "source": "ACM Transactions on Reconfigurable Technology and Systems",
        "abstract": "Hardware-accelerated cloud computing systems based on FPGA chips (FPGA cloud) or ASIC chips (ASIC cloud) have emerged as a new technology trend for power-efficient acceleration of various software applications. However, the operating systems and hypervisors currently used in cloud computing will lead to power, performance, and scalability problems in an exascale cloud computing environment. Consequently, the present study proposes a parallel hardware hypervisor system that is implemented entirely in special-purpose hardware, and that virtualizes application-specific multi-chip supercomputers, to enable virtual supercomputers to share available FPGA and ASIC resources in a cloud system. In addition to the virtualization of multi-chip supercomputers, the system’s other unique features include simultaneous migration of multiple communicating hardware tasks, and on-demand increase or decrease of hardware resources allocated to a virtual supercomputer. Partitioning the flat hardware design of the proposed hypervisor system into multiple partitions and applying the chip unioning technique to its partitions, the present study introduces a cloud building block chip that can be used to create FPGA or ASIC clouds as well. Single-chip and multi-chip verification studies have been done to verify the functional correctness of the hypervisor system, which consumes only a fraction of (10%) hardware resources.",
        "link": "https://dl.acm.org/doi/10.1145/3466822",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "How to Analyze, Preserve, and Communicate Leonardo's Drawing? A Solution to Visualize in RTR Fine Art Graphics Established from “the Best Sense”",
        "authors": "['Fabrizio Ivan Apollonio', 'Riccardo Foschi', 'Marco Gaiani', 'Simone Garagnani']",
        "date": "None",
        "source": "Journal on Computing and Cultural Heritage ",
        "abstract": "Original hand drawings by Leonardo are astonishing collections of knowledge, superb representations of the artist's way of working, which proves the technical and cultural peak of the Renaissance era. However, due to their delicate and fragile nature, they are hard to manipulate and compulsory to preserve. To overcome this problem we developed, in a 10-year-long research program, a complete workflow to produce a system able to replace, investigate, describe and communicate ancient fine drawings through what Leonardo calls “the best sense” (i.e., the view), the so-called ISLe (InSightLeonardo). The outcoming visualization app is targeted to a wide audience made of museum visitors and, most importantly, art historians, scholars, conservators, and restorers. This article describes a specific feature of the workflow: the appearance modeling with the aim of an accurate Real-Time Rendering (RTR) visualization. This development is based on the direct observation of five among the most known Leonardo da Vinci's drawings, spanning his entire activity as a draftsman, and it is the result of an accurate analysis of drawing materials used by Leonardo, in which peculiarities of materials are digitally reproduced at the various scales exploiting solutions that favor the accuracy of perceived reproduction instead of the fidelity to the physical model and their ability to be efficiently implemented over a standard GPU-accelerated RTR pipeline. Results of the development are exemplified on five of Leonardo's drawings and multiple evaluations of the results, subjective and objective, are illustrated, aiming to assess potential and critical issues of the application.",
        "link": "https://dl.acm.org/doi/10.1145/3433606",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "GenStore: a high-performance in-storage processing system for genome sequence analysis",
        "authors": "['Nika Mansouri Ghiasi', 'Jisung Park', 'Harun Mustafa', 'Jeremie Kim', 'Ataberk Olgun', 'Arvid Gollwitzer', 'Damla Senol Cali', 'Can Firtina', 'Haiyu Mao', 'Nour Almadhoun Alserr', 'Rachata Ausavarungnirun', 'Nandita Vijaykumar', 'Mohammed Alser', 'Onur Mutlu']",
        "date": "February 2022",
        "source": "ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "Read mapping is a fundamental step in many genomics applications. It is used to identify potential matches and differences between fragments (called reads) of a sequenced genome and an already known genome (called a reference genome). Read mapping is costly because it needs to perform approximate string matching (ASM) on large amounts of data. To address the computational challenges in genome analysis, many prior works propose various approaches such as accurate filters that select the reads within a dataset of genomic reads (called a read set) that must undergo expensive computation, efficient heuristics, and hardware acceleration. While effective at reducing the amount of expensive computation, all such approaches still require the costly movement of a large amount of data from storage to the rest of the system, which can significantly lower the end-to-end performance of read mapping in conventional and emerging genomics systems.  We propose GenStore, the first in-storage processing system designed for genome sequence analysis that greatly reduces both data movement and computational overheads of genome sequence analysis by exploiting low-cost and accurate in-storage filters. GenStore leverages hardware/software co-design to address the challenges of in-storage processing, supporting reads with 1) different properties such as read lengths and error rates, which highly depend on the sequencing technology, and 2) different degrees of genetic variation compared to the reference genome, which highly depends on the genomes that are being compared. Through rigorous analysis of read mapping processes of reads with different properties and degrees of genetic variation, we meticulously design low-cost hardware accelerators and data/computation flows inside a NAND flash-based solid-state drive (SSD). Our evaluation using a wide range of real genomic datasets shows that GenStore, when implemented in three modern NAND flash-based SSDs, significantly improves the read mapping performance of state-of-the-art software (hardware) baselines by 2.07-6.05× (1.52-3.32×) for read sets with high similarity to the reference genome and 1.45-33.63× (2.70-19.2×) for read sets with low similarity to the reference genome.",
        "link": "https://dl.acm.org/doi/10.1145/3503222.3507702",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "ReuseTracker: Fast Yet Accurate Multicore Reuse Distance Analyzer",
        "authors": "['Muhammad Aditya Sasongko', 'Milind Chabbi', 'Mandana Bagheri Marzijarani', 'Didem Unat']",
        "date": "None",
        "source": "ACM Transactions on Architecture and Code Optimization",
        "abstract": "One widely used metric that measures data locality is reuse distance—the number of unique memory locations that are accessed between two consecutive accesses to a particular memory location. State-of-the-art techniques that measure reuse distance in parallel applications rely on simulators or binary instrumentation tools that incur large performance and memory overheads. Moreover, the existing sampling-based tools are limited to measuring reuse distances of a single thread and discard interactions among threads in multi-threaded programs. In this work, we propose ReuseTracker—a fast and accurate reuse distance analyzer that leverages existing hardware features in commodity CPUs. ReuseTracker is designed for multi-threaded programs and takes cache-coherence effects into account. By utilizing hardware features like performance monitoring units and debug registers, ReuseTracker can accurately profile reuse distance in parallel applications with much lower overheads than existing tools. It introduces only 2.9× runtime and 2.8× memory overheads. Our tool achieves 92% accuracy when verified against a newly developed configurable benchmark that can generate a variety of different reuse distance patterns. We demonstrate the tool’s functionality with two use-case scenarios using PARSEC, Rodinia, and Synchrobench benchmark suites where ReuseTracker guides code refactoring in these benchmarks by detecting spatial reuses in shared caches that are also false sharing and successfully predicts whether some benchmarks in these suites can benefit from adjacent cache line prefetch optimization.",
        "link": "https://dl.acm.org/doi/10.1145/3484199",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Cohmeleon: Learning-Based Orchestration of Accelerator Coherence in Heterogeneous SoCs",
        "authors": "['Joseph Zuckerman', 'Davide Giri', 'Jihye Kwon', 'Paolo Mantovani', 'Luca P. Carloni']",
        "date": "October 2021",
        "source": "MICRO '21: MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture",
        "abstract": "One of the most critical aspects of integrating loosely-coupled accelerators in heterogeneous SoC architectures is orchestrating their interactions with the memory hierarchy, especially in terms of navigating the various cache-coherence options: from accelerators accessing off-chip memory directly, bypassing the cache hierarchy, to accelerators having their own private cache. By running real-size applications on FPGA-based prototypes of many-accelerator multi-core SoCs, we show that the best cache-coherence mode for a given accelerator varies at runtime, depending on the accelerator’s characteristics, the workload size, and the overall SoC status.  Cohmeleon applies reinforcement learning to select the best coherence mode for each accelerator dynamically at runtime, as opposed to statically at design time. It makes these selections adaptively, by continuously observing the system and measuring its performance. Cohmeleon is accelerator-agnostic, architecture-independent, and it requires minimal hardware support. Cohmeleon is also transparent to application programmers and has a negligible software overhead. FPGA-based experiments show that our runtime approach offers, on average, a 38% speedup with a 66% reduction of off-chip memory accesses compared to state-of-the-art design-time approaches. Moreover, it can match runtime solutions that are manually tuned for the target architecture.",
        "link": "https://dl.acm.org/doi/10.1145/3466752.3480065",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Multiple-tasks on multiple-devices (MTMD): exploiting concurrency in heterogeneous managed runtimes",
        "authors": "['Michail Papadimitriou', 'Eleni Markou', 'Juan Fumero', 'Athanasios Stratikopoulos', 'Florin Blanaru', 'Christos Kotselidis']",
        "date": "April 2021",
        "source": "VEE 2021: Proceedings of the 17th ACM SIGPLAN/SIGOPS International Conference on Virtual Execution Environments",
        "abstract": "Modern commodity devices are nowadays equipped with a plethora of heterogeneous devices serving different purposes. Being able to exploit such heterogeneous hardware accelerators to their full potential is of paramount importance in the pursuit of higher performance and energy efficiency. Towards these objectives, the reduction of idle time of each device as well as the concurrent program execution across different accelerators can lead to better scalability within the computing platform.  In this work, we propose a novel approach for enabling a Java-based heterogeneous managed runtime to automatically and efficiently deploy multiple tasks on multiple devices. We extend TornadoVM with parallel execution of bytecode interpreters to dynamically and concurrently manage and execute arbitrary tasks across multiple OpenCL-compatible devices. In addition, in order to achieve an efficient device-task allocation, we employ a machine learning approach with a multiple-classification architecture of Extra-Trees-Classifiers. Our proposed solution has been evaluated over a suite of 12 applications split into three different groups. Our experimental results showcase performance improvements up 83% compared to all tasks running on the single best device, while reaching up to 91% of the oracle performance.",
        "link": "https://dl.acm.org/doi/10.1145/3453933.3454019",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Vector runahead",
        "authors": "['Ajeya Naithani', 'Sam Ainsworth', 'Timothy M. Jones', 'Lieven Eeckhout']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "The memory wall places a significant limit on performance for many modern workloads. These applications feature complex chains of dependent, indirect memory accesses, which cannot be picked up by even the most advanced microarchitectural prefetchers. The result is that current out-of-order superscalar processors spend the majority of their time stalled. While it is possible to build special-purpose architectures to exploit the fundamental memory-level parallelism, a microarchitectural technique to automatically improve their performance in conventional processors has remained elusive. Runahead execution is a tempting proposition for hiding latency in program execution. However, to achieve high memory-level parallelism, a standard runahead execution skips ahead of cache misses. In modern workloads, this means it only prefetches the first cache-missing load in each dependent chain. We argue that this is not a fundamental limitation. If runahead were instead to stall on cache misses to generate dependent chain loads, then it could regain performance if it could stall on many at once. With this insight, we present Vector Runahead, a technique that prefetches entire load chains and speculatively reorders scalar operations from multiple loop iterations into vector format to bring in many independent loads at once. Vectorization of the runahead instruction stream increases the effective fetch/decode bandwidth with reduced resource requirements, to achieve high degrees of memory-level parallelism at a much faster rate. Across a variety of memory-latency-bound indirect workloads, Vector Runahead achieves a 1.79X performance speedup on a large out-of-order superscalar system, significantly improving on state-of-the-art techniques.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00024",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "BurstZ+: Eliminating The Communication Bottleneck of Scientific Computing Accelerators via Accelerated Compression",
        "authors": "['Gongjin Sun', 'Seongyoung Kang', 'Sang-Woo Jun']",
        "date": "None",
        "source": "ACM Transactions on Reconfigurable Technology and Systems",
        "abstract": "We present BurstZ+, an accelerator platform that eliminates the communication bottleneck between PCIe-attached scientific computing accelerators and their host servers, via hardware-optimized compression. While accelerators such as GPUs and FPGAs provide enormous computing capabilities, their effectiveness quickly deteriorates once data is larger than its on-board memory capacity, and performance becomes limited by the communication bandwidth of moving data between the host memory and accelerator. Compression has not been very useful in solving this issue due to performance and efficiency issues of compressing floating point numbers, which scientific data often consists of. BurstZ+ is an FPGA-based prototype accelerator platform which addresses the bandwidth issue via a class of novel hardware-optimized floating point compression algorithm called ZFP-V. We demonstrate that BurstZ+ can completely remove the host-side communication bottleneck for accelerators, using multiple stencil kernels with a wide range of operational intensities. Evaluated against hand-optimized implementations of kernel accelerators of the same architecture, our single-pipeline BurstZ+ prototype outperforms an accelerator without compression by almost 4×, and even an accelerator with enough memory for the entire dataset by over 2×. Furthermore, the projected performance of BurstZ+ on a future, faster FPGA scales to almost 7× that of the same accelerator without compression, whose performance is still limited by the PCIe bandwidth.",
        "link": "https://dl.acm.org/doi/10.1145/3476831",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "BlockMaestro: enabling programmer-transparent task-based execution in GPU systems",
        "authors": "['AmirAli Abdolrashidi', 'Hodjat Asghari Esfeden', 'Ali Jahanshahi', 'Kaustubh Singh', 'Nael Abu-Ghazaleh', 'Daniel Wong']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "As modern GPU workloads grow in size and complexity, there is an ever-increasing demand for GPU computational power. Emerging workloads contain hundreds or thousands of GPU kernel launches, which incur high overheads, and exhibit data-dependent behavior between kernels, which requires synchronization, leading to GPU under-utilization. Task-based execution models have been proposed to solve these issues, but they require significant programmer effort to port applications to proprietary task-based programming models in order to specify tasks and task dependencies. To address this need, we propose BlockMaestro, a software-hardware solution that combines command queue reordering, kernel-launch-time static analysis, and runtime hardware support to dynamically identify and resolve thread-block level data dependencies between kernels. Through static analysis of memory access patterns at kernel-launch-time, BlockMaestro can extract inter-kernel thread block-level data dependencies. BlockMaestro also introduces kernel pre-launching to reduce the kernel launch overheads experienced by multiple dependent kernels. Correctness is enforced by dynamically resolving thread block-level data dependency at runtime through hardware support. BlockMaestro achieves an average speedup of 51.76% (up to 2.92x) on data-dependent benchmarks, and requires minimal hardware overhead.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00034",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "HTNN: deep learning in heterogeneous transform domains with sparse-orthogonal weights",
        "authors": "['Yu Chen', 'Bowen Liu', 'Pierre Abillama', 'Hun-Seok Kim']",
        "date": "July 2021",
        "source": "ISLPED '21: Proceedings of the ACM/IEEE International Symposium on Low Power Electronics and Design",
        "abstract": "Convolutional neural networks (CNNs) achieved great success on various tasks in recent years. Their applications to low power and low cost hardware platforms, however, have been often limited due to extensive complexity of convolution layers. We present a new class of transform domain deep neural networks (DNNs), where convolution operations are replaced by element-wise multiplications in heterogeneous transform domains. To further reduce the network complexity, we propose a framework to learn sparse-orthogonal weights in heterogeneous transform domains co-optimized with a hardware-efficient accelerator architecture to minimize the overhead of handling sparse weights. Furthermore, sparse-orthogonal weights are non-uniformly quantized with canonical-signed-digit (CSD) representations to substitute multiplications with simpler additions. The proposed approach reduces the complexity by a factor of 4.9 -- 6.8 x without compromising the DNN accuracy compared to equivalent CNNs that employ sparse (pruned) weights. The code is available at https://github.com/unchenyu/HTNN.",
        "link": "https://dl.acm.org/doi/10.1109/ISLPED52811.2021.9502477",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "REDUCT: keep it close, keep it cool!: efficient scaling of DNN inference on multi-core CPUs with near-cache compute",
        "authors": "['Anant V. Nori', 'Rahul Bera', 'Shankar Balachandran', 'Joydeep Rakshit', 'Om J. Omer', 'Avishaii Abuhatzera', 'Belliappa Kuttanna', 'Sreenivas Subramoney']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "Deep Neural Networks (DNN) are used in a variety of applications and services. With the evolving nature of DNNs, the race to build optimal hardware (both in datacenter and edge) continues. General purpose multi-core CPUs offer unique attractive advantages for DNN inference at both datacenter [60] and edge [71]. Most of the CPU pipeline design complexity is targeted towards optimizing general-purpose single thread performance, and is overkill for relatively simpler, but still hugely important, data parallel DNN inference workloads. Addressing this disparity efficiently can enable both raw performance scaling and overall performance/Watt improvements for multi-core CPU DNN inference. We present REDUCT, where we build innovative solutions that bypass traditional CPU resources which impact DNN inference power and limit its performance. Fundamentally, REDUCT's \"Keep it close\" policy enables consecutive pieces of work to be executed close to each other. REDUCT enables instruction delivery/decode close to execution and instruction execution close to data. Simple ISA extensions encode the fixed-iteration count loop-y workload behavior enabling an effective bypass of many power-hungry front-end stages of the wide Out-of-Order (OoO) CPU pipeline. Per core performance scales efficiently by distributing lightweight tensor compute near all caches in a multi-level cache hierarchy. This maximizes the cumulative utilization of the existing architectural bandwidth resources in the system and minimizes movement of data. Across a number of DNN models, REDUCT achieves a 2.3X increase in convolution performance/Watt with a 2X to 3.94X scaling in raw performance. Similarly, REDUCT achieves a 1.8X increase in inner-product performance/Watt with 2.8X scaling in performance. REDUCT performance/power scaling is achieved with no increase to cache capacity or bandwidth and a mere 2.63% increase in area. Crucially, REDUCT operates entirely within the CPU programming and memory model, simplifying software development, while achieving performance similar to or better than state-of-the-art Domain Specific Accelerators (DSA) for DNN inference, providing fresh design choices in the AI era.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00022",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A Framework for Reproducible Data Plane Performance Modeling",
        "authors": "['Dominik Scholz', 'Hasanin Harkous', 'Sebastian Gallenmüller', 'Henning Stubbe', 'Max Helm', 'Benedikt Jaeger', 'Nemanja Deric', 'Endri Goshi', 'Zikai Zhou', 'Wolfgang Kellerer', 'Georg Carle']",
        "date": "December 2021",
        "source": "ANCS '21: Proceedings of the Symposium on Architectures for Networking and Communications Systems",
        "abstract": "Languages for programming data planes like P4 sparked a plethora of new applications in the data plane. The dynamic, evolving environment makes it challenging to understand what performance can be expected when running a program in a specific data plane target. However, knowing this is crucial for network operators when upgrading their networks. We present a framework for the reproducible analysis and modeling of P4 program components. By defining and generating precise specifications of the experiments, we separate fully auto-generated components from testbed- or target-specific parts. Measurement results are used to derive performance models automatically. These can then be used to compare the measured with the theoretical performance, or to model the cost of entire paths through the data plane. In two case studies, we use our framework to discover and model selected behavior for a DPDK-based software target and for the NFP-4000 SmartNIC platform.",
        "link": "https://dl.acm.org/doi/10.1145/3493425.3502756",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Survey of Transient Execution Attacks and Their Mitigations",
        "authors": "['Wenjie Xiong', 'Jakub Szefer']",
        "date": "None",
        "source": "ACM Computing Surveys",
        "abstract": "Transient execution attacks, also known as speculative execution attacks, have drawn much interest in the last few years as they can cause critical data leakage. Since the first disclosure of Spectre and Meltdown attacks in January 2018, a number of new transient execution attack types have been demonstrated targeting different processors. A transient execution attack consists of two main components: transient execution itself and a covert channel that is used to actually exfiltrate the information.Transient execution is a result of the fundamental features of modern processors that are designed to boost performance and efficiency, while covert channels are unintended information leakage channels that result from temporal and spatial sharing of the micro-architectural components. Given the severity of the transient execution attacks, they have motivated computer architects in both industry and academia to rethink the design of the processors and to propose hardware defenses. To help understand the transient execution attacks, this survey summarizes the phases of the attacks and the security boundaries across which the information is leaked in different attacks.This survey further analyzes the causes of transient execution as well as the different types of covert channels and presents a taxonomy of the attacks based on the causes and types. This survey in addition presents metrics for comparing different aspects of the transient execution attacks and uses them to evaluate the feasibility of the different attacks. This survey especially considers both existing attacks and potential new attacks suggested by our analysis. This survey finishes by discussing different mitigations that have so far been proposed at the micro-architecture level and discusses their benefits and limitations.",
        "link": "https://dl.acm.org/doi/10.1145/3442479",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "SCALPEL: Exploring the Limits of Tag-enforced Compartmentalization",
        "authors": "['Nick Roessler', 'André DeHon']",
        "date": "None",
        "source": "ACM Journal on Emerging Technologies in Computing Systems",
        "abstract": "We present Secure Compartments Automatically Learned and Protected by Execution using Lightweight metadata (SCALPEL), a tool for automatically deriving compartmentalization policies and lowering them to a tagged architecture for hardware-accelerated enforcement. SCALPEL allows a designer to explore high-quality points in the privilege-reduction vs. performance overhead tradeoff space using analysis tools and a detailed knowledge of the target architecture to make best use of the available hardware. SCALPEL automatically implements hundreds of compartmentalization strategies across the privilege-performance tradeoff space, all without manual tagging or code restructuring. SCALPEL uses two novel optimizations for achieving highly performant policies: the first is an algorithm for packing policies into working sets of rules for favorable rule cache characteristics, and the second is a rule prefetching system that allows it to exploit the highly predictable nature of compartmentalization rules. To create policies, SCALPEL introduces a quantitative privilege metric (the Overprivilege Ratio) that is used to drive its algorithmic compartment generation. We implement SCALPEL on a FreeRTOS stack and target a tag-extended RISC-V core. Our results show that SCALPEL-created policies can reduce overprivilege by orders of magnitude with hundreds of logical compartments while imposing low overheads (<5%).",
        "link": "https://dl.acm.org/doi/10.1145/3461673",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Zero inclusion victim: isolating core caches from inclusive last-level cache evictions",
        "authors": "['Mainak Chaudhuri']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "The most widely used last-level cache (LLC) architecture in the microprocessors has been the inclusive LLC design. The popularity of the inclusive design stems from the bandwidth optimization and simplification it offers to the implementation of the cache coherence protocols. However, inclusive LLCs have always been associated with the curse of inclusion victims. An inclusion victim is a block that must be forcefully replaced from the inner levels of the cache hierarchy when the copy of the block is replaced from the inclusive LLC. This tight coupling between the LLC victims and the inner-level cache contents leads to three major drawbacks. First, live inclusion victims can lead to severe performance degradation depending on the LLC replacement policies. Second, a process can victimize the blocks of another process in an LLC shared by multiple cores and this can be exploited to leak information through well-known eviction-based timing side-channels. An inclusive LLC makes these channels much less noisy due to the presence of inclusion victims which allow the malicious processes to control the contents of the per-core private caches through LLC evictions. Third, to reduce the impact of the aforementioned two drawbacks, the inner-level caches, particularly the mid-level cache in a three-level inclusive cache hierarchy, must be kept small even if a larger mid-level cache could have been beneficial in the absence of inclusion victims. We observe that inclusion victims are not fundamental to the inclusion property, but arise due to the way the contents of an inclusive LLC are managed. Motivated by this observation, we introduce a fundamentally new inclusive LLC design named the Zero Inclusion Victim (ZIV) LLC that guarantees freedom from inclusion victims while retaining all advantages of an inclusive LLC. This is the first inclusive LLC design proposal to offer such a guarantee, thereby completely isolating the core caches from LLC evictions. We observe that the root cause of inclusion victims is the constraint that an LLC victim must be chosen from the set pointed to by the set indexing function. The ZIV LLC relaxes this constraint only when necessary by efficiently and minimally enabling a global victim selection scheme in the inclusive LLC to avoid generation of inclusion victims. Detailed simulations conducted with a chip-multiprocessor model using multi-programmed and multi-threaded workloads show that the ZIV LLC gracefully supports large mid-level caches (e.g., half the size of the LLC) and delivers performance close to a non-inclusive LLC for different classes of LLC replacement policies. We also show that the ZIV LLC comfortably outperforms the existing related proposals and its performance lead grows with increasing mid-level cache capacity.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00015",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Specifying with Interface and Trait Abstractions in Abstract State Machines: A Controlled Experiment",
        "authors": "['Philipp Paulweber', 'Georg Simhandl', 'Uwe Zdun']",
        "date": "None",
        "source": "ACM Transactions on Software Engineering and Methodology",
        "abstract": "Abstract State Machine (ASM) theory is a well-known state-based formal method. As in other state-based formal methods, the proposed specification languages for ASMs still lack easy-to-comprehend abstractions to express structural and behavioral aspects of specifications. Our goal is to investigate object-oriented abstractions such as interfaces and traits for ASM-based specification languages. We report on a controlled experiment with 98 participants to study the specification efficiency and effectiveness in which participants needed to comprehend an informal specification as problem (stimulus) in form of a textual description and express a corresponding solution in form of a textual ASM specification using either interface or trait syntax extensions. The study was carried out with a completely randomized design and one alternative (interface or trait) per experimental group. The results indicate that specification effectiveness of the traits experiment group shows a better performance compared to the interfaces experiment group, but specification efficiency shows no statistically significant differences. To the best of our knowledge, this is the first empirical study studying the specification effectiveness and efficiency of object-oriented abstractions in the context of formal methods.",
        "link": "https://dl.acm.org/doi/10.1145/3450968",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Connection Pruning for Deep Spiking Neural Networks with On-Chip Learning",
        "authors": "['Thao N. N. Nguyen', 'Bharadwaj Veeravalli', 'Xuanyao Fong']",
        "date": "July 2021",
        "source": "ICONS 2021: International Conference on Neuromorphic Systems 2021",
        "abstract": "Long training time hinders the potential of the deep, large-scale Spiking Neural Network (SNN) with the on-chip learning capability to be realized on the embedded systems hardware. Our work proposes a novel connection pruning approach that can be applied during the on-chip Spike Timing Dependent Plasticity (STDP)-based learning to optimize the learning time and the network connectivity of the deep SNN. We applied our approach to a deep SNN with the Time To First Spike (TTFS) coding and has successfully achieved 2.1x speed-up and 64% energy savings in the on-chip learning and reduced the network connectivity by 92.83%, without incurring any accuracy loss. Moreover, the connectivity reduction results in 2.83x speed-up and 78.24% energy savings in the inference. Evaluation of our proposed approach on the Field Programmable Gate Array (FPGA) platform revealed 0.56% power overhead was needed to implement the pruning algorithm.",
        "link": "https://dl.acm.org/doi/10.1145/3477145.3477157",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "I see dead μops: leaking secrets via Intel/AMD micro-op caches",
        "authors": "['Xida Ren', 'Logan Moody', 'Mohammadkazem Taram', 'Matthew Jordan', 'Dean M. Tullsen', 'Ashish Venkat']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "Modern Intel, AMD, and ARM processors translate complex instructions into simpler internal micro-ops that are then cached in a dedicated on-chip structure called the micro-op cache. This work presents an in-depth characterization study of the micro-op cache, reverse-engineering many undocumented features, and further describes attacks that exploit the micro-op cache as a timing channel to transmit secret information. In particular, this paper describes three attacks - (1) a same thread cross-domain attack that leaks secrets across the user-kernel boundary, (2) a cross-SMT thread attack that transmits secrets across two SMT threads via the micro-op cache, and (3) transient execution attacks that have the ability to leak an unauthorized secret accessed along a misspeculated path, even before the transient instruction is dispatched to execution, breaking several existing invisible speculation and fencing-based solutions that mitigate Spectre.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00036",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "xDNN: Inference for Deep Convolutional Neural Networks",
        "authors": "[\"Paolo D'Alberto\", 'Victor Wu', 'Aaron Ng', 'Rahul Nimaiyar', 'Elliott Delaye', 'Ashish Sirasao']",
        "date": "None",
        "source": "ACM Transactions on Reconfigurable Technology and Systems",
        "abstract": "We present xDNN, an end-to-end system for deep-learning inference based on a family of specialized hardware processors synthesized on Field-Programmable Gate Array (FPGAs) and Convolution Neural Networks (CNN). We present a design optimized for low latency, high throughput, and high compute efficiency with no batching. The design is scalable and a parametric function of the number of multiply-accumulate units, on-chip memory hierarchy, and numerical precision. The design can produce a scale-down processor for embedded devices, replicated to produce more cores for larger devices, or resized to optimize efficiency. On Xilinx Virtex Ultrascale+ VU13P FPGA, we achieve 800 MHz that is close to the Digital Signal Processing maximum frequency and above 80% efficiency of on-chip compute resources.On top of our processor family, we present a runtime system enabling the execution of different networks for different input sizes (i.e., from 224× 224 to 2048× 1024). We present a compiler that reads CNNs from native frameworks (i.e., MXNet, Caffe, Keras, and Tensorflow), optimizes them, generates codes, and provides performance estimates. The compiler combines quantization information from the native environment and optimizations to feed the runtime with code as efficient as any hardware expert could write. We present tools partitioning a CNN into subgraphs for the division of work to CPU cores and FPGAs. Notice that the software will not change when or if the FPGA design becomes an ASIC, making our work vertical and not just a proof-of-concept FPGA project.We show experimental results for accuracy, latency, and power for several networks: In summary, we can achieve up to 4 times higher throughput, 3 times better power efficiency than the GPUs, and up to 20 times higher throughput than the latest CPUs. To our knowledge, we provide solutions faster than any previous FPGA-based solutions and comparable to any other top-of-the-shelves solutions.",
        "link": "https://dl.acm.org/doi/10.1145/3473334",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "DFI: The Data Flow Interface for High-Speed Networks",
        "authors": "['Lasse Thostrup', 'Jan Skrzypczak', 'Matthias Jasny', 'Tobias Ziegler', 'Carsten Binnig']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "In this paper, we propose the Data Flow Interface (DFI) as a way to make it easier for data processing systems to exploit high-speed networks without the need to deal with the complexity of RDMA. By lifting the level of abstraction, DFI factors out much of the complexity of network communication and makes it easier for developers to declaratively express how data should be efficiently routed to accomplish a given distributed data processing task. As we show in our experiments, DFI is able to support a wide variety of data-centric applications with high performance at a low complexity for the applications.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3452816",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "First-person Cinematographic Videogames: Game Model, Authoring Environment, and Potential for Creating Affection for Places",
        "authors": "['Ivan BlečIć', 'Sara Cuccu', 'Filippo Andrea Fanni', 'Vittoria Frau', 'Riccardo Macis', 'Valeria Saiu', 'Martina Senis', 'Lucio Davide Spano', 'Alessandro Tola']",
        "date": "None",
        "source": "Journal on Computing and Cultural Heritage ",
        "abstract": "We present and explore the fruitfulness of “first-person cinematographic videogames,” a game model we have devised for the promotion of cultural, environmental, and territorial heritage. To support and foster the development of these type of games, we have developed a Web-based user-friendly authoring environment, extensively presented in the article. While employing standard first-person point-and-click game mechanics, the game model's distinctive feature is that the game environment is not based on a digital reconstruction (3D model) of the real-world settings but on cinematographic techniques combining videos and photos of existing places, integrating videoclips of mostly practical effects to obtain the interactivity typical of the first-person point-and-click adventure games. Our goal with such a game model is to mobilise mechanisms of engendering affection for real-world places when they become settings of the game world, arousing in the player forms of affection, attachment, and desire to visit them.",
        "link": "https://dl.acm.org/doi/10.1145/3446977",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Temporal and SFQ pulse-streams encoding for area-efficient superconducting accelerators",
        "authors": "['Patricia Gonzalez-Guerrero', 'Meriam Gay Bautista', 'Darren Lyles', 'George Michelogiannakis']",
        "date": "February 2022",
        "source": "ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "Superconducting technology is a prime candidate for the future of computing. However, current superconducting prototypes are limited to small-scale examples due to stringent area constraints and complex architectures inspired from voltage-level encoding in CMOS; this is at odds with the ps-wide Single Quantum Flux (SFQ) pulses used in superconductors to carry information. In this work, we propose a wave-pipelined Unary SFQ (U-SFQ) architecture that leverages the advantages of two data representations: pulse-streams and Race Logic (RL). We introduce novel building blocks such as multipliers, adders, and memory cells, which leverage the natural properties of SFQ pulses to mitigate area constraints. We then design and simulate three popular hardware accelerators: i) a Processing Element (PE), typically used in spatial architectures; ii) A dot-product-unit (DPU), one of the most popular accelerators in artificial neural networks and digital signal processing (DSP); and iii) A Finite Impulse Response (FIR) filter, a popular and computationally demanding DSP accelerator. The proposed U-SFQ building blocks require up to 200× fewer JJs compared to their SFQ binary counterparts, exposing an area-delay trade-off. This work mitigates the stringent area constraints of superconducting technology.",
        "link": "https://dl.acm.org/doi/10.1145/3503222.3507765",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "CoSA: scheduling by <u>c</u>onstrained <u>o</u>ptimization for <u>s</u>patial <u>a</u>ccelerators",
        "authors": "['Qijing Huang', 'Minwoo Kang', 'Grace Dinh', 'Thomas Norell', 'Aravind Kalaiah', 'James Demmel', 'John Wawrzynek', 'Yakun Sophia Shao']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "Recent advances in Deep Neural Networks (DNNs) have led to active development of specialized DNN accelerators, many of which feature a large number of processing elements laid out spatially, together with a multi-level memory hierarchy and flexible interconnect. While DNN accelerators can take advantage of data reuse and achieve high peak throughput, they also expose a large number of runtime parameters to the programmers who need to explicitly manage how computation is scheduled both spatially and temporally. In fact, different scheduling choices can lead to wide variations in performance and efficiency, motivating the need for a fast and efficient search strategy to navigate the vast scheduling space. To address this challenge, we present CoSA, a constrained-optimization-based approach for scheduling DNN accelerators. As opposed to existing approaches that either rely on designers' heuristics or iterative methods to navigate the search space, CoSA expresses scheduling decisions as a constrained-optimization problem that can be deterministically solved using mathematical optimization techniques. Specifically, CoSA leverages the regularities in DNN operators and hardware to formulate the DNN scheduling space into a mixed-integer programming (MIP) problem with algorithmic and architectural constraints, which can be solved to automatically generate a highly efficient schedule in one shot. We demonstrate that CoSA-generated schedules significantly outperform state-of-the-art approaches by a geometric mean of up to 2.5X across a wide range of DNN networks while improving the time-to-solution by 90X.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00050",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Xar-trek: run-time execution migration among FPGAs and heterogeneous-ISA CPUs",
        "authors": "['Edson Horta', 'Ho-Ren Chuang', 'Naarayanan Rao VSathish', 'Cesar Philippidis', 'Antonio Barbalace', 'Pierre Olivier', 'Binoy Ravindran']",
        "date": "December 2021",
        "source": "Middleware '21: Proceedings of the 22nd International Middleware Conference",
        "abstract": "Datacenter servers are increasingly heterogeneous: from x86 host CPUs, to ARM or RISC-V CPUs in NICs/SSDs, to FPGAs. Previous works have demonstrated that migrating application execution at run-time across heterogeneous-ISA CPUs can yield significant performance and energy gains, with relatively little programmer effort. However, FPGAs have often been overlooked in that context: hardware acceleration using FPGAs involves statically implementing select application functions, which prohibits dynamic and transparent migration. We present Xar-Trek, a new compiler and run-time software framework that overcomes this limitation. Xar-Trek compiles an application for several CPU ISAs and select application functions for acceleration on an FPGA, allowing execution migration between heterogeneous-ISA CPUs and FPGAs at run-time. Xar-Trek's run-time monitors server workloads and migrates application functions to an FPGA or to heterogeneous-ISA CPUs based on a scheduling policy. We develop a heuristic policy that uses application workload profiles to make scheduling decisions. Our evaluations conducted on a system with x86-64 server CPUs, ARM64 server CPUs, and an Alveo accelerator card reveal 88%-l% performance gains over no-migration baselines.",
        "link": "https://dl.acm.org/doi/10.1145/3464298.3493388",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "CryoGuard: a near refresh-free robust DRAM design for cryogenic computing",
        "authors": "['Gyu-Hyeon Lee', 'Seongmin Na', 'Ilkwon Byun', 'Dongmoon Min', 'Jangwoo Kim']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "Cryogenic computing, which runs a computer device at an extremely low temperature, is highly promising thanks to the significant reduction of the wire latency and leakage current. A recently proposed cryogenic DRAM design achieved the promising performance improvement, but it also reveals that it must reduce the DRAM's dynamic power to overcome the huge cooling cost at 77 K. Therefore, researchers now target to reduce the cryogenic DRAM's refresh power by utilizing its significantly increased retention time driven by the reduced leakage current. To achieve the goal, however, architects should first answer many fundamental questions regarding the reliability and then design a refresh-free, but still robust cryogenic DRAM by utilizing the analysis result. In this work, we propose a near refresh-free, but robust cryogenic DRAM (NRFC-DRAM), which can almost eliminate its refresh overhead while ensuring reliable operations at 77 K. For the purpose, we first evaluate various DRAM samples of multiple vendors by conducting a thorough analysis to accurately estimate the cryogenic DRAM's retention time and reliability. Our analysis identifies a new critical challenge such that reducing DRAM's refresh rate can make the memory highly unreliable because normal memory operations can now appear as row-hammer attacks at 77 K. Therefore, NRFC-DRAM requires a cost-effective, cryogenic-friendly protection mechanism against the new row-hammer-like \"faults\" at 77 K. To resolve the challenge, we present CryoGuard, our cryogenic-friendly row-hammer protection method to ensure the NRFC-DRAM's reliable operations at 77 K. With CryoGuard applied, NRFC-DRAM reduces the overall power consumption by 25.9% even with its cooling cost included, whereas the existing cryogenic DRAM fails to reduce the power consumption.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00056",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "SnapperGPS: Algorithms for Energy-Efficient Low-Cost Location Estimation Using GNSS Signal Snapshots",
        "authors": "['Jonas Beuchert', 'Alex Rogers']",
        "date": "November 2021",
        "source": "SenSys '21: Proceedings of the 19th ACM Conference on Embedded Networked Sensor Systems",
        "abstract": "Snapshot GNSS is a more energy-efficient approach to location estimation than traditional GNSS positioning methods. This is beneficial for applications with long deployments on battery such as wildlife tracking. However, only a few snapshot GNSS implementations have been presented so far and all have disadvantages. Most significantly, they typically require the GNSS signals to be captured with a certain minimum resolution, which demands complex receiver hardware capable of capturing multi-bit data at sampling rates of 16 MHz and more. By contrast, we develop fast algorithms that reliably estimate locations from twelve-millisecond signals that are sampled at just 4 MHz and quantised with only a single bit per sample. This allows us to build a snapshot receiver at an unmatched low cost of $14, which can acquire one position per hour for a year. On a challenging public dataset with thousands of snapshots from real-world scenarios, our system achieves 97% reliability and 11 m median accuracy, comparable to existing solutions with more complex and expensive hardware and higher energy consumption. We provide an open implementation of the algorithms as well as a public web service for cloud-based location estimation from low-quality GNSS signal snapshots.",
        "link": "https://dl.acm.org/doi/10.1145/3485730.3485931",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "PacketMill: toward per-Core 100-Gbps networking",
        "authors": "['Alireza Farshin', 'Tom Barbette', 'Amir Roozbeh', 'Gerald Q. Maguire Jr.', 'Dejan Kostić']",
        "date": "April 2021",
        "source": "ASPLOS '21: Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "We present PacketMill, a system for optimizing software packet processing, which (i) introduces a new model to efficiently manage packet metadata and (ii) employs code-optimization techniques to better utilize commodity hardware. PacketMill grinds the whole packet processing stack, from the high-level network function configuration file to the low-level userspace network (specifically DPDK) drivers, to mitigate inefficiencies and produce a customized binary for a given network function. Our evaluation results show that PacketMill increases throughput (up to 36.4 Gbps -- 70%) & reduces latency (up to 101 us -- 28%) and enables nontrivial packet processing (e.g., router) at ~100 Gbps, when new packets arrive >10× faster than main memory access times, while using only one processing core.",
        "link": "https://dl.acm.org/doi/10.1145/3445814.3446724",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "CODIC: a low-cost substrate for enabling custom in-DRAM functionalities and optimizations",
        "authors": "['Lois Orosa', 'Yaohua Wang', 'Mohammad Sadrosadati', 'Jeremie S. Kim', 'Minesh Patel', 'Ivan Puddu', 'Haocong Luo', 'Kaveh Razavi', 'Juan Gómez-Luna', 'Hasan Hassan', 'Nika Mansouri-Ghiasi', 'Saugata Ghose', 'Onur Mutlu']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "DRAM is the dominant main memory technology used in modern computing systems. Computing systems implement a memory controller that interfaces with DRAM via DRAM commands. DRAM executes the given commands using internal components (e.g., access transistors, sense amplifiers) that are orchestrated by DRAM internal timings, which are fixed for each DRAM command. Unfortunately, the use of fixed internal timings limits the types of operations that DRAM can perform and hinders the implementation of new functionalities and custom mechanisms that improve DRAM reliability, performance and energy. To overcome these limitations, we propose enabling programmable DRAM internal timings for controlling in-DRAM components. To this end, we design CODIC, a new low-cost DRAM substrate that enables fine-grained control over four previously fixed internal DRAM timings that are key to many DRAM operations. We implement CODIC with only minimal changes to the DRAM chip and the DDRx interface. To demonstrate the potential of CODIC, we propose two new CODIC-based security mechanisms that outperform state-of-the-art mechanisms in several ways: (1) a new DRAM Physical Unclonable Function (PUF) that is more robust and has significantly higher throughput than state-of-the-art DRAM PUFs, and (2) the first cold boot attack prevention mechanism that does not introduce any performance or energy overheads at runtime.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00045",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Building an Internet Router with P4Pi",
        "authors": "['Radostin Stoyanov', 'Adam Wolnikowski', 'Robert Soulé', 'Sándor Laki', 'Noa Zilberman']",
        "date": "December 2021",
        "source": "ANCS '21: Proceedings of the Symposium on Architectures for Networking and Communications Systems",
        "abstract": "Building an Internet Router is a popular, hands-on project used to teach computer networks. However, there is currently no hardware target that allows students to develop the project in P4 without incurring significant cost or encountering FPGA knowledge barriers. This paper presents P4Pi as a target for the Building an Internet Router project. P4Pi is a platform for developing, testing, and evaluating P4 programs on a Raspberry Pi device. We describe the architecture of the router project on P4Pi, and discuss the practical aspects of running it as a class project. The P4Pi-based router project is low-cost and easy to adopt, enabling students to focus on their P4 programming skills and to evaluate their designs on a physical target through interoperability tests with their colleagues.",
        "link": "https://dl.acm.org/doi/10.1145/3493425.3502762",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "NASPipe: high performance and reproducible pipeline parallel supernet training via causal synchronous parallelism",
        "authors": "['Shixiong Zhao', 'Fanxin Li', 'Xusheng Chen', 'Tianxiang Shen', 'Li Chen', 'Sen Wang', 'Nicholas Zhang', 'Cheng Li', 'Heming Cui']",
        "date": "February 2022",
        "source": "ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "Supernet training, a prevalent and important paradigm in Neural Architecture Search, embeds the whole DNN architecture search space into one monolithic supernet, iteratively activates a subset of the supernet (i.e., a subnet) for fitting each batch of data, and searches a high-quality subnet which meets specific requirements. Although training subnets in parallel on multiple GPUs is desirable for acceleration, there inherently exists a race hazard that concurrent subnets may access the same DNN layers. Existing systems support neither efficiently parallelizing subnets’ training executions, nor resolving the race hazard deterministically, leading to unreproducible training procedures and potentiallly non-trivial accuracy loss.   We present NASPipe, the first high-performance and reproducible distributed supernet training system via causal synchronous parallel (CSP) pipeline scheduling abstraction: NASPipe partitions a supernet across GPUs and concurrently executes multiple generated sub-tasks (subnets) in a pipelined manner; meanwhile, it oversees the correlations between the subnets and deterministically resolves any causal dependency caused by subnets’ layer sharing. To obtain high performance, NASPipe’s CSP scheduler exploits the fact that the larger a supernet spans, the fewer dependencies manifest between chronologically close subnets; therefore, it aggressively schedules the subnets with larger chronological orders into execution, only if they are not causally dependent on unfinished precedent subnets. Moreover, to relieve the excessive GPU memory burden for holding the whole supernet’s parameters, NASPipe uses a context switch technique that stashes the whole supernet in CPU memory, precisely predicts the subnets’ schedule, and pre-fetches/evicts a subnet before/after its execution. The evaluation shows that NASPipe is the only system that retains supernet training reproducibility, while achieving a comparable and even higher performance (up to 7.8X) compared to three recent pipeline training systems (e.g., GPipe).",
        "link": "https://dl.acm.org/doi/10.1145/3503222.3507735",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Understanding and improving model-driven IoT systems through accompanying digital twins",
        "authors": "['Jörg Christian Kirchhof', 'Lukas Malcher', 'Bernhard Rumpe']",
        "date": "October 2021",
        "source": "GPCE 2021: Proceedings of the 20th ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences",
        "abstract": "Developers questioning why their system behaves differently than expected often have to rely on time-consuming and error-prone manual analysis of log files. Understanding the behavior of Internet of Things (IoT) applications is a challenging task because they are not only inherently hard-to-trace distributed systems, but their integration with the environment via sensors adds another layer of complexity. Related work proposes to record data during the execution of the system, which can later be replayed to analyze the system. We apply the model-driven development approach to this idea and leverage digital twins to collect the required data. We enable developers to replay and analyze the system’s executions by applying model-to-model transformations. These transformations instrument component and connector (C&C) architecture models with components that reproduce the system’s environment based on the data recorded by the system’s digital twin. We validate and evaluate the feasibility of our approach using a heating, ventilation, and air conditioning (HVAC) case study. By facilitating the reproduction of the system’s behavior, our method lowers the barrier to understanding the behavior of model-driven IoT systems.",
        "link": "https://dl.acm.org/doi/10.1145/3486609.3487210",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "NASPipe: high performance and reproducible pipeline parallel supernet training via causal synchronous parallelism",
        "authors": "['Shixiong Zhao', 'Fanxin Li', 'Xusheng Chen', 'Tianxiang Shen', 'Li Chen', 'Sen Wang', 'Nicholas Zhang', 'Cheng Li', 'Heming Cui']",
        "date": "February 2022",
        "source": "ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "Supernet training, a prevalent and important paradigm in Neural Architecture Search, embeds the whole DNN architecture search space into one monolithic supernet, iteratively activates a subset of the supernet (i.e., a subnet) for fitting each batch of data, and searches a high-quality subnet which meets specific requirements. Although training subnets in parallel on multiple GPUs is desirable for acceleration, there inherently exists a race hazard that concurrent subnets may access the same DNN layers. Existing systems support neither efficiently parallelizing subnets’ training executions, nor resolving the race hazard deterministically, leading to unreproducible training procedures and potentiallly non-trivial accuracy loss.   We present NASPipe, the first high-performance and reproducible distributed supernet training system via causal synchronous parallel (CSP) pipeline scheduling abstraction: NASPipe partitions a supernet across GPUs and concurrently executes multiple generated sub-tasks (subnets) in a pipelined manner; meanwhile, it oversees the correlations between the subnets and deterministically resolves any causal dependency caused by subnets’ layer sharing. To obtain high performance, NASPipe’s CSP scheduler exploits the fact that the larger a supernet spans, the fewer dependencies manifest between chronologically close subnets; therefore, it aggressively schedules the subnets with larger chronological orders into execution, only if they are not causally dependent on unfinished precedent subnets. Moreover, to relieve the excessive GPU memory burden for holding the whole supernet’s parameters, NASPipe uses a context switch technique that stashes the whole supernet in CPU memory, precisely predicts the subnets’ schedule, and pre-fetches/evicts a subnet before/after its execution. The evaluation shows that NASPipe is the only system that retains supernet training reproducibility, while achieving a comparable and even higher performance (up to 7.8X) compared to three recent pipeline training systems (e.g., GPipe).",
        "link": "https://dl.acm.org/doi/10.1145/3503222.3507735",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "MemOReL: A <u>Mem</u>ory-oriented <u>O</u>ptimization Approach to <u>Re</u>inforcement <u>L</u>earning on FPGA-based Embedded Systems",
        "authors": "['Siva Satyendra Sahoo', 'Akhil Raj Baranwal', 'Salim Ullah', 'Akash Kumar']",
        "date": "June 2021",
        "source": "GLSVLSI '21: Proceedings of the 2021 on Great Lakes Symposium on VLSI",
        "abstract": "Reinforcement Learning (RL) represents the machine learning method that has come closest to showing human-like learning. While Deep RL is becoming increasingly popular for complex applications such as AI-based gaming, it has a high implementation cost in terms of both power and latency. Q-Learning, on the other hand, is a much simpler method that makes it more feasible for implementation on resource-constrained embedded systems for control and navigation. However, the optimal policy search in Q-Learning is a compute-intensive and inherently sequential process and a software-only implementation may not be able to satisfy the latency and throughput constraints of such applications. To this end, we propose a novel accelerator design with multiple design trade-offs for implementing Q-Learning on FPGA-based SoCs. Specifically, we analyze the various stages of the Epsilon-Greedy algorithm for RL and propose a novel microarchitecture that reduces the latency by optimizing the memory access during each iteration. Consequently, we present multiple designs that provide varying trade-offs between performance, power dissipation, and resource utilization of the accelerator. With the proposed approach, we report considerable improvement in throughput with lower resource utilization over state-of-the-art design implementations.",
        "link": "https://dl.acm.org/doi/10.1145/3453688.3461533",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Coalescent computing",
        "authors": "['Kyle Hale']",
        "date": "August 2021",
        "source": "APSys '21: Proceedings of the 12th ACM SIGOPS Asia-Pacific Workshop on Systems",
        "abstract": "As computational infrastructure extends to the edge, it will increasingly offer the same fine-grained resource provisioning mechanisms used in large-scale cloud datacenters, and advances in low-latency, wireless networking technology will allow service providers to blur the distinction between local and remote resources for commodity computing. From the users' perspectives, their devices will no longer have fixed computational power, but rather will appear to have flexible computational capabilities that vary subject to the shared, disaggregated edge resources available in their physical proximity. System software will transparently leverage these ephemeral resources to provide a better end-user experience. We discuss key systems challenges to enabling such tightly-coupled, disaggregated, and ephemeral infrastructure provisioning, advocate for more research in the area, and outline possible paths forward.",
        "link": "https://dl.acm.org/doi/10.1145/3476886.3477503",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Accuracy and Resiliency of Analog Compute-in-Memory Inference Engines",
        "authors": "['Zhe Wan', 'Tianyi Wang', 'Yiming Zhou', 'Subramanian S. Iyer', 'Vwani P. Roychowdhury']",
        "date": "None",
        "source": "ACM Journal on Emerging Technologies in Computing Systems",
        "abstract": "Recently, analog compute-in-memory (CIM) architectures based on emerging analog non-volatile memory (NVM) technologies have been explored for deep neural networks (DNNs) to improve scalability, speed, and energy efficiency. Such architectures, however, leverage charge conservation, an operation with infinite resolution, and thus are susceptible to errors. Thus, the inherent stochasticity in any analog NVM used to execute DNNs, will compromise performance. Several reports have demonstrated the use of analog NVM for CIM in a limited scale. It is unclear whether the uncertainties in computations will prohibit large-scale DNNs. To explore this critical issue of scalability, this article first presents a simulation framework to evaluate the feasibility of large-scale DNNs based on CIM architecture and analog NVM. Simulation results show that DNNs trained for high-precision digital computing engines are not resilient against the uncertainty of the analog NVM devices. To avoid such catastrophic failures, this article introduces the analog bi-scale representation for the DNN, and the Hessian-aware Stochastic Gradient Descent training algorithm to enhance the inference accuracy of trained DNNs. As a result of such enhancements, DNNs such as Wide ResNets for CIFAR-100 image recognition problem are demonstrated to have significant performance improvements in accuracy without adding cost to the inference hardware.",
        "link": "https://dl.acm.org/doi/10.1145/3502721",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Efficient LLVM-based dynamic binary translation",
        "authors": "['Alexis Engelke', 'Dominik Okwieka', 'Martin Schulz']",
        "date": "April 2021",
        "source": "VEE 2021: Proceedings of the 17th ACM SIGPLAN/SIGOPS International Conference on Virtual Execution Environments",
        "abstract": "Emulation of other or newer processor architectures is necessary for a wide variety of use cases, from ensuring compatibility to offering a vehicle for computer architecture research. This problem is usually approached using dynamic binary translation, where machine code is translated, on the fly, to the host architecture during program execution. Existing systems, like QEMU, usually focus on translation performance rather than the overall program execution, and extensions, like HQEMU, are limited by their underlying implementation. Conversely, performance-focused systems are typically used for binary instrumentation. E.g., DynamoRIO reuses original instructions where possible, while Instrew utilizes the LLVM compiler infrastructure, but only supports same-architecture code generation.   In this short paper, we generalize Instrew to support different guest and host architectures by refactoring the lifter and by implementing target-independent optimizations to re-use host hardware features for emulated code. We demonstrate this flexibility by adding support for RISC-V as guest architecture and AArch64 as host architecture. Our performance results on SPEC CPU2017 show significant improvements compared to QEMU, HQEMU as well as the original Instrew.",
        "link": "https://dl.acm.org/doi/10.1145/3453933.3454022",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "PERI: A Configurable Posit Enabled RISC-V Core",
        "authors": "['Sugandha Tiwari', 'Neel Gala', 'Chester Rebeiro', 'V. Kamakoti']",
        "date": "None",
        "source": "ACM Transactions on Architecture and Code Optimization",
        "abstract": "Owing to the failure of Dennard’s scaling, the past decade has seen a steep growth of prominent new paradigms leveraging opportunities in computer architecture. Two technologies of interest are Posit and RISC-V. Posit was introduced in mid-2017 as a viable alternative to IEEE-754, and RISC-V provides a commercial-grade open source Instruction Set Architecture (ISA). In this article, we bring these two technologies together and propose a Configurable Posit Enabled RISC-V Core called PERI.The article provides insights on how the Single-Precision Floating Point (“F”) extension of RISC-V can be leveraged to support posit arithmetic. We also present the implementation details of a parameterized and feature-complete posit Floating Point Unit (FPU). The configurability and the parameterization features of this unit generate optimal hardware, which caters to the accuracy and energy/area tradeoffs imposed by the applications, a feature not possible with IEEE-754 implementation. The posit FPU has been integrated with the RISC-V compliant SHAKTI C-class core as an execution unit. To further leverage the potential of posit, we enhance our posit FPU to support two different exponent sizes (with posit-size being 32-bits), thereby enabling multiple-precision at runtime. To enable the compilation and execution of C programs on PERI, we have made minimal modifications to the GNU C Compiler (GCC), targeting the “F” extension of the RISC-V. We compare posit with IEEE-754 in terms of hardware area, application accuracy, and runtime. We also present an alternate methodology of integrating the posit FPU with the RISC-V core as an accelerator using the custom opcode space of RISC-V.",
        "link": "https://dl.acm.org/doi/10.1145/3446210",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Client-optimized algorithms and acceleration for encrypted compute offloading",
        "authors": "['McKenzie van der Hagen', 'Brandon Lucia']",
        "date": "February 2022",
        "source": "ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "Homomorphic Encryption (HE) enables secure cloud offload processing on encrypted data. HE schemes are limited in the complexity and type of operations they can perform, motivating client-aided implementations that distribute computation between client (unencrypted) and server (encrypted). Prior client-aided systems optimize server performance, ignoring client costs: client-aided models put encryption and decryption on the critical path and require communicating large ciphertexts. We introduce Client-aided HE for Opaque Compute Offloading (CHOCO), a client-optimized system for encrypted offload processing. CHOCO reduces ciphertext size, reducing communication and computing costs through HE parameter minimization and through “rotational redundancy”, a new HE algorithm optimization. We present Client-aided HE for Opaque Compute Offloading Through Accelerated Cryptographic Operations (CHOCO-TACO), an accelerator for HE encryption and decryption, making client-aided HE feasible for even resource-constrained clients. CHOCO supports two popular HE schemes (BFV and CKKS) and several applications, including DNNs, PageRank, KNN, and K-Means. CHOCO reduces communication by up to 2948× over prior work. With CHOCO-TACO client enc-/decryption is up to 1094× faster and uses up to 648× less energy.",
        "link": "https://dl.acm.org/doi/10.1145/3503222.3507737",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Efficient multi-GPU shared memory via automatic optimization of fine-grained transfers",
        "authors": "['Harini Muthukrishnan', 'David Nellans', 'Daniel Lustig', 'Jeffrey A. Fessler', 'Thomas F. Wenisch']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "Despite continuing research into inter-GPU communication mechanisms, extracting performance from multi-GPU systems remains a significant challenge. Inter-GPU communication via bulk DMA-based transfers exposes data transfer latency on the GPU's critical execution path because these large transfers are logically interleaved between compute kernels. Conversely, fine-grained peer-to-peer memory accesses during kernel execution lead to memory stalls that can exceed the GPUs' ability to cover these operations via multi-threading. Worse yet, these sub-cacheline transfers are highly inefficient on current inter-GPU interconnects. To remedy these issues, we propose PROACT, a system enabling remote memory transfers with the programmability and pipeline advantages of peer-to-peer stores, while achieving interconnect efficiency that rivals bulk DMA transfers. Combining compile-time instrumentation with fine-grain tracking of data block readiness within each GPU, PROACT enables interconnect-friendly data transfers while hiding the transfer latency via pipelining during kernel execution. This work describes both hardware and software implementations of PROACT and demonstrates the effectiveness of a PROACT software prototype on three generations of GPU hardware and interconnects. Achieving near-ideal interconnect efficiency, PROACT realizes a mean speedup of 3.0X over single-GPU performance for 4-GPU systems, capturing 83% of available performance opportunity. On a 16-GPU NVIDIA DGX-2 system, we demonstrate an 11.0X average strong-scaling speedup over single-GPU performance, 5.3X better than a bulk DMA-based approach.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00020",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Reproducing Spectre Attack with gem5: How To Do It Right?",
        "authors": "['Pierre Ayoub', 'Clémentine Maurice']",
        "date": "April 2021",
        "source": "EuroSec '21: Proceedings of the 14th European Workshop on Systems Security",
        "abstract": "As processors become more and more complex due to performance optimizations and energy savings, new attack surfaces raise. We know that the micro-architecture of a processor leaks some information into the architectural domain. Moreover, some mechanisms like speculative execution can be exploited to execute malicious instructions. As a consequence, it allows a process to spy another process or to steal data. These attacks are consequences of fundamental design issues, thus they are complicated to fix and reproduce. Simulation would be of a great help for scientific research for microarchitectural security, but it also leads to new challenges. We try to address the first challenges to demonstrate that simulation could be useful in research and an interesting technique to develop in the future.",
        "link": "https://dl.acm.org/doi/10.1145/3447852.3458715",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "MC-DeF: Creating Customized CGRAs for Dataflow Applications",
        "authors": "['George Charitopoulos', 'Dionisios N. Pnevmatikatos', 'Georgi Gaydadjiev']",
        "date": "None",
        "source": "ACM Transactions on Architecture and Code Optimization",
        "abstract": "Executing complex scientific applications on Coarse-Grain Reconfigurable Arrays (CGRAs) promises improvements in execution time and/or energy consumption compared to optimized software implementations or even fully customized hardware solutions. Typical CGRA architectures contain of multiple instances of the same compute module that consist of simple and general hardware units such as ALUs, simple processors. However, generality in the cell contents, while convenient for serving a wide variety of applications, penalizes performance and energy efficiency. To that end, a few proposed CGRAs use custom logic tailored to a particular application’s specific characteristics in the compute module. This approach, while much more efficient, restricts the versatility of the array. To date, versatility at hardware speeds is only supported with Field programmable gate arrays (FPGAs), that are reconfigurable at a very fine grain.This work proposes MC-DeF, a novel Mixed-CGRA Definition Framework targeting a Mixed-CGRA architecture that leverages the advantages of CGRAs by utilizing a customized cell array, and those of FPGAs by incorporating a separate LUT array used for adaptability. The framework presented aims to develop a complete CGRA architecture. First, a cell structure and functionality definition phase creates highly customized application/domain specific CGRA cells. Then, mapping and routing phases define the CGRA connectivity and cell-LUT array transactions. Finally, an energy and area estimation phase presents the user with area occupancy and energy consumption estimations of the final design. MC-DeF uses novel algorithms and cost functions driven by user defined metrics, threshold values, and area/energy restrictions. The benefits of our framework, besides creating fast and efficient CGRA designs, include design space exploration capabilities offered to the user.The validity of the presented framework is demonstrated by evaluating and creating CGRA designs of nine applications. Additionally, we provide comparisons of MC-DeF with state-of-the-art related works, and show that MC-DeF offers competitive performance (in terms of internal bandwidth and processing throughput) even compared against much larger designs, and requires fewer physical resources to achieve this level of performance. Finally, MC-DeF is able to better utilize the underlying FPGA fabric and achieves the best efficiency (measured in LUT/GOPs).",
        "link": "https://dl.acm.org/doi/10.1145/3447970",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Improving Performance-Power-Programmability in Space Avionics with Edge Devices: VBN on Myriad2 SoC",
        "authors": "['Vasileios Leon', 'George Lentaris', 'Evangelos Petrongonas', 'Dimitrios Soudris', 'Gianluca Furano', 'Antonis Tavoularis', 'David Moloney']",
        "date": "None",
        "source": "ACM Transactions on Embedded Computing Systems",
        "abstract": "The advent of powerful edge devices and AI algorithms has already revolutionized many terrestrial applications; however, for both technical and historical reasons, the space industry is still striving to adopt these key enabling technologies in new mission concepts. In this context, the current work evaluates an heterogeneous multi-core system-on-chip processor for use on-board future spacecraft to support novel, computationally demanding digital signal processors and AI functionalities. Given the importance of low power consumption in satellites, we consider the Intel Movidius Myriad2 system-on-chip and focus on SW development and performance aspects. We design a methodology and framework to accommodate efficient partitioning, mapping, parallelization, code optimization, and tuning of complex algorithms. Furthermore, we propose an avionics architecture combining this commercial off-the-shelf chip with a field programmable gate array device to facilitate, among others, interfacing with traditional space instruments via SpaceWire transcoding. We prototype our architecture in the lab targeting vision-based navigation tasks. We implement a representative computer vision pipeline to track the 6D pose of ENVISAT using megapixel images during hypothetical spacecraft proximity operations. Overall, we achieve 2.6 to 4.9 FPS with only 0.8 to 1.1 W on Myriad2, i.e., 10-fold acceleration versus modern rad-hard processors. Based on the results, we assess various benefits of utilizing Myriad2 instead of conventional field programmable gate arrays and CPUs.",
        "link": "https://dl.acm.org/doi/10.1145/3440885",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "NxTF: An API and Compiler for Deep Spiking Neural Networks on Intel Loihi",
        "authors": "['Bodo Rueckauer', 'Connor Bybee', 'Ralf Goettsche', 'Yashwardhan Singh', 'Joyesh Mishra', 'Andreas Wild']",
        "date": "None",
        "source": "ACM Journal on Emerging Technologies in Computing Systems",
        "abstract": "Spiking Neural Networks (SNNs) is a promising paradigm for efficient event-driven processing of spatio-temporally sparse data streams. Spiking Neural Networks (SNNs) have inspired the design of and can take advantage of the emerging class of neuromorphic processors like Intel Loihi. These novel hardware architectures expose a variety of constraints that affect firmware, compiler, and algorithm development alike. To enable rapid and flexible development of SNN algorithms on Loihi, we developed NxTF: a programming interface derived from Keras and compiler optimized for mapping deep convolutional SNNs to the multi-core Intel Loihi architecture. We evaluate NxTF on Deep Neural Networks (DNNs) trained directly on spikes as well as models converted from traditional DNNs, processing both sparse event-based and dense frame-based datasets. Further, we assess the effectiveness of the compiler to distribute models across a large number of cores and to compress models by exploiting Loihi’s weight-sharing features. Finally, we evaluate model accuracy, energy, and time-to-solution compared to other architectures. The compiler achieves near-optimal resource utilization of 80% across 16 Loihi chips for a 28-layer, 4M parameter MobileNet model with input size 128×128. In addition, we report the lowest error rate of 8.52% for the CIFAR-10 dataset on neuromorphic hardware, using an off-the-shelf MobileNet.",
        "link": "https://dl.acm.org/doi/10.1145/3501770",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Experimental Analysis and Design Guidelines for Microphone Virtualization in Automotive Scenarios",
        "authors": "['Alessandro Opinto', 'Marco Martalò', 'Alessandro Costalunga', 'Nicolò Strozzi', 'Carlo Tripodi', 'Riccardo Raheli']",
        "date": "None",
        "source": "IEEE/ACM Transactions on Audio, Speech and Language Processing",
        "abstract": "In this article, a performance analysis on the estimation of the so-called observation filter for the Virtual Microphone Technique (VMT) in a realistic automotive environment is presented. A performance comparison between adaptive and fixed observation filter estimation methods, namely Least Mean Square (LMS) and Minimum Mean Square Error (MMSE), respectively, was carried on. Two different experimental setups were implemented on a popular B-segment car. Eight microphones were placed at the monitoring and virtual positions in order to sense environmental acoustic noise propagating within the cabin of the car running at variable speed on a smooth asphalt. Our experimental results show that a large spectral coherence between monitoring and virtual microphone signals indicates a potentially effective and relatively wide-band virtual microphone signal reconstruction. The fixed observation filter estimation method achieves better performance than the adaptive one, guaranteeing remarkable broadband estimation accuracy. Moreover, for each considered setup, design guidelines are proposed to obtain a good trade-off between estimation accuracy and material costs.",
        "link": "https://dl.acm.org/doi/10.1109/TASLP.2022.3190727",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Research on Ultrasonic High-resolution Imaging Technology for the Reinforced Concrete",
        "authors": "['Lulu Ge', 'Hua Huang', 'Zhigang Wang', 'Dexiu Dong', 'Haitao Wang', 'Qiufeng Li']",
        "date": "October 2021",
        "source": "AIAM2021: 2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture",
        "abstract": "None",
        "link": "https://dl.acm.org/doi/10.1145/3495018.3495055",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Armed Cats: Formal Concurrency Modelling at Arm",
        "authors": "['Jade Alglave', 'Will Deacon', 'Richard Grisenthwaite', 'Antoine Hacquard', 'Luc Maranget']",
        "date": "None",
        "source": "ACM Transactions on Programming Languages and Systems",
        "abstract": "We report on the process for formal concurrency modelling at Arm. An initial formal consistency model of the Arm achitecture, written in the cat language, was published and upstreamed to the herd+diy tool suite in 2017. Since then, we have extended the original model with extra features, for example, mixed-size accesses, and produced two provably equivalent alternative formulations.In this article, we present a comprehensive review of work done at Arm on the consistency model. Along the way, we also show that our principle for handling mixed-size accesses applies to x86: We confirm this via vast experimental campaigns. We also show that our alternative formulations are applicable to any model phrased in a style similar to the one chosen by Arm.",
        "link": "https://dl.acm.org/doi/10.1145/3458926",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Pinned loads: taming speculative loads in secure processors",
        "authors": "['Zirui Neil Zhao', 'Houxiang Ji', 'Adam Morrison', 'Darko Marinov', 'Josep Torrellas']",
        "date": "February 2022",
        "source": "ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "In security frameworks for speculative execution, an instruction is said to reach its Visibility Point (VP) when it is no longer vulnerable to pipeline squashes. Before a potentially leaky instruction reaches its VP, it has to stall—unless a defense scheme such as invisible speculation provides protection. Unfortunately, either stalling or protecting the execution of pre-VP instructions typically has a performance cost.  One way to attain low-overhead safe execution is to develop techniques that speed-up the advance of the VP from older to younger instructions. In this paper, we propose one such technique. We find that the progress of the VP for loads is mostly impeded by waiting until no memory consistency violations (MCVs) are possible. Hence, our technique, called , tries to make loads invulnerable to MCVs as early as possible—a process we call pinning the loads in the pipeline. The result is faster VP progress and a reduction in the execution overhead of defense schemes. In this paper, we describe the hardware needed by , and two possible designs with different tradeoffs between hardware requirements and performance. Our evaluation shows that is very effective: extending three popular defense schemes against speculative execution attacks with reduces their average execution overhead on SPEC17 and on SPLASH2/PARSEC applications by about 50%. For example, on SPEC17, the execution overhead of the three defense schemes decreases from to , from to , and from to .",
        "link": "https://dl.acm.org/doi/10.1145/3503222.3507724",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "PMNet: in-network data persistence",
        "authors": "['Korakit Seemakhupt', 'Sihang Liu', 'Yasas Senevirathne', 'Muhammad Shahbaz', 'Samira Khan']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "To guarantee data persistence, storage workloads (such as key-value stores and databases) typically use a synchronous protocol that places the network and server stack latency on the critical path of request processing. The use of the fast and byte-addressable persistent memory (PM) has helped mitigate the storage overhead of the server stack; yet, networking is still a dominant factor in the end-to-end latency of request processing. Emerging programmable network devices can reduce network latency by moving parts of the applications' compute into the network (e.g., caching results for read requests); however, for update requests, the client still has to stall on the server to commit the updates, persistently. In this work, we introduce in-network data persistence that extends the data-persistence domain from servers to the network, and present PMNet, a programmable data plane (e.g., switch or NIC) with PM for persisting data in the network. PMNet logs incoming update requests and acknowledges clients directly without having them wait on the server to commit the request. In case of a failure, the logged requests act as redo logs for the server to recover. We implement PMNet on an FPGA and evaluate its performance using common PM workloads, including key-value stores and PM-backed applications. Our evaluation shows that PMNet can improve the throughput of update requests by 4.31X on average, and the 99th-percentile tail latency by 3.23X.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00068",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "YODA: A Pedagogical Tool for Teaching Systems Concepts",
        "authors": "['Apan Qasem']",
        "date": "February 2022",
        "source": "SIGCSE 2022: Proceedings of the 53rd ACM Technical Symposium on Computer Science Education - Volume 1",
        "abstract": "Computer science undergraduates often struggle in hardware-oriented courses like Computer Organization and Computer Architecture. Active learning instruments can improve student performance in these classes. Regrettably, few tools exist today to support the creation of active learning teaching material for such courses. This paper describes YODA, a pedagogical tool for creating active learning content to help teach systems concepts. At the core, YODA is a collection of functional simulators embedded into a custom Jupyter kernel. YODA produces notebooks that allow students to learn about a system through guided interaction and observation. We have been using YODA at our home institution for two years and have seen significant improvement in student learning outcomes.",
        "link": "https://dl.acm.org/doi/10.1145/3478431.3499322",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Practical Model Checking on FPGAs",
        "authors": "['Shenghsun Cho', 'Mrunal Patel', 'Michael Ferdman', 'Peter Milder']",
        "date": "None",
        "source": "ACM Transactions on Reconfigurable Technology and Systems",
        "abstract": "Software verification is an important stage of the software development process, particularly for mission-critical systems. As the traditional methodology of using unit tests falls short of verifying complex software, developers are increasingly relying on formal verification methods, such as explicit state model checking, to automatically verify that the software functions properly. However, due to the ever-increasing complexity of software designs, model checking cannot be performed in a reasonable amount of time when running on general-purpose cores, leading to the exploration of hardware-accelerated model checking. FPGAs have been demonstrated to be promising verification accelerators, exhibiting nearly three orders of magnitude speedup over software. Unfortunately, the “FPGA programmability wall,” particularly the long synthesis and place-and-route times, block the general adoption of FPGAs for model checking.To address this problem, we designed a runtime-programmable pipeline specifically for model checkers on FPGAs to minimize the “preparation time” before a model can be checked. Our design of the successor state generator and the state validator modules enables FPGA-acceleration of model checking without incurring the time-consuming FPGA implementation stages, reducing the preparation time before checking a model from hours to less than a minute, while incurring only a 26% execution time overhead compared to model-specific implementations.",
        "link": "https://dl.acm.org/doi/10.1145/3448272",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Branchboozle: a side-channel within a hidden pattern history table of modern branch prediction units",
        "authors": "['Andrés R. Hernández C.', 'Wonjun Lee', 'Wei-Ming Lin']",
        "date": "March 2021",
        "source": "SAC '21: Proceedings of the 36th Annual ACM Symposium on Applied Computing",
        "abstract": "We present Branchboozle, a side-channel that can be configured on top of a hidden Pattern History Table (PHT) now found in modern Branch Prediction Units (BPUs). In a similar fashion to known BranchScope attacks, Branchboozle works by closely monitoring the directional predictions issued by the BPU, i.e., whether a branch is predicted Taken or Non-Taken. However, our attack exclusively focuses on analyzing the predictions issued by a secondary, mostly-undocumented 3-bit PHT, whereas BranchScope attacks manipulate the predictions issued by a textbook-like 2-bit PHT. This work describes how Branchboozle can configure an extremely robust covert-channel among independent processes that works even across the physical threads of an execution core with Simultaneous Multi-Threading technology. Additionally, we demonstrate that branches protected by Intel's Software Guard eXtensions are also vulnerable to our attack setting. Finally, we illustrate how Branchboozle can potentiate transient execution attacks dependent on branch direction misprediction, i.e., Spectre Variant 1.",
        "link": "https://dl.acm.org/doi/10.1145/3412841.3442035",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Sieve: scalable in-situ DRAM-based accelerator designs for massively parallel k-mer matching",
        "authors": "['Lingxi Wu', 'Rasool Sharifi', 'Marzieh Lenjani', 'Kevin Skadron', 'Ashish Venkat']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "The rapid influx of biosequence data, coupled with the stagnation of the processing power of modern computing systems, highlights the critical need for exploring high-performance accelerators that can meet the ever-increasing throughput demands of modern bioinformatics applications. This work argues that processing in memory (PIM) is an effective solution to enhance the performance of k-mer matching, a critical bottleneck stage in standard bioinformatics pipelines, that is characterized by random access patterns and low computational intensity. This work proposes three DRAM-based in-situ k-mer matching accelerator designs (one optimized for area, one optimized for throughput, and one that strikes a balance between hardware cost and performance), dubbed Sieve, that leverage a novel data mapping scheme to allow for simultaneous comparisons of millions of DNA base pairs, lightweight matching circuitry for fast pattern matching, and an early termination mechanism that prunes unnecessary DRAM row activation to reduce latency and save energy. Evaluation of Sieve using state-of-the-art workloads with real-world datasets shows that the most aggressive design provides an average of 326x/32x speedup and 74X/48x energy savings over multi-core-CPU/GPU baselines for k-mer matching.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00028",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "STEAM for all: New Computational Thinking Curricula in Spanish Formal Secondary Education",
        "authors": "['Rocio Garcia-Robles', 'Santiago Fernández-Cabaleiro']",
        "date": "October 2021",
        "source": "ARTECH 2021: 10th International Conference on Digital and Interactive Arts",
        "abstract": "In this article, the authors introduce the new educational curricula for Computing and Robotics in Andalusia [1], that will be offered as an optional subject for more than three hundred thousand students during 2021/2022 academic year and so on. This curriculum offers young students, aged 12-15 years old, with the opportunity to gain a better understanding on how our digital world works, as well as, developing a set of skills known as computational thinking. This term refers to a way of understanding and solving problems, in any discipline, with the help of computers.ç",
        "link": "https://dl.acm.org/doi/10.1145/3483529.3483662",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Mahjong: A Generic Framework for Network Data Plane Verification",
        "authors": "['Yifan Li', 'Chengjun Jia', 'Xiaohe Hu', 'Jun Li']",
        "date": "December 2021",
        "source": "ANCS '21: Proceedings of the Symposium on Architectures for Networking and Communications Systems",
        "abstract": "Existing network data plane verification approaches check network correctness with various models and algorithms. With respect to a specific scenario, it is hard to judge which network model provides sufficient functionality and suitable performance, because existing verification approaches are implemented with different languages and evaluated against different datasets on different hardware platforms in their papers. A network operator usually has to try out a number of complex verification approaches to find the best one for her/his network and intents. Mahjong has a modular system architecture, a unified input format, and three classic verification tools built-in. Leveraging its well-defined partition interfaces and straight-forward configuration file, not only existing approaches can be refactored and merged into Mahjong, new approaches can also be introduced and evaluated with ease.",
        "link": "https://dl.acm.org/doi/10.1145/3493425.3502755",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "NNStreamer: efficient and Agile† development of on-device AI systems",
        "authors": "['MyungJoo Ham', 'Jijoong Moon', 'Geunsik Lim', 'Jaeyun Jung', 'Hyoungjoo Ahn', 'Wook Song', 'Sangjung Woo', 'Parichay Kapoor', 'Dongju Chae', 'Gichan Jang', 'Yongjoo Ahn', 'Jihoon Lee']",
        "date": "May 2021",
        "source": "ICSE-SEIP '21: Proceedings of the 43rd International Conference on Software Engineering: Software Engineering in Practice",
        "abstract": "We propose NNStreamer, a software system that handles neural networks as filters of stream pipelines, applying the stream processing paradigm to deep neural network applications. A new trend with the wide-spread of deep neural network applications is on-device AI. It is to process neural networks on mobile devices or edge/IoT devices instead of cloud servers. Emerging privacy issues, data transmission costs, and operational costs signify the need for on-device AI, especially if we deploy a massive number of devices. NNStreamer efficiently handles neural networks with complex data stream pipelines on devices, significantly improving the overall performance with minimal effort. Besides, NNStreamer simplifies implementations and allows reusing off-the-shelf media filters directly, which reduces developmental costs significantly. We are already deploying NNStreamer for a wide range of products and platforms, including the Galaxy series and various consumer electronic devices. The experimental results suggest a reduction in developmental costs and enhanced performance of pipeline architectures and NNStreamer. It is an open-source project incubated by Linux Foundation AI & Data, available to the public and applicable to various hardware and software platforms.",
        "link": "https://dl.acm.org/doi/10.1109/ICSE-SEIP52600.2021.00029",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "The Forward Slice Core: A High-Performance, Yet Low-Complexity Microarchitecture",
        "authors": "['Kartik Lakshminarasimhan', 'Ajeya Naithani', 'Josué Feliu', 'Lieven Eeckhout']",
        "date": "None",
        "source": "ACM Transactions on Architecture and Code Optimization",
        "abstract": "Superscalar out-of-order cores deliver high performance at the cost of increased complexity and power budget. In-order cores, in contrast, are less complex and have a smaller power budget, but offer low performance. A processor architecture should ideally provide high performance in a power- and cost-efficient manner. Recently proposed slice-out-of-order (sOoO) cores identify backward slices of memory operations which they execute out-of-order with respect to the rest of the dynamic instruction stream for increased instruction-level and memory-hierarchy parallelism. Unfortunately, constructing backward slices is imprecise and hardware-inefficient, leaving performance on the table.In this article, we propose Forward Slice Core (FSC), a novel core microarchitecture that builds on a stall-on-use in-order core and extracts more instruction-level and memory-hierarchy parallelism than slice-out-of-order cores. FSC does so by identifying and steering forward slices (rather than backward slices) to dedicated in-order FIFO queues. Moreover, FSC puts load-consumers that depend on L1 D-cache misses on the side to enable younger independent load-consumers to execute faster. Finally, FSC eliminates the need for dynamic memory disambiguation by replicating store-address instructions across queues. Considering 3-wide pipeline configurations, we find that FSC improves performance by 27.1%, 21.1%, and 14.6% on average compared to Freeway, the state-of-the-art sOoO core, across SPEC CPU2017, GAP, and DaCapo, respectively, while at the same time incurring reduced hardware complexity. Compared to an OoO core, FSC reduces power consumption by 61.3% and chip area by 47%, providing a microarchitecture with high performance at low complexity.",
        "link": "https://dl.acm.org/doi/10.1145/3499424",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Enzian: an open, general, CPU/FPGA platform for systems software research",
        "authors": "['David Cock', 'Abishek Ramdas', 'Daniel Schwyn', 'Michael Giardino', 'Adam Turowski', 'Zhenhao He', 'Nora Hossle', 'Dario Korolija', 'Melissa Licciardello', 'Kristina Martsenko', 'Reto Achermann', 'Gustavo Alonso', 'Timothy Roscoe']",
        "date": "February 2022",
        "source": "ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "Hybrid computing platforms, comprising CPU cores and FPGA logic, are increasingly used for accelerating data-intensive workloads in cloud deployments, and are a growing topic of interest in systems research. However, from a research perspective, existing hardware platforms are limited: they are often optimized for concrete, narrow use-cases and, therefore lack the flexibility needed to explore other applications and configurations.   We show that a research group can design and build a more general, open, and affordable hardware platform for hybrid systems research. The platform, Enzian, is capable of duplicating the functionality of existing CPU/FPGA systems with comparable performance but in an open, flexible system. It couples a large FPGA with a server-class CPU in an asymmetric cache-coherent NUMA system. Enzian also enables research not possible with existing hybrid platforms, through explicit access to coherence messages, extensive thermal and power instrumentation, and an open, programmable baseboard management processor.   Enzian is already being used in multiple projects, is open source (both hardware and software), and available for remote use. We present the design principles of Enzian, the challenges in building it, and evaluate it with a range of existing research use-cases alongside other, more specialized platforms, as well as demonstrating research not possible on existing platforms.",
        "link": "https://dl.acm.org/doi/10.1145/3503222.3507742",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "F1: A Fast and Programmable Accelerator for Fully Homomorphic Encryption",
        "authors": "['Nikola Samardzic', 'Axel Feldmann', 'Aleksandar Krastev', 'Srinivas Devadas', 'Ronald Dreslinski', 'Christopher Peikert', 'Daniel Sanchez']",
        "date": "October 2021",
        "source": "MICRO '21: MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture",
        "abstract": "Fully Homomorphic Encryption (FHE) allows computing on encrypted data, enabling secure offloading of computation to untrusted servers. Though it provides ideal security, FHE is expensive when executed in software, 4 to 5 orders of magnitude slower than computing on unencrypted data. These overheads are a major barrier to FHE’s widespread adoption.  We present F1, the first FHE accelerator that is programmable, i.e., capable of executing full FHE programs. F1 builds on an in-depth architectural analysis of the characteristics of FHE computations that reveals acceleration opportunities. F1 is a wide-vector processor with novel functional units deeply specialized to FHE primitives, such as modular arithmetic, number-theoretic transforms, and structured permutations. This organization provides so much compute throughput that data movement becomes the key bottleneck. Thus, F1 is primarily designed to minimize data movement. Hardware provides an explicitly managed memory hierarchy and mechanisms to decouple data movement from execution. A novel compiler leverages these mechanisms to maximize reuse and schedule off-chip and on-chip data movement.  We evaluate F1 using cycle-accurate simulation and RTL synthesis. F1 is the first system to accelerate complete FHE programs, and outperforms state-of-the-art software implementations by gmean 5,400 × and by up to 17,000 ×. These speedups counter most of FHE’s overheads and enable new applications, like real-time private deep learning in the cloud.",
        "link": "https://dl.acm.org/doi/10.1145/3466752.3480070",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Non-Intrusive Distributed Tracing of Wireless IoT Devices with the FlockLab 2 Testbed",
        "authors": "['Roman Trüb', 'Reto Da Forno', 'Lukas Daschinger', 'Andreas Biri', 'Jan Beutel', 'Lothar Thiele']",
        "date": "None",
        "source": "ACM Transactions on Internet of Things",
        "abstract": "Testbeds for wireless IoT devices facilitate testing and validation of distributed target nodes. A testbed usually provides methods to control, observe, and log the execution of the software. However, most of the methods used for tracing the execution require code instrumentation and change essential properties of the observed system. Methods that are non-intrusive are typically not applicable in a distributed fashion due to a lack of time synchronization or necessary hardware/software support. In this article, we present a tracing system for validating time-critical software running on multiple distributed wireless devices that does not require code instrumentation, is non-intrusive and is designed to trace the distributed state of an entire network. For this purpose, we make use of the on-chip debug and trace hardware that is part of most modern microcontrollers. We introduce a testbed architecture as well as models and methods that accurately synchronize the timestamps of observations collected by distributed observers. In a case study, we demonstrate how the tracing system can be applied to observe the distributed state of a flooding-based low-power communication protocol for wireless sensor networks. The presented non-intrusive tracing system is implemented as a service of the publicly accessible open source FlockLab 2 testbed.",
        "link": "https://dl.acm.org/doi/10.1145/3480248",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Superconducting computing with alternating logic elements",
        "authors": "['Georgios Tzimpragos', 'Jennifer Volk', 'Alex Wynn', 'James E. Smith', 'Timothy Sherwood']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "Although superconducting single flux quantum (SFQ) technologies offer the potential for low-latency operation with energy dissipation of the order of attojoules per gate, their inherently pulse-driven nature and stateful cells have led to designs in which every logic gate is clocked. This means that clocked buffers must be added to equalize logic path lengths, and every gate becomes a pipeline stage. We propose a different approach, where gates are clock-free and synchronous designs have a conventional look-and-feel. Despite being clock-free, however, the gates are state machines by nature. To properly manage these state machines, the logical clock cycle is composed of two synchronous alternating phases: the first of which implements the desired function, and the second of which returns the state machines to the ground state. Moreover, to address the challenges associated with the asynchronous implementation of Boolean NOT operations in pulse-based systems, values are represented as unordered binary codes - in particular, dual-rail codes. With unordered codes, AND and OR operations are functionally complete. We demonstrate that our new approach, xSFQ, with its dual-rail construction and alternating clock phases, along with \"double-pumped\" logical latches and a timing optimization through latch decomposition, is capable of implementing arbitrary digital designs without gate-level pipelining and the overheads that come with it. We evaluate energy-delay tradeoffs enabled by this approach through a mix of detailed analog circuit modeling, pulse-level discrete-event simulation, and high-level pipeline efficiency analysis. The resulting systems are shown to deliver energy-delay product (EDP) gains over conventional SFQ even with pipeline hazard ratios (HR) below 1%. For hazard ratios equal to 15% and 20% and a design resembling a RISC-V RV32I core (excluding the cost of interlock logic), xSFQ achieves 22x and 31x EDP savings, respectively.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00057",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Large-scale graph processing on FPGAs with caches for thousands of simultaneous misses",
        "authors": "['Mikhail Asiatici', 'Paolo Ienne']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "Efficient large-scale graph processing is crucial to many disciplines. Yet, while graph algorithms naturally expose massive parallelism opportunities, their performance is limited by the memory system because of irregular memory accesses. State-of-the-art FPGA graph processors, such as ForeGraph and FabGraph, address the memory issues by using scratchpads and regularly streaming edges from DRAM, but then they end up wasting bandwidth on unneeded data. Yet, where classic caches and scratchpads fail to deliver, FPGAs make powerful unorthodox solutions possible. In this paper, we resort to extreme nonblocking caches that handle tens of thousands of outstanding read misses. They significantly increase the ability of memory systems to coalesce multiple accelerator accesses into fewer DRAM memory requests; essentially, when latency is not the primary concern, they bring the advantages expected from a very large cache at a fraction of the cost. We prove our point with an adaptable graph accelerator running on Amazon AWS f1; our implementation takes into account all practical aspects of such a design, including the challenges involved when working with modern multidie FPGAs. Running classic algorithms (PageRank, SCC, and SSSP) on large graphs, we achieve 3X geometric mean speedup compared to state-of-the-art FPGA accelerators, 1.1--5.8X higher bandwidth efficiency and 3.0--15.3X better power efficiency than multicore CPUs, and we support much larger graphs than the state-of-the-art on GPUs.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00054",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Context-Based Interface Prototyping: Understanding the Effect of Prototype Representation on User Feedback",
        "authors": "['Marius Hoggenmüller', 'Martin Tomitsch', 'Luke Hespanhol', 'Tram Thi Minh Tran', 'Stewart Worrall', 'Eduardo Nebot']",
        "date": "May 2021",
        "source": "CHI '21: Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems",
        "abstract": "The rise of autonomous systems in cities, such as automated vehicles (AVs), requires new approaches for prototyping and evaluating how people interact with those systems through context-based user interfaces, such as external human-machine interfaces (eHMIs). In this paper, we present a comparative study of three prototype representations (real-world VR, computer-generated VR, real-world video) of an eHMI in a mixed-methods study with 42 participants. Quantitative results show that while the real-world VR representation results in higher sense of presence, no significant differences in user experience and trust towards the AV itself were found. However, interview data shows that participants focused on different experiential and perceptual aspects in each of the prototype representations. These differences are linked to spatial awareness and perceived realism of the AV behaviour and its context, affecting in turn how participants assess trust and the eHMI. The paper offers guidelines for prototyping and evaluating context-based interfaces through simulations.",
        "link": "https://dl.acm.org/doi/10.1145/3411764.3445159",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Exploiting page table locality for agile TLB prefetching",
        "authors": "['Georgios Vavouliotis', 'Lluc Alvarez', 'Vasileios Karakostas', 'Konstantinos Nikas', 'Nectarios Koziris', 'Daniel A. Jiménez', 'Marc Casas']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "Frequent Translation Lookaside Buffer (TLB) misses incur high performance and energy costs due to page walks required for fetching the corresponding address translations. Prefetching page table entries (PTEs) ahead of demand TLB accesses can mitigate the address translation performance bottleneck, but each prefetch requires traversing the page table, triggering additional accesses to the memory hierarchy. Therefore, TLB prefetching is a costly technique that may undermine performance when the prefetches are not accurate. In this paper we exploit the locality in the last level of the page table to reduce the cost and enhance the effectiveness of TLB prefetching by fetching cache-line adjacent PTEs \"for free\". We propose Sampling-Based Free TLB Prefetching (SBFP), a dynamic scheme that predicts the usefulness of these \"free\" PTEs and prefetches only the ones most likely to prevent TLB misses. We demonstrate that combining SBFP with novel and state-of-the-art TLB prefetchers significantly improves miss coverage and reduces most memory accesses due to page walks. Moreover, we propose Agile TLB Prefetcher (ATP), a novel composite TLB prefetcher particularly designed to maximize the benefits of SBFP. ATP efficiently combines three low-cost TLB prefetchers and disables TLB prefetching for those execution phases that do not benefit from it. Unlike state-of-the-art TLB prefetchers that correlate patterns with only one feature (e.g., strides, PC, distances), ATP correlates patterns with multiple features and dynamically enables the most appropriate TLB prefetcher per TLB miss. To alleviate the address translation performance bottleneck, we propose a unified solution that combines ATP and SBFP. Across an extensive set of industrial workloads provided by Qualcomm, ATP coupled with SBFP improves geometric speedup by 16.2%, and eliminates on average 37% of the memory references due to page walks. Considering the SPEC CPU 2006 and SPEC CPU 2017 benchmark suites, ATP with SBFP increases geometric speedup by 11.1%, and eliminates page walk memory references by 26%. Applied to big data workloads (GAP suite, XSBench), ATP with SBFP yields a geometric speedup of 11.8% while reducing page walk memory references by 5%. Over the best state-of-the-art TLB prefetcher for each benchmark suite, ATP with SBFP achieves speedups of 8.7%, 3.4%, and 4.2% for the Qualcomm, SPEC, and GAP+XSBench workloads, respectively.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00016",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "LILLIPUT: a lightweight low-latency lookup-table decoder for near-term Quantum error correction",
        "authors": "['Poulami Das', 'Aditya Locharla', 'Cody Jones']",
        "date": "February 2022",
        "source": "ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "The error rates of quantum devices are orders of magnitude higher than what is needed to run most quantum applications. To close this gap, Quantum Error Correction (QEC) encodes logical qubits and distributes information using several physical qubits. By periodically executing a syndrome extraction circuit on the logical qubits, information about errors (called syndrome) is extracted while running programs. A decoder uses these syndromes to identify and correct errors in real time, which is necessary to prevent accumulation of errors. Unfortunately, software decoders are slow and hardware decoders are fast but less accurate. Thus, almost all QEC studies so far have relied on offline decoding.  To enable real-time decoding in near-term QEC, we propose LILLIPUT– a Lightweight Low Latency Look-Up Table decoder. LILLIPUT consists of two parts– First, it translates syndromes into error detection events that index into a Look-Up Table (LUT) whose entry provides the error information in real-time. Second, it programs the LUTs with error assignments for all possible error events by running a software decoder offline. LILLIPUT tolerates an error on any operation in the quantum hardware, including gates and measurements, and the number of tolerated errors grows with the size of the code. LILLIPUT utilizes less than 7% logic on off-the-shelf FPGAs enabling practical adoption, as FPGAs are already used to design the control and readout circuits in existing systems. LILLIPUT incurs a latency of a few nanoseconds and enables real-time decoding. We also propose Compressed LUTs (CLUTs) to reduce the memory required by LILLIPUT. By exploiting the fact that not all error events are equally likely and only storing data for the most probable error events, CLUTs reduce the memory needed by up-to 107x (from 148 MB to 1.38 MB) without degrading the accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3503222.3507707",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Streamline: a fast, flushless cache covert-channel attack by enabling asynchronous collusion",
        "authors": "['Gururaj Saileshwar', 'Christopher W. Fletcher', 'Moinuddin Qureshi']",
        "date": "April 2021",
        "source": "ASPLOS '21: Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "Covert-channel attacks exploit contention on shared hardware resources such as processor caches to transmit information between colluding processes on the same system. In recent years, covert channels leveraging cacheline-flush instructions, such as Flush+Reload and Flush+Flush, have emerged as the fastest cross-core attacks. However, current attacks are limited in their applicability and bit-rate not due to any fundamental hardware limitations, but due to their protocol design requiring flush instructions and tight synchronization between sender and receiver, where both processes synchronize every bit-period to maintain low error-rates.   In this paper, we present Streamline, a flush-less covert-channel attack faster than all prior known attacks. The key insight behind the higher channel bandwidth is asynchronous communication. Streamline communicates over a sequence of shared addresses (larger than the cache size), where the sender can move to the next address after transmitting each bit without waiting for the receiver. Furthermore, it ensures that addresses accessed by the sender are preserved in the cache until the receiver has accessed them. Finally, by the time the sender accesses the entire sequence and wraps around, the cache-thrashing property ensures that the previously transmitted addresses are automatically evicted from the cache without any cacheline flushes, which ensures functional correctness while simultaneously improving channel bandwidth. To orchestrate Streamline on a real system, we overcome multiple challenges, such as circumventing hardware optimizations (prefetching and replacement policy), and ensuring that the sender and receiver have similar execution rates. We demonstrate Streamline on an Intel Skylake CPU and show that it achieves a bit-rate of 1801 KB/s, which is 3x to 3.6x faster than the previous fastest Take-a-Way (588 KB/s) and Flush+Flush (496 KB/s) attacks, at comparable error rates. Unlike prior attacks, Streamline only relies on generic properties of caches and is applicable to processors of all ISAs (x86, ARM, etc.) and micro-architectures (Intel, AMD, etc.).",
        "link": "https://dl.acm.org/doi/10.1145/3445814.3446742",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Software-defined address mapping: a case on 3D memory",
        "authors": "['Jialiang Zhang', 'Michael Swift', 'Jing (Jane) Li']",
        "date": "February 2022",
        "source": "ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "3D-stacking memory such as High-Bandwidth Memory (HBM) and Hybrid Memory Cube (HMC) provides orders of magnitude more bandwidth and significantly increased channel-level parallelism (CLP) due to its new parallel memory architecture. However, it is challenging to fully exploit the abundant CLP for performance as the bandwidth utilization is highly dependent on address mapping in the memory controller. Unfortunately, CLP is very sensitive to a program’s data access pattern, which is not made available to OS/hardware by existing mechanisms.  In this work, we address these challenges with software-defined address mapping (SDAM) that, for the first time, enables user program to obtain a direct control of the low-level memory hardware in a more intelligent and fine-grained manner. In particular, we develop new mechanisms that can effectively communicate a program’s data access properties to the OS and hardware and to use it to control data placement in hardware. To guarantee correctness and reduce overhead in storage and performance, we extend Linux kernel and C-language memory allocators to support multiple address mappings. For advanced system optimization, we develop machine learning methods that can automatically identify access patterns of major variables in a program and cluster these with similar access patterns to reduce the overhead for SDAM. We demonstrate the benefits of our design on real system prototype, comprising (1) a RISC-V processor, near memory accelerators and HBM modules using Xilinx FPGA platform, and (2) modified Linux and glibc. Our evaluation on standard CPU benchmarks and data-intensive benchmarks (for both CPU and accelerators) demonstrates 1.41×, 1.84× speedup on CPU and 2.58× on near memory accelerators in our system with SDAM compared to a baseline system that uses a fixed address mapping.",
        "link": "https://dl.acm.org/doi/10.1145/3503222.3507774",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A compiler infrastructure for accelerator generators",
        "authors": "['Rachit Nigam', 'Samuel Thomas', 'Zhijing Li', 'Adrian Sampson']",
        "date": "April 2021",
        "source": "ASPLOS '21: Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "We present Calyx, a new intermediate language (IL) for compiling high-level programs into hardware designs. Calyx combines a hardware-like structural language with a software-like control flow representation with loops and conditionals. This split representation enables a new class of hardware-focused optimizations that require both structural and control flow information which are crucial for high-level programming models for hardware design. The Calyx compiler lowers control flow constructs using finite-state machines and generates synthesizable hardware descriptions.  We have implemented Calyx in an optimizing compiler that translates high-level programs to hardware. We demonstrate Calyx using two DSL-to-RTL compilers, a systolic array generator and one for a recent imperative accelerator language, and compare them to equivalent designs generated using high-level synthesis (HLS). The systolic arrays are 4.6× faster and 1.11× larger on average than HLS implementations, and the HLS-like imperative language compiler is within a few factors of a highly optimized commercial HLS toolchain. We also describe three optimizations implemented in the Calyx compiler.",
        "link": "https://dl.acm.org/doi/10.1145/3445814.3446712",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Execution dependence extension (EDE): isa support for eliminating fences",
        "authors": "['Thomas Shull', 'Ilias Vougioukas', 'Nikos Nikoleris', 'Wendy Elsasser', 'Josep Torrellas']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "Fence instructions are a coarse-grained mechanism to enforce the order of instruction execution in an out-of-order pipeline. They are an overkill for cases when only one instruction must wait for the completion of one other instruction. For example, this is the case when performing undo logging in Non-Volatile Memory (NVM) systems: while the update of a variable needs to wait until the corresponding undo log entry is persisted, all other instructions can be reordered. Unfortunately, current ISAs do not provide a way to describe such an execution dependence between two instructions that have no register or memory dependences. As a result, programmers must place fences, which unnecessarily serialize many unrelated instructions. To remedy this limitation, we propose an ISA extension capable of describing these execution dependences. We call the proposal Execution Dependence Extension (EDE), and add it to Arm's AArch64 ISA. We also present two hardware realizations of EDE that enforce execution dependences at different stages of the pipeline: one in the issue queue (IQ) and another in the write buffer (WB). We implement IQ and WB in a simulator and test them with several NVM applications. Overall, by using EDE with IQ and WB rather than fences, we attain average workload speedups of 18% and 26%, respectively.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00043",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "CoolEdge: hotspot-relievable warm water cooling for energy-efficient edge datacenters",
        "authors": "['Qiangyu Pei', 'Shutong Chen', 'Qixia Zhang', 'Xinhui Zhu', 'Fangming Liu', 'Ziyang Jia', 'Yishuo Wang', 'Yongjie Yuan']",
        "date": "February 2022",
        "source": "ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "As the computing frontier drifts to the edge, edge datacenters play a crucial role in supporting various real-time applications. Different from cloud datacenters, the requirements of proximity to end-users, high density, and heterogeneity, present new challenges to cool the edge datacenters efficiently. Although warm water cooling has become a promising cooling technique for this infrastructure, the one-size-fits-all cooling control would lower the cooling efficiency considerably because of the severe thermal imbalance across servers, hardware, and even inside one hardware component in an edge datacenter. In this work, we propose CoolEdge, a hotspot-relievable warm water cooling system for improving the cooling efficiency and saving costs of edge datacenters. Specifically, through the elaborate design of water circulations, CoolEdge can dynamically adjust the water temperature and flow rate for each heterogeneous hardware component to eliminate the hardware-level hotspots. By redesigning cold plates, CoolEdge can quickly disperse the chip-level hotspots without manual intervention. We further quantify the power saving achieved by the warm water cooling theoretically, and propose a custom-designed cooling solution to decide an appropriate water temperature and flow rate periodically. Based on a hardware prototype and real-world traces from SURFsara, the evaluation results show that CoolEdge reduces the cooling energy by 81.81% and 71.92%, respectively, compared with conventional and state-of-the-art water cooling systems.",
        "link": "https://dl.acm.org/doi/10.1145/3503222.3507713",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Data Structures for a Generic Software System using the Composite Design Pattern",
        "authors": "['Stefan Nadschläger', 'Daniel Hofer', 'Josef Küng', 'Markus Jäger']",
        "date": "July 2021",
        "source": "EuroPLoP'21: 26th European Conference on Pattern Languages of Programs",
        "abstract": "A well-designed generic software system can be reused in many contexts by efficiently handling variability. In this paper, data structures and their application in a software architecture is presented that apply the basic idea of the Composite design pattern to keep it as simple as possible, but also as generic as possible. The application of these data structures is shown throughout a layered architecture so that software developers can follow and apply the concepts. The benefit of such an architecture is that it (1) only makes use of familiar concepts, and it is easy to read and understand by knowing especially one design pattern and (2) results in a generic software system, usable in different domains.",
        "link": "https://dl.acm.org/doi/10.1145/3489449.3489972",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "PF-DRAM: a precharge-free DRAM structure",
        "authors": "['Nezam Rohbani', 'Sina Darabi', 'Hamid Sarbazi-Azad']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "Although DRAM capacity and bandwidth have increased sharply by the advances in technology and standards, its latency and energy per access have remained almost constant in recent generations. The main portion of DRAM power/energy is dissipated by Read, Write, and Refresh operations, all initiated by a Precharge phase. Precharge phase not only imposes a large amount of energy consumption, but also increases the delay of closing a row in a memory block to open another one. By reduction of row-hit rate in recent workloads, especially in multi-core systems, precharge rate increases which exacerbates DRAM power dissipation and access latency. This work proposes a novel DRAM structure, called Precharge-Free DRAM (PF-DRAM), that eliminates the Precharge phase of DRAM. PF-DRAM uses the charge on bitlines from the previous Activation phase, as the starting point for the next Activation. The difference between PF-DRAM and conventional DRAM structure is limited to precharge and equalizer circuitry and simple modifications in sense amplifier, which are all limited to subarray level. PF-DRAM is compatible with the mainstream JEDEC memory standards like DDRx and HBM, with minimum modifications in memory controller. Furthermore, almost all of the previously proposed power/energy reduction techniques in DRAM are still applicable to PF-DRAM for further improvement. Our experimental results on a 8 GB memory system running SPEC CPU2017 and PAR-SEC 2.1 workloads show an average of 35.3% memory power consumption reduction (up to 54.2%) achieved by the system using PF-DRAM with respect to the system using conventional DRAM. Moreover, the overall performance is improved by 8.6%, in average (up to 24.3%). According to our analysis, all such improvements are achieved for less than 9% area overhead.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00019",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Time-optimal Qubit mapping",
        "authors": "['Chi Zhang', 'Ari B. Hayes', 'Longfei Qiu', 'Yuwei Jin', 'Yanhao Chen', 'Eddy Z. Zhang']",
        "date": "April 2021",
        "source": "ASPLOS '21: Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "Rapid progress in the physical implementation of quantum computers gave birth to multiple recent quantum machines implemented with superconducting technology. In these NISQ machines, each qubit is physically connected to a bounded number of neighbors. This limitation prevents most quantum programs from being directly executed on quantum devices. A compiler is required for converting a quantum program to a hardware-compliant circuit, in particular, making each two-qubit gate executable by mapping the two logical qubits to two physical qubits with a link between them. To solve this problem, existing studies focus on inserting SWAP gates to dynamically remap logical qubits to physical qubits. However, most of the schemes lack the consideration of time-optimality of generated quantum circuits, or are achieving time-optimality with certain constraints. In this work, we propose a theoretically time-optimal SWAP insertion scheme for the qubit mapping problem. Our model can also be extended to practical heuristic algorithms. We present exact analysis results by using our model for quantum programs with recurring execution patterns. We have for the first time discovered an optimal qubit mapping pattern for quantum fourier transformation (QFT) on 2D nearest neighbor architecture. We also present a scalable extension of our theoretical model that can be used to solve qubit mapping for large quantum circuits.",
        "link": "https://dl.acm.org/doi/10.1145/3445814.3446706",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Speculative vectorisation with selective replay",
        "authors": "['Peng Sun', 'Giacomo Gabrielli', 'Timothy M. Jones']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "While industry continues to develop SIMD vector ISAs by providing new instructions and wider data-paths, modern SIMD architectures still rely on the programmer or compiler to transform code to vector form only when it is safe. Limitations in the power of a compiler's memory alias analysis and the presence of infrequent memory data dependences mean that whole regions of code cannot be safely vectorised without risking changing the semantics of the application, restricting the available performance. We present a new SIMD architecture to address this issue, which relies on speculation to identify and catch memory-dependence violations that occur during vector execution. Once identified, only those SIMD lanes that have used erroneous data are replayed; other lanes, both older and younger, keep the results of their latest execution. We use the compiler to mark loops with possible cross-iteration dependences and safely vectorise them by executing on our architecture, termed selective-replay vectorisation (SRV). Evaluating on a range of general-purpose and HPC benchmarks gives an average loop speedup of 2.9X, and up to 5.3X in the best case, over already-vectorised code. This leads to a whole-program speedup of up to 1.19X (average 1.06X) over already-vectorised applications.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00026",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Boosting the Restoring Performance of Deduplication Data by Classifying Backup Metadata",
        "authors": "['Ru Yang', 'Yuhui Deng', 'Yi Zhou', 'Ping Huang']",
        "date": "None",
        "source": "ACM/IMS Transactions on Data Science",
        "abstract": "Restoring data is the main purpose of data backup in storage systems. The fragmentation issue, caused by physically scattering logically continuous data across a variety of disk locations, poses a negative impact on the restoring performance of a deduplication system. Rewriting algorithms are used to alleviate the fragmentation problem by improving the restoring speed of a deduplication system. However, rewriting methods give birth to a big sacrifice in terms of deduplication ratio, leading to a huge storage space waste. Furthermore, traditional backup approaches treat file metadata and chunk metadata as the same, which causes frequent on-disk metadata accesses. In this article, we start by analyzing storage characteristics of backup metadata. An intriguing finding shows that with 10 million files, the file metadata merely takes up approximately 340 MB. Motivated by this finding, we propose a Classified-Metadata based Restoring method (CMR) that classifies backup metadata into file metadata and chunk metadata. Because the file metadata merely takes up a meager amount of space, CMR maintains all file metadata in memory, whereas chunk metadata are aggressively prefetched to memory in a greedy manner. A deduplication system with CMR in place exhibits three salient features: (i) It avoids rewriting algorithms’ additional overhead by reducing the number of disk reads in a restoring process, (ii) it increases the restoring throughput without sacrificing the deduplication ratio, and (iii) it thoroughly leverages the hardware resources to boost the restoring performance. To quantitatively evaluate the performance of CMR, we compare our CMR against two state-of-the-art approaches, namely, a history-aware rewriting method (HAR) and a context-based rewriting scheme (CAP). The experimental results show that compared to HAR and CAP, CMR reduces the restoring time by 27.2% and 29.3%, respectively. Moreover, the deduplication ratio is improved by 1.91% and 4.36%, respectively.",
        "link": "https://dl.acm.org/doi/10.1145/3437261",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Automated HW/SW co-design for edge AI: state, challenges and steps ahead",
        "authors": "['Oliver Bringmann', 'Wolfgang Ecker', 'Ingo Feldner', 'Adrian Frischknecht', 'Christoph Gerum', 'Timo Hämäläinen', 'Muhammad Abdullah Hanif', 'Michael J. Klaiber', 'Daniel Mueller-Gritschneder', 'Paul Palomero Bernardo', 'Sebastian Prebeck', 'Muhammad Shafique']",
        "date": "September 2021",
        "source": "CODES/ISSS '21: Proceedings of the 2021 International Conference on Hardware/Software Codesign and System Synthesis",
        "abstract": "Gigantic rates of data production in the era of Big Data, Internet of Thing (IoT), and Smart Cyber Physical Systems (CPS) pose incessantly escalating demands for massive data processing, storage, and transmission while continuously interacting with the physical world using edge sensors and actuators. For IoT systems, there is now a strong trend to move the intelligence from the cloud to the edge or the extreme edge (known as TinyML). Yet, this shift to edge AI systems requires to design powerful machine learning systems under very strict resource constraints. This poses a difficult design task that needs to take the complete system stack from machine learning algorithm, to model optimization and compression, to software implementation, to hardware platform and ML accelerator design into account. This paper discusses the open research challenges to achieve such a holistic Design Space Exploration for a HW/SW Co-design for Edge AI Systems and discusses the current state with three currently developed flows: one design flow for systems with tightly-coupled accelerator architectures based on RISC-V, one approach using loosely-coupled, application-specific accelerators as well as one framework that integrates software and hardware optimization techniques to built efficient Deep Neural Network (DNN) systems.",
        "link": "https://dl.acm.org/doi/10.1145/3478684.3479261",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A pattern for a Secure IoT Thing",
        "authors": "['Eduardo B. Fernandez', 'Hernan Astudillo', 'Cristian Orellana']",
        "date": "July 2021",
        "source": "EuroPLoP'21: 26th European Conference on Pattern Languages of Programs",
        "abstract": "IoT systems are very complex systems with an extensive attack surface, which makes them susceptible to a large variety of threats. It is important that every component in an IoT architecture be secure, any weak point can allow an adversary to penetrate the system. We present here a pattern to add security to an IoT thing, where a “thing” is an entity that has an identity, some intelligence, is able to communicate with other entities, and is connected to the internet. We presented earlier a pattern for a Secure IoT Architecture, the Secure IoT Thing pattern complements that pattern by securing one of its main components. This pattern is an addition to a set of patterns focusing on the security of IoT ecosystems.",
        "link": "https://dl.acm.org/doi/10.1145/3489449.3489988",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "NASA: accelerating neural network design with a NAS processor",
        "authors": "['Xiaohan Ma', 'Chang Si', 'Ying Wang', 'Cheng Liu', 'Lei Zhang']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "Neural network search (NAS) projects a promising direction to automate the design process of efficient and powerful neural network architectures. Nevertheless, the NAS techniques have to dynamically generate a large number of candidate neural networks, and iteratively train and evaluate these on-line generated network architectures, thus they are extremely time-consuming even when deployed on large GPU clusters, which dramatically hinders the adoption of NAS. Though recently there are many specialized architectures proposed to accelerate the training or inference of neural networks, we observe that existing neural network accelerators are typically targeted at static neural network architectures, and they are not suitable to accelerate the evaluation of the dynamical neural network candidates evolving during the NAS process, which cannot be deployed onto current accelerators via the off-line compilation. To enable rapid and energy-efficient NAS in compact singlechip solutions, we propose NASA, a specialized architecture for one-shot based NAS acceleration. It is able to generate, schedule, and evaluate the candidate neural network architectures for the target machine learning workload with high speed, significantly alleviating the processing bottleneck of one-shot NAS. Motivated by the observation that there are considerable computation sharing opportunities among the different neural network candidates generated in one-shot NAS, NASA is equipped with an on-chip network fusion unit to remove the redundant computation during the network mapping stage. In addition, the NASA accelerator can partition and re-schedule the candidate neural network architectures at fine-granularity to maximize the chance of data reuse and improve the utilization of the accelerator arrays integrated to accelerate network evaluation. According to our experiments on multiple one-shot NAS tasks, NASA achieves 33.52X performance speedup and 214.33X energy consumption reduction on average when compared to a CPU-GPU system.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00067",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A real-time experimentation platform for sub-6 GHz and millimeter-wave MIMO systems",
        "authors": "['Jesus O. Lacruz', 'Rafael Ruiz Ortiz', 'Joerg Widmer']",
        "date": "June 2021",
        "source": "MobiSys '21: Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services",
        "abstract": "The performance of wireless communication systems is evolving rapidly, making it difficult to build experimentation platforms that meet the hardware requirements of new standards. The bandwidth of current systems ranges from 160 MHz for IEEE 802.11ac/ax to 2 GHz for Millimeter-Wave (mm-wave) IEEE 802.11ad/ay, and they support up to 8 spatial MIMO streams. Mobile 5G and beyond systems have a similarly diverse set of requirements. To address this, we propose a highly configurable wireless platform that meets such requirements and is both affordable and scalable. It is implemented on a single state-of-the-art FPGA board that can be configured from 4x4 mm-wave MIMO with 2 GHz channels to 8x8 MIMO with 160 MHz channels in sub-6 GHz bands. In addition, multi-band operation will play an important role in future wireless networks and our platform supports mixed configurations with simultaneous use of mm-wave and sub-6 GHz. Finally, the platform supports real-time operation, e.g., for closed-loop MIMO beam training with low-latency, by implementing suitable hardware/software accelerators. We demonstrate the platform's performance in a wide range of experiments. The platform is provided as open-source to build a community to use and extend it.",
        "link": "https://dl.acm.org/doi/10.1145/3458864.3466868",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Learning from the Past: Efficient High-level Synthesis Design Space Exploration for FPGAs",
        "authors": "['Zi Wang', 'Benjamin Carrion Schafer']",
        "date": "None",
        "source": "ACM Transactions on Design Automation of Electronic Systems",
        "abstract": "The quest to democratize the use of Field-Programmable Gate Arrays (FPGAs) has given High-Level Synthesis (HLS) the final push to be widely accepted with FPGA vendors strongly supporting this VLSI design methodology to expand the FPGA user base. HLS takes as input an untimed behavioral description and generates efficient RTL (Verilog or VHDL). One major advantage of HLS is that it allows us to generate a variety of different micro-architectures from the same behavioral description by simply specifying different combination of synthesis options. In particular, commercial HLS tools make extensive use of synthesize directives in the form pragmas. This strength is also a weakness as it forces HLS users to fully understand how these synthesis options work and how they interact to efficiently set them to get a hardware implementation with the desired characteristics. Luckily, this process can be automated. Unfortunately, the search space grows supra-linearly with the number of synthesis options. To address this, this work proposes an automatic synthesis option tuner dedicated for FPGAs. We have explored a larger number of behavioral descriptions targeting ASICs and FPGAs and found out that due to the internal structure of the FPGA a large number of synthesis options combinations never lead to a Pareto-optimal design and, hence, the search space can be drastically reduced. Moreover, we make use of large database of DSE results that we have generated since we started working in this field to further accelerate the exploration process. For this, we use a technique based on perceptual hashing that allows our proposed explorer to recognize similar program structures in the new description to be explored and match them with structures in our database. This allows us to directly retrieve the pragma settings that lead to Pareto-optimal configurations. Experimental results show that the search space can be accelerated substantially while leading to finding most of the Pareto-optimal designs.",
        "link": "https://dl.acm.org/doi/10.1145/3495531",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Polyhedral-Based Compilation Framework for In-Memory Neural Network Accelerators",
        "authors": "['Jianhui Han', 'Xiang Fei', 'Zhaolin Li', 'Youhui Zhang']",
        "date": "None",
        "source": "ACM Journal on Emerging Technologies in Computing Systems",
        "abstract": "Memristor-based processing-in-memory architecture is a promising solution to the memory bottleneck in the neural network (NN) processing. A major challenge for the programmability of such architectures is the automatic compilation of high-level NN workloads, from various operators to the memristor-based hardware that may provide programming interfaces with different granularities. This article proposes a source-to-source compilation framework for such memristor-based NN accelerators, which can conduct automatic detection and mapping of multiple NN operators based on the flexible and rich representation capability of the polyhedral model. In contrast to previous studies, it implements support for pipeline generation to exploit the parallelism in the NN loads to leverage hardware resources for higher efficiency. The evaluation based on synthetic kernels and NN benchmarks demonstrates that the proposed framework can reliably detect and map the target operators. Case studies on typical memristor-based architectures also show its generality over various architectural designs. The evaluation further demonstrates that compared with existing polyhedral-based compilation frameworks that do not support the pipelined execution, the performance can upgrade by an order of magnitude with the pipelined execution, which emphasizes the necessity of our improvement.",
        "link": "https://dl.acm.org/doi/10.1145/3469847",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "COSMO: Computing with Stochastic Numbers in Memory",
        "authors": "['Saransh Gupta', 'Mohsen Imani', 'Joonseop Sim', 'Andrew Huang', 'Fan Wu', 'Jaeyoung Kang', 'Yeseong Kim', 'Tajana Šimunić Rosing']",
        "date": "None",
        "source": "ACM Journal on Emerging Technologies in Computing Systems",
        "abstract": "Stochastic computing (SC) reduces the complexity of computation by representing numbers with long streams of independent bits. However, increasing performance in SC comes with either an increase in area or a loss in accuracy. Processing in memory (PIM) computes data in-place while having high memory density and supporting bit-parallel operations with low energy consumption. In this article, we propose COSMO, an architecture for computing with stochastic numbers in memory, which enables SC in memory. The proposed architecture is general and can be used for a wide range of applications. It is a highly dense and parallel architecture that supports most SC encodings and operations in memory. It maximizes the performance and energy efficiency of SC by introducing several innovations: (i) in-memory parallel stochastic number generation, (ii) efficient implication-based logic in memory, (iii) novel memory bit line segmenting, (iv) a new memory-compatible SC addition operation, and (v) enabling flexible block allocation. To show the generality and efficiency of our stochastic architecture, we implement image processing, deep neural networks (DNNs), and hyperdimensional (HD) computing on the proposed hardware. Our evaluations show that running DNN inference on COSMO is 141× faster and 80× more energy efficient as compared to GPU.",
        "link": "https://dl.acm.org/doi/10.1145/3484731",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Learning from the Past: Efficient High-level Synthesis Design Space Exploration for FPGAs",
        "authors": "['Zi Wang', 'Benjamin Carrion Schafer']",
        "date": "None",
        "source": "ACM Transactions on Design Automation of Electronic Systems",
        "abstract": "The quest to democratize the use of Field-Programmable Gate Arrays (FPGAs) has given High-Level Synthesis (HLS) the final push to be widely accepted with FPGA vendors strongly supporting this VLSI design methodology to expand the FPGA user base. HLS takes as input an untimed behavioral description and generates efficient RTL (Verilog or VHDL). One major advantage of HLS is that it allows us to generate a variety of different micro-architectures from the same behavioral description by simply specifying different combination of synthesis options. In particular, commercial HLS tools make extensive use of synthesize directives in the form pragmas. This strength is also a weakness as it forces HLS users to fully understand how these synthesis options work and how they interact to efficiently set them to get a hardware implementation with the desired characteristics. Luckily, this process can be automated. Unfortunately, the search space grows supra-linearly with the number of synthesis options. To address this, this work proposes an automatic synthesis option tuner dedicated for FPGAs. We have explored a larger number of behavioral descriptions targeting ASICs and FPGAs and found out that due to the internal structure of the FPGA a large number of synthesis options combinations never lead to a Pareto-optimal design and, hence, the search space can be drastically reduced. Moreover, we make use of large database of DSE results that we have generated since we started working in this field to further accelerate the exploration process. For this, we use a technique based on perceptual hashing that allows our proposed explorer to recognize similar program structures in the new description to be explored and match them with structures in our database. This allows us to directly retrieve the pragma settings that lead to Pareto-optimal configurations. Experimental results show that the search space can be accelerated substantially while leading to finding most of the Pareto-optimal designs.",
        "link": "https://dl.acm.org/doi/10.1145/3495531",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "The Uncertainty of Side-channel Analysis: A Way to Leverage from Heuristics",
        "authors": "['Unai Rioja', 'Servio Paguada', 'Lejla Batina', 'Igor Armendariz']",
        "date": "None",
        "source": "ACM Journal on Emerging Technologies in Computing Systems",
        "abstract": "Performing a comprehensive side-channel analysis evaluation of small embedded devices is a process known for its variability and complexity. In real-world experimental setups, the results are largely influenced by a huge amount of parameters, some of which are not easily adjusted without trial and error and are heavily relying on the experience of professional security analysts. In this article, we advocate the usage of an existing statistical methodology called Six Sigma (6\\(\\)) for side-channel analysis optimization. This well-known methodology is commonly used in other industrial fields, such as production and quality engineering, to reduce the variability of industrial processes. We propose a customized Six Sigma methodology, which allows even a less-experienced security analysis to select optimal values for the different variables that are critical for the side-channel analysis procedure. Moreover, we show how our methodology helps in improving different phases in the side-channel analysis process.",
        "link": "https://dl.acm.org/doi/10.1145/3446997",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "LION: real-time I/O transfer control for massively parallel processor arrays",
        "authors": "['Dominik Walter', 'Jürgen Teich']",
        "date": "November 2021",
        "source": "MEMOCODE '21: Proceedings of the 19th ACM-IEEE International Conference on Formal Methods and Models for System Design",
        "abstract": "The performance of many accelerator architectures depends on the communication with external memory. During execution, new I/O data is continuously fetched forth and back to memory. This data exchange is very often performance-critical and a careful orchestration thus vital. To satisfy the I/O demand for accelerators of loop nests, it was shown that the individual reads and writes can be merged into larger blocks, which are subsequently transferred by a single DMA transfer. Furthermore, the order in which such DMA transfers must be issued, was shown to be reducible to a real-time task scheduling problem to be solved at run time. Rather than just concepts, we investigate in this paper efficient algorithms, data structures and their implementation in hardware of such a programmable Loop I/O Controller architecture called LION that only needs to be synthesized once for each processor array size and I/O buffer configuration, thus supporting a large class of processor arrays. Based on a proposed heap-based priority queue, LION is able to issue every 6 cycles a new DMA request to a memory bus. Even on a simple FPGA prototype running at just 200 MHz, this allows for more than 33 million DMA requests to be issued per second. Since the execution time of a typical DMA request is in general at least one order of magnitude longer, we can conclude that this rate is sufficient to fully utilize a given memory interface. Finally, we present implementations on FPGA and also 22nm FDX ASIC showing that the overall overhead of a LION typically amounts to less than 5% of an overall processor array design.",
        "link": "https://dl.acm.org/doi/10.1145/3487212.3487349",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "“An Error Occurred!” - Trust Repair With Virtual Robot Using Levels of Mistake Explanation",
        "authors": "['Kasper Hald', 'Katharina Weitz', 'Elisabeth André', 'Matthias Rehm']",
        "date": "November 2021",
        "source": "HAI '21: Proceedings of the 9th International Conference on Human-Agent Interaction",
        "abstract": "Human-robot collaboration in industrial settings is an expanding research field in robotics. When working together, robot mistakes are an important factor to decrease trust and therefore interferes with cooperation. It is unclear whether explanations help to restore human-robot trust after a mistake. In our study, we investigate whether system explanations as a trust-repairing action after a robot makes a mistake in a collaborative task is helpful. Our pilot study revealed that users are more interested in solutions to errors than they are in just why the error happened. Therefore, in our main study, we evaluated three levels of mistake explanations (no explanation, explanation, and explanation with solution) after a robot in VR made a mistake in executing a shared objective. After testing with 30 participants we found that the robot making a mistake significantly affects trust toward the robot, compared to it completing the task successfully. While participants found the explanations helpful to trust or distrust the robot, the levels of the explanation did not lead to an increase in trust towards the robot after a mistake. In addition, we found no significant impact of explanations on self-efficacy and the emotional state of the participants. Our results show that explanations alone are not sufficient to increase human-computer trust after robot mistakes.",
        "link": "https://dl.acm.org/doi/10.1145/3472307.3484170",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Quantifying server memory frequency margin and using it to improve performance in HPC systems",
        "authors": "['Da Zhang', 'Gagandeep Panwar', 'Jagadish B. Kotra', 'Nathan DeBardeleben', 'Sean Blanchard', 'Xun Jian']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "To maintain strong reliability, memory manufacturers label server memories at much slower data rates than the highest data rates at which they can still operate correctly for most (e.g., 99.999%+ of) accesses; we refer to the gap between these two data rates as memory frequency margin. While many prior works have studied memory latency margins in a different context of consumer memories, none has publicly studied memory frequency margin (either for consumer or server memories). To close this knowledge gap in the public domain, we perform the first public study to characterize frequency margins in commodity server memory modules. Through our large-scale study, we find that under standard voltage and cooling, they can operate 27% faster, on average, without error(s) for 99.999%+ of accesses even at high temperatures. The current practice of conservatively operating server memory is far from ideal; it slows down 99.999%+ of accesses to benefit the <0.001% of accesses that would be erroneous at a faster data rate. An ideal system should only pay this reliability tax for the <0.001% of accesses that actually need it. Towards unleashing ideal performance, our second contribution is performing the first exploration on exploiting server memory frequency margin to maximize performance. We focus on High-Performance Computing (HPC) systems, where performance is paramount. We propose exploiting HPC systems' abundant free memory in the common case to store copies of every data block and operate the copies unreliably fast to speedup common-case accesses; we use the safely-operated original blocks for recovery when the unsafely-operated copies become corrupted. We refer to our idea as Heterogeneously-accessed Dual Module Redundancy (Hetero-DMR). Hetero-DMR improves node-level performance by 18%, on average across two CPU memory hierarchies and six HPC benchmark suites, while weighted by different frequency margins and different levels of memory utilization. We also use a real system to emulate the speedup of Hetero-DMR over a conventional system; it closely matches simulation. Our system-wide simulations show applying Hetero-DMR to an HPC system provides 1.4x average speedup on job turnaround time. To facilitate adoption, Hetero-DMR also rigorously preserves system reliability and works for commodity DIMMs and CPU-memory interfaces.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00064",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "SARA: scaling a reconfigurable dataflow accelerator",
        "authors": "['Yaqi Zhang', 'Nathan Zhang', 'Tian Zhao', 'Matt Vilim', 'Muhammad Shahbaz', 'Kunle Olukotun']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "The need for speed in modern data-intensive workloads and the rise of \"dark silicon\" in the semiconductor industry are pushing for larger, faster, and more energy and area-efficient architectures, such as Reconfigurable Dataflow Accelerators (RDAs). Nevertheless, challenges remain in developing mechanisms to effectively utilize the compute power of these large-scale RDAs. To address these challenges, we present SARA, a compiler that employs a novel mapping strategy to efficiently utilize large-scale RDAs. Starting from a single-threaded imperative abstraction, SARA spatially maps a program onto RDA's distributed resources, exploiting dataflow parallelism within and across hyperblocks to saturate the compute throughput of an RDA. SARA introduces (a) compiler-managed memory consistency (CMMC), a control paradigm that hierarchically pipelines a nested and data-dependent control-flow graph onto a dataflow architecture, and (b) a compilation flow that decomposes the program graph across distributed heterogeneous resources to hide low-level RDA constraints from programmers. Our evaluation shows that SARA achieves close to perfect performance scaling on a recently proposed RDA-Plasticine. Over a mix of deep-learning, graph-processing, and streaming applications, SARA achieves a 1.9X geo-mean speedup over a Tesla V100 GPU using only 12% of the silicon area.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00085",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Cost-efficient overclocking in immersion-cooled datacenters",
        "authors": "['Majid Jalili', 'Ioannis Manousakis', 'Íñigo Goiri', 'Pulkit A. Misra', 'Ashish Raniwala', 'Husam Alissa', 'Bharath Ramakrishnan', 'Phillip Tuma', 'Christian Belady', 'Marcus Fontoura', 'Ricardo Bianchini']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "Cloud providers typically use air-based solutions for cooling servers in datacenters. However, increasing transistor counts and the end of Dennard scaling will result in chips with thermal design power that exceeds the capabilities of air cooling in the near future. Consequently, providers have started to explore liquid cooling solutions (e.g., cold plates, immersion cooling) for the most power-hungry workloads. By keeping the servers cooler, these new solutions enable providers to operate server components beyond the normal frequency range (i.e., overclocking them) all the time. Still, providers must tradeoff the increase in performance via overclocking with its higher power draw and any component reliability implications. In this paper, we argue that two-phase immersion cooling (2PIC) is the most promising technology, and build three prototype 2PIC tanks. Given the benefits of 2PIC, we characterize the impact of overclocking on performance, power, and reliability. Moreover, we propose several new scenarios for taking advantage of overclocking in cloud platforms, including oversubscribing servers and virtual machine (VM) auto-scaling. For the auto-scaling scenario, we build a system that leverages overclocking for either hiding the latency of VM creation or postponing the VM creations in the hopes of not needing them. Using realistic cloud workloads running on a tank prototype, we show that overclocking can improve performance by 20%, increase VM packing density by 20%, and improve tail latency in auto-scaling scenarios by 54%. The combination of 2PIC and overclocking can reduce platform cost by up to 13% compared to air cooling.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00055",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Pioneering chiplet technology and design for the AMD EPYC™ and Ryzen™ processor families",
        "authors": "['Samuel Naffziger', 'Noah Beck', 'Thomas Burd', 'Kevin Lepak', 'Gabriel H. Loh', 'Mahesh Subramony', 'Sean White']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "For decades, Moore's Law has delivered the ability to integrate an exponentially increasing number of devices in the same silicon area at a roughly constant cost. This has enabled tremendous levels of integration, where the capabilities of computer systems that previously occupied entire rooms can now fit on a single integrated circuit. In recent times, the steady drum beat of Moore's Law has started to slow down. Whereas device density historically doubled every 18--24 months, the rate of recent silicon process advancements has declined. While improvements in device scaling continue, albeit at a reduced pace, the industry is simultaneously observing increases in manufacturing costs. In response, the industry is now seeing a trend toward reversing direction on the traditional march toward more integration. Instead, multiple industry and academic groups are advocating that systems on chips (SoCs) be \"disintegrated\" into multiple smaller \"chiplets.\" This paper details the technology challenges that motivated AMD to use chiplets, the technical solutions we developed for our products, and how we expanded the use of chiplets from individual processors to multiple product families.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00014",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "SmartIO: Zero-overhead Device Sharing through PCIe Networking",
        "authors": "['Jonas Markussen', 'Lars Bjørlykke Kristiansen', 'Pål Halvorsen', 'Halvor Kielland-Gyrud', 'Håkon Kvale Stensland', 'Carsten Griwodz']",
        "date": "None",
        "source": "ACM Transactions on Computer Systems",
        "abstract": "The large variety of compute-heavy and data-driven applications accelerate the need for a distributed I/O solution that enables cost-effective scaling of resources between networked hosts. For example, in a cluster system, different machines may have various devices available at different times, but moving workloads to remote units over the network is often costly and introduces large overheads compared to accessing local resources. To facilitate I/O disaggregation and device sharing among hosts connected using Peripheral Component Interconnect Express (PCIe) non-transparent bridges, we present SmartIO. NVMes, GPUs, network adapters, or any other standard PCIe device may be borrowed and accessed directly, as if they were local to the remote machines. We provide capabilities beyond existing disaggregation solutions by combining traditional I/O with distributed shared-memory functionality, allowing devices to become part of the same global address space as cluster applications. Software is entirely removed from the data path, and simultaneous sharing of a device among application processes running on remote hosts is enabled. Our experimental results show that I/O devices can be shared with remote hosts, achieving native PCIe performance. Thus, compared to existing device distribution mechanisms, SmartIO provides more efficient, low-cost resource sharing, increasing the overall system performance.",
        "link": "https://dl.acm.org/doi/10.1145/3462545",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Computing Utilization Enhancement for Chiplet-based Homogeneous Processing-in-Memory Deep Learning Processors",
        "authors": "['Bo Jiao', 'Haozhe Zhu', 'Jinshan Zhang', 'Shunli Wang', 'Xiaoyang Kang', 'Lihua Zhang', 'Mingyu Wang', 'Chixiao Chen']",
        "date": "June 2021",
        "source": "GLSVLSI '21: Proceedings of the 2021 on Great Lakes Symposium on VLSI",
        "abstract": "This paper presents a design strategy of chiplet-based processing-in-memory systems for deep neural network applications. Monolithic silicon chips are area and power limited, failing to catch the recent rapid growth of deep learning algorithms. The paper first demonstrates a straightforward layer-wise method that partitions the workload of a monolithic accelerator to a multi-chiplet pipeline. A quantitative analysis shows that the straightforward separation degrades the overall utilization of computing resources due to the reduced on-chiplet memory size, thus introducing a higher memory wall. A tile interleaving strategy is proposed to overcome such degradation. This strategy can segment one layer to different chiplets which maximizes the computing utilization. To facilitate the strategy, the modification of the chiplet system hardware is also discussed. To validate the proposed strategy, a nine-chiplet processing-in-memory system is evaluated with a custom-designed object detection network. Each chiplet can achieve a peak performance of 204.8GOPS at a 100-MHz rate. The peak performance of the overall system is 1.711TOPS, where no off-chip memory access is needed. By the tile interleaving strategy, the utilization is improved from 53.9 to 92.8",
        "link": "https://dl.acm.org/doi/10.1145/3453688.3461499",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Design and evaluation frameworks for advanced RISC-based ternary processor",
        "authors": "['Dongyun Kam', 'Jung Gyu Min', 'Jongho Yoon', 'Sunmean Kim', 'Seokhyeong Kang', 'Youngjoo Lee']",
        "date": "March 2022",
        "source": "DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe",
        "abstract": "In this paper, we introduce the design and verification frameworks for developing a fully-functional emerging ternary processor. Based on the existing compiling environments for binary processors, for the given ternary instructions, the software-level framework provides an efficient way to convert the given programs to the ternary assembly codes. We also present a hardware-level framework to rapidly evaluate the performance of a ternary processor implemented in arbitrary design technology. As a case study, the fully-functional 9-trit advanced RISC-based ternary (ART-9) core is newly developed by using the proposed frameworks. Utilizing 24 custom ternary instructions, the 5-stage ART-9 prototype architecture is successfully verified by a number of test programs including dhrystone benchmark in a ternary domain, achieving the processing efficiency of 57.8 DMIPS/W and 3.06×106 DMIPS/W in the FPGA-level ternary-logic emulations and the emerging CNTFET ternary gates, respectively.",
        "link": "https://dl.acm.org/doi/10.5555/3539845.3540092",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "VELTAIR: towards high-performance multi-tenant deep learning services via adaptive compilation and scheduling",
        "authors": "['Zihan Liu', 'Jingwen Leng', 'Zhihui Zhang', 'Quan Chen', 'Chao Li', 'Minyi Guo']",
        "date": "February 2022",
        "source": "ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "Deep learning (DL) models have achieved great success in many application domains. As such, many industrial companies such as Google and Facebook have acknowledged the importance of multi-tenant DL services. Although the multi-tenant service has been studied in conventional workloads, it is not been deeply studied on deep learning service, especially on general-purpose hardware.  In this work, we systematically analyze the opportunities and challenges of providing multi-tenant deep learning services on the general-purpose CPU architecture from the aspects of scheduling granularity and code generation. We propose an adaptive granularity scheduling scheme to both guarantee resource usage efficiency and reduce the scheduling conflict rate. We also propose an adaptive compilation strategy, by which we can dynamically and intelligently pick a program with proper exclusive and shared resource usage to reduce overall interference-induced performance loss. Compared to the existing works, our design can serve more requests under the same QoS target in various scenarios (e.g., +71%, +62%, +45% for light, medium, and heavy workloads, respectively), and reduce the averaged query latency by 50%.",
        "link": "https://dl.acm.org/doi/10.1145/3503222.3507752",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Supporting legacy libraries on non-volatile memory: a user-transparent approach",
        "authors": "['Chencheng Ye', 'Yuanchao Xu', 'Xipeng Shen', 'Xiaofei Liao', 'Hai Jin', 'Yan Solihin']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "As mainstream computing is poised to embrace the advent of byte-addressable non-volatile memory (NVM), an important roadblock has remained largely unnoticed, support of legacy libraries on NVM. Libraries underpin modern software everywhere. As current NVM programming interfaces all designate special types and constructs for NVM objects and references, legacy libraries, being incompatible with these data types, will face major obstacles for working with future applications written for NVM. This paper introduces a simple approach to mitigating the issue. The novel approach centers around user-transparent persistent reference, a new concept that allows programmers to reference a persistent object in the same way as reference a normal (volatile) object. The paper presents the implementation of the concept, carefully examines its soundness, and describes compiler and simple architecture support for keeping performance overheads very low.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00042",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Toward Evaluating High-Level Synthesis Portability and Performance between Intel and Xilinx FPGAs",
        "authors": "['Anthony M Cabrera', 'Aaron R Young', 'Jacob Lambert', 'Zhili Xiao', 'Amy An', 'Seyong Lee', 'Zheming Jin', 'Jungwon Kim', 'Jeremy Buhler', 'Roger D Chamberlain', 'Jeffrey S Vetter']",
        "date": "April 2021",
        "source": "IWOCL'21: International Workshop on OpenCL",
        "abstract": "Offloading computation from a CPU to a hardware accelerator is becoming a more common solution for improving performance because traditional gains enabled by Moore’s law and Dennard scaling have slowed. GPUs are often used as hardware accelerators, but field-programmable gate arrays (FPGAs) are gaining traction. FPGAs are beneficial because they allow hardware specific to a particular application to be created. However, they are notoriously difficult to program. To this end, two of the main FPGA manufacturers, Intel and Xilinx, have created tools and frameworks that enable the use of higher level languages to design FPGA hardware. Although Xilinx kernels can be designed by using C/C++, both Intel and Xilinx support the use of OpenCL C to architect FPGA hardware. However, not much is known about the portability and performance between these two device families other than the fact that it is theoretically possible to synthesize a kernel meant for Intel to Xilinx and vice versa.  In this work, we evaluate the portability and performance of Intel and Xilinx kernels. We use OpenCL C implementations of a subset of the Rodinia benchmarking suite that were designed for an Intel FPGA and make the necessary modifications to create synthesizable OpenCL C kernels for a Xilinx FPGA. We find that the difficulty of porting certain kernel optimizations varies, depending on the construct. Once the minimum amount of modifications is made to create synthesizable hardware for the Xilinx platform, more nontrivial work is needed to improve performance. However, we find that constructs that are known to be performant for an FPGA should improve performance regardless of the platform; the difficulty comes in deciding how to invoke certain kernel optimizations while also abiding by the constraints enforced by a given platform’s hardware compiler.",
        "link": "https://dl.acm.org/doi/10.1145/3456669.3456699",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Bitwise Neural Network Acceleration Using Silicon Photonics",
        "authors": "['Kyle Shiflett', 'Avinash Karanth', 'Ahmed Louri', 'Razvan Bunescu']",
        "date": "June 2021",
        "source": "GLSVLSI '21: Proceedings of the 2021 on Great Lakes Symposium on VLSI",
        "abstract": "Hardware accelerators provide significant speedup and improve energy efficiency for several demanding deep neural network (DNN) applications. DNNs have several hidden layers that perform concurrent matrix-vector multiplications (MVMs) between the network weights and input features. As MVMs are critical to the performance of DNNs, previous research has optimized the performance and energy efficiency of MVMs at both the architecture and algorithm levels. In this paper, we propose to use emerging silicon photonics technology to improve parallelism, speed and overall efficiency with the goal of providing real-time inference and fast training of neural nets. We use microring resonators (MRRs) and Mach-Zehnder interferometers (MZIs) to design two versions (all-optical and partial-optical) of hybrid matrix multiplications for DNNs. Our results indicate that our partial optical design gave the best performance in both energy efficiency and latency, with a reduction of 33.1% for energy-delay product (EDP) with conservative estimates and a 76.4% reduction for EDP with aggressive estimates.",
        "link": "https://dl.acm.org/doi/10.1145/3453688.3461515",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A Case For Intra-rack Resource Disaggregation in HPC",
        "authors": "['George Michelogiannakis', 'Benjamin Klenk', 'Brandon Cook', 'Min Yee Teh', 'Madeleine Glick', 'Larry Dennison', 'Keren Bergman', 'John Shalf']",
        "date": "None",
        "source": "ACM Transactions on Architecture and Code Optimization",
        "abstract": "The expected halt of traditional technology scaling is motivating increased heterogeneity in high-performance computing (HPC) systems with the emergence of numerous specialized accelerators. As heterogeneity increases, so does the risk of underutilizing expensive hardware resources if we preserve today’s rigid node configuration and reservation strategies. This has sparked interest in resource disaggregation to enable finer-grain allocation of hardware resources to applications. However, there is currently no data-driven study of what range of disaggregation is appropriate in HPC. To that end, we perform a detailed analysis of key metrics sampled in NERSC’s Cori, a production HPC system that executes a diverse open-science HPC workload. In addition, we profile a variety of deep-learning applications to represent an emerging workload. We show that for a rack (cabinet) configuration and applications similar to Cori, a central processing unit with intra-rack disaggregation has a 99.5% probability to find all resources it requires inside its rack. In addition, ideal intra-rack resource disaggregation in Cori could reduce memory and NIC resources by 5.36% to 69.01% and still satisfy the worst-case average rack utilization.",
        "link": "https://dl.acm.org/doi/10.1145/3514245",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "TaskStream: accelerating task-parallel workloads by recovering program structure",
        "authors": "['Vidushi Dadu', 'Tony Nowatzki']",
        "date": "February 2022",
        "source": "ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "Reconfigurable accelerators, like CGRAs and dataflow architectures, have come to prominence for addressing data-processing problems. However, they are largely limited to workloads with regular parallelism, precluding their applicability to prevalent task-parallel workloads. Reconfigurable architectures and task parallelism seem to be at odds, as the former requires repetitive and simple program structure, and the latter breaks program structure to create small, individually scheduled program units.   Our insight is that if tasks and their potential for communication structure are first-class primitives in the hardware, it is possible to recover program structure with extremely low overhead. We propose a task execution model for accelerators called TaskStream, which annotates task dependences with information sufficient to recover inter-task structure. TaskStream enables work-aware load balancing, recovery of pipelined inter-task dependences, and recovery of inter-task read sharing through multicasting.   We apply TaskStream to a reconfigurable dataflow architecture, creating a seamless hierarchical dataflow model for task-parallel workloads. We compare our accelerator, Delta, with an equivalent static-parallel design. Overall, we find that our execution model can improve performance by 2.2× with only 3.6% area overhead, while alleviating the programming burden of managing task distribution.",
        "link": "https://dl.acm.org/doi/10.1145/3503222.3507706",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Unlimited vector extension with data streaming support",
        "authors": "['Joao Mario Domingos', 'Nuno Neves', 'Nuno Roma', 'Pedro Tomás']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "Unlimited vector extension (UVE) is a novel instruction set architecture extension that takes streaming and SIMD processing together into the modern computing scenario. It aims to overcome the shortcomings of state-of-the-art scalable vector extensions by adding data streaming as a way to simultaneously reduce the overheads associated with loop control and memory access indexing, as well as with memory access latency. This is achieved through a new set of instructions that pre-configure the loop memory access patterns. These attain accurate and timely data prefetching on predictable access patterns, such as in multidimensional arrays or in indirect memory access patterns. Each of the configured data streams is associated to a general-purpose vector register, which is then used to interface with the streams. In particular, iterating over a given stream is simply achieved by reading/writing to the corresponding input/output stream, as the data is instantly consumed/produced. To evaluate the proposed UVE, a proof-of-concept gem5 implementation was integrated in an out-of-order processor model, based on the ARM Cortex-A76, thus taking into consideration the typical speculative and out-of-order execution paradigms found in high-performance computing processors. The evaluation was carried out with a set of representative kernels, by assessing the number of executed instructions, its impact on the memory bus and its overall performance. Compared to other state-of-the-art solutions, such as the upcoming ARM Scalable Vector Extension (SVE), the obtained results show that the proposed extension attains average performance speedups over 2.4 x for the same processor configuration, including vector length.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00025",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Memory-Aware Functional IR for Higher-Level Synthesis of Accelerators",
        "authors": "['Christof Schlaak', 'Tzung-Han Juang', 'Christophe Dubach']",
        "date": "None",
        "source": "ACM Transactions on Architecture and Code Optimization",
        "abstract": "Specialized accelerators deliver orders of a magnitude of higher performance than general-purpose processors. The ever-changing nature of modern workloads is pushing the adoption of Field Programmable Gate Arrays (FPGAs) as the substrate of choice. However, FPGAs are hard to program directly using Hardware Description Languages (HDLs). Even modern high-level HDLs, e.g., Spatial and Chisel, still require hardware expertise.This article adopts functional programming concepts to provide a hardware-agnostic higher-level programming abstraction. During synthesis, these abstractions are mechanically lowered into a functional Intermediate Representation (IR) that defines a specific hardware design point. This novel IR expresses different forms of parallelism and standard memory features such as asynchronous off-chip memories or synchronous on-chip buffers. Exposing such features at the IR level is essential for achieving high performance.The viability of this approach is demonstrated on two stencil computations and by exploring the optimization space of matrix-matrix multiplication. Starting from a high-level representation for these algorithms, our compiler produces low-level VHSIC Hardware Description Language (VHDL) code automatically. Several design points are evaluated on an Intel Arria 10 FPGA, demonstrating the ability of the IR to exploit different hardware features. This article also shows that the designs produced are competitive with highly tuned OpenCL implementations and outperform hardware-agnostic OpenCL code.",
        "link": "https://dl.acm.org/doi/10.1145/3501768",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A Comprehensive Analysis of Low-Impact Computations in Deep Learning Workloads",
        "authors": "['Hengyi Li', 'Zhichen Wang', 'Xuebin Yue', 'Wenwen Wang', 'Tomiyama Hiroyuki', 'Lin Meng']",
        "date": "June 2021",
        "source": "GLSVLSI '21: Proceedings of the 2021 on Great Lakes Symposium on VLSI",
        "abstract": "Deep Neural Networks (DNNs) have achieved great successes in various machine learning tasks involving a wide range of domains. Though there are multiple hardware platforms available, such as GPUs, CPUs, FPGAs, and etc, CPUs are still preferred choices for machine learning applications, especially in low-power and resource-constrained computation environments such as embedded systems. However, the power and performance efficiency become critical issues in such computation environments when applying DNN techniques. An attractive optimization to DNNs is to remove redundant computations to enhance the execution efficiency. To this end, this paper conducts extensive experiments and analyses on popular state-of-the-art deep learning models. The experimental results include the numbers of instructions, branches, branch prediction misses, cache misses, and etc, during the execution of the models. Besides, we also investigate the performance and sparsity of each layer in the models. Based on the analysis results, this paper also proposes an instruction-level optimization, which achieves the performance improvement ranging from 10.26% to 28.0% for certain convolution layers.",
        "link": "https://dl.acm.org/doi/10.1145/3453688.3461747",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "The deployment of FPGA Based on Network in Ultra-large-scale Data Center",
        "authors": "['Qianqian Zhao', 'Hongwei Kan', 'Yanwei Wang', 'Dongdong Su', 'Kefeng Zhu', 'Le Yang']",
        "date": "October 2021",
        "source": "EITCE '21: Proceedings of the 2021 5th International Conference on Electronic Information Technology and Computer Engineering",
        "abstract": "With the increasing acceptance of cloud computing in various fields, Field Programmable Gate Arrays(FPGA), as one of the core computing power due to its programmable, low power consumption and low characteristics, has been widely deployed in data center. Therefore, how to deploy and use FPGA devices in the cloud computing system of ultra large-scale data center has become a research topic for many units. Traditionally, the deployment of FPGA in the data center is directly inserted in the PCIE slot of server, which belongs to the FPGA-CPU binding method, and The number of FPGA boards supported by single host is limited by the server slot. We propose to change the CPU-FPGA mode by decoupling the FPGA from the CPU and connecting the FPGA to the data center network as an independent resource. This solution solves the scalability problem of FPGA deployment, making it easier for FPGA to deploy and schedule on a large scale. Based on the above FPGA, this paper proposes a solution for large-scale deployment of FPGA devices in kubernetes platform. The scheme adopts container based and kubernetes container deployment management technology. The experimental results proved that the prototype has good throughput, lower communication latency between cards and better scalability compared with the deployment in the way of PCIE.",
        "link": "https://dl.acm.org/doi/10.1145/3501409.3501596",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "PiMulator: a fast and flexible processing-in-memory emulation platform",
        "authors": "['Sergiu Mosanu', 'Mohammad Nazmus Sakib', 'Tommy Tracy', 'Ersin Cukurtas', 'Alif Ahmed', 'Preslav Ivanov', 'Samira Khan', 'Kevin Skadron', 'Mircea Stan']",
        "date": "March 2022",
        "source": "DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe",
        "abstract": "Motivated by the memory wall problem, researchers propose many new Processing-in-Memory (PiM) architectures to bring computation closer to data. However, evaluating the performance of these emerging architectures involves using a myriad of tools, including circuit simulators, behavioral RTL or software simulation models, hardware approximations, etc. It is challenging to mimic both software and hardware aspects of a PiM architecture using the currently available tools with high performance and fidelity. Until and unless actual products that include PiM become available, the next best thing is to emulate various hardware PiM solutions on FPGA fabric and boards. This paper presents a modular, parameterizable, FPGA synthesizable soft PiM model suitable for prototyping and rapid evaluation of Processing-in-Memory architectures. The PiM model is implemented in System Verilog and allows users to generate any desired memory configuration on the FPGA fabric with complete control over the structure and distribution of the PiM logic units. Moreover, the model is compatible with the LiteX framework, which provides a high degree of usability and compatibility with the FPGA and RISC-V ecosystem. Thus, the framework enables architects to easily prototype, emulate and evaluate a wide range of emerging PiM architectures and designs. We demonstrate strategies to model several pioneering bitwise-PiM architectures and provide detailed benchmark performance results that demonstrate the platform's ability to facilitate design space exploration. We observe an emulation vs. simulation weighted-average speedup of 28× when running a memory benchmark workload. The model can utilize 100% BRAM and only 1% FF and LUT of an Alveo U280 FPGA board. The project is entirely open-source.",
        "link": "https://dl.acm.org/doi/10.5555/3539845.3540186",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A Unified FPGA Virtualization Framework for General-Purpose Deep Neural Networks in the Cloud",
        "authors": "['Shulin Zeng', 'Guohao Dai', 'Hanbo Sun', 'Jun Liu', 'Shiyao Li', 'Guangjun Ge', 'Kai Zhong', 'Kaiyuan Guo', 'Yu Wang', 'Huazhong Yang']",
        "date": "None",
        "source": "ACM Transactions on Reconfigurable Technology and Systems",
        "abstract": "INFerence-as-a-Service (INFaaS) has become a primary workload in the cloud. However, existing FPGA-based Deep Neural Network (DNN) accelerators are mainly optimized for the fastest speed of a single task, while the multi-tenancy of INFaaS has not been explored yet. As the demand for INFaaS keeps growing, simply increasing the number of FPGA-based DNN accelerators is not cost-effective, while merely sharing these single-task optimized DNN accelerators in a time-division multiplexing way could lead to poor isolation and high-performance loss for INFaaS. On the other hand, current cloud-based DNN accelerators have excessive compilation overhead, especially when scaling out to multi-FPGA systems for multi-tenant sharing, leading to unacceptable compilation costs for both offline deployment and online reconfiguration. Therefore, it is far from providing efficient and flexible FPGA virtualization for public and private cloud scenarios.Aiming to solve these problems, we propose a unified virtualization framework for general-purpose deep neural networks in the cloud, enabling multi-tenant sharing for both the Convolution Neural Network (CNN), and the Recurrent Neural Network (RNN) accelerators on a single FPGA. The isolation is enabled by introducing a two-level instruction dispatch module and a multi-core based hardware resources pool. Such designs provide isolated and runtime-programmable hardware resources, which further leads to performance isolation for multi-tenant sharing. On the other hand, to overcome the heavy re-compilation overheads, a tiling-based instruction frame package design and a two-stage static-dynamic compilation, are proposed. Only the lightweight runtime information is re-compiled with ∼1 ms overhead, thus guaranteeing the private cloud’s performance. Finally, the extensive experimental results show that the proposed virtualized solutions achieve up to 3.12× and 6.18× higher throughput in the private cloud compared with the static CNN and RNN baseline designs, respectively.",
        "link": "https://dl.acm.org/doi/10.1145/3480170",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "How to Shrink My FPGAs — Optimizing Tile Interfaces and the Configuration Logic in FABulous FPGA Fabrics",
        "authors": "['King Lok Chung', 'Nguyen Dao', 'Jing Yu', 'Dirk Koch']",
        "date": "February 2022",
        "source": "FPGA '22: Proceedings of the 2022 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays",
        "abstract": "Commercial FPGAs from major vendors are extensively optimized, and fabrics use many hand-crafted custom cells, including switch matrix multiplexers and configuration memory cells. The physical design optimizations commonly improve area, latency (=speed), and power consumption together. This paper is dedicated to improving the physical implementation of FPGA tiles and the configuration storage in SRAM FPGAs. This paper proposes to remap configuration bits and interface wires to implement tightly packed tiles. Using the FABulous FPGA framework, we show that our optimizations are virtually for free but can save over 20% in area and improve latency at the same time. We will evaluate our approach in different scenarios by changing the available metal layers or the requested channel capacity. Our optimizations consider all tiles and we propose a flow that resolves dependencies between the CLBs and other tiles. Moreover, we will show that frame-based reconfiguration is, in almost all cases, better than shift register configuration.",
        "link": "https://dl.acm.org/doi/10.1145/3490422.3502371",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Mining Evidences of Internet of Robotic Things (IoRT) Software from Open Source Projects",
        "authors": "['Michel Albonico', 'Adair Rohling', 'Juliano Santos', 'Paulo Varela']",
        "date": "September 2021",
        "source": "SBCARS '21: Proceedings of the 15th Brazilian Symposium on Software Components, Architectures, and Reuse",
        "abstract": "The current world scenario is heading to contactless technologies, where robots are in the center. These systems usually benefit from Internet of Things (IoT) sensing, being named Internet of Robotics Things (IoRT) systems. Developing IoRT software naturally involves high levels of complexity, which may be softened with well-established architectural evidence. In this paper, we aim at mining IoRT software architectural evidence from open source IoRT software repositories. For this, we (i) extract a dataset from GitHub repositories containing real open-source IoRT systems, (ii) mine relevant information from those repositories, (iii) and compile a catalog of architectural software characteristics. The catalog from our study can then be used by practitioners architects.",
        "link": "https://dl.acm.org/doi/10.1145/3483899.3483900",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Athena: high-performance sparse tensor contraction sequence on heterogeneous memory",
        "authors": "['Jiawen Liu', 'Dong Li', 'Roberto Gioiosa', 'Jiajia Li']",
        "date": "June 2021",
        "source": "ICS '21: Proceedings of the ACM International Conference on Supercomputing",
        "abstract": "Sparse tensor contraction sequence has been widely employed in many fields, such as chemistry and physics. However, how to efficiently implement the sequence faces multiple challenges, such as redundant computations and memory operations, massive memory consumption, and inefficient utilization of hardware. To address the above challenges, we introduce Athena, a high-performance framework for SpTC sequences. Athena introduces new data structures, leverages emerging Optane-based heterogeneous memory (HM) architecture, and adopts stage parallelism. In particular, Athena introduces shared hash table-represented sparse accumulator to eliminate unnecessary input processing and data migration; Athena uses a novel data-semantic guided dynamic migration solution to make the best use of the Optane-based HM for high performance; Athena also co-runs execution phases with different characteristics to enable high hardware utilization. Evaluating with 12 datasets, we show that Athena brings 327-7362× speedup over the state-of-the-art SpTC algorithm. With the dynamic data placement guided by data semantics, Athena brings performance improvement on Optane-based HM over a state-of-the-art software-based data management solution, a hardware-based data management solution, and PMM-only by 1.58×, 1.82×, and 2.34× respectively. Athena also showcases its effectiveness in quantum chemistry and physics scenarios.",
        "link": "https://dl.acm.org/doi/10.1145/3447818.3460355",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Orchestrated trios: compiling for efficient communication in Quantum programs with 3-Qubit gates",
        "authors": "['Casey Duckering', 'Jonathan M. Baker', 'Andrew Litteken', 'Frederic T. Chong']",
        "date": "April 2021",
        "source": "ASPLOS '21: Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "Current quantum computers are especially error prone and require high levels of optimization to reduce operation counts and maximize the probability the compiled program will succeed. These computers only support operations decomposed into one- and two-qubit gates and only two-qubit gates between physically connected pairs of qubits. Typical compilers first decompose operations, then route data to connected qubits. We propose a new compiler structure, Orchestrated Trios, that first decomposes to the three-qubit Toffoli, routes the inputs of the higher-level Toffoli operations to groups of nearby qubits, then finishes decomposition to hardware-supported gates.   This significantly reduces communication overhead by giving the routing pass access to the higher-level structure of the circuit instead of discarding it. A second benefit is the ability to now select an architecture-tuned Toffoli decomposition such as the 8-CNOT Toffoli for the specific hardware qubits now known after the routing pass. We perform real experiments on IBM Johannesburg showing an average 35% decrease in two-qubit gate count and 23% increase in success rate of a single Toffoli over Qiskit. We additionally compile many near-term benchmark algorithms showing an average 344% increase in (or 4.44x) simulated success rate on the Johannesburg architecture and compare with other architecture types.",
        "link": "https://dl.acm.org/doi/10.1145/3445814.3446718",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "RealSWATT: Remote Software-based Attestation for Embedded Devices under Realtime Constraints",
        "authors": "['Sebastian Surminski', 'Christian Niesler', 'Ferdinand Brasser', 'Lucas Davi', 'Ahmad-Reza Sadeghi']",
        "date": "November 2021",
        "source": "CCS '21: Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security",
        "abstract": "Smart factories, critical infrastructures, and medical devices largely rely on embedded systems that need to satisfy realtime constraints to complete crucial tasks. Recent studies and reports have revealed that many of these devices suffer from crucial vulnerabilities that can be exploited with fatal consequences. Despite the security and safety-critical role of these devices, they often do not feature state-of-the-art security mechanisms. Moreover, since realtime systems have strict timing requirements, integrating new security mechanisms is not a viable option as they often influence the device's runtime behavior. One solution is to offload security enhancements to a remote instance, the so-called remote attestation. We present RealSWATT, the first software-based remote attestation system for realtime embedded devices. Remote attestation is a powerful security service that allows a party to verify the correct functionality of an untrusted remote device. In contrast to previous remote attestation approaches for realtime systems, RealSWATT does neither require custom hardware extensions nor trusted computing components. It is designed to work within real-world IoT networks, connected through Wi-Fi. RealSWATT leverages a dedicated processor core for remote attestation and provides the required timing guarantees without hardware extensions. We implement RealSWATT on the popular ESP32 microcontroller, and we evaluate it on a real-world medical device with realtime constraints. To demonstrate its applicability, we furthermore integrate RealSWATT into a framework for off-the-shelf IoT devices and apply it to a smart plug, a smoke detector, and a smart light bulb.",
        "link": "https://dl.acm.org/doi/10.1145/3460120.3484788",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Cerebros: Evading the RPC Tax in Datacenters",
        "authors": "['Arash Pourhabibi', 'Mark Sutherland', 'Alexandros Daglis', 'Babak Falsafi']",
        "date": "October 2021",
        "source": "MICRO '21: MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture",
        "abstract": "The emerging paradigm of microservices decomposes online services into fine-grained software modules frequently communicating over the datacenter network, often using Remote Procedure Calls (RPCs). Ongoing advancements in the network stack have exposed the RPC layer itself as a bottleneck, that we show accounts for 40–90% of a microservice’s total execution cycles. We break down the underlying modules that comprise production RPC layers and demonstrate, based on prior evidence, that CPUs can only expect limited improvements for such tasks, mandating a shift to hardware to remove the RPC layer as a limiter of microservice performance. Although recently proposed accelerators can efficiently handle a portion of the RPC layer, their overall benefit is limited by unnecessary CPU involvement, which occurs because the accelerators are architected as co-processors under the CPU’s control. Instead, we show that conclusively removing the RPC layer bottleneck requires all of the RPC layer’s modules to be executed by a NIC-attached hardware accelerator. We introduce Cerebros, a dedicated RPC processor that executes the Apache Thrift RPC layer and acts as an intermediary stage between the NIC and the microservice running on the CPU. Our evaluation using the DeathStarBench microservice suite shows that Cerebros reduces the CPU cycles spent in the RPC layer by 37–64 ×, yielding a 1.8–14 × reduction in total cycles expended per microservice request.",
        "link": "https://dl.acm.org/doi/10.1145/3466752.3480055",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "PMEM-spec: persistent memory speculation (strict persistency can trump relaxed persistency)",
        "authors": "['Jungi Jeong', 'Changhee Jung']",
        "date": "April 2021",
        "source": "ASPLOS '21: Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "Persistency models define the persist-order that controls the order in which stores update persistent memory (PM). As with memory consistency, the relaxed persistency models provide better performance than the strict ones by relaxing the ordering constraints. To support such relaxed persistency models, previous studies resort to APIs for annotating the persist-order in program and hardware implementations for enforcing the programmer-specified order. However, these approaches to supporting relaxed persistency impose costly burdens on both architects and programmers. In light of this, the goal of this study is to demonstrate that the strict persistency model can outperform the relaxed models with significantly less hardware complexity and programming difficulty. To achieve that, this paper presents PMEM-Spec that speculatively allows any PM accesses without stalling or buffering, detecting their ordering violation (e.g., misspeculation for PM loads and stores). PMEM-Spec treats misspeculation as power failure and thus leverages failure-atomic transactions to recover from misspeculation by aborting and restarting them purposely. Since the ordering violation rarely occurs, PMEM-Spec can accelerate persistent memory accesses without significant misspeculation penalty. Experimental results show that PMEM-Spec outperforms two epoch-based persistency models with Intel X86 ISA and the state-of-the-art hardware support by 27.2% and 10.6%, respectively.",
        "link": "https://dl.acm.org/doi/10.1145/3445814.3446698",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "I-GCN: A Graph Convolutional Network Accelerator with Runtime Locality Enhancement through Islandization",
        "authors": "['Tong Geng', 'Chunshu Wu', 'Yongan Zhang', 'Cheng Tan', 'Chenhao Xie', 'Haoran You', 'Martin Herbordt', 'Yingyan Lin', 'Ang Li']",
        "date": "October 2021",
        "source": "MICRO '21: MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture",
        "abstract": "Graph Convolutional Networks (GCNs) have drawn tremendous attention in the past three years. Compared with other deep learning modalities, high-performance hardware acceleration of GCNs is as critical but even more challenging. The hurdles arise from the poor data locality and redundant computation due to the large size, high sparsity, and irregular non-zero distribution of real-world graphs.  In this paper we propose a novel hardware accelerator for GCN inference, called I-GCN, that significantly improves data locality and reduces unnecessary computation. The mechanism is a new online graph restructuring algorithm we refer to as islandization. The proposed algorithm finds clusters of nodes with strong internal but weak external connections. The islandization process yields two major benefits. First, by processing islands rather than individual nodes, there is better on-chip data reuse and fewer off-chip memory accesses. Second, there is less redundant computation as aggregation for common/shared neighbors in an island can be reused. The parallel search, identification, and leverage of graph islands are all handled purely in hardware at runtime working in an incremental pipeline. This is done without any preprocessing of the graph data or adjustment of the GCN model structure. Experimental results show that I-GCN can significantly reduce off-chip accesses and prune 38% of aggregation operations, leading to performance speedups over CPUs, GPUs, the prior art GCN accelerators of 5549 ×, 403 ×, and 5.7 × on average, respectively.",
        "link": "https://dl.acm.org/doi/10.1145/3466752.3480113",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Optimizing Storage Performance with Calibrated Interrupts",
        "authors": "['Amy Tai', 'Igor Smolyar', 'Michael Wei', 'Dan Tsafrir']",
        "date": "None",
        "source": "ACM Transactions on Storage",
        "abstract": "After request completion, an I/O device must decide whether to minimize latency by immediately firing an interrupt or to optimize for throughput by delaying the interrupt, anticipating that more requests will complete soon and help amortize the interrupt cost. Devices employ adaptive interrupt coalescing heuristics that try to balance between these opposing goals. Unfortunately, because devices lack the semantic information about which I/O requests are latency-sensitive, these heuristics can sometimes lead to disastrous results.Instead, we propose addressing the root cause of the heuristics problem by allowing software to explicitly specify to the device if submitted requests are latency-sensitive. The device then “calibrates” its interrupts to completions of latency-sensitive requests. We focus on NVMe storage devices and show that it is natural to express these semantics in the kernel and the application and only requires a modest two-bit change to the device interface. Calibrated interrupts increase throughput by up to 35%, reduce CPU consumption by as much as 30%, and achieve up to 37% lower latency when interrupts are coalesced.",
        "link": "https://dl.acm.org/doi/10.1145/3505139",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Ax-BxP: Approximate Blocked Computation for Precision-reconfigurable Deep Neural Network Acceleration",
        "authors": "['Reena Elangovan', 'Shubham Jain', 'Anand Raghunathan']",
        "date": "None",
        "source": "ACM Transactions on Design Automation of Electronic Systems",
        "abstract": "Precision scaling has emerged as a popular technique to optimize the compute and storage requirements of Deep Neural Networks (DNNs). Efforts toward creating ultra-low-precision (sub-8-bit) DNNs for efficient inference suggest that the minimum precision required to achieve a given network-level accuracy varies considerably across networks, and even across layers within a network. This translates to a need to support variable precision computation in DNN hardware. Previous proposals for precision-reconfigurable hardware, such as bit-serial architectures, incur high overheads, significantly diminishing the benefits of lower precision. We propose Ax-BxP, a method for approximate blocked computation wherein each multiply-accumulate operation is performed block-wise (a block is a group of bits), facilitating re-configurability at the granularity of blocks. Further, approximations are introduced by only performing a subset of the required block-wise computations to realize precision re-configurability with high efficiency. We design a DNN accelerator that embodies approximate blocked computation and propose a method to determine a suitable approximation configuration for any given DNN. For the AlexNet, ResNet50, and MobileNetV2 DNNs, Ax-BxP achieves improvement in system energy and performance, respectively, over an 8-bit fixed-point (FxP8) baseline, with minimal loss (<1% on average) in classification accuracy. Further, by varying the approximation configurations at a finer granularity across layers and data-structures within a DNN, we achieve improvement in system energy and performance, respectively.",
        "link": "https://dl.acm.org/doi/10.1145/3492733",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Democratizing cellular access with CellBricks",
        "authors": "['Zhihong Luo', 'Silvery Fu', 'Mark Theis', 'Shaddi Hasan', 'Sylvia Ratnasamy', 'Scott Shenker']",
        "date": "August 2021",
        "source": "SIGCOMM '21: Proceedings of the 2021 ACM SIGCOMM 2021 Conference",
        "abstract": "Markets in which competition thrives are good for both consumers and innovation but, unfortunately, competition is not thriving in the increasingly important cellular market. We propose CellBricks, a novel cellular architecture that lowers the barrier to entry for new operators by enabling users to consume access on-demand from any available cellular operator — small or large, trusted or untrusted. CellBricks achieves this by moving support for mobility and user management (authentication and billing) out of the network and into end hosts. These changes, we believe, bring valuable benefits beyond enabling competition: they lead to a cellular infrastructure that is simpler and more efficient. We design, build, and evaluate CellBricks, showing that its benefits come at little-to-no cost in performance, with application performance overhead between -1.6% to 3.1% of that achieved by current cellular infrastructure.",
        "link": "https://dl.acm.org/doi/10.1145/3452296.3473336",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Quick-Div: Rethinking Integer Divider Design for FPGA-based Soft-processors",
        "authors": "['Eric Matthews', 'Alec Lu', 'Zhenman Fang', 'Lesley Shannon']",
        "date": "None",
        "source": "ACM Transactions on Reconfigurable Technology and Systems",
        "abstract": "In today’s FPGA-based soft-processors, one of the slowest instructions is integer division. Compared to the low single-digit latency of other arithmetic operations, the fixed 32-cycle latency of radix-2 division is substantially longer. Given that today’s soft-processors typically only implement radix-2 division—if they support hardware division at all—there is significant potential to improve the performance of integer dividers.In this work, we present a set of high-performance, data-dependent, variable-latency integer dividers for FPGA-based soft-processors that we call Quick-Div. We compare them to various radix-N dividers and provide a thorough analysis in terms of latency and resource usage. In addition, we analyze the frequency scaling for such divider designs when (1) treated as a stand-alone unit and (2) integrated as part of a high-performance soft-processor. Moreover, we provide additional theoretical analysis of different dividers’ behaviour and develop a new better-performing Quick-Div variant, called Quick-radix-4. Experimental results show that our Quick-radix-4 design can achieve up to 6.8× better performance and 6.1× better performance-per-LUT over the radix-2 divider for applications such as random number generation. Even in cases where division operations constitute as little as 1% of all executed instructions, Quick-radix-4 provides a performance uplift of 16% compared to the radix-2 divider.",
        "link": "https://dl.acm.org/doi/10.1145/3502492",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A Cost-Efficient Platform Design for Distributed UAV Swarm Research",
        "authors": "['Zhongxuan Cai', 'Xuefeng Chang', 'Minglong Li']",
        "date": "November 2021",
        "source": "AISS '21: Proceedings of the 3rd International Conference on Advanced Information Science and System",
        "abstract": "Unmanned Aerial Vehicles (UAVs) have been attracting more and more attention in research and education. Specifically, Swarm intelligence is a promising future technology of UAVs and the frontier of multi-agent system research. It has the characteristics of low individual cost, strong system flexibility and robustness, and has great potential in many tasks. However, due to the constraints of research conditions and cost, most of the current researches on large-scale swarm UAVs are carried out in the simulation environment. Building a low-cost open-source software and hardware platform for swarm UAVs is an important basis for promoting researches on swarm UAVs and multi-agent systems. In this paper, we propose a design of a UAV platform with common cost-efficient hardware and a rich open-source software ecosystem, and provide a software solution for swarm robots based on the open-source robot operating system ROS. These software packages support the rapid programming development of swarm behaviors and different communication topology. Experiments have been conducted for typical UAV tasks like flocking and formation, indicating the effectiveness of the proposed platform.",
        "link": "https://dl.acm.org/doi/10.1145/3503047.3503070",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Intersection Prediction for Accelerated GPU Ray Tracing",
        "authors": "['Lufei Liu', 'Wesley Chang', 'Francois Demoullin', 'Yuan Hsi Chou', 'Mohammadreza Saed', 'David Pankratz', 'Tyler Nowicki', 'Tor M. Aamodt']",
        "date": "October 2021",
        "source": "MICRO '21: MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture",
        "abstract": "Ray tracing has been used for years in motion picture to generate photorealistic images while faster raster-based shading techniques have been preferred for video games to meet real-time requirements. However, recent Graphics Processing Units (GPUs) incorporate hardware accelerator units designed for ray tracing. These accelerator units target the process of traversing hierarchical tree data structures used to test for ray-object intersections. Distinct rays following similar paths through these structures execute many redundant ray-box intersection tests. We propose a ray intersection predictor that speculatively elides redundant operations during this process and proceeds directly to test primitives that the ray is likely to intersect. A key aspect of our predictor strategy involves identifying hash functions that preserve enough spatial information to identify redundant traversals. We explore how to integrate our ray prediction strategy into existing GPU pipelines along with improving the predictor effectiveness by predicting nodes higher in the tree as well as regrouping and scheduling traversal operations in a low cost, judicious manner. On a mobile class GPU with a ray tracing accelerator unit, we find the addition of a 5.5KB predictor per streaming multiprocessor improves performance for ambient occlusion workloads by a geometric mean of 26%.",
        "link": "https://dl.acm.org/doi/10.1145/3466752.3480097",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Machine Learning–enabled Scalable Performance Prediction of Scientific Codes",
        "authors": "['Gopinath Chennupati', 'Nandakishore Santhi', 'Phill Romero', 'Stephan Eidenbenz']",
        "date": "None",
        "source": "ACM Transactions on Modeling and Computer Simulation",
        "abstract": "Hardware architectures become increasingly complex as the compute capabilities grow to exascale. We present the Analytical Memory Model with Pipelines (AMMP) of the Performance Prediction Toolkit (PPT). PPT-AMMP takes high-level source code and hardware architecture parameters as input and predicts runtime of that code on the target hardware platform, which is defined in the input parameters. PPT-AMMP transforms the code to an (architecture-independent) intermediate representation, then (i) analyzes the basic block structure of the code, (ii) processes architecture-independent virtual memory access patterns that it uses to build memory reuse distance distribution models for each basic block, and (iii) runs detailed basic-block level simulations to determine hardware pipeline usage.PPT-AMMP uses machine learning and regression techniques to build the prediction models based on small instances of the input code, then integrates into a higher-order discrete-event simulation model of PPT running on Simian PDES engine. We validate PPT-AMMP on four standard computational physics benchmarks and present a use case of hardware parameter sensitivity analysis to identify bottleneck hardware resources on different code inputs. We further extend PPT-AMMP to predict the performance of a scientific application code, namely, the radiation transport mini-app SNAP. To this end, we analyze multi-variate regression models that accurately predict the reuse profiles and the basic block counts. We validate predicted SNAP runtimes against actual measured times.",
        "link": "https://dl.acm.org/doi/10.1145/3450264",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A spiking network model for semantic representation and replay-based association acquisition",
        "authors": "['Brian Robinson', 'Adam Polevoy', 'Sean McDaniel', 'Will Coon', 'Clara Scholl', 'Mark McLean', 'Erik Johnson']",
        "date": "July 2021",
        "source": "ICONS 2021: International Conference on Neuromorphic Systems 2021",
        "abstract": "The ability to form and store several types of associations between representations of natural images is an area of ongoing research in artificial deep neural networks, which may be informed by biologically-inspired computational models. It is hypothesized that replay of sensory stimuli through cortical-hippocampal connections is responsible for training associations between events, as a powerful form of associative learning. While models of associative memories and sensory processing have been studied extensively, there is a potential for spiking models encompassing sensory processing, reasoning over associations, and learning representations which has not been previously demonstrated. Such networks would be suitable for reasoning and learning from visual data on neuromorphic hardware. In this work, we demonstrate a novel visual reasoning network capable of representing semantic relationships and learning new associations through replay-based association with spiking models using natural images. This is demonstrated through associations of natural images from Tiny Imagenet with a knowledge graph derived from WordNet, and we show that relations in the knowledge graph can be accurately traversed for multiple sequential queries. We also demonstrate learning of a novel association after replayed presentations of natural images. This represents a novel capability for machine learning and reasoning with spiking neural networks which may be amenable to neuromorphic hardware.",
        "link": "https://dl.acm.org/doi/10.1145/3477145.3477259",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "NumaPerf: predictive NUMA profiling",
        "authors": "['Xin Zhao', 'Jin Zhou', 'Hui Guan', 'Wei Wang', 'Xu Liu', 'Tongping Liu']",
        "date": "June 2021",
        "source": "ICS '21: Proceedings of the ACM International Conference on Supercomputing",
        "abstract": "It is extremely challenging to achieve optimal performance of parallel applications on a NUMA architecture, which necessitates the assistance of profiling tools. However, existing NUMA-profiling tools share some similar shortcomings, such as portability, effectiveness, and helpfulness issues. This paper proposes a novel profiling tool–NumaPerf–that overcomes these issues. NumaPerf aims to identify potential performance issues for any NUMA architecture, instead of only on the current hardware. To achieve this, NumaPerf focuses on memory sharing patterns between threads, instead of real remote accesses. NumaPerf further detects potential thread migrations and load imbalance issues that could significantly affect the performance but are omitted by existing profilers. NumaPerf also identifies cache coherence issues separately that may require different fix strategies. Based on our extensive evaluation, NumaPerf can identify more performance issues than any existing tool, while fixing them leads to significant performance speedup.",
        "link": "https://dl.acm.org/doi/10.1145/3447818.3460361",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "ArkDB: A Key-Value Engine for Scalable Cloud Storage Services",
        "authors": "['Zhu Pang', 'Qingda Lu', 'Shuo Chen', 'Rui Wang', 'Yikang Xu', 'Jiesheng Wu']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "Persistent key-value stores play a crucial role in enabling internet-scale services. At Alibaba Cloud, scale-out cloud storage services including Object Storage Service, File Storage Service and Tablestore are built on distributed key-value stores. Key challenges in the design of the underlying key-value engine for these services lie in utilization of disaggregated storage, supporting write and range query-heavy workloads, and balancing of scalability, availability and resource usage. This paper presents ArkDB, a key-value engine designed to address these challenges by combining advantages of both LSM tree and Bw-tree, and leveraging advances in hardware technologies. Built on top of Pangu, an append-only distributed file system, ArkDB's innovations include shrinkable page mapping table, clear separation of system and user states for fast recovery, write amplification reduction, efficient garbage collection and lightweight partition split and merge. Experimental results demonstrate ArkDB's improvements over existing designs. Compared with Bw-tree, ArkDB efficiently stabilizes the mapping table size despite continuous write working set growth. Compared with RocksDB, an LSM tree-based key-value engine, ArkDB increases ingestion throughput by 2.16x, while reducing write amplification by 3.1x. It outperforms RocksDB by 52% and 37% respectively on a write-heavy workload and a range query-intensive workload of the Yahoo! Cloud Serving Benchmark. Experiments running in Tablestore in a cluster environment further demonstrate ArkDB's performance on Pangu and its efficient partition split/merge support.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3457553",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "RAP-NoC: Reliability Assessment of Photonic Network-on-Chips, A simulator",
        "authors": "['Meisam Abdollahi', 'Mohammad Baharloo', 'Fateme Shokouhinia', 'Masoumeh Ebrahimi']",
        "date": "September 2021",
        "source": "NANOCOM '21: Proceedings of the Eight Annual ACM International Conference on Nanoscale Computing and Communication",
        "abstract": "Nowadays, optical network-on-chip is accepted as a promising alternative solution for traditional electrical interconnects due to lower transmission delay and power consumption as well as considerable high data bandwidth. However, silicon photonics struggles with some particular challenges that threaten the reliability of the data transmission process. The most important challenges can be considered as temperature fluctuation, process variation, aging, crosstalk noise, and insertion loss. Although several attempts have been made to investigate the effect of these issues on the reliability of optical network-on-chip, none of them modeled the reliability of photonic network-on-chip in a system-level approach based on basic element failure rate. In this paper, an analytical model-based simulator, called Reliability Assessment of Photonic Network-on-Chips (RAP-NoC), is proposed to evaluate the reliability of different 2D optical network-on-chip architectures and data traffic. The experimental results show that, in general, Mesh topology is more reliable than Torus considering the same size. Increasing the reliability of Microring Resonator (MR) has a more significant impact on the reliability of an optical router rather than a network.",
        "link": "https://dl.acm.org/doi/10.1145/3477206.3477455",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Architecting for Artificial Intelligence with Emerging Nanotechnology",
        "authors": "['Sourabh Kulkarni', 'Sachin Bhat', 'Csaba Andras Moritz']",
        "date": "None",
        "source": "ACM Journal on Emerging Technologies in Computing Systems",
        "abstract": "Artificial Intelligence is becoming ubiquitous in products and services that we use daily. Although the domain of AI has seen substantial improvements over recent years, its effectiveness is limited by the capabilities of current computing technology. Recently, there have been several architectural innovations for AI using emerging nanotechnology. These architectures implement mathematical computations of AI with circuits that utilize physical behavior of nanodevices purpose-built for such computations. This approach leads to a much greater efficiency vs. software algorithms running on von Neumann processors or CMOS architectures, which emulate the operations with transistor circuits. In this article, we provide a comprehensive survey of these architectural directions and categorize them based on their contributions. Furthermore, we discuss the potential offered by these directions with real-world examples. We also discuss major challenges and opportunities in this field.",
        "link": "https://dl.acm.org/doi/10.1145/3445977",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "EdgeBERT: Sentence-Level Energy Optimizations for Latency-Aware Multi-Task NLP Inference",
        "authors": "['Thierry Tambe', 'Coleman Hooper', 'Lillian Pentecost', 'Tianyu Jia', 'En-Yu Yang', 'Marco Donato', 'Victor Sanh', 'Paul Whatmough', 'Alexander M. Rush', 'David Brooks', 'Gu-Yeon Wei']",
        "date": "October 2021",
        "source": "MICRO '21: MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture",
        "abstract": "Transformer-based language models such as BERT provide significant accuracy improvement to a multitude of natural language processing (NLP) tasks. However, their hefty computational and memory demands make them challenging to deploy to resource-constrained edge platforms with strict latency requirements.  We present EdgeBERT, an in-depth algorithm-hardware co-design for latency-aware energy optimizations for multi-task NLP. EdgeBERT employs entropy-based early exit predication in order to perform dynamic voltage-frequency scaling (DVFS), at a sentence granularity, for minimal energy consumption while adhering to a prescribed target latency. Computation and memory footprint overheads are further alleviated by employing a calibrated combination of adaptive attention span, selective network pruning, and floating-point quantization.  Furthermore, in order to maximize the synergistic benefits of these algorithms in always-on and intermediate edge computing settings, we specialize a 12nm scalable hardware accelerator system, integrating a fast-switching low-dropout voltage regulator (LDO), an all-digital phase-locked loop (ADPLL), as well as, high-density embedded non-volatile memories (eNVMs) wherein the sparse floating-point bit encodings of the shared multi-task parameters are carefully stored. Altogether, latency-aware multi-task NLP inference acceleration on the EdgeBERT hardware system generates up to 7 ×, 2.5 ×, and 53 × lower energy compared to the conventional inference without early stopping, the latency-unbounded early exit approach, and CUDA adaptations on an Nvidia Jetson Tegra X2 mobile GPU, respectively.",
        "link": "https://dl.acm.org/doi/10.1145/3466752.3480095",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "DOSAGE: generating domain-specific accelerators for resource-constrained computing",
        "authors": "['Ankur Limaye', 'Tosiron Adegbija']",
        "date": "July 2021",
        "source": "ISLPED '21: Proceedings of the ACM/IEEE International Symposium on Low Power Electronics and Design",
        "abstract": "Integrating low-overhead domain-specific accelerators with low-energy general-purpose processors can improve the processors' performance efficiency in resource-constrained systems (e.g., embedded systems). However, current function-based approaches for designing domain-specific accelerators require substantial programmer efforts for hardware/software partitioning and program modifications to access the best available hardware accelerators. This paper presents DOSAGE, an LLVM compiler-based methodology to generate domain-specific accelerators for resource-constrained computing systems. Given a set of applications, DOSAGE automatically identifies and ranks the recurrent and similar code blocks that would benefit the most from hardware acceleration, based on the code blocks' composition. We illustrate the benefits of the proposed approach using a case study that involves generating domain-specific accelerators for a diverse set of healthcare applications and evaluate the accelerators via FPGA-based prototyping. Compared to a base low-resource RISC-V processor, DOSAGE accelerators improved the system's performance and energy by 24.85% and 8.54%, respectively. Furthermore, compared to a state-of-the-art function-based accelerator generation approach, DOSAGE eliminated the function-level granularity constraint of the generation process and reduced the number of required accelerators---and, in effect, the interfacing overhead---by 33.33%, while achieving equal or better program coverage and performance/energy results.",
        "link": "https://dl.acm.org/doi/10.1109/ISLPED52811.2021.9502501",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Towards approximate computing for achieving energy vs. accuracy trade-offs",
        "authors": "['Aleksandr Ometov', 'Jari Nurmi']",
        "date": "March 2022",
        "source": "DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe",
        "abstract": "Despite the recent advances in semiconductor technology and energy-aware system design, the overall energy consumption of computing and communication systems is rapidly growing. On the one hand, the pervasiveness of these technologies everywhere in the form of mobile devices, cyber-physical embedded systems, sensor networks, wearables, social media and context-awareness, intelligent machines, broadband cellular networks, Cloud computing, and Internet of Things (IoT) has drastically increased the demand for computing and communications. On the other hand, the user expectations on features and battery life of online devices are increasing all the time, and it creates another incentive for finding good trade-offs between performance and energy consumption. One of the opportunities to address this growing demand is to utilize an Approximate Computing approach through software and hardware design. The APROPOS project aims at finding the balance between accuracy and energy consumption, and this short paper provides an initial overview of the corresponding roadmap, as the project is still in the initial stage.",
        "link": "https://dl.acm.org/doi/10.5555/3539845.3540001",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "LADDER: Architecting Content and Location-aware Writes for Crossbar Resistive Memories",
        "authors": "['Md Hafizul Islam Chowdhuryy', 'Muhammad Rashedul Haq Rashed', 'Amro Awad', 'Rickard Ewetz', 'Fan Yao']",
        "date": "October 2021",
        "source": "MICRO '21: MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture",
        "abstract": "Resistive memories (ReRAM) organized in the form of crossbars are promising for main memory integration. While offering high cell density, crossbar-based ReRAMs suffer from variable write latency requirement for RESET operations due to the varying impact of IR drop, which jointly depends on the data pattern of the crossbar and the location of target cells being RESET. The exacerbated worst-case RESET latencies can significantly limit system performance.  In this paper, we propose LADDER, an effective and low-cost processor-side framework that performs writes with variable latency by exploiting both content and location dependencies. To enable content awareness, LADDER incorporates a novel scheme that maintains metadata for per-row data pattern (i.e., number of 1’s) in memory, and performs efficient metadata management and caching through the memory controller. LADDER does not require hardware changes to the ReRAM chip. We design several optimizations that further boost the performance of LADDER, including LRS-metadata estimation that eliminates stale memory block reads, intra-line bit-level shifting that reduces the worst-case LRS-counter values and multi-granularity LRS-metadata design that optimizes the number of counters to maintain. We evaluate the efficacy of LADDER using 16 single- and multi-programmed workloads. Our results show that LADDER exhibits on average 46% performance improvement as compared to a baseline scheme and up to 33% over state-of-the-art designs. Furthermore, LADDER achieves 28.8% average dynamic memory energy saving compared to the existing architecture schemes and has less than 3% impact on device lifetime.",
        "link": "https://dl.acm.org/doi/10.1145/3466752.3480054",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Flynn’s Reconciliation: Automating the Register Cache Idiom for Cross-accelerator Programming",
        "authors": "['Daniel Thuerck', 'Nicolas Weber', 'Roberto Bifulco']",
        "date": "None",
        "source": "ACM Transactions on Architecture and Code Optimization",
        "abstract": "A large portion of the recent performance increase in the High Performance Computing (HPC) and Machine Learning (ML) domains is fueled by accelerator cards. Many popular ML frameworks support accelerators by organizing computations as a computational graph over a set of highly optimized, batched general-purpose kernels. While this approach simplifies the kernels’ implementation for each individual accelerator, the increasing heterogeneity among accelerator architectures for HPC complicates the creation of portable and extensible libraries of such kernels. Therefore, using a generalization of the CUDA community’s warp register cache programming idiom, we propose a new programming idiom (CoRe) and a virtual architecture model (PIRCH), abstracting over SIMD and SIMT paradigms. We define and automate the mapping process from a single source to PIRCH’s intermediate representation and develop backends that issue code for three different architectures: Intel AVX512, NVIDIA GPUs, and NEC SX-Aurora. Code generated by our source-to-source compiler for batched kernels, borG, competes favorably with vendor-tuned libraries and is up to 2× faster than hand-tuned kernels across architectures.",
        "link": "https://dl.acm.org/doi/10.1145/3458357",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Byte-Select Compression",
        "authors": "['Matthew Tomei', 'Shomit Das', 'Mohammad Seyedzadeh', 'Philip Bedoukian', 'Bradford Beckmann', 'Rakesh Kumar', 'David Wood']",
        "date": "None",
        "source": "ACM Transactions on Architecture and Code Optimization",
        "abstract": "Cache-block compression is a highly effective technique for both reducing accesses to lower levels in the memory hierarchy (cache compression) and minimizing data transfers (link compression). While many effective cache-block compression algorithms have been proposed, the design of these algorithms is largely ad hoc and manual and relies on human recognition of patterns. In this article, we take an entirely different approach. We introduce a class of “byte-select” compression algorithms, as well as an automated methodology for generating compression algorithms in this class. We argue that, based on upper bounds within the class, the study of this class of byte-select algorithms has potential to yield algorithms with better performance than existing cache-block compression algorithms. The upper bound we establish on the compression ratio is 2X that of any existing algorithm. We then offer a generalized representation of a subset of byte-select compression algorithms and search through the resulting space guided by a set of training data traces. Using this automated process, we find efficient and effective algorithms for various hardware applications. We find that the resulting algorithms exploit novel patterns that can inform future algorithm designs. The generated byte-select algorithms are evaluated against a separate set of traces and evaluations show that Byte-Select has a 23% higher compression ratio on average. While no previous algorithm performs best for all our data sets which include CPU and GPU applications, our generated algorithms do. Using an automated hardware generator for these algorithms, we show that their decompression and compression latency is one and two cycles respectively, much lower than any existing algorithm with a competitive compression ratio.",
        "link": "https://dl.acm.org/doi/10.1145/3462209",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Quingo: A Programming Framework for Heterogeneous Quantum-Classical Computing with NISQ Features",
        "authors": "['X. Fu', 'Jintao Yu', 'Xing Su', 'Hanru Jiang', 'Hua Wu', 'Fucheng Cheng', 'Xi Deng', 'Jinrong Zhang', 'Lei Jin', 'Yihang Yang', 'Le Xu', 'Chunchao Hu', 'Anqi Huang', 'Guangyao Huang', 'Xiaogang Qiang', 'Mingtang Deng', 'Ping Xu', 'Weixia Xu', 'Wanwei Liu', 'Yu Zhang', 'Yuxin Deng', 'Junjie Wu', 'Yuan Feng']",
        "date": "None",
        "source": "ACM Transactions on Quantum Computing",
        "abstract": "The increasing control complexity of Noisy Intermediate-Scale Quantum (NISQ) systems underlines the necessity of integrating quantum hardware with quantum software. While mapping heterogeneous quantum-classical computing (HQCC) algorithms to NISQ hardware for execution, we observed a few dissatisfactions in quantum programming languages (QPLs), including difficult mapping to hardware, limited expressiveness, and counter-intuitive code. In addition, noisy qubits require repeatedly performed quantum experiments, which explicitly operate low-level configurations, such as pulses and timing of operations. This requirement is beyond the scope or capability of most existing QPLs.We summarize three execution models to depict the quantum-classical interaction of existing QPLs. Based on the refined HQCC model, we propose the Quingo framework to integrate and manage quantum-classical software and hardware to provide the programmability over HQCC applications and map them to NISQ hardware. We propose a six-phase quantum program life-cycle model matching the refined HQCC model, which is implemented by a runtime system. We also propose the Quingo programming language, an external domain-specific language highlighting timer-based timing control and opaque operation definition, which can be used to describe quantum experiments. We believe the Quingo framework could contribute to the clarification of key techniques in the design of future HQCC systems.",
        "link": "https://dl.acm.org/doi/10.1145/3483528",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "OAuth 2.0-based authentication solution for FPGA-enabled cloud computing",
        "authors": "['Semih Ince', 'David Espes', 'Guy Gogniat', 'Julien Lallet', 'Renaud Santoro']",
        "date": "December 2021",
        "source": "UCC '21: Proceedings of the 14th IEEE/ACM International Conference on Utility and Cloud Computing Companion",
        "abstract": "FPGA-enabled cloud computing is getting more and more common as cloud providers offer hardware accelerated solutions. In this context, clients need confidential remote computing. However Intellectual Properties and data are being used and communicated. So current security models require the client to trust the cloud provider blindly by disclosing sensitive information. In addition, the lack of strong authentication and access control mechanisms, for both the client and the provided FPGA in current solutions, is a major security drawback. To enhance security measures and privacy between the client, the cloud provider and the FPGA, an additional entity needs to be introduced: the trusted authority. Its role is to authenticate the client-FPGA pair and isolate them from the cloud provider. With our novel OAuth 2.0-based access delegation solution for FPGA-accelerated clouds, a remote confidential FPGA environment with a token-based access can be created for the client. Our solution allows to manage and securely allocate heterogeneous resource pools with enhanced privacy & confidentiality for the client. Our formal analysis shows that our protocol adds a very small latency which is suitable for real-time application.",
        "link": "https://dl.acm.org/doi/10.1145/3492323.3495635",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "One Glitch to Rule Them All: Fault Injection Attacks Against AMD's Secure Encrypted Virtualization",
        "authors": "['Robert Buhren', 'Hans-Niklas Jacob', 'Thilo Krachenfels', 'Jean-Pierre Seifert']",
        "date": "November 2021",
        "source": "CCS '21: Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security",
        "abstract": "AMD Secure Encrypted Virtualization (SEV) offers protection mechanisms for virtual machines in untrusted environments through memory and register encryption. To separate security-sensitive operations from software executing on the main x86 cores, SEV leverages the AMD Secure Processor (AMD-SP). This paper introduces a new approach to attack SEV-protected virtual machines (VMs) by targeting the AMD-SP. We present a voltage glitching attack that allows an attacker to execute custom payloads on the AMD-SPs of all microarchitectures that support SEV currently on the market (Zen 1, Zen 2, and Zen 3). The presented methods allow us to deploy a custom SEV firmware on the AMD-SP, which enables an adversary to decrypt a VM's memory. Furthermore, using our approach, we can extract endorsement keys of SEV-enabled CPUs, which allows us to fake attestation reports or to pose as a valid target for VM migration without requiring physical access to the target host. Moreover, we reverse-engineered the Versioned Chip Endorsement Key (VCEK) mechanism introduced with SEV Secure Nested Paging (SEV-SNP). The VCEK binds the endorsement keys to the firmware version of TCB components relevant for SEV. Building on the ability to extract the endorsement keys, we show how to derive valid VCEKs for arbitrary firmware versions. With our findings, we prove that SEV cannot adequately protect confidential data in cloud environments from insider attackers, such as rogue administrators, on currently available CPUs.",
        "link": "https://dl.acm.org/doi/10.1145/3460120.3484779",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Metron: High-performance NFV Service Chaining Even in the Presence of Blackboxes",
        "authors": "['Georgios P. Katsikas', 'Tom Barbette', 'Dejan Kostić', 'JR. Gerald Q. Maguire', 'Rebecca Steinert']",
        "date": "None",
        "source": "ACM Transactions on Computer Systems",
        "abstract": "Deployment of 100Gigabit Ethernet (GbE) links challenges the packet processing limits of commodity hardware used for Network Functions Virtualization (NFV). Moreover, realizing chained network functions (i.e., service chains) necessitates the use of multiple CPU cores, or even multiple servers, to process packets from such high speed links.Our system Metron jointly exploits the underlying network and commodity servers’ resources: (i) to offload part of the packet processing logic to the network, (ii)  by using smart tagging to setup and exploit the affinity of traffic classes, and (iii)  by using tag-based hardware dispatching to carry out the remaining packet processing at the speed of the servers’ cores, with zero inter-core communication. Moreover, Metron transparently integrates, manages, and load balances proprietary “blackboxes” together with Metron service chains.Metron realizes stateful network functions at the speed of 100GbE network cards on a single server, while elastically and rapidly adapting to changing workload volumes. Our experiments demonstrate that Metron service chains can coexist with heterogeneous blackboxes, while still leveraging Metron’s accurate dispatching and load balancing. In summary, Metron has (i)  2.75–8× better efficiency, up to (ii)  4.7× lower latency, and (iii)  7.8× higher throughput than OpenBox, a state-of-the-art NFV system.",
        "link": "https://dl.acm.org/doi/10.1145/3465628",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "FlexFilt: Towards Flexible Instruction Filtering for Security",
        "authors": "['Leila Delshadtehrani', 'Sadullah Canakci', 'William Blair', 'Manuel Egele', 'Ajay Joshi']",
        "date": "December 2021",
        "source": "ACSAC '21: Proceedings of the 37th Annual Computer Security Applications Conference",
        "abstract": "As the complexity of software applications increases, there has been a growing demand for intra-process memory isolation. The commercially available intra-process memory isolation mechanisms in modern processors, e.g., Intel’s memory protection keys, trade-off between efficiency and security guarantees. Recently, researchers have tended to leverage the features with low security guarantees for intra-process memory isolation. Subsequently, they have relied on binary scanning and runtime binary rewriting to prevent the execution of unsafe instructions, which improves the security guarantees. Such intra-process memory isolation mechanisms are not the only security solutions that have to prevent the execution of unsafe instructions in untrusted parts of the code. In fact, we identify a similar requirement in a variety of other security solutions. Although binary scanning and runtime binary rewriting approaches can be leveraged to address this requirement, it is challenging to efficiently implement these approaches.  In this paper, we propose an efficient and flexible hardware-assisted feature for runtime filtering of user-specified instructions. This flexible feature, called FlexFilt, assists with securing various isolation-based mechanisms. FlexFilt enables the software developer to create up to 16 instruction domains, where each instruction domain can be configured to filter the execution of user-specified instructions. In addition to filtering unprivileged instructions, FlexFilt is capable of filtering privileged instructions. To illustrate the effectiveness of FlexFilt compared to binary scanning approaches, we measure the overhead caused by scanning the JIT compiled code while browsing various webpages. We demonstrate the feasibility of FlexFilt by implementing our design on the RISC-V Rocket core, providing the Linux kernel support for it, and prototyping our full design on an FPGA.",
        "link": "https://dl.acm.org/doi/10.1145/3485832.3488019",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Leaky buddies: cross-component covert channels on integrated CPU-GPU systems",
        "authors": "['Sankha Baran Dutta', 'Hoda Naghibijouybari', 'Nael Abu-Ghazaleh', 'Andres Marquez', 'Kevin Barker']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "Graphics Processing Units (GPUs) are ubiquitous components used across the range of today's computing platforms, from phones and tablets, through personal computers, to high-end server class platforms. With the increasing importance of graphics and video workloads, recent processors are shipped with GPU devices that are integrated on the same chip. Integrated GPUs share some resources with the CPU and as a result, there is a potential for microarchitectural attacks from the GPU to the CPU or vice versa. We consider the potential for covert channel attacks that arise either from shared microarchitectural components (such as caches) or through shared contention domains (e.g., shared buses). We illustrate these two types of channels by developing two reliable covert channel attacks. The first covert channel uses the shared LLC cache in Intel's integrated GPU architectures. The second is a contention based channel targeting the ring bus connecting the CPU and GPU to the LLC. This is the first demonstrated microarchitectural attack crossing the component boundary (GPU to CPU or vice versa). Cross-component channels introduce a number of new challenges that we had to overcome since they occur across heterogeneous components that use different computation models and are interconnected using asymmetric memory hierarchies. We also exploit GPU parallelism to increase the bandwidth of the communication, even without relying on a common clock. The LLC based channel achieves a bandwidth of 120 kbps with a low error rate of 2%, while the contention based channel delivers up to 400 kbps with a 0.8% error rate. We also demonstrate a proof-of-concept prime-and-probe side channel attack that probes the full LLC from the GPU.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00080",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Deployment and scalability of an inter-domain multi-path routing infrastructure",
        "authors": "['Cyrill Krähenbühl', 'Seyedali Tabaeiaghdaei', 'Christelle Gloor', 'Jonghoon Kwon', 'Adrian Perrig', 'David Hausheer', 'Dominik Roos']",
        "date": "December 2021",
        "source": "CoNEXT '21: Proceedings of the 17th International Conference on emerging Networking EXperiments and Technologies",
        "abstract": "Path aware networking (PAN) is a promising approach that enables endpoints to participate in end-to-end path selection. PAN unlocks numerous benefits, such as fast failover after link failures, application-based path selection and optimization, and native interdomain multi-path. The utility of PAN hinges on the availability of a large number of high-quality path options. In an inter-domain context, two core questions arise. Can we deploy such an architecture natively in today's Internet infrastructure without creating an overlay relying on BGP? Can we build a scalable multi-path routing system that provides a large number of high-quality paths? We first report on the real-world native deployment of the SCION next-generation architecture, providing a usable PAN infrastructure operating in parallel to today's Internet. We then analyze the scalability of the architecture in an Internet-scale topology. Finally, we introduce a new routing approach to further improve scalability.",
        "link": "https://dl.acm.org/doi/10.1145/3485983.3494862",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Faster enclave transitions for IO-intensive network applications",
        "authors": "['Jakob Svenningsson', 'Nicolae Paladi', 'Arash Vahidi']",
        "date": "August 2021",
        "source": "SPIN '21: Proceedings of the ACM SIGCOMM 2021 Workshop on Secure Programmable network INfrastructure",
        "abstract": "Process-based confidential computing enclaves such as Intel SGX have been proposed for protecting the confidentiality and integrity of network applications, without the overhead of virtualization. However, these solutions introduce other types of overhead, particularly the cost transitioning in and out of an enclave context. This makes the use of enclaves impractical for running IO-intensive applications, such as network packet processing. We build on earlier approaches to improve the IO performance of workloads in Intel SGX enclaves and propose the HotCall-Bundler library that helps reduce the cost of individual single enclave transitions and the total number of enclave transitions in trusted applications running in Intel SGX enclaves. We describe the implementation of the HotCall-Bundler library, evaluate its performance and demonstrate its practicality using the case study of Open vSwitch, a widely used software switch implementation.",
        "link": "https://dl.acm.org/doi/10.1145/3472873.3472879",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Distance-aware Approximate Nanophotonic Interconnect",
        "authors": "['Jaechul Lee', 'Cédric Killian', 'Sebastien Le Beux', 'Daniel Chillet']",
        "date": "None",
        "source": "ACM Transactions on Design Automation of Electronic Systems",
        "abstract": "The energy consumption of manycore architectures is dominated by data movement, which calls for energy-efficient and high-bandwidth interconnects. To overcome the bandwidth limitation of electrical interconnects, integrated optics appear as a promising technology. However, it suffers from high power overhead related to low laser efficiency, which calls for the use of techniques and methods to improve its energy costs. Besides, approximate computing is emerging as an efficient method to reduce energy consumption and improve execution speed of embedded computing systems. It relies on allowing accuracy reduction on data at the cost of tolerable application output error. In this context, the work presented in this article exploits both features by defining approximate communications for error-tolerant applications. We propose a method to design realistic and scalable nanophotonic interconnect supporting approximate data transmission and power adaption according to the communication distance to improve the energy efficiency. For this purpose, the data can be sent by mixing low optical power signal and truncation for the Least Significant Bits (LSB) of the floating-point numbers, while the overall power is adapted according to the communication distance. We define two ranges of communications, short and long, which require only four power levels. This reduces area and power overhead to control the laser output power. A transmission model allows estimating the laser power according to the targeted BER and the number of truncated bits, while the optical network interface allows configuring, at runtime, the number of approximated and truncated bits and the laser output powers. We explore the energy efficiency provided by each communication scheme, and we investigate the error resilience of the benchmarks over several approximation and truncation schemes. The simulation results of ApproxBench applications show that, compared to an interconnect involving only robust communications, approximations in the optical transmission led to up to 53% laser power reduction with a limited degradation at the application level with less than 9% of output error. Finally, we show that our solution is scalable and leads to 10% reduction in the total energy consumption, 35× reduction in the laser driver size, and 10× reduction in the laser controller compared to state-of-the-art solution.",
        "link": "https://dl.acm.org/doi/10.1145/3484309",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "RFClock: timing, phase and frequency synchronization for distributed wireless networks",
        "authors": "['Kubra Alemdar', 'Divashree Varshney', 'Subhramoy Mohanti', 'Ufuk Muncuk', 'Kaushik Chowdhury']",
        "date": "October 2021",
        "source": "MobiCom '21: Proceedings of the 27th Annual International Conference on Mobile Computing and Networking",
        "abstract": "Emerging applications like distributed coordinated beamforming (DCB), intelligent reflector arrays, and networked robotic devices will transform wireless applications. However, for systems-centric work on these topics, the research community must first overcome the hurdle of implementing fine-grained, over-the-air timing synchronization, which is critical for any coordinated operation. To address this gap, this paper presents an open-source design and implementation of 'RFClock' that provides timing, frequency and phase synchronization for software defined radios (SDRs). It shows how RFClock can be used for a practical, 5-node DCB application without modifying existing physical/link layer protocols. By utilizing a leader-follower architecture, RFClock-leader allows follower clocks to synchronize with mean offset under 0.107Hz, and then corrects the time/phase alignment to be within a 5ns deviation. RFClock is designed to operate in generalized environments: as standalone unit, it generates a 10MHz/1PPS signal reference suitable for most commercial-off-the-shelf (COTS) SDRs today; it does not require custom protocol-specific headers or messaging; and it is robust to interference through a frequency-agile operation. Using RFClock for DCB, we verify significant increase in channel gain and low BER in a range of [0 -- 10--3] for different modulation schemes. We also demonstrate performance that is similar to a popular wired solution and significant improvement over a GPS-based solution, while delivering this functionality at a fractional price/power point.",
        "link": "https://dl.acm.org/doi/10.1145/3447993.3448623",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Durable Queues: The Second Amendment",
        "authors": "['Gal Sela', 'Erez Petrank']",
        "date": "July 2021",
        "source": "SPAA '21: Proceedings of the 33rd ACM Symposium on Parallelism in Algorithms and Architectures",
        "abstract": "We consider durable data structures for non-volatile main memory, such as the new Intel Optane memory architecture. Substantial recent work has concentrated on making concurrent data structures durable with low overhead, by adding a minimal number of blocking persist operations (i.e., flushes and fences). In this work we show that focusing on minimizing the number of persist instructions is important, but not enough. We show that access to flushed content is of high cost due to cache invalidation in current architectures. Given this finding, we present a design of the queue data structure that properly takes care of minimizing blocking persist operations as well as minimizing access to flushed content. The proposed design outperforms state-of-the-art durable queues.   We start by providing a durable version of the Michael Scott queue (MSQ ). We amend MSQ by adding a minimal number of persist instructions, fewer than in available durable queues, and meeting the theoretical lower bound on the number of blocking persist operations. We then proceed with a second amendment to this design, that eliminates accesses to flushed data. Evaluation shows that the second amendment yields substantial performance improvement, outperforming the state of the art and demonstrating the importance of reduced accesses to flushed content. The presented queues are durably linearizable and lock-free. Finally, we discuss the theoretical optimal number of accesses to flushed content.",
        "link": "https://dl.acm.org/doi/10.1145/3409964.3461791",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Efficiently detecting concurrency bugs in persistent memory programs",
        "authors": "['Zhangyu Chen', 'Yu Hua', 'Yongle Zhang', 'Luochangqi Ding']",
        "date": "February 2022",
        "source": "ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "Due to the salient DRAM-comparable performance, TB-scale capacity, and non-volatility, persistent memory (PM) provides new opportunities for large-scale in-memory computing with instant crash recovery. However, programming PM systems is error-prone due to the existence of crash-consistency bugs, which are challenging to diagnose especially with concurrent programming widely adopted in PM applications to exploit hardware parallelism. Existing bug detection tools for DRAM-based concurrency issues cannot detect PM crash-consistency bugs because they are oblivious to PM operations and PM consistency. On the other hand, existing PM-specific debugging tools only focus on sequential PM programs and cannot effectively detect crash-consistency issues hidden in concurrent executions.   In order to effectively detect crash-consistency bugs that only manifest in concurrent executions, we propose PMRace, the first PM-specific concurrency bug detection tool. We identify and define two new types of concurrent crash-consistency bugs: PM Inter-thread Inconsistency and PM Synchronization Inconsistency. In particular, PMRace adopts PM-aware and coverage-guided fuzz testing to explore PM program executions. For PM Inter-thread Inconsistency, which denotes the data inconsistency hidden in thread interleavings, PMRace performs PM-aware interleaving exploration and thread scheduling to drive the execution towards executions that reveal such inconsistencies. For PM Synchronization Inconsistency between persisted synchronization variables and program data, PMRace identifies the inconsistency during interleaving exploration. The post-failure validation reduces the false positives that come from custom crash recovery mechanisms. PMRace has found 14 bugs (10 new bugs) in real-world concurrent PM systems including PM-version memcached.",
        "link": "https://dl.acm.org/doi/10.1145/3503222.3507755",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Task-RM: A Resource Manager for Energy Reduction in Task-Parallel Applications under Quality of Service Constraints",
        "authors": "['M. Waqar Azhar', 'Miquel Pericàs', 'Per Stenström']",
        "date": "None",
        "source": "ACM Transactions on Architecture and Code Optimization",
        "abstract": "Improving energy efficiency is an important goal of computer system design. This article focuses on a general model of task-parallel applications under quality-of-service requirements on the completion time. Our technique, called Task-RM, exploits the variance in task execution-times and imbalance between tasks to allocate just enough resources in terms of voltage-frequency and core-allocation so that the application completes before the deadline. Moreover, we provide a solution that can harness additional energy savings with the availability of additional processors. We observe that, for the proposed run-time resource manager to allocate resources, it requires specification of the soft deadlines to the tasks. This is accomplished by analyzing the energy-saving scenarios offline and by providing Task-RM with the performance requirements of the tasks. The evaluation shows an energy saving of 33% compared to race-to-idle and 22% compared to dynamic slack allocation (DSA) with an overhead of less than 1%.",
        "link": "https://dl.acm.org/doi/10.1145/3494537",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Horizontal Side-Channel Vulnerabilities of Post-Quantum Key Exchange and Encapsulation Protocols",
        "authors": "['Furkan Aydin', 'Aydin Aysu', 'Mohit Tiwari', 'Andreas Gerstlauer', 'Michael Orshansky']",
        "date": "None",
        "source": "ACM Transactions on Embedded Computing Systems",
        "abstract": "Key exchange protocols and key encapsulation mechanisms establish secret keys to communicate digital information confidentially over public channels. Lattice-based cryptography variants of these protocols are promising alternatives given their quantum-cryptanalysis resistance and implementation efficiency. Although lattice cryptosystems can be mathematically secure, their implementations have shown side-channel vulnerabilities. But such attacks largely presume collecting multiple measurements under a fixed key, leaving the more dangerous single-trace attacks unexplored. This article demonstrates successful single-trace power side-channel attacks on lattice-based key exchange and encapsulation protocols. Our attack targets both hardware and software implementations of matrix multiplications used in lattice cryptosystems. The crux of our idea is to apply a horizontal attack that makes hypotheses on several intermediate values within a single execution all relating to the same secret, and to combine their correlations for accurately estimating the secret key. We illustrate that the design of protocols combined with the nature of lattice arithmetic enables our attack. Since a straightforward attack suffers from false positives, we demonstrate a novel extend-and-prune procedure to recover the key by following the sequence of intermediate updates during multiplication. We analyzed two protocols, Frodo and FrodoKEM, and reveal that they are vulnerable to our attack. We implement both stand-alone hardware and RISC-V based software realizations and test the effectiveness of the proposed attack by using concrete parameters of these protocols on physical platforms with real measurements. We show that the proposed attack can estimate secret keys from a single power measurement with over 99% success rate.",
        "link": "https://dl.acm.org/doi/10.1145/3476799",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Federated Learning for Internet of Things",
        "authors": "['Tuo Zhang', 'Chaoyang He', 'Tianhao Ma', 'Lei Gao', 'Mark Ma', 'Salman Avestimehr']",
        "date": "November 2021",
        "source": "SenSys '21: Proceedings of the 19th ACM Conference on Embedded Networked Sensor Systems",
        "abstract": "Federated learning can be a promising solution for enabling IoT cybersecurity (i.e., anomaly detection in the IoT environment) while preserving data privacy and mitigating the high communication/storage overhead (e.g., high-frequency data from time-series sensors) of centralized over-the-cloud approaches. In this paper, to further push forward this direction with a comprehensive study in both algorithm and system design, we build FedIoT platform that contains FedDetect algorithm for on-device anomaly data detection and a system design for realistic evaluation of federated learning on IoT devices. Furthermore, the proposed FedDetect learning framework improves the performance by utilizing a local adaptive optimizer (e.g., Adam) and a cross-round learning rate scheduler. In a network of realistic IoT devices (Raspberry PI), we evaluate FedIoT platform and FedDetect algorithm in both model and system performance. Our results demonstrate the efficacy of federated learning in detecting a wider range of attack types occurred at multiple devices. The system efficiency analysis indicates that both end-to-end training time and memory cost are affordable and promising for resource-constrained IoT devices. The source code is publicly available at https://github.com/FedML-AI/FedIoT.",
        "link": "https://dl.acm.org/doi/10.1145/3485730.3493444",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Chimera: A Low-power Reconfigurable Platform for Internet of Things",
        "authors": "['Emekcan Aras', 'Stéphane Delbruel', 'Fan Yang', 'Wouter Joosen', 'Danny Hughes']",
        "date": "None",
        "source": "ACM Transactions on Internet of Things",
        "abstract": "The Internet of Things (IoT) is being deployed in an ever-growing range of applications, from industrial monitoring to smart buildings to wearable devices. Each of these applications has specific computational requirements arising from their networking, system security, and edge analytics functionality. This diversity in requirements motivates the need for adaptable end-devices, which can be re-configured and re-used throughout their lifetime to handle computation-intensive tasks without sacrificing battery lifetime. To tackle this problem, this article presents Chimera, a low-power platform for research and experimentation with reconfigurable hardware for the IoT end-devices. Chimera achieves flexibility and re-usability through an architecture based on a Flash Field Programmable Gate Array (FPGA) with a reconfigurable software stack that enables over-the-air hardware and software evolution at runtime. This adaptability enables low-cost hardware/software upgrades on the end-devices and an increased ability to handle computationally-intensive tasks. This article describes the design of the Chimera hardware platform and software stack, evaluates it through three application scenarios, and reviews the factors that have thus far prevented FPGAs from being utilized in IoT end-devices.",
        "link": "https://dl.acm.org/doi/10.1145/3440995",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "IoT in the Wild: An expedition of discovery for remote monitoring.",
        "authors": "['Graham Coulby', 'Adrian K Clear', 'Oliver Jones', 'Alan Godfrey']",
        "date": "September 2021",
        "source": "UbiComp/ISWC '21 Adjunct: Adjunct Proceedings of the 2021 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2021 ACM International Symposium on Wearable Computers",
        "abstract": "Free-living assessment and remote monitoring is important for healthcare researchers. Moving research beyond the laboratory provides habitual environments for remote assessment that allows research to remain agile even when facing uncontrollable external factors e.g., the SARS-COV-2 pandemic. Emergent technologies have the potential to make this form of assessment feasible by providing accessible and affordable mechanisms for conducting free-living research. This paper presents findings from a study that was halted due to the pandemic, but this work highlighted a series of challenges that may present themselves to researchers conducting similar work. By transparently reporting the challenges and solutions rather than just methods, it is hoped that the lessons learned from this study could provide researchers with greater awareness in future studies.",
        "link": "https://dl.acm.org/doi/10.1145/3460418.3479364",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Vision Paper: Towards Software-Defined Video Analytics with Cross-Camera Collaboration",
        "authors": "['Juheon Yi', 'Chulhong Min', 'Fahim Kawsar']",
        "date": "November 2021",
        "source": "SenSys '21: Proceedings of the 19th ACM Conference on Embedded Networked Sensor Systems",
        "abstract": "Video cameras are becoming ubiquitous in our daily lives. With the recent advancement of Artificial Intelligence (AI), live video analytics are enabling various useful services, including traffic monitoring and campus surveillance. However, current video analytics systems are highly limited in leveraging the enormous opportunities of the deployed cameras due to (i) centralized processing architecture (i.e., cameras are treated as dumb streaming-only sensors), (ii) hard-coded analytics capabilities from tightly coupled hardware and software, (iii) isolated and fragmented camera deployment from different service providers, and (iv) independent processing of camera streams without any collaboration. In this paper, we envision a full-fledged system for software-defined video analytics with cross-camera collaboration that overcomes the aforementioned limitations. We illustrate its detailed system architecture, carefully analyze the key system requirements with representative app scenarios, and derive potential research issues along with a summary of the status quo of existing works.",
        "link": "https://dl.acm.org/doi/10.1145/3485730.3493453",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Enabling Passive Backscatter Tag Localization Without Active Receivers",
        "authors": "['Abeer Ahmad', 'Xiao Sha', 'Milutin Stanaćević', 'Akshay Athalye', 'Petar M. Djurić', 'Samir R. Das']",
        "date": "November 2021",
        "source": "SenSys '21: Proceedings of the 19th ACM Conference on Embedded Networked Sensor Systems",
        "abstract": "Backscattering tags transmit passively without an on-board active radio transmitter. Almost all present-day backscatter systems, however, rely on active radio receivers. This presents a significant scalability, power and cost challenge for backscatter systems. To overcome this barrier, recent research has empowered these passive tags with the ability to reliably receive backscatter signals from other tags. This forms the building block of passive networks wherein tags talk to each other without an active radio on either the transmit or receive side. For wider functionality, accurate localization of such tags is critical. All known backscatter tag localization techniques rely on active receivers for measuring and characterizing the received signal. As a result, they cannot be directly applied to passive tag-to-tag networks. This paper overcomes the gap by developing a localization technique for such passive networks based on a novel method for phase-based ranging in passive receivers. This method allows pairs of passive tags to collaboratively determine the inter-tag channel phase while effectively minimizing the effects of multipath and noise in the surrounding environment. Building on this, we develop a localization technique that benefits from large link diversity uniquely available in a passive tag-to-tag network. We evaluate the performance of our techniques with extensive micro-benchmarking experiments in an indoor environment using fabricated prototypes of tag hardware. We show that our phase-based ranging performs similar to active receivers, providing median 1D ranging error <1 cm and median localization error also <1 cm. Benefiting from the large-scale link diversity our localization technique outperforms several state-of-the-art techniques that use active receivers.",
        "link": "https://dl.acm.org/doi/10.1145/3485730.3485950",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "FULL-W2V: fully exploiting data reuse for W2V on GPU-accelerated systems",
        "authors": "['Thomas Randall', 'Tyler Allen', 'Rong Ge']",
        "date": "June 2021",
        "source": "ICS '21: Proceedings of the ACM International Conference on Supercomputing",
        "abstract": "Word2Vec remains one of the highly-impactful innovations in the field of Natural Language Processing (NLP) that represents latent grammatical and syntactical information in human text with dense vectors in a low dimension. Word2Vec has high computational cost due to the algorithm’s inherent sequentiality, intensive memory accesses, and the large vocabularies it represents. While prior studies have investigated technologies to explore parallelism and improve memory system performance, they struggle to effectively gain throughput on powerful GPUs. We identify memory data access and latency as the primary bottleneck in prior works on GPUs, which prevents highly optimized kernels from attaining the architecture’s peak performance. We present a novel algorithm, FULL-W2V, which maximally exploits the opportunities for data reuse in the W2V algorithm and leverages GPU architecture and resources to reduce access to low memory levels and improve temporal locality. FULL-W2V is capable of reducing accesses to GPU global memory significantly, e.g., by more than 89%, compared to prior state-of-the-art GPU implementations, resulting in significant performance improvement that scales across successive hardware generations. Our prototype implementation achieves 2.97X speedup when ported from Nvidia Pascal P100 to Volta V100 cards, and outperforms the state-of-the-art by 5.72X on V100 cards with the same embedding quality. In-depth analysis indicates that the reduction of memory accesses through register and shared memory caching and high-throughput shared memory reduction leads to a significantly improved arithmetic intensity. FULL-W2V can potentially benefit many applications in NLP and other domains.",
        "link": "https://dl.acm.org/doi/10.1145/3447818.3460373",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Leveraging Automatic High-Level Synthesis Resource Sharing to Maximize Dynamical Voltage Overscaling with Error Control",
        "authors": "['Prattay Chowdhury', 'Benjamin Carrion Schafer']",
        "date": "None",
        "source": "ACM Transactions on Design Automation of Electronic Systems",
        "abstract": "Approximate Computing has emerged as an alternative way to further reduce the power consumption of integrated circuits (ICs) by trading off errors at the output with simpler, more efficient logic. So far the main approaches in approximate computing have been to simplify the hardware circuit by pruning the circuit until the maximum error threshold is met. One of the critical issues, though, is the training data used to prune the circuit. The output error can significantly exceed the maximum error if the final workload does not match the training data. Thus, most previous work typically assumes that training data matches with the workload data distribution. In this work, we present a method that dynamically overscales the supply voltage based on different workload distribution at runtime. This allows to adaptively select the supply voltage that leads to the largest power savings while ensuring that the error will never exceed the maximum error threshold. This approach also allows restoring of the original error-free circuit if no matching workload distribution is found. The proposed method also leverages the ability of High-Level Synthesis (HLS) to automatically generate circuits with different properties by setting different synthesis constraints to maximize the available timing slack and, hence, maximize the power savings. Experimental results show that our proposed method works very well, saving on average 47.08% of power as compared to the exact output circuit and 20.25% more than a traditional approximation method.",
        "link": "https://dl.acm.org/doi/10.1145/3473909",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "CryoWire: wire-driven microarchitecture designs for cryogenic computing",
        "authors": "['Dongmoon Min', 'Yujin Chung', 'Ilkwon Byun', 'Junpyo Kim', 'Jangwoo Kim']",
        "date": "February 2022",
        "source": "ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "Cryogenic computing, which runs a computer device at an extremely low temperature, is promising thanks to its significant reduction of wire resistance as well as leakage current. Recent studies on cryogenic computing have focused on various architectural units including the main memory, cache, and CPU core running at 77K. However, little research has been conducted to fully exploit the fast cryogenic wires, even though the slow wires are becoming more serious performance bottleneck in modern processors. In this paper, we propose a CPU microarchitecture which extensively exploits the fast wires at 77K. For this goal, we first introduce our validated cryogenic-performance models for the CPU pipeline and network on chip (NoC), whose performance can be significantly limited by the slow wires. Next, based on the analysis with the models, we architect CryoSP and CryoBus as our pipeline and NoC designs to fully exploit the fast wires. Our evaluation shows that our cryogenic computer equipped with both microarchitectures achieves 3.82 times higher system-level performance compared to the conventional computer system thanks to the 96% higher clock frequency of CryoSP and five times lower NoC latency of CryoBus.",
        "link": "https://dl.acm.org/doi/10.1145/3503222.3507749",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "AUTO-PRUNE: automated DNN pruning and mapping for ReRAM-based accelerator",
        "authors": "['Siling Yang', 'Weijian Chen', 'Xuechen Zhang', 'Shuibing He', 'Yanlong Yin', 'Xian-He Sun']",
        "date": "June 2021",
        "source": "ICS '21: Proceedings of the ACM International Conference on Supercomputing",
        "abstract": "Emergent ReRAM-based accelerators support in-memory computation to accelerate deep neural network (DNN) inference. Weight matrix pruning of DNNs is a widely used technique to reduce the size of DNN models, thereby reducing the resource and energy consumption of ReRAM-based accelerators. However, conventional works on weight matrix pruning for ReRAM-based accelerators have three major issues. First, they use heuristics or rules from domain experts to prune the weights, leading to suboptimal pruning policies. Second, they mostly focus on improving compression ratio, thus may not meet accuracy constraints. Third, they ignore direct feedback of hardware. In this paper, we introduce an automated DNN pruning and mapping framework, named AUTO-PRUNE. It leverages reinforcement learning (RL) to automatically determine the pruning policy considering the constraint of accuracy loss. The reward function of RL agents is designed using hardware’s direct feedback (i.e., accuracy and compression rate of occupied crossbars). The function directs the search of the pruning ratio of each layer for a global optimum considering the characteristics of individual layers of DNN models. Then AUTO-PRUNE maps the pruned weight matrices to crossbars to store only nontrivial elements. Finally, to avoid the dislocation problem, we design a new data-path in ReRAM-based accelerators to correctly index and feed input to matrix-vector computation leveraging the mechanism of operation units. Experimental results show that, compared to the state-of-the-art work, AUTO-PRUNE achieves up to 3.3X compression rate, 3.1X area efficiency, and 3.3X energy efficiency with a similar or even higher accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3447818.3460366",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Temporal State Machines: Using Temporal Memory to Stitch Time-based Graph Computations",
        "authors": "['Advait Madhavan', 'Matthew W. Daniels', 'Mark D. Stiles']",
        "date": "None",
        "source": "ACM Journal on Emerging Technologies in Computing Systems",
        "abstract": "Race logic, an arrival-time-coded logic family, has demonstrated energy and performance improvements for applications ranging from dynamic programming to machine learning. However, the various ad hoc mappings of algorithms into hardware rely on researcher ingenuity and result in custom architectures that are difficult to systematize. We propose to associate race logic with the mathematical field of tropical algebra, enabling a more methodical approach toward building temporal circuits. This association between the mathematical primitives of tropical algebra and generalized race logic computations guides the design of temporally coded tropical circuits. It also serves as a framework for expressing high-level timing-based algorithms. This abstraction, when combined with temporal memory, allows for the systematic exploration of race logic–based temporal architectures by making it possible to partition feed-forward computations into stages and organize them into a state machine. We leverage analog memristor-based temporal memories to design such a state machine that operates purely on time-coded wavefronts. We implement a version of Dijkstra’s algorithm to evaluate this temporal state machine. This demonstration shows the promise of expanding the expressibility of temporal computing to enable it to deliver significant energy and throughput advantages.",
        "link": "https://dl.acm.org/doi/10.1145/3451214",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Morphy: Software Defined Charge Storage for the IoT",
        "authors": "['Fan Yang', 'Ashok Samraj Thangarajan', 'Sam Michiels', 'Wouter Joosen', 'Danny Hughes']",
        "date": "November 2021",
        "source": "SenSys '21: Proceedings of the 19th ACM Conference on Embedded Networked Sensor Systems",
        "abstract": "Recent innovations in energy harvesting promise extended operational life and reduced maintenance costs for the next generation of Internet of Things (IoT) platforms. However, energy management in these platforms remains problematic due to dynamism in energy supply and demand, inefficiency in storing and converting energy and a lack of per-task charge isolation. This paper tackles this problem by proposing a software defined charge storage module called Morphy, which combines a polymorphic capacitor array with intelligent power management software. Morphy delivers energy to application tasks in a flexible, efficient, and isolated manner. Morphy provides two software extensions to the Operating System scheduler: the energy semaphore blocks the execution of tasks until sufficient charge is available to safely run them, and the energy watchdog monitors and mitigates energy management bugs. We have realized a prototype of Morphy with the hardware form factor of a standard 9V (PP3) battery package and a software library that integrates with the FreeRTOS scheduler. Our evaluation shows that, in comparison to standard energy storage and management approaches, our prototype reaches an operational voltage more quickly, sustains operation longer in the case of power failure and effectively isolates charge storage for dedicated tasks with minimal compute, memory and energy overhead.",
        "link": "https://dl.acm.org/doi/10.1145/3485730.3485947",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "SPARSE: Spatially Aware LFI Resilient State Machine Encoding",
        "authors": "['Choudhury Muhtadi', 'Tajik Shahin', 'Domenic Forte']",
        "date": "October 2021",
        "source": "HASP '21: Proceedings of the 10th International Workshop on Hardware and Architectural Support for Security and Privacy",
        "abstract": "As finite state machines (FSMs) control the behavior of sequential circuits, they can be a target for attacks. With laser-based fault injection (LFI), an adversary may attain unauthorized access to sensitive states by altering the values of individual state flip-flops (FFs). Although standard error correction/detection techniques improve FSM resiliency, all states and FFs of an FSM are assumed equally critical to protect, incurring significant overhead. In this paper, we introduce a novel spatial vulnerability metric to aid the security analysis, which precisely manifests the susceptibility of FSM designs to LFI based on state FF sensitivity and placement. A novel encoding and spatially aware physical design framework (SPARSE) are then proposed that co-optimize the FSM encoding and state FF placement to minimize LFI susceptibility. SPARSE’s encoding uses the minimum number of FFs by placing security-sensitive FFs a sufficient distance apart from other FFs. SPARSE is demonstrated on 5 benchmarks using commercial CAD tools and outperforms other FSM encoding schemes in terms of security, area, and PDP.",
        "link": "https://dl.acm.org/doi/10.1145/3505253.3505254",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "You Only Traverse Twice: A YOTT Placement, Routing, and Timing Approach for CGRAs",
        "authors": "['Michael Canesche', 'Westerley Carvalho', 'Lucas Reis', 'Matheus Oliveira', 'Salles Magalhães', 'Peter Jamieson', 'Jaugusto M. Nacif', 'Ricardo Ferreira']",
        "date": "None",
        "source": "ACM Transactions on Embedded Computing Systems",
        "abstract": "Coarse-grained reconfigurable architecture (CGRA) mapping involves three main steps: placement, routing, and timing. The mapping is an NP-complete problem, and a common strategy is to decouple this process into its independent steps. This work focuses on the placement step, and its aim is to propose a technique that is both reasonably fast and leads to high-performance solutions. Furthermore, a near-optimal placement simplifies the following routing and timing steps. Exact solutions cannot find placements in a reasonable execution time as input designs increase in size. Heuristic solutions include meta-heuristics, such as Simulated Annealing (SA) and fast and straightforward greedy heuristics based on graph traversal. However, as these approaches are probabilistic and have a large design space, it is not easy to provide both run-time efficiency and good solution quality. We propose a graph traversal heuristic that provides the best of both: high-quality placements similar to SA and the execution time of graph traversal approaches. Our placement introduces novel ideas based on “you only traverse twice” (YOTT) approach that performs a two-step graph traversal. The first traversal generates annotated data to guide the second step, which greedily performs the placement, node per node, aided by the annotated data and target architecture constraints. We introduce three new concepts to implement this technique: I/O and reconvergence annotation, degree matching, and look-ahead placement. Our analysis of this approach explores the placement execution time/quality trade-offs. We point out insights on how to analyze graph properties during dataflow mapping. Our results show that YOTT is 60.6\\(\\), 9.7 \\(\\), and 2.3\\(\\) faster than a high-quality SA, bounding box SA VPR, and multi-single traversal placements, respectively. Furthermore, YOTT reduces the average wire length and the maximal FIFO size (additional timing requirement on CGRAs) to avoid delay mismatches in fully pipelined architectures.",
        "link": "https://dl.acm.org/doi/10.1145/3477038",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "An experimental framework for improving the performance of BFT consensus for future permissioned blockchains",
        "authors": "['Man-Kit Sit', 'Manuel Bravo', 'Zsolt István']",
        "date": "June 2021",
        "source": "DEBS '21: Proceedings of the 15th ACM International Conference on Distributed and Event-based Systems",
        "abstract": "Permissioned Blockchains are increasingly considered in enterprise use-cases, many of which do not require geo-distribution, or even disallow it due to legislation. Examples include countrywide networks, such as Alastria, or those deployed using cloud-based platforms such as IBM Blockchain Platform. We expect these blockchains to eventually run in environments with high bandwidth and low latency modern networks, as well as with advanced programmable hardware accelerators. Even though there is renewed interest in BFT consensus algorithms with various proposals targeting Permissioned Blockchains, related work does not optimize for fast networks and does not incorporate hardware accelerators - we make the case that doing so will pay off in the long run. To this end, we re-implemented the seminal PBFT algorithm in a way that allows us to measure different configurations of the protocol. Through this we explore the benefits of various common optimization strategies and show that the protocol is unlikely to saturate more than 10Gbps networks without relying on specialized hardware-based offloading. Based on the experimental results, we discuss two concrete ways in which the cost of consensus in Permissioned Blockchains could be reduced in high-speed networking environments, namely, offloading to SmartNICs and implementing the protocol on standalone FPGAs.",
        "link": "https://dl.acm.org/doi/10.1145/3465480.3466922",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Ginkgo: A Modern Linear Operator Algebra Framework for High Performance Computing",
        "authors": "['Hartwig Anzt', 'Terry Cojean', 'Goran Flegar', 'Fritz Göbel', 'Thomas Grützmacher', 'Pratik Nayak', 'Tobias Ribizel', 'Yuhsiang Mike Tsai', 'Enrique S. Quintana-Ortí']",
        "date": "None",
        "source": "ACM Transactions on Mathematical Software",
        "abstract": "In this article, we present Ginkgo, a modern C++ math library for scientific high performance computing. While classical linear algebra libraries act on matrix and vector objects, Ginkgo’s design principle abstracts all functionality as “linear operators,” motivating the notation of a “linear operator algebra library.” Ginkgo’s current focus is oriented toward providing sparse linear algebra functionality for high performance graphics processing unit (GPU) architectures, but given the library design, this focus can be easily extended to accommodate other algorithms and hardware architectures. We introduce this sophisticated software architecture that separates core algorithms from architecture-specific backends and provide details on extensibility and sustainability measures. We also demonstrate Ginkgo’s usability by providing examples on how to use its functionality inside the MFEM and deal.ii finite element ecosystems. Finally, we offer a practical demonstration of Ginkgo’s high performance on state-of-the-art GPU architectures.",
        "link": "https://dl.acm.org/doi/10.1145/3480935",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Enhancing the Scalability of Multi-FPGA Stencil Computations via Highly Optimized HDL Components",
        "authors": "['Enrico Reggiani', 'Emanuele Del Sozzo', 'Davide Conficconi', 'Giuseppe Natale', 'Carlo Moroni', 'Marco D. Santambrogio']",
        "date": "None",
        "source": "ACM Transactions on Reconfigurable Technology and Systems",
        "abstract": "Stencil-based algorithms are a relevant class of computational kernels in high-performance systems, as they appear in a plethora of fields, from image processing to seismic simulations, from numerical methods to physical modeling. Among the various incarnations of stencil-based computations, Iterative Stencil Loops (ISLs) and Convolutional Neural Networks (CNNs) represent two well-known examples of kernels belonging to the stencil class. Indeed, ISLs apply the same stencil several times until convergence, while CNN layers leverage stencils to extract features from an image. The computationally intensive essence of ISLs, CNNs, and in general stencil-based workloads, requires solutions able to produce efficient implementations in terms of throughput and power efficiency. In this context, FPGAs are ideal candidates for such workloads, as they allow design architectures tailored to the stencil regular computational pattern. Moreover, the ever-growing need for performance enhancement leads FPGA-based architectures to scale to multiple devices to benefit from a distributed acceleration. For this reason, we propose a library of HDL components to effectively compute ISLs and CNNs inference on FPGA, along with a scalable multi-FPGA architecture, based on custom PCB interconnects. Our solution eases the design flow and guarantees both scalability and performance competitive with state-of-the-art works.",
        "link": "https://dl.acm.org/doi/10.1145/3461478",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Ten lessons from three generations shaped Google's TPUv4i",
        "authors": "['Norman P. Jouppi', 'Doe Hyun Yoon', 'Matthew Ashcraft', 'Mark Gottscho', 'Thomas B. Jablin', 'George Kurian', 'James Laudon', 'Sheng Li', 'Peter Ma', 'Xiaoyu Ma', 'Thomas Norrie', 'Nishant Patil', 'Sushma Prasad', 'Cliff Young', 'Zongwei Zhou', 'David Patterson']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "Google deployed several TPU generations since 2015, teaching us lessons that changed our views: semi-conductor technology advances unequally; compiler compatibility trumps binary compatibility, especially for VLIW domain-specific architectures (DSA); target total cost of ownership vs initial cost; support multi-tenancy; deep neural networks (DNN) grow 1.5X annually; DNN advances evolve workloads; some inference tasks require floating point; inference DSAs need air-cooling; apps limit latency, not batch size; and backwards ML compatibility helps deploy DNNs quickly. These lessons molded TPUv4i, an inference DSA deployed since 2020.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00010",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Short Paper: Device- and Locality-Specific Fingerprinting of Shared NISQ Quantum Computers",
        "authors": "['Allen Mi', 'Shuwen Deng', 'Jakub Szefer']",
        "date": "October 2021",
        "source": "HASP '21: Proceedings of the 10th International Workshop on Hardware and Architectural Support for Security and Privacy",
        "abstract": "Fingerprinting of quantum computer devices is a new threat that poses a challenge to shared, cloud-based quantum computers. Fingerprinting can allow adversaries to map quantum computer infrastructures, uniquely identify cloud-based devices which otherwise have no public identifiers, and it can assist other adversarial attacks. This work shows idle tomography-based fingerprinting method based on crosstalk-induced errors in NISQ quantum computers. The device- and locality-specific fingerprinting results show prediction accuracy values of 99.1% and 95.3%, respectively.",
        "link": "https://dl.acm.org/doi/10.1145/3505253.3505261",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "L2D2: low latency distributed downlink for LEO satellites",
        "authors": "['Deepak Vasisht', 'Jayanth Shenoy', 'Ranveer Chandra']",
        "date": "August 2021",
        "source": "SIGCOMM '21: Proceedings of the 2021 ACM SIGCOMM 2021 Conference",
        "abstract": "Large constellations of Low Earth Orbit satellites promise to provide near real-time high-resolution Earth imagery. Yet, getting this large amount of data back to Earth is challenging because of their low orbits and fast motion through space. Centralized architectures with few multi-million dollar ground stations incur large hour-level data download latency and are hard to scale. We propose a geographically distributed ground station design, L2D2, that uses low-cost commodity hardware to offer low latency robust downlink. L2D2 is the first system to use a hybrid ground station model, where only a subset of ground stations are uplink-capable. We design new algorithms for scheduling and rate adaptation that enable low latency and high robustness despite the limitations of the receive-only ground stations. We evaluate L2D2 through a combination of trace-driven simulations and real-world satellite-ground station measurements. Our results demonstrate that L2D2's geographically distributed design can reduce data downlink latency from 90 minutes to 21 minutes.",
        "link": "https://dl.acm.org/doi/10.1145/3452296.3472932",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "CRISP: critical slice prefetching",
        "authors": "['Heiner Litz', 'Grant Ayers', 'Parthasarathy Ranganathan']",
        "date": "February 2022",
        "source": "ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "The high access latency of DRAM continues to be a performance challenge for contemporary microprocessor systems. Prefetching is a well-established technique to address this problem, however, existing implemented designs fail to provide any performance benefits in the presence of irregular memory access patterns. The hardware complexity of prior techniques that can predict irregular memory accesses such as runahead execution has proven untenable for implementation in real hardware. We propose a lightweight mechanism to hide the high latency of irregular memory access patterns by leveraging criticality-based scheduling. In particular, our technique executes delinquent loads and their load slices as early as possible, hiding a significant fraction of their latency. Furthermore, we observe that the latency induced by branch mispredictions and other high latency instructions can be hidden with a similar approach. Our proposal only requires minimal hardware modifications by performing memory access classification, load and branch slice extraction, as well as priority analysis exclusively in software. As a result, our technique is feasible to implement, introducing only a simple new instruction prefix while requiring minimal modifications of the instruction scheduler. Our technique increases the IPC of memory-latency-bound applications by up to 38% and by 8.4% on average.",
        "link": "https://dl.acm.org/doi/10.1145/3503222.3507745",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "AGAPE: anomaly detection with generative adversarial network for improved performance, energy, and security in manycore systems",
        "authors": "['Ke Wang', 'Hao Zheng', 'Yuan Li', 'Jiajun Li', 'Ahmed Louri']",
        "date": "March 2022",
        "source": "DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe",
        "abstract": "The security of manycore systems has become increasingly critical. In system-on-chips (SoCs), Hardware Trojans (HTs) manipulate the functionalities of the routing components to saturate the on-chip network, degrade performance, and result in the leakage of sensitive data. Existing HT detection techniques, including runtime monitoring and state-of-the-art learning-based methods, are unable to timely and accurately identify the implanted HTs, due to the increasingly dynamic and complex nature of on-chip communication behaviors. We propose AGAPE, a novel Generative Adversarial Network (GAN)-based anomaly detection and mitigation method against HTs for secured on-chip communication. AGAPE learns the distribution of the multivariate time series of a number of NoC attributes captured by on-chip sensors under both HT-free and HT-infected working conditions. The proposed GAN can learn the potential latent interactions among different runtime attributes concurrently, accurately distinguish abnormal attacked situations from normal SoC behaviors, and identify the type and location of the implanted HTs. Using the detection results, we apply the most suitable protection techniques to each type of detected HTs instead of simply isolating the entire HT-infected router, with the aim to mitigate security threats as well as reducing performance loss. Simulation results show that AGAPE enhances the HT detection accuracy by 19%, reduces network latency and power consumption by 39% and 30%, respectively, as compared to state-of-the-art security designs.",
        "link": "https://dl.acm.org/doi/10.5555/3539845.3540045",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Speculative taint tracking (STT): a comprehensive protection for speculatively accessed data",
        "authors": "['Jiyong Yu', 'Mengjia Yan', 'Artem Khyzha', 'Adam Morrison', 'Josep Torrellas', 'Christopher W. Fletcher']",
        "date": "December 2021",
        "source": "Communications of the ACM",
        "abstract": "Speculative execution attacks present an enormous security threat, capable of reading arbitrary program data under malicious speculation, and later exfiltrating that data over microarchitectural covert channels. This paper proposes speculative taint tracking (STT), a high security and high performance hardware mechanism to block these attacks. The main idea is that it is safe to execute and selectively forward the results of speculative instructions that read secrets, as long as we can prove that the forwarded results do not reach potential covert channels. The technical core of the paper is a new abstraction to help identify all micro-architectural covert channels, and an architecture to quickly identify when a covert channel is no longer a threat. We further conduct a detailed formal analysis on the scheme in a companion document. When evaluated on SPEC06 workloads, STT incurs 8.5% or 14.5% performance overhead relative to an insecure machine.",
        "link": "https://dl.acm.org/doi/10.1145/3491201",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "GenSeq&#x002B;: A Scalable High-Performance Accelerator for Genome Sequencing",
        "authors": "['Chao Wang', 'Lei Gong', 'Shiming Lei', 'Haijie Fang', 'Xi Li', 'Aili Wang', 'Xuehai Zhou']",
        "date": "None",
        "source": "IEEE/ACM Transactions on Computational Biology and Bioinformatics",
        "abstract": "Genome sequencing is one of the most challenging problems in computational biology and bioinformatics. As a traditional algorithm, the string match meets a challenge with the development of the massive volume of data because of gene sequencing. Surveys show that there will be a huge amount of short read segments during the process of gene sequencing and the need for a highly efficient is urgent. As a classic fast and exact single pattern matching algorithm, Knuth-Morris-Pratt (KMP) algorithm has been demonstrated in network security and computational biology. However, with the increasing amount of data in the modern society, it becomes increasingly important and essential to provide a High-performance implementation of KMP algorithm. In this article, we implement a scalable KMP accelerator based on FPGA, named GeneKMP. The accelerator is composed of different computing units to achieve a pipelined organization for higher throughput with satisfying scalability. A novel programming model is provided to alleviate the burden of the high-level programmers. We provide a greedy-based partitioning algorithm for the software/hardware design paradigms. Experimental results on the state-of-the-art Xilinx FPGA hardware prototype show that our accelerator can achieve up to a promising speedup with insignificant hardware cost and power consumption.",
        "link": "https://dl.acm.org/doi/10.1109/TCBB.2019.2947059",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A Threat Model Analysis of a Mobile Agent-based system on Raspberry Pi",
        "authors": "['Iván García Aguilar', 'Antonio Muñoz Gallego']",
        "date": "August 2021",
        "source": "ARES '21: Proceedings of the 16th International Conference on Availability, Reliability and Security",
        "abstract": "Security is considered one of the critical points in any computer system. Nowadays, a multitude of protocols and computer models are appearing along with new attacks increasing the need to develop solutions. This work focuses on the protection of the agent as well as the information it processes in a distributed environment throughout the network. Mobile agents move between various network-enabled platforms to process the information they manage. To simulate an environment based on the Internet of Things (IoT), a scheme has been presented which details the necessary steps to be carried out by the agent to perform the migration.  Today it was proved that there is no infallible solution that guarantees the security of the whole system. However, the importance of security mechanisms to reduce and/or mitigate security threats is fundamental. This work is a study based on a mobile agent-based approach that travels from host to host. A review of different threats to this particular model is presented. Throughout this work a detailed study is presented based on the migration protocol of the agents, which will be determined by using modeling tools such as Microsoft Modeling Tool (MMT) used in this case, to discover and detail each of the threats presented by this protocol. Additionally, an alternative as a solution according to a protocol that runs thanks to the implementation of hardware elements is proposed, which makes use of a TPM, thus determining which threats are mitigated or solved by implementing such hardware in conjunction with the protocol developed for this purpose.",
        "link": "https://dl.acm.org/doi/10.1145/3465481.3470064",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "CacheRewinder: revoking speculative cache updates exploiting write-back buffer",
        "authors": "['Jongmin Lee', 'Junyeon Lee', 'Taeweon Suh', 'Gunjae Koo']",
        "date": "March 2022",
        "source": "DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe",
        "abstract": "Transient execution attacks are critical security threats since those attacks exploit speculative execution which is an essential architectural solution that can improve the performance of out-of-order processors significantly. Such attacks change cache state by accessing secret data during speculative executions, then the attackers leak the secret information exploiting cache timing side-channels. Even though software patches against transient execution attacks have been proposed, the software solutions significantly slow down the performance of a system. In this paper, we propose CacheRewinder, an efficient hardware-based defense mechanism against transient execution attacks. CacheRewinder prevents leakage of secret information by revoking the cache updates done by speculative executions. To restore the cache state efficiently, CacheRewinder exploits the underutilized write-back buffer space as the temporary storage for victimized cache blocks evicted during speculative executions. Hence, when speculation fails CacheRewinder can quickly restore the cache state using the victim blocks held in the write-back buffer. Our evaluation exhibits CacheRewinder can effectively defend against transient execution attacks. The performance overhead by CacheRewinder is only 0.6%, which is negligible compared to the unprotected baseline processor. CacheRewinder also requires minimal storage cost since it exploits unused writeback buffer entries as storage for evicted cache blocks.",
        "link": "https://dl.acm.org/doi/10.5555/3539845.3539965",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "SILVerIn: Systematic Integrity Verification of Printed Circuit Board Using JTAG Infrastructure",
        "authors": "['Shubhra Deb Paul', 'Swarup Bhunia']",
        "date": "None",
        "source": "ACM Journal on Emerging Technologies in Computing Systems",
        "abstract": "A printed circuit board (PCB) provides necessary mechanical support to an electronic system and acts as a platform for connecting electronic components. Counterfeiting and in-field tampering of PCBs have become significant security concerns in the semiconductor industry as a result of increasing untrusted entities in the supply chain. These counterfeit components may result in performance degradation, profit reduction, and reputation risk for the manufacturers. While Integrated Circuit (IC) level authentication using physical unclonable functions (PUFs) has been widely investigated, countermeasures at the PCB level are scarce. These approaches either suffer from significant overhead issues, or opportunistic counterfeiters can breach them like clockwork. Besides, they cannot be extended to system-level (both chip and PCB together), and their applications are also limited to a specific purpose (i.e., either counterfeiting or tampering). In this article, we introduce SILVerIn, a novel systematic approach to verify the authenticity of all chips used in a PCB as well as the board for combating attacks such as counterfeiting, cloning, and in-field malicious modifications. We develop this approach by utilizing the existing boundary scan architecture (BSA) of modern ICs and PCBs. As a result, its implementation comes at a negligible (∼0.5%) hardware overhead. SILVerIn is integrated into a PCB design during the manufacturing phase. We implement our technique on a custom hardware platform consisting of an FPGA and a microcontroller. We incorporate the industry-standard JTAG (Joint Test Action Group) interface to transmit test data into the BSA and perform hands-on measurement of supply current at both chip and PCB levels on 20 boards. We reconstruct these current values to digital signatures that exhibit high uniqueness, robustness, and randomness features. Our approach manifests strong reproducibility of signatures at different supply voltage levels, even with a low-resolution measurement setup. SILVerIn also demonstrates a high resilience against machine learning-based modeling attacks, with an average prediction accuracy of ∼51%. Finally, we conduct intentional alteration experiments by replacing the on-board FPGA to replicate the scenario of PCB tampering, and the results indicate successful detection of in-field modifications in a PCB.",
        "link": "https://dl.acm.org/doi/10.1145/3460232",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Defining an open source CAD workflow for experimental music and media arts",
        "authors": "['Nicolò Merendino', 'Antonio Rodà']",
        "date": "October 2021",
        "source": "ARTECH 2021: 10th International Conference on Digital and Interactive Arts",
        "abstract": "The practice of designing and building instruments, interfaces and hardware in general, became a crucial part of contemporary audio and media arts productions. This task could benefit from the high performance tools offered by state of the art Open source Computer Aided Design (CAD). Although these applications have reached a good level of maturity, their use in the artistic field is still not so widespread, due to an initial barrier probably caused by a lack of accessible documentation and best practices.  This article aims to analyse and experiment a variety of open source Computer Aided Design (CAD) applications with the goal of further spread the use of open source CAD software among media artists, designers,and researchers in the field of STEAM (Science, Technology, Engineering, Art, Mathematics) applications[17]. Following a research through design approach, we will provide up date guidelines regarding how to design every aspect of an electronic music interface using exclusively open source software. To represent the various topics of our research we defined a hypothetical electronic device, and the workflow will be illustrated by describing and analysing all the steps that is necessary to cover in order to to bring such object from a breadboard overloaded with wires and components to a more stable and reliable prototype.  Open source software can play an important role in terms of democratization and long term sustainability of many initiatives [7], and this article aims to help a vast number of workers and researchers in the field of sound and media art to embrace those virtuous software in their artistic practices.",
        "link": "https://dl.acm.org/doi/10.1145/3483529.3483715",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Satori: efficient and fair resource partitioning by sacrificing short-term benefits for long-term gains",
        "authors": "['Rohan Basu Roy', 'Tirthak Patel', 'Devesh Tiwari']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "Multi-core architectures have enabled data centers to increasingly co-locate multiple jobs to improve resource utilization and lower the operational cost. Unfortunately, naively co-locating multiple jobs may lead to only a modest increase in system throughput. Worse, some users may observe proportionally higher performance degradation compared to other users co-located on the same physical multi-core system. SATORI is a novel strategy to partition multi-core architectural resources to achieve two conflicting goals simultaneously: increasing system throughput and achieving fairness among the co-located jobs.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00031",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "(Mis)managed: A Novel TLB-based Covert Channel on GPUs",
        "authors": "['Ajay Nayak', 'Pratheek B.', 'Vinod Ganapathy', 'Arkaprava Basu']",
        "date": "May 2021",
        "source": "ASIA CCS '21: Proceedings of the 2021 ACM Asia Conference on Computer and Communications Security",
        "abstract": "GPUs are now commonly available in most modern computing platforms. They are increasingly being adopted in cloud platforms and data centers due to their immense computing capability. In response to this growth in usage, manufacturers continuously try to improve GPU hardware by adding new features. However, this increase in usage and the addition of utility-improving features can create new, unexpected attack channels. In this paper, we show that two such features-unified virtual memory (UVM) and multi-process service (MPS)-primarily introduced to improve the programmability and efficiency of GPU kernels have an unexpected consequence-that of creating a novel covert-timing channel via the GPU's translation lookaside buffer (TLB) hierarchy. To enable this covert channel, we first perform experiments to understand the characteristics of TLBs present on a GPU. The use of UVM allows fine-grained management of translations, and helps us discover several idiosyncrasies of the TLB hierarchy, such as three-levels of TLB, coalesced entries. We use this newly-acquired understanding to demonstrate a novel covert channel via the shared TLB. We then leverage MPS to increase the bandwidth of this channel by 40×. Finally, we demonstrate the channel's utility by leaking data from a GPU-accelerated database application.",
        "link": "https://dl.acm.org/doi/10.1145/3433210.3453077",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "HEART: Hybrid Memory and Energy-Aware Real-Time Scheduling for Multi-Processor Systems",
        "authors": "['Mario Günzel', 'Christian Hakert', 'Kuan-Hsun Chen', 'Jian-Jia Chen']",
        "date": "None",
        "source": "ACM Transactions on Embedded Computing Systems",
        "abstract": "Dynamic power management (DPM) reduces the power consumption of a computing system when it idles, by switching the system into a low power state for hibernation. When all processors in the system share the same component, e.g., a shared memory, powering off this component during hibernation is only possible when all processors idle at the same time. For a real-time system, the schedulability property has to be guaranteed on every processor, especially if idle intervals are considered to be actively introduced.In this work, we consider real-time systems with hybrid shared-memory architectures, which consist of shared volatile memory (VM) and non-volatile memory (NVM). Energy-efficient execution is achieved by applying DPM to turn off all memories during the hibernation mode. Towards this, we first explore the hybrid memory architectures and suggest a task model, which features configurable hibernation overheads. We propose a multi-processor procrastination algorithm (HEART), based on partitioned earliest-deadline-first (pEDF) scheduling. Our algorithm facilitates reducing the energy consumption by actively enlarging the hibernation time. It enforces all processors to idle simultaneously without violating the schedulability condition, such that the system can enter the hibernation state, where shared memories are turned off. Throughout extensive evaluation of HEART, we demonstrate (1) the increase in potential hibernation time, respectively the decrease in energy consumption, and (2) that our algorithm is not only more general but also has better performance than the state of the art with respect to energy efficiency in most cases.",
        "link": "https://dl.acm.org/doi/10.1145/3477019",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Maximizing Persistent Memory Bandwidth Utilization for OLAP Workloads",
        "authors": "['Björn Daase', 'Lars Jonas Bollmeier', 'Lawrence Benson', 'Tilmann Rabl']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "Modern database systems for online analytical processing (OLAP) typically rely on in-memory processing. Keeping all active data in DRAM severely limits the data capacity and makes larger deployments much more expensive than disk-based alternatives. Byte-addressable persistent memory (PMEM) is an emerging storage technology that bridges the gap between slow-but-cheap SSDs and fast-but-expensive DRAM. Thus, research and industry have identified it as a promising alternative to pure in-memory data warehouses. However, recent work shows that PMEM's performance is strongly dependent on access patterns and does not always yield good results when simply treated like DRAM. To characterize PMEM's behavior in OLAP workloads, we systematically evaluate PMEM on a large, multi-socket server commonly used for OLAP workloads. Our evaluation shows that PMEM can be treated like DRAM for most read access but must be used differently when writing. To support our findings, we run the Star Schema Benchmark on PMEM and DRAM. We show that PMEM is suitable for large, read-heavy OLAP workloads with an average query runtime slowdown of 1.66x compared to DRAM. Following our evaluation, we present 7 best practices on how to maximize PMEM's bandwidth utilization in future system designs.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3457292",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Yashme: detecting persistency races",
        "authors": "['Hamed Gorjiara', 'Guoqing Harry Xu', 'Brian Demsky']",
        "date": "February 2022",
        "source": "ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "Persistent memory (PM) or Non-Volatile Random-Access Memory (NVRAM) hardware such as Intel’s Optane memory product promises to transform how programs store and manipulate information. Ensuring that persistent memory programs are crash consistent is a major challenge. We present a novel class of crash consistency bugs for persistent memory programs, which we call persistency races. Persistency races can cause non-atomic stores to be made partially persistent. Persistency races arise due to the interaction of standard compiler optimizations with persistent memory semantics.   We present Yashme, the first detector for persistency races. A major challenge is that in order to detect persistency races, the execution must crash in a very narrow window between a store with a persistency race and its corresponding cache flush operation, making it challenging for naïve techniques to be effective. Yashme overcomes this challenge with a novel technique for detecting races in executions that are prefixes of the pre-crash execution. This technique enables Yashme to effectively find persistency races even if the injected crashes do not fall into that window. We have evaluated Yashme on a range of persistent memory benchmarks and have found 26 real persistency races that have never been reported before.",
        "link": "https://dl.acm.org/doi/10.1145/3503222.3507766",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Short Paper: A Quantum Circuit Obfuscation Methodology for Security and Privacy",
        "authors": "['Aakarshitha Suresh', 'Abdullah Ash Saki', 'Mahababul Alam', 'Rasit Onur Topaloglu', 'Swaroop Ghosh']",
        "date": "October 2021",
        "source": "HASP '21: Proceedings of the 10th International Workshop on Hardware and Architectural Support for Security and Privacy",
        "abstract": "In the Noisy Intermediate-Scale Quantum (NISQ) realm, efficient quantum circuit compilation is critical to ensure successful computation. Several third-party compilers are improving the compilation times and depth/gate counts. Untrusted third parties or a particular version of a trusted compiler may allow an attacker to steal, clone, and/or reverse engineer the quantum circuit. We propose to obfuscate quantum circuits by employing dummy CNOT gates to prevent such threats. If the adversary clones the obfuscated design, he/she will get faulty results. We propose a metric-based dummy gate insertion process to ensure maximum corruption of functionality measured using Total Variation Distance (TVD) and validated using IBM’s noisy simulators. Our metric guided dummy gate insertion process achieves TVD of up to 28.83%, and performs 10.14% better than the average TVD and performs within 12.45% of the best obtainable TVD for the test benchmarks. The removal of dummy gates by the designer post-compilation to restore functionality as well as other finer details have been addressed.",
        "link": "https://dl.acm.org/doi/10.1145/3505253.3505260",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Towards All-optical Stochastic Computing Using Photonic Crystal Nanocavities",
        "authors": "['Hassnaa El-Derhalli', 'Léa Constans', 'Sébastien Le Beux', 'Alfredo De Rossi', 'Fabrice Raineri', 'Sofiène Tahar']",
        "date": "None",
        "source": "ACM Journal on Emerging Technologies in Computing Systems",
        "abstract": "Stochastic computing allows a drastic reduction in hardware complexity using serial processing of bit streams. While the induced high computing latency can be overcome using integrated optics technology, the design of realistic optical stochastic computing architectures calls for energy efficient switching devices. Photonics Crystal (PhC) nanocavities are μm2 scale devices offering 100fJ switching operation under picoseconds-scale switching speed. Fabrication process allows controlling the Quality factor of each nanocavity resonance, leading to opportunities to implement architectures involving cascaded gates and multi-wavelength signaling. In this paper, we investigate the design of cascaded gates architecture using nanocavities in the context of stochastic computing. We propose a transmission model considering key nanocavity device parameters, such as Quality factors, resonance wavelength, and switching efficiency. The model is calibrated with experimental measurements. We propose the design of XOR gate and multiplexer. We illustrate the use of the gates to design an edge detection filter. System-level exploration of laser power, bit-stream length and bit-error rate is carried out for the processing of gray-scale images. The results show that the proposed architecture leads to 8.5nJ/pixel energy consumption and 512ns/pixel processing time.",
        "link": "https://dl.acm.org/doi/10.1145/3484871",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Dynamic Reliability Management in Neuromorphic Computing",
        "authors": "['Shihao Song', 'Jui Hanamshet', 'Adarsha Balaji', 'Anup Das', 'Jeffrey L. Krichmar', 'Nikil D. Dutt', 'Nagarajan Kandasamy', 'Francky Catthoor']",
        "date": "None",
        "source": "ACM Journal on Emerging Technologies in Computing Systems",
        "abstract": "Neuromorphic computing systems execute machine learning tasks designed with spiking neural networks. These systems are embracing non-volatile memory to implement high-density and low-energy synaptic storage. Elevated voltages and currents needed to operate non-volatile memories cause aging of CMOS-based transistors in each neuron and synapse circuit in the hardware, drifting the transistor’s parameters from their nominal values. If these circuits are used continuously for too long, the parameter drifts cannot be reversed, resulting in permanent degradation of circuit performance over time, eventually leading to hardware faults. Aggressive device scaling increases power density and temperature, which further accelerates the aging, challenging the reliable operation of neuromorphic systems. Existing reliability-oriented techniques periodically de-stress all neuron and synapse circuits in the hardware at fixed intervals, assuming worst-case operating conditions, without actually tracking their aging at run-time. To de-stress these circuits, normal operation must be interrupted, which introduces latency in spike generation and propagation, impacting the inter-spike interval and hence, performance (e.g., accuracy). We observe that in contrast to long-term aging, which permanently damages the hardware, short-term aging in scaled CMOS transistors is mostly due to bias temperature instability. The latter is heavily workload-dependent and, more importantly, partially reversible. We propose a new architectural technique to mitigate the aging-related reliability problems in neuromorphic systems by designing an intelligent run-time manager (NCRTM), which dynamically de-stresses neuron and synapse circuits in response to the short-term aging in their CMOS transistors during the execution of machine learning workloads, with the objective of meeting a reliability target. NCRTM de-stresses these circuits only when it is absolutely necessary to do so, otherwise reducing the performance impact by scheduling de-stress operations off the critical path. We evaluate NCRTM with state-of-the-art machine learning workloads on a neuromorphic hardware. Our results demonstrate that NCRTM significantly improves the reliability of neuromorphic hardware, with marginal impact on performance.",
        "link": "https://dl.acm.org/doi/10.1145/3462330",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Spiking Neural Networks in Spintronic Computational RAM",
        "authors": "['Hüsrev Cılasun', 'Salonik Resch', 'Zamshed I. Chowdhury', 'Erin Olson', 'Masoud Zabihi', 'Zhengyang Zhao', 'Thomas Peterson', 'Keshab K. Parhi', 'Jian-Ping Wang', 'Sachin S. Sapatnekar', 'Ulya R. Karpuzcu']",
        "date": "None",
        "source": "ACM Transactions on Architecture and Code Optimization",
        "abstract": "Spiking Neural Networks (SNNs) represent a biologically inspired computation model capable of emulating neural computation in human brain and brain-like structures. The main promise is very low energy consumption. Classic Von Neumann architecture based SNN accelerators in hardware, however, often fall short of addressing demanding computation and data transfer requirements efficiently at scale. In this article, we propose a promising alternative to overcome scalability limitations, based on a network of in-memory SNN accelerators, which can reduce the energy consumption by up to 150.25= when compared to a representative ASIC solution. The significant reduction in energy comes from two key aspects of the hardware design to minimize data communication overheads: (1) each node represents an in-memory SNN accelerator based on a spintronic Computational RAM array, and (2) a novel, De Bruijn graph based architecture establishes the SNN array connectivity.",
        "link": "https://dl.acm.org/doi/10.1145/3475963",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A pluggable vector unit for RISC-V vector extension",
        "authors": "['Vincenzo Maisto', 'Alessandro Cilardo']",
        "date": "March 2022",
        "source": "DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe",
        "abstract": "Vector extensions have become increasingly important for accelerating data-parallel applications in areas like multimedia, data-streaming, and Machine Learning. This interactive presentation introduces a microarchitectural design of a vector unit compliant with the RISC-V vector extension v1.0. While we targeted a specific core for demonstration, CVA6, our architecture is designed so as to ensure extensibility, maintainability, and re-usability in other cores. Furthermore, as a distinctive feature, we support speculative execution and precise vector traps. The paper provides an overview of the main motivation, design choices, and implementation details, followed by a qualitative and quantitative discussion of the results collected from the synthesis of the extended CVA6 RISC-V core.",
        "link": "https://dl.acm.org/doi/10.5555/3539845.3540113",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Systolic-Array Deep-Learning Acceleration Exploring Pattern-Indexed Coordinate-Assisted Sparsity for Real-Time On-Device Speech Processing",
        "authors": "['Shiwei Liu', 'Zihao Zhao', 'Yanhong Wang', 'Qiaosha Zou', 'Yiyun Zhang', 'C- J. Richard Shi']",
        "date": "June 2021",
        "source": "GLSVLSI '21: Proceedings of the 2021 on Great Lakes Symposium on VLSI",
        "abstract": "This paper presents a hardware-software co-design for efficient sparse deep neural networks (DNNs) implementation in a regular systolic array for real-time on-device speech processing. The weight pruning format, exploring pattern-based coordinate-assisted (PICA) sparsity, expands the pattern-based pruning into both convolutional neural networks (CNNs) and recurrent neural networks (RNNs). It reduces the index storage overhead as well as avoids accuracy degradation. The proposed systolic accelerator leverages the intrinsic data reuse and locality to accommodate the PICA-based sparsity without using complex data distribution networks. It also supports DNNs with different topologies. By reducing the model size by 16x, PICA sparsification reduces 6.02x index storage overhead while still achieving 20.7% WER in TIMIT dataset. For the pruned WaveNet and LSTM, the accelerator achieves 0.62 and 2.69 TOPS/W energy efficiency, 1.7x to 10x higher than the state-of-the-art.",
        "link": "https://dl.acm.org/doi/10.1145/3453688.3461530",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "SRAM has no chill: exploiting power domain separation to steal on-chip secrets",
        "authors": "['Jubayer Mahmod', 'Matthew Hicks']",
        "date": "February 2022",
        "source": "ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "The abundance of embedded systems and smart devices increases the risk of physical memory disclosure attacks. One such classic non-invasive attack exploits dynamic RAM's temperature-dependent ability to retain information across power cycles---known as a cold boot attack. When exposed to low temperatures, DRAM cells preserve their state for a short time without power, mimicking non-volatile memories in that time frame. Attackers exploit this physical phenomenon to gain access to a system's secrets, leading to data theft from encrypted storage. To prevent cold boot attacks, programmers hide secrets on-chip in Static Random-Access Memory (SRAM); by construction, on-chip SRAM is isolated from external probing and has little intrinsic capacitance, making it robust against cold boot attacks.   While it is the case that SRAM protects against traditional cold boot attacks, we show that there is another way to retain information in on-chip SRAM across power cycles and software changes. This paper presents Volt Boot, an attack that demonstrates a vulnerability of on-chip volatile memories due to the physical separation common to modern system-on-chip power distribution networks. Volt Boot leverages asymmetrical power states (e.g., on vs. off) to force SRAM state retention across power cycles, eliminating the need for traditional cold boot attack enablers, such as low-temperature or intrinsic data retention time. Using several modern ARM Cortex-A devices, we demonstrate the effectiveness of the attack in caches, registers, and iRAMs. Unlike other forms of SRAM data retention attacks, Volt Boot retrieves data with 100% accuracy---without any complex post-processing.",
        "link": "https://dl.acm.org/doi/10.1145/3503222.3507710",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Invenio: Communication Affinity Computation for Low-Latency Microservices",
        "authors": "['Amit Sheoran', 'Sonia Fahmy', 'Puneet Sharma', 'Navin Modi']",
        "date": "December 2021",
        "source": "ANCS '21: Proceedings of the Symposium on Architectures for Networking and Communications Systems",
        "abstract": "Microservices enable rapid service deployment and scaling. Integrating poorly-understood microservice components into Service Function Chains (SFCs) or graphs limits a provider's control over service delivery latency, however. Orchestration frameworks currently instantiate and place myriads of microservice components without knowing the impact of placement decisions on latency. In this paper, we explore challenges that service providers encounter in managing complex SFCs, and propose Invenio to empower providers to effectively place microservices without prior knowledge of service functionality. Invenio correlates user actions with procedure messages in network traces, and computes procedural affinity of communication among microservices for each user action. The procedural affinity values can then be used to make placement decisions to meet latency constraints of individual user actions. Our experiments with two microservice-based cellular network implementations demonstrate that placement with Invenio-computed affinity values significantly reduces failures by bounding message processing latency, resulting in up to 21% performance gain compared to message count-based placement algorithms, and up to 51% gain over default placement.",
        "link": "https://dl.acm.org/doi/10.1145/3493425.3502750",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Trusted-DNN: A TrustZone-based Adaptive Isolation Strategy for Deep Neural Networks",
        "authors": "['Zhuang Liu', 'Ye Lu', 'Xueshuo Xie', 'Yaozheng Fang', 'Zhaolong Jian', 'Tao Li']",
        "date": "July 2021",
        "source": "ACM TURC '21: Proceedings of the ACM Turing Award Celebration Conference - China",
        "abstract": "Deep neural network (DNN) models have been widely deployed on embedded and mobile devices in lots of application fields such as health care, face recognition, driver assistance, etc. These applications usually require privacy or trusted computing protection. However, diverse hardware resources, various transport protocols, and limited computation and storage capacity make it challenging for traditional embedded systems to provide complex security protection mechanism oriented DNN models. To meet the challenges, we propose Trusted-DNN, a TrustZone-based adaptive isolation strategy for DNN models. We first design a normal pattern to exploit TrustZone technology to provide overall protection for running DNNs. To deploy arbitrary DNN models into TrustZone, we then develop a dynamic model partition method, which makes our strategy easily adaptive to various DNN models and devices. Finally, we employ several optimization techniques to reduce the inference latency of Trusted-DNN models. We perform AlexNet on OP-TEE, which is a TrustZone-based secure operating system, based on a Raspberry Pi 3B+ board. The extensive experimental results highlight that the optimized Trusted-DNN can reduce memory footprint by up to 98% compared with the ordinary program and Trusted-DNN only increase inference latency by 22.8%. Our code is available at https://gitee.com/PaintZero/alexnet-tee.",
        "link": "https://dl.acm.org/doi/10.1145/3472634.3472652",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Rhythmic pixel regions: multi-resolution visual sensing system towards high-precision visual computing at low power",
        "authors": "['Venkatesh Kodukula', 'Alexander Shearer', 'Van Nguyen', 'Srinivas Lingutla', 'Yifei Liu', 'Robert LiKamWa']",
        "date": "April 2021",
        "source": "ASPLOS '21: Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "High spatiotemporal resolution can offer high precision for vision applications, which is particularly useful to capture the nuances of visual features, such as for augmented reality. Unfortunately, capturing and processing high spatiotemporal visual frames generates energy-expensive memory traffic. On the other hand, low resolution frames can reduce pixel memory throughput, but reduce also the opportunities of high-precision visual sensing. However, our intuition is that not all parts of the scene need to be captured at a uniform resolution. Selectively and opportunistically reducing resolution for different regions of image frames can yield high-precision visual computing at energy-efficient memory data rates.   To this end, we develop a visual sensing pipeline architecture that flexibly allows application developers to dynamically adapt the spatial resolution and update rate of different \"rhythmic pixel regions\" in the scene. We develop a system that ingests pixel streams from commercial image sensors with their standard raster-scan pixel read-out patterns, but only encodes relevant pixels prior to storing them in the memory. We also present streaming hardware to decode the stored rhythmic pixel region stream into traditional frame-based representations to feed into standard computer vision algorithms. We integrate our encoding and decoding hardware modules into existing video pipelines. On top of this, we develop runtime support allowing developers to flexibly specify the region labels. Evaluating our system on a Xilinx FPGA platform over three vision workloads shows 43-64% reduction in interface traffic and memory footprint, while providing controllable task accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3445814.3446737",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "High-Performance Sparse Linear Algebra on HBM-Equipped FPGAs Using HLS: A Case Study on SpMV",
        "authors": "['Yixiao Du', 'Yuwei Hu', 'Zhongchun Zhou', 'Zhiru Zhang']",
        "date": "February 2022",
        "source": "FPGA '22: Proceedings of the 2022 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays",
        "abstract": "Sparse linear algebra operators are memory bound due to low compute to memory access ratio and irregular data access patterns. The exceptional bandwidth improvement provided by the emerging high-bandwidth memory (HBM) technologies, coupled with the ability of FPGAs to customize the memory hierarchy and compute engines, brings the potential to significantly boost the performance of sparse linear algebra operators. In this paper we identify four challenges when developing high-performance sparse linear algebra accelerators on HBM-equipped FPGAs --- low HBM bandwidth utilization with conventional sparse storage, limited on-chip memory capacity being the bottleneck when scaling to multiple HBM channels, low compute occupancy due to bank conflicts and inter-iteration carried dependencies, and timing closure on multi-die heterogeneous fabrics. We conduct an in-depth case study on sparse matrix-vector multiplication (SpMV) to explore techniques that tackle the four challenges. These techniques include (1) a customized sparse matrix format tailored for HBMs, (2) a scalable on-chip buffer design that combines replication and banking, (3) best practices of using HLS to implement hardware modules that dynamically resolve bank conflicts and carried dependencies for achieving high compute occupancy, and (4) a split-kernel design methodology for frequency optimization. Using the techniques, we demonstrate HiSparse, a high-performance SpMV accelerator on a multi-die HBM-equipped FPGA device. We evaluated HiSparse on a variety of matrix datasets. The results show that HiSparse achieves a high frequency and delivers promising speedup with increased bandwidth efficiency when compared to prior arts on CPUs, GPUs, and FPGAs. HiSparse is available at https://github.com/cornell-zhang/HiSparse.",
        "link": "https://dl.acm.org/doi/10.1145/3490422.3502368",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "KickTree: A Recursive Algorithmic Scheme for Packet Classification with Bounded Worst-Case Performance",
        "authors": "['Yao Xin', 'Yuxi Liu', 'Wenjun Li', 'Ruyi Yao', 'Yang Xu', 'Yi Wang']",
        "date": "December 2021",
        "source": "ANCS '21: Proceedings of the Symposium on Architectures for Networking and Communications Systems",
        "abstract": "As a promising alternative to TCAM-based solutions for packet classification, FPGA has received increasing attention. Although extensive research has been conducted in this area, existing FPGA-based packet classifiers cannot satisfy the burgeoning needs from OpenFlow, which demands large-scale rule sets and frequent rule updates. As a recently proposed hardware-specific approach, TabTree avoids rule replication and supports dynamic rule update. However, it still faces problems of unbalanced rule subset partition, unevenly distributed subtrees and excessive TSS leaf nodes when implemented on FPGA. In this paper, we propose a hardware-friendly packet classification approach called KickTree, which is elaborated by considering hardware properties. To take advantage of intrinsic parallelism of FPGA, KickTree adopts multiple balanced decision trees which can run simultaneously. The bit selection is more flexible which breaks the restriction of rule subset. Moreover, each subset size is strictly limited, leading to bounded and evenly-distributed",
        "link": "https://dl.acm.org/doi/10.1145/3493425.3502752",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A Composable Monitoring System for Heterogeneous Embedded Platforms",
        "authors": "['Giacomo Valente', 'Tiziana Fanni', 'Carlo Sau', 'Tania Di Mascio', 'Luigi Pomante', 'Francesca Palumbo']",
        "date": "None",
        "source": "ACM Transactions on Embedded Computing Systems",
        "abstract": "Advanced computations on embedded devices are nowadays a must in any application field. Often, to cope with such a need, embedded systems designers leverage on complex heterogeneous reconfigurable platforms that offer high performance, thanks to the possibility of specializing/customizing some computing elements on board, and are usually flexible enough to be optimized at runtime. In this context, monitoring the system has gained increasing interest. Ideally, monitoring systems should be non-intrusive, serve several purposes, and provide aggregated information about the behavior of the different system components. However, current literature is not close to such ideality: For example, existing monitoring systems lack in being applicable to modern heterogeneous platforms. This work presents a hardware monitoring system that is intended to be minimally invasive on system performance and resources, composable, and capable of providing to the user homogeneous observability and transparent access to the different components of a heterogeneous computing platform, so system metrics can be easily computed from the aggregation of the collected information. Building on a previous work, this article is primarily focused on the extension of an existing hardware monitoring system to cover also specialized coprocessing units, and the assessment is done on a Xilinx FPGA-based System on Programmable Chip. Different explorations are presented to explain the level of customizability of the proposed hardware monitoring system, the tradeoffs available to the user, and the benefits with respect to standard de facto monitoring support made available by the targeted FPGA vendor.",
        "link": "https://dl.acm.org/doi/10.1145/3461647",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Flare: flexible in-network allreduce",
        "authors": "['Daniele De Sensi', 'Salvatore Di Girolamo', 'Saleh Ashkboos', 'Shigang Li', 'Torsten Hoefler']",
        "date": "November 2021",
        "source": "SC '21: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",
        "abstract": "The allreduce operation is one of the most commonly used communication routines in distributed applications. To improve its bandwidth and to reduce network traffic, this operation can be accelerated by offloading it to network switches, that aggregate the data received from the hosts, and send them back the aggregated result. However, existing solutions provide limited customization opportunities and might provide suboptimal performance when dealing with custom operators and data types, with sparse data, or when reproducibility of the aggregation is a concern. To deal with these problems, in this work we design a flexible programmable switch by using as a building block PsPIN, a RISC-V architecture implementing the sPIN programming model. We then design, model, and analyze different algorithms for executing the aggregation on this architecture, showing performance improvements compared to state-of-the-art approaches.",
        "link": "https://dl.acm.org/doi/10.1145/3458817.3476178",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Specializing FGPU for Persistent Deep Learning",
        "authors": "['Rui Ma', 'Jia-Ching Hsu', 'Tian Tan', 'Eriko Nurvitadhi', 'David Sheffield', 'Rob Pelt', 'Martin Langhammer', 'Jaewoong Sim', 'Aravind Dasu', 'Derek Chiou']",
        "date": "None",
        "source": "ACM Transactions on Reconfigurable Technology and Systems",
        "abstract": "Overlay architectures are a good way to enable fast development and debug on FPGAs at the expense of potentially limited performance compared to fully customized FPGA designs. When used in concert with hand-tuned FPGA solutions, performant overlay architectures can improve time-to-solution and thus overall productivity of FPGA solutions. This work tunes and specializes FGPU, an open source OpenCL-programmable GPU overlay for FPGAs. We demonstrate that our persistent deep learning (PDL)-FGPU architecture maintains the ease-of-programming and generality of GPU programming while achieving high performance from specialization for the persistent deep learning domain. We also propose an easy method to specialize for other domains. PDL-FGPU includes new instructions, along with micro-architecture and compiler enhancements. We evaluate both the FGPU baseline and the proposed PDL-FGPU on a modern high-end Intel Stratix 10 2800 FPGA in simulation running persistent DL applications (RNN, GRU, LSTM), and non-DL applications to demonstrate generality. PDL-FGPU requires 1.4–3× more ALMs, 4.4–6.4× more M20ks, and 1–9.5× more DSPs than baseline, but improves performance by 56–693× for PDL applications with an average 23.1% degradation on non-PDL applications. We integrated the PDL-FGPU overlay into Intel OPAE to measure real-world performance/power and demonstrate that PDL-FGPU is only 4.0–10.4× slower than the Nvidia V100.",
        "link": "https://dl.acm.org/doi/10.1145/3457886",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Analyzing Security Vulnerabilities Induced by High-level Synthesis",
        "authors": "['Nitin Pundir', 'Sohrab Aftabjahani', 'Rosario Cammarota', 'Mark Tehranipoor', 'Farimah Farahmandi']",
        "date": "None",
        "source": "ACM Journal on Emerging Technologies in Computing Systems",
        "abstract": "High-level synthesis (HLS) is essential to map the high-level language (HLL) description (e.g., in C/C++) of hardware design to the corresponding Register Transfer Level (RTL) to produce hardware-independent design specifications with reduced design complexity for ASICs and FPGAs. Adopting HLS is crucial for industrial and government applications to lower development costs, verification efforts, and time-to-market. Current research practices focus on optimizing HLS for performance, power, and area constraints. However, the literature does not include an analysis of the security implications carried through HLS-generated RTL translations (e.g., from an untimed high-level sequential specification to a fully scheduled implementation). This article demonstrates the evidence of security vulnerabilities that emerge during the HLS translation of a high-level description of system-on-chip (SoC) intellectual properties to their corresponding RTL. The evidence provided in this manuscript highlights the need for (a) guidelines for high-level programmers to prevent these security issues at the design time and (b) automated HLS verification solutions that cover security in their optimization flow.",
        "link": "https://dl.acm.org/doi/10.1145/3492345",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "On the role of system software in energy management of neuromorphic computing",
        "authors": "['Twisha Titirsha', 'Shihao Song', 'Adarsha Balaji', 'Anup Das']",
        "date": "May 2021",
        "source": "CF '21: Proceedings of the 18th ACM International Conference on Computing Frontiers",
        "abstract": "Neuromorphic computing systems such as DYNAPs and Loihi have recently been introduced to the computing community to improve performance and energy efficiency of machine learning programs, especially those that are implemented using Spiking Neural Network (SNN). The role of a system software for neuromorphic systems is to cluster a large machine learning model (e.g., with many neurons and synapses) and map these clusters to the computing resources of the hardware. In this work, we formulate the energy consumption of a neuromorphic hardware, considering the power consumed by neurons and synapses, and the energy consumed in communicating spikes on the interconnect. Based on such formulation, we first evaluate the role of a system software in managing the energy consumption of neuromorphic systems. Next, we formulate a simple heuristic-based mapping approach to place the neurons and synapses onto the computing resources to reduce energy consumption. We evaluate our approach with 10 machine learning applications and demonstrate that the proposed mapping approach leads to a significant reduction of energy consumption of neuromorphic computing systems.",
        "link": "https://dl.acm.org/doi/10.1145/3457388.3458664",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Two ICS Security Datasets and Anomaly Detection Contest on the HIL-based Augmented ICS Testbed",
        "authors": "['Hyeok-Ki Shin', 'Woomyo Lee', 'Jeong-Han Yun', 'Byung-Gil Min']",
        "date": "August 2021",
        "source": "CSET '21: Proceedings of the 14th Cyber Security Experimentation and Test Workshop",
        "abstract": "Security datasets with various operating characteristics and abnormal situations of industrial control system (ICS) are essential to develop artificial intelligence (AI)-based control system security technology. In this study, we built a hardware-in-the-loop (HIL)-based augmented ICS (HAI) testbed and developed ICS security datasets. Here, we introduce the second dataset (HAI 21.03), which was developed with the user feedback of the first released version (HAI 20.07). All HAI datasets are publicly available at https://github.com/icsdataset/hai. HAI 21.03 was expanded by adding data points and normal/attack scenarios to HAI 20.07. We also held an AI-based anomaly detection contest (HAICon 2020) utilizing the HAI datasets developed so far, giving many AI researchers an opportunity to discuss and share ideas for ICS anomaly detection research. This paper presents the results of the HAICon 2020. The results of the top teams in the competition can be used as a performance comparison criterion when using HAI 21.03.",
        "link": "https://dl.acm.org/doi/10.1145/3474718.3474719",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Reservoir Computing Using Networks of CMOS Logic Gates",
        "authors": "['Heidi Komkov', 'Liam Pocher', 'Alessandro Restelli', 'Brian Hunt', 'Daniel Lanthrop']",
        "date": "July 2021",
        "source": "ICONS 2021: International Conference on Neuromorphic Systems 2021",
        "abstract": "Reservoir computing is a brain-inspired architecture for machine learning, capable of rapid time series processing. A recurrent neural network called a reservoir is created, and a simple trained readout map is applied to the reservoir state. A real-world dynamical system at the edge of criticality can be used as a reservoir for information processing in place of a software model. In this paper we test the dynamics of a reservoir comprised of discrete digital logic chips on a printed circuit board. The logic gates run freely without a clock, exhibiting complex behavior that expands an input into a higher dimensional representation. By testing these circuits in a dataset-agnostic manner, we identify promising configurations for machine learning. We demonstrate that the reservoir circuit substantially improves the accuracy of a simple classifier on a noisy waveform classification machine learning task.",
        "link": "https://dl.acm.org/doi/10.1145/3477145.3477163",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "An instruction-level power and energy model for the rocket chip generator",
        "authors": "['Zhiping Wang', 'W. Rhett Davis']",
        "date": "July 2021",
        "source": "ISLPED '21: Proceedings of the ACM/IEEE International Symposium on Low Power Electronics and Design",
        "abstract": "As digital systems become more power and energy constrained, the need for optimizing these quantities early in the design process grows ever more important. Fast and accurate power and energy models are needed for complex hardware blocks, such as processor cores, in order to optimize systems that contain these blocks. Today accurate energy/power estimation can be achieved only after physical design is complete, which is too late to affect the system architecture. This paper demonstrates the development of a fast instruction-level model for the Rocket Chip Generator to facilitate power- and energy-efficient software optimization. We first discuss an event-based power modeling methodology which is the foundation of our model and is compatible with emerging power- and energy-modeling standards such as IEEE-2416. Detailed energy characterization for basic events is explained along with an evaluation of a model with and without cache-fill events. The validation results show that the proposed instruction-level power model achieves less than 3% error on simple C program benchmarks.",
        "link": "https://dl.acm.org/doi/10.1109/ISLPED52811.2021.9502485",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Quantifying the design-space tradeoffs in autonomous drones",
        "authors": "['Ramyad Hadidi', 'Bahar Asgari', 'Sam Jijina', 'Adriana Amyette', 'Nima Shoghi', 'Hyesoon Kim']",
        "date": "April 2021",
        "source": "ASPLOS '21: Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "With fully autonomous flight capabilities coupled with user-specific applications, drones, in particular quadcopter drones, are becoming prevalent solutions in myriad commercial and research contexts. However, autonomous drones must operate within constraints and design considerations that are quite different from any other compute-based agent. At any given time, a drone must arbitrate among its limited compute, energy, and electromechanical resources. Despite huge technological advances in this area, each of these problems has been approached in isolation and drone systems design-space tradeoffs are largely unknown. To address this knowledge gap, we formalize the fundamental drone subsystems and find how computations impact this design space. We present a design-space exploration of autonomous drone systems and quantify how we can provide productive solutions. As an example, we study widely used simultaneous localization and mapping (SLAM) on various platforms and demonstrate that optimizing SLAM on FPGA is more fruitful for the drones. Finally, to address the lack of publicly available experimental drones, we release our open-source drone that is customizable across the hardware-software stack.",
        "link": "https://dl.acm.org/doi/10.1145/3445814.3446721",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "SKT: a one-pass multi-sketch data analytics accelerator",
        "authors": "['Monica Chiosa', 'Thomas B. Preußer', 'Gustavo Alonso']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Data analysts often need to characterize a data stream as a first step to its further processing. Some of the initial insights to be gained include, e.g., the cardinality of the data set and its frequency distribution. Such information is typically extracted by using sketch algorithms, now widely employed to process very large data sets in manageable space and in a single pass over the data. Often, analysts need more than one parameter to characterize the stream. However, computing multiple sketches becomes expensive even when using high-end CPUs. Exploiting the increasing adoption of hardware accelerators, this paper proposes SKT, an FPGA-based accelerator that can compute several sketches along with basic statistics (average, max, min, etc.) in a single pass over the data. SKT has been designed to characterize a data set by calculating its cardinality, its second frequency moment, and its frequency distribution. The design processes data streams coming either from PCIe or TCP/IP, and it is built to fit emerging cloud service architectures, such as Microsoft's Catapult or Amazon's AQUA. The paper explores the trade-offs of designing sketch algorithms on a spatial architecture and how to combine several sketch algorithms into a single design. The empirical evaluation shows how SKT on an FPGA offers a significant performance gain over high-end, server-class CPUs.",
        "link": "https://dl.acm.org/doi/10.14778/3476249.3476287",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Research on the Development of Key Technologies of Tactical Edge Cloud",
        "authors": "['Sicong Yu', 'Huiji Zheng', 'Yang Fan', 'Caihong Ma']",
        "date": "November 2021",
        "source": "ICISE '21: Proceedings of the 6th International Conference on Information Systems Engineering",
        "abstract": "In view of the current situation that the traditional centralized cloud service architecture cannot meet the high requirements of tactical edge for data processing and storage, this paper introduces an emerging tactical cloud service model——tactical edge cloud, and summarizes the status quo of technology development around two key aspects: tactical edge cloud architecture design and computing offloading, and finally related problems to be solved are put forward to provide reference for the development of cloud service architecture of tactical edge.",
        "link": "https://dl.acm.org/doi/10.1145/3503928.3508348",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Listen to Your Heart: Evaluation of the Cardiologic Ecosystem",
        "authors": "['Endres Puschner', 'Christoph Saatjohann', 'Markus Willing', 'Christian Dresen', 'Julia Köbe', 'Benjamin Rath', 'Christof Paar', 'Lars Eckardt', 'Uwe Haverkamp', 'Sebastian Schinzel']",
        "date": "August 2021",
        "source": "ARES '21: Proceedings of the 16th International Conference on Availability, Reliability and Security",
        "abstract": "Modern implantable cardiologic devices communicate via radio frequency techniques and nearby gateways to a backend server on the internet. Those implanted devices, gateways, and servers form an ecosystem of proprietary hardware and protocols that process sensitive medical data and is often vital for patients’ health.  This paper analyzes the security of this Ecosystem, from technical gateway aspects, via the programmer, to configure the implanted device, up to the processing of personal medical data from large cardiological device producers. Based on a real-world attacker model, we evaluated different devices and found several severe vulnerabilities. Furthermore, we could purchase a fully functional programmer for implantable cardiological devices, allowing us to re-program such devices or even induce electric shocks on untampered implanted devices.  Additionally, we sent several Art. 15 and Art. 20 GDPR inquiries to manufacturers of implantable cardiologic devices, revealing non-conforming processes and a lack of awareness about patients’ rights and companies’ obligations. This, and the fact that many vulnerabilities are still to be found after many vulnerability disclosures in recent years, present a worrying security state of the whole ecosystem.",
        "link": "https://dl.acm.org/doi/10.1145/3465481.3465753",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Loopback strategy for in-vehicle network processing in automotive gateway network on chip",
        "authors": "['Angela Gonzalez Mariño', 'Francesc Fons', 'Zhang Haigang', 'Juan Manuel Moreno Arostegui']",
        "date": "October 2021",
        "source": "NoCArc '21: Proceedings of the 14th International Workshop on Network on Chip Architectures",
        "abstract": "In this work, authors introduce an innovative loopback strategy for In-Vehicle Network (IVN) processing in automotive gateway (GW) Network on Chip. The new proposed architecture is fully HW centric, and allows performing any IVN processing algorithms without intervention from the CPU. In essence, the loopback strategy allows for adapting the number of stages in the pipeline of the processing stage by betting on the centralization of the processing resources and recirculating frames from output to input when further stages are needed. It permits even to select to which stage to send them depending on the processing required, optimizing thus the number of stages traversed by frames and consequently reducing latency. The processing unit is built as a stack of parallel tasks with the required interconnection resources that allow performing any processing over any frame, and to handle several frames in parallel. With this architecture, the GW data path is fully adaptable per frame, optimizing latency and Quality of Service, allowing for fulfilling the high demanding requirements of future IVNs.",
        "link": "https://dl.acm.org/doi/10.1145/3477231.3490429",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Getting to the HART of the Matter: An Evaluation of Real-World Safety System OT/IT Interfaces, Attacks, and Countermeasures",
        "authors": "['Laura Tinnel', 'Mike Cochrane']",
        "date": "August 2021",
        "source": "CSET '21: Proceedings of the 14th Cyber Security Experimentation and Test Workshop",
        "abstract": "This paper discusses our experience evaluating attack paths and security controls in commonly used, real-world ICS safety system architectures. Specifically, we sought to determine if an SIS-mediated architecture could provide better protection against unauthorized and malicious safety instrument configuration changes than could a MUX-mediated architecture.  An assessment question-driven approach was layered on top of standard penetration assessment methods. Test cases were planned around the questions and a sample set of vendor products typically used in the oil and gas sector. Four systems were composed from different product subsets and were assessed using the test cases. We analyzed results from the four assessments to illuminate issues that existed regardless of system composition.  Analysis revealed recurring vulnerabilities that exist in all safety systems due to issues in the design of safety instruments and the HART protocol. We found that device-native hardware write-protections provide the best defense, followed by SIS write protections. We concluded that, when using SIS security controls, an SIS-mediated system can protect against unauthorized device reconfigurations better than can a MUX-based system. When SIS security controls are not used, there is no added security benefit.  We present lessons learned for ICS stakeholders and for people who are interested in conducting this kind of evaluation.",
        "link": "https://dl.acm.org/doi/10.1145/3474718.3474726",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Fragments of the Past: Curating Peer Support with Perpetrators of Domestic Violence",
        "authors": "['Rosanna Bellini', 'Alexander Wilson', 'Jan David Smeddinck']",
        "date": "May 2021",
        "source": "CHI '21: Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems",
        "abstract": "There is growing evidence that digital peer-support networks can have a positive influence on behaviour change and wellbeing outcomes for people who harm themselves and others. However, making and sustaining such networks are subject to ethical and pragmatic challenges, particularly for perpetrators of domestic violence whom pose unique risks when brought together. In this work we report on a ten-month study where we worked with six support workers and eighteen perpetrators in the design and deployment of Fragments of the Past; a socio-material system that connects audio messages with tangible artefacts. We share how crafting digitally-augmented artefacts - ‘fragments’ - of experiences of desisting from violence can translate messages for motivation and rapport between peers, without subjecting the process to risks inherent with direct inter-personal communication. These insights provide the basis for practical considerations for future network design with challenging populations.",
        "link": "https://dl.acm.org/doi/10.1145/3411764.3445611",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "SOSP: A SmartNIC-based Offloading Framework for Cloud Storage Pooling",
        "authors": "['Yan Mu', 'Kehan Yao', 'Yang Li', 'Zhiqiang Li', 'Tao Sun', 'Lu Lu', 'Jian He', 'Mingfei Huang']",
        "date": "January 2022",
        "source": "icWCSN '22: Proceedings of the 2022 9th International Conference on Wireless Communication and Sensor Networks",
        "abstract": "As Moore's Law is gradually reaching its limitation, traditional CPU-centric computing architecture cannot meet the ever growing computational requirements, especially in large distributed data centers. There is a growing consensus in the industry that the future architecture is a data-centric fabric which can better integrate the function of computation, storage and network. In order to reduce the pressure on CPUs in data centers and further facilitate the transformation of the entire architecture towards higher flexibility and efficiency, this paper presents a SmartNIC-based offloading framework for storage pooling (SOSP) by leveraging the functionality of SmartNICs to offload both the local and remote storage services from CPUs. Our experiments show that the SmartNIC-based solution SOSP can increase the I/O random Read/Write operation performance by around 20-25% and 13-15% respectively, and at the meantime, free up host CPU cores by saving 16.7% virtual cores compared to the solution without using SmartNICs.",
        "link": "https://dl.acm.org/doi/10.1145/3514105.3514110",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A Real-Time Deep Learning OFDM Receiver",
        "authors": "['Stefan Brennsteiner', 'Tughrul Arslan', 'John Thompson', 'Andrew McCormick']",
        "date": "None",
        "source": "ACM Transactions on Reconfigurable Technology and Systems",
        "abstract": "Machine learning in the physical layer of communication systems holds the potential to improve performance and simplify design methodology. Many algorithms have been proposed; however, the model complexity is often unfeasible for real-time deployment. The real-time processing capability of these systems has not been proven yet. In this work, we propose a novel, less complex, fully connected neural network to perform channel estimation and signal detection in an orthogonal frequency division multiplexing system. The memory requirement, which is often the bottleneck for fully connected neural networks, is reduced by ≈ 27 times by applying known compression techniques in a three-step training process. Extensive experiments were performed for pruning and quantizing the weights of the neural network detector. Additionally, Huffman encoding was used on the weights to further reduce memory requirements. Based on this approach, we propose the first field-programmable gate array based, real-time capable neural network accelerator, specifically designed to accelerate the orthogonal frequency division multiplexing detector workload. The accelerator is synthesized for a Xilinx RFSoC field-programmable gate array, uses small-batch processing to increase throughput, efficiently supports branching neural networks, and implements superscalar Huffman decoders.",
        "link": "https://dl.acm.org/doi/10.1145/3494049",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Demystifying memory access patterns of FPGA-based graph processing accelerators",
        "authors": "['Jonas Dann', 'Daniel Ritter', 'Holger Fröning']",
        "date": "June 2021",
        "source": "GRADES-NDA '21: Proceedings of the 4th ACM SIGMOD Joint International Workshop on Graph Data Management Experiences &amp; Systems (GRADES) and Network Data Analytics (NDA)",
        "abstract": "Recent advances in reprogrammable hardware (e. g., FPGAs) and memory technology (e. g., DDR4, HBM) promise to solve performance problems inherent to graph processing like irregular memory access patterns on traditional hardware (e. g., CPU). While several of these graph accelerators were proposed in recent years, it remains difficult to assess their performance and compare them on common graph workloads and accelerator platforms, due to few open source implementations and excessive implementation effort. In this work, we build on a simulation environment for graph processing accelerators, to make several existing accelerator approaches comparable. This allows us to study relevant performance dimensions such as partitioning schemes and memory technology, among others. The evaluation yields insights into the strengths and weaknesses of current graph processing accelerators along these dimensions, and features a novel in-depth comparison.",
        "link": "https://dl.acm.org/doi/10.1145/3461837.3464512",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A Guideline on Pseudorandom Number Generation (PRNG) in the IoT",
        "authors": "['Peter Kietzmann', 'Thomas C. Schmidt', 'Matthias Wählisch']",
        "date": "None",
        "source": "ACM Computing Surveys",
        "abstract": "Random numbers are an essential input to many functions on the Internet of Things (IoT). Common use cases of randomness range from low-level packet transmission to advanced algorithms of artificial intelligence as well as security and trust, which heavily rely on unpredictable random sources. In the constrained IoT, though, unpredictable random sources are a challenging desire due to limited resources, deterministic real-time operations, and frequent lack of a user interface.In this article, we revisit the generation of randomness from the perspective of an IoT operating system (OS) that needs to support general purpose or crypto-secure random numbers. We analyze the potential attack surface, derive common requirements, and discuss the potentials and shortcomings of current IoT OSs. A systematic evaluation of current IoT hardware components and popular software generators based on well-established test suits and on experiments for measuring performance give rise to a set of clear recommendations on how to build such a random subsystem and which generators to use.",
        "link": "https://dl.acm.org/doi/10.1145/3453159",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Efficient Fruit and Vegetable Classification and Counting for Retail Applications Using Deep Learning",
        "authors": "['Kirill Bogomasov', 'Stefan Conrad']",
        "date": "November 2021",
        "source": "ICAAI '21: Proceedings of the 5th International Conference on Advances in Artificial Intelligence",
        "abstract": "The process of manual classification and counting of fruits and vegetables, from the moment the customer places items on the conveyor belt to their weighing by the cashier on the checkout scale is time consuming and may be burdensome for cashiers, who need to look up or remember the identification code for each product. Not any more: We built a real-life application, which is capable of doing both tasks simultaneously. The presented research is focused on a case that is attractive for its practical applications, in which data is expanded by product weight information. We approach the problem as that of estimating the object count as a classification task and evade the more resource consuming object detection. We introduce a new hybrid architecture which is an ensemble of EfficientNet [31] for image classification and a Decision Tree [3] for object counting based on weight and previous classification result. The trained architecture provides accurate object count and requires fewer resources and less time than current object detection architectures. The proposed architecture accomplishes a counting accuracy of around 80% and an inference time of 0.2 sec. per image. It is a good candidate for handling huge amount of visual information involving fast processing on a CPU.",
        "link": "https://dl.acm.org/doi/10.1145/3505711.3505720",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Tolerating Defects in Low-Power Neural Network Accelerators Via Retraining-Free Weight Approximation",
        "authors": "['Fateme S. Hosseini', 'Fanruo Meng', 'Chengmo Yang', 'Wujie Wen', 'Rosario Cammarota']",
        "date": "None",
        "source": "ACM Transactions on Embedded Computing Systems",
        "abstract": "Hardware accelerators are essential to the accommodation of ever-increasing Deep Neural Network (DNN) workloads on the resource-constrained embedded devices. While accelerators facilitate fast and energy-efficient DNN operations, their accuracy is threatened by faults in their on-chip and off-chip memories, where millions of DNN weights are held. The use of emerging Non-Volatile Memories (NVM) further exposes DNN accelerators to a non-negligible rate of permanent defects due to immature fabrication, limited endurance, and aging. To tolerate defects in NVM-based DNN accelerators, previous work either requires extra redundancy in hardware or performs defect-aware retraining, imposing significant overhead. In comparison, this paper proposes a set of algorithms that exploit the flexibility in setting the fault-free bits in weight memory to effectively approximate weight values, so as to mitigate defect-induced accuracy drop. These algorithms can be applied as a one-step solution when loading the weights to embedded devices. They only require trivial hardware support and impose negligible run-time overhead. Experiments on popular DNN models show that the proposed techniques successfully boost inference accuracy even in the face of elevated defect rates in the weight memory.",
        "link": "https://dl.acm.org/doi/10.1145/3477016",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "High-throughput Near-Memory Processing on CNNs with 3D HBM-like Memory",
        "authors": "['Naebeom Park', 'Sungju Ryu', 'Jaeha Kung', 'Jae-Joon Kim']",
        "date": "None",
        "source": "ACM Transactions on Design Automation of Electronic Systems",
        "abstract": "This article discusses the high-performance near-memory neural network (NN) accelerator architecture utilizing the logic die in three-dimensional (3D) High Bandwidth Memory– (HBM) like memory. As most of the previously reported 3D memory-based near-memory NN accelerator designs used the Hybrid Memory Cube (HMC) memory, we first focus on identifying the key differences between HBM and HMC in terms of near-memory NN accelerator design. One of the major differences between the two 3D memories is that HBM has the centralized through-silicon-via (TSV) channels while HMC has distributed TSV channels for separate vaults. Based on the observation, we introduce the Round-Robin Data Fetching and Groupwise Broadcast schemes to exploit the centralized TSV channels for improvement of the data feeding rate for the processing elements. Using synthesized designs in a 28-nm CMOS technology, performance and energy consumption of the proposed architectures with various dataflow models are evaluated. Experimental results show that the proposed schemes reduce the runtime by 16.4–39.3% on average and the energy consumption by 2.1–5.1% on average compared to conventional data fetching schemes.",
        "link": "https://dl.acm.org/doi/10.1145/3460971",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Automation of Domain-specific FPGA-IP Generation and Test",
        "authors": "['Yuya Nakazato', 'Motoki Amagasaki', 'Qian Zhao', 'Masahiro Iida', 'Morihiro Kuga']",
        "date": "June 2021",
        "source": "HEART '21: Proceedings of the 11th International Symposium on Highly Efficient Accelerators and Reconfigurable Technologies",
        "abstract": "Multi-access edge computing (MEC) devices that perform processing between the edge and cloud are becoming important in the Internet of Things infrastructure. MEC devices are designed to reduce the load on the edge devices, ensure real-time performance, and reduce the communication traffic between the edge and cloud. In this paper, to enable high-performance and low-power hardware-accelerated processing for different application domains in MEC devices, we propose an automated flow for domain-specific field-programmable gate array intellectual property core (FPGA-IP) generation and testing. First, we perform logic cell exploration using a target user application to find the optimal scalable logic module (SLM) structure, and use the optimal SLM instead of a lookup table to reduce the logic area. Second, we perform routing and FPGA array exploration to determine other FPGA-IP architecture parameters. Finally, the proposed flow uses the explored parameters to automatically generate the entire FPGA-IP and LSI test bitstreams. In a case study, we optimized an FPGA-IP for a differential privacy encryption circuit using the proposed flow. We implemented and evaluated the FPGA-IP with a 55nm TEG chip design. Furthermore, the simulation-based LSI test showed that 100% of the stuck-at faults in the routing paths of the FPGA-IP were detected.",
        "link": "https://dl.acm.org/doi/10.1145/3468044.3468048",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Extending C++ for Heterogeneous Quantum-Classical Computing",
        "authors": "['Alexander Mccaskey', 'Thien Nguyen', 'Anthony Santana', 'Daniel Claudino', 'Tyler Kharazi', 'Hal Finkel']",
        "date": "None",
        "source": "ACM Transactions on Quantum Computing",
        "abstract": "We present qcor—a language extension to C++ and compiler implementation that enables heterogeneous quantum-classical programming, compilation, and execution in a single-source context. Our work provides a first-of-its-kind C++ compiler enabling high-level quantum kernel (function) expression in a quantum-language agnostic manner, as well as a hardware-agnostic, retargetable compiler workflow targeting a number of physical and virtual quantum computing backends. qcor leverages novel Clang plugin interfaces and builds upon the XACC system-level quantum programming framework to provide a state-of-the-art integration mechanism for quantum-classical compilation that leverages the best from the community at-large. qcor translates quantum kernels ultimately to the XACC intermediate representation, and provides user-extensible hooks for quantum compilation routines like circuit optimization, analysis, and placement. This work details the overall architecture and compiler workflow for qcor, and provides a number of illuminating programming examples demonstrating its utility for near-term variational tasks, quantum algorithm expression, and feed-forward error correction schemes.",
        "link": "https://dl.acm.org/doi/10.1145/3462670",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Design Automation for Tree-based Nearest Neighborhood–aware Placement of High-speed Cellular Automata on FPGA with Scan Path Insertion",
        "authors": "['Ayan Palchaudhuri', 'Sandeep Sharma', 'Anindya Sundar Dhar']",
        "date": "None",
        "source": "ACM Transactions on Design Automation of Electronic Systems",
        "abstract": "Cellular Automata (CA) is attractive for high-speed VLSI implementation due to modularity, cascadability, and locality of interconnections confined to neighboring logic cells. However, this outcome is not easily transferable to tree-structured CA, since the neighbors having half and double the index value of the current CA cell under question can be sufficiently distanced apart on the FPGA floor. Challenges to meet throughput requirements, seamlessly translate algorithmic modifications for changing application specifications to gate level architectures and to address reliability challenges of semiconductor chips are ever increasing. Thus, a proper design framework assisting automation of synthesizable, delay-optimized VLSI architecture descriptions facilitating testability is desirable. In this article, we have automated the generation of hardware description of tree-structured CA that includes a built-in scan path realized with zero area and delay overhead. The scan path facilitates seeding the CA, state modification, and fault localization on the FPGA fabric. Three placement algorithms were proposed to ensure maximum physical adjacency amongst neighboring CA cells, arranged in a multi-columnar fashion on the FPGA grid. Our proposed architectures outperform implementations arising out of standard placers and behavioral designs, existing tree mapping strategies, and state-of-the-art FPGA centric error detection architectures in area and speed.",
        "link": "https://dl.acm.org/doi/10.1145/3446206",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "UNO: virtualizing and unifying nonlinear operations for emerging neural networks",
        "authors": "['Di Wu', 'Jingjie Li', 'Setareh Behroozi', 'Younghyun Kim', 'Joshua San Miguel']",
        "date": "July 2021",
        "source": "ISLPED '21: Proceedings of the ACM/IEEE International Symposium on Low Power Electronics and Design",
        "abstract": "Linear multiply-accumulate (MAC) operations have been the main focus of prior efforts in improving the energy efficiency of neural network inference due to their dominant contribution to energy consumption in traditional models. On the other hand, nonlinear operations, such as division, exponentiation, and logarithm, that are becoming increasingly significant in emerging neural network models, have been largely underexplored. In this paper, we propose UNO, a low-area, low-energy processing element that virtualizes the Taylor approximation of nonlinear operations on top of off-the-shelf linear MAC units already present in inference hardware. Such virtualization approximates multiple nonlinear operations in a unified, MAC-compatible manner to achieve dynamic run-time accuracy-energy scaling. Compared to the baseline, our scheme reduces the energy consumption by up to 38.4% for individual operations and increases the energy efficiency by up to 274.5% for emerging neural network models with negligible inference loss.",
        "link": "https://dl.acm.org/doi/10.5555/3489049.3489051",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Fast Key-Value Lookups with Node Tracker",
        "authors": "['Mustafa Cavus', 'Mohammed Shatnawi', 'Resit Sendag', 'Augustus K. Uht']",
        "date": "None",
        "source": "ACM Transactions on Architecture and Code Optimization",
        "abstract": "Lookup operations for in-memory databases are heavily memory bound, because they often rely on pointer-chasing linked data structure traversals. They also have many branches that are hard-to-predict due to random key lookups. In this study, we show that although cache misses are the primary bottleneck for these applications, without a method for eliminating the branch mispredictions only a small fraction of the performance benefit is achieved through prefetching alone. We propose the Node Tracker (NT), a novel programmable prefetcher/pre-execution unit that is highly effective in exploiting inter key-lookup parallelism to improve single-thread performance. We extend NT with branch outcome streaming (BOS) to reduce branch mispredictions and show that this achieves an extra 3× speedup. Finally, we evaluate the NT as a pre-execution unit and demonstrate that we can further improve the performance in both single- and multi-threaded execution modes. Our results show that, on average, NT improves single-thread performance by 4.1× when used as a prefetcher; 11.9× as a prefetcher with BOS; 14.9× as a pre-execution unit and 18.8× as a pre-execution unit with BOS. Finally, with 24 cores of the latter version, we achieve a speedup of 203× and 11× over the single-core and 24-core baselines, respectively.",
        "link": "https://dl.acm.org/doi/10.1145/3452099",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Provable Advantages for Graph Algorithms in Spiking Neural Networks",
        "authors": "['James B. Aimone', 'Yang Ho', 'Ojas Parekh', 'Cynthia A. Phillips', 'Ali Pinar', 'William Severa', 'Yipu Wang']",
        "date": "July 2021",
        "source": "SPAA '21: Proceedings of the 33rd ACM Symposium on Parallelism in Algorithms and Architectures",
        "abstract": "We present a theoretical framework for designing and assessing the performance of algorithms executing in networks consisting of spiking artificial neurons. Although spiking neural networks (SNNs) are capable of general-purpose computation, few algorithmic results with rigorous asymptotic performance analysis are known. SNNs are exceptionally well-motivated practically, as neuromorphic computing systems with 100 million spiking neurons are available, and systems with a billion neurons are anticipated in the next few years. Beyond massive parallelism and scalability, neuromorphic computing systems offer energy consumption orders of magnitude lower than conventional high-performance computing systems. We employ our framework to design and analyze neuromorphic graph algorithms, focusing on shortest path problems. Our neuromorphic algorithms are message-passing algorithms relying critically on data movement for computation, and we develop data-movement lower bounds for conventional algorithms. A fair and rigorous comparison with conventional algorithms and architectures is challenging but paramount. We prove a polynomial-factor advantage even when we assume an SNN consisting of a simple grid-like network of neurons. To the best of our knowledge, this is one of the first examples of a provable asymptotic computational advantage for neuromorphic computing.",
        "link": "https://dl.acm.org/doi/10.1145/3409964.3461813",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "UNO: virtualizing and unifying nonlinear operations for emerging neural networks",
        "authors": "['Di Wu', 'Jingjie Li', 'Setareh Behroozi', 'Younghyun Kim', 'Joshua San Miguel']",
        "date": "July 2021",
        "source": "ISLPED '21: Proceedings of the ACM/IEEE International Symposium on Low Power Electronics and Design",
        "abstract": "Linear multiply-accumulate (MAC) operations have been the main focus of prior efforts in improving the energy efficiency of neural network inference due to their dominant contribution to energy consumption in traditional models. On the other hand, nonlinear operations, such as division, exponentiation, and logarithm, that are becoming increasingly significant in emerging neural network models, have been largely underexplored. In this paper, we propose UNO, a low-area, low-energy processing element that virtualizes the Taylor approximation of nonlinear operations on top of off-the-shelf linear MAC units already present in inference hardware. Such virtualization approximates multiple nonlinear operations in a unified, MAC-compatible manner to achieve dynamic run-time accuracy-energy scaling. Compared to the baseline, our scheme reduces the energy consumption by up to 38.4% for individual operations and increases the energy efficiency by up to 274.5% for emerging neural network models with negligible inference loss.",
        "link": "https://dl.acm.org/doi/10.1109/ISLPED52811.2021.9502473",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "CAMeleon: Reconfigurable B(T)CAM in Computational RAM",
        "authors": "['Zamshed I. Chowdhury', 'Salonik Resch', 'Hüsrev Cilasun', 'Zhengyang Zhao', 'Masoud Zabihi', 'Sachin S. Sapatnekar', 'Jian-Ping Wang', 'Ulya R. Karpuzcu']",
        "date": "June 2021",
        "source": "GLSVLSI '21: Proceedings of the 2021 on Great Lakes Symposium on VLSI",
        "abstract": "Embedded/edge computing comes with a very stringent hardware resource (area) budget and a need for extreme energy efficiency. This motivates repurposing, i.e., reconfiguring hardware resources on demand, where the overhead of reconfiguration itself is subject to the very same tight budgets in area and energy efficiency. Numerous applications running on resource constrained environments such as wearable devices and Internet-of-Things incorporate CAM (Content Addressable Memory) as a key computational building block. In this paper we present CAMeleon -- a novel energy-efficient compute substrate which can seamlessly be reconfigured to perform CAM operations in addition to logic and memory functions. CAMeleon has a similar level of latency to conventional CAM designs based on SRAM and emerging memory technologies (such as STT-MTJ, ReRAM and PCM), however, performs CAM operations more energy-efficiently, consumes less area, and can support traditional logic and memory functions beyond CAM operations on demand thanks to its reconfigurability.",
        "link": "https://dl.acm.org/doi/10.1145/3453688.3461507",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Learning to Train CNNs on Faulty ReRAM-based Manycore Accelerators",
        "authors": "['Biresh Kumar Joardar', 'Janardhan Rao Doppa', 'Hai Li', 'Krishnendu Chakrabarty', 'Partha Pratim Pande']",
        "date": "None",
        "source": "ACM Transactions on Embedded Computing Systems",
        "abstract": "The growing popularity of convolutional neural networks (CNNs) has led to the search for efficient computational platforms to accelerate CNN training. Resistive random-access memory (ReRAM)-based manycore architectures offer a promising alternative to commonly used GPU-based platforms for training CNNs. However, due to the immature fabrication process and limited write endurance, ReRAMs suffer from different types of faults. This makes training of CNNs challenging as weights are misrepresented when they are mapped to faulty ReRAM cells. This results in unstable training, leading to unacceptably low accuracy for the trained model. Due to the distributed nature of the mapping of the individual bits of a weight to different ReRAM cells, faulty weights often lead to exploding gradients. This in turn introduces a positive feedback in the training loop, resulting in extremely large and unstable weights. In this paper, we propose a lightweight and reliable CNN training methodology using weight clipping to prevent this phenomenon and enable training even in the presence of many faults. Weight clipping prevents large weights from destabilizing CNN training and provides the backpropagation algorithm with the opportunity to compensate for the weights mapped to faulty cells. The proposed methodology achieves near-GPU accuracy without introducing significant area or performance overheads. Experimental evaluation indicates that weight clipping enables the successful training of CNNs in the presence of faults, while also reducing training time by 4X on average compared to a conventional GPU platform. Moreover, we also demonstrate that weight clipping outperforms a recently proposed error correction code (ECC)-based method when training is carried out using faulty ReRAMs.",
        "link": "https://dl.acm.org/doi/10.1145/3476986",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Ghost routing to enable oblivious computation on memory-centric networks",
        "authors": "['Yeonju Ro', 'Seongwook Jin', 'Jaehyuk Huh', 'John Kim']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "With offloading of data to the cloud, ensuring privacy and securing data has become more important. However, encrypting data alone is insufficient as the memory address itself can leak sensitive information. In this work, we exploit packetized memory interface to provide secure memory access and support oblivious computation in a system with multiple memory modules interconnected with a multi-hop, memory-centric network. While the memory address can be encrypted with a packetized memory interface, simply encrypting the address does not provide full oblivious computation since coarse-grain memory access patterns can be leaked. In this work, we first propose a scalable encryption microarchitecture with source-based routing where the packet is only encrypted once at source and latency overhead in intermediate routers is minimized. We then define secure routing in memory-centric networks to enable oblivious computation such that memory access patterns across the memory modules are completely obfuscated. We explore different naive secure routing algorithms to ensure oblivious computation but they come with high performance overhead. To minimize performance overhead, we propose ghost packets that replace dummy packets with existing network traffic. We also propose Ghost routing that batches multiple ghost packets together to minimize bandwidth loss from naive secure routing while exploiting random routing.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00077",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "55nm CMOS analog circuit implementation of LIF and STDP functions for low-power SNNs",
        "authors": "['Zhitao Yang', 'Zhujiang Han', 'Yucong Huang', 'Terry Tao Ye']",
        "date": "July 2021",
        "source": "ISLPED '21: Proceedings of the ACM/IEEE International Symposium on Low Power Electronics and Design",
        "abstract": "Spiking neural networks (SNNs) demonstrate great potentials to achieve low-power computation for AI applications. SNN uses spike trains, instead of binary bit-steams to encode input and output information, therefore, analog implementation of SNN will have more advantages than digital implementation in terms of power consumption and hardware overheads. Leaky Integrate-and-Fire (LIF) and Spike Timing Dependent Plasticity (STDP) models are the two fundamental mechanisms of SNN operation. In this paper, we propose a 55nm analog CMOS implementation of the LIF and STDP functions. Testing results demonstrate that the circuit can closely imitate the behavior of the LIF and STDP mechanisms, while demanding a much lower power consumption (around 1nJ per spike with the pulse width of 0.5ms). The proposed LIF and STDP circuits can be used as building blocks to construct a complete SNN architecture.",
        "link": "https://dl.acm.org/doi/10.1109/ISLPED52811.2021.9502497",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Impact of On-chip Interconnect on In-memory Acceleration of Deep Neural Networks",
        "authors": "['Gokul Krishnan', 'Sumit K. Mandal', 'Chaitali Chakrabarti', 'Jae-Sun Seo', 'Umit Y. Ogras', 'Yu Cao']",
        "date": "None",
        "source": "ACM Journal on Emerging Technologies in Computing Systems",
        "abstract": "With the widespread use of Deep Neural Networks (DNNs), machine learning algorithms have evolved in two diverse directions—one with ever-increasing connection density for better accuracy and the other with more compact sizing for energy efficiency. The increase in connection density increases on-chip data movement, which makes efficient on-chip communication a critical function of the DNN accelerator. The contribution of this work is threefold. First, we illustrate that the point-to-point (P2P)-based interconnect is incapable of handling a high volume of on-chip data movement for DNNs. Second, we evaluate P2P and network-on-chip (NoC) interconnect (with a regular topology such as a mesh) for SRAM- and ReRAM-based in-memory computing (IMC) architectures for a range of DNNs. This analysis shows the necessity for the optimal interconnect choice for an IMC DNN accelerator. Finally, we perform an experimental evaluation for different DNNs to empirically obtain the performance of the IMC architecture with both NoC-tree and NoC-mesh. We conclude that, at the tile level, NoC-tree is appropriate for compact DNNs employed at the edge, and NoC-mesh is necessary to accelerate DNNs with high connection density. Furthermore, we propose a technique to determine the optimal choice of interconnect for any given DNN. In this technique, we use analytical models of NoC to evaluate end-to-end communication latency of any given DNN. We demonstrate that the interconnect optimization in the IMC architecture results in up to 6 × improvement in energy-delay-area product for VGG-19 inference compared to the state-of-the-art ReRAM-based IMC architectures.",
        "link": "https://dl.acm.org/doi/10.1145/3460233",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "EXAMINER: automatically locating inconsistent instructions between real devices and CPU emulators for ARM",
        "authors": "['Muhui Jiang', 'Tianyi Xu', 'Yajin Zhou', 'Yufeng Hu', 'Ming Zhong', 'Lei Wu', 'Xiapu Luo', 'Kui Ren']",
        "date": "February 2022",
        "source": "ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "Emulators are widely used to build dynamic analysis frameworks due to its fine-grained tracing capability, full system monitoring functionality, and scalability of running on different operating systems and architectures. However, whether emulators are consistent with real devices is unknown. To understand this problem, we aim to automatically locate inconsistent instructions, which behave differently between emulators and real devices.   We target the ARM architecture, which provides machine-readable specifications. Based on the specification, we propose a sufficient test case generator by designing and implementing the first symbolic execution engine for the ARM architecture specification language (ASL). We generate 2,774,649 representative instruction streams and conduct differential testing between four ARM real devices in different architecture versions (i.e., ARMv5, ARMv6, ARMv7, and ARMv8) and three state-of-the-art emulators (i.e., QEMU, Unicorn, and Angr). We locate a huge number of inconsistent instruction streams (171,858 for QEMU, 223,264 for unicorn, and 120,169 for Angr). We find that undefined implementation in ARM manual and bugs of emulators are the major causes of inconsistencies. Furthermore, we discover 12 bugs, which influence commonly used instructions (e.g., BLX). With the inconsistent instructions, we build three security applications and demonstrate the capability of these instructions on detecting emulators, anti-emulation, and anti-fuzzing.",
        "link": "https://dl.acm.org/doi/10.1145/3503222.3507736",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Monolithically Integrating Non-Volatile Main Memory over the Last-Level Cache",
        "authors": "['Candace Walden', 'Devesh Singh', 'Meenatchi Jagasivamani', 'Shang Li', 'Luyi Kang', 'Mehdi Asnaashari', 'Sylvain Dubois', 'Bruce Jacob', 'Donald Yeung']",
        "date": "None",
        "source": "ACM Transactions on Architecture and Code Optimization",
        "abstract": "Many emerging non-volatile memories are compatible with CMOS logic, potentially enabling their integration into a CPU’s die. This article investigates such monolithically integrated CPU–main memory chips. We exploit non-volatile memories employing 3D crosspoint subarrays, such as resistive RAM (ReRAM), and integrate them over the CPU’s last-level cache (LLC). The regular structure of cache arrays enables co-design of the LLC and ReRAM main memory for area efficiency. We also develop a streamlined LLC/main memory interface that employs a single shared internal interconnect for both the cache and main memory arrays, and uses a unified controller to service both LLC and main memory requests.We apply our monolithic design ideas to a many-core CPU by integrating 3D ReRAM over each core’s LLC slice. We find that co-design of the LLC and ReRAM saves 27% of the total LLC–main memory area at the expense of slight increases in delay and energy. The streamlined LLC/main memory interface saves an additional 12% in area. Our simulation results show monolithic integration of CPU and main memory improves performance by 5.3× and 1.7× over HBM2 DRAM for several graph and streaming kernels, respectively. It also reduces the memory system’s energy by 6.0× and 1.7×, respectively. Moreover, we show that the area savings of co-design permits the CPU to have 23% more cores and main memory, and that streamlining the LLC/main memory interface incurs a small 4% performance penalty.",
        "link": "https://dl.acm.org/doi/10.1145/3462632",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "ShEF: shielded enclaves for cloud FPGAs",
        "authors": "['Mark Zhao', 'Mingyu Gao', 'Christos Kozyrakis']",
        "date": "February 2022",
        "source": "ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "FPGAs are now used in public clouds to accelerate a wide range of applications, including many that operate on sensitive data such as financial and medical records. We present ShEF, a trusted execution environment (TEE) for cloud-based reconfigurable accelerators. ShEF is independent from CPU-based TEEs and allows secure execution under a threat model where the adversary can control all software running on the CPU connected to the FPGA, has physical access to the FPGA, and can compromise the FPGA interface logic of the cloud provider. ShEF provides a secure boot and remote attestation process that relies solely on existing FPGA mechanisms for root of trust. It also includes a Shield component that provides secure access to data while the accelerator is in use. The Shield is highly customizable and extensible, allowing users to craft a bespoke security solution that fits their accelerator's memory access patterns, bandwidth, and security requirements at minimum performance and area overheads. We describe a prototype implementation of ShEF for existing cloud FPGAs, map ShEF to a performant and secure storage application, and measure the performance benefits of customizable security using five additional accelerators.",
        "link": "https://dl.acm.org/doi/10.1145/3503222.3507733",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "CrypTag: Thwarting Physical and Logical Memory Vulnerabilities using Cryptographically Colored Memory",
        "authors": "['Pascal Nasahl', 'Robert Schilling', 'Mario Werner', 'Jan Hoogerbrugge', 'Marcel Medwed', 'Stefan Mangard']",
        "date": "May 2021",
        "source": "ASIA CCS '21: Proceedings of the 2021 ACM Asia Conference on Computer and Communications Security",
        "abstract": "Memory vulnerabilities are a major threat to many computing systems. To effectively thwart spatial and temporal memory vulnerabilities, full logical memory safety is required. However, current mitigation techniques for memory safety are either too expensive or trade security against efficiency. One promising attempt to detect memory safety vulnerabilities in hardware is memory coloring, a security policy deployed on top of tagged memory architectures. However, due to the memory storage and bandwidth overhead of large tags, commodity tagged memory architectures usually only provide small tag sizes, thus limiting their use for security applications. Irrespective of logical memory safety, physical memory safety is a necessity in hostile environments prevalent for modern cloud computing and IoT devices. Architectures from Intel and AMD already implement transparent memory encryption to maintain confidentiality and integrity of all off-chip data. Surprisingly, the combination of both, logical and physical memory safety, has not yet been extensively studied in previous research, and a naive combination of both security strategies would accumulate both overheads. In this paper, we propose CrypTag, an efficient hardware/software co-design mitigating a large class of logical memory safety issues and providing full physical memory safety. At its core, CrypTag utilizes a transparent memory encryption engine not only for physical memory safety, but also for memory coloring at hardly any additional costs. The design avoids any overhead for tag storage by embedding memory colors in the upper bits of a pointer and using these bits as an additional input for the memory encryption. A custom compiler extension automatically leverages CrypTag to detect logical memory safety issues for commodity programs and is fully backward compatible. For evaluating the design, we extended a RISC-V processor with memory encryption with CrypTag. Furthermore, we developed a LLVM-based toolchain automatically protecting all dynamic, local, and global data. Our evaluation shows a hardware overhead of less than 1% and an average runtime overhead between 1.5% and 6.1% for thwarting logical memory safety vulnerabilities on a system already featuring memory encryption. Enhancing a system with memory encryption typically induces a runtime overhead between 5% and 109.8% for commercial and open-source encryption units.",
        "link": "https://dl.acm.org/doi/10.1145/3433210.3453684",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "EPEX: Processor Verification by Equivalent Program Execution",
        "authors": "['Lucas Klemmer', 'Daniel Große']",
        "date": "June 2021",
        "source": "GLSVLSI '21: Proceedings of the 2021 on Great Lakes Symposium on VLSI",
        "abstract": "Verifying processors has been and still is a major challenge. Therefore, intensive research has led to advanced verification solutions ranging from ISS-based reference models, (cross-level) simulation down to formal verification at the RTL. During the verification of the processor implementation at the Instruction Set Architecture (ISA) level, test stimuli, i.e. test programs are needed. They are either created manually or with the aid of sophisticated test program generators. However, significant effort is required to produce thorough test programs. In this paper, we devise a novel approach for processor verification by Equivalent Program EXecution (EPEX). Our approach is based on a new form of equivalence checking Instead of comparing the architectural states of two models which execute the same program P, we derive a second, but equivalent program P^ from P (wrt. to a formal ISA model), and check that executing P and P^ will produce equal architectural states on the same design. We show that EPEX can easily be used in a simulation-based verification environment and broadens existing tests automatically. In a RISC-V case study using different core configurations of the well-known VexRiscv core, we demonstrate the bug-finding capabilities of EPEX.",
        "link": "https://dl.acm.org/doi/10.1145/3453688.3461497",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "HeteroGen: transpiling C to heterogeneous HLS code with automated test generation and program repair",
        "authors": "['Qian Zhang', 'Jiyuan Wang', 'Guoqing Harry Xu', 'Miryung Kim']",
        "date": "February 2022",
        "source": "ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "Despite the trend of incorporating heterogeneity and specialization in hardware, the development of heterogeneous applications is limited to a handful of engineers with deep hardware expertise. We propose HeteroGen that takes C/C++ code as input and automatically generates an HLS version with test behavior preservation and better performance. Key to the success of HeteroGen is adapting the idea of search-based program repair to the heterogeneous computing domain, while addressing two technical challenges. First, the turn-around time of HLS compilation and simulation is much longer than the usual C/C++ compilation and execution time; therefore, HeteroGen applies pattern-oriented program edits guided by common fix patterns and their dependences. Second, behavior and performance checking requires testing, but test cases are often unavailable. Thus, HeteroGen auto-generates test inputs suitable for checking C to HLS-C conversion errors, while providing high branch coverage for the original C code.   An evaluation of HeteroGen shows that it produces an HLS-compatible version for nine out of ten real-world heterogeneous applications fully automatically, applying up to 438 lines of edits to produce an HLS version 1.63x faster than the original version.",
        "link": "https://dl.acm.org/doi/10.1145/3503222.3507748",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Enabling pipeline parallelism in heterogeneous managed runtime environments via batch processing",
        "authors": "['Florin Blanaru', 'Athanasios Stratikopoulos', 'Juan Fumero', 'Christos Kotselidis']",
        "date": "February 2022",
        "source": "VEE 2022: Proceedings of the 18th ACM SIGPLAN/SIGOPS International Conference on Virtual Execution Environments",
        "abstract": "During the last decade, managed runtime systems have been constantly evolving to become capable of exploiting underlying hardware accelerators, such as GPUs and FPGAs. Regardless of the programming language and their corresponding runtime systems, the majority of the work has been focusing on the compiler front trying to tackle the challenging task of how to enable just-in-time compilation and execution of arbitrary code segments on various accelerators. Besides this challenging task, another important aspect that defines both functional correctness and performance of managed runtime systems is that of automatic memory management. Although automatic memory management improves productivity by abstracting away memory allocation and maintenance, it hinders the capability of using specific memory regions, such as pinned memory, in order to perform data transfer times between the CPU and hardware accelerators.   In this paper, we introduce and evaluate a series of memory optimizations specifically tailored for heterogeneous managed runtime systems. In particular, we propose: (i) transparent and automatic \"parallel batch processing\" for overlapping data transfers and computation between the host and hardware accelerators in order to enable pipeline parallelism, and (ii) \"off-heap pinned memory\" in combination with parallel batch processing in order to increase the performance of data transfers without posing any on-heap overheads. These two techniques have been implemented in the context of the state-of-the-art open-source TornadoVM and their combination can lead up to 2.5x end-to-end performance speedup against sequential batch processing.",
        "link": "https://dl.acm.org/doi/10.1145/3516807.3516821",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Prepare: Power-Aware Approximate Real-time Task Scheduling for Energy-Adaptive QoS Maximization",
        "authors": "['Shounak Chakraborty', 'Sangeet Saha', 'Magnus Själander', 'Klaus Mcdonald-Maier']",
        "date": "None",
        "source": "ACM Transactions on Embedded Computing Systems",
        "abstract": "Achieving high result-accuracy in approximate computing (AC) based real-time applications without violating power constraints of the underlying hardware is a challenging problem. Execution of such AC real-time tasks can be divided into the execution of the mandatory part to obtain a result of acceptable quality, followed by a partial/complete execution of the optional part to improve accuracy of the initially obtained result within the given time-limit. However, enhancing result-accuracy at the cost of increased execution length might lead to deadline violations with higher energy usage. We propose Prepare, a novel hybrid offline-online approximate real-time task-scheduling approach, that first schedules AC-based tasks and determines operational processing speeds for each individual task constrained by system-wide power limit, deadline, and task-dependency. At runtime, by employing fine-grained DVFS, the energy-adaptive processing speed governing mechanism of Prepare reduces processing speed during each last level cache miss induced stall and scales up the processing speed once the stall finishes to a higher value than the predetermined one. To ensure on-chip thermal safety, this higher processing speed is maintained only for a short time-span after each stall, however, this reduces execution times of the individual task and generates slacks. Prepare exploits the slacks either to enhance result-accuracy of the tasks, or to improve thermal and energy efficiency of the underlying hardware, or both. With a 70 - 80% workload, Prepare offers 75% result-accuracy with its constrained scheduling, which is enhanced by 5.3% for our benchmark based evaluation of the online energy-adaptive mechanism on a 4-core based homogeneous chip multi-processor, while meeting the deadline constraint. Overall, while maintaining runtime thermal safety, Prepare reduces peak temperature by up to 8.6 °C for our baseline system. Our empirical evaluation shows that constrained scheduling of Prepare outperforms a state-of-the-art scheduling policy, whereas our runtime energy-adaptive mechanism surpasses two current DVFS based thermal management techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3476993",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Enabling compute-communication overlap in distributed deep learning training platforms",
        "authors": "['Saeed Rashidi', 'Matthew Denton', 'Srinivas Sridharan', 'Sudarshan Srinivasan', 'Amoghavarsha Suresh', 'Jade Nie', 'Tushar Krishna']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "Deep Learning (DL) training platforms are built by interconnecting multiple DL accelerators (e.g., GPU/TPU) via fast, customized interconnects with 100s of gigabytes (GBs) of bandwidth. However, as we identify in this work, driving this bandwidth is quite challenging. This is because there is a pernicious balance between using the accelerator's compute and memory for both DL computations and communication. This work makes two key contributions. First, via real system measurements and detailed modeling, we provide an understanding of compute and memory bandwidth demands for DL compute and comms. Second, we propose a novel DL collective communication accelerator called Accelerator Collectives Engine (ACE) that sits alongside the compute and networking engines at the accelerator endpoint. ACE frees up the endpoint's compute and memory resources for DL compute, which in turn reduces the required memory BW by 3.5X on average to drive the same network BW compared to state-of-the-art baselines. For modern DL workloads and different network sizes, ACE, on average, increases the effective network bandwidth utilization by 1.44X (up to 2.67X), resulting in an average of 1.41X (up to 1.51X), 1.12X (up to 1.17X), and 1.13X (up to 1.19X) speedup in iteration time for ResNet-50, GNMT and DLRM when compared to the best baseline configuration, respectively.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00049",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "CLU: A Near-Memory Accelerator Exploiting the Parallelism in Convolutional Neural Networks",
        "authors": "['Palash Das', 'Hemangee K. Kapoor']",
        "date": "None",
        "source": "ACM Journal on Emerging Technologies in Computing Systems",
        "abstract": "Convolutional/Deep Neural Networks (CNNs/DNNs) are rapidly growing workloads for the emerging AI-based systems. The gap between the processing speed and the memory-access latency in multi-core systems affects the performance and energy efficiency of the CNN/DNN tasks. This article aims to alleviate this gap by providing a simple and yet efficient near-memory accelerator-based system that expedites the CNN inference. Towards this goal, we first design an efficient parallel algorithm to accelerate CNN/DNN tasks. The data is partitioned across the multiple memory channels (vaults) to assist in the execution of the parallel algorithm. Second, we design a hardware unit, namely the convolutional logic unit (CLU), which implements the parallel algorithm. To optimize the inference, the CLU is designed, and it works in three phases for layer-wise processing of data. Last, to harness the benefits of near-memory processing (NMP), we integrate homogeneous CLUs on the logic layer of the 3D memory, specifically the Hybrid Memory Cube (HMC). The combined effect of these results in a high-performing and energy-efficient system for CNNs/DNNs. The proposed system achieves a substantial gain in the performance and energy reduction compared to multi-core CPU- and GPU-based systems with a minimal area overhead of 2.37%.",
        "link": "https://dl.acm.org/doi/10.1145/3427472",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Enabling energy-efficient DNN training on hybrid GPU-FPGA accelerators",
        "authors": "['Xin He', 'Jiawen Liu', 'Zhen Xie', 'Hao Chen', 'Guoyang Chen', 'Weifeng Zhang', 'Dong Li']",
        "date": "June 2021",
        "source": "ICS '21: Proceedings of the ACM International Conference on Supercomputing",
        "abstract": "DNN training consumes orders of magnitude more energy than inference and requires innovative use of accelerators to improve energy-efficiency. However, despite having complementary features, GPUs and FPGAs have been mostly used independently for the entire training process, thus neglecting the opportunity in assigning individual but distinct operations to the most suitable hardware. In this paper, we take the initiative to explore new opportunities and viable solutions in enabling energy-efficient DNN training on hybrid accelerators. To overcome fundamental challenges including avoiding training throughput loss, enabling fast design space exploration, and efficient scheduling, we propose a comprehensive framework, Hype-training, that utilizes a combination of offline characterization, performance modeling, and online scheduling of individual operations. Experimental tests using NVIDIA V100 GPUs and Intel Stratix 10 FPGAs show that, Hype-training is able to exploit a mixture of GPUs and FPGAs at a fine granularity to achieve significant energy reduction, by 44.3% on average and up to 59.7%, without any loss in training throughput. Hype-training can also enforce power caps more effectively than state-of-the-art power management mechanisms on GPUs.",
        "link": "https://dl.acm.org/doi/10.1145/3447818.3460371",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A resource-efficient spiking neural network accelerator supporting emerging neural encoding",
        "authors": "['Daniel Gerlinghoff', 'Zhehui Wang', 'Xiaozhe Gu', 'Rick Siow Mong Goh', 'Tao Luo']",
        "date": "March 2022",
        "source": "DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe",
        "abstract": "Spiking neural networks (SNNs) recently gained momentum due to their low-power multiplication-free computing and the closer resemblance of biological processes in the nervous system of humans. However, SNNs require very long spike trains (up to 1000) to reach an accuracy similar to their artificial neural network (ANN) counterparts for large models, which offsets efficiency and inhibits its application to low-power systems for real-world use cases. To alleviate this problem, emerging neural encoding schemes are proposed to shorten the spike train while maintaining the high accuracy. However, current accelerators for SNN cannot well support the emerging encoding schemes. In this work, we present a novel hardware architecture that can efficiently support SNN with emerging neural encoding. Our implementation features energy and area efficient processing units with increased parallelism and reduced memory accesses. We verified the accelerator on FPGA and achieve 25% and 90% improvement over previous work in power consumption and latency, respectively. At the same time, high area efficiency allows us to scale for large neural network models. To the best of our knowledge, this is the first work to deploy the large neural network model VGG on physical FPGA-based neuromorphic hardware.",
        "link": "https://dl.acm.org/doi/10.5555/3539845.3539873",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Bridging the Gap between RTL and Software Fault Injection",
        "authors": "['J. Laurent', 'C. Deleuze', 'F. Pebay-Peyroula', 'V. Beroulle']",
        "date": "None",
        "source": "ACM Journal on Emerging Technologies in Computing Systems",
        "abstract": "Protecting programs against hardware fault injection requires accurate software fault models. However, typical models, such as the instruction skip, do not take into account the microarchitecture specificities of a processor. We propose in this article an approach to study the relation between faults at the Register Transfer Level (RTL) and faults at the software level. The goal is twofold: accurately model RTL faults at the software level and materialize software fault models to actual RTL injections. These goals lead to a better understanding of a system's security against hardware fault injection, which is important to design effective and cost-efficient countermeasures. Our approach is based on the comparison between results from RTL simulations and software injections (using a program mutation tool). Various analyses are included in this article to give insight on the relevance of software fault models, such as the computation of a coverage and fidelity metric, and to link software fault models to hardware RTL descriptions. These analyses are applied on various single-bit and multiple-bit injection campaigns to study the faulty behaviors of a RISC-V processor.",
        "link": "https://dl.acm.org/doi/10.1145/3446214",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Dolmen: FPGA swarm for safety and liveness verification",
        "authors": "['Emilien Fournier', 'Ciprian Teodorov', 'Loïc Lagadec']",
        "date": "March 2022",
        "source": "DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe",
        "abstract": "To ensure correctness of critical systems, swarm verification produces proofs of failure on systems too large to be verified using model-checking. Recent research efforts exploit both intrinsic parallelism and low-latency on-chip memory offered by FPGAs to achieve 3 orders of magnitude speedups over software. However, these approaches are limited to safety verification that encodes only what the system should not do. Liveness properties express what the system should do, and are widely used in the verification of operating systems, distributed systems, and communication protocols. Both safety and liveness properties are of paramount importance to ensure systems correctness. This paper presents Dolmen, the first FPGA implementation of a swarm verification engine that supports both safety and liveness properties. Dolmen features a deeply pipelined verification core, along with a scalable architecture to allow high-frequency synthesis on large FPGAs. Our experimental results, on a Xilinx Virtex Ultrascale+ FPGA, show that the Dolmen architecture can achieve up to 4 orders of magnitude speedups compared to software model-checking.",
        "link": "https://dl.acm.org/doi/10.5555/3539845.3540176",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Invisible bits: hiding secret messages in SRAM’s analog domain",
        "authors": "['Jubayer Mahmod', 'Matthew Hicks']",
        "date": "February 2022",
        "source": "ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "Electronic devices are increasingly the subject of inspection by authorities. While encryption hides secret messages, it does not hide the transmission of those secret messages---in fact, it calls attention to them. Thus, an adversary, seeing encrypted data, turns to coercion to extract the credentials required to reveal the secret message. Steganographic techniques hide secret messages in plain sight, providing the user with plausible deniability, removing the threat of coercion.   This paper unveils Invisible Bits a new steganographic technique that hides secret messages in the analog domain of Static Random Access Memory (SRAM) embedded within a computing device. Unlike other memory technologies, the power-on state of SRAM reveals the analog-domain properties of its individual cells. We show how to quickly and systematically change the analog-domain properties of SRAM cells to encode data in the analog domain and how to reveal those changes by capturing SRAM's power-on state. Experiments with commercial devices show that Invisible Bits provides over 90% capacity---two orders-of-magnitude more than previous on-chip steganographic approaches, while retaining device functionality---even when the device undergoes subsequent normal operation or is shelved for months. Experiments also show that adversaries cannot differentiate between devices with encoded messages and those without. Lastly, we show how to layer encryption and error correction on top of our message encoding scheme in an end-to-end demonstration.",
        "link": "https://dl.acm.org/doi/10.1145/3503222.3507756",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Design space for scaling-in general purpose computing within the DDR DRAM hierarchy for map-reduce workloads",
        "authors": "['Siddhartha Balakrishna Rai', 'Anand Sivasubramaniam', 'Adithya Kumar', 'Prasanna Venkatesh Rengasamy', 'Vijaykrishnan Narayanan', 'Ameen Akel', 'Sean Eilert']",
        "date": "May 2021",
        "source": "CF '21: Proceedings of the 18th ACM International Conference on Computing Frontiers",
        "abstract": "This paper conducts a design space exploration of placing general purpose RISCV cores within the DDR DRAM hierarchy to boost the performance of important data analytics applications in the datacenter. We investigate the hardware (where? how many? how to interface?) and software (how to place data? how to map computations?) choices for placing these cores within the rank, chip, and bank of the DIMM slots to take advantage of the locality vs. parallelism trade-offs. We use the popular MapReduce paradigm, normally used to scale out workloads across servers, to scale in these workloads into the DDR DRAM hierarchy. We evaluate the design space using diverse off-the-shelf Apache Spark Workloads to show the pros-and-cons of different hardware placement and software mapping strategies. Results show that bank-level RISCV cores can provide tremendous speedup (up to 363X) for the offload-able parts of these applications, amounting to 14X speedup overall in some applications. Even in the non-amenable applications, we get at least 31% performance boost for the entire application. To realize this, we incur an area overhead of 4% at the bank level, and increase in temperature of < 4°C over the chip averaged over all applications.",
        "link": "https://dl.acm.org/doi/10.1145/3457388.3458661",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "NVOverlay: enabling efficient and scalable high-frequency snapshotting to NVM",
        "authors": "['Ziqi Wang', 'Chul-Hwan Choo', 'Michael A. Kozuch', 'Todd C. Mowry', 'Gennady Pekhimenko', 'Vivek Seshadri', 'Dimitrios Skarlatos']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "The ability to capture frequent (per millisecond) persistent snapshots to NVM would enable a number of compelling use cases. Unfortunately, existing NVM snapshotting techniques suffer from a combination of persistence barrier stalls, write amplification to NVM, and/or lack of scalability beyond a single socket. In this paper, we present NVOverlay, which is a scalable and efficient technique for capturing frequent persistent snapshots to NVM such that they can be randomly accessed later. NVOverlay uses Coherent Snapshot Tracking to efficiently track changes to memory (since the previous snapshot) across multi-socket parallel systems, and it uses Multi-snapshot NVM Mapping to store these snapshots to NVM while avoiding excessive write amplification. Our experiments demonstrate that NVOverlay successfully hides the overhead of capturing these snapshots while reducing write amplification by 29%-47% compared with state-of-the-art logging-based snapshotting techniques.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00046",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A one-for-all and o(v log(v ))-cost solution for parallel merge style operations on sorted key-value arrays",
        "authors": "['Bangyan Wang', 'Lei Deng', 'Fei Sun', 'Guohao Dai', 'Liu Liu', 'Yu Wang', 'Yuan Xie']",
        "date": "February 2022",
        "source": "ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "The processing of sorted key-value arrays using a “merge style operation (MSO)” is a very basic and important problem in domains like scientific computing, deep learning, database, graph analysis, sorting, set-operation etc. MSOs dominate the execution time in some important applications like SpGEMM and graph mining. For example, sparse vector addition as an MSO takes up to 98% execution time in SpGEMM in our experiment. For this reason, accelerating MSOs on CPU, GPU, and accelerators using parallel execution has been extensively studied but the solutions in prior work have three major limitations. (1) They treat different MSOs as isolated problems using incompatible methods and an unified solution is still lacking. (2) They do not have the flexibility to support variable key/value sizes and value calculations in the runtime given a fixed hardware design. (3) They require a quadratic hardware cost (O(V2)) for given parallelism V in most cases.  To address above three limitations, we make the following efforts. (1) We present a one-for-all solution to support all interested MSOs based on a unified abstraction model “restricted zip machine (RZM)”. (2) We propose a set of composable and parallel primitives for RZM to provide the flexibility to support variable key/value sizes and value calculations. (3) We provide the hardware design to implement the proposed primitives using only O(Vlog(V)) resource. With the above techniques, a flexible and efficient solution for MSOs has been built. Our design can be used either as a drop-in replacement of the merge unit in prior accelerators to reduce the cost from O(V2) to O(Vlog(V)), or as an extension to the SIMD ISA of CPU and GPU. In our evaluation on CPU, when V=16 (512-bit SIMD, 32-bit element), we achieve significant speedup on a range of representative kernels including set operations (8.4×), database joins (7.3×), sparse vector/matrix/tensor addition/multiplication on real/complex numbers (6.5×), merge sort (8.0× over scalar, 3.4× over the state-of-the-art SIMD), and SpGEMM (4.4× over the best one in the baseline collection).",
        "link": "https://dl.acm.org/doi/10.1145/3503222.3507728",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Exploiting Different Levels of Parallelism in the Quantum Control Microarchitecture for Superconducting Qubits",
        "authors": "['Mengyu Zhang', 'Lei Xie', 'Zhenxing Zhang', 'Qiaonian Yu', 'Guanglei Xi', 'Hualiang Zhang', 'Fuming Liu', 'Yarui Zheng', 'Yicong Zheng', 'Shengyu Zhang']",
        "date": "October 2021",
        "source": "MICRO '21: MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture",
        "abstract": "As current Noisy Intermediate Scale Quantum (NISQ) devices suffer from decoherence errors, any delay in the instruction execution of quantum control microarchitecture can lead to the loss of quantum information and incorrect computation results. Hence, it is crucial for the control microarchitecture to issue quantum operations to the Quantum Processing Unit (QPU) in time. As in classical microarchitecture, parallelism in quantum programs needs to be exploited for speedup. However, three challenges emerge in the quantum scenario: 1) the quantum feedback control can introduce significant pipeline stall latency; 2) timing control is required for all quantum operations; 3) QPU requires a deterministic operation supply to prevent the accumulation of quantum errors.  In this paper, we propose a novel control microarchitecture design to exploit Circuit Level Parallelism (CLP) and Quantum Operation Level Parallelism (QOLP). Firstly, we develop a Multiprocessor architecture to exploit CLP, which supports dynamic scheduling of different sub-circuits. This architecture can handle parallel feedback control and minimize the potential overhead that disrupts the timing control. Secondly, we propose a Quantum Superscalar approach that exploits QOLP by efficiently executing massive quantum instructions in parallel. Both methods issue quantum operations to QPU deterministically. In the benchmark test of a Shor syndrome measurement, a six-core implementation of our proposal achieves up to 2.59 × speedup compared with a single core. For various canonical quantum computing algorithms, our superscalar approach achieves an average of 4.04 × improvement over a baseline design. Finally, We perform a simultaneous randomized benchmarking (simRB) experiment on a real QPU using the proposed microarchitecture for validation.",
        "link": "https://dl.acm.org/doi/10.1145/3466752.3480116",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Distance-in-time versus distance-in-space",
        "authors": "['Mahmut Taylan Kandemir', 'Xulong Tang', 'Hui Zhao', 'Jihyun Ryoo', 'Mustafa Karakoy']",
        "date": "June 2021",
        "source": "PLDI 2021: Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation",
        "abstract": "Cache behavior is one of the major factors that influence the performance of applications. Most of the existing compiler techniques that target cache memories focus exclusively on reducing data reuse distances in time (DIT). However, current manycore systems employ distributed on-chip caches that are connected using an on-chip network. As a result, a reused data element/block needs to travel over this on-chip network, and the distance to be traveled -- reuse distance in space (DIS) -- can be as influential in dictating application performance as reuse DIT. This paper represents the first attempt at defining a compiler framework that accommodates both DIT and DIS. Specifically, it first classifies data reuses into four groups: G1: (low DIT, low DIS), G2: (high DIT, low DIS), G3: (low DIT, high DIS), and G4: (high DIT, high DIS). Then, observing that reuses in G1 represent the ideal case and there is nothing much to be done in computations in G4, it proposes a \"reuse transfer\" strategy that transfers select reuses between G2 and G3, eventually, transforming each reuse to either G1 or G4. Finally, it evaluates the proposed strategy using a set of 10 multithreaded applications. The collected results reveal that the proposed strategy reduces parallel execution times of the tested applications between 19.3% and 33.3%.",
        "link": "https://dl.acm.org/doi/10.1145/3453483.3454069",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "TCN mapping optimization for ultra-low power time-series edge inference",
        "authors": "['Alessio Burrello', 'Alberto Dequino', 'Daniele Jahier Pagliari', 'Francesco Conti', 'Marcello Zanghieri', 'Enrico Macii', 'Luca Benini', 'Massimo Poncino']",
        "date": "July 2021",
        "source": "ISLPED '21: Proceedings of the ACM/IEEE International Symposium on Low Power Electronics and Design",
        "abstract": "Temporal Convolutional Networks (TCNs) are emerging lightweight Deep Learning models for Time Series analysis. We introduce an automated exploration approach and a library of optimized kernels to map TCNs on Parallel Ultra-Low Power (PULP) microcontrollers. Our approach minimizes latency and energy by exploiting a layer tiling optimizer to jointly find the tiling dimensions and select among alternative implementations of the causal and dilated 1D-convolution operations at the core of TCNs. We benchmark our approach on a commercial PULP device, achieving up to 103X lower latency and 20.3X lower energy than the Cube-AI toolkit executed on the STM32L4 and from 2.9X to 26.6X lower energy compared to commercial closed-source and academic open-source approaches on the same hardware target.",
        "link": "https://dl.acm.org/doi/10.1109/ISLPED52811.2021.9502494",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Toward accurate platform-aware performance modeling for deep neural networks",
        "authors": "['Chuan-Chi Wang', 'Ying-Chiao Liao', 'Ming-Chang Kao', 'Wen-Yew Liang', 'Shih-Hao Hung']",
        "date": "March 2021",
        "source": "ACM SIGAPP Applied Computing Review",
        "abstract": "In this paper, we provide a fine-grain machine learning-based method, PerfNetV2, which improves the accuracy of our previous work for modeling the neural network performance on a variety of GPU accelerators. Given an application, the proposed method can be used to predict the inference time and training time of the convolutional neural networks used in the application, which enables the system developer to optimize the performance by choosing the neural networks and/or incorporating the hardware accelerators to deliver satisfactory results in time. Furthermore, the proposed method is capable of predicting the performance of an unseen or non-existing device, e.g. a new GPU which has a higher operating frequency with less processor cores, but more memory capacity. This allows a system developer to quickly search the hardware design space and/or fine-tune the system configuration. Compared to the previous works, PerfNetV2 delivers more accurate results by modeling detailed host-accelerator interactions in executing the full neural networks and improving the architecture of the machine learning model used in the predictor. Our case studies show that PerfNetV2 yields a mean absolute percentage error within 13.1% on LeNet, AlexNet, and VGG16 on NVIDIA GTX-1080Ti, while the error rate on a previous work published in ICBD 2018 could be as large as 200%.",
        "link": "https://dl.acm.org/doi/10.1145/3477133.3477137",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Automatic Code Generation and Optimization of Large-scale Stencil Computation on Many-core Processors",
        "authors": "['Mingzhen Li', 'Yi Liu', 'Hailong Yang', 'Yongmin Hu', 'Qingxiao Sun', 'Bangduo Chen', 'Xin You', 'Xiaoyan Liu', 'Zhongzhi Luan', 'Depei Qian']",
        "date": "August 2021",
        "source": "ICPP '21: Proceedings of the 50th International Conference on Parallel Processing",
        "abstract": "Stencil computation is an indispensable building block of many scientific applications and is widely used by the numerical solvers of partial differential equations (PDEs). Due to the complex computation patterns of different stencils and the various hardware targets (e.g., many-core processors), many domain-specific languages (DSLs) have been proposed to optimize stencil computation. However, existing stencil DSLs mostly focus on the performance optimizations on homogeneous many-core processors such as CPUs and GPUs, and fail to embrace emerging heterogeneous many-core processors such as Sunway. In addition, few of them can support expressing stencil with multiple time dependencies and optimizations from both spatial and temporal dimensions. Moreover, most stencil DSLs are unable to generate codes that can run efficiently in large scale, which limits their practical applicability. In this paper, we propose MSC, a new stencil DSL designed to express stencil computation in both spatial and temporal dimensions. It can generate high-performance stencil codes for large-scale execution on emerging many-core processors. Specially, we design several optimization primitives for improving parallelism and data locality, and a communication library for efficient halo exchange in large scale execution. The experiment results show that our MSC achieves better performance compared to the state-of-the-art stencil DSLs.",
        "link": "https://dl.acm.org/doi/10.1145/3472456.3473517",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Mind mappings: enabling efficient algorithm-accelerator mapping space search",
        "authors": "['Kartik Hegde', 'Po-An Tsai', 'Sitao Huang', 'Vikas Chandra', 'Angshuman Parashar', 'Christopher W. Fletcher']",
        "date": "April 2021",
        "source": "ASPLOS '21: Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "Modern day computing increasingly relies on specialization to satiate growing performance and efficiency requirements. A core challenge in designing such specialized hardware architectures is how to perform mapping space search, i.e., search for an optimal mapping from algorithm to hardware. Prior work shows that choosing an inefficient mapping can lead to multiplicative-factor efficiency overheads. Additionally, the search space is not only large but also non-convex and non-smooth, precluding advanced search techniques. As a result, previous works are forced to implement mapping space search using expert choices or sub-optimal search heuristics.  This work proposes Mind Mappings, a novel gradient-based search method for algorithm-accelerator mapping space search. The key idea is to derive a smooth, differentiable approximation to the otherwise non-smooth, non-convex search space. With a smooth, differentiable approximation, we can leverage efficient gradient-based search algorithms to find high-quality mappings. We extensively compare Mind Mappings to black-box optimization schemes used in prior work. When tasked to find mappings for two important workloads (CNN and MTTKRP), Mind Mapping finds mappings that achieve an average 1.40×, 1.76×, and 1.29× (when run for a fixed number of steps) and 3.16×, 4.19×, and 2.90× (when run for a fixed amount of time) better energy-delay product (EDP) relative to Simulated Annealing, Genetic Algorithms and Reinforcement Learning, respectively. Meanwhile, Mind Mappings returns mappings with only 5.32× higher EDP than a possibly unachievable theoretical lower-bound, indicating proximity to the global optima.",
        "link": "https://dl.acm.org/doi/10.1145/3445814.3446762",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Aion: Enabling Open Systems through Strong Availability Guarantees for Enclaves",
        "authors": "['Fritz Alder', 'Jo Van Bulck', 'Frank Piessens', 'Jan Tobias Mühlberg']",
        "date": "November 2021",
        "source": "CCS '21: Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security",
        "abstract": "Embedded Trusted Execution Environments (TEEs) can provide strong security for software in the IoT or in critical control systems. Approaches to combine this security with real-time and availability guarantees are currently missing. In this paper we present Aion, a configurable security architecture that provides a notion of guaranteed real-time execution for dynamically loaded enclaves. We implement preemptive multitasking and restricted atomicity on top of strong enclave software isolation and attestation. Our approach allows the hardware to enforce confidentiality and integrity protections, while a decoupled small enclaved scheduler software component can enforce availability and guarantee strict deadlines of a bounded number of protected applications, without necessarily introducing a notion of priorities amongst these applications. We implement a prototype on a light-weight TEE processor and provide a case study. Our implementation can guarantee that protected applications can handle interrupts and make progress with deterministic activation latencies, even in the presence of a strong adversary with arbitrary code execution capabilities.",
        "link": "https://dl.acm.org/doi/10.1145/3460120.3484782",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Evaluation of large scale RoI mining applications in edge computing environments",
        "authors": "['Loris Belcastro', 'Alberto Falcone', 'Alfredo Garro', 'Fabrizio Marozzo']",
        "date": "September 2021",
        "source": "DS-RT '21: Proceedings of the 2021 IEEE/ACM 25th International Symposium on Distributed Simulation and Real Time Applications",
        "abstract": "Researchers and leading IT companies are increasingly proposing hybrid cloud/edge solutions, which allow to move part of the workload from the cloud to the edge nodes, by reducing the network traffic and energy consumption, but also getting low latency responses near to real time. This paper proposes a novel hybrid cloud/edge architecture for efficiently extracting Regions-of-Interest (RoI) in a large scale urban computing environment, where a huge amount of geotagged data are generated and collected through users's mobile devices. The proposal is organized in two parts: (i) a modeling part that defines the hybrid cloud/edge architecture capable of managing a large number of devices; (ii) a simulation part in which different design choices are evaluated to improve the performance of RoI mining algorithms in terms of processing time, network delay, task failure and computing resource utilization. Several experiments have been carried out to evaluate the performance of the proposed architecture starting from different configurations and orchestration policies. The achieved results showed that the proposed hybrid cloud/edge architecture, with the use of two novel orchestration policies (network- and utilization-based), permits to improve the exploitation of resources, also granting low network latency and task failure rate in comparison with other standard scenarios (only-edge or only-cloud).",
        "link": "https://dl.acm.org/doi/10.1109/DS-RT52167.2021.9576131",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Cooperative Slack Management: Saving Energy of Multicore Processors by Trading Performance Slack Between QoS-Constrained Applications",
        "authors": "['Mehrzad Nejat', 'Madhavan Manivannan', 'Miquel Pericàs', 'Per Stenström']",
        "date": "None",
        "source": "ACM Transactions on Architecture and Code Optimization",
        "abstract": "Processor resources can be adapted at runtime according to the dynamic behavior of applications to reduce the energy consumption of multicore processors without affecting the Quality-of-Service (QoS). To achieve this, an online resource management scheme is needed to control processor configurations such as cache partitioning, dynamic voltage-frequency scaling, and dynamic adaptation of core resources.Prior State-of-the-art has shown the potential for reducing energy without any performance degradation by coordinating the control of different resources. However, in this article, we show that by allowing short-term variations in processing speed (e.g., instructions per second rate), in a controlled fashion, we can enable substantial improvements in energy savings while maintaining QoS. We keep track of such variations in the form of performance slack. Slack can be generated, at some energy cost, by processing faster than the performance target. On the other hand, it can be utilized to save energy by allowing a temporary relaxation in the performance target. Based on this insight, we present Cooperative Slack Management (CSM). During runtime, CSM finds opportunities to generate slack at low energy cost by estimating the performance and energy for different resource configurations using analytical models. This slack is used later when it enables larger energy savings. CSM performs such trade-offs across multiple applications, which means that the slack collected for one application can be used to reduce the energy consumption of another. This cooperative approach significantly increases the opportunities to reduce system energy compared with independent slack management for each application. For example, we show that CSM can potentially save up to 41% of system energy (on average, 25%) in a scenario in which both prior art and an extended version with local slack management for each core are ineffective.",
        "link": "https://dl.acm.org/doi/10.1145/3505559",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "G-GPU: a fully-automated generator of GPU-like ASIC accelerators",
        "authors": "['Tiago D. Perez', 'Márcio M. Gonçalves', 'Leonardo Gobatto', 'Marcelo Brandalero', 'José Rodrigo Azambuja', 'Samuel Pagliarini']",
        "date": "March 2022",
        "source": "DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe",
        "abstract": "Modern Systems on Chip (SoC), almost as a rule, require accelerators for achieving energy efficiency and high performance for specific tasks that are not necessarily well suited for execution in standard processing units. Considering the broad range of applications and necessity for specialization, the design of SoCs has thus become expressively more challenging. In this paper, we put forward the concept of G-GPU, a general-purpose GPU-like accelerator that is not application-specific but still gives benefits in energy efficiency and throughput. Furthermore, we have identified an existing gap for these accelerators in ASIC, for which no known automated generation platform/tool exists. Our solution, called GPUPlanner, is an open-source generator of accelerators, from RTL to GDSII, that addresses this gap. Our analysis results show that our automatically generated G-GPU designs are remarkably efficient when compared against the popular CPU architecture RISC-V, presenting speed-ups of up to 223 times in raw performance and up to 11 times when the metric is performance derated by area. These results are achieved by executing a design space exploration of the GPU-like accelerators, where the memory hierarchy is broken in a smart fashion and the logic is pipelined on demand. Finally, tapeout-ready layouts of the G-GPU in 65nm CMOS are presented.",
        "link": "https://dl.acm.org/doi/10.5555/3539845.3539972",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Microarchitectural Exploration of STT-MRAM Last-level Cache Parameters for Energy-efficient Devices",
        "authors": "['Tommaso Marinelli', 'José Ignacio Gómez Pérez', 'Christian Tenllado', 'Manu Komalan', 'Mohit Gupta', 'Francky Catthoor']",
        "date": "None",
        "source": "ACM Transactions on Embedded Computing Systems",
        "abstract": "As the technology scaling advances, limitations of traditional memories in terms of density and energy become more evident. Modern caches occupy a large part of a CPU physical size and high static leakage poses a limit to the overall efficiency of the systems, including IoT/edge devices. Several alternatives to CMOS SRAM memories have been studied during the past few decades, some of which already represent a viable replacement for different levels of the cache hierarchy. One of the most promising technologies is the spin-transfer torque magnetic RAM (STT-MRAM), due to its small basic cell design, almost absent static current and non-volatility as an added value. However, nothing comes for free, and designers will have to deal with other limitations, such as the higher latencies and dynamic energy consumption for write operations compared to reads. The goal of this work is to explore several microarchitectural parameters that may overcome some of those drawbacks when using STT-MRAM as last-level cache (LLC) in embedded devices. Such parameters include: number of cache banks, number of miss status handling registers (MSHRs) and write buffer entries, presence of hardware prefetchers. We show that an effective tuning of those parameters may virtually remove any performance loss while saving more than 60% of the LLC energy on average. The analysis is then extended comparing the energy results from calibrated technology models with data obtained with freely available tools, highlighting the importance of using accurate models for architectural exploration.",
        "link": "https://dl.acm.org/doi/10.1145/3490391",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A Case for Precise, Fine-Grained Pointer Synthesis in High-Level Synthesis",
        "authors": "['Nadesh Ramanathan', 'George A. Constantinides', 'John Wickerson']",
        "date": "None",
        "source": "ACM Transactions on Design Automation of Electronic Systems",
        "abstract": "This article combines two practical approaches to improve pointer synthesis within HLS tools. Both approaches focus on inefficiencies in how HLS tools treat the points-to graph—a mapping that connects each instruction to the memory locations that it might access at runtime. HLS pointer synthesis first computes the points-to graph via pointer analysis and then implements its connections in hardware, which gives rise to two inefficiencies. First, HLS tools typically favour pointer analysis that is fast, sacrificing precision. Second, they also favour centralising memory connections in hardware for instructions that can point to more than one location.In this article, we demonstrate that a more precise pointer analysis coupled with decentralised memory connections in hardware can substantially reduce the unnecessary sharing of memory resources. We implement both flow- and context-sensitive pointer analysis and fine-grained memory connections in two modern HLS tools, LegUp and Vitis HLS. An evaluation on three benchmark suites, ranging from non-trivial pointer use to standard HLS benchmarks, indicates that when we improve both precision and granularity of pointer synthesis, on average, we can reduce area and latency by around 42% and 37%, respectively.",
        "link": "https://dl.acm.org/doi/10.1145/3491430",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "PeQES: a platform for privacy-enhanced quantitative empirical studies",
        "authors": "['Dominik Meißner', 'Felix Engelmann', 'Frank Kargl', 'Benjamin Erb']",
        "date": "March 2021",
        "source": "SAC '21: Proceedings of the 36th Annual ACM Symposium on Applied Computing",
        "abstract": "Empirical sciences and in particular psychology suffer a methodological crisis due to the non-reproducibility of results, and in rare cases, questionable research practices. Pre-registered studies and the publication of raw data sets have emerged as effective countermeasures. However, this approach represents only a conceptual procedure and may in some cases exacerbate privacy issues associated with data publications. We establish a novel, privacy-enhanced workflow for pre-registered studies. We also introduce PeQES, a corresponding platform that technically enforces the appropriate execution while at the same time protecting the participants' data from unauthorized use or data repurposing. Our PeQES prototype proves the overall feasibility of our privacy-enhanced workflow while introducing only a negligible performance overhead for data acquisition and data analysis of an actual study. Using trusted computing mechanisms, PeQES is the first platform to enable privacy-enhanced studies, to ensure the integrity of study protocols, and to safeguard the confidentiality of participants' data at the same time.",
        "link": "https://dl.acm.org/doi/10.1145/3412841.3441997",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "LogStore: A Cloud-Native and Multi-Tenant Log Database",
        "authors": "['Wei Cao', 'Xiaojie Feng', 'Boyuan Liang', 'Tianyu Zhang', 'Yusong Gao', 'Yunyang Zhang', 'Feifei Li']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "With the prevalence of cloud computing, more and more enterprises are migrating applications to cloud infrastructures. Logs are the key to helping customers understand the status of their applications running on the cloud. They are vital for various scenarios, such as service stability assessment, root cause analysis and user activity profiling. Therefore, it is essential to manage the massive amount of logs collected on the cloud and tap their value. Although various log storages have been widely used in the past few decades, it is still a non-trivial problem to design a cost-effective log storage for cloud applications. It faces challenges of heavy write throughput of tens of millions of log records per second, retrieval on PB-level logs and massive hundreds of thousands of tenants. Traditional log processing systems cannot satisfy all these requirements. To address these challenges, we propose the cloud-native log database LogStore. It combines shared-nothing and shared-data architecture, and utilizes highly scalable and low-cost cloud object storage, while overcoming the bandwidth limitations and high latency of using remote storage when writing a large number of logs. We also propose a multi-tenant management method that physically isolates tenant data to ensure compliance and flexible data expiration policies, and uses a novel traffic scheduling algorithm to mitigate the impact of traffic skew and hotspots among tenants. In addition, we design an efficient column index structure LogBlock to support queries with full-text search, and combined several query optimization techniques to reduce query latency on cloud object storage. LogStore has been deployed in Alibaba Cloud on a large scale (more than 500 machines), processing logs of more than 100 GB per second, and has been running stably for more than two years.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3457565",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "FlexOS: towards flexible OS isolation",
        "authors": "['Hugo Lefeuvre', 'Vlad-Andrei Bădoiu', 'Alexander Jung', 'Stefan Lucian Teodorescu', 'Sebastian Rauch', 'Felipe Huici', 'Costin Raiciu', 'Pierre Olivier']",
        "date": "February 2022",
        "source": "ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "At design time, modern operating systems are locked in a specific safety and isolation strategy that mixes one or more hardware/software protection mechanisms (e.g. user/kernel separation); revisiting these choices after deployment requires a major refactoring effort. This rigid approach shows its limits given the wide variety of modern applications' safety/performance requirements, when new hardware isolation mechanisms are rolled out, or when existing ones break.   We present FlexOS, a novel OS allowing users to easily specialize the safety and isolation strategy of an OS at compilation/deployment time instead of design time. This modular LibOS is composed of fine-grained components that can be isolated via a range of hardware protection mechanisms with various data sharing strategies and additional software hardening. The OS ships with an exploration technique helping the user navigate the vast safety/performance design space it unlocks. We implement a prototype of the system and demonstrate, for several applications (Redis/Nginx/SQLite), FlexOS' vast configuration space as well as the efficiency of the exploration technique: we evaluate 80 FlexOS configurations for Redis and show how that space can be probabilistically subset to the 5 safest ones under a given performance budget. We also show that, under equivalent configurations, FlexOS performs similarly or better than existing solutions which use fixed safety configurations.",
        "link": "https://dl.acm.org/doi/10.1145/3503222.3507759",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "MIMHD: accurate and efficient hyperdimensional inference using multi-bit in-memory computing",
        "authors": "['Arman Kazemi', 'Mohammad Mehdi Sharifi', 'Zhuowen Zou', 'Michael Niemier', 'X. Sharon Hu', 'Mohsen Imani']",
        "date": "July 2021",
        "source": "ISLPED '21: Proceedings of the ACM/IEEE International Symposium on Low Power Electronics and Design",
        "abstract": "Hyperdimensional Computing (HDC) is an emerging computational framework that mimics important brain functions by operating over high-dimensional vectors, called hypervectors (HVs). In-memory computing implementations of HDC are desirable since they can significantly reduce data transfer overheads. All existing in-memory HDC platforms consider binary HVs where each dimension is represented with a single bit. However, utilizing multi-bit HVs allows HDC to achieve acceptable accuracies in lower dimensions which in turn leads to higher energy efficiencies. Thus, we propose a highly accurate and efficient multi-bit in-memory HDC inference platform called MIMHD. MIMHD supports multi-bit operations using ferroelectric field-effect transistor (FeFET) crossbar arrays for multiply-and-add and FeFET multi-bit content-addressable memories for associative search. We also introduce a novel hardware-aware retraining framework (HWART) that trains the HDC model to learn to work with MIMHD. For six popular datasets and 4000 dimension HVs, MIMHD using 3-bit (2-bit) precision HVs achieves (i) average accuracies of 92.6% (88.9%) which is 8.5% (4.8%) higher than binary implementations; (ii) 84.1x (78.6x) energy improvement over a GPU, and (iii) 38.4x (34.3x) speedup over a GPU, respectively. The 3-bit MIMHD is 4.3x and 13x faster and more energy-efficient than binary HDC accelerators while achieving similar accuracies.",
        "link": "https://dl.acm.org/doi/10.1109/ISLPED52811.2021.9502498",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "LB Scalability: Achieving the Right Balance Between Being Stateful and Stateless",
        "authors": "['Reuven Cohen', 'Matty Kadosh', 'Alan Lo', 'Qasem Sayah']",
        "date": "None",
        "source": "IEEE/ACM Transactions on Networking",
        "abstract": "A high performance Layer-4 load balancer (LB) is one of the most important components of a cloud service infrastructure. Such an LB uses network and transport layer information for deciding how to distribute client requests across a group of servers. A crucial requirement for a stateful LB is per connection consistency (PCC); namely, that all the packets of the same connection will be forwarded to the same server, as long as the server is alive, even if the pool of servers or the assignment function changes. The challenge is in designing a high throughput, low latency solution that is also scalable. This paper proposes a highly scalable LB, called Prism, implemented using a programmable switch ASIC. As far as we know, Prism is the first reported stateful LB that can process millions of connections per second and hundreds of millions connections in total, while ensuring PCC. This is due to the fact that Prism forwards all the packets in hardware, even during server pool changes, <bold>while avoiding the need to maintain a hardware state per every active connection</bold>. We implemented a prototype of the proposed architecture and showed that Prism can scale to 100 million simultaneous connections, and can accommodate more than one pool update per second.",
        "link": "https://dl.acm.org/doi/10.1109/TNET.2021.3112517",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "JFMNet: Joint Fusion Multi-Networks for Image Dehazing and Denoising in The Port Environment",
        "authors": "['Guancheng Lin', 'Yijie Zheng', 'Zhihong Xu', 'Tianzhi Xia', 'Peng Yuan']",
        "date": "February 2022",
        "source": "ICMLC '22: Proceedings of the 2022 14th International Conference on Machine Learning and Computing",
        "abstract": "The bad weather events, such as haze, in maritime traffic dramatically reduce the visibility, which can seriously affect the ship navigation especially in areas with intensive port traffic. Meanwhile, unwanted signals are inevitably introduced by the maritime imaging device during image capturing and transmission in hazy conditions. Therefore, the captured image is not only degraded by the haze, but also may contain unwanted noise. These low-quality images interfere with the subsequent image processing and increase the potential for maritime traffic accidents. It is therefore imperative to improve the image quality in hazy conditions. To reveal the information hidden in the haze while suppress noise, this paper proposes the joint fusion multi-networks (termed JFMNet) for Image dehazing and denoising in the port environment. The multi-networks use the dehazing module (DHNet) and the denoising module (DNNet) to suppress the noise and haze. Then use the information fusion module (FNet) to integrate the results of the DNNet and DHNet with the information of the original input images to achieve the goal of dehazing and denoising while preserving the details. The modules in multi-networks are based on an encoder-decoder structure. Experiments on a number of challenging hazy images with noise are present to reveal the efficacy of this structure. Meanwhile, experiments also show our JFMNet's superiority over several state-of-the-arts in terms of dehaze quality and efficiency.",
        "link": "https://dl.acm.org/doi/10.1145/3529836.3529923",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "VeGen: a vectorizer generator for SIMD and beyond",
        "authors": "['Yishen Chen', 'Charith Mendis', 'Michael Carbin', 'Saman Amarasinghe']",
        "date": "April 2021",
        "source": "ASPLOS '21: Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "Vector instructions are ubiquitous in modern processors. Traditional compiler auto-vectorization techniques have focused on targeting single instruction multiple data (SIMD) instructions. However, these auto-vectorization techniques are not sufficiently powerful to model non-SIMD vector instructions, which can accelerate applications in domains such as image processing, digital signal processing, and machine learning. To target non-SIMD instruction, compiler developers have resorted to complicated, ad hoc peephole optimizations, expending significant development time while still coming up short. As vector instruction sets continue to rapidly evolve, compilers cannot keep up with these new hardware capabilities.   In this paper, we introduce Lane Level Parallelism (LLP), which captures the model of parallelism implemented by both SIMD and non-SIMD vector instructions. We present VeGen, a vectorizer generator that automatically generates a vectorization pass to uncover target-architecture-specific LLP in programs while using only instruction semantics as input. VeGen decouples, yet coordinates automatically generated target-specific vectorization utilities with its target-independent vectorization algorithm. This design enables us to systematically target non-SIMD vector instructions that until now require ad hoc coordination between different compiler stages. We show that VeGen can use non-SIMD vector instructions effectively, for example, getting speedup 3× (compared to LLVM’s vectorizer) on x265’s idct4 kernel.",
        "link": "https://dl.acm.org/doi/10.1145/3445814.3446692",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A low-cost methodology for EM fault emulation on FPGA",
        "authors": "['Paolo Maistri', 'Jiayun Po']",
        "date": "March 2022",
        "source": "DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe",
        "abstract": "In embedded systems, the presence of a security layer is now a well-established requirement. In order to guarantee the suitable level of performance and resistance against attacks, dedicated hardware implementations are often proposed to accelerate cryptographic computations in a controllable environment. On the other hand, these same implementations may be vulnerable to physical attacks, such as side channel analysis or fault injections. In this scenario, the designer must hence be able to assess the robustness of the implementation (and of the adopted countermeasures) as soon as possible in the design flow against several different threats. In this paper, we propose a methodology to characterize the robustness of a generic hardware design described at RTL against EM fault injections. Thanks to our framework, we are able to emulate the EM faults on FPGA platforms, without the need of expensive equipment or lengthy experimental campaigns. We present a tool supporting our methodology and the first validations tests done on several AES designs confirming the feasibility of the proposed approach.",
        "link": "https://dl.acm.org/doi/10.5555/3539845.3540127",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Q-VR: system-level design for future mobile collaborative virtual reality",
        "authors": "['Chenhao Xie', 'Xie Li', 'Yang Hu', 'Huwan Peng', 'Michael Taylor', 'Shuaiwen Leon Song']",
        "date": "April 2021",
        "source": "ASPLOS '21: Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "High Quality Mobile Virtual Reality (VR) is what the incoming graphics technology era demands: users around the world, regardless of their hardware and network conditions, can all enjoy the immersive virtual experience. However, the state-of-the-art software-based mobile VR designs cannot fully satisfy the realtime performance requirements due to the highly interactive nature of user's actions and complex environmental constraints during VR execution. Inspired by the unique human visual system effects and the strong correlation between VR motion features and realtime hardware-level information, we propose Q-VR, a novel dynamic collaborative rendering solution via software-hardware co-design for enabling future low-latency high-quality mobile VR. At software-level, Q-VR provides flexible high-level tuning interface to reduce network latency while maintaining user perception. At hardware-level, Q-VR accommodates a wide spectrum of hardware and network conditions across users by effectively leveraging the computing capability of the increasingly powerful VR hardware. Extensive evaluation on real-world games demonstrates that Q-VR can achieve an average end-to-end performance speedup of 3.4x (up to 6.7x) over the traditional local rendering design in commercial VR devices, and a 4.1x frame rate improvement over the state-of-the-art static collaborative rendering.",
        "link": "https://dl.acm.org/doi/10.1145/3445814.3446715",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "ThundeRiNG: generating multiple independent random number sequences on FPGAs",
        "authors": "['Hongshi Tan', 'Xinyu Chen', 'Yao Chen', 'Bingsheng He', 'Weng-Fai Wong']",
        "date": "June 2021",
        "source": "ICS '21: Proceedings of the ACM International Conference on Supercomputing",
        "abstract": "In this paper, we propose ThundeRiNG, a resource-efficient and high-throughput system for generating multiple independent sequences of random numbers (MISRN) on FPGAs. Generating MISRN can be a time-consuming step in many applications such as numeric computation and approximate computing. Despite that decades of studies on generating a single sequence of random numbers on FPGAs have achieved very high throughput and high quality of randomness, existing MISRN approaches either suffer from heavy resource consumption or fail to achieve statistical independence among sequences. In contrast, ThundeRiNG resolves the dependence by using a resource-efficient decorrelator among multiple sequences, guaranteeing a high statistical quality of randomness. Moreover, ThundeRiNG develops a novel state sharing among a massive number of pseudo-random number generator instances on FPGAs. The experimental results show that ThundeRiNG successfully passes the widely used statistical test, TestU01, only consumes a constant number of DSPs (less than 1% of the FPGA resource capacity) for generating any number of sequences, and achieves a throughput of 655 billion random numbers per second. Compared to the state-of-the-art GPU library, ThundeRiNG demonstrates a 10.62x speedup on MISRN and delivers up to 9.15x performance and 26.63x power efficiency improvement on two applications (pi estimation and Monte Carlo option pricing). This work is open-sourced on Github at https://github.com/Xtra-Computing/ThundeRiNG.",
        "link": "https://dl.acm.org/doi/10.1145/3447818.3461664",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Tile size selection of affine programs for GPGPUs using polyhedral cross-compilation",
        "authors": "['Khaled Abdelaal', 'Martin Kong']",
        "date": "June 2021",
        "source": "ICS '21: Proceedings of the ACM International Conference on Supercomputing",
        "abstract": "Loop tiling is a key high-level transformation which is known to maximize locality in loop intensive programs. It has been successfully applied to a number of applications including tensor contractions, iterative stencils and machine learning. This technique has also been extended to a wide variety of computational domains and architectures. The performance achieved with this critical transformation largely depends on a set of inputs given, the tile sizes, due to the complex trade-off between locality and parallelism. This problem is exacerbated in GPGPU architectures due to limited hardware resources such as the available shared-memory. In this paper we present a new technique to compute resource conscious tile sizes for affine programs. We use Integer Linear Programming (ILP) constraints and objectives in a cross-compiler fashion to faithfully and effectively mimic the transformations applied in a polyhedral GPU compiler (PPCG). Our approach significantly reduces the need for experimental auto-tuning by generating only two tile size configurations that achieve strong out-of-the-box performance. We evaluate the effectiveness of our technique using the Polybench benchmark suite on two GPGPUs, an AMD Radeon VII and an NVIDIA Tesla V100, using OpenCL and CUDA programming models. Experimental validation reveals that our approach achieves nearly 75% of the best empirically found tile configuration across both architectures.",
        "link": "https://dl.acm.org/doi/10.1145/3447818.3460369",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Accurate and Accelerated Neuromorphic Network Design Leveraging A Bayesian Hyperparameter Pareto Optimization Approach",
        "authors": "['Maryam Parsa', 'Catherine Schuman', 'Nitin Rathi', 'Amir Ziabari', 'Derek Rose', 'J. Parker Mitchell', 'J. Travis Johnston', 'Bill Kay', 'Steven Young', 'Kaushik Roy']",
        "date": "July 2021",
        "source": "ICONS 2021: International Conference on Neuromorphic Systems 2021",
        "abstract": "Neuromorphic systems allow for extremely efficient hardware implementations for neural networks (NNs). In recent years, several algorithms have been presented to train spiking NNs (SNNs) for neuromorphic hardware. However, SNNs often provide lower accuracy than their artificial NNs (ANNs) counterparts or require computationally expensive and slow training/inference methods. To close this gap, designers typically rely on reconfiguring SNNs through adjustments in the neuron/synapse model or training algorithm itself. Nevertheless, these steps incur significant design time, while still lacking the desired improvement in terms of training/inference times (latency). Designing SNNs that can mimic the accuracy of ANNs with reasonable training times is an exigent challenge in neuromorphic computing. In this work, we present an alternative approach that looks at such designs as an optimization problem rather than algorithm or architecture redesign. We develop a versatile multiobjective hyperparameter optimization (HPO) for automatically tuning HPs of two state-of-the-art SNN training algorithms, SLAYER and HYBRID. We emphasize that, to the best of our knowledge, this is the first work trying to improve SNNs’ computational efficiency, accuracy, and training time using an efficient HPO. We demonstrate significant performance improvements for SNNs on several datasets without the need to redesign or invent new training algorithms/architectures. Our approach results in more accurate networks with lower latency and, in turn, higher energy efficiency than previous implementations. In particular, we demonstrate improvement in accuracy and more than 5 × reduction in the training/inference time for the SLAYER algorithm on the DVS Gesture dataset. In the case of HYBRID, we demonstrate 30% reduction in timesteps while surpassing the accuracy of the state-of-the-art networks on CIFAR10. Further, our analysis suggests that even a seemingly minor change in HPs could change the accuracy by 5 − 6 ×.",
        "link": "https://dl.acm.org/doi/10.1145/3477145.3477160",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Benchmarking Quantum Computers and the Impact of Quantum Noise",
        "authors": "['Salonik Resch', 'Ulya R. Karpuzcu']",
        "date": "None",
        "source": "ACM Computing Surveys",
        "abstract": "Benchmarking is how the performance of a computing system is determined. Surprisingly, even for classical computers this is not a straightforward process. One must choose the appropriate benchmark and metrics to extract meaningful results. Different benchmarks test the system in different ways, and each individual metric may or may not be of interest. Choosing the appropriate approach is tricky. The situation is even more open ended for quantum computers, where there is a wider range of hardware, fewer established guidelines, and additional complicating factors. Notably, quantum noise significantly impacts performance and is difficult to model accurately. Here, we discuss benchmarking of quantum computers from a computer architecture perspective and provide numerical simulations highlighting challenges that suggest caution.",
        "link": "https://dl.acm.org/doi/10.1145/3464420",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Blockchain-based root of trust management in security credential management system for vehicular communications",
        "authors": "['Arijet Sarker', 'SangHyun Byun', 'Wenjun Fan', 'Sang-Yoon Chang']",
        "date": "March 2021",
        "source": "SAC '21: Proceedings of the 36th Annual ACM Symposium on Applied Computing",
        "abstract": "Security Credential Management System (SCMS) provides the Public Key Infrastructure (PKI) for vehicular networking. SCMS builds the state-of-the-art distributed PKI to protect the vehicular networking privacy against an honest-but-curious authority (by the use of multiple PKI authorities) and to decentralize the PKI root of trust (by the Elector-Based Root Management or EBRM, having the distributed electors manage the Root Certificate Authority or RCA). We build on the EBRM architecture and construct a Blockchain-Based Root Management (BBRM) to provide even greater decentralization and security. More specifically, BBRM uses blockchain to i) replace the existing RCA and have the electors directly involved in the root certificate generation, ii) control the elector network membership including elector addition and revocation, and iii) provide greater accountability and transparency on the aforementioned functionalities. We implement BBRM on Hyperledger Fabric using smart contract for system experimentation and analyses. Our experiments show that BBRM is lightweight in processing, efficient in ledger size, and supports a bandwidth of multiple transactions per second. Our results show that the BBRM blockchain is appropriate for the root certificate generation and the elector membership control for EBRM within SCMS, which are significantly smaller in number and occurrences than the SCMS outputs of vehicle certificates. We also experiment to analyze how the BBRM distributed consensus protocol parameters, such as the number of electors and the number of required votes, affect the overall scheme's performances.",
        "link": "https://dl.acm.org/doi/10.1145/3412841.3441905",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "The Physical Design of Biological Systems - Insights from the Fly Brain",
        "authors": "['Louis K. Scheffer']",
        "date": "March 2021",
        "source": "ISPD '21: Proceedings of the 2021 International Symposium on Physical Design",
        "abstract": "Many different physical substrates can support complex computation. This is particularly apparent when considering human made and biological systems that perform similar functions, such as visually guided navigation. In common, however, is the need for good physical design, as such designs are smaller, faster, lighter, and lower power, factors in both the jungle and the marketplace. Although the physical design of man-made systems is relatively well understood, the physical design of biological computation has remained murky due to a lack of detailed information on their construction. The recent EM (electron microscope) reconstruction of the central brain of the fruit fly now allows us to start to examine these issues. Here we look at the physical design of the fly brain, including such factors as fan-in and fanout, logic depth, division into physical compartments and how this affects electrical response, pin to computation ratios (Rent's rule), and other physical characteristics of at least one biological computation substrate. From this we speculate on how physical design algorithms might change if the target implementation was a biological neural network.",
        "link": "https://dl.acm.org/doi/10.1145/3439706.3446898",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "We need kernel interposition over the network dataplane",
        "authors": "['Hugo Sadok', 'Zhipeng Zhao', 'Valerie Choung', 'Nirav Atre', 'Daniel S. Berger', 'James C. Hoe', 'Aurojit Panda', 'Justine Sherry']",
        "date": "June 2021",
        "source": "HotOS '21: Proceedings of the Workshop on Hot Topics in Operating Systems",
        "abstract": "Kernel-bypass networking, which allows applications to circumvent the kernel and interface directly with NIC hardware, is one of the main tools for improving application network performance. However, allowing applications to circumvent the kernel makes it impossible to use tools (e.g., tcpdump) or impose policies (e.g., QoS and filters) that need to interpose on traffic sent by different applications running on a host. This makes maintainability and manageability a challenge for kernel-bypass applications. In response, we propose Kernel On-Path Interposition (KOPI), in which traditional kernel data-plane functionality is retained but implemented in a fully programmable SmartNIC. We hypothesize that KOPI can support the same tools and policies as the kernel stack while retaining the performance benefits of kernel bypass.",
        "link": "https://dl.acm.org/doi/10.1145/3458336.3465281",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Experimental Teaching Reform of Computer Graphics Oriented to GPU",
        "authors": "['Lin Wan', 'Ying Cheng']",
        "date": "May 2021",
        "source": "ICMET 2021: 2021 3rd International Conference on Modern Educational Technology",
        "abstract": "By analyzing the current situation of \"computer graphics\" experimental teaching, the results point out that there are deficiencies in guiding students to recognize GPU (Graphics Processing Unit) in the current experimental teaching. Through the construction of GPU rendering architecture cognitive experiments, GPU computing performance cognitive experiments and GPU graphics interface consistency testing experiments, the progressive planning of each part, the introduction of shaders and shading language, the design of typical cases such as particle system and deferred rendering, a complete graphics experiment system oriented to GPU is obtained, which effectively supplements and improves the original course experiment.",
        "link": "https://dl.acm.org/doi/10.1145/3468978.3468983",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "SparseCore: stream ISA and processor specialization for sparse computation",
        "authors": "['Gengyu Rao', 'Jingji Chen', 'Jason Yik', 'Xuehai Qian']",
        "date": "February 2022",
        "source": "ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "Computation on sparse data is becoming increasingly important for many applications. Recent sparse computation accelerators are designed for specific algorithm/application, making them inflexible with software optimizations. This paper proposes SparseCore, the first general-purpose processor extension for sparse computation that can flexibly accelerate complex code patterns and fast-evolving algorithms. We extend the instruction set architecture (ISA) to make stream or sparse vector first-class citizens, and develop efficient architectural components to support the stream ISA. The novel ISA extension intrinsically operates on streams, realizing both efficient data movement and computation. The simulation results show that SparseCore achieves significant speedups for sparse tensor computation and graph pattern computation.",
        "link": "https://dl.acm.org/doi/10.1145/3503222.3507705",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Experience Paper: Danaus: isolation and efficiency of container I/O at the client side of network storage",
        "authors": "['Giorgos Kappes', 'Stergios V. Anastasiadis']",
        "date": "December 2021",
        "source": "Middleware '21: Proceedings of the 22nd International Middleware Conference",
        "abstract": "Containers are a mainstream virtualization technique commonly used to run stateful workloads over persistent storage. In multi-tenant hosts with high utilization, resource contention at the system kernel often leads to inefficient handling of the container I/O. Assuming a distributed storage architecture for scalability, resource sharing is particularly problematic at the client hosts serving the applications of competing tenants. Although increasing the scalability of a system kernel can improve resource efficiency, it is highly challenging to refactor the kernel for fair access to system services. As a realistic alternative, we isolate the storage I/O paths of different tenants by serving them with distinct clients running at user level. We introduce the Danaus client architecture to let each tenant access the container root and application filesystems over a private host path. We developed a Danaus prototype that integrates a union filesystem with a Ceph distributed filesystem client and a configurable shared cache. Across different host configurations, workloads and systems, Danaus achieves improved performance stability because it handles I/O with reserved per-tenant resources and avoids intensive kernel locking. Danaus offers up to 14.4x higher throughput than a popular kernel-based client under conditions of I/O contention. In comparison to a FUSE-based user-level client, Danaus also reduces by 14.2x the time to start 256 high-performance webservers. Based on our extensive experience from building and evaluating Danaus, we share several valuable lessons that we learned about resource contention, file management, service separation and performance stability.",
        "link": "https://dl.acm.org/doi/10.1145/3464298.3493390",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "ViK: practical mitigation of temporal memory safety violations through object ID inspection",
        "authors": "['Haehyun Cho', 'Jinbum Park', 'Adam Oest', 'Tiffany Bao', 'Ruoyu Wang', 'Yan Shoshitaishvili', 'Adam Doupé', 'Gail-Joon Ahn']",
        "date": "February 2022",
        "source": "ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "Temporal memory safety violations, such as use-after-free (UAF) vulnerabilities, are a critical security issue for software written in memory-unsafe languages such as C and C++.   In this paper, we introduce ViK, a novel, lightweight, and widely applicable runtime defense that can protect both operating system (OS) kernels and user-space applications against temporal memory safety violations. ViK performs object ID inspection, where it assigns a random identifier to every allocated object and stores the identifier in the unused bits of the corresponding pointer. When the pointer is used, ViK inspects the value of a pointer before dereferencing, ensuring that the pointer still references the original object. To the best of our knowledge, this is the first mitigation against temporal memory safety violations that scales to OS kernels. We evaluated the software prototype of ViK on Android and Linux kernels and observed runtime overhead of around 20%. Also, we evaluated a hardware-assisted prototype of ViK on Android kernel, where the runtime overhead was as low as 2%.",
        "link": "https://dl.acm.org/doi/10.1145/3503222.3507780",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "REVAMP: a systematic framework for heterogeneous CGRA realization",
        "authors": "['Thilini Kaushalya Bandara', 'Dhananjaya Wijerathne', 'Tulika Mitra', 'Li-Shiuan Peh']",
        "date": "February 2022",
        "source": "ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "Coarse-Grained Reconfigurable Architectures (CGRAs) provide an excellent balance between performance, energy efficiency, and flexibility. However, increasingly sophisticated applications, especially on the edge devices, demand even better energy efficiency for longer battery life.   Most CGRAs adhere to a canonical structure where a homogeneous set of processing elements and memories communicate through a regular interconnect due to the simplicity of the design. Unfortunately, the homogeneity leads to substantial idle resources while mapping irregular applications and creates inefficiency. We plan to mitigate the inefficiency by systematically and judiciously introducing heterogeneity in CGRAs in tandem with appropriate compiler support.   We propose REVAMP, an automated design space exploration framework that helps architects uncover and add pertinent heterogeneity to a diverse range of originally homogeneous CGRAs when fed with a suite of target applications. REVAMP explores a comprehensive set of optimizations encompassing compute, network, and memory heterogeneity, thereby converting a uniform CGRA into a more irregular architecture with improved energy efficiency. As CGRAs are inherently software scheduled, any micro-architectural optimizations need to be partnered with corresponding compiler support, which is challenging with heterogeneity. The REVAMP framework extends compiler support for efficient mapping of loop kernels on the derived heterogeneous CGRA architectures.   We showcase REVAMP on three state-of-the-art homogeneous CGRAs, demonstrating how REVAMP derives a heterogeneous variant of each homogeneous architecture, with its corresponding compiler optimizations. Our results show that the derived heterogeneous architectures achieve up to 52.4% power reduction, 38.1% area reduction, and 36% average energy reduction over the corresponding homogeneous versions with minimal performance impact for the selected kernel suite.",
        "link": "https://dl.acm.org/doi/10.1145/3503222.3507772",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "BPLight-CNN: A Photonics-Based Backpropagation Accelerator for Deep Learning",
        "authors": "['Dharanidhar Dang', 'Sai Vineel Reddy Chittamuru', 'Sudeep Pasricha', 'Rabi Mahapatra', 'Debashis Sahoo']",
        "date": "None",
        "source": "ACM Journal on Emerging Technologies in Computing Systems",
        "abstract": "Training deep learning networks involves continuous weight updates across the various layers of the deep network while using a backpropagation (BP) algorithm. This results in expensive computation overheads during training. Consequently, most deep learning accelerators today employ pretrained weights and focus only on improving the design of the inference phase. The recent trend is to build a complete deep learning accelerator by incorporating the training module. Such efforts require an ultra-fast chip architecture for executing the BP algorithm. In this article, we propose a novel photonics-based backpropagation accelerator for high-performance deep learning training. We present the design for a convolutional neural network (CNN), BPLight-CNN, which incorporates the silicon photonics-based backpropagation accelerator. BPLight-CNN is a first-of-its-kind photonic and memristor-based CNN architecture for end-to-end training and prediction. We evaluate BPLight-CNN using a photonic CAD framework (IPKISS) on deep learning benchmark models, including LeNet and VGG-Net. The proposed design achieves (i) at least 34× speedup, 34× improvement in computational efficiency, and 38.5× energy savings during training; and (ii) 29× speedup, 31× improvement in computational efficiency, and 38.7× improvement in energy savings during inference compared with the state-of-the-art designs. All of these comparisons are done at a 16-bit resolution, and BPLight-CNN achieves these improvements at a cost of approximately 6% lower accuracy compared with the state-of-the-art.",
        "link": "https://dl.acm.org/doi/10.1145/3446212",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "TMO: transparent memory offloading in datacenters",
        "authors": "['Johannes Weiner', 'Niket Agarwal', 'Dan Schatzberg', 'Leon Yang', 'Hao Wang', 'Blaise Sanouillet', 'Bikash Sharma', 'Tejun Heo', 'Mayank Jain', 'Chunqiang Tang', 'Dimitrios Skarlatos']",
        "date": "February 2022",
        "source": "ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "The unrelenting growth of the memory needs of emerging datacenter applications, along with ever increasing cost and volatility of DRAM prices, has led to DRAM being a major infrastructure expense. Alternative technologies, such as NVMe SSDs and upcoming NVM devices, offer higher capacity than DRAM at a fraction of the cost and power. One promising approach is to transparently offload colder memory to cheaper memory technologies via kernel or hypervisor techniques. The key challenge, however, is to develop a datacenter-scale solution that is robust in dealing with diverse workloads and large performance variance of different offload devices such as compressed memory, SSD, and NVM. This paper presents TMO, Meta’s transparent memory offloading solution for heterogeneous datacenter environments. TMO introduces a new Linux kernel mechanism that directly measures in realtime the lost work due to resource shortage across CPU, memory, and I/O. Guided by this information and without any prior application knowledge, TMO automatically adjusts how much memory to offload to heterogeneous devices (e.g., compressed memory or SSD) according to the device’s performance characteristics and the application’s sensitivity to memory-access slowdown. TMO holistically identifies offloading opportunities from not only the application containers but also the sidecar containers that provide infrastructure-level functions. To maximize memory savings, TMO targets both anonymous memory and file cache, and balances the swap-in rate of anonymous memory and the reload rate of file pages that were recently evicted from the file cache. TMO has been running in production for more than a year, and has saved between 20-32% of the total memory across millions of servers in our large datacenter fleet. We have successfully upstreamed TMO into the Linux kernel.",
        "link": "https://dl.acm.org/doi/10.1145/3503222.3507731",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Distributed federated service chaining for heterogeneous network environments",
        "authors": "['Chen Chen', 'Lars Nagel', 'Lin Cui', 'Fung Po Tso']",
        "date": "December 2021",
        "source": "UCC '21: Proceedings of the 14th IEEE/ACM International Conference on Utility and Cloud Computing",
        "abstract": "Future networks are expected to support cross-domain, cost-aware and fine-grained services in an efficient and flexible manner. Service Function Chaining (SFC) has been introduced as a promising approach to deliver these services. In the literature, centralized resource orchestration is usually employed to process SFC requests and manage computing and network resources. However, centralized approaches inhibit the scalability and domain autonomy in multi-domain networks. They also neglect location and hardware dependencies of service chains. In this paper, we propose federated service chaining, a distributed framework which orchestrates and maintains the SFC placement while sharing a minimal amount of domain information and control. We first formulate a deployment cost minimization problem as an Integer Linear Programming (ILP) problem with fine-grained constraints for location and hardware dependencies, which is NP-hard. We then devise a Distributed Federated Service Chaining placement approach (DFSC) using inter-domain paths and border nodes information. Our extensive experiments demonstrate that DFSC efficiently optimizes the deployment cost, supports domain autonomy and enables faster decision-making. The results show that DFSC finds solutions within a factor 1.15 of the optimal solution. Compared to a centralized approach in the literature, DFSC reduces the deployment cost by 12% while being one order of magnitude faster.",
        "link": "https://dl.acm.org/doi/10.1145/3468737.3494091",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "HeteroFuzz: fuzz testing to detect platform dependent divergence for heterogeneous applications",
        "authors": "['Qian Zhang', 'Jiyuan Wang', 'Miryung Kim']",
        "date": "August 2021",
        "source": "ESEC/FSE 2021: Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
        "abstract": "As specialized hardware accelerators like FPGAs become a prominent part of the current computing landscape, software applications are increasingly constructed to leverage heterogeneous architectures. Such a trend is already happening in the domain of machine learning and Internet-of-Things (IoT) systems built on edge devices. Yet, debugging and testing methods for heterogeneous applications are currently lacking. These applications may look similar to regular C/C++ code but include hardware synthesis details in terms of preprocessor directives. Therefore, their behavior under heterogeneous architectures may diverge significantly from CPU due to hardware synthesis details. Further, the compilation and hardware simulation cycle takes an enormous amount of time, prohibiting frequent invocations required for fuzz testing.   We propose a novel fuzz testing technique, called HeteroFuzz, designed to specifically target heterogeneous applications and to detect platform-dependent divergence. The key essence of HeteroFuzz is that it uses a three-pronged approach to reduce the long latency of repetitively invoking a hardware simulator on a heterogeneous application. First, in addition to monitoring code coverage as a fuzzing guidance mechanism, we analyze synthesis pragmas in kernel code and monitor accelerator-relevant value spectra. Second, we design dynamic probabilistic mutations to increase the chance of hitting divergent behavior under different platforms. Third, we memorize the boundaries of seen kernel inputs and skip HLS simulator invocation if it can expose only redundant divergent behavior. We evaluate HeteroFuzz on seven real-world heterogeneous applications with FPGA kernels. HeteroFuzz is 754X faster in exposing the same set of distinct divergence symptoms than naive fuzzing. Probabilistic mutations contribute to 17.5X speed up than the one without. Selective invocation of HLS simulation contributes to 8.8X speed up than the one without.",
        "link": "https://dl.acm.org/doi/10.1145/3468264.3468610",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "GREENHOME: A Household Energy Consumption and CO2 Footprint Metering Environment",
        "authors": "['Genoveva Vargas-Solar', 'Maysaa Khalil', 'Javier A. Espinosa-Oviedo', 'José-Luis Zechinelli-Martini']",
        "date": "None",
        "source": "ACM Transactions on Internet Technology",
        "abstract": "This article presents the GREENHOME environment, a toolkit providing several data analytical tools for metering household energy consumption and CO2 footprint under different perspectives. GREENHOME enables a multi-perspective analysis of household energy consumption and CO2 footprint using and combining several variables through various statistics and data mining algorithms. To test GREENHOME, the article reports on experiments conducted for modelling and forecasting energy consumption and CO2 footprint in the context of the Triple-A European project.",
        "link": "https://dl.acm.org/doi/10.1145/3505264",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Revizor: testing black-box CPUs against speculation contracts",
        "authors": "['Oleksii Oleksenko', 'Christof Fetzer', 'Boris Köpf', 'Mark Silberstein']",
        "date": "February 2022",
        "source": "ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "Speculative vulnerabilities such as Spectre and Meltdown expose speculative execution state that can be exploited to leak information across security domains via side-channels. Such vulnerabilities often stay undetected for a long time as we lack the tools for systematic testing of CPUs to find them.   In this paper, we propose an approach to automatically detect microarchitectural information leakage in commercial black-box CPUs. We build on speculation contracts, which we employ to specify the permitted side effects of program execution on the CPU's microarchitectural state. We propose a Model-based Relational Testing (MRT) technique to empirically assess the CPU compliance with these specifications.   We implement MRT in a testing framework called Revizor, and showcase its effectiveness on real Intel x86 CPUs. Revizor automatically detects violations of a rich set of contracts, or indicates their absence. A highlight of our findings is that Revizor managed to automatically surface Spectre, MDS, and LVI, as well as several previously unknown variants.",
        "link": "https://dl.acm.org/doi/10.1145/3503222.3507729",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Non-recurring engineering (NRE) best practices: a case study with the NERSC/NVIDIA OpenMP contract",
        "authors": "['Christopher S. Daley', 'Annemarie Southwell', 'Rahulkumar Gayatri', 'Scott Biersdorfff', 'Craig Toepfer', 'Güray Özen', 'Nicholas J. Wright']",
        "date": "November 2021",
        "source": "SC '21: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",
        "abstract": "The NERSC supercomputer, Perlmutter, consists of AMD CPUs and NVIDIA GPUs. NERSC users expect to be able to use OpenMP to take advantage of the highly capable GPUs. This paper describes how NERSC/NVIDIA constructed a Non-Recurring Engineering (NRE) contract to add OpenMP GPU-offload support to the NVIDIA HPC compilers. The paper describes how the contract incorporated the strengths of both parties and encouraged collaboration to improve the quality of the final deliverable. We include our best practices and how this particular contract took into account emerging OpenMP specifications, NERSC workload requirements, and how to use OpenMP most efficiently on GPU hardware. This paper includes OpenMP application performance results obtained with the NVIDIA compilers distributed in the NVIDIA HPC SDK.",
        "link": "https://dl.acm.org/doi/10.1145/3458817.3476213",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A Survey of Microarchitectural Side-channel Vulnerabilities, Attacks, and Defenses in Cryptography",
        "authors": "['Xiaoxuan Lou', 'Tianwei Zhang', 'Jun Jiang', 'Yinqian Zhang']",
        "date": "None",
        "source": "ACM Computing Surveys",
        "abstract": "Side-channel attacks have become a severe threat to the confidentiality of computer applications and systems. One popular type of such attacks is the microarchitectural attack, where the adversary exploits the hardware features to break the protection enforced by the operating system and steal the secrets from the program. In this article, we systematize microarchitectural side channels with a focus on attacks and defenses in cryptographic applications. We make three contributions. (1) We survey past research literature to categorize microarchitectural side-channel attacks. Since these are hardware attacks targeting software, we summarize the vulnerable implementations in software, as well as flawed designs in hardware. (2) We identify common strategies to mitigate microarchitectural attacks, from the application, OS, and hardware levels. (3) We conduct a large-scale evaluation on popular cryptographic applications in the real world and analyze the severity, practicality, and impact of side-channel vulnerabilities. This survey is expected to inspire side-channel research community to discover new attacks, and more importantly, propose new defense solutions against them.",
        "link": "https://dl.acm.org/doi/10.1145/3456629",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "MC2-RAM: an in-8T-SRAM computing macro featuring multi-bit charge-domain computing and ADC-reduction weight encoding",
        "authors": "['Zhiyu Chen', 'Qing Jin', 'Jingyu Wang', 'Yanzhi Wang', 'Kaiyuan Yang']",
        "date": "July 2021",
        "source": "ISLPED '21: Proceedings of the ACM/IEEE International Symposium on Low Power Electronics and Design",
        "abstract": "In-memory computing (IMC) is a promising hardware architecture to circumvent the memory walls in data-intensive applications, like deep learning. Among various memory technologies, static random-access memory (SRAM) is promising thanks to its high computing accuracy, reliability, and scalability to advanced technology nodes. This paper presents a novel multi-bit capacitive convolution in-SRAM computing macro for high accuracy, high throughput and high efficiency deep learning inference. It realizes fully parallel charge-domain multiply-and-accumulate (MAC) within compact 8-transistor 1-capacitor (8T1C) SRAM arrays that is only 41% larger than the standard 6T cells. It performs MAC with multi-bit activations without conventional digital bit-serial shift-and-add schemes, leading to drastically improved throughput for high-precision CNN models. An ADC-reduction encoding scheme complements the compact sram design, by reducing the number of needed ADCs by half for energy and area savings. A 576x130 macro with 64 ADCs is evaluated in 65nm with post-layout simulations, showing 4.60 TOPS/mm2 compute density and 59.7 TOPS/W energy efficiency with 4/4-bit activations/weights. The MC2-RAM also achieves excellent linearity with only 0.14 mV (4.5% of the LSB) standard deviation of the output voltage in Monte Carlo simulations.",
        "link": "https://dl.acm.org/doi/10.1109/ISLPED52811.2021.9502505",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Sentry-NoC: a statically-scheduled NoC for secure SoCs",
        "authors": "['Ahmed Shalaby', 'Yaswanth Tavva', 'Trevor E. Carlson', 'Li-Shiuan Peh']",
        "date": "October 2021",
        "source": "NOCS '21: Proceedings of the 15th IEEE/ACM International Symposium on Networks-on-Chip",
        "abstract": "SoC security has become essential with devices now pervasive in critical infrastructure in homes and businesses. Today's embedded SoCs are becoming increasingly high-performance and complex, comprising multiple cores, accelerators, and IP blocks interconnected with a Network-on-Chip (NoC). As these IPs can originate from diverse sources, they cannot be trusted to form the root of trust in SoCs. However, the NoC itself, being the communication backbone linking all IPs, is naturally positioned to be the basis for a secure SoC. Therefore, there is a need for an efficient solution that both meets the stringent requirements of modern embedded SoC designs, while maintaining a high level of security. In this paper, we demonstrate how statically-scheduled NoCs inherently enforce traffic isolation and non-interference of communication. The time-division multiplexing (TDM) of NoC links across applications provably ensures that security properties are fulfilled. However, conventional TDM NoCs are still vulnerable to side-channel attacks. We thus propose temporal and data obfuscation schemes that can be embedded within static TDM NoCs, randomizing source-destination communication patterns and switching activity over the links. Our proposed statically-scheduled Sentry-NoC links up untrusted IP blocks to form a secure SoC. Sentry-NoC targets key security properties to effectively mitigate side-channel attacks with an extremely low overhead, reducing average temporal correlation by 81% and average data correlation by 91%.",
        "link": "https://dl.acm.org/doi/10.1145/3479876.3481595",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Accelerating Distributed Deep Learning using Multi-Path RDMA in Data Center Networks",
        "authors": "['Feng Tian', 'Yang Zhang', 'Wei Ye', 'Cheng Jin', 'Ziyan Wu', 'Zhi-Li Zhang']",
        "date": "October 2021",
        "source": "SOSR '21: Proceedings of the ACM SIGCOMM Symposium on SDN Research (SOSR)",
        "abstract": "Data center networks (DCNs) have widely deployed RDMA to support data-intensive applications such as machine learning. While DCNs are designed with rich multi-path topology, current RDMA (hardware) technology does not support multi-path transport. In this paper we advance Maestro- a purely software-basedmulti-path RDMA solution - to effectively utilize the rich multi-path topology for load balancing and reliability. As a \"middleware\" operating at the user-space, Maestro is modulaR@and software-defined:Maestro decouples path selection and load balancing mechanisms from hardware features, and allows DCN operators and applications to make flexible decisions by employing the best mechanisms as needed. As such, Maestro can be readily deployed using existing RDMA hardware (NICs) to support distributed deep learning (DDL) applications. Our experiments show that Maestro is capable of fully utilizing multiple paths with negligible CPU overheads, thereby enhancing the performance of DDL applications.",
        "link": "https://dl.acm.org/doi/10.1145/3482898.3483363",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Vectorization for digital signal processors via equality saturation",
        "authors": "['Alexa VanHattum', 'Rachit Nigam', 'Vincent T. Lee', 'James Bornholt', 'Adrian Sampson']",
        "date": "April 2021",
        "source": "ASPLOS '21: Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "Applications targeting digital signal processors (DSPs) benefit from fast implementations of small linear algebra kernels. While existing auto-vectorizing compilers are effective at extracting performance from large kernels, they struggle to invent the complex data movements necessary to optimize small kernels. To get the best performance, DSP engineers must hand-write and tune specialized small kernels for a wide spectrum of applications and architectures. We present Diospyros, a search-based compiler that automatically finds efficient vectorizations and data layouts for small linear algebra kernels. Diospyros combines symbolic evaluation and equality saturation to vectorize computations with irregular structure. We show that a collection of Diospyros-compiled kernels outperform implementations from existing DSP libraries by 3.1× on average, that Diospyros can generate kernels that are competitive with expert-tuned code, and that optimizing these small kernels offers end-to-end speedup for a DSP application.",
        "link": "https://dl.acm.org/doi/10.1145/3445814.3446707",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Design Patterns for Submission Evaluation within E-Assessment Systems",
        "authors": "['Michael Striewe']",
        "date": "July 2021",
        "source": "EuroPLoP'21: 26th European Conference on Pattern Languages of Programs",
        "abstract": "Many software systems in the area of educational technology can produce grades or other kind of feedback for students’ submissions automatically. Depending on the context of a particular system, there are different software engineering challenges regarding performance or flexibility of the submission evaluation process. During the experimental design of educational technology, these challenges and their consequences are often not considered appropriately, which leads to sub-optimal design decisions that limit productive use. This paper establishes a pattern catalogue that captures available design choices and their consequences in order to support developers and researchers in the domain of educational technology in making their design decisions. Two small case studies demonstrate the usefulness of the catalogue and the gains from applying appropriate patterns for each context.",
        "link": "https://dl.acm.org/doi/10.1145/3489449.3490010",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Flex: high-availability datacenters with zero reserved power",
        "authors": "['Chaojie Zhang', 'Alok Gautam Kumbhare', 'Ioannis Manousakis', 'Deli Zhang', 'Pulkit A. Misra', 'Rod Assis', 'Kyle Woolcock', 'Nithish Mahalingam', 'Brijesh Warrier', 'David Gauthier', 'Lalu Kunnath', 'Steve Solomon', 'Osvaldo Morales', 'Marcus Fontoura', 'Ricardo Bianchini']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "Cloud providers, like Amazon and Microsoft, must guarantee high availability for a large fraction of their workloads. For this reason, they build datacenters with redundant infrastructures for power delivery and cooling. Typically, the redundant resources are reserved for use only during infrastructure failure or maintenance events, so that workload performance and availability do not suffer. Unfortunately, the reserved resources also produce lower power utilization and, consequently, require more datacenters to be built. To address these problems, in this paper we propose \"zero-reserved-power\" datacenters and the Flex system to ensure that workloads still receive their desired performance and availability. Flex leverages the existence of software-redundant workloads that can tolerate lower infrastructure availability, while imposing minimal (if any) performance degradation for those that require high infrastructure availability. Flex mainly comprises (1) a new offline workload placement policy that reduces stranded power while ensuring safety during failure or maintenance events, and (2) a distributed system that monitors for failures and quickly reduces the power draw while respecting the workloads' requirements, when it detects a failure. Our evaluation shows that Flex produces less than 5% stranded power and increases the number of deployed servers by up to 33%, which translates to hundreds of millions of dollars in construction cost savings per datacenter site. We end the paper with lessons from our experience bringing Flex to production in Microsoft's datacenters.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00033",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "The case for adding privacy-related offloading to smart storage",
        "authors": "['Claudiu Mihali', 'Anca Hangan', 'Gheorghe Sebestyen', 'Zsolt István']",
        "date": "June 2021",
        "source": "SYSTOR '21: Proceedings of the 14th ACM International Conference on Systems and Storage",
        "abstract": "It is important to ensure that personally identifiable information (PII) is protected within large distributed systems and is used only for intended purposes. Achieving this is challenging and several techniques have been proposed for privacy-preserving analytics, but they typically focus on the end hosts only. We argue that future storage solutions should include, in addition to emerging compute offload, also privacy-related operators. Since many privacy operators, such as perturbation and anonymization, take place as the very first step before other computations, query offload to a Smart Storage device might be only feasible in the future if privacy-related operators can also be offloaded. In this work we demonstrate that privacy-preserving operators can be implemented in hardware without reducing read bandwidths. We focus on perturbations and extend an FPGA-based network-attached Smart Storage solution to show that it is possible to provide these operations at 10Gbps line-rate while using only a small amount of additional FPGA real-estate. We also discuss how future faster smart storage nodes should look like in the light of these additional requirements.",
        "link": "https://dl.acm.org/doi/10.1145/3456727.3463769",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Research on Real-time Data Sharing Method of Power Information Acquisition System",
        "authors": "['Wei Huang', 'CaiXia Long', 'Xi Wang']",
        "date": "October 2021",
        "source": "ICITEE '21: Proceedings of the 4th International Conference on Information Technologies and Electrical Engineering",
        "abstract": "The traditional data sharing architecture of the Power Information Acquisition System is based on Oracle database, with OGG synchronization and ETL extraction as the main methods. With the increasing amount of data collected, architecture limitations and other issues lead to the large amount of real-time power data in the system can not be quickly shared, which limits the development of real-time analysis business such as distribution network monitoring, power supply command. This paper puts forward a mass real-time data sharing architecture centered on distributed message queuing, which is based on Redis, Kafka and other components for integrated design and development. It implements two real-time data sharing modes: one transmitter and multiple receivers sharing, real-time data collecting service sharing, and effectively solves the problem that real-time data cannot be shared efficiently in Power Information Acquisition System. The research results of this paper have been deployed and applied in the Hunan Electric Power Company of China National Grid, and have achieved remarkable results in the application of big power data such as environmental protection monitoring.",
        "link": "https://dl.acm.org/doi/10.1145/3513142.3513213",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Every walk’s a hit: making page walks single-access cache hits",
        "authors": "['Chang Hyun Park', 'Ilias Vougioukas', 'Andreas Sandberg', 'David Black-Schaffer']",
        "date": "February 2022",
        "source": "ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "As memory capacity has outstripped TLB coverage, large data applications suffer from frequent page table walks. We investigate two complementary techniques for addressing this cost: reducing the number of accesses required and reducing the latency of each access. The first approach is accomplished by opportunistically \"flattening\" the page table: merging two levels of traditional 4 KB page table nodes into a single 2 MB node, thereby reducing the table's depth and the number of indirections required to traverse it. The second is accomplished by biasing the cache replacement algorithm to keep page table entries during periods of high TLB miss rates, as these periods also see high data miss rates and are therefore more likely to benefit from having the smaller page table in the cache than to suffer from increased data cache misses.   We evaluate these approaches for both native and virtualized systems and across a range of realistic memory fragmentation scenarios, describe the limited changes needed in our kernel implementation and hardware design, identify and address challenges related to self-referencing page tables and kernel memory allocation, and compare results across server and mobile systems using both academic and industrial simulators for robustness.   We find that flattening does reduce the number of accesses required on a page walk (to 1.0), but its performance impact (+2.3%) is small due to Page Walker Caches (already 1.5 accesses). Prioritizing caching has a larger effect (+6.8%), and the combination improves performance by +9.2%. Flattening is more effective on virtualized systems (4.4 to 2.8 accesses, +7.1% performance), due to 2D page walks. By combining the two techniques we demonstrate a state-of-the-art +14.0% performance gain and -8.7% dynamic cache energy and -4.7% dynamic DRAM energy for virtualized execution with very simple hardware and software changes.",
        "link": "https://dl.acm.org/doi/10.1145/3503222.3507718",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Concentrated isolation for container networks toward application-aware sandbox tailoring",
        "authors": "['Yuki Nakata', 'Katsuya Matsubara', 'Ryosuke Matsumoto']",
        "date": "December 2021",
        "source": "UCC '21: Proceedings of the 14th IEEE/ACM International Conference on Utility and Cloud Computing",
        "abstract": "Containers provide a lightweight and fine-grained isolation for computational resources such as CPUs, memory, storage, and networks, but their weak isolation raises security concerns. As a result, research and development efforts have focused on redesigning truly sandboxed containers with system call intercept and hardware virtualization techniques such as gVisor and Kata Containers. However, such fully integrated sandboxing could overwhelm the lightweight and scalable nature of the containers. In this work, we propose a partially fortified sandboxing mechanism that concentratedly fortifies the network isolation, focusing on the fact that containerized clouds and the applications running on them require different isolation levels in accordance with their unique characteristics. We describe how to efficiently implement the mechanism to fortify network isolation for containers with a para-passthrough hypervisor and report evaluation results with benchmarks and real applications. Our findings demonstrate that this fortified network isolation has good potential to tailor sandboxes for containerized PaaS/FaaS clouds.",
        "link": "https://dl.acm.org/doi/10.1145/3468737.3494092",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Next-generation internet at terabit speed: SCION in P4",
        "authors": "['Joeri de Ruiter', 'Caspar Schutijser']",
        "date": "December 2021",
        "source": "CoNEXT '21: Proceedings of the 17th International Conference on emerging Networking EXperiments and Technologies",
        "abstract": "Regularly, new architectures are proposed to address shortcomings in the current internet. It is not always trivial to evaluate how these proposals would perform in practice. This situation is improved significantly with the introduction of the P4 programming language and programmable network equipment. In this paper we discuss our implementation of one particular future internet architecture, namely SCION. We implemented a SCION router in P4 for switches based on the Intel Tofino ASIC. Having an open source P4 implementation of SCION that runs on high-speed hardware can contribute to its adoption as well as support research in this area. Our work lead to several recommendations for and subsequent changes to the SCION protocol, as well as some generic guidelines when designing protocols. A first analysis of our implementation shows it can process SCION packets at high speeds.",
        "link": "https://dl.acm.org/doi/10.1145/3485983.3494839",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Development of a Neuromorphic Circuit in Reconfigurable Technology for the Simulation of Synapses",
        "authors": "['Christos Kompogiannis', 'Maria Sapounaki', 'Athanasios Kakarountas']",
        "date": "November 2021",
        "source": "PCI '21: Proceedings of the 25th Pan-Hellenic Conference on Informatics",
        "abstract": "In last decades, neuromorphic circuits have received widespread attention across various scientific fields. Such circuits mathematically model the behaviour of biological neurons, synapses as well as their interaction. This work implements a neuromorphic synapse on an FPGA board and it improves previously proposed synapses in terms of performance and synchronization to novel neuron implementations. The proposed architecture is designed with the goal of being compatible with neuromorphic neurons based on the mathematical equations of the Izhikevich neuron model. The implementation consists of two computation cores; one core is responsible for computing the update of currents and the second core is computing the exponential decays of currents. Compared to similar neuromorphic synapses, the proposed retains low complexity and can calculate the needed synaptic currents of the connected neurons quickly and reliably. The speed of computation achieved by the parallel execution of instructions indicates that the system can function in real time.",
        "link": "https://dl.acm.org/doi/10.1145/3503823.3503918",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "NURA: A Framework for Supporting Non-Uniform Resource Accesses in GPUs",
        "authors": "['Sina Darabi', 'Negin Mahani', 'Hazhir Baxishi', 'Ehsan Yousefzadeh-Asl-Miandoab', 'Mohammad Sadrosadati', 'Hamid Sarbazi-Azad']",
        "date": "None",
        "source": "Proceedings of the ACM on Measurement and Analysis of Computing Systems",
        "abstract": "Multi-application execution in Graphics Processing Units (GPUs), a promising way to utilize GPU resources, is still challenging. Some pieces of prior work (e.g., spatial multitasking) have limited opportunity to improve resource utilization, while other works, e.g., simultaneous multi-kernel, provide fine-grained resource sharing at the price of unfair execution. This paper proposes a new multi-application paradigm for GPUs, called NURA, that provides high potential to improve resource utilization and ensures fairness and Quality-of-Service (QoS). The key idea is that each streaming multiprocessor (SM) executes Cooperative Thread Arrays (CTAs) belong to only one application (similar to the spatial multi-tasking) and shares its unused resources with the SMs running other applications demanding more resources. NURA handles resource sharing process mainly using a software approach to provide simplicity, low hardware cost, and flexibility. We also perform some hardware modifications as an architectural support for our software-based proposal. We conservatively analyze the hardware cost of our proposal, and observe less than 1.07% area overhead with respect to the whole GPU die. Our experimental results over various mixes of GPU workloads show that NURA improves GPU system throughput by 26% compared to state-of-the-art spatial multi-tasking, on average, while meeting the QoS target. In terms of fairness, NURA has almost similar results to spatial multitasking, while it outperforms simultaneous multi-kernel by an average of 76%.",
        "link": "https://dl.acm.org/doi/10.1145/3508036",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "SDN Enabled QoE and Security Framework for Multimedia Applications in 5G Networks",
        "authors": "['Prabhakar Krishnan', 'Kurunandan Jain', 'Pramod George Jose', 'Krishnashree Achuthan', 'Rajkumar Buyya']",
        "date": "None",
        "source": "ACM Transactions on Multimedia Computing, Communications, and Applications",
        "abstract": "The technologies for real-time multimedia transmission and immersive 3D gaming applications are rapidly emerging, posing challenges in terms of performance, security, authentication, data privacy, and encoding. The communication channel for these multimedia applications must be secure and reliable from network attack vectors and data-contents must employ strong encryption to preserve privacy and confidentiality. Towards delivering secure multimedia application environment for 5G networks, we propose an SDN/NFV (Software-Defined-Networking/Network-Function-Virtualization) framework called STREK, which attempts to deliver highly adaptable Quality-of-Experience (QoE), Security, and Authentication functions for multi-domain Cloud to Edge networks. The STREK architecture consists of a holistic SDNFV dataplane, NFV service-chaining and network slicing, a lightweight adaptable hybrid cipher scheme called TREK, and an open RESTful API for applications to deploy custom policies at runtime for multimedia services. For multi-domain/small-cell deployments, the key-generation scheme is dynamic at flow/session-level, and the handover authentication scheme uses a novel method to exchange security credentials with the Access Points (APs) of neighborhood cells. This scheme is designed to improve authentication function during handover with low overhead, delivering the 5G ultra-low latency requirements. We present the experiments with both software and hardware-based implementations and compare our solution with popular lightweight cryptographic solutions, standard open source software, and SDN-based research proposals for 5G multimedia. In the microbenchmarks, STREK achieves smaller hardware, low overhead, low computation, higher attack resistance, and offers better network performance for multimedia streaming applications. In real-time multimedia use-cases, STREK shows greater level of quality distortion for multimedia contents with minimal encryption bitrate overhead to deliver data confidentiality, immunity to common cryptanalysis, and significant resistance to communication channel attacks, in the context of low-latency 5G networks.",
        "link": "https://dl.acm.org/doi/10.1145/3377390",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "HAMMER: boosting fidelity of noisy Quantum circuits by exploiting Hamming behavior of erroneous outcomes",
        "authors": "['Swamit Tannu', 'Poulami Das', 'Ramin Ayanzadeh', 'Moinuddin Qureshi']",
        "date": "February 2022",
        "source": "ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "Quantum computers with hundreds of qubits will be available soon. Unfortunately, high device error-rates pose a significant challenge in using these near-term quantum systems to power real-world applications. Executing a program on existing quantum systems generates both correct and incorrect outcomes, but often, the output distribution is too noisy to distinguish between them. In this paper, we show that erroneous outcomes are not arbitrary but exhibit a well-defined structure when represented in the Hamming space. Our experiments on IBM and Google quantum computers show that the most frequent erroneous outcomes are more likely to be close in the Hamming space to the correct outcome. We exploit this behavior to improve the ability to infer the correct outcome.  We propose Hamming Reconstruction (HAMMER), a post-processing technique that leverages the observation of Hamming behavior to reconstruct the noisy output distribution, such that the resulting distribution has higher fidelity. We evaluate HAMMER using experimental data from Google and IBM quantum computers with more than 500 unique quantum circuits and obtain an average improvement of 1.37x in the quality of solution. On Google’s publicly available QAOA datasets, we show that HAMMER sharpens the gradients on the cost function landscape.",
        "link": "https://dl.acm.org/doi/10.1145/3503222.3507703",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Using Meta Reinforcement Learning to Bridge the Gap between Simulation and Experiment in Energy Demand Response",
        "authors": "['Doseok Jang', 'Lucas Spangher', 'Manan Khattar', 'Utkarsha Agwan', 'Costas Spanos']",
        "date": "June 2021",
        "source": "e-Energy '21: Proceedings of the Twelfth ACM International Conference on Future Energy Systems",
        "abstract": "Our team is proposing to run a full-scale energy demand response experiment in an office building. Although this is an exciting endeavor which will provide value to the community, collecting training data for the reinforcement learning agent is costly and will be limited. In this work, we apply a meta-learning architecture to warm start the experiment with simulated tasks, to increase sample efficiency. We present results that demonstrate a similar a step up in complexity still corresponds with better learning.",
        "link": "https://dl.acm.org/doi/10.1145/3447555.3466589",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Impact of AVX-512 Instructions on Graph Partitioning Problems.",
        "authors": "['Md Maruf Hossain', 'Erik Saule']",
        "date": "August 2021",
        "source": "ICPP Workshops '21: 50th International Conference on Parallel Processing Workshop",
        "abstract": "Graph analysis now percolates society with applications ranging from advertising and transportation to medical research. The structure of graphs is becoming more complex every day while they are getting larger. The increasing size of graph networks has made many of the classical algorithms reasonably slow. Fortunately, CPU architectures have evolved to adjust to new and more complex problems in terms of core-level parallelism and vector-level parallelism (SIMD-level).  In this paper, we are exploring how the modern vector architecture of CPUs can help with community detection, partitioning, and coloring kernels by studying two representatives algorithms. We consider the Intel SkylakeX and Cascade Lake architectures, which support gather and scatter instructions on 512-bit vectors.  The existing vectorized graph algorithms of classic graph problems, such as BFS and PageRank, do not apply well to community detection; we show the support of gather and scatter are necessary. In particular for the implementation of the reduce-scatter patterns. We evaluate the performances achieved on the two architectures and conclude that good hardware support for scatter instructions is necessary to fully leverage the vector processing for graph partitioning problems.",
        "link": "https://dl.acm.org/doi/10.1145/3458744.3473362",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Cross-layer approximation for printed machine learning circuits",
        "authors": "['Giorgos Armeniakos', 'Georgios Zervakis', 'Dimitrios Soudris', 'Mehdi B. Tahoori', 'Jörg Henkel']",
        "date": "March 2022",
        "source": "DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe",
        "abstract": "Printed electronics (PE) feature low non-recurring engineering costs and low per unit-area fabrication costs, enabling thus extremely low-cost and on-demand hardware. Such low-cost fabrication allows for high customization that would be infeasible in silicon, and bespoke architectures prevail to improve the efficiency of emerging PE machine learning (ML) applications. However, even with bespoke architectures, the large feature sizes in PE constraint the complexity of the ML models that can be implemented. In this work, we bring together, for the first time, approximate computing and PE design targeting to enable complex ML models, such as Multi-Layer Perceptrons (MLPs) and Support Vector Machines (SVMs), in PE. To this end, we propose and implement a cross-layer approximation, tailored for bespoke ML architectures. At the algorithmic level we apply a hardware-driven coefficient approximation of the ML model and at the circuit level we apply a netlist pruning through a full search exploration. In our extensive experimental evaluation we consider 14 MLPs and SVMs and evaluate more than 4300 approximate and exact designs. Our results demonstrate that our cross approximation delivers Pareto optimal designs that, compared to the state-of-the-art exact designs, feature 47% and 44% average area and power reduction, respectively, and less than 1% accuracy loss.",
        "link": "https://dl.acm.org/doi/10.5555/3539845.3539898",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Securing Interruptible Enclaved Execution on Small Microprocessors",
        "authors": "['Matteo Busi', 'Job Noorman', 'Jo Van Bulck', 'Letterio Galletta', 'Pierpaolo Degano', 'Jan Tobias Mühlberg', 'Frank Piessens']",
        "date": "None",
        "source": "ACM Transactions on Programming Languages and Systems",
        "abstract": "Computer systems often provide hardware support for isolation mechanisms such as privilege levels, virtual memory, or enclaved execution. Over the past years, several successful software-based side-channel attacks have been developed that break, or at least significantly weaken, the isolation that these mechanisms offer. Extending a processor with new architectural or micro-architectural features brings a risk of introducing new software-based side-channel attacks.This article studies the problem of extending a processor with new features without weakening the security of the isolation mechanisms that the processor offers. Our solution is heavily based on techniques from research on programming languages. More specifically, we propose to use the programming language concept of full abstraction as a general formal criterion for the security of a processor extension. We instantiate the proposed criterion to the concrete case of extending a microprocessor that supports enclaved execution with secure interruptibility. This is a very relevant instantiation, as several recent papers have shown that interruptibility of enclaves leads to a variety of software-based side-channel attacks. We propose a design for interruptible enclaves and prove that it satisfies our security criterion. We also implement the design on an open-source enclave-enabled microprocessor and evaluate the cost of our design in terms of performance and hardware size.",
        "link": "https://dl.acm.org/doi/10.1145/3470534",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "CARAT CAKE: replacing paging via compiler/kernel cooperation",
        "authors": "['Brian Suchy', 'Souradip Ghosh', 'Drew Kersnar', 'Siyuan Chai', 'Zhen Huang', 'Aaron Nelson', 'Michael Cuevas', 'Alex Bernat', 'Gaurav Chaudhary', 'Nikos Hardavellas', 'Simone Campanoni', 'Peter Dinda']",
        "date": "February 2022",
        "source": "ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "Virtual memory, specifically paging, is undergoing significant innovation due to being challenged by new demands from modern workloads. Recent work has demonstrated an alternative software only design that can result in simplified hardware requirements, even supporting purely physical addressing. While we have made the case for this Compiler- And Runtime-based Address Translation (CARAT) concept, its evaluation was based on a user-level prototype. We now report on incorporating CARAT into a kernel, forming Compiler- And Runtime-based Address Translation for CollAborative Kernel Environments (CARAT CAKE). In our implementation, a Linux-compatible x64 process abstraction can be based either on CARAT CAKE, or on a sophisticated paging implementation. Implementing CARAT CAKE involves kernel changes and compiler optimizations/transformations that must work on all code in the system, including kernel code. We evaluate CARAT CAKE in comparison with paging and find that CARAT CAKE is able to achieve the functionality of paging (protection, mapping, and movement properties) with minimal overhead. In turn, CARAT CAKE allows significant new benefits for systems including energy savings, larger L1 caches, and arbitrary granularity memory management.",
        "link": "https://dl.acm.org/doi/10.1145/3503222.3507771",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Eavesdropping user credentials via GPU side channels on smartphones",
        "authors": "['Boyuan Yang', 'Ruirong Chen', 'Kai Huang', 'Jun Yang', 'Wei Gao']",
        "date": "February 2022",
        "source": "ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "Graphics Processing Unit (GPU) on smartphones is an effective target for hardware attacks. In this paper, we present a new side channel attack on mobile GPUs of Android smartphones, allowing an unprivileged attacker to eavesdrop the user's credentials, such as login usernames and passwords, from their inputs through on-screen keyboard. Our attack targets on Qualcomm Adreno GPUs and investigate the amount of GPU overdraw when rendering the popups of user's key presses of inputs. Such GPU overdraw caused by each key press corresponds to unique variations of selected GPU performance counters, from which these key presses can be accurately inferred. Experiment results from practical use on multiple models of Android smartphones show that our attack can correctly infer more than 80% of user's credential inputs, but incur negligible amounts of computing overhead and network traffic on the victim device. To counter this attack, this paper suggests mitigations of access control on GPU performance counters, or applying obfuscations on the values of GPU performance counters.",
        "link": "https://dl.acm.org/doi/10.1145/3503222.3507757",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Enabling Industry 4.0 Communication Protocol Interoperability: An OPC UA Case Study",
        "authors": "['Subash Kannoth', 'Frank Schnicke', 'Pablo Oliveira Antonino']",
        "date": "May 2021",
        "source": "ECBS 2021: 7th Conference on the Engineering of Computer Based Systems",
        "abstract": "Rapid advances in digitalization are leading the automation and manufacturing sector towards the fourth industrial revolution also known as Industry 4.0, whose main goal is to realize the changeable production processes, which is currently expensive and effort-intensive. The Open Platform Communications Unified Architecture (OPC UA) is an established and well-known communication protocol in the industrial domain. The Reference Architecture Model Industry4.0 (RAMI 4.0) proposes OPC UA as the core communication protocol among assets such as machines, robots, and appliances. Despite the key role of OPC UA in Industry 4.0, there is still a lack of technical guidance on how to integrate OPC UA with other communication protocols, especially with legacy devices that communicate through proprietary protocols. To address this challenge, we propose a solution that is characterized by a set of communication primitives, a platform-independent type system and an intermediate language. We also evaluate the overhead created through integration in terms of the round-trip time and message size imposed by metadata for abstraction. We have implemented the proposed approach in reference Industry 4.0 projects, and in this paper, we report our experiences in integrating OPC UA in a homogeneous communication system comprised of Create, Read, Update, Delete and Invoke primitives which improves the protocol interoperability and reduces integration effort.",
        "link": "https://dl.acm.org/doi/10.1145/3459960.3459977",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "HyCSim: A rapid design space exploration tool for emerging hybrid last-level caches",
        "authors": "['Carlos Escuin', 'Asif Ali Khan', 'Pablo Ibañez', 'Teresa Monreal', 'Victor Viñals', 'Jeronimo Castrillon']",
        "date": "January 2022",
        "source": "DroneSE and RAPIDO: System Engineering for constrained embedded systems",
        "abstract": "Recent years have seen a rising trend in the exploration of nonvolatile memory (NVM) technologies in the memory subsystem. Particularly in the cache hierarchy, hybrid last-level cache (LLC) solutions are proposed to meet the wide-ranging performance and energy requirements of modern days applications. These emerging hybrid solutions need simulation and detailed exploration to fully understand their capabilities before exploiting them. Existing simulation tools are either too slow or incapable of prototyping such systems and optimizing for NVM devices. To this end, we propose HyCSim1, a trace-driven simulation infrastructure that enables rapid comparison of various hybrid LLC configurations for different optimization objectives. Notably, HyCSim makes it possible to quickly estimate the impact of various hybrid LLC insertion and replacement policies, disabling of a cache region at byte or cache frame granularity for different fault maps. In addition, HyCSim allows to evaluate the impact of various compression schemes on the overall performance (hit and miss rate) and the number of writes to the LLC. Our evaluation on ten multi-program workloads from the SPEC 2006 benchmarks suite shows that HyCSim accelerates the simulation time by 24 ×, compared to the cycle-accurate Gem5 simulator, with high-fidelity.",
        "link": "https://dl.acm.org/doi/10.1145/3522784.3522801",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Embedding an interactive art installation into a building for enhancing citizen's awareness on urban environmental conditions",
        "authors": "['Penny Papageorgopoulou', 'Dimitris Delinikolas', 'Natalia Arsenopoulou', 'Louiza Katsarou', 'Charalampos Rizopoulos', 'Antonios Psaltis', 'Iouliani Theona', 'Alexandros Drymonitis', 'Antonios Korosidis', 'Dimitrios Charitos']",
        "date": "June 2021",
        "source": "MAB20: Media Architecture Biennale 20",
        "abstract": "The paper presents an artwork, embedded in a public-use building complex, comprising four interactive, sight specific installations. The artwork employs ubiquitous computing technologies and a variety of other components (projector, LED matrix displays, physical and custom-made objects) to sense, collect and translate urban, environmental data and human input into evolving multisensory representations, affording hybrid spatial experiences. The artwork aims at highlighting the impact of human activity upon the environment and the nonhuman entities that inhabit it, shifting the focus to a post-anthropocentric view of the world.",
        "link": "https://dl.acm.org/doi/10.1145/3469410.3469426",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "GPM: leveraging persistent memory from a GPU",
        "authors": "['Shweta Pandey', 'Aditya K Kamath', 'Arkaprava Basu']",
        "date": "February 2022",
        "source": "ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "The GPU is a key computing platform for many application domains. While the new non-volatile memory technology has brought the promise of byte-addressable persistence (a.k.a., persistent memory, or PM) to CPU applications, the same, unfortunately, is beyond the reach of GPU programs.   We take three key steps toward enabling GPU programs to access PM directly. First, enable direct access to PM from within a GPU kernel without needing to modify the hardware. Next, we demonstrate three classes of GPU-accelerated applications that benefit from PM. In the process, we create a workload suite with nine such applications. We then create a GPU library, written in CUDA, to support logging, checkpointing, and primitives for native persistence for programmers to easily leverage PM.",
        "link": "https://dl.acm.org/doi/10.1145/3503222.3507758",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "IOCost: block IO control for containers in datacenters",
        "authors": "['Tejun Heo', 'Dan Schatzberg', 'Andrew Newell', 'Song Liu', 'Saravanan Dhakshinamurthy', 'Iyswarya Narayanan', 'Josef Bacik', 'Chris Mason', 'Chunqiang Tang', 'Dimitrios Skarlatos']",
        "date": "February 2022",
        "source": "ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "Resource isolation is a fundamental requirement in datacenter environments. However, our production experience in Meta’s large-scale datacenters shows that existing IO control mechanisms for block storage are inadequate in containerized environments. IO control needs to provide proportional resources to containers while taking into account the hardware heterogeneity of storage devices and the idiosyncrasies of the workloads deployed in datacenters. The speed of modern SSDs requires IO control to execute with low-overheads. Furthermore, IO control should strive for work conservation, take into account the interactions with the memory management subsystem, and avoid priority inversions that lead to isolation failures. To address these challenges, this paper presents IOCost, an IO control solution that is designed for containerized environments and provides scalable, work-conserving, and low-overhead IO control for heterogeneous storage devices and diverse workloads in datacenters. IOCost performs offline profiling to build a device model and uses it to estimate device occupancy of each IO request. To minimize runtime overhead, it separates IO control into a fast per-IO issue path and a slower periodic planning path. A novel work-conserving budget donation algorithm enables containers to dynamically share unused budget. We have deployed IOCost across the entirety of Meta’s datacenters comprised of millions of ma- chines, upstreamed IOCost to the Linux kernel, and open-sourced our device-profiling tools. IOCost has been running in production for two years, providing IO control for Meta’s fleet. We describe the design of IOCost and share our experience deploying it at scale.",
        "link": "https://dl.acm.org/doi/10.1145/3503222.3507727",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "An Energy-Efficient Inference Method in Convolutional Neural Networks Based on Dynamic Adjustment of the Pruning Level",
        "authors": "['Mohammad-Ali Maleki', 'Alireza Nabipour-Meybodi', 'Mehdi Kamal', 'Ali Afzali-Kusha', 'Massoud Pedram']",
        "date": "None",
        "source": "ACM Transactions on Design Automation of Electronic Systems",
        "abstract": "In this article, we present a low-energy inference method for convolutional neural networks in image classification applications. The lower energy consumption is achieved by using a highly pruned (lower-energy) network if the resulting network can provide a correct output. More specifically, the proposed inference method makes use of two pruned neural networks (NNs), namely mildly and aggressively pruned networks, which are both designed offline. In the system, a third NN makes use of the input data for the online selection of the appropriate pruned network. The third network, for its feature extraction, employs the same convolutional layers as those of the aggressively pruned NN, thereby reducing the overhead of the online management. There is some accuracy loss induced by the proposed method where, for a given level of accuracy, the energy gain of the proposed method is considerably larger than the case of employing any one pruning level. The proposed method is independent of both the pruning method and the network architecture. The efficacy of the proposed inference method is assessed on Eyeriss hardware accelerator platform for some of the state-of-the-art NN architectures. Our studies show that this method may provide, on average, 70% energy reduction compared to the original NN at the cost of about 3% accuracy loss on the CIFAR-10 dataset.",
        "link": "https://dl.acm.org/doi/10.1145/3460972",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Research of Quantum Neural Networks and Development of a Single-qubit Model of Neuron",
        "authors": "['Sergey Gushanskiy', 'Viktor Potapov', 'Alexey Samoylov']",
        "date": "November 2021",
        "source": "PCI '21: Proceedings of the 25th Pan-Hellenic Conference on Informatics",
        "abstract": "Recently, there has been a rapid increase interest in quantum computers. Their work is based on the use of quantum-mechanical phenomena such as superposition and entanglement for computing to transform input data into outputs, which can actually provide effective performance 3-4 orders of magnitude higher than any modern computing devices. In the past few decades, there has been an acute problem of creating a quantum computer that uses quantum mechanical effects for computations. Currently, there are prototypes of devices of this class, but it is not yet possible to achieve an effective solution to the planned tasks with their help due to a number of difficulties in implementation. However, there is a mathematical apparatus that describes the behavior of quantum particles using a wave function. Hence, it becomes possible to simulate quantum computations on the classical architecture and check the reliability of algorithms belonging to the NP class, which are theoretically executed on a quantum computer in polynomial time and which cannot be solved on classical computers within an acceptable time frame. Effectively simulate the operation of a quantum system on devices with classical architecture is impossible, it remains to propose methods for acceleration of quantum computing on classical systems.",
        "link": "https://dl.acm.org/doi/10.1145/3503823.3503893",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Computing with time: microarchitectural weird machines",
        "authors": "['Dmitry Evtyushkin', 'Thomas Benjamin', 'Jesse Elwell', 'Jeffrey A. Eitel', 'Angelo Sapello', 'Abhrajit Ghosh']",
        "date": "April 2021",
        "source": "ASPLOS '21: Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "Side-channel attacks such as Spectre rely on properties of modern CPUs that permit discovery of microarchitectural state via timing of various operations. The Weird Machine concept is an increasingly popular model for characterization of emergent execution that arises from side-effects of conventional computing constructs. In this work we introduce Microarchitectural Weird Machines (µWM): code constructions that allow performing computation through the means of side effects and conflicts between microarchitectual entities such as branch predictors and caches. The results of such computations are observed as timing variations. We demonstrate how µWMs can be used as a powerful obfuscation engine where computation operates based on events unobservable to conventional anti-obfuscation tools based on emulation, debugging, static and dynamic analysis techniques. We demonstrate that µWMs can be used to reliably perform arbitrary computation by implementing a SHA-1 hash function. We then present a practical example in which we use a µWM to obfuscate malware code such that its passive operation is invisible to an observer with full power to view the architectural state of the system until the code receives a trigger. When the trigger is received the malware decrypts and executes its payload. To show the effectiveness of obfuscation we demonstrate its use in the concealment and subsequent execution of a payload that exfiltrates a shadow password file, and a payload that creates a reverse shell.",
        "link": "https://dl.acm.org/doi/10.1145/3445814.3446729",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Towards failure correlation for improved cloud application service resilience",
        "authors": "['Dhanya R Mathews', 'Mudit Verma', 'Pooja Aggarwal', 'J. Lakshmi']",
        "date": "December 2021",
        "source": "UCC '21: Proceedings of the 14th IEEE/ACM International Conference on Utility and Cloud Computing Companion",
        "abstract": "Autonomously dealing with disruptions is necessary for maintaining the quality of a cloud application service. A fault, error, or failure in any component across the application service stack can potentially disrupt the service delivery. Fault localization and failure prediction are essential techniques in managing service failures. Emerging cloud computing paradigms are pushing application services to be built as loosely coupled distributed components for independent scaling. However, such architectures render existing approaches for fault localization and failure prediction to be limiting. Prevalent works on fault localization and failure prediction focus on a specific cloud service architecture layer or a subset of service components or specific fault types. These approaches restrict the view on the impact of the fault on the application service and obviate more intelligent methods for localizing faults or predicting failures, and thus efficiently dealing with service disruptions in an autonomous way. This paper contemplates the propagation of faults in multi-tiered architectures like clouds and uses a real-world disruption scenario to emphasize the need for correlating the faults across the service layers to acquire insights for end-to-end fault analysis for cloud application services.",
        "link": "https://dl.acm.org/doi/10.1145/3492323.3495586",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A Survey on Silicon Photonics for Deep Learning",
        "authors": "['Febin P. Sunny', 'Ebadollah Taheri', 'Mahdi Nikdast', 'Sudeep Pasricha']",
        "date": "None",
        "source": "ACM Journal on Emerging Technologies in Computing Systems",
        "abstract": "Deep learning has led to unprecedented successes in solving some very difficult problems in domains such as computer vision, natural language processing, and general pattern recognition. These achievements are the culmination of decades-long research into better training techniques and deeper neural network models, as well as improvements in hardware platforms that are used to train and execute the deep neural network models. Many application-specific integrated circuit (ASIC) hardware accelerators for deep learning have garnered interest in recent years due to their improved performance and energy-efficiency over conventional CPU and GPU architectures. However, these accelerators are constrained by fundamental bottlenecks due to (1) the slowdown in CMOS scaling, which has limited computational and performance-per-watt capabilities of emerging electronic processors; and (2) the use of metallic interconnects for data movement, which do not scale well and are a major cause of bandwidth, latency, and energy inefficiencies in almost every contemporary processor. Silicon photonics has emerged as a promising CMOS-compatible alternative to realize a new generation of deep learning accelerators that can use light for both communication and computation. This article surveys the landscape of silicon photonics to accelerate deep learning, with a coverage of developments across design abstractions in a bottom-up manner, to convey both the capabilities and limitations of the silicon photonics paradigm in the context of deep learning acceleration.",
        "link": "https://dl.acm.org/doi/10.1145/3459009",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Suppressing ZZ crosstalk of Quantum computers through pulse and scheduling co-optimization",
        "authors": "['Lei Xie', 'Jidong Zhai', 'ZhenXing Zhang', 'Jonathan Allcock', 'Shengyu Zhang', 'Yi-Cong Zheng']",
        "date": "February 2022",
        "source": "ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "Noise is a significant obstacle to quantum computing, and ZZ crosstalk is one of the most destructive types of noise affecting superconducting qubits. Previous approaches to suppressing ZZ crosstalk have mainly relied on specific chip design that can complicate chip fabrication and aggravate decoherence. To some extent, special chip design can be avoided by relying on pulse optimization to suppress ZZ crosstalk. However, existing approaches are non-scalable, as their required time and memory grow exponentially with the number of qubits involved.  To address the above problems, we propose a scalable approach by co-optimizing pulses and scheduling. We optimize pulses to offer an ability to suppress ZZ crosstalk surrounding a gate, and then design scheduling strategies to exploit this ability and achieve suppression across the whole circuit. A main advantage of such co-optimization is that it does not require special hardware support. Besides, we implement our approach as a general framework that is compatible with different pulse optimization methods. We have conducted extensive evaluations by simulation and on a real quantum computer. Simulation results show that our proposal can improve the fidelity of quantum computing on 4∼12 qubits by up to 81× (11× on average). Ramsey experiments on a real quantum computer also demonstrate that our method can eliminate the effect of ZZ crosstalk to a great extent.",
        "link": "https://dl.acm.org/doi/10.1145/3503222.3507761",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "PUF based Secure and Lightweight Authentication and Key-Sharing Scheme for Wireless Sensor Network",
        "authors": "['Mahabub Hasan Mahalat', 'Dipankar Karmakar', 'Anindan Mondal', 'Bibhash Sen']",
        "date": "None",
        "source": "ACM Journal on Emerging Technologies in Computing Systems",
        "abstract": "The deployment of wireless sensor networks (WSN) in an untended environment and the openness of the wireless channel bring various security threats to WSN. The resource limitations of the sensor nodes make the conventional security systems less attractive for WSN. Moreover, conventional cryptography alone cannot ensure the desired security against the physical attacks on sensor nodes. Physically unclonable function (PUF) is an emerging hardware security primitive that provides low-cost hardware security exploiting the unique inherent randomness of a device. In this article, we have proposed an authentication and key sharing scheme for the WSN integrating Pedersen’s verifiable secret sharing (Pedersen’s VSS) and Shamir’s secret sharing (Shamir’s SS) scheme with PUF which ensure the desired security with low overhead. The security analysis depicts the resilience of the proposed scheme against different active, passive and physical attacks. Also, the performance analysis shows that the proposed scheme possesses low computation, communication and storage overhead. The scheme only needs to store a polynomial number of PUF challenge-response pairs to the user node. The sink or senor nodes do not require storing any secret key. Finally, the comparison with the previous protocols establishes the dominance of the proposed scheme to use in WSN.",
        "link": "https://dl.acm.org/doi/10.1145/3466682",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Reducing the configuration overhead of the distributed two-level control system",
        "authors": "['Yu Yang', 'Dimitrios Stathis', 'Ahmed Hemani']",
        "date": "March 2022",
        "source": "DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe",
        "abstract": "With the growing demand for more efficient hardware accelerators for streaming applications, a novel Coarse-Grained Reconfigurable Architecture (CGRA) that uses a Distributed Two-Level Control (D2LC) system has been proposed in the literature. Even though the highly distributed and parallel structure makes it fast and energy-efficient, the single-issue instruction channel between the level-1 and level-2 controller in each D2LC cell becomes the bottleneck of its performance. In this paper, we improve its design to mimic a multi-issued architecture by inserting shadow instruction buffers between the level-1 and level-2 controllers. Together with a zero-overhead hardware loop, the improved D2LC architecture can enable efficient overlap between loop iterations. We also propose a complete constraint programming based instruction scheduling algorithm to support the above hardware features. The experiment result shows that the improved D2LC architecture can achieve up to 25% of reduction on the instruction execution cycles and 35% reduction on the energy-delay product.",
        "link": "https://dl.acm.org/doi/10.5555/3539845.3539877",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Eliminating Iterations of Iterative Methods: Solving Large-Scale Sparse Linear System in O(1) with RRAM-based In-Memory Accelerator",
        "authors": "['Tao Song', 'Xiaoming Chen', 'Yinhe Han']",
        "date": "June 2021",
        "source": "GLSVLSI '21: Proceedings of the 2021 on Great Lakes Symposium on VLSI",
        "abstract": "The sparse linear solver is an important component in lots of scientific computing applications. For large-scale sparse linear systems, general-purpose processors such as CPUs and GPUs are facing challenges of high time complexity and massive data movements between processors and main memories. This work utilizes the ability of in-situ analog computing of RRAMs and builds an RRAMbased accelerator for iterative linear solvers.We first propose a basic principle of mapping iterative solvers onto RRAM-based crossbar arrays. The proposed principle eliminates not only the iterations but also the convergence condition. Based on the principle, we propose a scalable architecture that can solve large-scale sparse matrices in O(1) time complexity. Compared with a massively parallel iterative solver on GPU, our accelerator shows 100× higher performance and 1000× energy reduction. If the solution obtained by our accelerator is used as the seed for a further refinement on GPU, about 35% of the solving time and energy consumption can be saved compared with a pure GPU solving process.",
        "link": "https://dl.acm.org/doi/10.1145/3453688.3461510",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Serverless computing on heterogeneous computers",
        "authors": "['Dong Du', 'Qingyuan Liu', 'Xueqiang Jiang', 'Yubin Xia', 'Binyu Zang', 'Haibo Chen']",
        "date": "February 2022",
        "source": "ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "Existing serverless computing platforms are built upon homogeneous computers, limiting the function density and restricting serverless computing to limited scenarios. We introduce Molecule, the first serverless computing system utilizing heterogeneous computers. Molecule enables both general-purpose devices (e.g., Nvidia DPU) and domain-specific accelerators (e.g., FPGA and GPU) for serverless applications that significantly improve function density (50% higher) and application performance (up to 34.6x). To achieve these results, we first propose XPU-Shim, a distributed shim to bridge the gap between underlying multi-OS systems (when using general-purpose devices) and our serverless runtime (i.e., Molecule). We further introduce vectorized sandbox, a sandbox abstraction to abstract hardware heterogeneity (when using domain-specific accelerators). Moreover, we also review state-of-the-art serverless optimizations on startup and communication latency and overcome the challenges to implement them on heterogeneous computers. We have implemented Molecule on real platforms with Nvidia DPUs and Xilinx FPGAs and evaluate it using benchmarks and real-world applications.",
        "link": "https://dl.acm.org/doi/10.1145/3503222.3507732",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Understanding I/O Direct Cache Access Performance for End Host Networking",
        "authors": "['Minhu Wang', 'Mingwei Xu', 'Jianping Wu']",
        "date": "None",
        "source": "Proceedings of the ACM on Measurement and Analysis of Computing Systems",
        "abstract": "Direct Cache Access (DCA) enables a network interface card (NIC) to load and store data directly on the processor cache, as conventional Direct Memory Access (DMA) is no longer suitable as the bridge between NIC and CPU in the era of 100 Gigabit Ethernet. As numerous I/O devices and cores compete for scarce cache resources, making the most of DCA for networking applications with varied objectives and constraints is a challenge, especially given the increasing complexity of modern cache hardware and I/O stacks. In this paper, we reverse engineer details of one commercial implementation of DCA, Intel's Data Direct I/O (DDIO), to explicate the importance of hardware-level investigation into DCA. Based on the learned knowledge of DCA and network I/O stacks, we (1) develop an analytical framework to predict the effectiveness of DCA (i.e., its hit rate) under certain hardware specifications, system configurations, and application properties; (2) measure penalties of the ineffective use of DCA (i.e., its miss penalty) to characterize its benefits; and (3) show that our reverse engineering, measurement, and model contribute to a deeper understanding of DCA, which in turn helps diagnose, optimize, and design end-host networking.",
        "link": "https://dl.acm.org/doi/10.1145/3508042",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A Critical Review of how Public Display Interfaces Facilitate Placemaking",
        "authors": "['Paul Biedermann', 'Andrew Vande Moere']",
        "date": "June 2021",
        "source": "MAB20: Media Architecture Biennale 20",
        "abstract": "Although public interfaces are promised to facilitate placemaking by offering a technological platform between citizens and decision makers, little is known about whether they actually bring these stakeholders closer together towards local transformative change. By systematically analysing the infrastructural concepts, methods and tools of 40 interface deployments, this review presents a relational model that describes how a public interface can afford the communication, reflection or inquiry of civic feedback. Our analysis also reveals how most public interfaces: are based on utilitarian motives rather than facilitating placemaking; provoke dialogues among citizens instead of between stakeholders; fail to upend the hierachical dependencies between stakeholders; make use of certain technological means that limit citizen agency; and are controlled by gatekeepers who operate covertly and without accountability. Based on these findings, we propose five ”middle-out” considerations that inform how the next generation of placemaking interfaces can facilitate more meaningful and democratic bilateral dialogues between citizens and decision makers.",
        "link": "https://dl.acm.org/doi/10.1145/3469410.3469427",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "GPUReplay: a 50-KB GPU stack for client ML",
        "authors": "['Heejin Park', 'Felix Xiaozhu Lin']",
        "date": "February 2022",
        "source": "ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "GPUReplay (GR) is a novel way for deploying GPU-accelerated computation on mobile and embedded devices. It addresses high complexity of a modern GPU stack for deployment ease and security. The idea is to record GPU executions on the full GPU stack ahead of time and replay the executions on new input at run time. We address key challenges towards making GR feasible, sound, and practical to use. The resultant replayer is a drop-in replacement of the original GPU stack. It is tiny (50 KB of executable), robust (replaying long executions without divergence), portable (running in a commodity OS, in TEE, and baremetal), and quick to launch (speeding up startup by up to two orders of magnitude). We show that GPUReplay works with a variety of integrated GPU hardware, GPU APIs, ML frameworks, and 33 neural network (NN) implementations for inference or training. The code is available at https://github.com/bakhi/GPUReplay.",
        "link": "https://dl.acm.org/doi/10.1145/3503222.3507754",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "High-Level Synthesis Implementation of an Embedded Real-Time HEVC Intra Encoder on FPGA for Media Applications",
        "authors": "['Panu Sjövall', 'Ari Lemmetti', 'Jarno Vanne', 'Sakari Lahti', 'Timo D. Hämäläinen']",
        "date": "None",
        "source": "ACM Transactions on Design Automation of Electronic Systems",
        "abstract": "High Efficiency Video Coding (HEVC) is the key enabling technology for numerous modern media applications. Overcoming its computational complexity and customizing its rich features for real-time HEVC encoder implementations, calls for automated design methodologies. This article introduces the first complete High-Level Synthesis (HLS) implementation for HEVC intra encoder on FPGA. The C source code of our open-source Kvazaar HEVC encoder is used as a design entry point for HLS that is applied throughout the whole encoder design process, from data-intensive coding tools like intra prediction and discrete transforms to more control-oriented tools such as context-adaptive binary arithmetic coding (CABAC). Our prototype is run on Nokia AirFrame Cloud Server equipped with 2.4 GHz dual 14-core Intel Xeon processors and two Intel Arria 10 PCIe FPGA accelerator cards with 40 Gigabit Ethernet. This proof-of-concept system is designed for hardware-accelerated HEVC encoding and it achieves real-time 4K coding speed up to 120 fps. The coding performance can be easily scaled up by adding practically any number of network-connected FPGA cards to the system. These results indicate that our HLS proposal not only boosts development time, but also provides previously unseen design scalability with competitive performance over the existing FPGA and ASIC encoder implementations.",
        "link": "https://dl.acm.org/doi/10.1145/3491215",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Improving Loop Parallelization by a Combination of Static and Dynamic Analyses in HLS",
        "authors": "['Florian Dewald', 'Johanna Rohde', 'Christian Hochberger', 'Heiko Mantel']",
        "date": "None",
        "source": "ACM Transactions on Reconfigurable Technology and Systems",
        "abstract": "High-level synthesis (HLS) can be used to create hardware accelerators for compute-intense software parts such as loop structures. Usually, this process requires significant amount of user interaction to steer kernel selection and optimizations. This can be tedious and time-consuming. In this article, we present an approach that fully autonomously finds independent loop iterations and reductions to create parallelized accelerators. We combine static analysis with information available only at runtime to maximize the parallelism exploited by the created accelerators. For loops where we see potential for parallelism, we create fully parallelized kernel implementations. If static information does not suffice to deduce independence, then we assume independence at compile time. We verify this assumption by statically created checks that are dynamically evaluated at runtime, before using the optimized kernel. Evaluating our approach, we can generate speedups for five out of seven benchmarks. With four loop iterations running in parallel, we achieve ideal speedups of up to 4× and on average speedups of 2.27×, both in comparison to an unoptimized accelerator.",
        "link": "https://dl.acm.org/doi/10.1145/3501801",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Pho$: a case for shared optical cache hierarchies",
        "authors": "['Haiyang Han', 'Theoni Alexoudi', 'Chris Vagionas', 'Nikos Pleros', 'Nikos Hardavellas']",
        "date": "July 2021",
        "source": "ISLPED '21: Proceedings of the ACM/IEEE International Symposium on Low Power Electronics and Design",
        "abstract": "Conventional electronic memory hierarchies are intrinsically limited in their ability to overcome the memory wall due to scaling constraints. Optical caches and interconnects can mitigate these constraints, and enable processors to reach performance and energy efficiency unattainable by purely electronic means. However, the promised benefits cannot be realized through a simple replacement process; to reach its full potential, the architecture needs to be holistically redesigned. This paper proposes Pho$, an opto-electronic memory hierarchy architecture for multicores. Pho$ replaces conventional core-private electronic caches with a large shared optical L1 built with optical SRAMs. A novel optical NoC provides low-latency and high-bandwidth communication between the electronic cores and the shared optical L1 at low optical loss. Our results show that Pho$ achieves on average 1.41x performance speedup (3.89x max) and 31% lower energy-delay product (90% max) against conventional designs. Moreover, the optical NoC for core-cache communication consumes 70% less power compared to directly applying previously-proposed optical NoC architectures.",
        "link": "https://dl.acm.org/doi/10.1109/ISLPED52811.2021.9502487",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Real-time, High-resolution Depth Upsampling on Embedded Accelerators",
        "authors": "['David Langerman', 'Alan George']",
        "date": "None",
        "source": "ACM Transactions on Embedded Computing Systems",
        "abstract": "High-resolution, low-latency apps in computer vision are ubiquitous in today’s world of mixed-reality devices. These innovations provide a platform that can leverage the improving technology of depth sensors and embedded accelerators to enable higher-resolution, lower-latency processing for 3D scenes using depth-upsampling algorithms. This research demonstrates that filter-based upsampling algorithms are feasible for mixed-reality apps using low-power hardware accelerators. The authors parallelized and evaluated a depth-upsampling algorithm on two different devices: a reconfigurable-logic FPGA embedded within a low-power SoC; and a fixed-logic embedded graphics processing unit. We demonstrate that both accelerators can meet the real-time requirements of 11 ms latency for mixed-reality apps.1",
        "link": "https://dl.acm.org/doi/10.1145/3436878",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "PSSM: achieving secure memory for GPUs with partitioned and sectored security metadata",
        "authors": "['Shougang Yuan', 'Yan Solihin', 'Huiyang Zhou']",
        "date": "June 2021",
        "source": "ICS '21: Proceedings of the ACM International Conference on Supercomputing",
        "abstract": "In this paper, we investigate the secure memory architecture for GPUs and point out that conventional CPU secure memory architecture can not be directly adopted to the GPUs. The key reasons include: (1) accessing the security metadata, including encryption counters, message authentication codes (MACs) and integrity trees, requires significant memory bandwidth, which may lead to severe bandwidth competition with normal data accesses and degrade the GPU performance; (2) contemporary GPUs use partitioned memory organization, which results in storage and coherence problems for encryption counters and integrity trees since different partitions may need to update the same counter/integrity tree blocks; and (3) the existing split-counter block organization is not friendly to sectored caches, which are commonly used in GPU for bandwidth savings. Based on these observations, we propose partitioned and sectored security metadata (PSSM), which has two components: (a) using the offset addresses (referred to as local addresses) within each partition, instead of the virtual or physical addresses, to generate the metadata so as to solve the counter or integrity tree storage and coherence problem and (b) reorganizing the security metadata to make them friendly to the sectored cache structure so as to reduce the memory bandwidth consumption of metadata accesses. With these proposed schemes, the performance overhead of secure GPU memory is reduced from 59.22% to 16.84% on average. If only memory encryption is required, the performance overhead is reduced from 29.53% to 5.18%.",
        "link": "https://dl.acm.org/doi/10.1145/3447818.3460374",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Mansard Roofline Model: Reinforcing the Accuracy of the Roofs",
        "authors": "['Diogo Marques', 'Aleksandar Ilic', 'Leonel Sousa']",
        "date": "None",
        "source": "ACM Transactions on Modeling and Performance Evaluation of Computing Systems",
        "abstract": "Continuous enhancements and diversity in modern multi-core hardware, such as wider and deeper core pipelines and memory subsystems, bring to practice a set of hard-to-solve challenges when modeling their upper-bound capabilities and identifying the main application bottlenecks. Insightful roofline models are widely used for this purpose, but the existing approaches overly abstract the micro-architecture complexity, thus providing unrealistic performance bounds that lead to a misleading characterization of real-world applications. To address this problem, the Mansard Roofline Model (MaRM), proposed in this work, uncovers a minimum set of architectural features that must be considered to provide insightful, but yet accurate and realistic, modeling of performance upper bounds for modern processors. By encapsulating the retirement constraints due to the amount of retirement slots, Reorder-Buffer and Physical Register File sizes, the proposed model accurately models the capabilities of a real platform (average rRMSE of 5.4%) and characterizes 12 application kernels from standard benchmark suites. By following a herein proposed MaRM interpretation methodology and guidelines, speed-ups of up to 5× are obtained when optimizing real-world bioinformatic application, as well as a super-linear speedup of 18.5× when parallelized.",
        "link": "https://dl.acm.org/doi/10.1145/3475866",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Beyond Cache Attacks: Exploiting the Bus-based Communication Structure for Powerful On-Chip Microarchitectural Attacks",
        "authors": "['Johanna Sepúlveda', 'Mathieu Gross', 'Andreas Zankl', 'Georg Sigl']",
        "date": "None",
        "source": "ACM Transactions on Embedded Computing Systems",
        "abstract": "System-on-Chips (SoCs) are a key enabling technology for the Internet-of-Things (IoT), a hyper-connected world where on- and inter-chip communication is ubiquitous. SoCs usually integrate cryptographic hardware cores for confidentiality and authentication services. However, these components are prone to implementation attacks. During the operation of a cryptographic core, the secret key may passively be inferred through cache observations. Access-driven attacks exploiting these observations are therefore a vital threat to SoCs operating in IoT environments. Previous works have shown the feasibility of these attacks in the SoC context. Yet, the SoC communication structure can be used to further improve access-based cache attacks. The communication attacks are not as well-understood as other micro-architectural attacks. It is important to raise the awareness of SoC designers of such a threat. To this end, we present four contributions. First, we demonstrate an improved Prime+Probe attack on four different AES-128 implementations (original transformation tables, T0-Only, T2KB, and S-Box). As a novelty, this attack exploits the collisions of the bus-based SoC communication to further increase its efficiency. Second, we explore the impact of preloading on the efficiency of our communication-optimized attack. Third, we integrate three countermeasures (shuffling, mini-tables, and Time-Division Multiple Access (TDMA) bus arbitration) and evaluate their impact on the attack. Although shuffling and mini-tables countermeasures were proposed in previous work, their application as countermeasures against the bus-based attack was not studied before. In addition, TDMA as a countermeasure for bus-based attacks is an original contribution of this work. Fourth, we further discuss the implications of our work in the SoC design and its perspective with the new cryptographic primitives proposed in the ongoing National Institute of Standard and Technology Lightweight Cryptography competition. The results show that our improved communication-optimized attack is efficient, speeding up full key recovery by up to 400 times when compared to the traditional Prime+Probe technique. Moreover, the protection techniques are feasible and effectively mitigate the proposed improved attack.",
        "link": "https://dl.acm.org/doi/10.1145/3433653",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Design of many-core big little μbrains for energy-efficient embedded neuromorphic computing",
        "authors": "['M. Lakshmi Varshika', 'Adarsha Balaji', 'Federico Corradi', 'Anup Das', 'Jan Stuijt', 'Francky Catthoor']",
        "date": "March 2022",
        "source": "DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe",
        "abstract": "As spiking-based deep learning inference applications are increasing in embedded systems, these systems tend to integrate neuromorphic accelerators such as μBrain to improve energy efficiency. We propose a μBrain-based scalable many-core neuromorphic hardware design to accelerate the computations of spiking deep convolutional neural networks (SDCNNs). To increase energy efficiency, cores are designed to be heterogeneous in terms of their neuron and synapse capacity (i.e., big vs. little cores), and they are interconnected using a parallel segmented bus interconnect, which leads to lower latency and energy compared to a traditional mesh-based Network-on-Chip (NoC). We propose a system software framework called SentryOS to map SDCNN inference applications to the proposed design. SentryOS consists of a compiler and a run-time manager. The compiler compiles an SDCNN application into sub-networks by exploiting the internal architecture of big and little μBrain cores. The run-time manager schedules these sub-networks onto cores and pipeline their execution to improve throughput. We evaluate the proposed big little many-core neuromorphic design and the system software framework with five commonly-used SDCNN inference applications and show that the proposed solution reduces energy (between 37% and 98%), reduces latency (between 9% and 25%), and increases application throughput (between 20% and 36%). We also show that SentryOS can be easily extended for other spiking neuromorphic accelerators such as Loihi and DYNAPs.",
        "link": "https://dl.acm.org/doi/10.5555/3539845.3540079",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A Pattern for Proof of Work Consensus Algorithm in Blockchain",
        "authors": "['Zain ul Abadin', 'Madiha Syed']",
        "date": "July 2021",
        "source": "EuroPLoP'21: 26th European Conference on Pattern Languages of Programs",
        "abstract": "Blockchain technology has gained immense popularity over the last decade. It has transformed several industries by enabling processes to work in a secure and decentralized manner. Blockchain consists of blocks that are linked together through cryptography. Blockchain is a distributed ledger that contains a set of sequenced blocks of data. Each block records transaction data in a transparent, immutable, and secure fashion. Consensus algorithm is the key part of this technology as it is used for the decision-making process among nodes in the blockchain network. Nodes in the verifying network use consensus algorithm to reach mutual agreement about state of the blockchain whenever a new block is added. This makes it possible to accept any transaction in the blockchain. Many consensus algorithms have been proposed in the literature each having its own performance and security characteristics. In this paper, we present a pattern for one of the most widely used blockchain consensus algorithms, which is the Proof of Work (PoW) algorithm. This paper is a part of our project on blockchain architecture and patterns.",
        "link": "https://dl.acm.org/doi/10.1145/3489449.3489994",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Toward Multi-FPGA Acceleration of the Neural Networks",
        "authors": "['Saman Biookaghazadeh', 'Pravin Kumar Ravi', 'Ming Zhao']",
        "date": "None",
        "source": "ACM Journal on Emerging Technologies in Computing Systems",
        "abstract": "High-throughput and low-latency Convolutional Neural Network (CNN) inference is increasingly important for many cloud- and edge-computing applications. FPGA-based acceleration of CNN inference has demonstrated various benefits compared to other high-performance devices such as GPGPUs. Current FPGA CNN-acceleration solutions are based on a single FPGA design, which are limited by the available resources on an FPGA. In addition, they can only accelerate conventional 2D neural networks. To address these limitations, we present a generic multi-FPGA solution, written in OpenCL, which can accelerate more complex CNNs (e.g., C3D CNN) and achieve a near linear speedup with respect to the available single-FPGA solutions. The design is built upon the Intel Deep Learning Accelerator architecture, with three extensions. First, it includes updates for better area efficiency (up to 25%) and higher performance (up to 24%). Second, it supports 3D convolutions for more challenging applications such as video learning. Third, it supports multi-FPGA communication for higher inference throughput. The results show that utilizing multiple FPGAs can linearly increase the overall bandwidth while maintaining the same end-to-end latency. In addition, the design can outperform other FPGA 2D accelerators by up to 8.4 times and 3D accelerators by up to 1.7 times.",
        "link": "https://dl.acm.org/doi/10.1145/3432816",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "PLANAR: a programmable accelerator for near-memory data rearrangement",
        "authors": "['Adrián Barredo', 'Adrià Armejach', 'Jonathan Beard', 'Miquel Moreto']",
        "date": "June 2021",
        "source": "ICS '21: Proceedings of the ACM International Conference on Supercomputing",
        "abstract": "Many applications employ irregular and sparse memory accesses that cannot take advantage of existing cache hierarchies in high performance processors. To solve this problem, Data Layout Transformation (DLT) techniques rearrange sparse data into a dense representation, improving locality and cache utilization. However, prior proposals in this space fail to provide a design that (i) scales with multi-core systems, (ii) hides rearrangement latency, and (iii) provides the necessary interfaces to ease programmability. In this work we present PLANAR, a programmable near-memory accelerator that rearranges sparse data into dense. By placing PLANAR devices at the memory controller level we enable a design that scales well with multi-core systems, hides operation latency by performing non-blocking fine-grain data rearrangements, and eases programmability by supporting virtual memory and conventional memory allocation mechanisms. Our evaluation shows that PLANAR leads to significant reductions in data movement and dynamic energy, providing an average 4.58× speedup.",
        "link": "https://dl.acm.org/doi/10.1145/3447818.3460368",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "An Effective Forest Fire Detection Framework Using Heterogeneous Wireless Multimedia Sensor Networks",
        "authors": "['Burak Kizilkaya', 'Enver Ever', 'Hakan Yekta Yatbaz', 'Adnan Yazici']",
        "date": "None",
        "source": "ACM Transactions on Multimedia Computing, Communications, and Applications",
        "abstract": "With improvements in the area of Internet of Things (IoT), surveillance systems have recently become more accessible. At the same time, optimizing the energy requirements of smart sensors, especially for data transmission, has always been very important and the energy efficiency of IoT systems has been the subject of numerous studies. For environmental monitoring scenarios, it is possible to extract more accurate information using smart multimedia sensors. However, multimedia data transmission is an expensive operation. In this study, a novel hierarchical approach is presented for the detection of forest fires. The proposed framework introduces a new approach in which multimedia and scalar sensors are used hierarchically to minimize the transmission of visual data. A lightweight deep learning model is also developed for devices at the edge of the network to improve detection accuracy and reduce the traffic between the edge devices and the sink. The framework is evaluated using a real testbed, network simulations, and 10-fold cross-validation in terms of energy efficiency and detection accuracy. Based on the results of our experiments, the validation accuracy of the proposed system is 98.28%, and the energy saving is 29.94%. The proposed deep learning model’s validation accuracy is very close to the accuracy of the best performing architectures when the existing studies and lightweight architectures are considered. In terms of suitability for edge computing, the proposed approach is superior to the existing ones with reduced computational requirements and model size.",
        "link": "https://dl.acm.org/doi/10.1145/3473037",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Design of English Teaching Interactive System Based on Artificial Intelligence",
        "authors": "['Huimin Zhao']",
        "date": "February 2022",
        "source": "ICBDE '22: Proceedings of the 5th International Conference on Big Data and Education",
        "abstract": "By exploring the characteristic of interaction of network teaching, we proposed to develop an English teaching interaction system based on artificial intelligence, which can solve the problems in current English teaching class. We analyzed the functional requirements of the system corresponding to different user. By buiding Ubuntu operating system as the computing platform and exploring the object-oriented dynamic teaching environment provided by artificial intelligence platform, we build the LAMP development environment with MySQL database and PHP script language. We constructed a three-tier separated architecture system by using B/S mode, and refined the design and implementation of each module function through the secondary development of platform module plug-ins. To exhibit the advantages of the artificial intelligence system, we take college English teaching as an example to illustrate its functions. The functions of curriculum creation, teaching resources and activity design can be realized in the system, which verifies the effectiveness of the artificial intelligence system in interactive teaching and learning.",
        "link": "https://dl.acm.org/doi/10.1145/3524383.3533247",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Ratatoskr: An Open-Source Framework for In-Depth Power, Performance, and Area Analysis and Optimization in 3D NoCs",
        "authors": "['Jan Moritz Joseph', 'Lennart Bamberg', 'Imad Hajjar', 'Behnam Razi Perjikolaei', 'Alberto García-Ortiz', 'Thilo Pionteck']",
        "date": "None",
        "source": "ACM Transactions on Modeling and Computer Simulation",
        "abstract": "We introduce Ratatoskr, an open-source framework for in-depth power, performance, and area (PPA) analysis in Networks-on-Chips (NoCs) for 3D-integrated and heterogeneous System-on-Chips (SoCs). It covers all layers of abstraction by providing an NoC hardware implementation on Register Transfer Level (RTL), an NoC simulator on cycle-accurate level and an application model on transaction level. By this comprehensive approach, Ratatoskr can provide the following specific PPA analyses: Dynamic power of links can be measured within 2.4% accuracy of bit-level simulations while maintaining cycle-accurate simulation speed. Router power is determined from RTL-to-gate-level synthesis combined with cycle-accurate simulations. The performance of the whole NoC can be measured both via cycle-accurate and RTL simulations. The performance (i.e., timing) of individual routers and the NoC area are obtained from RTL synthesis results. Despite these manifold features, Ratatoskr offers easy two-step user interaction: (1) A single point-of-entry allows setting design parameters. (2) PPA reports are generated automatically. For both the input and the output, different levels of abstraction can be chosen for high-level rapid network analysis or low-level improvement of architectural details. The synthesizable NoC-RTL model shows improved total router power and area in comparison to a conventional standard router. As a forward-thinking and unique feature not found in other NoC PPA-measurement tools, Ratatoskr supports heterogeneous 3D integration that is one of the most promising integration paradigms for upcoming SoCs. Thereby, Ratatoskr lays the groundwork to design their communication architectures. The framework is publicly available at  https://github.com/ratatoskr-project.",
        "link": "https://dl.acm.org/doi/10.1145/3472754",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Binary Precision Neural Network Manycore Accelerator",
        "authors": "['Morteza Hosseini', 'Tinoosh Mohsenin']",
        "date": "None",
        "source": "ACM Journal on Emerging Technologies in Computing Systems",
        "abstract": "This article presents a low-power, programmable, domain-specific manycore accelerator, Binarized neural Network Manycore Accelerator (BiNMAC), which adopts and efficiently executes binary precision weight/activation neural network models. Such networks have compact models in which weights are constrained to only 1 bit and can be packed several in one memory entry that minimizes memory footprint to its finest. Packing weights also facilitates executing single instruction, multiple data with simple circuitry that allows maximizing performance and efficiency. The proposed BiNMAC has light-weight cores that support domain-specific instructions, and a router-based memory access architecture that helps with efficient implementation of layers in binary precision weight/activation neural networks of proper size. With only 3.73% and 1.98% area and average power overhead, respectively, novel instructions such as Combined Population-Count-XNOR, Patch-Select, and Bit-based Accumulation are added to the instruction set architecture of the BiNMAC, each of which replaces execution cycles of frequently used functions with 1 clock cycle that otherwise would have taken 54, 4, and 3 clock cycles, respectively. Additionally, customized logic is added to every core to transpose 16×16-bit blocks of memory on a bit-level basis, that expedites reshaping intermediate data to be well-aligned for bitwise operations. A 64-cluster architecture of the BiNMAC is fully placed and routed in 65-nm TSMC CMOS technology, where a single cluster occupies an area of 0.53 mm2 with an average power of 232 mW at 1-GHz clock frequency and 1.1 V. The 64-cluster architecture takes 36.5 mm2 area and, if fully exploited, consumes a total power of 16.4 W and can perform 1,360 Giga Operations Per Second (GOPS) while providing full programmability. To demonstrate its scalability, four binarized case studies including ResNet-20 and LeNet-5 for high-performance image classification, as well as a ConvNet and a multilayer perceptron for low-power physiological applications were implemented on BiNMAC. The implementation results indicate that the population-count instruction alone can expedite the performance by approximately 5×. When other new instructions are added to a RISC machine with existing population-count instruction, the performance is increased by 58% on average. To compare the performance of the BiNMAC with other commercial-off-the-shelf platforms, the case studies with their double-precision floating-point models are also implemented on the NVIDIA Jetson TX2 SoC (CPU+GPU). The results indicate that, within a margin of ∼2.1%--9.5% accuracy loss, BiNMAC on average outperforms the TX2 GPU by approximately 1.9× (or 7.5× with fabrication technology scaled) in energy consumption for image classification applications. On low power settings and within a margin of ∼3.7%--5.5% accuracy loss compared to ARM Cortex-A57 CPU implementation, BiNMAC is roughly ∼9.7×--17.2× (or 38.8×--68.8× with fabrication technology scaled) more energy efficient for physiological applications while meeting the application deadline.",
        "link": "https://dl.acm.org/doi/10.1145/3423136",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Zerializer: towards zero-copy serialization",
        "authors": "['Adam Wolnikowski', 'Stephen Ibanez', 'Jonathan Stone', 'Changhoon Kim', 'Rajit Manohar', 'Robert Soulé']",
        "date": "June 2021",
        "source": "HotOS '21: Proceedings of the Workshop on Hot Topics in Operating Systems",
        "abstract": "Achieving zero-copy I/O has long been an important goal in the networking community. However, data serialization obviates the benefits of zero-copy I/O, because it requires the CPU to read, transform, and write message data, resulting in additional memory copies between the real object instances and the contiguous socket buffer. Therefore, we argue for offloading serialization logic to the DMA path via specialized hardware. We propose an initial hardware design for such an accelerator, and give preliminary evidence of its feasibility and expected benefits.",
        "link": "https://dl.acm.org/doi/10.1145/3458336.3465283",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "An Efficient Video Prediction Recurrent Network using Focal Loss and Decomposed Tensor Train for Imbalance Dataset",
        "authors": "['Mingshuo Liu', 'Kevin Han', 'Shiyi Luo', 'Mingze Pan', 'Mousam Hossain', 'Bo Yuan', 'Ronald F. DeMara', 'Yu Bai']",
        "date": "June 2021",
        "source": "GLSVLSI '21: Proceedings of the 2021 on Great Lakes Symposium on VLSI",
        "abstract": "Nowadays, from companies to academics, researchers across the world are interested in developing recurrent neural networks due to their incredible feats in various applications, such as speech recognition, video detection, predictions, and machine translation. However, the advantages of recurrent neural networks accompanied by high computational and power demands, which are a major design constraint for electronic devices with limited resources used in such network implementations. Optimizing the recurrent neural networks, such as model compression, is crucial to ensure the broad deployment of recurrent neural networks and promote recurrent neural networks for implementing most resource-constrained scenarios. Among many techniques, tensor train (TT) decomposition is considered an up-and-coming technology. Although our previous efforts have achieved 1) expanding limits of many multiplications within eliminating all redundant computations; and 2) decomposing into multi-stage processing to reduce memory traffic, this work still faces some limitations. In particular, current TT decomposition on recurrent neural networks leads to a complex computation sensitive to the quality of training datasets. In this paper, we investigate a new method for TT decomposition on recurrent neural networks for constructing an efficient model within imbalance datasets to overcome this issue. Experimental results show that the proposed new training method can achieve significant improvements in accuracy, precision, recall, F1-score, False Negative Rate (FNR), and False Omission Rate (FOR).",
        "link": "https://dl.acm.org/doi/10.1145/3453688.3461748",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Electromyography Data Transmission via Galvanic Coupling Intra-body Communication Link",
        "authors": "['Anna Vizziello', 'Pietro Savazzi', 'Giovanni Magenes']",
        "date": "September 2021",
        "source": "NANOCOM '21: Proceedings of the Eight Annual ACM International Conference on Nanoscale Computing and Communication",
        "abstract": "Intra-body communication (IBC) is a novel research filed that will promote personalized medicine by allowing real time and in situ monitoring in daily life. In this work, the energy efficient galvanic coupling (GC) technology is used to send electromyography (EMG) data through intra-body links. Real EMG data are first acquired and recorded with needle electrodes inserted in the muscle of a person's forearm. Then, the data are transferred via GC intra-body communication employing a GC sound card-based testbed. The experiments are conducted by transferring EMG data in both ex-vivo and in-vivo tissue with different electrodes placements. Almost error free performance is achieved with a robust and reliable communication, a valuable result in medical applications.",
        "link": "https://dl.acm.org/doi/10.1145/3477206.3477450",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "NVAlloc: rethinking heap metadata management in persistent memory allocators",
        "authors": "['Zheng Dang', 'Shuibing He', 'Peiyi Hong', 'Zhenxin Li', 'Xuechen Zhang', 'Xian-He Sun', 'Gang Chen']",
        "date": "February 2022",
        "source": "ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "Persistent memory allocation is a fundamental building block for developing high-performance and in-memory applications. Existing persistent memory allocators suffer from suboptimal heap organizations that introduce repeated cache line flushes and small random accesses in persistent memory. Worse, many allocators use static slab segregation resulting in a dramatic increase in memory consumption when allocation request size is changed. In this paper, we design a novel allocator, named NVAlloc, to solve the above issues simultaneously. First, NVAlloc eliminates cache line reflushes by mapping contiguous data blocks in slabs to interleaved metadata entries stored in different cache lines. Second, it writes small metadata units to a persistent bookkeeping log in a sequential pattern to remove random heap metadata accesses in persistent memory. Third, instead of using static slab segregation, it supports slab morphing, which allows slabs to be transformed between size classes to significantly improve slab usage. NVAlloc is complementary to the existing consistency models. Results on 6 benchmarks demonstrate that NVAlloc improves the performance of state-of-the-art persistent memory allocators by up to 6.4x and 57x for small and large allocations, respectively. Using NVAlloc reduces memory usage by up to 57.8%. Besides, we integrate NVAlloc in a persistent FPTree. Compared to the state-of-the-art allocators, NVAlloc improves the performance of this application by up to 3.1x.",
        "link": "https://dl.acm.org/doi/10.1145/3503222.3507743",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "RecSSD: near data processing for solid state drive based recommendation inference",
        "authors": "['Mark Wilkening', 'Udit Gupta', 'Samuel Hsia', 'Caroline Trippel', 'Carole-Jean Wu', 'David Brooks', 'Gu-Yeon Wei']",
        "date": "April 2021",
        "source": "ASPLOS '21: Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "Neural personalized recommendation models are used across a wide variety of datacenter applications including search, social media, and entertainment. State-of-the-art models comprise large embedding tables that have billions of parameters requiring large memory capacities. Unfortunately, large and fast DRAM-based memories levy high infrastructure costs. Conventional SSD-based storage solutions offer an order of magnitude larger capacity, but have worse read latency and bandwidth, degrading inference performance. RecSSD is a near data processing based SSD memory system customized for neural recommendation inference that reduces end-to-end model inference latency by 2× compared to using COTS SSDs across eight industry-representative models.",
        "link": "https://dl.acm.org/doi/10.1145/3445814.3446763",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Hipernetch: High-Performance FPGA Network Switch",
        "authors": "['Philippos Papaphilippou', 'Jiuxi Meng', 'Nadeen Gebara', 'Wayne Luk']",
        "date": "None",
        "source": "ACM Transactions on Reconfigurable Technology and Systems",
        "abstract": "We present Hipernetch, a novel FPGA-based design for performing high-bandwidth network switching. FPGAs have recently become more popular in data centers due to their promising capabilities for a wide range of applications. With the recent surge in transceiver bandwidth, they could further benefit the implementation and refinement of network switches used in data centers. Hipernetch replaces the crossbar with a “combined parallel round-robin arbiter”. Unlike a crossbar, the combined parallel round-robin arbiter is easy to pipeline, and does not require centralised iterative scheduling algorithms that try to fit too many steps in a single or a few FPGA cycles. The result is a network switch implementation on FPGAs operating at a high frequency and with a low port-to-port latency. Our proposed Hipernetch architecture additionally provides a competitive switching performance approaching output-queued crossbar switches. Our implemented Hipernetch designs exhibit a throughput that exceeds 100 Gbps per port for switches of up to 16 ports, reaching an aggregate throughput of around 1.7 Tbps.",
        "link": "https://dl.acm.org/doi/10.1145/3477054",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Client layer becomes bottleneck: workload analysis of an ultra-large-scale cloud storage system",
        "authors": "['Xiaoyi Sun', 'Kaishi Li', 'Yaodanjun Ren', 'Jiale Lin', 'Zhenyu Ren', 'Shuzhi Feng', 'Yin Jian', 'Zhengwei Qi']",
        "date": "December 2021",
        "source": "UCC '21: Proceedings of the 14th IEEE/ACM International Conference on Utility and Cloud Computing Companion",
        "abstract": "Recent years have witnessed the fast development of file and storage systems. Many improvements of file and storage systems are inspired by Workload analysis, which reveals the characteristics of I/O behavior. Although cloud storage systems are becoming increasingly prominent, few real-world and large-scale cloud storage workload studies are presented. Alibaba Cloud is one of the world's largest cloud providers, and we have collected and analyzed workloads from Alibaba for an extended period. We observe that modern cloud network architecture can easily handle the peak load during busy festivals. However, the client layer is the system bottleneck during the peak period, which calls for further optimization. We also find that the workload is heavily skewed toward a small percentage of virtual disks, and its distribution conforms 80/20 rule. In summary, the characteristics of such a large-scale cloud storage system in production environments are important for future cloud storage system modifications.",
        "link": "https://dl.acm.org/doi/10.1145/3492323.3495625",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "The Strong Scaling Advantage of FPGAs in HPC for N-body Simulations",
        "authors": "['Johannes Menzel', 'Christian Plessl', 'Tobias Kenter']",
        "date": "None",
        "source": "ACM Transactions on Reconfigurable Technology and Systems",
        "abstract": "N-body methods are one of the essential algorithmic building blocks of high-performance and parallel computing. Previous research has shown promising performance for implementing n-body simulations with pairwise force calculations on FPGAs. However, to avoid challenges with accumulation and memory access patterns, the presented designs calculate each pair of forces twice, along with both force sums of the involved particles. Also, they require large problem instances with hundreds of thousands of particles to reach their respective peak performance, limiting the applicability for strong scaling scenarios. This work addresses both issues by presenting a novel FPGA design that uses each calculated force twice and overlaps data transfers and computations in a way that allows to reach peak performance even for small problem instances, outperforming previous single precision results even in double precision, and scaling linearly over multiple interconnected FPGAs. For a comparison across architectures, we provide an equally optimized CPU reference, which for large problems actually achieves higher peak performance per device, however, given the strong scaling advantages of the FPGA design, in parallel setups with few thousand particles per device, the FPGA platform achieves highest performance and power efficiency.",
        "link": "https://dl.acm.org/doi/10.1145/3491235",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Vector instruction selection for digital signal processors using program synthesis",
        "authors": "['Maaz Bin Safeer Ahmad', 'Alexander J. Root', 'Andrew Adams', 'Shoaib Kamil', 'Alvin Cheung']",
        "date": "February 2022",
        "source": "ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "Instruction selection, whereby input code represented in an intermediate representation is translated into executable instructions from the target platform, is often the most target-dependent component in optimizing compilers. Current approaches include pattern matching, which is brittle and tedious to design, or search-based methods, which are limited by scalability of the search algorithm. In this paper, we propose a new algorithm that first abstracts the target platform instructions into high-level uber-instructions, with each uber-instruction unifying multiple concrete instructions from the target platform. Program synthesis is used to lift input code sequences into semantically equivalent sequences of uber-instructions and then to lower from uber-instructions to machine code. Using 21 real-world benchmarks, we show that our synthesis-based instruction selection algorithm can generate instruction sequences for a hardware target, with the synthesized code performing up to 2.1x faster as compared to code generated by a professionally-developed optimizing compiler for the same platform.",
        "link": "https://dl.acm.org/doi/10.1145/3503222.3507714",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Protecting Network-on-Chip Intellectual Property Using Timing Channel Fingerprinting",
        "authors": "['Arnab Kumar Biswas', 'Biplab Sikdar']",
        "date": "None",
        "source": "ACM Transactions on Embedded Computing Systems",
        "abstract": "The theft of Intellectual property (IP) is a serious security threat for all businesses that are involved in the creation of IP. In this article, we consider such attacks against IP for Network-on-Chip (NoC) that are commonly used as a popular on-chip scalable communication medium for Multiprocessor System-on-Chip. As a protection mechanism, we propose a timing channel fingerprinting method and show its effectiveness by implementing five different solutions using this method. We also provide a formal proof of security of the proposed method. We show that the proposed technique provides better security and requires much lower hardware overhead (64%–74% less) compared to an existing NoC IP security solution without affecting the normal packet latency or degrading the NoC performance.",
        "link": "https://dl.acm.org/doi/10.1145/3495565",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "The end of Moore's law and the rise of the data processor",
        "authors": "['Niv Dayan', 'Moshe Twitto', 'Yuval Rochman', 'Uri Beitler', 'Itai Ben Zion', 'Edward Bortnikov', 'Shmuel Dashevsky', 'Ofer Frishman', 'Evgeni Ginzburg', 'Igal Maly', 'Avraham (Poza) Meir', 'Mark Mokryn', 'Iddo Naiss', 'Noam Rabinovich']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "With the end of Moore's Law, database architects are turning to hardware accelerators to offload computationally intensive tasks from the CPU. In this paper, we show that accelerators can facilitate far more than just computation: they enable algorithms and data structures that lavishly expand computation in order to optimize for disparate cost metrics. We introduce the Pliops Extreme Data Processor (XDP), a novel storage engine implemented from the ground up using customized hardware. At its core, XDP consists of an accelerated hash table to index the data in storage using less memory and fewer storage accesses for queries than the best alternative. XDP also employs an accelerated compressor, a capacitor, and a lock-free RAID sub-system to minimize storage space and recovery time while minimizing performance penalties. As a result, XDP overcomes cost contentions that have so far been inescapable.",
        "link": "https://dl.acm.org/doi/10.14778/3476311.3476373",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A Logarithmic Floating-Point Multiplier for the Efficient Training of Neural Networks",
        "authors": "['Zijing Niu', 'Honglan Jiang', 'Mohammad Saeed Ansari', 'Bruce F. Cockburn', 'Leibo Liu', 'Jie Han']",
        "date": "June 2021",
        "source": "GLSVLSI '21: Proceedings of the 2021 on Great Lakes Symposium on VLSI",
        "abstract": "The development of important applications of increasingly large neural networks (NNs) is spurring research that aims to increase the power efficiency of the arithmetic circuits that perform the huge amount of computation in NNs. The floating-point (FP) representation with a large dynamic range is usually used for training. In this paper, it is shown that the FP representation is naturally suited for the binary logarithm of numbers. Thus, it favors a design based on logarithmic arithmetic. Specifically, we propose an efficient hardware implementation of logarithmic FP multiplication that uses simpler operations to replace complex multipliers for the training of NNs. This design produces a double-sided error distribution that mitigates the accumulative effect of errors in iterative operations, so it is up to 45% more accurate than a recent logarithmic FP design. The proposed multiplier also consumes up to 23.5x less energy and 10.7x smaller area compared to exact FP multipliers. Benchmark NN applications, including a 922-neuron model for the MNIST dataset, show that the classification accuracy can be slightly improved using the proposed multiplier, while achieving up to 2.4x less energy and 2.8x smaller area with a better performance.",
        "link": "https://dl.acm.org/doi/10.1145/3453688.3461509",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Challenging the Security of Logic Locking Schemes in the Era of Deep Learning: A Neuroevolutionary Approach",
        "authors": "['Dominik Sisejkovic', 'Farhad Merchant', 'Lennart M. Reimann', 'Harshit Srivastava', 'Ahmed Hallawa', 'Rainer Leupers']",
        "date": "None",
        "source": "ACM Journal on Emerging Technologies in Computing Systems",
        "abstract": "Logic locking is a prominent technique to protect the integrity of hardware designs throughout the integrated circuit design and fabrication flow. However, in recent years, the security of locking schemes has been thoroughly challenged by the introduction of various deobfuscation attacks. As in most research branches, deep learning is being introduced in the domain of logic locking as well. Therefore, in this article we present SnapShot, a novel attack on logic locking that is the first of its kind to utilize artificial neural networks to directly predict a key bit value from a locked synthesized gate-level netlist without using a golden reference. Hereby, the attack uses a simpler yet more flexible learning model compared to existing work. Two different approaches are evaluated. The first approach is based on a simple feedforward fully connected neural network. The second approach utilizes genetic algorithms to evolve more complex convolutional neural network architectures specialized for the given task. The attack flow offers a generic and customizable framework for attacking locking schemes using machine learning techniques. We perform an extensive evaluation of SnapShot for two realistic attack scenarios, comprising both reference combinational and sequential benchmark circuits as well as silicon-proven RISC-V core modules. The evaluation results show that SnapShot achieves an average key prediction accuracy of 82.60% for the selected attack scenario, with a significant performance increase of 10.49 percentage points compared to the state of the art. Moreover, SnapShot outperforms the existing technique on all evaluated benchmarks. The results indicate that the security foundation of common logic locking schemes is built on questionable assumptions. Based on the lessons learned, we discuss the vulnerabilities and potentials of logic locking uncovered by SnapShot. The conclusions offer insights into the challenges of designing future logic locking schemes that are resilient to machine learning attacks.",
        "link": "https://dl.acm.org/doi/10.1145/3431389",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Hybrid, scalable, trace-driven performance modeling of GPGPUs",
        "authors": "['Yehia Arafa', 'Abdel-Hameed Badawy', 'Ammar ElWazir', 'Atanu Barai', 'Ali Eker', 'Gopinath Chennupati', 'Nandakishore Santhi', 'Stephan Eidenbenz']",
        "date": "November 2021",
        "source": "SC '21: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",
        "abstract": "In this paper, we present PPT-GPU, a scalable performance prediction toolkit for GPUs. PPT-GPU achieves scalability through a hybrid high-level modeling approach where some computations are extrapolated and multiple parts of the model are parallelized. The tool primary prediction models use pre-collected memory and instructions traces of the workloads to accurately capture the dynamic behavior of the kernels. PPT-GPU reports an extensive array of GPU performance metrics accurately while being easily extensible. We use a broad set of benchmarks to verify predictions accuracy. We compare the results against hardware metrics collected using vendor profiling tools and cycle-accurate simulators. The results show that the performance predictions are highly correlated to the actual hardware (MAPE: < 16% and Correlation: > 0.98). Moreover, PPT-GPU is orders of magnitude faster than cycle-accurate simulators. This comprehensiveness of the collected metrics can guide architects and developers to perform design space explorations. Moreover, the scalability of the tool enables conducting efficient and fast sensitivity analyses for performance-critical applications.",
        "link": "https://dl.acm.org/doi/10.1145/3458817.3476221",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "One size does not fit all: security hardening of MIPS embedded systems via static binary debloating for shared libraries",
        "authors": "['Haotian Zhang', 'Mengfei Ren', 'Yu Lei', 'Jiang Ming']",
        "date": "February 2022",
        "source": "ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "Embedded systems have become prominent targets for cyberattacks. To exploit firmware’s memory corruption vulnerabilities, cybercriminals harvest reusable code gadgets from the large shared library codebase (e.g., uClibc). Unfortunately, unlike their desktop counterparts, embedded systems lack essential computing resources to enforce security hardening techniques. Recently, we have witnessed a surge of software debloating as a new defense mechanism against code-reuse attacks; it erases unused code to significantly diminish the possibilities of constructing reusable gadgets. Because of the single firmware image update style, static library debloating shows promise to fortify embedded systems without compromising performance and forward compatibility. However, static library debloating on stripped binaries (e.g., firmware’s shared libraries) is still an enormous challenge. In this paper, we show that this challenge is not insurmountable for MIPS firmware. We develop a novel system, named uTrimmer, to identify and wipe out unused basic blocks from shared libraries’ binary code, without causing additional runtime overhead or memory consumption. We propose a new method to identify address-taken blocks/functions, which further help us maintain an inter-procedural control flow graph to conservatively include library code that could be potentially used by firmware. By capturing address access patterns for position-independent code, we circumvent the challenge of determining code-pointer targets and safely eliminate unused code. We run uTrimmer to debloat shared libraries for SPEC CPU2017 benchmarks, popular firmware applications (e.g., Apache, BusyBox, and OpenSSL), and a real-world wireless router firmware image. Our experiments show that not only does uTrimmer deliver functional programs, but also it can cut the exposed code surface and eliminate various reusable code gadgets remarkably. uTrimmer’s debloating capability can compete with the static linking results.",
        "link": "https://dl.acm.org/doi/10.1145/3503222.3507768",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A Design Methodology for Energy-Aware Processing in Unmanned Aerial Vehicles",
        "authors": "['Jingyu He', 'Yao Xiao', 'Corina Bogdan', 'Shahin Nazarian', 'Paul Bogdan']",
        "date": "None",
        "source": "ACM Transactions on Design Automation of Electronic Systems",
        "abstract": "Unmanned Aerial Vehicles (UAVs) have rapidly become popular for monitoring, delivery, and actuation in many application domains such as environmental management, disaster mitigation, homeland security, energy, transportation, and manufacturing. However, the UAV perception and navigation intelligence (PNI) designs are still in their infancy and demand fundamental performance and energy optimizations to be eligible for mass adoption. In this article, we present a generalizable three-stage optimization framework for PNI systems that (i) abstracts the high-level programs representing the perception, mining, processing, and decision making of UAVs into complex weighted networks tracking the interdependencies between universal low-level intermediate representations; (ii) exploits a differential geometry approach to schedule and map the discovered PNI tasks onto an underlying manycore architecture. To mine the complexity of optimal parallelization of perception and decision modules in UAVs, this proposed design methodology relies on an Ollivier-Ricci curvature-based load-balancing strategy that detects the parallel communities of the PNI applications for maximum parallel execution, while minimizing the inter-core communication; and (iii) relies on an energy-aware mapping scheme to minimize the energy dissipation when assigning the communities onto tile-based networks-on-chip. We validate this approach based on various drone PNI designs including flight controller, path planning, and visual navigation. The experimental results confirm that the proposed framework achieves 23% flight time reduction and up to 34% energy savings for the flight controller application. In addition, the optimization on a 16-core platform improves the on-time visit rate of the path planning algorithm by 14% while reducing 81% of run time for ConvNet visual navigation.",
        "link": "https://dl.acm.org/doi/10.1145/3470451",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "The Android Platform Security Model",
        "authors": "['René Mayrhofer', 'Jeffrey Vander Stoep', 'Chad Brubaker', 'Nick Kralevich']",
        "date": "None",
        "source": "ACM Transactions on Privacy and Security",
        "abstract": "Android is the most widely deployed end-user focused operating system. With its growing set of use cases encompassing communication, navigation, media consumption, entertainment, finance, health, and access to sensors, actuators, cameras, or microphones, its underlying security model needs to address a host of practical threats in a wide variety of scenarios while being useful to non-security experts. The model needs to strike a difficult balance between security, privacy, and usability for end users, assurances for app developers, and system performance under tight hardware constraints. While many of the underlying design principles have implicitly informed the overall system architecture, access control mechanisms, and mitigation techniques, the Android security model has previously not been formally published. This article aims to both document the abstract model and discuss its implications. Based on a definition of the threat model and Android ecosystem context in which it operates, we analyze how the different security measures in past and current Android implementations work together to mitigate these threats. There are some special cases in applying the security model, and we discuss such deliberate deviations from the abstract model.",
        "link": "https://dl.acm.org/doi/10.1145/3448609",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Defensive approximation: securing CNNs using approximate computing",
        "authors": "['Amira Guesmi', 'Ihsen Alouani', 'Khaled N. Khasawneh', 'Mouna Baklouti', 'Tarek Frikha', 'Mohamed Abid', 'Nael Abu-Ghazaleh']",
        "date": "April 2021",
        "source": "ASPLOS '21: Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "In the past few years, an increasing number of machine-learning and deep learning structures, such as Convolutional Neural Networks (CNNs), have been applied to solving a wide range of real-life problems. However, these architectures are vulnerable to adversarial attacks: inputs crafted carefully to force the system output to a wrong label. Since machine-learning is being deployed in safety-critical and security-sensitive domains, such attacks may have catastrophic security and safety consequences. In this paper, we propose for the first time to use hardware-supported approximate computing to improve the robustness of machine learning classifiers. We show that our approximate computing implementation achieves robustness across a wide range of attack scenarios. Specifically, we show that successful adversarial attacks against the exact classifier have poor transferability to the approximate implementation. The transferability is even poorer for the black-box attack scenarios, where adversarial attacks are generated using a proxy model. Surprisingly, the robustness advantages also apply to white-box attacks where the attacker has unrestricted access to the approximate classifier implementation: in this case, we show that substantially higher levels of adversarial noise are needed to produce adversarial examples. Furthermore, our approximate computing model maintains the same level in terms of classification accuracy, does not require retraining, and reduces resource utilization and energy consumption of the CNN. We conducted extensive experiments on a set of strong adversarial attacks; We empirically show that the proposed implementation increases the robustness of a LeNet-5 and an Alexnet CNNs by up to 99% and 87%, respectively for strong transferability-based attacks along with up to 50% saving in energy consumption due to the simpler nature of the approximate logic. We also show that a white-box attack requires a remarkably higher noise budget to fool the approximate classifier, causing an average of 4 dB degradation of the PSNR of the input image relative to the images that succeed in fooling the exact classifier.",
        "link": "https://dl.acm.org/doi/10.1145/3445814.3446747",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "FlyNet: a platform to support scientific workflows from the edge to the core for UAV applications",
        "authors": "['Eric Lyons', 'Hakan Saplakoglu', 'Michael Zink', 'Komal Thareja', 'Anirban Mandal', 'Chengyi Qu', 'Songjie Wang', 'Prasad Calyam', 'George Papadimitriou', 'Ryan Tanaka', 'Ewa Deelman']",
        "date": "December 2021",
        "source": "UCC '21: Proceedings of the 14th IEEE/ACM International Conference on Utility and Cloud Computing",
        "abstract": "Many Internet of Things (IoT) applications require compute resources that cannot be provided by the devices themselves. At the same time, processing of the data generated by IoT devices often has to be performed in real- or near real-time. Examples of such scenarios are autonomous vehicles in the form of cars and drones where the processing of observational data (e.g., video feeds) needs to be performed expeditiously to allow for safe operation. To support the computational needs and timeliness requirements of such applications it is essential to include suitable edge resources to execute these applications. In this paper, we present our FlyNet architecture which has the goal to provide a new platform to support workflows that include applications executing at the network edge, at the computing core, and leverage deeply programmable networks. We discuss the challenges associated with provisioning such networking and compute infrastructure on demand, tailored to IoT application workflows. We describe a strategy to leverage the end-to-end integrated infrastructure that covers all points in the spectrum of response latency for application processing. We present our prototype implementation of the architecture and evaluate its performance for the case of drone video analytics workflows with varying computational requirements.",
        "link": "https://dl.acm.org/doi/10.1145/3468737.3494098",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Fast ReRoute on Programmable Switches",
        "authors": "['Marco Chiesa', 'Roshan Sedar', 'Gianni Antichi', 'Michael Borokhovich', 'Andrzej Kamisiński', 'Georgios Nikolaidis', 'Stefan Schmid']",
        "date": "None",
        "source": "IEEE/ACM Transactions on Networking",
        "abstract": "Highly dependable communication networks usually rely on some kind of Fast Re-Route (FRR) mechanism which allows to quickly re-route traffic upon failures, entirely in the data plane. This paper studies the design of FRR mechanisms for emerging reconfigurable switches. Our main contribution is an FRR primitive for <italic>programmable</italic> data planes, PURR, which provides low failover latency and high switch throughput, by <italic>avoiding packet recirculation</italic>. PURR tolerates multiple concurrent failures and comes with minimal memory requirements, ensuring <italic>compact</italic> forwarding tables, by unveiling an intriguing connection to classic &#x201C;string theory&#x201D; (<italic>i.e.</italic>, stringology), and in particular, the shortest common supersequence problem. PURR is well-suited for high-speed match-action forwarding architectures (<italic>e.g.</italic>, PISA) and supports the implementation of a broad variety of FRR mechanisms. Our simulations and prototype implementation (on an FPGA and a Tofino switch) show that PURR improves TCAM memory occupancy by a factor of <inline-formula> <tex-math notation=\"LaTeX\">$1.5 \\times$ </tex-math></inline-formula>&#x2013;<inline-formula> <tex-math notation=\"LaTeX\">$10.8 \\times$ </tex-math></inline-formula> compared to a na&#x00EF;ve encoding when implementing state-of-the-art FRR mechanisms. PURR also improves the latency and throughput of datacenter traffic up to a factor of <inline-formula> <tex-math notation=\"LaTeX\">$2.8 \\times$ </tex-math></inline-formula>&#x2013;<inline-formula> <tex-math notation=\"LaTeX\">$5.5 \\times$ </tex-math></inline-formula> and <inline-formula> <tex-math notation=\"LaTeX\">$1.2 \\times$ </tex-math></inline-formula>&#x2013;<inline-formula> <tex-math notation=\"LaTeX\">$2 \\times$ </tex-math></inline-formula>, respectively, compared to approaches based on recirculating packets.",
        "link": "https://dl.acm.org/doi/10.1109/TNET.2020.3045293",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Design Exploration and Scalability Analysis of a CMOS-Integrated, Polymorphic, Nanophotonic Arithmetic-Logic Unit",
        "authors": "['Venkata Sai Praneeth Karempudi', 'Shreyan Datta', 'Ishan G Thakkar']",
        "date": "November 2021",
        "source": "SenSys '21: Proceedings of the 19th ACM Conference on Embedded Networked Sensor Systems",
        "abstract": "Over the past two decades, the clock speed, and hence, the singlecore performance of microprocessors has already stagnated. Following this, the recent faltering of Moore's law due to the CMOS fabrication technology reaching its unavoidable physical limit has presaged daunting challenges for designing power-efficient and ultrafast microprocessors. To overcome these challenges, vigorous efforts have been made to develop new more-than-Moore technologies and architectures for computing. Among these, nanophotonic integrated circuits based computing architectures have shown revolutionary potential. Among recent demonstrations of nanophotonic circuits for computing, a polymorphic, nanophotonic ALU (PoN-ALU) carries a notable importance since it has shown very high flexibility, high speed, and low power consumption for computing. In this paper, we carry out a design space exploration of this PoN-ALU to derive new design guidelines that can help scale the speed and energy efficiency of PoNALU even further.",
        "link": "https://dl.acm.org/doi/10.1145/3485730.3494042",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Energy-aware scheduling of multi-version tasks on heterogeneous real-time systems",
        "authors": "['Julius Roeder', 'Benjamin Rouxel', 'Sebastian Altmeyer', 'Clemens Grelck']",
        "date": "March 2021",
        "source": "SAC '21: Proceedings of the 36th Annual ACM Symposium on Applied Computing",
        "abstract": "The emergence of battery-powered devices has led to an increase of interest in the energy consumption of computing devices. For embedded systems, dispatching the workload on different computing units enables the optimisation of the overall energy consumption on high-performance heterogeneous platforms. However, to use the full power of heterogeneity, architecture specific binary blocks are required, each with different energy/time trade-offs. Finding a scheduling strategy that minimises the energy consumption, while guaranteeing timing constraints creates new challenges. These challenges can only be met by using the full heterogeneous capacity of the platform (e.g. heterogeneous CPU, GPU, DVFS, dynamic frequency changes from within an application). We propose an off-line scheduling algorithm for dependent multiversion tasks based on Forward List Scheduling to minimise the overall energy consumption. Our heuristic accounts for Dynamic Voltage and Frequency Scaling (DVFS) and enables applications to dynamically adapt voltage and frequency during run time. We demonstrate the benefits of multi-version task models coupled with an energy-aware scheduler. We observe that selecting the most energy efficient version for each task does not lead to the lowest energy consumption for the whole application. Then we show that our approach produces schedules that are on average 45.6% more energy efficient than schedules produced by a state-of-the-art scheduling algorithm. Next we compare our heuristic against an optimal solution derived by an Integer Linear Programming (ILP) formulation (deviation of 1.6% on average). Lastly, we empirically show that the energy consumption predicted by our scheduler is close to the actual measured energy consumption on a Odroid-XU4 board (at most-15.8%).",
        "link": "https://dl.acm.org/doi/10.1145/3412841.3441930",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Automated Orchestration of Online Educational Collaboration in Cloud-based Environments",
        "authors": "['Łukasz Czekierda', 'Krzysztof Zieliński', 'Sławomir Zieliński']",
        "date": "None",
        "source": "ACM Transactions on Multimedia Computing, Communications, and Applications",
        "abstract": "Integrated collaboration environments (ICEs) are widely used by corporations to increase productivity by fostering groupwide and interpersonal collaboration. In this article, we discuss the enhancements of such environment needed to build an educational ICE (E-ICE) that addresses the specific needs of educational users. The motivation for the research was the Małopolska Educational Cloud (MEC) project conducted by AGH University and its partners.The E-ICE developed by MEC project fosters collaboration between universities and high schools by creating an immersive virtual collaboration space. MEC is a unique project due to its scale and usage domain. Multiple online collaboration events are organized weekly between over 150 geographically scattered institutions. Such events, aside from videoconferencing, require various services. The MEC E-ICE is a complex composition of a significant number of services and various terminals that require very specific configuration and management.In this article, we focus on a model-driven approach to automating the organization of online meetings in their preparation, execution, and conclusion phases. We present a conceptual model of E-ICE-supported educational courses, introduce a taxonomy of online educational services, identify planes and modes of their operation, as well as discuss the most common collaboration patterns.The MEC E-ICE, which we present as a case study, is built in accordance with the presented, model-driven approach. MEC educational services are described in a way that allows for converting the declarative specification of E-ICE application models into platform-independent models, platform-specific models, and, finally, working sets of orchestrated service instances. Such approach both reduces the level of technical knowledge required from the end-users and considerably speeds up the construction of online educational collaboration environments.",
        "link": "https://dl.acm.org/doi/10.1145/3412381",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Localizing Acoustic Objects on a Single Phone",
        "authors": "['Hongzi Zhu', 'Yuxiao Zhang', 'Zifan Liu', 'Xiao Wang', 'Shan Chang', 'Yingying Chen']",
        "date": "None",
        "source": "IEEE/ACM Transactions on Networking",
        "abstract": "Finding a small object (e.g., earbuds, keys or a wallet) in an indoor environment (e.g., in a house or an office) can be frustrating. In this paper, we propose an innovative system, called <italic>HyperEar</italic>, to localize such an object using only a single smartphone, based on enhanced time-difference-of-arrival (TDoA) measurements over acoustic signals issued from the object. One major challenge is the hardware limitations of a Commercial-Off-The-Shelf (COTS) phone with a short separation between the two microphones and the low sampling rate of such microphones. HyperEar enhances the accuracy of TDoA measurements by virtually increasing distances between microphones through sliding the phone in the air. HyperEar requires no communication for synchronization between the phone and the object and is a low-cost and easy-to-use system. We evaluate the performance of HyperEar via extensive experiments in various indoor conditions and the results demonstrate that, for an object of 7 m away, HyperEar can achieve a mean localization accuracy of about 15 cm when the object in normal indoor environments.",
        "link": "https://dl.acm.org/doi/10.1109/TNET.2021.3080820",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "VIoLET: An Emulation Environment for Validating IoT Deployments at Large Scales",
        "authors": "['Shrey Baheti', 'Shreyas Badiger', 'Yogesh Simmhan']",
        "date": "None",
        "source": "ACM Transactions on Cyber-Physical Systems",
        "abstract": "Internet of Things (IoT) deployments have been growing manifold, encompassing sensors, networks, edge, fog, and cloud resources. Despite the intense interest from researchers and practitioners, most do not have access to large-scale IoT testbeds for validation. Simulation environments that allow analytical modeling are a poor substitute for evaluating software platforms or application workloads in realistic computing environments. Here, we propose a virtual environment for validating Internet of Things at large scales (VIoLET), an emulator for defining and launching large-scale IoT deployments within cloud VMs. It allows users to declaratively specify container-based compute resources that match the performance of native IoT compute devices using Docker. These can be inter-connected by complex topologies on which bandwidth and latency rules are enforced. Users can configure synthetic sensors for data generation as well. We also incorporate models for CPU resource dynamism, and for failure and recovery of the underlying devices. We offer a detailed comparison of VIoLET’s compute and network performance between the virtual and physical deployments, evaluate its scaling with deployments with up to 1, 000 devices and 4, 000 device-cores, and validate its ability to model resource dynamism. Our extensive experiments show that the performance of the virtual IoT environment accurately matches the expected behavior, with deviations levels within what is seen in actual physical devices. It also scales to 1, 000s of devices and at a modest cloud computing costs of under 0.15% of the actual hardware cost, per hour of use, with minimal management effort. This IoT emulation environment fills an essential gap between IoT simulators and real deployments.",
        "link": "https://dl.acm.org/doi/10.1145/3446346",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Astraea: towards QoS-aware and resource-efficient multi-stage GPU services",
        "authors": "['Wei Zhang', 'Quan Chen', 'Kaihua Fu', 'Ningxin Zheng', 'Zhiyi Huang', 'Jingwen Leng', 'Minyi Guo']",
        "date": "February 2022",
        "source": "ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "Multi-stage user-facing applications on GPUs are widely-used nowa- days, and are often implemented to be microservices. Prior re- search works are not applicable to ensuring QoS of GPU-based microservices due to the different communication patterns and shared resource contentions. We propose Astraea to manage GPU microservices considering the above factors. In Astraea, a microser- vice deployment policy is used to maximize the supported peak service load while ensuring the required QoS. To adaptively switch the communication methods between microservices according to different deployments, we propose an auto-scaling GPU communi- cation framework. The framework automatically scales based on the currently used hardware topology and microservice location, and adopts global memory-based techniques to reduce intra-GPU communication. Astraea increases the supported peak load by up to 82.3% while achieving the desired 99%-ile latency target compared with state-of-the-art solutions.",
        "link": "https://dl.acm.org/doi/10.1145/3503222.3507721",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Paulihedral: a generalized block-wise compiler optimization framework for Quantum simulation kernels",
        "authors": "['Gushu Li', 'Anbang Wu', 'Yunong Shi', 'Ali Javadi-Abhari', 'Yufei Ding', 'Yuan Xie']",
        "date": "February 2022",
        "source": "ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "The quantum simulation kernel is an important subroutine appearing as a very long gate sequence in many quantum programs. In this paper, we propose Paulihedral, a block-wise compiler framework that can deeply optimize this subroutine by exploiting high-level program structure and optimization opportunities. Paulihedral first employs a new Pauli intermediate representation that can maintain the high-level semantics and constraints in quantum simulation kernels. This naturally enables new large-scale optimizations that are hard to implement at the low gate-level. In particular, we propose two technology-independent instruction scheduling passes, and two technology-dependent code optimization passes which reconcile the circuit synthesis, gate cancellation, and qubit mapping stages of the compiler. Experimental results show that Paulihedral can outperform state-of-the-art compiler infrastructures in a wide-range of applications on both near-term superconducting quantum processors and future fault-tolerant quantum computers.",
        "link": "https://dl.acm.org/doi/10.1145/3503222.3507715",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "ALPINE: An Agile Processing-in-Memory Macro Compilation Framework",
        "authors": "['Jinshan Zhang', 'Bo Jiao', 'Yunzhengmao Wang', 'Haozhe Zhu', 'Lihua Zhang', 'Chixiao Chen']",
        "date": "June 2021",
        "source": "GLSVLSI '21: Proceedings of the 2021 on Great Lakes Symposium on VLSI",
        "abstract": "Processing-in-Memory architectures and circuit designs are playing significant roles in the recent energy-efficient machine learning chips. This paper proposes a PIM macro compilation framework called ALPINE to speed up previously tedious and error-prone PIM design flow, paving the way towards open-source and process-portable PIM chips. Relying on an extensible PIM standard cell library, ALPINE can generate the corresponding topology according to the specification, and process placement and routing. The proposed PIM macro is compatible with different storage devices such as SRAM and RRAM, and can support various quantization bit-widths and dataflows. To verify the effectiveness, a 128×128 SRAM-based PIM macro instance is implemented, and the simulation results show that it can achieve an energy efficiency of 19.05TOPS/W under 65nm CMOS technology. The macro performance is not inferior to the state-of-the-art custom PIM designs.",
        "link": "https://dl.acm.org/doi/10.1145/3453688.3461532",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Betalogger: Smartphone Sensor-based Side-channel Attack Detection and Text Inference Using Language Modeling and Dense MultiLayer Neural Network",
        "authors": "['Abdul Rehman Javed', 'Saif Ur Rehman', 'Mohib Ullah Khan', 'Mamoun Alazab', 'Habib Ullah Khan']",
        "date": "None",
        "source": "ACM Transactions on Asian and Low-Resource Language Information Processing",
        "abstract": "With the recent advancement of smartphone technology in the past few years, smartphone usage has increased on a tremendous scale due to its portability and ability to perform many daily life tasks. As a result, smartphones have become one of the most valuable targets for hackers to perform cyberattacks, since the smartphone can contain individuals’ sensitive data. Smartphones are embedded with highly accurate sensors. This article proposes BetaLogger, an Android-based application that highlights the issue of leaking smartphone users’ privacy using smartphone hardware sensors (accelerometer, magnetometer, and gyroscope). BetaLogger efficiently infers the typed text (long or short) on a smartphone keyboard using Language Modeling and a Dense Multi-layer Neural Network (DMNN). BetaLogger is composed of two major phases: In the first phase, Text Inference Vector is given as input to the DMNN model to predict the target labels comprising the alphabet, and in the second phase, sequence generator module generate the output sequence in the shape of a continuous sentence. The outcomes demonstrate that BetaLogger generates highly accurate short and long sentences, and it effectively enhances the inference rate in comparison with conventional machine learning algorithms and state-of-the-art studies.",
        "link": "https://dl.acm.org/doi/10.1145/3460392",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Formal verification of high-level synthesis",
        "authors": "['Yann Herklotz', 'James D. Pollard', 'Nadesh Ramanathan', 'John Wickerson']",
        "date": "October 2021",
        "source": "Proceedings of the ACM on Programming Languages",
        "abstract": "High-level synthesis (HLS), which refers to the automatic compilation of software into hardware, is rapidly gaining popularity. In a world increasingly reliant on application-specific hardware accelerators, HLS promises hardware designs of comparable performance and energy efficiency to those coded by hand in a hardware description language such as Verilog, while maintaining the convenience and the rich ecosystem of software development. However, current HLS tools cannot always guarantee that the hardware designs they produce are equivalent to the software they were given, thus undermining any reasoning conducted at the software level. Furthermore, there is mounting evidence that existing HLS tools are quite unreliable, sometimes generating wrong hardware or crashing when given valid inputs. To address this problem, we present the first HLS tool that is mechanically verified to preserve the behaviour of its input software. Our tool, called Vericert, extends the CompCert verified C compiler with a new hardware-oriented intermediate language and a Verilog back end, and has been proven correct in Coq. Vericert supports most C constructs, including all integer operations, function calls, local arrays, structs, unions, and general control-flow statements. An evaluation on the PolyBench/C benchmark suite indicates that Vericert generates hardware that is around an order of magnitude slower (only around 2× slower in the absence of division) and about the same size as hardware generated by an existing, optimising (but unverified) HLS tool.",
        "link": "https://dl.acm.org/doi/10.1145/3485494",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Power Side-Channel Analysis of RNS GLV ECC Using Machine and Deep Learning Algorithms",
        "authors": "['Mohamad Ali Mehrabi', 'Naila Mukhtar', 'Alireza Jolfaei']",
        "date": "None",
        "source": "ACM Transactions on Internet Technology",
        "abstract": "Many Internet of Things applications in smart cities use elliptic-curve cryptosystems due to their efficiency compared to other well-known public-key cryptosystems such as RSA. One of the important components of an elliptic-curve-based cryptosystem is the elliptic-curve point multiplication which has been shown to be vulnerable to various types of side-channel attacks. Recently, substantial progress has been made in applying deep learning to side-channel attacks. Conceptually, the idea is to monitor a core while it is running encryption for information leakage of a certain kind, for example, power consumption. The knowledge of the underlying encryption algorithm can be used to train a model to recognise the key used for encryption. The model is then applied to traces gathered from the crypto core in order to recover the encryption key. In this article, we propose an RNS GLV elliptic curve cryptography core which is immune to machine learning and deep learning based side-channel attacks. The experimental analysis confirms the proposed crypto core does not leak any information about the private key and therefore it is suitable for hardware implementations.",
        "link": "https://dl.acm.org/doi/10.1145/3423555",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Increasing the Channel Capacity: Parallel Data Transmission in a Testbed for Molecular Communication",
        "authors": "['Max Bartunik', 'Matthias Streb', 'Harald Unterweger', 'Jakob Haller', 'Jens Kirchner']",
        "date": "September 2021",
        "source": "NANOCOM '21: Proceedings of the Eight Annual ACM International Conference on Nanoscale Computing and Communication",
        "abstract": "In the field of molecular communication, molecules or other particles in the nanoscale are used to transmit information. This rather new communication paradigm has a large application potential, ranging from medicine to industrial systems. As testbeds are essential for evaluating the capabilities of this new form of communication, we improve an existing water-based testbed to achieve parallel data transmission within one communication channel in this work. To this end, two different information carriers, superparamagnetic iron oxide nanoparticles and fluorescent particles, are used to simultaneously transmit independent data streams. A detection system for fluorescent particles was specifically developed for the fluid molecular communication testbed. Using parallel data transmission in the modified testbed the channel capacity was nearly doubled in comparison to a transmission using only a single type of information particles.",
        "link": "https://dl.acm.org/doi/10.1145/3477206.3477449",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Paging and the Address-Translation Problem",
        "authors": "['Michael A. Bender', 'Abhishek Bhattacharjee', 'Alex Conway', 'Martín Farach-Colton', 'Rob Johnson', 'Sudarsun Kannan', 'William Kuszmaul', 'Nirjhar Mukherjee', 'Don Porter', 'Guido Tagliavini', 'Janet Vorobyeva', 'Evan West']",
        "date": "July 2021",
        "source": "SPAA '21: Proceedings of the 33rd ACM Symposium on Parallelism in Algorithms and Architectures",
        "abstract": "The classical paging problem, introduced by Sleator and Tarjan in 1985, formalizes the problem of caching pages in RAM in order to minimize IOs. Their online formulation ignores the cost of address translation: programs refer to data via virtual addresses, and these must be translated into physical locations in RAM. Although the cost of an individual address translation is much smaller than that of an IO, every memory access involves an address translation, whereas IOs can be infrequent. In practice, one can spend money to avoid paging by over-provisioning RAM; in contrast, address translation is effectively unavoidable. Thus address-translation costs can sometimes dominate paging costs, and systems must simultaneously optimize both.   To mitigate the cost of address translation, all modern CPUs have translation lookaside buffers (TLBs), which are hardware caches of common address translations. What makes TLBs interesting is that a single TLB entry can potentially encode the address translation for many addresses. This is typically achieved via the use of huge pages, which translate runs of contiguous virtual addresses to runs of contiguous physical addresses. Huge pages reduce TLB misses at the cost of increasing the IOs needed to maintain contiguity in RAM. This tradeoff between TLB misses and IOs suggests that the classical paging problem does not tell the full story.   This paper introduces the Address-Translation Problem, which formalizes the problem of maintaining a TLB, a page table, and RAM in order to minimize the total cost of both TLB misses and IOs. We present an algorithm that achieves the benefits of huge pages for TLB misses without the downsides of huge pages for IOs.",
        "link": "https://dl.acm.org/doi/10.1145/3409964.3461814",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "ANT-UNet: Accurate and Noise-Tolerant Segmentation for Pathology Image Processing",
        "authors": "['Yufei Chen', 'Tingtao Li', 'Qinming Zhang', 'Wei Mao', 'Nan Guan', 'Mei Tian', 'Hao Yu', 'Cheng Zhuo']",
        "date": "None",
        "source": "ACM Journal on Emerging Technologies in Computing Systems",
        "abstract": "Pathology image segmentation is an essential step in early detection and diagnosis for various diseases. Due to its complex nature, precise segmentation is not a trivial task. Recently, deep learning has been proved as an effective option for pathology image processing. However, its efficiency is highly restricted by inconsistent annotation quality. In this article, we propose an accurate and noise-tolerant segmentation approach to overcome the aforementioned issues. This approach consists of two main parts: a preprocessing module for data augmentation and a new neural network architecture, ANT-UNet. Experimental results demonstrate that, even on a noisy dataset, the proposed approach can achieve more accurate segmentation with 6% to 35% accuracy improvement versus other commonly used segmentation methods. In addition, the proposed architecture is hardware friendly, which can reduce the amount of parameters to one-tenth of the original and achieve 1.7× speed-up.",
        "link": "https://dl.acm.org/doi/10.1145/3451213",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "T-Cache: Efficient Policy-Based Forwarding Using Small TCAM",
        "authors": "['Ying Wan', 'Haoyu Song', 'Yang Xu', 'Yilun Wang', 'Tian Pan', 'Chuwen Zhang', 'Yi Wang', 'Bin Liu']",
        "date": "None",
        "source": "IEEE/ACM Transactions on Networking",
        "abstract": "Ternary Content Addressable Memory (TCAM) is widely used by modern routers and switches to support policy-based forwarding due to its incomparable lookup speed and flexible matching patterns. However, the limited TCAM capacity does not scale with the ever-increasing rule table size due to the high hardware cost and high power consumption. At present, using TCAM just as a rule cache is an appealing solution, but one must resolve several tricky issues including the rule dependency and the associated TCAM updates. In this paper, we propose a new approach which can generate dependency-free rules to cache. By removing the rule dependency, the complex TCAM update problem also disappears. We provide the complete T-cache system design including slow path processing and cache replacement, and implement a T-cache prototype on Barefoot Tofino switches. We conduct comprehensive software simulations and hardware experiments based on real-world and synthesized rule tables and packet traces to show that T-cache is efficient and robust for network traffic in various scenarios.",
        "link": "https://dl.acm.org/doi/10.1109/TNET.2021.3098320",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Ghost Thread: Effective User-Space Cache Side Channel Protection",
        "authors": "['Robert Brotzman', 'Danfeng Zhang', 'Mahmut Kandemir', 'Gang Tan']",
        "date": "April 2021",
        "source": "CODASPY '21: Proceedings of the Eleventh ACM Conference on Data and Application Security and Privacy",
        "abstract": "Cache-based side channel attacks pose a serious threat to computer security. Numerous cache attacks have been demonstrated, highlighting the need for effective and efficient defense mechanisms to shield systems from this threat. In this paper, we propose a novel application-level protection mechanism, called Ghost Thread. Ghost Thread is a flexible library that allows a user to protect cache accesses to a requested sensitive region to mitigate cache-based side channel attacks. This is accomplished by injecting random cache accesses to the sensitive cache region by separate threads. Compared with prior work that injects noise in a modified OS and hardware, our novel approach is applicable to commodity OS and hardware. Compared with other user-space mitigation mechanisms, our novel approach does not require any special hardware support, and it only requires slight code changes in the protected application making it readily deployable. Evaluation results on an Apache server show that Ghost Thread provides both strong protection and negligible overhead on real-world applications where only a fragment requires protection. In the worst-case scenario where the entire application requires protection, Ghost Thread still incurs negligible overhead when a system is under utilized, and moderate overhead when a system is fully utilized.",
        "link": "https://dl.acm.org/doi/10.1145/3422337.3447846",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "NASCENT2: Generic Near-Storage Sort Accelerator for Data Analytics on SmartSSD",
        "authors": "['Sahand Salamat', 'Hui Zhang', 'Yang Seok Ki', 'Tajana Rosing']",
        "date": "None",
        "source": "ACM Transactions on Reconfigurable Technology and Systems",
        "abstract": "As the size of data generated every day grows dramatically, the computational bottleneck of computer systems has shifted toward storage devices. The interface between the storage and the computational platforms has become the main limitation due to its limited bandwidth, which does not scale when the number of storage devices increases. Interconnect networks do not provide simultaneous access to all storage devices and thus limit the performance of the system when executing independent operations on different storage devices. Offloading the computations to the storage devices eliminates the burden of data transfer from the interconnects. Near-storage computing offloads a portion of computations to the storage devices to accelerate big data applications. In this article, we propose a generic near-storage sort accelerator for data analytics, NASCENT2, which utilizes Samsung SmartSSD, an NVMe flash drive with an on-board FPGA chip that processes data in situ.NASCENT2 consists of dictionary decoder, sort, and shuffle FPGA-based accelerators to support sorting database tables based on a key column with any arbitrary data type. It exploits data partitioning applied by data processing management systems, such as SparkSQL, to breakdown the sort operations on colossal tables to multiple sort operations on smaller tables. NASCENT2 generic sort provides 2 × speedup and 15.2 × energy efficiency improvement as compared to the CPU baseline. It moreover considers the specifications of the SmartSSD (e.g., the FPGA resources, interconnect network, and solid-state drive bandwidth) to increase the scalability of computer systems as the number of storage devices increases. With 12 SmartSSDs, NASCENT2 is 9.9× (137.2 ×) faster and 7.3 × (119.2 ×) more energy efficient in sorting the largest tables of TPCC and TPCH benchmarks than the FPGA (CPU) baseline.",
        "link": "https://dl.acm.org/doi/10.1145/3472769",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Randomized row-swap: mitigating Row Hammer by breaking spatial correlation between aggressor and victim rows",
        "authors": "['Gururaj Saileshwar', 'Bolin Wang', 'Moinuddin Qureshi', 'Prashant J. Nair']",
        "date": "February 2022",
        "source": "ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "Row Hammer is a fault-injection attack in which rapid activations to a single DRAM row causes bit-flips in nearby rows. Several recent defenses propose tracking aggressor-rows and applying mitigating action on neighboring victim rows by refreshing them. However, all such proposals using victim-focused mitigation preserve the spatial connection between victim and aggressor rows. Therefore, these proposals are susceptible to access patterns causing bit-flips in rows beyond the immediate neighbor. For example, the Half-Double attack causes bit-flips in the presence of victim-focused mitigation.   We propose Randomized Row-Swap (RRS), a novel mitigation action that breaks the spatial connection between the aggressor and victim DRAM rows. This enables RRS to provide robust defense against even complex Row Hammer access patterns. RRS is an aggressor-focused mitigation that periodically swaps aggressor-rows with other randomly selected rows in memory. This limits the possible damage in any one locality within the DRAM memory. While RRS can be used with any tracking mechanism, we implement it with a Misra-Gries tracker and target a Row Hammer Threshold of 4.8K activations (similar to the state-of-the-art attacks). Our evaluations show that RRS has negligible slowdown (0.4% on average) and provides strong security guarantees for avoiding Row Hammer bit flips even under several years of continuous attack.",
        "link": "https://dl.acm.org/doi/10.1145/3503222.3507716",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Discrete samplers for approximate inference in probabilistic machine learning",
        "authors": "['Shirui Zhao', 'Nimish Shah', 'Wannes Meert', 'Marian Verhelst']",
        "date": "March 2022",
        "source": "DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe",
        "abstract": "Probabilistic reasoning models (PMs) and probabilistic inference bring advantages when dealing with small datasets or uncertainty on the observed data, and allow to integrate expert knowledge and create interpretable models. The main challenge of using these PMs in practice is that their inference is very compute-intensive. Therefore, custom hardware architectures for the exact and approximate inference of PMs have been proposed in the SotA. The throughput, energy and area efficiency of approximate PM inference accelerators are strongly dominated by the sampler blocks required to sample arbitrary discrete distributions. This paper proposes and studies novel discrete sampler architectures towards efficient and flexible hardware implementations for PM accelerators. Both cumulative distribution table (CDT) and Knuth-Yao (KY) based sampling algorithms are assessed, based on which different sampler hardware architectures were implemented. Innovation is brought in terms of a reconfigurable CDT sampling architecture with a flexible range and a reconfigurable Knuth-Yao sampling architecture that supports both flexible range and dynamic precision. All architectures are benchmarked on real-world Bayesian Networks, demonstrating up to 13× energy efficiency benefits and 11× area efficiency improvement of the optimized reconfigurable Knuth-Yao sampler over the traditional linear CDT-based samplers used in the PM SotA.",
        "link": "https://dl.acm.org/doi/10.5555/3539845.3540135",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "STM-multifrontal QR: streaming task mapping multifrontal QR factorization empowered by GCN",
        "authors": "['Shengle Lin', 'Wangdong Yang', 'Haotian Wang', 'Qinyun Tsai', 'Kenli Li']",
        "date": "November 2021",
        "source": "SC '21: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",
        "abstract": "Multifrontal QR algorithm, which consists of symbolic analysis and numerical factorization, is a high-performance algorithm for orthogonal factorizing sparse matrix. In this work, a graph convolutional network (GCN) for adaptively selecting the optimal reordering algorithm is proposed in symbolic analysis. Using our GCN adaptive classifier, the average numerical factorization time is reduced by 20.78% compared with the default approach, and the additional memory overhead is approximately 4% higher than that of prior work. Moreover, for numerical factorization, an optimized tasks stream parallel processing strategy is proposed and a more efficient computing task mapping framework for NUMA architecture is adopted in this paper, which called STM-Multifrontal QR factorization. Numerical experiments on the TaiShan Server show average 1.22x performance gains over the original SuiteSparseQR. Nearly 80% of datasets have achieved better performance compared with the MKL sparse QR on Intel Xeon 6248.",
        "link": "https://dl.acm.org/doi/10.1145/3458817.3476199",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "SGXoMeter: Open and Modular Benchmarking for Intel SGX",
        "authors": "['Mohammad Mahhouk', 'Nico Weichbrodt', 'Rüdiger Kapitza']",
        "date": "April 2021",
        "source": "EuroSec '21: Proceedings of the 14th European Workshop on Systems Security",
        "abstract": "Intel's Software Guard Extensions (SGX) are currently the most wide-spread commodity trusted execution environment, which provides integrity and confidentiality of sensitive code and data. Thereby, it offers protection even against privileged attackers and various forms of physical attacks. As a technology that only became available in late 2015, it has received massive interest and undergone a rapid evolution. Despite first ad-hoc attempts, there is so far no standardised approach to benchmark the SGX hardware, its associated environment, and techniques that were designed to harden SGX-based applications. In this paper, we present SGXoMeter, an open and modular framework designed to benchmark different SGX-aware CPUs, μcode revisions, SDK versions and extensions to mitigate side-channel attacks. SGXoMeter provides a set of practical SGX test case scenarios and eases the development of custom benchmarks. Furthermore, we compare it to sgx-nbench, the only other SGX application benchmark tool we are aware of, and evaluate their differences. Through our benchmark results, we identified a performance overhead of up to ã10 times induced between two different SGX-SDK versions for certain workload scenarios.",
        "link": "https://dl.acm.org/doi/10.1145/3447852.3458722",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "FlowAcc: real-time high-accuracy DNN-based optical flow accelerator in FPGA",
        "authors": "['Yehua Ling', 'Yuanxing Yan', 'Kai Huang', 'Gang Chen']",
        "date": "March 2022",
        "source": "DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe",
        "abstract": "Recently, accelerator architectures have been designed to use deep neural networks (DNNs) to accelerate computer vision tasks, possessing the advantages of both accuracy and speed. Optical flow accelerator is however not among these architectures that DNNs have been successfully deployed. Existing hardware accelerators for optical flow estimation are all designed for classic methods and generally perform poorly in estimated accuracy. In this paper, we present FlowAcc, a dedicated hardware accelerator for DNN-based optical flow estimation, adopting a pipelined hardware design for real-time processing of image streams. We design an efficient multiplexing binary neural network (BNN) architecture for pyramidal feature extraction to significantly reduce the hardware cost and make it independent of the pyramid level number. Furthermore, efficient hamming distance calculation and competent flow regularization are utilized for hierarchical optical flow estimation to greatly improve the system efficiency. Comprehensive experimental results demonstrate that FlowAcc achieves state-of-the-art estimation accuracy and real-time performance on the Middlebury dataset when compared with the existing optical flow accelerators.",
        "link": "https://dl.acm.org/doi/10.5555/3539845.3539880",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "FaaSFlow: enable efficient workflow execution for function-as-a-service",
        "authors": "['Zijun Li', 'Yushi Liu', 'Linsong Guo', 'Quan Chen', 'Jiagan Cheng', 'Wenli Zheng', 'Minyi Guo']",
        "date": "February 2022",
        "source": "ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "Serverless computing (Function-as-a-Service) provides fine-grain resource sharing by running functions (or Lambdas) in containers. Data-dependent functions are required to be invoked following a pre-defined logic, which is known as serverless workflows. However, our investigation shows that the traditional master-worker based workflow execution architecture performs poorly in serverless context. One significant overhead results from the master-side workflow schedule pattern, with which the functions are triggered in the master node and assigned to worker nodes for execution. Besides, the data movement between workers also reduces the throughput.  To this end, we present a worker-side workflow schedule pattern for serverless workflow execution. Following the design, we implement FaaSFlow to enable efficient workflow execution in the serverless context. Besides, we propose an adaptive storage library FaaStore that enables fast data transfer between functions on the same node without through the database. Experiment results show that FaaSFlow effectively mitigates the workflow scheduling overhead by 74.6% on average and data transmission overhead by 95% at most. When the network bandwidth fluctuates, FaaSFlow-FaaStore reduces the throughput degradation by 23.0%, and is able to multiply the utilization of network bandwidth by 1.5X-4X.",
        "link": "https://dl.acm.org/doi/10.1145/3503222.3507717",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "The Processing-in-Memory Model",
        "authors": "['Hongbo Kang', 'Phillip B. Gibbons', 'Guy E. Blelloch', 'Laxman Dhulipala', 'Yan Gu', 'Charles McGuffey']",
        "date": "July 2021",
        "source": "SPAA '21: Proceedings of the 33rd ACM Symposium on Parallelism in Algorithms and Architectures",
        "abstract": "As computational resources become more efficient and data sizes grow, data movement is fast becoming the dominant cost in computing. Processing-in-Memory is emerging as a key technique for reducing costly data movement, by enabling computation to be executed on compute resources embedded in the memory modules themselves.   This paper presents the Processing-in-Memory (PIM) model, for the design and analysis of parallel algorithms on systems providing processing-in-memory modules. The PIM model focuses on keys aspects of such systems, while abstracting the rest. Namely, the model combines (i) a CPU-side consisting of parallel cores with fast access to a small shared memory of size M words (as in traditional parallel computing), (ii) a PIM-side consisting of P PIM modules, each with a core and a local memory of size Θ(n/P) words for an input of size n (as in traditional distributed computing), and (iii) a network between the two sides. The model combines standard parallel complexity metrics for both shared memory (work and depth) and distributed memory (local work, communication time) computing. A key algorithmic challenge is to achieve load balance among the PIM modules in both their communication and their local work, while minimizing the communication time. We demonstrate how to overcome this challenge for an ordered search structure, presenting a parallel PIM-skiplist data structure that efficiently supports a wide range of batch-parallel queries and updates.",
        "link": "https://dl.acm.org/doi/10.1145/3409964.3461816",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "NOVIA: A Framework for Discovering Non-Conventional Inline Accelerators",
        "authors": "['David Trilla', 'John-David Wellman', 'Alper Buyuktosunoglu', 'Pradip Bose']",
        "date": "October 2021",
        "source": "MICRO '21: MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture",
        "abstract": "Accelerators provide an increasingly valuable source of performance in modern computing systems. In most cases, accelerators are implemented as stand-alone, offload engines to which the processor can send large computation tasks. For many edge devices, as performance needs increase accelerators become essential, but the tight constraints on these devices limit the extent to which offload engines can be incorporated. An alternative is inline accelerators, which can be integrated as part of the core and provide performance with much smaller start-up times and area overheads. While inline accelerators allow greater flexibility in the interface and acceleration of finer grain code, determining good inline candidate accelerators is non-trivial. In this paper, we present NOVIA, a framework to derive inline accelerators by examining the workload source code and identifying inline accelerator candidates that provide benefits across many different regions of the workload. These NOVIA-derived accelerators are then integrated into an embedded core. For this core, NOVIA produces inline accelerators that improve the performance of various benchmark suites like EEMBC Autobench 2.0 and Mediabench by 1.37x with only a 3% core area increase.",
        "link": "https://dl.acm.org/doi/10.1145/3466752.3480094",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Application Threats to Exploit Northbound Interface Vulnerabilities in Software Defined Networks",
        "authors": "['Bilal Rauf', 'Haider Abbas', 'Muhammad Usman', 'Tanveer A. Zia', 'Waseem Iqbal', 'Yawar Abbas', 'Hammad Afzal']",
        "date": "None",
        "source": "ACM Computing Surveys",
        "abstract": "Software Defined Networking (SDN) is an evolving technology that decouples the control functionality from the underlying hardware managed by the control plane. The application plane supports programmers to develop numerous applications (such as networking, management, security, etc.) that can even be executed from remote locations. Northbound interface (NBI) bridges the control and application planes to execute the third-party applications business logic. Due to the software bugs in applications and existing vulnerabilities such as illegal function calling, resource exhaustion, lack of trust, and so on, NBIs are susceptible to different attacks. Based on the extensive literature review, we have identified that the researchers and academia have mainly focused on the security of the control plane, data plane, and southbound interface (SBI). NBI, in comparison, has received far less attention. In this article, the security of the least explored, but a critical component of the SDN architecture, i.e., NBI, is analyzed. The article provides a brief overview of SDN, followed by a detailed discussion on the categories of NBI, vulnerabilities of NBI, and threats posed by malicious applications to NBI. Efforts of the researchers to counter malicious applications and NBI issues are then discussed in detail. The standardization efforts for the single acceptable NBI and security requirements of SDN by Open Networking Foundation (ONF) are also presented. The article concludes with the future research directions for the security of a single acceptable NBI.",
        "link": "https://dl.acm.org/doi/10.1145/3453648",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "DAGguise: mitigating memory timing side channels",
        "authors": "['Peter W. Deutsch', 'Yuheng Yang', 'Thomas Bourgeat', 'Jules Drean', 'Joel S. Emer', 'Mengjia Yan']",
        "date": "February 2022",
        "source": "ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "This paper studies the mitigation of memory timing side channels, where attackers utilize contention within DRAM controllers to infer a victim’s secrets. Already practical, this class of channels poses an important challenge to secure computing in shared memory environments.  Existing state-of-the-art memory timing side channel mitigations have several key performance and security limitations. Prior schemes require onerous static bandwidth partitioning, extensive profiling phases, or simply fail to protect against attacks which exploit fine-grained timing and bank information. We present DAGguise, a defense mechanism which fully protects against memory timing side channels while allowing for dynamic traffic contention in order to achieve good performance. DAGguise utilizes a novel abstract memory access representation, the Directed Acyclic Request Graph (rDAG for short), to model memory access patterns which experience contention. DAGguise shapes a victim’s memory access patterns according to a publicly known rDAG obtained through a lightweight profiling stage, completely eliminating information leakage.  We formally verify the security of DAGguise, proving that it maintains strong security guarantees. Moreover, by allowing dynamic traffic contention, DAGguise achieves a 12% overall system speedup relative to Fixed Service, which is the state-of-the-art mitigation mechanism, with up to a 20% relative speedup for co-located applications which do not require protection. We further claim that the principles of DAGguise can be generalized to protect against other types of scheduler-based timing side channels, such as those targeting on-chip networks, or functional units in SMT cores.",
        "link": "https://dl.acm.org/doi/10.1145/3503222.3507747",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Network intelligence in 6G: challenges and opportunities",
        "authors": "['Albert Banchs', 'Marco Fiore', 'Andres Garcia-Saavedra', 'Marco Gramaglia']",
        "date": "October 2021",
        "source": "MobiArch '21: Proceedings of the 16th ACM Workshop on Mobility in the Evolving Internet Architecture",
        "abstract": "The success of the upcoming 6G systems will largely depend on the quality of the Network Intelligence (NI) that will fully automate network management. Artificial Intelligence (AI) models are commonly regarded as the cornerstone for NI design, as they have proven extremely successful at solving hard problems that require inferring complex relationships from entangled, massive (network traffic) data. However, the common approach of plugging ‘vanilla’ AI models into controllers and orchestrators does not fulfil the potential of the technology. Instead, AI models should be tailored to the specific network level and respond to the specific needs of network functions, eventually coordinated by an end-to-end NI-native architecture for 6G. In this paper, we discuss these challenges and provide results for a candidate NI-driven functionality that is properly integrated into the proposed architecture: network capacity forecasting.",
        "link": "https://dl.acm.org/doi/10.1145/3477091.3482761",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Predictive auto-scaling with OpenStack Monasca",
        "authors": "['Giacomo Lanciano', 'Filippo Galli', 'Tommaso Cucinotta', 'Davide Bacciu', 'Andrea Passarella']",
        "date": "December 2021",
        "source": "UCC '21: Proceedings of the 14th IEEE/ACM International Conference on Utility and Cloud Computing",
        "abstract": "Cloud auto-scaling mechanisms are typically based on reactive automation rules that scale a cluster whenever some metric, e.g., the average CPU usage among instances, exceeds a predefined threshold. Tuning these rules becomes particularly cumbersome when scaling-up a cluster involves non-negligible times to bootstrap new instances, as it happens frequently in production cloud services. To deal with this problem, we propose an architecture for auto-scaling cloud services based on the status in which the system is expected to evolve in the near future. Our approach leverages on time-series forecasting techniques, like those based on machine learning and artificial neural networks, to predict the future dynamics of key metrics, e.g., resource consumption metrics, and apply a threshold-based scaling policy on them. The result is a predictive automation policy that is able, for instance, to automatically anticipate peaks in the load of a cloud application and trigger ahead of time appropriate scaling actions to accommodate the expected increase in traffic. We prototyped our approach as an open-source OpenStack component, which relies on, and extends, the monitoring capabilities offered by Monasca, resulting in the addition of predictive metrics that can be leveraged by orchestration components like Heat or Senlin. We show experimental results using a recurrent neural network and a multi-layer perceptron as predictor, which are compared with a simple linear regression and a traditional non-predictive auto-scaling policy. However, the proposed framework allows for the easy customization of the prediction policy as needed.",
        "link": "https://dl.acm.org/doi/10.1145/3468737.3494104",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Power and energy efficient routing for Mach-Zehnder interferometer based photonic switches",
        "authors": "['Markos Kynigos', 'Jose A. Pascual', 'Javier Navaridas', 'John Goodacre', 'Mikel Lujan']",
        "date": "June 2021",
        "source": "ICS '21: Proceedings of the ACM International Conference on Supercomputing",
        "abstract": "Silicon Photonic top-of-rack (ToR) switches are highly desirable for the datacenter (DC) and high-performance computing (HPC) domains for their potential high-bandwidth and energy efficiency. Recently, photonic Beneš switching fabrics based on Mach-Zehnder Interferometers (MZIs) have been proposed as a promising candidate for the internals of high-performance switches. However, state-of-the-art routing algorithms that control these switching fabrics are either computationally complex or unable to provide non-blocking, energy efficient routing permutations.To address this, we propose for the first time a combination of energy efficient routing algorithms and time-division multiplexing (TDM). We evaluate this approach by conducting a simulation-based performance evaluation of a 16x16 Beneš fabric, deployed as a ToR switch, when handling a set of 8 representative workloads from the DC and HPC domains. Our results show that state-of-the-art approaches (circuit switched energy efficient routing algorithms) introduce up to 23% contention in the switching fabric for some workloads, thereby increasing communication time. We show that augmenting the algorithms with TDM can ameliorate switch fabric contention by segmenting communication data and gracefully interleaving the segments, thus reducing communication time by up to 20% in the best case. We also discuss the impact of the TDM segment size, finding that although a 10KB segment size is the most beneficial in reducing communication time, a 100KB segment size offers similar performance while requiring a less stringent path-computation time window. Finally, we assess the impact of TDM on path-dependent insertion loss and switching energy consumption, finding it to be minimal in all cases.",
        "link": "https://dl.acm.org/doi/10.1145/3447818.3460363",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Software-Defined Vector Processing on Manycore Fabrics",
        "authors": "['Philip Bedoukian', 'Neil Adit', 'Edwin Peguero', 'Adrian Sampson']",
        "date": "October 2021",
        "source": "MICRO '21: MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture",
        "abstract": "We describe a tiled architecture that can fluidly transition between manycore (MIMD) and vector (SIMD) execution. The hardware provides a software-defined vector programming model that lets applications aggregate groups of manycore tiles into logical vector engines. In manycore mode, the machine behaves as a standard parallel processor. In vector mode, groups of tiles repurpose their functional units as vector execution lanes and scratchpads as vector memory banks. The key mechanism is an instruction forwarding network: a single tile fetches instructions and sends them to other trailing cores. Most cores disable their frontends and instruction caches, so vector groups amortize the intrinsic hardware costs of von Neumann control. Vector groups also use a decoupled access/execute scheme to centralize their memory requests and issue coalesced, wide loads.  We augment an existing RISC-V manycore design with a minimal hardware extension to implement software-defined vectors. Cycle-level simulation results show that software-defined vectors improve performance by an average of 1.7 × over standard MIMD execution while saving 22% of the energy. Compared to a similarly configured GPU, the architecture improves performance by 1.9 ×.",
        "link": "https://dl.acm.org/doi/10.1145/3466752.3480099",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "CeMux: Maximizing the Accuracy of Stochastic Mux Adders and an Application to Filter Design",
        "authors": "['Timothy J. Baker', 'John P. Hayes']",
        "date": "None",
        "source": "ACM Transactions on Design Automation of Electronic Systems",
        "abstract": "Stochastic computing (SC) is a low-cost computational paradigm that has promising applications in digital filter design, image processing, and neural networks. Fundamental to these applications is the weighted addition operation, which is most often implemented by a multiplexer (mux) tree. Mux-based adders have very low area but typically require long bitstreams to reach practical accuracy thresholds when the number of summands is large. In this work, we first identify the main contributors to mux adder error. We then demonstrate with analysis and experiment that two new techniques, precise sampling and full correlation, can target and mitigate these error sources. Implementing these techniques in hardware leads to the design of CeMux (Correlation-enhanced Multiplexer), a stochastic mux adder that is significantly more accurate and uses much less area than traditional weighted adders. We compare CeMux to other SC and hybrid designs for an electrocardiogram filtering case study that employs a large digital filter. One major result is that CeMux is shown to be accurate even for large input sizes. CeMux's higher accuracy leads to a latency reduction of 4× to 16× over other designs. Furthermore, CeMux uses about 35% less area than existing designs, and we demonstrate that a small amount of accuracy can be traded for a further 50% reduction in area. Finally, we compare CeMux to a conventional binary design and we show that CeMux can achieve a 50% to 73% area reduction for similar power and latency as the conventional design but at a slightly higher level of error.",
        "link": "https://dl.acm.org/doi/10.1145/3491213",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Prolonging 3D NAND SSD lifetime via read latency relaxation",
        "authors": "['Chun-Yi Liu', 'Yunju Lee', 'Myoungsoo Jung', 'Mahmut Taylan Kandemir', 'Wonil Choi']",
        "date": "April 2021",
        "source": "ASPLOS '21: Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "The adoption of 3D NAND has significantly increased the SSD density; however, 3D NAND density-increasing techniques, such as extensive stacking of cell layers, can amplify read disturbances and shorten SSD lifetime. From our lifetime-impact characterization on 8 state-of-the-art SSDs, we observe that the 3D TLC/QLC SSDs can be worn-out by low read-only workloads within their warranty period since a huge amount of read disturbance-induced rewrites are performed in the background. To understand alternative read disturbance mitigation opportunities, we also conducted read-latency characterizations on 2 other SSDs without the background rewrite mechanism. The collected results indicate that, without the background rewriting, the read latencies of the majority of data become higher, as the number of reads on the data increases. Motivated by these two characterizations, in this paper, we propose to relax the short read latency constraint on the high-density 3D SSDs. Specifically, our proposal relies on the hint information passed from applications to SSDs that specifies the expected read performance. By doing so, the lifetime consumption caused by the read-induced writes can be reduced, thereby prolonging the SSD lifetime. The detailed experimental evaluations show that our proposal can reduce up to 56% of the rewrite-induced spent-lifetime with only 2% lower performance, under a file-server application.",
        "link": "https://dl.acm.org/doi/10.1145/3445814.3446733",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Parallel application power and performance prediction modeling using simulation",
        "authors": "['Kishwar Ahmed', 'Kazutomo Yoshii', 'Samia Tasnim']",
        "date": "December 2021",
        "source": "WSC '21: Proceedings of the Winter Simulation Conference",
        "abstract": "High performance computing (HPC) system runs compute-intensive parallel applications requiring large number of nodes. An HPC system consists of heterogeneous computer architecture nodes, including CPUs, GPUs, field programmable gate arrays (FPGAs), etc. Power capping is a method to improve parallel application performance subject to variable power constraints. In this paper, we propose a parallel application power and performance prediction simulator. We present prediction model to predict application power and performance for unknown power-capping values considering heterogeneous computing architecture. We develop a job scheduling simulator based on parallel discrete-event simulation engine. The simulator includes a power and performance prediction model, as well as a resource allocation model. Based on real-life measurements and trace data, we show the applicability of our proposed prediction model and simulator.",
        "link": "https://dl.acm.org/doi/10.5555/3522802.3522980",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "APUF-BNN: An Automated Framework for Efficient Combinational Logic Based Implementation of Arbiter PUF through Binarized Neural Network",
        "authors": "['Pranesh Santikellur', 'Rijoy Mukherjee', 'Rajat Subhra Chakraborty']",
        "date": "June 2021",
        "source": "GLSVLSI '21: Proceedings of the 2021 on Great Lakes Symposium on VLSI",
        "abstract": "Analysis of Physically Unclocnable Functions (PUFs) from a Boolean function perspective, and the efficient hardware implementation of such Boolean representations, can potentially lead to interesting insights about their behavior and robustness. Such a circuit implementation can also be a convenient substitute for the machine learning model of a PUF instance in PUF-based security protocols. In this paper, we present APUF-BN, a novel computer-aided design (CAD) framework to efficiently generate a combinational circuit representation of an Arbiter PUF (APUF) instance, which accurately mimics its input-output behavior. This representation is derived from an optimized fully-connected Binarized Neural Network (BNN) model of the APUF. Our fully-automated CAD framework takes challenge-response pairs (CRPs) of an APUF instance as input, and generates Verilog description corresponding to the optimized combinational circuit representation as output. The optimized Boolean logic representation achieves more than 24% reduction in area overhead compared to the unoptimized BNN representation, while achieving close to 98% modeling accuracy. We also validate the derived combinational circuit representation on Xilinx Artix-7 FPGA platform.",
        "link": "https://dl.acm.org/doi/10.1145/3453688.3461484",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "SortCache: Intelligent Cache Management for Accelerating Sparse Data Workloads",
        "authors": "['Sriseshan Srikanth', 'Anirudh Jain', 'Thomas M. Conte', 'Erik P. Debenedictis', 'Jeanine Cook']",
        "date": "None",
        "source": "ACM Transactions on Architecture and Code Optimization",
        "abstract": "Sparse data applications have irregular access patterns that stymie modern memory architectures. Although hyper-sparse workloads have received considerable attention in the past, moderately-sparse workloads prevalent in machine learning applications, graph processing and HPC have not. Where the former can bypass the cache hierarchy, the latter fit in the cache. This article makes the observation that intelligent, near-processor cache management can improve bandwidth utilization for data-irregular accesses, thereby accelerating moderately-sparse workloads. We propose SortCache, a processor-centric approach to accelerating sparse workloads by introducing accelerators that leverage the on-chip cache subsystem, with minimal programmer intervention.",
        "link": "https://dl.acm.org/doi/10.1145/3473332",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Using Auto-Encoder Neural Networks for Memory Fault Tolerance in Gesture Recognition System",
        "authors": "['Bo ZHANG', 'Lei ZHANG', 'Mojun WU', 'Yan WANG']",
        "date": "March 2021",
        "source": "ICMAI '21: Proceedings of the 2021 6th International Conference on Mathematics and Artificial Intelligence",
        "abstract": "Data faults and retention characteristics in memories induce inaccuracy and failure in conventional electronic systems. While intelligent hardware which using AI algorithms can tolerate these faults with the advantage of neural network. This paper proposes using auto-encoder (AE) neural networks for memory fault tolerance in gesture recognition system based on RF sensors. This paper models the data faults of memories with defects or operating in ultra-low power state by a binary-type noise distribution. Then the model is used to test the effect of AE neural network in gesture recognition algorithm. Experimental results show AE neural network compress and extract useful features from noisy RF images, and higher gesture recognition accuracy is achieved based on these features. The algorithm achieves a recognition accuracy of 93% considering 20% bit level faults in RF images. The purpose of this method is to reduce the power consumption and improve the yield of the embedded RF based gesture recognition chip.",
        "link": "https://dl.acm.org/doi/10.1145/3460569.3460571",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "PLD: fast FPGA compilation to make reconfigurable acceleration compatible with modern incremental refinement software development",
        "authors": "['Yuanlong Xiao', 'Eric Micallef', 'Andrew Butt', 'Matthew Hofmann', 'Marc Alston', 'Matthew Goldsmith', 'Andrew Merczynski-Hait', 'André DeHon']",
        "date": "February 2022",
        "source": "ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "FPGA-based accelerators are demonstrating significant absolute performance and energy efficiency compared with general-purpose CPUs. While FPGA computations can now be described in standard, programming languages, like C, development for FPGAs accelerators remains tedious and inaccessible to modern software engineers. Slow compiles (potentially taking tens of hours) inhibit the rapid, incremental refinement of designs that is the hallmark of modern software engineering. To address this issue, we introduce separate compilation and linkage into the FPGA design flow, providing faster design turns more familiar to software development. To realize this flow, we provide abstractions, compiler options, and compiler flow that allow the same C source code to be compiled to processor cores in seconds and to FPGA regions in minutes, providing the missing -O0 and -O1 options familiar in software development. This raises the FPGA programming level and standardizes the programming experience, bringing FPGA-based accelerators into a more familiar software platform ecosystem for software engineers.",
        "link": "https://dl.acm.org/doi/10.1145/3503222.3507740",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "BenQ: <u>ben</u>chmarking automated quantization on deep neural network accelerators",
        "authors": "['Zheng Wei', 'Xingjun Zhang', 'Jingbo Li', 'Zeyu Ji', 'Jia Wei']",
        "date": "March 2022",
        "source": "DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe",
        "abstract": "Hardware-aware automated quantization promises to unlock an entirely new algorithm-hardware co-design paradigm for efficiently accelerating deep neural network (DNN) inference by incorporating the hardware cost into the reinforcement learning (RL) -based quantization strategy search process. Existing works usually design an automated quantization algorithm targeting one hardware accelerator with a device-specific performance model or pre-collected data. However, determining the hardware cost is non-trivial for algorithm experts due to their lack of cross-disciplinary knowledge in computer architecture, compiler, and physical chip design. Such a barrier limits reproducibility and fair comparison. Moreover, it is notoriously challenging to interpret the results due to the lack of quantitative metrics. To this end, we first propose BenQ, which includes various RL-based automated quantization algorithms with aligned settings and encapsulates two off-the-shelf performance predictors with standard OpenAI Gym API. Then, we leverage cosine similarity and manhattan distance to interpret the similarity between the searched policies. The experiments show that different automated quantization algorithms can achieve near equivalent optimal trade-offs because of the high similarity between the searched policies, which provides insights for revisiting the innovations in automated quantization algorithms.",
        "link": "https://dl.acm.org/doi/10.5555/3539845.3540187",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Achieving crash consistency by employing persistent L1 cache",
        "authors": "['Akshay Krishna Ramanathan', 'Sara Mahdizadeh Shahri', 'Yi Xiao', 'Vijaykrishnan Narayanan']",
        "date": "March 2022",
        "source": "DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe",
        "abstract": "Emerging non-volatile memory technologies promise the opportunity for maintaining persistent data in memory. However, providing crash-consistency in such systems can be costly as any update to the persistent data has to reach the persistent domain in a specific order, imposing high overhead. Prior works, proposed solutions both in software (SW) and hardware (HW) to address this problem but fall short to remove this overhead completely. In this work, we propose Non-Volatile Cache (NVC) architecture design that employs a hybrid volatile, non-volatile memory cell employing monolithic 3D and Ferroelectric technology in L1 data cache to guarantee crash consistency with almost no performance overhead. We show that NVC achieves up to 5.1x speedup over state-of-the-art (SOTA) SW undo logging and 11% improvement over SOTA HW solution without yielding the conventional architecture, while incurring 7% hardware overhead.",
        "link": "https://dl.acm.org/doi/10.5555/3539845.3540172",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Reconfigurable Framework for Resilient Semantic Segmentation for Space Applications",
        "authors": "['Sebastian Sabogal', 'Alan George', 'Gary Crum']",
        "date": "None",
        "source": "ACM Transactions on Reconfigurable Technology and Systems",
        "abstract": "Deep learning (DL) presents new opportunities for enabling spacecraft autonomy, onboard analysis, and intelligent applications for space missions. However, DL applications are computationally intensive and often infeasible to deploy on radiation-hardened (rad-hard) processors, which traditionally harness a fraction of the computational capability of their commercial-off-the-shelf counterparts. Commercial FPGAs and system-on-chips present numerous architectural advantages and provide the computation capabilities to enable onboard DL applications; however, these devices are highly susceptible to radiation-induced single-event effects (SEEs) that can degrade the dependability of DL applications. In this article, we propose Reconfigurable ConvNet (RECON), a reconfigurable acceleration framework for dependable, high-performance semantic segmentation for space applications. In RECON, we propose both selective and adaptive approaches to enable efficient SEE mitigation. In our selective approach, control-flow parts are selectively protected by triple-modular redundancy to minimize SEE-induced hangs, and in our adaptive approach, partial reconfiguration is used to adapt the mitigation of dataflow parts in response to a dynamic radiation environment. Combined, both approaches enable RECON to maximize system performability subject to mission availability constraints. We perform fault injection and neutron irradiation to observe the susceptibility of RECON and use dependability modeling to evaluate RECON in various orbital case studies to demonstrate a 1.5–3.0× performability improvement in both performance and energy efficiency compared to static approaches.",
        "link": "https://dl.acm.org/doi/10.1145/3472770",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Microphone array backscatter: an application-driven design for lightweight spatial sound recording over the air",
        "authors": "['Jia Zhao', 'Wei Gong', 'Jiangchuan Liu']",
        "date": "October 2021",
        "source": "MobiCom '21: Proceedings of the 27th Annual International Conference on Mobile Computing and Networking",
        "abstract": "Modern acoustic wearables with microphone arrays are promising to offer rich experience (e.g., 360° sound and acoustic imaging) to consumers. Realtime multi-track audio streaming with precise synchronization however poses significant challenges to the existing wireless microphone array designs that depend on complex digital synchronization as well as bulky and power-hungry hardware. This paper presents a novel microphone array sensor architecture that enables synchronous concurrent transmission of multitrack audio signals using analog backscatter communication. We develop novel Pulse Position Modulation (PPM) and Differential Pulse Position Modulation (DPPM) baseband circuits that can generate a spectral-efficient, time-multiplexing, and multi-track-synchronous baseband signal for backscattering. Its lightweight analog synchronization supports parallel multimedia signals without using any ADCs, DSPs, codecs and RF transceivers, hence largely reducing the complexity, latency, and power consumption. To further enhance self-sustainability, we also design an energy harvester that can extract energy from both sound and RF. We have built a microphone array backscatter sensor prototype using an FPGA, discrete components, and analog devices. Our experiments demonstrate a communication range (sensor-to-reader) of up to 28 meters for 8 audio tracks, and an equivalent throughput of up to 6.4 Mbps with a sample rate over 48KHz. Our sensor achieves 87.4μs of streaming latency for 4 tracks, which is 650x improvement as compared with digital solutions. ASIC design results show that it consumes as low as 175.2μW of power. Three sample applications including an acoustic imaging system, a beamform filter, and a voice control system, all built with our phased-array microphone, further demonstrate the applicability of our design.",
        "link": "https://dl.acm.org/doi/10.1145/3447993.3483265",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "LIBSHALOM: optimizing small and irregular-shaped matrix multiplications on ARMv8 multi-cores",
        "authors": "['Weiling Yang', 'Jianbin Fang', 'Dezun Dong', 'Xing Su', 'Zheng Wang']",
        "date": "November 2021",
        "source": "SC '21: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",
        "abstract": "General Matrix Multiplication (GEMM) is a key subroutine in highperformance computing. While the mainstream linear algebra libraries can deliver high performance on large and regular-shaped GEMM, they are inadequate for optimizing small and irregular-shaped GEMMs, which are commonly seen in new HPC applications. Some of the recent works in this direction have made promising progress on x86 architectures and GPUs but still leave much room for improvement on emerging HPC hardware built upon the ARMv8 architecture. We present LibShalom, an open-source library for optimizing small and irregular-shaped GEMMs, explicitly targeting the ARMv8 architecture. LibShalom builds upon the classical Goto algorithm but tailors it to minimize the expensive memory accessing overhead for data packing and processing small matrices. It uses analytic methods to determine GEMM kernel optimization parameters, enhancing the computation and parallelization efficiency of the GEMM kernels. We evaluate LibShalom by applying it to three ARMv8 multi-core architectures and comparing it against five mainstream linear algebra libraries. Experimental results show that LibShalom can consistently outperform existing solutions across GEMM workloads and hardware architectures.",
        "link": "https://dl.acm.org/doi/10.1145/3458817.3476217",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Research on Mixed-Precision Quantization and Fault-Tolerant of Deep Neural Networks",
        "authors": "['Zhaoxin Wang', 'Jing Wang', 'Kun Qian']",
        "date": "October 2021",
        "source": "CSAE '21: Proceedings of the 5th International Conference on Computer Science and Application Engineering",
        "abstract": "As deep neural networks become more and more common in mission-critical applications, such as smart medical care, drones, and autonomous driving, ensuring their reliable operation becomes critical. The data in the hardware memory is susceptible to bit-flip due to external factors, which leads to a decrease in the inference accuracy of the deep neural network deployed on the hardware. We solve this problem from the perspective of the deep neural network itself, We use a reinforcement learning algorithm to search for the optimal bit width for the weights of each layer of the deep neural network. According to this bit width strategy, the deep neural network is quantified, which maximizes the limitation of data fluctuations caused by bit-flip and improves the fault-tolerance of the neural network. The fault-tolerance of the network model compared with the original model, the solution proposed in this paper improves the fault-tolerance of LeNet5 model by 8.5x , the fault tolerance of MobileNetV2 model by 15.6x , the fault tolerance of VGG16 model by 14.5x , and the accuracy decreases negligibly.",
        "link": "https://dl.acm.org/doi/10.1145/3487075.3487135",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "An Efficient Parallelized Huffman Decoding PE",
        "authors": "['Yan Wu', 'Ruizhen Wu', 'Heng Ma', 'Lin Wang', 'Jianlong Su']",
        "date": "June 2021",
        "source": "ITCC '21: Proceedings of the 2021 3rd International Conference on Information Technology and Computer Communications",
        "abstract": "An efficient parallelized Huffman decoding PE is proposed in this paper. The classic Huffman decoding algorithm is optimized first in parallel processing and then four hardware modules are designed based on that. The proposed PE can decode different numbers of streams with various compression ratios. The hardware architecture is fully verified at RTL level and synthesized with GF 12nm technology lib. The simulation shows a better decoding performance especially with wide data width and high compression ratio settings.",
        "link": "https://dl.acm.org/doi/10.1145/3473465.3473485",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "BEACON: BEst Approximations for Complete BehaviOral HeterogeNeous SoCs",
        "authors": "['Prattay Chowdhury', 'Benjamin Carrion Schafer']",
        "date": "July 2021",
        "source": "ISLPED '21: Proceedings of the ACM/IEEE International Symposium on Low Power Electronics and Design",
        "abstract": "Approximate computing has shown to be an effective approach to generate smaller and more power-efficient circuits by trading the accuracy of the circuit vs. area/power. So far, most work on approximate computing has focused on specific components within a system. This severely limits the approximation potential as most Integrated Circuits (ICs) are now complex heterogeneous systems. This paper investigates if lower-power designs can be found through mixing approximations across the different components in the SoC as opposed to only aggressively approximating a single component. The main hypothesis is that some approximations amplify across the system, while others tend to cancel each other out, thus, allowing to maximize the power savings while meeting the given maximum error threshold. In this work, we consider the Analog-to-Digital Converter (ADC), CPU, hardware accelerators, and interconnect between all these components. Moreover, to quickly measure the effect of different approximation mixes, we have developed a framework that allows generating complete SoCs at the behavioral level through a bus generator and a library of synthesizable bus interfaces. This enables the use of fast simulation models (transaction and cycle-accurate) to accurately measure the error at the system's output while measuring the benefit in terms of area or energy reduction of different mixes of approximations. Experimental results show that taking into account the entire system as oppose to only individual components leads to an additional average energy savings of 14% to 17% for different maximum error thresholds and the best case up to 39%.",
        "link": "https://dl.acm.org/doi/10.1109/ISLPED52811.2021.9502479",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Jamais vu: thwarting microarchitectural replay attacks",
        "authors": "['Dimitrios Skarlatos', 'Zirui Neil Zhao', 'Riccardo Paccagnella', 'Christopher W. Fletcher', 'Josep Torrellas']",
        "date": "April 2021",
        "source": "ASPLOS '21: Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "Microarchitectural Replay Attacks (MRAs) enable an attacker to eliminate the measurement variation in potentially any microarchitectural side channel—even if the victim instruction is supposed to execute only once. In an MRA, the attacker forces pipeline flushes in order to repeatedly re-execute the victim instruction and denoise the channel. MRAs are not limited to transient execution attacks: the replayed victim can be an instruction that will eventually retire. This paper presents the first technique to thwart MRAs. The technique, called Jamais Vu, detects when an instruction is squashed. Then, as the instruction is re-inserted into the pipeline, Jamais Vu automatically places a fence before it to prevent the attacker from squashing it again. This paper presents several Jamais Vu designs that offer different trade-offs between security, execution overhead, and implementation complexity. One design, called Epoch-Loop-Rem, effectively mitigates MRAs, has an average execution time overhead of 13.8% in benign executions, and only needs counting Bloom filters. An even simpler design, called Clear-on-Retire, has an average execution time overhead of only 2.9%, although it is less secure.",
        "link": "https://dl.acm.org/doi/10.1145/3445814.3446716",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Accelerating Deep Neuroevolution on Distributed FPGAs for Reinforcement Learning Problems",
        "authors": "['Alexis Asseman', 'Nicolas Antoine', 'Ahmet S. Ozcan']",
        "date": "None",
        "source": "ACM Journal on Emerging Technologies in Computing Systems",
        "abstract": "Reinforcement learning, augmented by the representational power of deep neural networks, has shown promising results on high-dimensional problems, such as game playing and robotic control. However, the sequential nature of these problems poses a fundamental challenge for computational efficiency. Recently, alternative approaches such as evolutionary strategies and deep neuroevolution demonstrated competitive results with faster training time on distributed CPU cores. Here we report record training times (running at about 1 million frames per second) for Atari 2600 games using deep neuroevolution implemented on distributed FPGAs. Combined hardware implementation of the game console, image preprocessing and the neural network in an optimized pipeline, multiplied with the system level parallelism enabled the acceleration. These results are the first application demonstration on the IBM Neural Computer, which is a custom designed system that consists of 432 Xilinx FPGAs interconnected in a 3D mesh network topology. In addition to high performance, experiments also showed improvement in accuracy for all games compared to the CPU implementation of the same algorithm.",
        "link": "https://dl.acm.org/doi/10.1145/3425500",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Automatic Sublining for Efficient Sparse Memory Accesses",
        "authors": "['Wim Heirman', 'Stijn Eyerman', 'Kristof Du Bois', 'Ibrahim Hur']",
        "date": "None",
        "source": "ACM Transactions on Architecture and Code Optimization",
        "abstract": "Sparse memory accesses, which are scattered accesses to single elements of a large data structure, are a challenge for current processor architectures. Their lack of spatial and temporal locality and their irregularity makes caches and traditional stream prefetchers useless. Furthermore, performing standard caching and prefetching on sparse accesses wastes precious memory bandwidth and thrashes caches, deteriorating performance for regular accesses. Bypassing prefetchers and caches for sparse accesses, and fetching only a single element (e.g., 8 B) from main memory (subline access), can solve these issues.Deciding which accesses to handle as sparse accesses and which as regular cached accesses, is a challenging task, with a large potential impact on performance. Not only is performance reduced by treating sparse accesses as regular accesses, not caching accesses that do have locality also negatively impacts performance by significantly increasing their latency and bandwidth consumption. Furthermore, this decision depends on the dynamic environment, such as input set characteristics and system load, making a static decision by the programmer or compiler suboptimal.We propose the Instruction Spatial Locality Estimator (ISLE), a hardware detector that finds instructions that access isolated words in a sea of unused data. These sparse accesses are dynamically converted into uncached subline accesses, while keeping regular accesses cached. ISLE does not require modifying source code or binaries, and adapts automatically to a changing environment (input data, available bandwidth, etc.). We apply ISLE to a graph analytics processor running sparse graph workloads, and show that ISLE outperforms the performance of no subline accesses, manual sublining, and prior work on detecting sparse accesses.",
        "link": "https://dl.acm.org/doi/10.1145/3452141",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Terminator: A Secure Coprocessor to Accelerate Real-Time AntiViruses Using Inspection Breakpoints",
        "authors": "['Marcus Botacin', 'Francis B. Moreira', 'Philippe O. A. Navaux', 'André Grégio', 'Marco A. Z. Alves']",
        "date": "None",
        "source": "ACM Transactions on Privacy and Security",
        "abstract": "AntiViruses (AVs) are essential to face the myriad of malware threatening Internet users. AVs operate in two modes: on-demand checks and real-time verification. Software-based real-time AVs intercept system and function calls to execute AV’s inspection routines, resulting in significant performance penalties as the monitoring code runs among the suspicious code. Simultaneously, dark silicon problems push the industry to add more specialized accelerators inside the processor to mitigate these integration problems. In this article, we propose Terminator, an AV-specific coprocessor to assist software AVs by outsourcing their matching procedures to the hardware, thus saving CPU cycles and mitigating performance degradation. We designed Terminator   to be flexible and compatible with existing AVs by using YARA and ClamAVrules. Our experiments show that our approach can save up to 70 million CPU cycles per rule when outsourcing on-demand checks for matching typical, unmodified YARA rules against a dataset of 30 thousand in-the-wild malware samples. Our proposal eliminates the AV’s need for blocking the CPU to perform full system checks, which can now occur in parallel. We also designed a new inspection breakpoint mechanism that signals to the coprocessor the beginning of a monitored region, allowing it to scan the regions in parallel with their execution. Overall, our mechanism mitigated up to 44% of the overhead imposed to execute and monitor the SPEC benchmark applications in the most challenging scenario.",
        "link": "https://dl.acm.org/doi/10.1145/3494535",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Software-Managed Read and Write Wear-Leveling for Non-Volatile Main Memory",
        "authors": "['Christian Hakert', 'Kuan-Hsun Chen', 'Horst Schirmeier', 'Lars Bauer', 'Paul R. Genssler', 'Georg von der Brüggen', 'Hussam Amrouch', 'Jörg Henkel', 'Jian-Jia Chen']",
        "date": "None",
        "source": "ACM Transactions on Embedded Computing Systems",
        "abstract": "In-memory wear-leveling has become an important research field for emerging non-volatile main memories over the past years. Many approaches in the literature perform wear-leveling by making use of special hardware. Since most non-volatile memories only wear out from write accesses, the proposed approaches in the literature also usually try to spread write accesses widely over the entire memory space. Some non-volatile memories, however, also wear out from read accesses, because every read causes a consecutive write access. Software-based solutions only operate from the application or kernel level, where read and write accesses are realized with different instructions and semantics. Therefore different mechanisms are required to handle reads and writes on the software level. First, we design a method to approximate read and write accesses to the memory to allow aging aware coarse-grained wear-leveling in the absence of special hardware, providing the age information. Second, we provide specific solutions to resolve access hot-spots within the compiled program code (text segment) and on the application stack. In our evaluation, we estimate the cell age by counting the total amount of accesses per cell. The results show that employing all our methods improves the memory lifetime by up to a factor of 955×.",
        "link": "https://dl.acm.org/doi/10.1145/3483839",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "HyperTester: High-Performance Network Testing Driven by Programmable Switches",
        "authors": "['Dai Zhang', 'Yu Zhou', 'Zhaowei Xi', 'Yangyang Wang', 'Mingwei Xu', 'Jianping Wu']",
        "date": "None",
        "source": "IEEE/ACM Transactions on Networking",
        "abstract": "Modern network devices and systems are raising higher requirements on network testers that are regularly used to evaluate performance and assess correctness. These requirements include high scale, high accuracy, flexibility and low cost, which existing testers cannot fulfill at the same time. In this paper, we propose HyperTester, a network tester leveraging new-generation programmable switches and achieving all of the above goals simultaneously. Programmable switches are born with features like high throughput and linerate, deterministic processing pipelines and nanosecond-level hardware timestamps, the P4 programming model as well as comparable pricing with commodity servers, but they come with limited programmability and memory resources. HyperTester uses template-based packet generation to overcome the limitations of the switch ASIC in programmability and designs a stateless connection mechanism as well as counter-based state compression algorithms to overcome the memory resource constraints in the data plane. We have implemented HyperTester on Tofino, and the evaluations on the hardware testbed show that HyperTester supports high-scale packet generation (more than 1.6Tbps) and achieves highly accurate rate control and timestamping. We demonstrate that programmable switches can be potential and attractive targets for realizing network testers.",
        "link": "https://dl.acm.org/doi/10.1109/TNET.2021.3077652",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "AnaCoNGA: analytical HW-CNN co-design using nested genetic algorithms",
        "authors": "['Nael Fasfous', 'Manoj Rohit Vemparala', 'Alexander Frickenstein', 'Emanuele Valpreda', 'Driton Salihu', 'Julian Höfer', 'Anmol Singh', 'Naveen-Shankar Nagaraja', 'Hans-Joerg Voegel', 'Nguyen Anh Vu Doan', 'Maurizio Martina', 'Juergen Becker', 'Walter Stechele']",
        "date": "March 2022",
        "source": "DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe",
        "abstract": "We present AnaCoNGA, an analytical co-design methodology, which enables two genetic algorithms to evaluate the fitness of design decisions on layer-wise quantization of a neural network and hardware (HW) resource allocation. We embed a hardware architecture search (HAS) algorithm into a quantization strategy search (QSS) algorithm to evaluate the hardware design Pareto-front of each considered quantization strategy. We harness the speed and flexibility of analytical HW-modeling to enable parallel HW-CNN co-design. With this approach, the QSS is focused on seeking high-accuracy quantization strategies which are guaranteed to have efficient hardware designs at the end of the search. Through AnaCoNGA, we improve the accuracy by 2.88 p.p. with respect to a uniform 2-bit ResNet20 on CIFAR-10, and achieve a 35% and 37% improvement in latency and DRAM accesses, while reducing LUT and BRAM resources by 9% and 59% respectively, when compared to a standard edge variant of the accelerator. The nested genetic algorithm formulation also reduces the search time by 51% compared to an equivalent, sequential co-design formulation.",
        "link": "https://dl.acm.org/doi/10.5555/3539845.3539907",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Ookami: Deployment and Initial Experiences",
        "authors": "['Andrew Burford', 'Alan Calder', 'David Carlson', 'Barbara Chapman', 'Firat Coskun', 'Tony Curtis', 'Catherine Feldman', 'Robert Harrison', 'Yan Kang', 'Benjamin Michalowicz', 'Eric Raut', 'Eva Siegmann', 'Daniel Wood', 'Robert DeLeon', 'Mathew Jones', 'Nikolay Simakov', 'Joseph White', 'Dossay Oryspayev']",
        "date": "July 2021",
        "source": "PEARC '21: Practice and Experience in Advanced Research Computing",
        "abstract": "Ookami [3] is a computer technology testbed supported by the United States National Science Foundation. It provides researchers with access to the A64FX processor developed by Fujitsu [17] in collaboration with RIKΞN  [35, 37] for the Japanese path to exascale computing, as deployed in Fugaku [36], the fastest computer in the world [34]. By focusing on crucial architectural details, the ARM-based, multi-core, 512-bit SIMD-vector processor with ultrahigh-bandwidth memory promises to retain familiar and successful programming models while achieving very high performance for a wide range of applications. We review relevant technology and system details, and the main body of the paper focuses on initial experiences with the hardware and software ecosystem for micro-benchmarks, mini-apps, and full applications, and starts to answer questions about where such technologies fit into the NSF ecosystem.",
        "link": "https://dl.acm.org/doi/10.1145/3437359.3465578",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Interpreting Intermediate Convolutional Layers of Generative CNNs Trained on Waveforms",
        "authors": "['Gašper Beguš', 'Alan Zhou']",
        "date": "None",
        "source": "IEEE/ACM Transactions on Audio, Speech and Language Processing",
        "abstract": "This paper presents a technique to interpret and visualize intermediate layers in generative CNNs trained on raw speech data in an unsupervised manner. We argue that averaging over feature maps after ReLU activation in each transpose convolutional layer yields interpretable time-series data. This technique allows for acoustic analysis of intermediate layers that parallels the acoustic analysis of human speech data: we can extract F0, intensity, duration, formants, and other acoustic properties from intermediate layers in order to test where and how CNNs encode various types of information. We further combine this technique with linear interpolation of a model&#x0027;s latent space to show a causal relationship between individual variables in the latent space and activations in a model&#x0027;s intermediate convolutional layers. In particular, observing the causal effect between linear interpolation and the resulting changes in intermediate layers can reveal how individual latent variables get transformed into spikes in activation in intermediate layers. We train and probe internal representations of two models &#x2014; a bare WaveGAN architecture and a ciwGAN extension which forces the Generator to output informative data and results in the emergence of linguistically meaningful representations. Interpretation and visualization is performed for three basic acoustic properties of speech: periodic vibration (corresponding to vowels), aperiodic noise vibration (corresponding to fricatives), and silence (corresponding to stops). The proposal also allows testing of higher-level morphophonological alternations such as reduplication (copying). In short, using the proposed technique, we can analyze how linguistically meaningful units in speech get encoded in each convolutional layer of a generative neural network.",
        "link": "https://dl.acm.org/doi/10.1109/TASLP.2022.3209938",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "The Hera framework for fault-tolerant sensor fusion with Erlang and GRiSP on an IoT network",
        "authors": "['Sébastien Kalbusch', 'Vincent Verpoten', 'Peter Van Roy']",
        "date": "August 2021",
        "source": "Erlang 2021: Proceedings of the 20th ACM SIGPLAN International Workshop on Erlang",
        "abstract": "Classical sensor fusion approaches require to work directly with the hardware and involve a lot of low-level programming, which is not suited for reliable and user-friendly sensor fusion for Internet of Things (IoT) applications. In this paper, we propose and analyze Hera, a Kalman filter-based sensor fusion framework for Erlang. Hera offers a high-level approach for asynchronous and fault-tolerant sensor fusion directly at the edge of an IoT network. We use the GRiSP-Base board, a low-cost platform specially designed for Erlang and to avoid soldering or dropping down to C. We emphasize on the importance of performing all the computations directly at the sensor-equipped devices themselves, completely removing the cloud necessity. We show that we can perform sensor fusion for position and orientation tracking at a high level of abstraction and with the strong guarantee that the system will keep running as long as one GRiSP board is alive. With Hera, the implementation effort is significantly reduced which makes it an excellent candidate for IoT prototyping and education in the field of sensor fusion.",
        "link": "https://dl.acm.org/doi/10.1145/3471871.3472962",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Best Approximate Quantum Compiling Problems",
        "authors": "['Liam Madden', 'Andrea Simonetto']",
        "date": "None",
        "source": "ACM Transactions on Quantum Computing",
        "abstract": "We study the problem of finding the best approximate circuit that is the closest (in some pertinent metric) to a target circuit, and which satisfies a number of hardware constraints, like gate alphabet and connectivity. We look at the problem in the CNOT+rotation gate set from a mathematical programming standpoint, offering contributions both in terms of understanding the mathematics of the problem and its efficient solution. Among the results that we present, we are able to derive a 14-CNOT 4-qubit Toffoli decomposition from scratch, and show that the Quantum Shannon Decomposition can be compressed by a factor of two without practical loss of fidelity.",
        "link": "https://dl.acm.org/doi/10.1145/3505181",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Sense Your Power: The ECO Approach to Energy Awareness for IoT Devices",
        "authors": "['Michel Rottleuthner', 'Thomas C. Schmidt', 'Matthias Wählisch']",
        "date": "None",
        "source": "ACM Transactions on Embedded Computing Systems",
        "abstract": "Energy-constrained sensor nodes can adaptively optimize their energy consumption if a continuous measurement is provided. This is of particular importance in scenarios of high dynamics such as with energy harvesting. Still, self-measuring of power consumption at reasonable cost and complexity is unavailable as a generic system service.In this article, we present ECO, a hardware-software co-design that adds autonomous energy management capabilities to a large class of low-end IoT devices. ECO consists of a highly portable hardware shield built from inexpensive commodity components and software integrated into the RIOT operating system. RIOT supports more than 200 popular microcontrollers. Leveraging this flexibility, we assembled a variety of sensor nodes to evaluate key performance properties for different device classes. An overview and comparison with related work shows how ECO fills the gap of in situ power attribution transparently for consumers and how it improves over existing solutions. We also report about two different real-world field trials, which validate our solution for long-term production use.",
        "link": "https://dl.acm.org/doi/10.1145/3441643",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Zero-Shot Normalization Driven Multi-Speaker Text to Speech Synthesis",
        "authors": "['Neeraj Kumar', 'Ankur Narang', 'Brejesh Lall']",
        "date": "None",
        "source": "IEEE/ACM Transactions on Audio, Speech and Language Processing",
        "abstract": "Text-to-speech (TTS) systems are designed to synthesize natural and expressive speech, adapt to an unseen voice, and capture the speaking style of an unseen speaker by converting text into speech. The introduction of an unseen speaker&#x2019;s speaking style into a TTS system offers a wide range of application scenarios, including personal assistant, news broadcast, and audio navigation, among others. The style of the speech varies from person to person and every person exhibits his or her style of speaking that is determined by the language, demography, culture and other factors. Style is best captured by the prosody of a signal. It is an ongoing research area with numerous real-world applications that produces high-quality multi-speaker voice synthesis while taking into account prosody and in a zero-shot manner. Despite the fact that several efforts have been made in this area, it continues to be an interesting and difficult topic to solve. In this paper, we present a novel zero-shot multi-speaker speech synthesis approach (ZSM-SS) that leverages the normalization architecture and speaker encoder with non-autoregressive multi-head attention driven encoder-decoder architecture. Given an input text and a reference speech sample of an unseen person, ZSM-SS can generate speech in that person&#x2019;s style in a zero-shot manner. Additionally, we demonstrate how the affine parameters of normalization help in capturing the prosodic features such as energy and fundamental frequency in a disentangled fashion and can be used to generate morphed speech output. We generate the 256 dimensional speaker embedding using a speaker encoder based on wav2vec2.0 based architecture. We demonstrate the efficacy of our proposed architecture on multi-speaker VCTK [1] and LibriTTS [2] datasets, using visualization of hessian of proposed model, multiple quantitative metrics that measure generated speech distortion and MOS, along with speaker embedding analysis of the proposed speaker encoder model.",
        "link": "https://dl.acm.org/doi/10.1109/TASLP.2022.3169634",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Integrated Power Signature Generation Circuit for IoT Abnormality Detection",
        "authors": "['David Thompson', 'Haibo Wang']",
        "date": "None",
        "source": "ACM Journal on Emerging Technologies in Computing Systems",
        "abstract": "This work presents a methodology to monitor the power signature of IoT devices for detecting operation abnormality. It does not require bulky measurement equipment thanks to the proposed power signature generation circuit which can be integrated into LDO voltage regulators. The proposed circuit is implemented using a 130 nm CMOS technology and simulated with power trace measured from a wireless sensor. It shows the generated power signature accurately reflects the power consumption and can be used to distinguish different operation conditions, such as wireless transmission levels, data sampling rates and microcontroller UART communications.",
        "link": "https://dl.acm.org/doi/10.1145/3460476",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Parallelizing Intra-Window Join on Multicores: An Experimental Study",
        "authors": "['Shuhao Zhang', 'Yancan Mao', 'Jiong He', 'Philipp M. Grulich', 'Steffen Zeuch', 'Bingsheng He', 'Richard T. B. Ma', 'Volker Markl']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "The intra-window join (IaWJ), i.e., joining two input streams over a single window, is a core operation in modern stream processing applications. This paper presents the first comprehensive study on parallelizing the IaWJ on modern multicore architectures. In particular, we classify IaWJ algorithms into lazy and eager execution approaches. For each approach, there are further design aspects to consider, including different join methods and partitioning schemes, leading to a large design space. Our results show that none of the algorithms always performs the best, and the choice of the most performant algorithm depends on: (i) workload characteristics, (ii) application requirements, and (iii) hardware architectures. Based on the evaluation results, we propose a decision tree that can guide the selection of an appropriate algorithm.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3452793",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Quantum Annealing versus Digital Computing: An Experimental Comparison",
        "authors": "['Michael Jünger', 'Elisabeth Lobe', 'Petra Mutzel', 'Gerhard Reinelt', 'Franz Rendl', 'Giovanni Rinaldi', 'Tobias Stollenwerk']",
        "date": "None",
        "source": "ACM Journal of Experimental Algorithmics",
        "abstract": "Quantum annealing is getting increasing attention in combinatorial optimization. The quantum processing unit by D-Wave is constructed to approximately solve Ising models on so-called Chimera graphs. Ising models are equivalent to quadratic unconstrained binary optimization (QUBO) problems and maximum cut problems on the associated graphs. We have tailored branch-and-cut as well as semidefinite programming algorithms for solving Ising models for Chimera graphs to provable optimality and use the strength of these approaches for comparing our solution values to those obtained on the current quantum annealing machine, D-Wave 2000Q. This allows for the assessment of the quality of solutions produced by the D-Wave hardware. In addition, we also evaluate the performance of a heuristic by Selby. It has been a matter of discussion in the literature how well the D-Wave hardware performs at its native task, and our experiments shed some more light on this issue. In particular, we examine how reliably the D-Wave computer can deliver true optimum solutions and present some surprising results.",
        "link": "https://dl.acm.org/doi/10.1145/3459606",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Deep Optimization of Parametric IIR Filters for Audio Equalization",
        "authors": "['Giovanni Pepe', 'Leonardo Gabrielli', 'Stefano Squartini', 'Carlo Tripodi', 'Nicolò Strozzi']",
        "date": "None",
        "source": "IEEE/ACM Transactions on Audio, Speech and Language Processing",
        "abstract": "This paper describes a novel Deep Learning method for the design of IIR parametric filters for automatic multipoint audio equalization, that is the task of improving the sound quality of a listening environment at multiple listening points employing multiple loudspeakers. The filters are designed to approximate the inverse of the RIR and achieve almost flat magnitude response. A simple and effective neural architecture, named BiasNet, is proposed to determine the IIR equalizer parameters. This novel architecture is conceived for optimization and, as such, is able to produce optimal IIR equalizer parameters at its output, after training, with no input required. In absence of input, the presence of learnable non-zero bias terms ensures that the network works properly. An output scaling method is used to obtain accurate tuning of the IIR filters center frequency, quality factor and gain. All layers involved in the proposed method are shown to be differentiable, allowing backpropagation to optimize the network weights and achieve, after a number of training iterations, the optimal output according to a given RIR. The parameters are optimized with respect to a loss function based on a spectral distance between the measured and desired magnitude response, and a regularization term is used to keep the same microphone-loudspeaker energy balance after equalization. Two experimental scenarios are employed, a room and a car cabin, with several loudspeakers. The performance of the proposed method improves over the baseline techniques and achieves an almost flat band at a lower computational cost.",
        "link": "https://dl.acm.org/doi/10.1109/TASLP.2022.3155289",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "SOURCE: a Freesound Community Music Sampler",
        "authors": "['Frederic Font']",
        "date": "September 2021",
        "source": "AM '21: Proceedings of the 16th International Audio Mostly Conference",
        "abstract": "SOURCE is an open-source hardware music sampler powered by Freesound’s collection of near 500k Creative Commons sounds contributed by a community of thousands of people around the world. SOURCE provides a hardware interface with Freesound and implements different methods to search and load sounds into the sampler. We see SOURCE as a proof of concept device that can be used as and extendable base system on top of which further research on the interaction between hardware devices and online large sound collections can be carried out. This paper describes the architecture of SOURCE and the different ways in which it interacts with Freesound. Even though we have not carried out a formal evaluation of SOURCE our informal tests show that SOURCE can be successfully integrated into a live music performance setup, and influence the creative process in interesting ways by being able to quickly generate new rich sound palettes that would otherwise be difficult to create with traditional hardware music samplers.",
        "link": "https://dl.acm.org/doi/10.1145/3478384.3478388",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A Deep Learning Framework to Predict Routability for FPGA Circuit Placement",
        "authors": "['Abeer Al-Hyari', 'Hannah Szentimrey', 'Ahmed Shamli', 'Timothy Martin', 'Gary Gréwal', 'Shawki Areibi']",
        "date": "None",
        "source": "ACM Transactions on Reconfigurable Technology and Systems",
        "abstract": "The ability to accurately and efficiently estimate the routability of a circuit based on its placement is one of the most challenging and difficult tasks in the Field Programmable Gate Array (FPGA) flow. In this article, we present a novel, deep learning framework based on a Convolutional Neural Network (CNN) model for predicting the routability of a placement. Since the performance of the CNN model is strongly dependent on the hyper-parameters selected for the model, we perform an exhaustive parameter tuning that significantly improves the model’s performance and we also avoid overfitting the model. We also incorporate the deep learning model into a state-of-the-art placement tool and show how the model can be used to (1) avoid costly, but futile, place-and-route iterations, and (2) improve the placer’s ability to produce routable placements for hard-to-route circuits using feedback based on routability estimates generated by the proposed model. The model is trained and evaluated using over 26K placement images derived from 372 benchmarks supplied by Xilinx Inc. We also explore several opportunities to further improve the reliability of the predictions made by the proposed DLRoute technique by splitting the model into two separate deep learning models for (a) global and (b) detailed placement during the optimization process. Experimental results show that the proposed framework achieves a routability prediction accuracy of 97% while exhibiting runtimes of only a few milliseconds.",
        "link": "https://dl.acm.org/doi/10.1145/3465373",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "The Case for SIMDified Analytical Query Processing on GPUs",
        "authors": "['Johannes Fett', 'Annett Ungethüm', 'Dirk Habich', 'Wolfgang Lehner']",
        "date": "June 2021",
        "source": "DAMON '21: Proceedings of the 17th International Workshop on Data Management on New Hardware",
        "abstract": "Data-level parallelism (DLP) is a heavily used hardware-driven parallelization technique to optimize the analytical query processing, especially in in-memory column stores. This kind of parallelism is characterized by executing essentially the same operation on different data elements simultaneously. Besides Single Instruction Multiple Data (SIMD) extensions on common x86-processors, GPUs also provide DLP but with a different execution model called Single Instruction Multiple Threads (SIMT), where multiple scalar threads are executed in a SIMD manner. Unfortunately, a complete GPU-specific implementation of all query operators has to be set up, since the state of the vectorized implementations cannot be ported from x86-processors to GPUs right now. To avoid this implementation effort, we present our vision to virtualize GPUs as virtual vector engines with software-defined SIMD instructions and to specialize hardware-oblivious vectorized operators to GPUs using our Template Vector Library (TVL) in this paper.",
        "link": "https://dl.acm.org/doi/10.1145/3465998.3466015",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "When wearable technology meets computing in future networks: a road ahead",
        "authors": "['Aleksandr Ometov', 'Olga Chukhno', 'Nadezhda Chukhno', 'Jari Nurmi', 'Elena Simona Lohan']",
        "date": "May 2021",
        "source": "CF '21: Proceedings of the 18th ACM International Conference on Computing Frontiers",
        "abstract": "Rapid technology advancement, economic growth, and industrialization have paved the way for developing a new niche of small body-worn personal devices, gathered together under a wearable-technology title. The triggers stimulated by end-users interest have introduced the first generation of mass-consumer wearables in just the past decade. Evidently, the trailblazing ones were not designed with strict energy-consumption restrictions in mind. Thus, wearable-computing-related research remained fragmented. Advanced and sophisticated batteries and communication technologies could be already procurable on devices. Additional solutions for efficient utilization of processing power are still a white spot on the wearable technology roadmap. A-WEAR EU project aims to enhance the understanding of how the superimposition of those technologies would improve wearable devices' energy efficiency, with the research area being far from saturation. We foresee enormous room for research as the Edge computing paradigm is emerging towards hand-held devices.",
        "link": "https://dl.acm.org/doi/10.1145/3457388.3458614",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "CO2 Meter: A do-it-yourself carbon dioxide measuring device for the classroom",
        "authors": "['Thomas Dey', 'Ingo Elsen', 'Alexander Ferrein', 'Tobias Frauenrath', 'Michael Reke', 'Stefan Schiffer']",
        "date": "June 2021",
        "source": "PETRA '21: Proceedings of the 14th PErvasive Technologies Related to Assistive Environments Conference",
        "abstract": "In this paper we report on CO2 Meter, a do-it-yourself carbon dioxide measuring device for the classroom. Part of the current measures for dealing with the SARS-CoV-2 pandemic is proper ventilation in indoor settings. This is especially important in schools with students coming back to the classroom even with high incidents rates. Static ventilation patterns do not consider the individual situation for a particular class. Influencing factors like the type of activity, the physical structure or the room occupancy are not incorporated. Also, existing devices are rather expensive and often provide only limited information and only locally without any networking. This leaves the potential of analysing the situation across different settings untapped. Carbon dioxide level can be used as an indicator of air quality, in general, and of aerosol load in particular. Since, according to the latest findings, SARS-CoV-2 can be transmitted primarily in the form of aerosols, carbon dioxide may be used as a proxy for the risk of a virus infection. Hence, schools could improve the indoor air quality and potentially reduce the infection risk if they actually had measuring devices available in the classroom. Our device supports schools in ventilation and it allows for collecting data over the Internet to enable a detailed data analysis and model generation. First deployments in schools at different levels were received very positively. A pilot installation with a larger data collection and analysis is underway.",
        "link": "https://dl.acm.org/doi/10.1145/3453892.3462697",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Writing P4 compiler backend for packet processing engines",
        "authors": "['Balachandher Sambasivam', 'Maheswari Subramanian', 'Deb Chatterjee', 'Mallikarjuna Gouda', 'Sosutha Sethuramapandian', 'Yogender Singh Saroha']",
        "date": "December 2021",
        "source": "ANCS '21: Proceedings of the Symposium on Architectures for Networking and Communications Systems",
        "abstract": "The advent of P4 as a protocol-independent and platform-independent network packet processing language has revolutionized the way networks are designed and the way networking devices are programmed. There are few programmable devices, whether ASICs or FPGA-based devices, that are designed with P4 programmability as the end goal right from the beginning. As a consequence, although these packet processing engines are programmable, writing a P4 compiler for these targets requires overcoming some technical challenges. Our team has worked on a variety of packet processing pipelines in recent years, in this article, we are presenting some of these challenges as well as the solutions we found to work around them.",
        "link": "https://dl.acm.org/doi/10.1145/3493425.3502769",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Research on Full-Chip Programming for Sunway Heterogeneous Many-core Processor",
        "authors": "['Wei Wu', 'Hong Qian', 'Qi Zhu', 'Jue Wang', 'XingJian Fan']",
        "date": "September 2021",
        "source": "WSSE '21: Proceedings of the 3rd World Symposium on Software Engineering",
        "abstract": "Programming on many-core processors is a challenging task. It's a difficult topic to program and compile on heterogeneous many-core architectures in high-performance computing area. The bottom-level programming support on Sunway many-core processors is insufficient and can hardly satisfy the growing need in applications. This paper conducts an thread accelerated programming model and a multi-mode accelerating thread library on Sunway architectures. A fast thread group mode is proposed, which can reduce the consumption of thread spawn and join according to hardware's features. Compared to GPU, it achieves a speedup of 24.88. We also designs a globally sharing executing mode, which supports many-core acceleration programming from the view of full-chip. Evaluation results on the Parboil benchmark shows that, the average performance is 1.16 times as NVIDIA V100 GPU.",
        "link": "https://dl.acm.org/doi/10.1145/3488838.3488868",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "MIND: In-Network Memory Management for Disaggregated Data Centers",
        "authors": "['Seung-seob Lee', 'Yanpeng Yu', 'Yupeng Tang', 'Anurag Khandelwal', 'Lin Zhong', 'Abhishek Bhattacharjee']",
        "date": "October 2021",
        "source": "SOSP '21: Proceedings of the ACM SIGOPS 28th Symposium on Operating Systems Principles",
        "abstract": "Memory disaggregation promises transparent elasticity, high resource utilization and hardware heterogeneity in data centers by physically separating memory and compute into network-attached resource \"blades\". However, existing designs achieve performance at the cost of resource elasticity, restricting memory sharing to a single compute blade to avoid costly memory coherence traffic over the network. In this work, we show that emerging programmable network switches can enable an efficient shared memory abstraction for disaggregated architectures by placing memory management logic in the network fabric. We find that centralizing memory management in the network permits bandwidth and latency-efficient realization of in-network cache coherence protocols, while programmable switch ASICs support other memory management logic at line-rate. We realize these insights into MIND1, an in-network memory management unit for rack-scale disaggregation. MIND enables transparent resource elasticity while matching the performance of prior memory disaggregation proposals for real-world workloads.",
        "link": "https://dl.acm.org/doi/10.1145/3477132.3483561",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Finding and Finessing Static Islands in Dynamically Scheduled Circuits",
        "authors": "['Jianyi Cheng', 'John Wickerson', 'George A. Constantinides']",
        "date": "February 2022",
        "source": "FPGA '22: Proceedings of the 2022 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays",
        "abstract": "In high-level synthesis, scheduling is the process that determines the start time of each operation in hardware. A hardware design can be scheduled either at compile time (static), run time (dynamic), or both. Recent research has shown that combining dynamic and static scheduling can achieve high performance and small area. However, there is still a challenge to determine which part to schedule statically and which part dynamically. An inappropriate choice can lead to suboptimal design quality. This paper proposes a heuristic-driven approach to automatically determine 'static islands' - i.e., code regions that are amenable for static scheduling. Over a set of benchmarks where our approach is applicable, we show that our tool can achieve on average a 3.8-fold reduction in area combined with a 13% performance boost through automatic identification and synthesis of static islands from fully dynamically scheduled circuits. The performance of the resulting hardware is close to optimum (as determined by an exhaustive enumeration of all possible static islands).",
        "link": "https://dl.acm.org/doi/10.1145/3490422.3502362",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "SumMerge: an efficient algorithm and implementation for weight repetition-aware DNN inference",
        "authors": "['Rohan Baskar Prabhakar', 'Sachit Kuhar', 'Rohit Agrawal', 'Christopher J. Hughes', 'Christopher W. Fletcher']",
        "date": "June 2021",
        "source": "ICS '21: Proceedings of the ACM International Conference on Supercomputing",
        "abstract": "Deep Neural Network (DNN) inference efficiency is a key concern across the myriad of domains now relying on Deep Learning. A recent promising direction to speed-up inference is to exploit \\emph{weight repetition}. The key observation is that due to DNN quantization schemes---which attempt to reduce DNN storage requirements by reducing the number of bits needed to represent each weight---the same weight is bound to repeat many times within and across filters. This enables a weight-repetition aware inference kernel to factorize and memoize out common sub-computations, reducing arithmetic per inference while still maintaining the compression benefits of quantization. Yet, significant challenges remain. For instance, weight repetition introduces significant irregularity in the inference operation and hence (up to this point) has required custom hardware accelerators to derive net benefit. This paper proposes SumMerge: a new algorithm and set of implementation techniques to make weight repetition practical on general-purpose devices such as CPUs. The key idea is to formulate inference as traversing a sequence of data-flow graphs \\emph{with weight-dependent structure}. We develop an offline heuristic to select a data-flow graph structure that minimizes arithmetic operations per inference (given trained weight values) and use an efficient online procedure to traverse each data-flow graph and compute the inference result given DNN inputs. We implement the above as an optimized C++ routine that runs on a commercial multicore processor with vector extensions and evaluate performance relative to Intel's optimized library oneDNN and the prior-art weight repetition algorithm (AGR). When applied on top of six different quantization schemes, SumMerge achieves a speedup of between 1.09x-2.05x and 1.04x-1.51x relative to oneDNN and AGR, respectively, while simultaneously compressing the DNN model by 8.7x to 15.4x.",
        "link": "https://dl.acm.org/doi/10.1145/3447818.3460375",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Built-in Self-Test and Fault Localization for Inter-Layer Vias in Monolithic 3D ICs",
        "authors": "['Arjun Chaudhuri', 'Sanmitra Banerjee', 'Jinwoo Kim', 'Heechun Park', 'Bon Woong Ku', 'Sukeshwar Kannan', 'Krishnendu Chakrabarty', 'Sung Kyu Lim']",
        "date": "None",
        "source": "ACM Journal on Emerging Technologies in Computing Systems",
        "abstract": "Monolithic 3D (M3D) integration provides massive vertical integration through the use of nanoscale inter-layer vias (ILVs). However, high integration density and aggressive scaling of the inter-layer dielectric make ILVs especially prone to defects. We present a low-cost built-in self-test (BIST) method that requires only two test patterns to detect opens, stuck-at faults, and bridging faults (shorts) in ILVs. We also propose an extended BIST architecture for fault detection, called Dual-BIST, to guarantee zero ILV fault masking due to single BIST faults and negligible ILV fault masking due to multiple BIST faults. We analyze the impact of coupling between adjacent ILVs arranged in a 1D array in block-level partitioned designs. Based on this analysis, we present a novel test architecture called Shared-BIST with the added functionality of localizing single and multiple faults, including coupling-induced faults. We introduce a systematic clustering-based method for designing and integrating a delay bank with the Shared-BIST architecture for testing small-delay defects in ILVs with minimal yield loss. Simulation results for four two-tier M3D benchmark designs highlight the effectiveness of the proposed BIST framework.",
        "link": "https://dl.acm.org/doi/10.1145/3464430",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Optimizing large-scale plasma simulations on persistent memory-based heterogeneous memory with effective data placement across memory hierarchy",
        "authors": "['Jie Ren', 'Jiaolin Luo', 'Ivy Peng', 'Kai Wu', 'Dong Li']",
        "date": "June 2021",
        "source": "ICS '21: Proceedings of the ACM International Conference on Supercomputing",
        "abstract": "Particle simulations of plasma are important for understanding plasma dynamics in space weather and fusion devices. However, production simulations that use billions and even trillions of computational particles require high memory capacity. In this work, we explore the latest persistent memory (PM) hardware to enable large-scale plasma simulations at unprecedented scales on a single machine. We use WarpX, an advanced plasma simulation code which is mission-critical and targets future exascale systems. We analyze the performance of WarpX on PM-based heterogeneous memory systems and propose to make the best use of memory hierarchy to avoid the impact of inferior performance of PM. We introduce a combination of static and dynamic data placement, and processor-cache prefetch mechanism for performance optimization. We develop a performance model to enable efficient data migration between PM and DRAM in the background, without reducing available bandwidth and parallelism to the application threads. We also build an analytical model to decide when to prefetch for the best use of caches. Our design achieves 66.4% performance improvement over the PM-only baseline and outperforms DRAM-cached, NUMA first-touch, and a state-of-the-art software solution by 38.8%, 45.1% and 83.3%, respectively.",
        "link": "https://dl.acm.org/doi/10.1145/3447818.3460356",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Elk Audio OS: An Open Source Operating System for the Internet of Musical Things",
        "authors": "['Luca Turchet', 'Carlo Fischione']",
        "date": "None",
        "source": "ACM Transactions on Internet of Things",
        "abstract": "As the Internet of Musical Things (IoMusT) emerges, audio-specific operating systems (OSs) are required on embedded hardware to ease development and portability of IoMusT applications. Despite the increasing importance of IoMusT applications, in this article, we show that there is no OS able to fulfill the diverse requirements of IoMusT systems. To address such a gap, we propose the Elk Audio OS as a novel and open source OS in this space. It is a Linux-based OS optimized for ultra-low-latency and high-performance audio and sensor processing on embedded hardware, as well as for handling wireless connectivity to local and remote networks. Elk Audio OS uses the Xenomai real-time kernel extension, which makes it suitable for the most demanding of low-latency audio tasks. We provide the first comprehensive overview of Elk Audio OS, describing its architecture and the key components of interest to potential developers and users. We explain operational aspects like the configuration of the architecture and the control mechanisms of the internal sound engine, as well as the tools that enable an easier and faster development of connected musical devices. Finally, we discuss the implications of Elk Audio OS, including the development of an open source community around it.",
        "link": "https://dl.acm.org/doi/10.1145/3446393",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Stream-AI-MD: streaming AI-driven adaptive molecular simulations for heterogeneous computing platforms",
        "authors": "['Alexander Brace', 'Michael Salim', 'Vishal Subbiah', 'Heng Ma', 'Murali Emani', 'Anda Trifa', 'Austin R. Clyde', 'Corey Adams', 'Thomas Uram', 'Hyunseung Yoo', 'Andew Hock', 'Jessica Liu', 'Venkatram Vishwanath', 'Arvind Ramanathan']",
        "date": "July 2021",
        "source": "PASC '21: Proceedings of the Platform for Advanced Scientific Computing Conference",
        "abstract": "Emerging hardware tailored for artificial intelligence (AI) and machine learning (ML) methods provide novel means to couple them with traditional high performance computing (HPC) workflows involving molecular dynamics (MD) simulations. We propose Stream-AI-MD, a novel instance of applying deep learning methods to drive adaptive MD simulation campaigns in a streaming manner. We leverage the ability to run ensemble MD simulations on GPU clusters, while the data from atomistic MD simulations are streamed continuously to AI/ML approaches to guide the conformational search in a biophysically meaningful manner on a wafer-scale AI accelerator. We demonstrate the efficacy of Stream-AI-MD simulations for two scientific use-cases: (1) folding a small prototypical protein, namely ββα-fold (BBA) FSD-EY and (2) understanding protein-protein interaction (PPI) within the SARS-CoV-2 proteome between two proteins, nsp16 and nsp10. We show that Stream-AI-MD simulations can improve time-to-solution by ~50X for BBA protein folding. Further, we also discuss performance trade-offs involved in implementing AI-coupled HPC workflows on heterogeneous computing architectures.",
        "link": "https://dl.acm.org/doi/10.1145/3468267.3470578",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "ShuffleFL: gradient-preserving federated learning using trusted execution environment",
        "authors": "['Yuhui Zhang', 'Zhiwei Wang', 'Jiangfeng Cao', 'Rui Hou', 'Dan Meng']",
        "date": "May 2021",
        "source": "CF '21: Proceedings of the 18th ACM International Conference on Computing Frontiers",
        "abstract": "Federated Learning (FL) is a promising approach to privacy-preserving machine learning. However, recent works reveal that gradients can leak private data. Using trusted SGX-processors for this task yields gradient-preserving but requires to prevent exploitation of any side-channel attacks. In this work, we present ShuffleFL, a gradient-preserving system using trusted SGX, which combines random group structure and intra-group gradient segment aggregation for combating any side-channel attacks. We analyze the security of our system against semi-honest adversaries. ShuffleFL effectively guarantees the participants' gradient privacy. We demonstrate the performance of ShuffleFL and show its applicability in the federated learning system.",
        "link": "https://dl.acm.org/doi/10.1145/3457388.3458665",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Building blocks for redundancy-free vector integer multiplication",
        "authors": "['James You', 'Christopher W. Schankula', \"Bill O'Farrell\", 'Christopher K. Anand']",
        "date": "November 2021",
        "source": "CASCON '21: Proceedings of the 31st Annual International Conference on Computer Science and Software Engineering",
        "abstract": "Commercial applications of cryptography require arithmetic in prime fields with primes larger than the sizes of architected registers, and over time there will be pressure to use even larger fields to keep up with the increasing resources available for brute-force attacks and the threat that quantum computers will reach the power required for unconventional attacks. Integer multiplication is the bottleneck for most computations, and most algorithm innovations revolve around strategic composition of efficient hardware multipliers for smaller integers into algorithms for larger integer multiplication. In this paper we present an novel vector instruction which would allow hardware multipliers to be used optimally for school-book multiplication by flexibly grouping multiplications to avoid empty slots in vector instructions resulting in unused hardware capacity. We give general conditions for optimality, consider latency/throughput tradeoffs and optionally pair the new instruction, mammma, with a novel shift-and-sum instruction.",
        "link": "https://dl.acm.org/doi/10.5555/3507788.3507819",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Precise Cache Profiling for Studying Radiation Effects",
        "authors": "['James Marshall', 'Robert Gifford', 'Gedare Bloom', 'Gabriel Parmer', 'Rahul Simha']",
        "date": "None",
        "source": "ACM Transactions on Embedded Computing Systems",
        "abstract": "Increased access to space has led to an increase in the usage of commodity processors in radiation environments. These processors are vulnerable to transient faults such as single event upsets that may cause bit-flips in processor components. Caches in particular are vulnerable due to their relatively large area, yet are often omitted from fault injection testing because many processors do not provide direct access to cache contents and they are often not fully modeled by simulators. The performance benefits of caches make disabling them undesirable, and the presence of error correcting codes is insufficient to correct for increasingly common multiple bit upsets.This work explores building a program’s cache profile by collecting cache usage information at an instruction granularity via commonly available on-chip debugging interfaces. The profile provides a tighter bound than cache utilization for cache vulnerability estimates (50% for several benchmarks). This can be applied to reduce the number of fault injections required to characterize behavior by at least two-thirds for the benchmarks we examine. The profile enables future work in hardware fault injection for caches that avoids the biases of existing techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3442339",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Enhancing Privacy in PUF-Cash through Multiple Trusted Third Parties and Reinforcement Learning",
        "authors": "['Georgios Fragkos', 'Cyrus Minwalla', 'Eirini Eleni Tsiropoulou', 'Jim Plusquellic']",
        "date": "None",
        "source": "ACM Journal on Emerging Technologies in Computing Systems",
        "abstract": "Electronic cash (e-Cash) is a digital alternative to physical currency such as coins and bank notes. Suitably constructed, e-Cash has the ability to offer an anonymous offline experience much akin to cash, and in direct contrast to traditional forms of payment such as credit and debit cards. Implementing security and privacy within e-Cash, i.e., preserving user anonymity while preventing counterfeiting, fraud, and double spending, is a non-trivial challenge. In this article, we propose major improvements to an e-Cash protocol, termed PUF-Cash, based on physical unclonable functions (PUFs). PUF-Cash was created as an offline-first, secure e-Cash scheme that preserved user anonymity in payments. In addition, PUF-Cash supports remote payments; an improvement over traditional currency. In this work, a novel multi-trusted-third-party exchange scheme is introduced, which is responsible for “blinding” Alice’s e-Cash tokens; a feature at the heart of preserving her anonymity. The exchange operations are governed by machine learning techniques which are uniquely applied to optimize user privacy, while remaining resistant to identity-revealing attacks by adversaries and trusted authorities. Federation of the single trusted third party into multiple entities distributes the workload, thereby improving performance and resiliency within the e-Cash system architecture. Experimental results indicate that improvements to PUF-Cash enhance user privacy and scalability.",
        "link": "https://dl.acm.org/doi/10.1145/3441139",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "SoK: Enabling Security Analyses of Embedded Systems via Rehosting",
        "authors": "['Andrew Fasano', 'Tiemoko Ballo', 'Marius Muench', 'Tim Leek', 'Alexander Bulekov', 'Brendan Dolan-Gavitt', 'Manuel Egele', 'Aurélien Francillon', 'Long Lu', 'Nick Gregory', 'Davide Balzarotti', 'William Robertson']",
        "date": "May 2021",
        "source": "ASIA CCS '21: Proceedings of the 2021 ACM Asia Conference on Computer and Communications Security",
        "abstract": "Closely monitoring the behavior of a software system during its execution enables developers and analysts to observe, and ultimately understand, how it works. This kind of dynamic analysis can be instrumental to reverse engineering, vulnerability discovery, exploit development, and debugging. While these analyses are typically well-supported for homogeneous desktop platforms (e.g., x86 desktop PCs), they can rarely be applied in the heterogeneous world of embedded systems. One approach to enable dynamic analyses of embedded systems is to move software stacks from physical systems into virtual environments that sufficiently model hardware behavior. This process which we call \"rehosting\" poses a significant research challenge with major implications for security analyses. Although rehosting has traditionally been an unscientific and ad-hoc endeavor undertaken by domain experts with varying time and resources at their disposal, researchers are beginning to address rehosting challenges systematically and in earnest. In this paper, we establish that emulation is insufficient to conduct large-scale dynamic analysis of real-world hardware systems and present rehosting as a firmware-centric alternative. Furthermore, we taxonomize preliminary rehosting efforts, identify the fundamental components of the rehosting process, and propose directions for future research.",
        "link": "https://dl.acm.org/doi/10.1145/3433210.3453093",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Adaptive brokerage framework for the cloud with functional testing",
        "authors": "['Sheriffo Ceesay', 'Yuhui Lin', 'Adam Barker']",
        "date": "December 2021",
        "source": "UCC '21: Proceedings of the 14th IEEE/ACM International Conference on Utility and Cloud Computing Companion",
        "abstract": "In this paper, we present an Adaptive Brokerage for the Cloud (ABC) that can be used to simplify application deployment, monitoring and management processes in the cloud. The broker uses modern cloud infrastructure automation tools to test, deploy, monitor and optimise cloud resources. We used an e-commerce application to evaluate the entire functionality of the broker, we found out that different deployment options such as single-tier vs two-tier lead to interesting hardware and application performance insights. These insights are used to make effective infrastructure optimisation decisions.",
        "link": "https://dl.acm.org/doi/10.1145/3492323.3495624",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "S-Vectors and TESA: Speaker Embeddings and a Speaker Authenticator Based on Transformer Encoder",
        "authors": "['Narla John Metilda Sagaya Mary', 'Srinivasan Umesh', 'Sandesh Varadaraju Katta']",
        "date": "None",
        "source": "IEEE/ACM Transactions on Audio, Speech and Language Processing",
        "abstract": "One of the most popular speaker embeddings is x-vectors, which are obtained from an architecture that gradually builds a larger temporal context with layers. In this paper, we propose to derive speaker embeddings from Transformer&#x2019;s encoder trained for speaker classification. Self-attention, on which Transformer&#x2019;s encoder is built, attends to all the features over the entire utterance and might be more suitable in capturing the speaker characteristics in an utterance. We refer to the speaker embeddings obtained from the proposed speaker classification model as s-vectors to emphasize that they are obtained from an architecture that heavily relies on self-attention. Through experiments, we demonstrate that s-vectors perform better than x-vectors. In addition to the s-vectors, we also propose a new architecture based on Transformer&#x2019;s encoder for speaker verification as a replacement for speaker verification based on conventional probabilistic linear discriminant analysis (PLDA). This architecture is inspired by the next sentence prediction task of bidirectional encoder representations from Transformers (BERT), and we feed the s-vectors of two utterances to verify whether they belong to the same speaker. We name this architecture the Transformer encoder speaker authenticator (TESA). Our experiments show that the performance of s-vectors with TESA is better than s-vectors with conventional PLDA-based speaker verification.",
        "link": "https://dl.acm.org/doi/10.1109/TASLP.2021.3134566",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Lane Compression: A Lightweight Lossless Compression Method for Machine Learning on Embedded Systems",
        "authors": "['Yousun Ko', 'Alex Chadwick', 'Daniel Bates', 'Robert Mullins']",
        "date": "None",
        "source": "ACM Transactions on Embedded Computing Systems",
        "abstract": "This article presents Lane Compression, a lightweight lossless compression technique for machine learning that is based on a detailed study of the statistical properties of machine learning data. The proposed technique profiles machine learning data gathered ahead of run-time and partitions values bit-wise into different lanes with more distinctive statistical characteristics. Then the most appropriate compression technique is chosen for each lane out of a small number of low-cost compression techniques. Lane Compression’s compute and memory requirements are very low and yet it achieves a compression rate comparable to or better than Huffman coding. We evaluate and analyse Lane Compression on a wide range of machine learning networks for both inference and re-training. We also demonstrate the profiling prior to run-time and the ability to configure the hardware based on the profiling guarantee robust performance across different models and datasets. Hardware implementations are described and the scheme’s simplicity makes it suitable for compressing both on-chip and off-chip traffic.",
        "link": "https://dl.acm.org/doi/10.1145/3431815",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Reducing solid-state drive read latency by optimizing read-retry",
        "authors": "['Jisung Park', 'Myungsuk Kim', 'Myoungjun Chun', 'Lois Orosa', 'Jihong Kim', 'Onur Mutlu']",
        "date": "April 2021",
        "source": "ASPLOS '21: Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "3D NAND flash memory with advanced multi-level cell techniques provides high storage density, but suffers from significant performance degradation due to a large number of read-retry operations. Although the read-retry mechanism is essential to ensuring the reliability of modern NAND flash memory, it can significantly in-crease the read latency of an SSD by introducing multiple retry steps that read the target page again with adjusted read-reference voltage values. Through a detailed analysis of the read mechanism and rigorous characterization of 160 real 3D NAND flash memory chips, we find new opportunities to reduce the read-retry latency by exploiting two advanced features widely adopted in modern NAND flash-based SSDs: 1) the CACHE READ command and 2) strong ECC engine. First, we can reduce the read-retry latency using the advanced CACHE READ command that allows a NAND flash chip to perform consecutive reads in a pipelined manner. Second, there exists a large ECC-capability margin in the final retry step that can be used for reducing the chip-level read latency. Based on our new findings, we develop two new techniques that effectively reduce the read-retry latency: 1) Pipelined Read-Retry (PR²) and 2) Adaptive Read-Retry (AR²). PR² reduces the latency of a read-retry operation by pipelining consecutive retry steps using the CACHE READ command. AR² shortens the latency of each retry step by dynamically reducing the chip-level read latency depending on the current operating conditions that determine the ECC-capability margin. Our evaluation using twelve real-world workloads shows that our proposal improves SSD response time by up to 31.5% (17% on average)over a state-of-the-art baseline with only small changes to the SSD controller.",
        "link": "https://dl.acm.org/doi/10.1145/3445814.3446719",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Sensor Virtualization for Efficient Sharing of Mobile and Wearable Sensors",
        "authors": "['Jian Xu', 'Arani Bhattacharya', 'Aruna Balasubramanian', 'Donald E. Porter']",
        "date": "November 2021",
        "source": "SenSys '21: Proceedings of the 19th ACM Conference on Embedded Networked Sensor Systems",
        "abstract": "Users are surrounded by sensors that are available through various devices beyond their smartphones. However, these sensors are not fully utilized by current end-user applications. A key reason sensor use is so limited is that application developers must exactly identify how the sensor data can be used by smartphone apps. To mitigate this problem, we present SenseWear, a sensor-sharing platform that extends the functionality of a smartphone to use remote sensors with limited additional developer effort. Sensor sharing has several uses, including augmenting the hardware in smartphones, creating new gestural interactions with smartphone applications, and improving application's Quality of Experience via higher-quality sensors from other devices, such as wearables. We developed and present six use cases that use remote sensors in various smartphone applications. Each extension requires adding fewer than 20 lines of code on average. Furthermore, using remote sensors did not introduce a perceptible increase in latency, and creates more convenient interaction options for smartphone apps.",
        "link": "https://dl.acm.org/doi/10.1145/3485730.3493451",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Cores that don't count",
        "authors": "['Peter H. Hochschild', 'Paul Turner', 'Jeffrey C. Mogul', 'Rama Govindaraju', 'Parthasarathy Ranganathan', 'David E. Culler', 'Amin Vahdat']",
        "date": "June 2021",
        "source": "HotOS '21: Proceedings of the Workshop on Hot Topics in Operating Systems",
        "abstract": "We are accustomed to thinking of computers as fail-stop, especially the cores that execute instructions, and most system software implicitly relies on that assumption. During most of the VLSI era, processors that passed manufacturing tests and were operated within specifications have insulated us from this fiction. As fabrication pushes towards smaller feature sizes and more elaborate computational structures, and as increasingly specialized instruction-silicon pairings are introduced to improve performance, we have observed ephemeral computational errors that were not detected during manufacturing tests. These defects cannot always be mitigated by techniques such as microcode updates, and may be correlated to specific components within the processor, allowing small code changes to effect large shifts in reliability. Worse, these failures are often \"silent\" - the only symptom is an erroneous computation. We refer to a core that develops such behavior as \"mercurial.\" Mercurial cores are extremely rare, but in a large fleet of servers we can observe the disruption they cause, often enough to see them as a distinct problem - one that will require collaboration between hardware designers, processor vendors, and systems software architects. This paper is a call-to-action for a new focus in systems research; we speculate about several software-based approaches to mercurial cores, ranging from better detection and isolating mechanisms, to methods for tolerating the silent data corruption they cause.",
        "link": "https://dl.acm.org/doi/10.1145/3458336.3465297",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Request, Coalesce, Serve, and Forget: Miss-Optimized Memory Systems for Bandwidth-Bound Cache-Unfriendly Applications on FPGAs",
        "authors": "['Mikhail Asiatici', 'Paolo Ienne']",
        "date": "None",
        "source": "ACM Transactions on Reconfigurable Technology and Systems",
        "abstract": "Applications such as large-scale sparse linear algebra and graph analytics are challenging to accelerate on FPGAs due to the short irregular memory accesses, resulting in low cache hit rates. Nonblocking caches reduce the bandwidth required by misses by requesting each cache line only once, even when there are multiple misses corresponding to it. However, such reuse mechanism is traditionally implemented using an associative lookup. This limits the number of misses that are considered for reuse to a few tens, at most. In this article, we present an efficient pipeline that can process and store thousands of outstanding misses in cuckoo hash tables in on-chip SRAM with minimal stalls. This brings the same bandwidth advantage as a larger cache for a fraction of the area budget, because outstanding misses do not need a data array, which can significantly speed up irregular memory-bound latency-insensitive applications. In addition, we extend nonblocking caches to generate variable-length bursts to memory, which increases the bandwidth delivered by DRAMs and their controllers. The resulting miss-optimized memory system provides up to 25% speedup with 24× area reduction on 15 large sparse matrix-vector multiplication benchmarks evaluated on an embedded and a datacenter FPGA system.",
        "link": "https://dl.acm.org/doi/10.1145/3466823",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Improving railway track coverage with mmWave bridges: A Measurement Campaign",
        "authors": "['Adrian Schumacher', 'Nima Jamaly', 'Ruben Merz', 'Andreas Burg']",
        "date": "August 2021",
        "source": "5G-MeMU '21: Proceedings of the 1st Workshop on 5G Measurements, Modeling, and Use Cases",
        "abstract": "Bringing cellular capacity into modern trains is challenging because they act as Faraday cages. Building a radio frequency (RF) corridor along the railway tracks ensures a high signal-to-noise ratio and limits handovers. However, building such RF corridors is difficult because of the administrative burden of excessive formalities to obtain construction permissions and costly because of the sheer number of base stations. Our contribution in this paper is an unconventional solution of mmWave fronthauled low-power out-of-band repeater nodes deployed in short intervals on existing masts between high-power macro cell sites. The paper demonstrates the feasibility of the concept with an extensive measurement campaign on a commercial railway line. The benefit of using many low-power nodes with low-gain antennas compared to a baseline with only high-gain macro antennas is discussed, and the coverage improvement is evaluated. Based on the measurement results, a simple path loss model is calibrated. This model allows evaluation of the potential of the mmWave repeater architecture to increase the macro cell inter-site distance and reduce deployment costs.",
        "link": "https://dl.acm.org/doi/10.1145/3472771.3472774",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "ProMT: optimizing integrity tree updates for write-intensive pages in secure NVMs",
        "authors": "['Mazen Alwadi', 'Aziz Mohaisen', 'Amro Awad']",
        "date": "June 2021",
        "source": "ICS '21: Proceedings of the ACM International Conference on Supercomputing",
        "abstract": "Current computer systems are vulnerable to a wide range of attacks caused by the proliferation of accelerators, and the fact that current system comprise multiple SoCs provided from different vendors. Thus, major processor vendors are moving towards limiting the trust boundary to the processor chip only as in Intel's SGX, AMD's SME, and ARM's TrustZone. This secure boundary limitation requires protecting the memory content against data remanence attacks, which were performed against DRAM in the form of cold-boot attack and are more successful against NVM due to NVM's data persistency feature. However, implementing secure memory features, such as memory encryption and integrity verification has a non-trivial performance overhead, and can significantly reduce the emerging NVM's expected lifetime. Previous work looked at reducing the overheads of the secure memory implementation by packing more counters into a cache line, increasing the cacheability of security metadata, slightly reducing the size of the integrity tree, or using the ECC chip to store the MAC values. However, the root update process is barely studied, which requires a sequential update of the MAC values in all the integrity tree levels. In this paper, we propose ProMT, a novel memory controller design that ensures a persistently secure system with minimal overheads. ProMT protects the data confidentiality and ensures the data integrity with minimal overheads. ProMT reduces the performance overhead of secure memory implementation to 11.7%, extends the NVM's life time by 3.59x, and enables the system recovery in a fraction of a second.",
        "link": "https://dl.acm.org/doi/10.1145/3447818.3460377",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "TAS: <u>t</u>ernarized neural <u>a</u>rchitecture <u>s</u>earch for resource-constrained edge devices",
        "authors": "['Mohammad Loni', 'Hamid Mousavi', 'Mohammad Riazati', 'Masoud Daneshtalab', 'Mikael Sjödin']",
        "date": "March 2022",
        "source": "DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe",
        "abstract": "Ternary Neural Networks (TNNs) compress network weights and activation functions into 2-bit representation resulting in remarkable network compression and energy efficiency. However, there remains a significant gap in accuracy between TNNs and full-precision counterparts. Recent advances in Neural Architectures Search (NAS) promise opportunities in automated optimization for various deep learning tasks. Unfortunately, this area is unexplored for optimizing TNNs. This paper proposes TAS, a framework that drastically reduces the accuracy gap between TNNs and their full-precision counterparts by integrating quantization into the network design. We experienced that directly applying NAS to the ternary domain provides accuracy degradation as the search settings are customized for full-precision networks. To address this problem, we propose (i) a new cell template for ternary networks with maximum gradient propagation; and (ii) a novel learnable quantizer that adaptively relaxes the ternarization mechanism from the distribution of the weights and activation functions. Experimental results reveal that TAS delivers 2.64% higher accuracy and ≈2.8× memory saving over competing methods with the same bit-width resolution on the CIFAR-10 dataset. These results suggest that TAS is an effective method that paves the way for the efficient design of the next generation of quantized neural networks.",
        "link": "https://dl.acm.org/doi/10.5555/3539845.3540104",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Algorithms for Right-Sizing Heterogeneous Data Centers",
        "authors": "['Susanne Albers', 'Jens Quedenfeld']",
        "date": "July 2021",
        "source": "SPAA '21: Proceedings of the 33rd ACM Symposium on Parallelism in Algorithms and Architectures",
        "abstract": "Power consumption is a dominant and still growing cost factor in data centers. In time periods with low load, the energy consumption can be reduced by powering down unused servers. We resort to a model introduced by Lin, Wierman, Andrew and Thereska (23,24) that considers data centers with identical machines, and generalize it to heterogeneous data centers with d different server types. The operating cost of a server depends on its load and is modeled by an increasing, convex function for each server type. In contrast to earlier work, we consider the discrete setting, where the number of active servers must be integral. Thereby, we seek truly feasible solutions. For homogeneous data centers (d=1), both the offline and the online problem were solved optimally in (3,4)  In this paper, we study heterogeneous data centers with general time-dependent operating cost functions. We develop an online algorithm based on a work function approach which achieves a competitive ratio of 2d + 1 + ε for any ε > 0. For time-independent operating cost functions, the competitive ratio can be reduced to 2d + 1. There is a lower bound of 2d shown in (5), so our algorithm is nearly optimal. For the offline version, we give a graph-based (1+ε)-approximation algorithm. Additionally, our offline algorithm is able to handle time-variable data-center sizes.",
        "link": "https://dl.acm.org/doi/10.1145/3409964.3461789",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Wideband Full-Duplex Phased Array With Joint Transmit and Receive Beamforming: Optimization and Rate Gains",
        "authors": "['Tingjun Chen', 'Mahmood Baraani Dastjerdi', 'Harish Krishnaswamy', 'Gil Zussman']",
        "date": "None",
        "source": "IEEE/ACM Transactions on Networking",
        "abstract": "Full-duplex (FD) wireless and phased arrays are both promising techniques that can significantly improve data rates in future wireless networks. However, integrating FD with transmit (Tx) and receive (Rx) phased arrays is extremely challenging, due to the large number of self-interference (SI) channels. Previous work relies on either RF canceller hardware or on analog/digital Tx beamforming (TxBF) to achieve SI cancellation (SIC). However, Rx beamforming (RxBF) and the data rate gain introduced by FD nodes employing beamforming have not been considered yet. We study FD phased arrays with joint TxBF and RxBF with the objective of achieving improved FD data rates. The key idea is to carefully select the TxBF and RxBF weights to achieve wideband RF SIC in the spatial domain with minimal TxBF and RxBF gain losses. Essentially, TxBF and RxBF are <italic>repurposed</italic>, thereby not requiring specialized RF canceller circuitry. We formulate the corresponding optimization problem and develop an iterative algorithm to obtain an approximate solution with provable performance guarantees. Using SI channel measurements and datasets, we extensively evaluate the performance of the proposed approach in different use cases under various network settings. The results show that an FD phased array with 9/36/72 elements can cancel the total SI power to below the noise floor with sum TxBF and RxBF gain losses of 10.6/7.2/6.9dB, even at Tx power level of 30dBm. Moreover, the corresponding FD rate gains are at least 1.33/1.66/1.68<inline-formula> <tex-math notation=\"LaTeX\">$\\times $ </tex-math></inline-formula>.",
        "link": "https://dl.acm.org/doi/10.1109/TNET.2021.3069125",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Automated Testing of Graphics Units by Deep-Learning Detection of Visual Anomalies",
        "authors": "['Lev Faivishevsky', 'Adi Szeskin', 'Ashwin K. Muppalla', 'Ravid Shwartz-Ziv', 'Itamar Ben Ari', 'Ronen Laperdon', 'Benjamin Melloul', 'Tahi Hollander', 'Tom Hope', 'Amitai Armon']",
        "date": "August 2021",
        "source": "KDD '21: Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "We present a novel system for performing real-time detection of diverse visual corruptions in videos, for validating the quality of graphics units in our company. The system is used for several types of content, including movies and 3D graphics, with strict constraints on low false alert rates and real-time processing of millions of video frames per day. These constraints required novel solutions involving both hardware and software, including new supervised and weakly-supervised methods we developed. Our deployed system has enabled a ~20X reduction of human effort and discovering new corruptions missed by humans and existing approaches.",
        "link": "https://dl.acm.org/doi/10.1145/3447548.3467116",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "SALAD: Static Analyzer for Loop Acceleration by Exploiting DLP",
        "authors": "['Yang Li', 'Xianfeng Li', 'Mingtao Chen', 'Weikang Zhou', 'Fan Deng']",
        "date": "June 2021",
        "source": "HP3C '21: Proceedings of the 5th International Conference on High Performance Compilation, Computing and Communications",
        "abstract": "Data-intensive applications are becoming increasingly popular. However, only a few of them with high volume can afford dedicated hardware acceleration (such as Neural Network Processor, or NPU) or platform-specific software implementation (such as Tensorflow running on GPU). In this paper, we propose a hardware and software transparent framework for the acceleration of general-purpose data-intensive applications. Our framework is based on a key insight that most data-intensive applications spend the vast majority of their execution time on some inner loops with abundant opportunities for Data-Level Parallelism (DLP). In particular, we propose SALAD, a static analyzer for loop acceleration by exploiting DLP in hot loops under the LLVM (LLVM compiler infrastructure) framework. In contrast to traditional DLP exploration techniques, SALAD is both software and architectural transparent, without the need to change either the source code or binary code, and does not need vectorized instruction set architecture (ISA) extensions. Instead, it directly works on the program binary code and generates a profile for DLP opportunities in the binary. This profile will be fed to the hardware accelerator transparently to speed up execution. With the experiments result, we estimate that the DLP information provided by SALAD could result in 3.6x-60.2x speedups on a set of benchmarks, depending on their inherent DLP.",
        "link": "https://dl.acm.org/doi/10.1145/3471274.3471279",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Red Alert for Power Leakage: Exploiting Intel RAPL-Induced Side Channels",
        "authors": "['Zhenkai Zhang', 'Sisheng Liang', 'Fan Yao', 'Xing Gao']",
        "date": "May 2021",
        "source": "ASIA CCS '21: Proceedings of the 2021 ACM Asia Conference on Computer and Communications Security",
        "abstract": "RAPL (Running Average Power Limit) is a hardware feature introduced by Intel to facilitate power management. Even though RAPL and its supporting software interfaces can benefit power management significantly, they are unfortunately designed without taking certain security issues into careful consideration. In this paper, we demonstrate that information leaked through RAPL-induced side channels can be exploited to mount realistic attacks. Specifically, we have constructed a new RAPL-based covert channel using a single AVX instruction, which can exfiltrate data across different boundaries (e.g., those established by containers in software or even CPUs in hardware); and, we have investigated the first RAPL-based website fingerprinting technique that can identify visited webpages with a high accuracy (up to 99% in the case of the regular network using a browser like Chrome or Safari, and up to 81% in the case of the anonymity network using Tor). These two studies form a preliminary examination into RAPL-imposed security implications. In addition, we discuss some possible countermeasures.",
        "link": "https://dl.acm.org/doi/10.1145/3433210.3437517",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Declarative Power Sequencing",
        "authors": "['Jasmin Schult', 'Daniel Schwyn', 'Michael Giardino', 'David Cock', 'Reto Achermann', 'Timothy Roscoe']",
        "date": "None",
        "source": "ACM Transactions on Embedded Computing Systems",
        "abstract": "Modern computer server systems are increasingly managed at a low level by baseboard management controllers (BMCs). BMCs are processors with access to the most critical parts of the platform, below the level of OS or hypervisor, including control over power delivery to every system component. Buggy or poorly designed BMC software not only poses a security threat to a machine, it can permanently render the hardware inoperative. Despite this, there is little published work on how to rigorously engineer the power management functionality of BMCs so as to prevent this happening.This article takes a first step toward putting BMC software on a sound footing by specifying the hardware environment and the constraints necessary for safe and correct operation. This is best accomplished through automation: correct-by-construction power control sequences can be efficiently generated from a simple, trustworthy model of the platform’s power tree that incorporates the sequencing requirements and safe voltage ranges of all components.We present both a modeling language for complex power-delivery networks and a tool to automatically generate safe, efficient power sequences for complex modern platforms. This not only increases the trustworthiness of a hitherto opaque yet critical element of platform firmware: regulator and chip power models are significantly simpler to produce than hand-written power sequences. This, combined with model reuse for common components, reduces both time and cost associated with platform bring-up for new hardware.We evaluate our tool using a new high-performance 2-socket server platform with >100W per socket TDP, tight voltage limits and 25 distinct power regulators needing configuration, showing both fast (<10s) tool runtime, and correct power sequencing of a live system.",
        "link": "https://dl.acm.org/doi/10.1145/3477039",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Archytas: A Framework for Synthesizing and Dynamically Optimizing Accelerators for Robotic Localization",
        "authors": "['Weizhuang Liu', 'Bo Yu', 'Yiming Gan', 'Qiang Liu', 'Jie Tang', 'Shaoshan Liu', 'Yuhao Zhu']",
        "date": "October 2021",
        "source": "MICRO '21: MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture",
        "abstract": "Despite many recent efforts, accelerating robotic computing is still fundamentally challenging for two reasons. First, robotics software stack is extremely complicated. Manually designing an accelerator while meeting the latency, power, and resource specifications is unscalable. Second, the environment in which an autonomous machine operates constantly changes; a static accelerator design leads to wasteful computation.  This paper takes a first step in tackling these two challenges using localization as a case study. We describe, a framework that automatically generates a synthesizable accelerator from the high-level algorithm description while meeting design constraints. The accelerator continuously optimizes itself at run time according to the operating environment to save power while sustaining performance and accuracy. is able to generate FPGA-based accelerator designs that cover large a design space and achieve orders of magnitude performance improvement and/or energy savings compared to state-of-the-art baselines.",
        "link": "https://dl.acm.org/doi/10.1145/3466752.3480077",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Macchiato: Importing Cache Side Channels to SDNs",
        "authors": "['Amir Sabzi', 'Liron Schiff', 'Kashyap Thimmaraju', 'Andreas Blenk', 'Stefan Schmid']",
        "date": "December 2021",
        "source": "ANCS '21: Proceedings of the Symposium on Architectures for Networking and Communications Systems",
        "abstract": "Since caches are shared and coherent, a memory access of one process may evict from the cache another process' memory block with an address mapped to the same cache line. This property is exploited by several attacks to form side channels. We show that MAC learning in Software Defined Networks (SDNs) has a similar property in the sense that a MAC address discovered by one network device may be revoked by the discovery of the same address at another switch. This allows us to implement Macchiato, a covert channel for SDNs between any two network devices (including hosts); prior SDN covert channels required at least one malicious switch. We evaluate a prototype implementation of Macchiato and discuss how methods to improve the performance of cache side channels (such as deep neural networks) can also be used in Macchiato.",
        "link": "https://dl.acm.org/doi/10.1145/3493425.3502758",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Distance learning of electronic engineering based on Arduino platform",
        "authors": "['Alipbay Mansur-Matritdinovich Dairbayev', 'Bahytzhan Sergeevich Baikenov', 'Yevgeniya Alexandrovna Daineko', 'Madina Tolegenovna Ipalakova']",
        "date": "October 2021",
        "source": "ICEMIS'21: The 7th International Conference on Engineering &amp; MIS 2021",
        "abstract": "The principles of learning based on the Arduino platform, its features and development prospects are considered. Examples of using distance learning methods based on the Arduino platform, advantages and disadvantages of teaching methods are given. An effective, hybrid method of distance learning of electronic equipment on this platform is proposed. The laboratory work developed according to the proposed method of distance learning is presented.",
        "link": "https://dl.acm.org/doi/10.1145/3492547.3492644",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "The last CPU",
        "authors": "['Joel Nider', 'Alexandra (Sasha) Fedorova']",
        "date": "June 2021",
        "source": "HotOS '21: Proceedings of the Workshop on Hot Topics in Operating Systems",
        "abstract": "Since the end of Dennard scaling and Moore's Law have been foreseen, specialized hardware has become the focus for continued scaling of application performance. Programmable accelerators such as smart memory, smart disks, and smart NICs are now being integrated into our systems. Many accelerators can be programmed to process their data autonomously and require little or no intervention during normal operation. In this way, entire applications are offloaded, leaving the CPU with the minimal responsibilities of initialization, coordination and error handling. We claim that these responsibilities can also be handled in simple hardware other than the CPU and that it is wasteful to use a CPU for these purposes. We explore the role and the structure of the OS in a system that has no CPU and demonstrate that all necessary functionality can be moved to other hardware. We show that almost all of the pieces for such a system design are already available today. The responsibilities of the operating system must be split between self-managing devices and a system bus that handles privileged operations.",
        "link": "https://dl.acm.org/doi/10.1145/3458336.3465291",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Fortifying Vehicular Security through Low Overhead Physically Unclonable Functions",
        "authors": "['Carson Labrado', 'Himanshu Thapliyal', 'Saraju P. Mohanty']",
        "date": "None",
        "source": "ACM Journal on Emerging Technologies in Computing Systems",
        "abstract": "Within vehicles, the Controller Area Network (CAN) allows efficient communication between the electronic control units (ECUs) responsible for controlling the various subsystems. The CAN protocol was not designed to include much support for secure communication. The fact that so many critical systems can be accessed through an insecure communication network presents a major security concern. Adding security features to CAN is difficult due to the limited resources available to the individual ECUs and the costs that would be associated with adding the necessary hardware to support any additional security operations without overly degrading the performance of standard communication. Replacing the protocol is another option, but it is subject to many of the same problems. The lack of security becomes even more concerning as vehicles continue to adopt smart features. Smart vehicles have a multitude of communication interfaces an attacker could exploit to gain access to the networks. In this work, we propose a security framework that is based on physically unclonable functions (PUFs) and lightweight cryptography (LWC). The framework does not require any modification to the standard CAN protocol while also minimizing the amount of additional message overhead required for its operation. The improvements in our proposed framework result in major reduction in the number of CAN frames that must be sent during operation. For a system with 20 ECUs, for example, our proposed framework only requires 6.5% of the number of CAN frames that is required by the existing approach to successfully authenticate every ECU.",
        "link": "https://dl.acm.org/doi/10.1145/3442443",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Octopus+: An RDMA-Enabled Distributed Persistent Memory File System",
        "authors": "['Bohong Zhu', 'Youmin Chen', 'Qing Wang', 'Youyou Lu', 'Jiwu Shu']",
        "date": "None",
        "source": "ACM Transactions on Storage",
        "abstract": "Non-volatile memory and remote direct memory access (RDMA) provide extremely high performance in storage and network hardware. However, existing distributed file systems strictly isolate file system and network layers, and the heavy layered software designs leave high-speed hardware under-exploited. In this article, we propose an RDMA-enabled distributed persistent memory file system, Octopus+, to redesign file system internal mechanisms by closely coupling non-volatile memory and RDMA features. For data operations, Octopus+ directly accesses a shared persistent memory pool to reduce memory copying overhead, and actively fetches and pushes data all in clients to rebalance the load between the server and network. For metadata operations, Octopus+ introduces self-identified remote procedure calls for immediate notification between file systems and networking, and an efficient distributed transaction mechanism for consistency. Octopus+ is enabled with replication feature to provide better availability. Evaluations on Intel Optane DC Persistent Memory Modules show that Octopus+ achieves nearly the raw bandwidth for large I/Os and orders of magnitude better performance than existing distributed file systems.",
        "link": "https://dl.acm.org/doi/10.1145/3448418",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Probabilistic Risk-Aware Scheduling with Deadline Constraint for Heterogeneous SoCs",
        "authors": "['Xing Chen', 'Umit Ogras', 'Chaitali Chakrabarti']",
        "date": "None",
        "source": "ACM Transactions on Embedded Computing Systems",
        "abstract": "Hardware Trojans can compromise System-on-Chip (SoC) performance. Protection schemes implemented to combat these threats cannot guarantee 100% detection rate and may also introduce performance overhead. This paper defines the risk of running a job on an SoC as a function of the misdetection rate of the hardware Trojan detection methods implemented on the cores in the SoC. Given the user-defined deadlines of each job, our goal is to minimize the job-level risk as well as the deadline violation rate for both static and dynamic scheduling scenarios. We assume that there is no relationship between the execution time and risk of a task executed on a core. Our risk-aware scheduling algorithm first calculates the probability of possible task allocations and then uses it to derive the task-level deadlines. Each task is then allocated to the core with minimum risk that satisfies the task-level deadline. In addition, in dynamic scheduling, where multiple jobs are injected randomly, we propose to explicitly operate with a reduced virtual deadline to avoid possible future deadline violations. Simulations on randomly generated graphs show that our static scheduler has no deadline violations and achieves 5.1%–17.2% lower job-level risk than the popular Earliest Time First (ETF) algorithm when the deadline constraint is 1.2×–3.0× the makespan of ETF. In the dynamic case, the proposed algorithm achieves a violation rate comparable to that of Earliest Deadline First (EDF), an algorithm optimized for dynamic scenarios. Even when the injection rate is high, it outperforms EDF with 8.4%–10% lower risk when the deadline is 1.5×–3.0× the makespan of ETF.",
        "link": "https://dl.acm.org/doi/10.1145/3489409",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Construction and Realisation of a Virtual Simulation Experiment Platform",
        "authors": "['Dong Liu', 'Jing Chang']",
        "date": "February 2022",
        "source": "ICBDE '22: Proceedings of the 5th International Conference on Big Data and Education",
        "abstract": "This paper examines the problems existing in traditional teaching, proposes the application of virtual simulation technology in teaching as a response and summarises the current state of use of virtual simulation technology in modern teaching. It describes the application of virtual simulation experiment technology and the construction of an experimental platform, as well as introducing the system architecture, composition and advantages of the virtual experiment platform. In accordance with the design goal of the virtual simulation experiment platform, this paper examines the architecture, functional modules and software and hardware facilities of the virtual simulation experiment platform, and it analyses the application of the computer virtual simulation experiment platform using specific examples.",
        "link": "https://dl.acm.org/doi/10.1145/3524383.3524407",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A POWER MANAGEMENT CIRCUIT COMPATIBLE with Ti SmartReflex and Xilinx Ultrascale+ MPSoC DUAL VOLTAGE MECHANISM",
        "authors": "['Chenyun Li', 'Xuwen Li', 'Changjiang Tang', 'Qiang Wu']",
        "date": "October 2021",
        "source": "ICCPR '21: Proceedings of the 2021 10th International Conference on Computing and Pattern Recognition",
        "abstract": "Since the semiconductor technology has entered the age of 90 nm, the static power consumption of the chip has increased rapidly. For edge computing devices, the waste of power can not be ignored. This paper uses the relationship between power consumption and power supply voltage of FPGA and DSP, analyzes the characteristics of the traditional scheme, and designs a hardware circuit according to the different mechanisms of reducing static power consumption of FPGA and DSP According to the design of FPGA and DSP core power supply voltage compatibility, the system realizes the requirement of FPGA and DSP for power supply voltage amplitude and realizes dynamic control of power supply voltage. After verification and comparison, FPGA and DSP can realize the adjustment of performance power consumption and stable operation under the power system.",
        "link": "https://dl.acm.org/doi/10.1145/3497623.3497681",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Scale-down experiments on TPCx-HS",
        "authors": "['Maximilian Böther', 'Tilmann Rabl']",
        "date": "June 2021",
        "source": "BiDEDE '21: Proceedings of the International Workshop on Big Data in Emergent Distributed Environments",
        "abstract": "The Transaction Processing Performance Council's (TPC) benchmarks are the standard for evaluating data processing performance and are extensively used in academia and industry. Official TPC results are usually produced on high-end deployments, making transferability to commodity hardware difficult. Recent performance improvements on low-power ARM CPUs have made low-end computers, such as the Raspberry Pi, a candidate platform for distributed, low-scale data processing. In this paper, we conduct a feasibility study of executing scaled-down big data workloads on low-power ARM clusters. To this end, we run the TPCx-HS benchmark on two Raspberry Pi clusters. TPCx-HS is the ideal candidate for hardware comparisons and understanding hardware characteristics for data processing workloads because TPCx-HS results do not depend on specific software implementations and the benchmark has limited options for workload-specific tuning. Our evaluation shows that Pis exhibit similar behavior to large-scale big data systems in terms of price performance and relative throughput to performance results. Current generation Pi clusters are becoming a reasonable choice for GB-scale data processing due to the increasing amount of available memory, while older versions struggle with stable execution of high-load scenarios.",
        "link": "https://dl.acm.org/doi/10.1145/3460866.3461774",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A performance portability framework for Python",
        "authors": "['Nader Al Awar', 'Steven Zhu', 'George Biros', 'Milos Gligoric']",
        "date": "June 2021",
        "source": "ICS '21: Proceedings of the ACM International Conference on Supercomputing",
        "abstract": "Kokkos is a programming model for writing performance portable applications for all major high performance computing platforms. It provides abstractions for data management and common parallel operations, allowing developers to write portable high performance code with minimal knowledge of architecture-specific details. Kokkos is implemented as a heavily-templated C++ library. However, C++ is not ideal for rapid prototyping and quick algorithmic exploration. An increasing number of developers use Python for scientific computing, machine learning, and data analytics. In this paper, we present a new Python framework, dubbed PyKokkos, for writing performance portable applications entirely in Python. PyKokkos provides Kokkos-like abstractions that are easier to use and more concise than the C++ interface. We implemented PyKokkos by building a translator from a subset of Python to C++ Kokkos and bridging necessary function calls via automatically generated Python bindings. PyKokkos is also compatible with NumPy, a widely-used high performance Python library. By porting several existing Kokkos applications to PyKokkos, including ExaMiniMD (∼3k lines of code in C++), we show that the latter can achieve efficient execution with low performance overhead.",
        "link": "https://dl.acm.org/doi/10.1145/3447818.3460376",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "On-Demand SIMO Channel Impulse Response Shaping in Smart On-Chip Electromagnetic Environments",
        "authors": "['Mohammadreza F. Imani', 'Sergi Abadal', 'Philipp del Hougne']",
        "date": "November 2021",
        "source": "SenSys '21: Proceedings of the 19th ACM Conference on Embedded Networked Sensor Systems",
        "abstract": "We recently introduced the concept of reconfigurable Wireless Networks on Chips (r-WNoCs) for which an on-chip reconfigurable intelligent surface (RIS) endows the wireless on-chip propagation environment with programmability. In this work-in-progress report, we apply this idea to a single-input multiple-output (SIMO) context. Specifically, we demonstrate that using an on-chip RIS we can simultaneously shape multiple channel impulse responses (CIRs) such that they become essentially pulse-like despite rich scattering inside the chip enclosure. Pulse-like CIRs are essential to enable high-speed information exchange between different processors on the same chip with the simple on-off-keying modulation schemes envisaged for WNoCs.",
        "link": "https://dl.acm.org/doi/10.1145/3485730.3494043",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Guardian: Symbolic Validation of Orderliness in SGX Enclaves",
        "authors": "['Pedro Antonino', 'Wojciech Aleksander Woloszyn', 'A. W. Roscoe']",
        "date": "November 2021",
        "source": "CCSW '21: Proceedings of the 2021 on Cloud Computing Security Workshop",
        "abstract": "Modern processors can offer hardware primitives that allow a process to run in isolation. These primitives implement a trusted execution environment (TEE) in which a program can run such that the integrity and confidentiality of its execution are guaranteed. Intel's Software Guard eXtensions (SGX) is an example of such primitives and its isolated processes are called enclaves. These guarantees, however, can be easily thwarted if the enclave has not been properly designed. Its interface with the untrusted software stack is a perhaps the largest attack surface that adversaries can exploit; unintended interactions with untrusted code can expose the enclave to memory corruption attacks, for instance. In this paper, we propose a notion of an orderly enclave which splits its behaviour into the following execution phases: entry, secure, ocall, and exit. Each of them imposes a set of restrictions that enforce a particular policy of access to untrusted memory and, in some cases, sanitisation conditions. A violation of these policies and conditions might indicate an undesired interaction with untrusted data/code or a lack of sanitisation, both of which can be harnessed to perpetrate attacks against the enclave. We also introduce Guardian: an open-source tool that uses symbolic execution to carry out the validation of an enclave against our notion of an orderly enclave; in this process, it also looks for some other typical attack primitives. We discuss how our approach can prevent and flag enclave vulnerabilities that have been identified in the literature. Moreover, we have evaluated how our approach fares in the analysis of some enclave samples. In this process, Guardian identified some security issues previously undetected in some of these samples that were acknowledged and fixed by the corresponding maintainers.",
        "link": "https://dl.acm.org/doi/10.1145/3474123.3486755",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "In-situ Programmable Switching using rP4: Towards Runtime Data Plane Programmability",
        "authors": "['Yong Feng', 'Haoyu Song', 'Jiahao Li', 'Zhikang Chen', 'Wenquan Xu', 'Bin Liu']",
        "date": "November 2021",
        "source": "HotNets '21: Proceedings of the 20th ACM Workshop on Hot Topics in Networks",
        "abstract": "The existing chip architecture and programming language are incapable of supporting in-service updates by loading or offloading on-demand protocols and functions at runtime. We examine the fundamental reasons for the inflexibility and design a new In-situ Programmable Switch Architecture (IPSA) as a fix. We further design rP4, a P4 extension, for programming IPSA-based devices. To manifest the in-situ programming feasibility, we develop an rP4 compiler and demonstrate several use cases on both a software switch, ipbm, and an FPGA-based prototype. Our preliminary experiments and analysis show that, compared to PISA, IPSA provides higher flexibility in enabling runtime functional update with limited performance and gate-count penalty. The in-situ programming capability enabled by IPSA and rP4 opens a promising design space for programmable networks.",
        "link": "https://dl.acm.org/doi/10.1145/3484266.3487367",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "AsTAR: Sustainable Energy Harvesting for the Internet of Things through Adaptive Task Scheduling",
        "authors": "['Fan Yang', 'Ashok Samraj Thangarajan', 'Gowri Sankar Ramachandran', 'Wouter Joosen', 'Danny Hughes']",
        "date": "None",
        "source": "ACM Transactions on Sensor Networks",
        "abstract": "Battery-free Internet-of-Things devices equipped with energy harvesting hold the promise of extended operational lifetime, reduced maintenance costs, and lower environmental impact. Despite this clear potential, it remains complex to develop applications that deliver sustainable operation in the face of variable energy availability and dynamic energy demands. This article aims to reduce this complexity by introducing AsTAR, an energy-aware task scheduler that automatically adapts task execution rates to match available environmental energy. AsTAR enables the developer to prioritize tasks based upon their importance, energy consumption, or a weighted combination thereof. In contrast to prior approaches, AsTAR is autonomous and self-adaptive, requiring no a priori modeling of the environment or hardware platforms. We evaluate AsTAR based on its capability to efficiently deliver sustainable operation for multiple tasks on heterogeneous platforms under dynamic environmental conditions. Our evaluation shows that (1) comparing to conventional approaches, AsTAR guarantees Sustainability by maintaining a user-defined optimum level of charge, and (2) AsTAR reacts quickly to environmental and platform changes, and achieves Efficiency by allocating all the surplus resources following the developer-specified task priorities. (3) Last, the benefits of AsTAR are achieved with minimal performance overhead in terms of memory, computation, and energy.",
        "link": "https://dl.acm.org/doi/10.1145/3467894",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Processing-in-Memory Acceleration of MAC-based Applications Using Residue Number System: A Comparative Study",
        "authors": "['Shaahin Angizi', 'Arman Roohi', 'MohammadReza Taheri', 'Deliang Fan']",
        "date": "June 2021",
        "source": "GLSVLSI '21: Proceedings of the 2021 on Great Lakes Symposium on VLSI",
        "abstract": "Processing-in-memory (PIM) has raised as a viable solution for the memory wall crisis and has attracted great interest in accelerating computationally intensive AI applications ranging from filtering to complex neural networks. In this paper, we try to take advantage of both PIM and the residue number system (RNS) as an alternative for the conventional binary number representation to accelerate multiplication-and-accumulations (MACs), primary operations of target applications. The PIM architecture utilizes the maximum internal bandwidth of memory chips to realize a local and parallel computation to eliminates the off-chip data transfer. Moreover, RNS limits inter-digit carry propagation by performing arithmetic operations on small residues independently and in parallel. Thus, we develop a PIM-RNS, entitled PRIMS, and analyze the potential of intertwining PIM architecture with the inherent parallelism of the RNS arithmetic to delineate the opportunities and challenges. To this end, we build a comprehensive device-to-architecture evaluation framework to quantitatively study this problem considering the impact of PIM technology for a well-known three-moduli set as a case study.",
        "link": "https://dl.acm.org/doi/10.1145/3453688.3461529",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Secure Video Transmission System for UAV Applications",
        "authors": "['Dimitrios Psilias', 'Athanasios Milidonis', 'Ioannis Voyiatzis']",
        "date": "November 2021",
        "source": "PCI '21: Proceedings of the 25th Pan-Hellenic Conference on Informatics",
        "abstract": "UAV applications are starting to increase nowadays. Their data demanding applications are implemented mostly using software platforms. These applications have critical requirements on high performance, power consumption and there should be security at their transmissions, especially for video data. In this paper, an architecture is proposed for secure video transmission for UAV applications. The system consists of a digital camera a transmission board to transmit the video and a FPGA for implementing the security encryption tasks. The camera sends the video data to the FPGA and the inside circuit encrypts the video data. The transmission module transmits the encrypted data to the Ground Station (GS). Measurements taken concerning the execution time and power consumption, reveal the benefits of the proposed architecture in comparison with well-known software platforms.",
        "link": "https://dl.acm.org/doi/10.1145/3503823.3503882",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "cREAtIve: reconfigurable embedded artificial intelligence",
        "authors": "['Poona Bahrebar', 'Leon Denis', 'Maxim Bonnaerens', 'Kristof Coddens', 'Joni Dambre', 'Wouter Favoreel', 'Illia Khvastunov', 'Adrian Munteanu', 'Hung Nguyen-Duc', 'Stefan Schulte', 'Dirk Stroobandt', 'Ramses Valvekens', 'Nick Van den Broeck', 'Geert Verbruggen']",
        "date": "May 2021",
        "source": "CF '21: Proceedings of the 18th ACM International Conference on Computing Frontiers",
        "abstract": "cREAtIve targets the development of novel highly-adaptable embedded deep learning solutions for automotive and traffic monitoring applications, including position sensor processing, scene interpretation based on LiDAR, and object detection and classification in thermal images for traffic camera systems. These applications share the need for deep learning solutions tailored for deployment on embedded devices with limited resources and featuring high adaptability and robustness to changing environmental conditions. cREAtIve develops knowledge, tools and methods that enable hardware-efficient, adaptable, and robust deep learning.",
        "link": "https://dl.acm.org/doi/10.1145/3457388.3458857",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A case against (most) context switches",
        "authors": "['Jack Tigar Humphries', 'Kostis Kaffes', 'David Mazières', 'Christos Kozyrakis']",
        "date": "June 2021",
        "source": "HotOS '21: Proceedings of the Workshop on Hot Topics in Operating Systems",
        "abstract": "Multiplexing software threads onto hardware threads and serving interrupts, VM-exits, and system calls require frequent context switches, causing high overheads and significant kernel and application complexity. We argue that context switching is an idea whose time has come and gone, and propose eliminating it through a radically different hardware threading model targeted to solve software rather than hardware problems. The new model adds a large number of hardware threads to each physical core - making thread multiplexing unnecessary - and lets software manage them. The only state change directly triggered in hardware by system calls, exceptions, and asynchronous hardware events will be blocking and unblocking hardware threads. We also present ISA extensions to allow kernel and user software to exploit this new threading model. Developers can use these extensions to eliminate interrupts and implement fast I/O without polling, exception-less system and hypervisor calls, practical microkernels, simple distributed programming models, and untrusted but fast hypervisors. Finally, we suggest practical hardware implementations and discuss the hardware and software challenges toward realizing this novel approach.",
        "link": "https://dl.acm.org/doi/10.1145/3458336.3465274",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A ReRAM Memory Compiler for Monolithic 3D Integrated Circuits in a Carbon Nanotube Process",
        "authors": "['Edward Lee', 'Daehyun Kim', 'Jinwoo Kim', 'Sung Kyu Lim', 'Saibal Mukhopadhyay']",
        "date": "None",
        "source": "ACM Journal on Emerging Technologies in Computing Systems",
        "abstract": "We present a ReRAM memory compiler for monolithic 3D (M3D) integrated circuits (IC). We develop ReRAM architectures for M3D ICs using 1T-1R bit cells and single and multiple tiers of transistors for access and peripheral circuits. The compiler includes an automated flow for generation of subarrays of different dimensions and larger arrays of a target capacity by integrating multiple subarrays. The compiler is demonstrated using an M3D process design kit (PDK) based on a Carbon Nanotube Transistor technology. The PDK includes multiple layers of transistors and back-end-of-the-line integrated ReRAM. Simulations show the compiled ReRAM macros with multiple tiers of transistors reduces footprint and improves performance over the macros with single-tier transistors. The compiler creates layout views that are exported into library exchange format or graphic data system for full-array assembly and schematic/symbol views to extract per-bit read/write energy and read latency. Comparison of the proposed M3D subarray architectures with baseline 2D subarrays, generated with a custom-designed set of bit cells and peripherals, demonstrate up to 48% area reduction and 13% latency improvement.",
        "link": "https://dl.acm.org/doi/10.1145/3466681",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Low-precision Logarithmic Number Systems: Beyond Base-2",
        "authors": "['Syed Asad Alam', 'James Garland', 'David Gregg']",
        "date": "None",
        "source": "ACM Transactions on Architecture and Code Optimization",
        "abstract": "Logarithmic number systems (LNS) are used to represent real numbers in many applications using a constant base raised to a fixed-point exponent making its distribution exponential. This greatly simplifies hardware multiply, divide, and square root. LNS with base-2 is most common, but in this article, we show that for low-precision LNS the choice of base has a significant impact.We make four main contributions. First, LNS is not closed under addition and subtraction, so the result is approximate. We show that choosing a suitable base can manipulate the distribution to reduce the average error. Second, we show that low-precision LNS addition and subtraction can be implemented efficiently in logic rather than commonly used ROM lookup tables, the complexity of which can be reduced by an appropriate choice of base. A similar effect is shown where the result of arithmetic has greater precision than the input. Third, where input data from external sources is not expected to be in LNS, we can reduce the conversion error by selecting a LNS base to match the expected distribution of the input. Thus, there is no one base that gives the global optimum, and base selection is a trade-off between different factors. Fourth, we show that circuits realized in LNS require lower area and power consumption for short word lengths.",
        "link": "https://dl.acm.org/doi/10.1145/3461699",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "UNIQ: Uniform Noise Injection for Non-Uniform Quantization of Neural Networks",
        "authors": "['Chaim Baskin', 'Natan Liss', 'Eli Schwartz', 'Evgenii Zheltonozhskii', 'Raja Giryes', 'Alex M. Bronstein', 'Avi Mendelson']",
        "date": "None",
        "source": "ACM Transactions on Computer Systems",
        "abstract": "We present a novel method for neural network quantization. Our method, named UNIQ, emulates a non-uniform k-quantile quantizer and adapts the model to perform well with quantized weights by injecting noise to the weights at training time. As a by-product of injecting noise to weights, we find that activations can also be quantized to as low as 8-bit with only a minor accuracy degradation. Our non-uniform quantization approach provides a novel alternative to the existing uniform quantization techniques for neural networks. We further propose a novel complexity metric of number of bit operations performed (BOPs), and we show that this metric has a linear relation with logic utilization and power. We suggest evaluating the trade-off of accuracy vs. complexity (BOPs). The proposed method, when evaluated on ResNet18/34/50 and MobileNet on ImageNet, outperforms the prior state of the art both in the low-complexity regime and the high accuracy regime. We demonstrate the practical applicability of this approach, by implementing our non-uniformly quantized CNN on FPGA.",
        "link": "https://dl.acm.org/doi/10.1145/3444943",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Deep Learning for Sensor-based Human Activity Recognition: Overview, Challenges, and Opportunities",
        "authors": "['Kaixuan Chen', 'Dalin Zhang', 'Lina Yao', 'Bin Guo', 'Zhiwen Yu', 'Yunhao Liu']",
        "date": "None",
        "source": "ACM Computing Surveys",
        "abstract": "The vast proliferation of sensor devices and Internet of Things enables the applications of sensor-based activity recognition. However, there exist substantial challenges that could influence the performance of the recognition system in practical scenarios. Recently, as deep learning has demonstrated its effectiveness in many areas, plenty of deep methods have been investigated to address the challenges in activity recognition. In this study, we present a survey of the state-of-the-art deep learning methods for sensor-based human activity recognition. We first introduce the multi-modality of the sensory data and provide information for public datasets that can be used for evaluation in different challenge tasks. We then propose a new taxonomy to structure the deep methods by challenges. Challenges and challenge-related deep methods are summarized and analyzed to form an overview of the current research progress. At the end of this work, we discuss the open issues and provide some insights for future directions.",
        "link": "https://dl.acm.org/doi/10.1145/3447744",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Judging a type by its pointer: optimizing GPU virtual functions",
        "authors": "['Mengchi Zhang', 'Ahmad Alawneh', 'Timothy G. Rogers']",
        "date": "April 2021",
        "source": "ASPLOS '21: Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "Programmable accelerators aim to provide the flexibility of traditional CPUs with significantly improved performance. A well-known impediment to the widespread adoption of programmable accelerators, like GPUs, is the software engineering overhead involved in porting the code. Existing support for C++ on GPUs allows programmers to port polymorphic code with little effort. However, the overhead from the virtual functions introduced by polymorphic code has not been well studied or mitigated on GPUs.  To alleviate the performance cost of virtual functions, we propose two novel techniques that determine an object’s type based only on the object’s address, without accessing the object’s embedded virtual table pointer. The first technique, Coordinated Object Allocation and function Lookup (COAL), is a software-only solution that allocates objects by type and uses the compiler and runtime to find the object’s vTable without accessing an embedded pointer. COAL improves performance by 80%, 47%, and 6% over contemporary CUDA, prior research, and our newly-proposed type-based allocator, respectively. The second solution, TypePointer, introduces a hardware modification that allows unused bits in the object pointer to encode the object’s type, improving performance by 90%, 56%, and 12% over CUDA, prior work, and our new allocator. TypePointer can also be used with the default CUDA allocator to achieve an 18% performance improvement without modifying object allocation.",
        "link": "https://dl.acm.org/doi/10.1145/3445814.3446734",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Generalizable coordination of large multiscale workflows: challenges and learnings at scale",
        "authors": "['Harsh Bhatia', 'Francesco Di Natale', 'Joseph Y. Moon', 'Xiaohua Zhang', 'Joseph R. Chavez', 'Fikret Aydin', 'Chris Stanley', 'Tomas Oppelstrup', 'Chris Neale', 'Sara Kokkila Schumacher', 'Dong H. Ahn', 'Stephen Herbein', 'Timothy S. Carpenter', 'Sandrasegaram Gnanakaran', 'Peer-Timo Bremer', 'James N. Glosli', 'Felice C. Lightstone', 'Helgi I. Ingólfsson']",
        "date": "November 2021",
        "source": "SC '21: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",
        "abstract": "The advancement of machine learning techniques and the heterogeneous architectures of most current supercomputers are propelling the demand for large multiscale simulations that can automatically and autonomously couple diverse components and map them to relevant resources to solve complex problems at multiple scales. Nevertheless, despite the recent progress in workflow technologies, current capabilities are limited to coupling two scales. In the first-ever demonstration of using three scales of resolution, we present a scalable and generalizable framework that couples pairs of models using machine learning and in situ feedback. We expand upon the massively parallel Multiscale Machine-Learned Modeling Infrastructure (MuMMI), a recent, award-winning workflow, and generalize the framework beyond its original design. We discuss the challenges and learnings in executing a massive multiscale simulation campaign that utilized over 600,000 node hours on Summit and achieved more than 98% GPU occupancy for more than 83% of the time. We present innovations to enable several orders of magnitude scaling, including simultaneously coordinating 24,000 jobs, and managing several TBs of new data per day and over a billion files in total. Finally, we describe the generalizability of our framework and, with an upcoming open-source release, discuss how the presented framework may be used for new applications.",
        "link": "https://dl.acm.org/doi/10.1145/3458817.3476210",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Trust assessment in 32 KiB of RAM: multi-application trust-based task offloading for resource-constrained IoT nodes",
        "authors": "['Matthew Bradbury', 'Arshad Jhumka', 'Tim Watson']",
        "date": "March 2021",
        "source": "SAC '21: Proceedings of the 36th Annual ACM Symposium on Applied Computing",
        "abstract": "There is an increasing demand for Internet of Things (IoT) systems comprised of resource-constrained sensor and actuator nodes executing increasingly complex applications, possibly simultaneously. IoT devices will not be able to execute computationally expensive tasks and will require more powerful computing nodes, called edge nodes, for such execution, in a process called computation offloading. When multiple powerful nodes are available, a selection problem arises: which edge node should a task be submitted to? This problem is even more acute when the system is subjected to attacks, such as DoS, or network perturbations such as system overload. In this paper, we present a trust model-based system architecture for computation offloading, based on behavioural evidence. The system architecture provides confidentiality, authentication and non-repudiation of messages in required scenarios and will operate within the resource constraints of embedded IoT nodes. We demonstrate the viability of the architecture with an example deployment of Beta Reputation System trust model on real hardware.",
        "link": "https://dl.acm.org/doi/10.1145/3412841.3441898",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "To Partition, or Not to Partition, That is the Join Question in a Real System",
        "authors": "['Maximilian Bandle', 'Jana Giceva', 'Thomas Neumann']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "An efficient implementation of a hash join has been a highly researched problem for decades. Recently, the radix join has been shown to have superior performance over the alternatives (e.g., the non-partitioned hash join), albeit on synthetic microbenchmarks. Therefore, it is unclear whether one can simply replace the hash join in an RDBMS or use the radix join as a performance booster for selected queries. If the latter, it is still unknown when one should rely on the radix join to improve performance. In this paper, we address these questions, show how to integrate the radix join in Umbra, a code-generating DBMS, and make it competitive for selective queries by introducing a Bloom-filter based semi-join reducer. We have evaluated how well it runs when used in queries from more representative workloads like TPC-H. Surprisingly, the radix join brings a noticeable improvement in only one out of all 59 joins in TPC-H. Thus, with an extensive range of microbenchmarks, we have isolated the effects of the most important workload factors and synthesized the range of values where partitioning the data for the radix join pays off. Our analysis shows that the benefit of data partitioning quickly diminishes as soon as we deviate from the optimal parameters, and even late materialization rarely helps in real workloads. We thus, conclude that integrating the radix join within a code-generating database rarely justifies the increase in code and optimizer complexity and advise against it for processing real-world workloads.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3452831",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Spatial Dependency Analysis to Extract Information from Side-Channel Mixtures",
        "authors": "['Aurélien Vasselle', 'Hugues Thiebeauld', 'Philippe Maurine']",
        "date": "November 2021",
        "source": "ASHES '21: Proceedings of the 5th Workshop on Attacks and Solutions in Hardware Security",
        "abstract": "Practical side-channel attacks on recent devices may be challenging due to the poor quality of acquired signals. It can originate from different factors, such as the growing architecture complexity, especially in System-on-Chips, creating unpredictable and concurrent operation of multiple signal sources in the device. This work makes use of mixture distributions to formalize this complexity, allowing us to explain the benefit of using a technique like Scatter, where different samples of the traces are aggregated into the same distribution. Some observations of the conditional mixture distributions are made in order to model the leakage in such context. From this, we infer local coherency of information held in the distribution as a general expression of the leakage in mixture distributions. This leads us to introduce how spatial analysis tools, such as Moran's Index, can be used to significantly improve non-profiled attacks compared to other techniques from the state-of-the-art. Exploitation of this technique is experimentally shown very promising, as demonstrated by its application to ASCAD dataset.",
        "link": "https://dl.acm.org/doi/10.1145/3474376.3487280",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Judging a type by its pointer: optimizing GPU virtual functions",
        "authors": "['Mengchi Zhang', 'Ahmad Alawneh', 'Timothy G. Rogers']",
        "date": "April 2021",
        "source": "ASPLOS '21: Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "Programmable accelerators aim to provide the flexibility of traditional CPUs with significantly improved performance. A well-known impediment to the widespread adoption of programmable accelerators, like GPUs, is the software engineering overhead involved in porting the code. Existing support for C++ on GPUs allows programmers to port polymorphic code with little effort. However, the overhead from the virtual functions introduced by polymorphic code has not been well studied or mitigated on GPUs.  To alleviate the performance cost of virtual functions, we propose two novel techniques that determine an object’s type based only on the object’s address, without accessing the object’s embedded virtual table pointer. The first technique, Coordinated Object Allocation and function Lookup (COAL), is a software-only solution that allocates objects by type and uses the compiler and runtime to find the object’s vTable without accessing an embedded pointer. COAL improves performance by 80%, 47%, and 6% over contemporary CUDA, prior research, and our newly-proposed type-based allocator, respectively. The second solution, TypePointer, introduces a hardware modification that allows unused bits in the object pointer to encode the object’s type, improving performance by 90%, 56%, and 12% over CUDA, prior work, and our new allocator. TypePointer can also be used with the default CUDA allocator to achieve an 18% performance improvement without modifying object allocation.",
        "link": "https://dl.acm.org/doi/10.1145/3445814.3446734",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "To Partition, or Not to Partition, That is the Join Question in a Real System",
        "authors": "['Maximilian Bandle', 'Jana Giceva', 'Thomas Neumann']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "An efficient implementation of a hash join has been a highly researched problem for decades. Recently, the radix join has been shown to have superior performance over the alternatives (e.g., the non-partitioned hash join), albeit on synthetic microbenchmarks. Therefore, it is unclear whether one can simply replace the hash join in an RDBMS or use the radix join as a performance booster for selected queries. If the latter, it is still unknown when one should rely on the radix join to improve performance. In this paper, we address these questions, show how to integrate the radix join in Umbra, a code-generating DBMS, and make it competitive for selective queries by introducing a Bloom-filter based semi-join reducer. We have evaluated how well it runs when used in queries from more representative workloads like TPC-H. Surprisingly, the radix join brings a noticeable improvement in only one out of all 59 joins in TPC-H. Thus, with an extensive range of microbenchmarks, we have isolated the effects of the most important workload factors and synthesized the range of values where partitioning the data for the radix join pays off. Our analysis shows that the benefit of data partitioning quickly diminishes as soon as we deviate from the optimal parameters, and even late materialization rarely helps in real workloads. We thus, conclude that integrating the radix join within a code-generating database rarely justifies the increase in code and optimizer complexity and advise against it for processing real-world workloads.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3452831",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Accelerating applications using edge tensor processing units",
        "authors": "['Kuan-Chieh Hsu', 'Hung-Wei Tseng']",
        "date": "November 2021",
        "source": "SC '21: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",
        "abstract": "Neural network (NN) accelerators have been integrated into a wide-spectrum of computer systems to accommodate the rapidly growing demands for artificial intelligence (AI) and machine learning (ML) applications. NN accelerators share the idea of providing native hardware support for operations on multidimensional tensor data. Therefore, NN accelerators are theoretically tensor processors that can improve system performance for any problem that uses tensors as inputs/outputs. Unfortunately, commercially available NN accelerators only expose computation capabilities through AI/ML-specific interfaces. Furthermore, NN accelerators reveal very few hardware design details, so applications cannot easily leverage the tensor operations NN accelerators provide. This paper introduces General-Purpose Computing on Tensor Processing Units (GPTPU), an open-source, open-architecture framework that allows the developer and research communities to discover opportunities that NN accelerators enable for applications. GPTPU includes a powerful programming interface with efficient runtime system-level support---similar to that of CUDA/OpenCL in GPGPU computing---to bridge the gap between application demands and mismatched hardware/software interfaces. We built GPTPU machine uses Edge Tensor Processing Units (Edge TPUs), which are widely available and representative of many commercial NN accelerators. We identified several novel use cases and revisited the algorithms. By leveraging the underlying Edge TPUs to perform tensor-algorithm-based compute kernels, our results reveal that GPTPU can achieve a 2.46× speedup over high-end CPUs and reduce energy consumption by 40%.",
        "link": "https://dl.acm.org/doi/10.1145/3458817.3476177",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Faulty Point Unit: ABI Poisoning Attacks on Trusted Execution Environments",
        "authors": "['Fritz Alder', 'Jo Van Bulck', 'Jesse Spielman', 'David Oswald', 'Frank Piessens']",
        "date": "None",
        "source": "Digital Threats: Research and Practice",
        "abstract": "This article analyzes a previously overlooked attack surface that allows unprivileged adversaries to impact floating-point computations in enclaves through the Application Binary Interface (ABI). In a comprehensive study across 7 industry-standard and research enclave shielding runtimes for Intel Software Guard Extensions (SGX), we show that control and state registers of the x87 Floating-Point Unit (FPU) and Intel Streaming SIMD Extensions are not always properly sanitized on enclave entry. We furthermore show that this attack goes beyond the x86 architecture and can also affect RISC-V enclaves. Focusing on SGX, we abuse the adversary’s control over precision and rounding modes as an ABI fault injection primitive to corrupt enclaved floating-point operations. Our analysis reveals that this is especially relevant for applications that use the older x87 FPU, which is still under certain conditions used by modern compilers. We exemplify the potential impact of ABI quality-degradation attacks for enclaved machine learning and for the SPEC benchmarks. We then explore the impact on confidentiality, showing that control over exception masks can be abused as a controlled channel to recover enclaved multiplication operands. Our findings, affecting 5 of 7 studied SGX runtimes and one RISC-V runtime, demonstrate the challenges of implementing high-assurance trusted execution across computing architectures.",
        "link": "https://dl.acm.org/doi/10.1145/3491264",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "BCD deduplication: effective memory compression using partial cache-line deduplication",
        "authors": "['Sungbo Park', 'Ingab Kang', 'Yaebin Moon', 'Jung Ho Ahn', 'G. Edward Suh']",
        "date": "April 2021",
        "source": "ASPLOS '21: Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "In this paper, we identify new partial data redundancy among multiple cache lines that are not exploited by traditional memory compression or memory deduplication. We propose Base and Compressed Difference (BCD) deduplication that effectively utilizes the partial matches among cache lines through a novel combination of compression and deduplication to increase the effective capacity of main memory. Experimental results show that BCD achieves the average compression ratio of 1.94× for SPEC2017, DaCapo, TPC-DS, and TPC-H, which is 48.4% higher than the best prior work. We also present an efficient implementation of BCD in a modern memory hierarchy, which compresses data in both the last-level cache (LLC) and main memory with modest area overhead. Even with additional meta-data accesses and compression/deduplication operations, cycle-level simulations show that BCD improves the performance of the SPEC2017 benchmarks by 2.7% on average because it increases the effective capacity of the LLC. Overall, the results show that BCD can significantly increase the capacity of main memory with little performance overhead.",
        "link": "https://dl.acm.org/doi/10.1145/3445814.3446722",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Generalizable coordination of large multiscale workflows: challenges and learnings at scale",
        "authors": "['Harsh Bhatia', 'Francesco Di Natale', 'Joseph Y. Moon', 'Xiaohua Zhang', 'Joseph R. Chavez', 'Fikret Aydin', 'Chris Stanley', 'Tomas Oppelstrup', 'Chris Neale', 'Sara Kokkila Schumacher', 'Dong H. Ahn', 'Stephen Herbein', 'Timothy S. Carpenter', 'Sandrasegaram Gnanakaran', 'Peer-Timo Bremer', 'James N. Glosli', 'Felice C. Lightstone', 'Helgi I. Ingólfsson']",
        "date": "November 2021",
        "source": "SC '21: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",
        "abstract": "The advancement of machine learning techniques and the heterogeneous architectures of most current supercomputers are propelling the demand for large multiscale simulations that can automatically and autonomously couple diverse components and map them to relevant resources to solve complex problems at multiple scales. Nevertheless, despite the recent progress in workflow technologies, current capabilities are limited to coupling two scales. In the first-ever demonstration of using three scales of resolution, we present a scalable and generalizable framework that couples pairs of models using machine learning and in situ feedback. We expand upon the massively parallel Multiscale Machine-Learned Modeling Infrastructure (MuMMI), a recent, award-winning workflow, and generalize the framework beyond its original design. We discuss the challenges and learnings in executing a massive multiscale simulation campaign that utilized over 600,000 node hours on Summit and achieved more than 98% GPU occupancy for more than 83% of the time. We present innovations to enable several orders of magnitude scaling, including simultaneously coordinating 24,000 jobs, and managing several TBs of new data per day and over a billion files in total. Finally, we describe the generalizability of our framework and, with an upcoming open-source release, discuss how the presented framework may be used for new applications.",
        "link": "https://dl.acm.org/doi/10.1145/3458817.3476210",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Co-Exploration of Graph Neural Network and Network-on-Chip Design Using AutoML",
        "authors": "['Daniel Manu', 'Shaoyi Huang', 'Caiwen Ding', 'Lei Yang']",
        "date": "June 2021",
        "source": "GLSVLSI '21: Proceedings of the 2021 on Great Lakes Symposium on VLSI",
        "abstract": "Recently, Graph Neural Networks (GNNs) have exhibited high efficiency in several graph-based machine learning tasks. Compared with the neural networks for computer vision or speech tasks (e.g., Convolutional Neural Networks), GNNs have much higher requirements on communication due to the complicated graph structures; however, when applying GNNs for real-world applications, say in recommender systems (e.g. Uber Eats), it commonly has the real-time requirements. To deal with the tradeoff between the complicated architecture and the high-demand timing performance, both GNN architecture and hardware accelerator need to be optimized. Network-on-Chip (NoC), derived for efficiently managing the high-volume of communications, naturally becomes one of the top candidates to accelerate GNNs. However, there is a missing link between the optimize of GNN architecture and the NoC design. In this work, we present an AutoML-based framework GN-NAS, aiming at searching for the optimum GNN architecture, which can be suitable for the NoC accelerator. We devise a robust reinforcement learning based controller to validate the retained best GNN architectures, coupled with a parameter sharing approach, namely ParamShare, to improve search efficiency. Experimental results on four graph-based benchmark datasets, Cora, Citeseer, Pubmed and Protein-Protein Interaction show that the GNN architectures obtained by our framework outperform that of the state-of-the-art and baseline models, whilst reducing model size which makes them easy to deploy onto the NoC platform.",
        "link": "https://dl.acm.org/doi/10.1145/3453688.3461741",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A Convolutional Neural Network Accelerator Based on NVDLA",
        "authors": "['Kangjin Zhao', 'Jing Wang', 'Di Zang']",
        "date": "September 2021",
        "source": "ICACS '21: Proceedings of the 5th International Conference on Algorithms, Computing and Systems",
        "abstract": "In recent years, Convolutional Neural Network (CNN) has been successfully applied to a wider range of fields, such as image recognition and natural language processing. With the application of CNN to solve more complex problems, their computing and storage requirements are also greatly increased. Traditionally, CNN is executed on CPU and GPU, but their low throughput and energy efficiency are the bottleneck of using them. Field Programmable Gate Array (FPGA) has many characteristics suitable for acceleration, it has become an ideal platform for hardware acceleration of CNN. We design and implement a convolutional neural network accelerator based on NVIDIA deep learning accelerator (NVDLA) on FPGA platform. We give the detailed structure of NVDLA, design the hardware system and software system. The neural networks that NVDLA can support are limited, but our architecture can realize the high bandwidth data communication between NVDLA and CPU. CPU handle the operations that NVDLA does not support. The accelerator will support more and more complex networks in the future.",
        "link": "https://dl.acm.org/doi/10.1145/3490700.3490708",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "RAS: Continuously Optimized Region-Wide Datacenter Resource Allocation",
        "authors": "['Andrew Newell', 'Dimitrios Skarlatos', 'Jingyuan Fan', 'Pavan Kumar', 'Maxim Khutornenko', 'Mayank Pundir', 'Yirui Zhang', 'Mingjun Zhang', 'Yuanlai Liu', 'Linh Le', 'Brendon Daugherty', 'Apurva Samudra', 'Prashasti Baid', 'James Kneeland', 'Igor Kabiljo', 'Dmitry Shchukin', 'Andre Rodrigues', 'Scott Michelson', 'Ben Christensen', 'Kaushik Veeraraghavan', 'Chunqiang Tang']",
        "date": "October 2021",
        "source": "SOSP '21: Proceedings of the ACM SIGOPS 28th Symposium on Operating Systems Principles",
        "abstract": "Capacity reservation is a common offering in public clouds and on-premise infrastructure. However, no prior work provides capacity reservation with SLO guarantees that takes into account random and correlated hardware failures, datacenter maintenance, and heterogeneous hardware. In this paper, we describe how Facebook's region-scale Resource Allowance System (RAS) addresses these issues and provides guaranteed capacity. RAS uses a capacity abstraction called reservation to represent a set of servers dynamically assigned to a logical cluster. We take a two-level approach to scale resource allocation to all datacenters in a region, where a mixed-integer-programming solver continuously optimizes server-to-reservation assignments off the critical path, and a traditional container allocator does real-time placement of containers on servers in a reservation. As a relatively new component of Facebook's 10-year old cluster manager Twine, RAS has been running in production for almost two years, continuously optimizing the allocation of millions of servers to thousands of reservations. We describe the design of RAS and share our experience of deploying it at scale.",
        "link": "https://dl.acm.org/doi/10.1145/3477132.3483578",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Grand Challenges in Immersive Analytics",
        "authors": "['Barrett Ens', 'Benjamin Bach', 'Maxime Cordeil', 'Ulrich Engelke', 'Marcos Serrano', 'Wesley Willett', 'Arnaud Prouzeau', 'Christoph Anthes', 'Wolfgang Büschel', 'Cody Dunne', 'Tim Dwyer', 'Jens Grubert', 'Jason H. Haga', 'Nurit Kirshenbaum', 'Dylan Kobayashi', 'Tica Lin', 'Monsurat Olaosebikan', 'Fabian Pointecker', 'David Saffo', 'Nazmus Saquib', 'Dieter Schmalstieg', 'Danielle Albers Szafir', 'Matt Whitlock', 'Yalong Yang']",
        "date": "May 2021",
        "source": "CHI '21: Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems",
        "abstract": "Immersive Analytics is a quickly evolving field that unites several areas such as visualisation, immersive environments, and human-computer interaction to support human data analysis with emerging technologies. This research has thrived over the past years with multiple workshops, seminars, and a growing body of publications, spanning several conferences. Given the rapid advancement of interaction technologies and novel application domains, this paper aims toward a broader research agenda to enable widespread adoption. We present 17 key research challenges developed over multiple sessions by a diverse group of 24 international experts, initiated from a virtual scientific workshop at ACM CHI 2020. These challenges aim to coordinate future work by providing a systematic roadmap of current directions and impending hurdles to facilitate productive and effective applications for Immersive Analytics.",
        "link": "https://dl.acm.org/doi/10.1145/3411764.3446866",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "WasmAndroid: a cross-platform runtime for native programming languages on Android (WIP paper)",
        "authors": "['Elliott Wen', 'Gerald Weber', 'Suranga Nanayakkara']",
        "date": "June 2021",
        "source": "LCTES 2021: Proceedings of the 22nd ACM SIGPLAN/SIGBED International Conference on Languages, Compilers, and Tools for Embedded Systems",
        "abstract": "Open-source hardware such as RISC-V has been gaining substantial momentum. Recently, they have begun to embrace Google's Android operating system to leverage its software ecosystem. Despite the encouraging progress, a challenging issue arises: a majority of Android applications are written in native languages and need to be recompiled to target new hardware platforms. Unfortunately, this recompilation process is not scalable because of the explosion of new hardware platforms. To address this issue, we present WasmAndroid, a high-performance cross-platform runtime for native programming languages on Android. WasmAndroid only requires developers to compile their source code to WebAssembly, an efficient and portable bytecode format that can be executed everywhere without additional reconfiguration. WasmAndroid can also trans-pile existing application binaries to WebAssembly when source code is not available. WebAssembly's language model is very different from C/C++ and this mismatch leads to many unique implementation challenges. In this paper, we provide workable solutions and conduct a preliminary system evaluation. We show that WasmAndroid provides acceptable performance to execute native applications in a cross-platform manner.",
        "link": "https://dl.acm.org/doi/10.1145/3461648.3463849",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Switches for HIRE: resource scheduling for data center in-network computing",
        "authors": "['Marcel Blöcher', 'Lin Wang', 'Patrick Eugster', 'Max Schmidt']",
        "date": "April 2021",
        "source": "ASPLOS '21: Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "The recent trend towards more programmable switching hardware in data centers opens up new possibilities for distributed applications to leverage in-network computing (INC). Literature so far has largely focused on individual application scenarios of INC, leaving aside the problem of coordinating usage of potentially scarce and heterogeneous switch resources among multiple INC scenarios, applications, and users. The traditional model of resource pools of isolated compute containers does not fit an INC-enabled data center.   This paper describes HIRE, a Holistic INC-aware Resource managEr which allows for server-local and INC resources to be coordinated in a unified manner. HIRE introduces a novel flexible resource (meta-)model to address heterogeneity, resource interchangeability, and non-linear resource requirements, and integrates dependencies between resources and locations in a unified cost model, cast as a min-cost max-flow problem. In absence of prior work, we compare HIRE against variants of state-of-the-art schedulers retrofitted to handle INC requests. Experiments with a workload trace of a 4000 machine cluster show that HIRE makes better use of INC resources by serving 8-30% more INC requests, while at the same time reducing network detours by 20%, and reducing tail placement latency by 50%.",
        "link": "https://dl.acm.org/doi/10.1145/3445814.3446760",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Sunflower: an environment for standardized communication of IoMusT",
        "authors": "['Rômulo Vieira', 'Flávio Schiavoni']",
        "date": "September 2021",
        "source": "AM '21: Proceedings of the 16th International Audio Mostly Conference",
        "abstract": "The Internet of Musical Things (IoMusT) area, although recent, has well-defined aspects concerning musical practice via the network. However, several challenges are also present, from those related to musical and artistic practice, even those dealing with environmental and social issues. From a computational point of view, the main dilemmas revolve around the lack of resources to deal with heterogeneity and the lack of standard in the communication of the devices that make up this scenario. Therefore, this paper presents Sunflower, a tool inspired by the Pipes-and-Filters architecture that allows communication between different objects, and focuses on its usage protocol. Its layered structure is also presented, showing the types of data, messages, and musical things present in each one of them. After all, the tests and results that certify to the functionality of this environment are demonstrated.",
        "link": "https://dl.acm.org/doi/10.1145/3478384.3478414",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Llama: A Heterogeneous & Serverless Framework for Auto-Tuning Video Analytics Pipelines",
        "authors": "['Francisco Romero', 'Mark Zhao', 'Neeraja J. Yadwadkar', 'Christos Kozyrakis']",
        "date": "November 2021",
        "source": "SoCC '21: Proceedings of the ACM Symposium on Cloud Computing",
        "abstract": "The proliferation of camera-enabled devices and large video repositories has led to a diverse set of video analytics applications. These applications rely on video pipelines, represented as DAGs of operations, to transform videos, process extracted metadata, and answer questions like, \"Is this intersection congested?\" The latency and resource efficiency of pipelines can be optimized using configurable knobs for each operation (e.g., sampling rate, batch size, or type of hardware used). However, determining efficient configurations is challenging because (a) the configuration search space is exponentially large, and (b) the optimal configuration depends on users' desired latency and cost targets, (c) input video contents may exercise different paths in the DAG and produce a variable amount intermediate results. Existing video analytics and processing systems leave it to the users to manually configure operations and select hardware resources. We present Llama: a heterogeneous and serverless framework for auto-tuning video pipelines. Given an end-to-end latency target, Llama optimizes for cost efficiency by (a) calculating a latency target for each operation invocation, and (b) dynamically running a cost-based optimizer to assign configurations across heterogeneous hardware that best meet the calculated per-invocation latency target. This makes the problem of auto-tuning large video pipelines tractable and allows us to handle input-dependent behavior, conditional branches in the DAG, and execution variability. We describe the algorithms in Llama and evaluate it on a cloud platform using serverless CPU and GPU resources. We show that compared to state-of-the-art cluster and serverless video analytics and processing systems, Llama achieves 7.8x lower latency and 16x cost reduction on average.",
        "link": "https://dl.acm.org/doi/10.1145/3472883.3486972",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "MultiScatter: Multistatic Backscatter Networking for Battery-Free Sensors",
        "authors": "['Mohamad Katanbaf', 'Ali Saffari', 'Joshua R. Smith']",
        "date": "November 2021",
        "source": "SenSys '21: Proceedings of the 19th ACM Conference on Embedded Networked Sensor Systems",
        "abstract": "Realizing the vision of ubiquitous battery-free sensing has proven to be challenging, mainly due to the practical energy and range limitations of current wireless communication systems. To address this, we design the first wide-area and scalable backscatter network with multiple receivers (RX) and transmitters (TX) base units to communicate with battery-free sensor nodes. Our system circumvents the inherent limitations of backscatter systems -including the limited coverage area, frequency-dependent operability, and sensor node limitations in handling network tasks- by introducing several coordination techniques between the base units starting from a single RX-TX pair to networks with many RX and TX units. We build low-cost RX and TX base units and battery-free sensor nodes with multiple sensing modalities and evaluate the performance of the MultiScatter system in various deployments. Our evaluation shows that we can successfully communicate with battery-free sensor nodes across 23400 ft2 of a two-floor educational complex using 5 RX and 20 TX units, costing $569. Also, we show that the aggregated throughput of the backscatter network increases linearly as the number of RX units and the network coverage grows.",
        "link": "https://dl.acm.org/doi/10.1145/3485730.3485939",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "On Efficient Oblivious Wavelength Assignments for Programmable Wide-Area Topologies",
        "authors": "['Thomas Fenz', 'Klaus-Tycho Foerster', 'Stefan Schmid']",
        "date": "December 2021",
        "source": "ANCS '21: Proceedings of the Symposium on Architectures for Networking and Communications Systems",
        "abstract": "Given the explosively growing traffic related to data-centric applications and AI, especially to and from the cloud, it is crucial to make the best use of the given resources of wide-area backbone networks (WANs). An intriguing approach to improve both efficiency and performance of WANs is to render networks more adaptive and \"demand-aware\", on the physical layer: innovative programmable wide-area topologies support dynamic wavelength assignments. This is enabled by the application of colorless and directionless Reconfigurable Optical Add/Drop Multiplexers (CD ROADM), and by leveraging the capabilities of software-defined controllers. This paper investigates the benefit of such fully dynamic wavelength assignments in programmable WAN topologies, compared to an oblivious wavelength assignment. To this end, we also propose a new demand-oblivious strategy to optimize the capacity of a WAN. Considering both real and synthetic scenarios, we find that our proposed demand-oblivious strategy can perform close to dynamic approaches with respect to throughput, without entailing reconfiguration costs.",
        "link": "https://dl.acm.org/doi/10.1145/3493425.3502753",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "WNOS: Enabling Principled Software-Defined Wireless Networking",
        "authors": "['Zhangyu Guan', 'Lorenzo Bertizzolo', 'Emrecan Demirors', 'Tommaso Melodia']",
        "date": "None",
        "source": "IEEE/ACM Transactions on Networking",
        "abstract": "This article investigates the basic design principles for a new <italic>Wireless Network Operating System (WNOS), a radically different approach to software-defined</italic> networking (SDN) for <italic>infrastructure-less</italic> wireless networks. Departing from well-understood approaches inspired by OpenFlow, WNOS provides the network designer with an abstraction hiding (i) the lower-level details of the wireless protocol stack and (ii) the distributed nature of the network operations. Based on this abstract representation, the WNOS takes network control programs written on a centralized, high-level view of the network and automatically generates distributed cross-layer control programs based on distributed optimization theory that are executed by each individual node on an abstract representation of the radio hardware. We first discuss the main architectural principles of WNOS. Then, we discuss a new approach to automatically generate solution algorithms for each of the resulting subproblems in an automated fashion. Finally, we illustrate a prototype implementation of WNOS on software-defined radio devices and test its effectiveness by considering specific cross-layer control problems. Experimental results indicate that, based on the automatically generated distributed control programs, WNOS achieves 18&#x0025;, 56&#x0025; and 80.4&#x0025; utility gain in networks with low, medium and high levels of interference; maybe more importantly, we illustrate how the global network behavior can be controlled by modifying a few lines of code on a centralized abstraction.",
        "link": "https://dl.acm.org/doi/10.1109/TNET.2021.3064824",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Towards practical quantum applications using hybrid problem solving techniques",
        "authors": "['Mehdi Bozzo-Rey', 'Robert Loredo', 'Hausi A. Müller', 'Sean Wagner', 'Ulrike Stege', 'Prashanti Priya Angara']",
        "date": "November 2021",
        "source": "CASCON '21: Proceedings of the 31st Annual International Conference on Computer Science and Software Engineering",
        "abstract": "Quantum computing is transitioning from scientific research to a quantum technology industry. Much progress is still needed to solve industry relevant problems and achieve quantum advantage. Moving towards quantum computing readiness is now an urgent goal shared across many industry verticals, academic institutions and governments. Hybrid models, algorithms and architectures is now seen as way to overcome the limitations of Noisy Intermediate Scale Quantum (NISQ) devices for near-term quantum computation. CASCON 2021 features a 2-day quantum computing workshop, which discusses using hybrid problem solving techniques as a way to move towards practical quantum applications, featuring speakers from, or related to the Canadian ecosystem, including start-up companies and academic research programs. The key goal of this workshop is to discuss the state of these hybrid problem solving techniques, their potential but also challenges.",
        "link": "https://dl.acm.org/doi/10.5555/3507788.3507854",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Octopus: a practical and versatile wideband MIMO sensing platform",
        "authors": "['Zhe Chen', 'Tianyue Zheng', 'Jun Luo']",
        "date": "October 2021",
        "source": "MobiCom '21: Proceedings of the 27th Annual International Conference on Mobile Computing and Networking",
        "abstract": "Radio frequency (RF) technologies have achieved a great success in data communication. In recent years, pervasive RF signals are further exploited for sensing; RF sensing has since attracted attentions from both academia and industry. Existing developments mainly employ commodity Wi-Fi hardware or rely on sophisticated SDR platforms. While promising in many aspects, there still remains a gap between lab prototypes and real-life deployments. On one hand, due to its narrow bandwidth and communication-oriented design, Wi-Fi sensing offers a coarse sensing granularity and its performance is very unstable in harsh real-world environments. On the other hand, SDR-based designs may hardly be adopted in practice due to its large size and high cost. To this end, we propose, design, and implement Octopus, a compact and flexible wideband MIMO sensing platform, built using commercial-grade low-power impulse radio. Octopus provides a standalone and fully programmable RF sensing solution; it allows for quick algorithm design and application development, and it specifically leverages the wideband radio to achieve a competent and robust performance in practice. We evaluate the performance of Octopus via micro-benchmarking, and further demonstrate its applicability using representative RF sensing applications, including passive localization, vibration sensing, and human/object imaging.",
        "link": "https://dl.acm.org/doi/10.1145/3447993.3483267",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Characterising the Role of Pre-Processing Parameters in Audio-based Embedded Machine Learning",
        "authors": "['Wiebke Toussaint', 'Akhil Mathur', 'Aaron Yi Ding', 'Fahim Kawsar']",
        "date": "November 2021",
        "source": "SenSys '21: Proceedings of the 19th ACM Conference on Embedded Networked Sensor Systems",
        "abstract": "When deploying machine learning (ML) models on embedded and IoT devices, performance encompasses more than an accuracy metric: inference latency, energy consumption, and model fairness are necessary to ensure reliable performance under heterogeneous and resource-constrained operating conditions. To this end, prior research has studied model-centric approaches, such as tuning the hyperparameters of the model during training and later applying model compression techniques to tailor the model to the resource needs of an embedded device. In this paper, we take a data-centric view of embedded ML and study the role that pre-processing parameters in the data pipeline can play in balancing the various performance metrics of an embedded ML system. Through an in-depth case study with audio-based keyword spotting (KWS) models, we show that pre-processing parameter tuning is a remarkable tool that model developers can adopt to trade-off between a model's accuracy, fairness, and system efficiency, as well as to make an embedded ML model resilient to unseen deployment conditions.",
        "link": "https://dl.acm.org/doi/10.1145/3485730.3493448",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Design and Implementation of English Online Translation Software Based on Intelligent Proofreading System",
        "authors": "['Jing Qiu']",
        "date": "May 2021",
        "source": "ICAIIS 2021: 2021 2nd International Conference on Artificial Intelligence and Information Systems",
        "abstract": "With the continuous development of modern information technology, the ways and means of learning English have become more diversified. Especially, the emergence of English electronic dictionary makes English learning more and more simple. This paper puts forward the design idea of English online translation software based on intelligent proofreading system. Firstly, the semantic ontology model is created by creating semantic ontology model and phrase translation combination translation algorithm. In addition, the overall system is designed. The hardware and software of the system are designed through the overall architecture of the system, and the hardware design is mainly the search module, so as to improve the search accuracy. In the process of software module, it mainly includes information transmission model, system network topology design and system function design, and can support translation and proofreading of multiple languages. Finally, the designed system is tested. The test results show that this method is accurate and intelligent in English automatic translation.",
        "link": "https://dl.acm.org/doi/10.1145/3469213.3471370",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "3U-EdgeAI: Ultra-Low Memory Training, Ultra-Low Bitwidth Quantization, and Ultra-Low Latency Acceleration",
        "authors": "['Yao Chen', 'Cole Hawkins', 'Kaiqi Zhang', 'Zheng Zhang', 'Cong Hao']",
        "date": "June 2021",
        "source": "GLSVLSI '21: Proceedings of the 2021 on Great Lakes Symposium on VLSI",
        "abstract": "The deep neural network (DNN) based AI applications on the edge require both low-cost computing platforms and high-quality services. However, the limited memory, computing resources, and power budget of the edge devices constrain the effectiveness of the DNN algorithms. Developing edge-oriented AI algorithms and implementations (e.g., accelerators) is challenging. In this paper, we summarize our recent efforts for efficient on-device AI development from three aspects, including both training and inference. First, we present on-device training with ultra-low memory usage. We propose a novel rank-adaptive tensor-based tensorized neural network model, which offers orders-of-magnitude memory reduction during training. Second, we introduce an ultra-low bitwidth quantization method for DNN model compression, achieving the state-of-the-art accuracy under the same compression ratio. Third, we introduce an ultra-low latency DNN accelerator design, practicing the software/hardware co-design methodology. This paper emphasizes the importance and efficacy of training, quantization and accelerator design, and calls for more research breakthroughs in the area for AI on the edge.",
        "link": "https://dl.acm.org/doi/10.1145/3453688.3461738",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Game-theoretic modeling of DDoS attacks in cloud computing",
        "authors": "['Kaho Wan', 'Joel Coffman']",
        "date": "December 2021",
        "source": "UCC '21: Proceedings of the 14th IEEE/ACM International Conference on Utility and Cloud Computing",
        "abstract": "The benefits of cloud computing have attracted many organizations to migrate their IT infrastructures into the cloud. In an infrastructure as a service (IaaS) model, the cloud service provider offers services to multiple consumers using shared physical hardware resources. However, by sharing a cloud environment with other consumers, organizations may also share security risks with their cotenants. Distributed denial of service (DDoS) attacks are considered one of the major security threats in cloud computing. Without a proper defense mechanism, an attack against one tenant can also affect the availability of cotenants. This work uses a game-theoretic approach to analyze the interactions between various entities when the cloud is under attack. The resulting Nash equilibrium shows that collateral damage to cotenants is unlikely if the cloud service provider is unbiased and chooses a rational strategy, but the Nash equilibrium can change when the cloud service provider does not treat cloud consumers equally. The cloud service provider's bias can influence its strategy selection and create a situation where untargeted users suffer unnecessary collateral damage from DDoS attacks.",
        "link": "https://dl.acm.org/doi/10.1145/3468737.3494093",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Adapting Surprise Minimizing Reinforcement Learning Techniques for Transactive Control",
        "authors": "['William Arnold', 'Tarang Srivastava', 'Lucas Spangher', 'Utkarsha Agwan', 'Costas Spanos']",
        "date": "June 2021",
        "source": "e-Energy '21: Proceedings of the Twelfth ACM International Conference on Future Energy Systems",
        "abstract": "Optimizing prices for energy demand response requires a flexible controller with ability to navigate complex environments. We propose a reinforcement learning controller with surprise minimizing modifications in its architecture. We suggest that surprise minimization can be used to improve learning speed, taking advantage of predictability in peoples' energy usage. Our architecture performs well in a simulation of energy demand response. We propose this modification to improve functionality and savin a large-scale experiment.",
        "link": "https://dl.acm.org/doi/10.1145/3447555.3466590",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A Holistic Solution for Reliability of 3D Parallel Systems",
        "authors": "['Javad Bagherzadeh', 'Aporva Amarnath', 'Jielun Tan', 'Subhankar Pal', 'Ronald G. Dreslinski']",
        "date": "None",
        "source": "ACM Journal on Emerging Technologies in Computing Systems",
        "abstract": "Monolithic 3D technology is emerging as a promising solution that can bring massive opportunities, but the gains can be hindered due to the reliability issues exaggerated by high temperature. Conventional reliability solutions focus on one specific feature and assume that the other required features would be provided by different solutions. Hence, this assumption has resulted in solutions that are proposed in isolation of each other and fail to consider the overall compatibility and the implied overheads of multiple isolated solutions for one system.This article proposes a holistic reliability management engine, R2D3, for post-Moore’s M3D parallel systems that have low yield and high failure rate. The proposed engine, comprising a controller, reconfigurable crossbars, and detection circuitry, provides concurrent single-replay detection and diagnosis, fault-mitigating repair, and aging-aware lifetime management at runtime. This holistic view enables us to create a solution that is highly effective while achieving a low overhead. Our solution achieves 96% coverage of defect; reduces Vth degradation by 53%, leading to a 78% performance improvement on average over 8 years for an eight-core system; and ultimately yields a 2.16× longer mean-time-to-failure (MTTF) while incurring an overhead of 7.4% in area, 6.5% in power, and an 8.2% decrease in frequency.",
        "link": "https://dl.acm.org/doi/10.1145/3488900",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A data-driven approach to optimize building energy performance and thermal comfort using machine learning models",
        "authors": "['Ziqi Chen', 'Zhuoang Tao', 'Aiwei Chang']",
        "date": "June 2021",
        "source": "ICCIR '21: Proceedings of the 2021 1st International Conference on Control and Intelligent Robotics",
        "abstract": "Buildings account for 30% of the world's total energy consumption, and heating, ventilation and air conditioning systems account for more than 70% of the world's total energy consumption. People pay more and more attention to the energy efficiency improvement of building energy saving system, especially HVAC system. Open-plan office is one of the most popular office types in recent decades, which can not only improve communication efficiency, but also save considerable construction costs. But it can't satisfy everyone's comfort requirements, especially indoor air temperature and relative humidity. In this paper, a data-driven thermal comfort model is established based on ASHRAE Global Thermal Comfort Database II. Two machine learning algorithms for building thermal comfort models are studied: support vector machine (SVM) and random forest. The model optimizes energy consumption while ensuring thermal comfort of commercial buildings. Under the thermal comfort condition, the purpose of energy saving is achieved by controlling the indoor temperature setting value. We also set up a co-simulation model of building energy consumption, compared the benchmark control strategy with the optimal control strategy by using the data-driven thermal comfort model, and analyzed the economic benefits of the enterprise by using the whole-life cycle cost analysis method. The results have shown the optimized control strategy outperforms the baseline owing to better thermal comfort prediction performances with machine learning. Therefore, this paper contributes to intelligent human building interaction areas with artificial intelligence.",
        "link": "https://dl.acm.org/doi/10.1145/3473714.3473794",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Super-efficient super resolution for fast adversarial defense at the edge",
        "authors": "['Kartikeya Bhardwaj', 'Dibakar Gope', 'James Ward', 'Paul Whatmough', 'Danny Loh']",
        "date": "March 2022",
        "source": "DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe",
        "abstract": "Autonomous systems are highly vulnerable to a variety of adversarial attacks on Deep Neural Networks (DNNs). Training-free model-agnostic defenses have recently gained popularity due to their speed, ease of deployment, and ability to work across many DNNs. To this end, a new technique has emerged for mitigating attacks on image classification DNNs, namely, preprocessing adversarial images using super resolution - upscaling low-quality inputs into high-resolution images. This defense requires running both image classifiers and super resolution models on constrained autonomous systems. However, super resolution incurs a heavy computational cost. Therefore, in this paper, we investigate the following question: Does the robustness of image classifiers suffer if we use tiny super resolution models? To answer this, we first review a recent work called Super-Efficient Super Resolution (SESR) [1] that achieves similar or better image quality than prior art while requiring 2X to 330X fewer Multiply-Accumulate (MAC) operations. We demonstrate that despite being orders of magnitude smaller than existing models, SESR achieves the same level of robustness as significantly larger networks. Finally, we estimate end-to-end performance of super resolution-based defenses on a commercial Arm Ethos-U55 micro-NPU. Our findings show that SESR achieves nearly 3X higher FPS than a baseline while achieving similar robustness.",
        "link": "https://dl.acm.org/doi/10.5555/3539845.3539945",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Exploring the potential of context-aware dynamic CPU undervolting",
        "authors": "['Emmanouil Maroudas', 'Spyros Lalis', 'Nikolaos Bellas', 'Christos D. Antonopoulos']",
        "date": "May 2021",
        "source": "CF '21: Proceedings of the 18th ACM International Conference on Computing Frontiers",
        "abstract": "CPU operation at sub-nominal voltage levels has been researched to reduce the power and energy consumption of computer systems. While it is possible to determine a safe undervolting level for each application, typically only the most conservative setting is applied statically across all workloads. In this paper, we go a step further and investigate the gains that can be achieved by dynamically and transparently changing the level of CPU undervolting at runtime. To enable this functionality, we design and implement a novel, OS-level, context-aware dynamic undervolting mechanism, able to decide and apply voltage levels according to the specific tolerance of each workload that executes on a multicore CPU at a particular time. Our mechanism can further differentiate between the user- and kernel-level code executed within the same application thread, enabling the exploitation of differences in their undervolting potential. User- and kernel-level code have inherently different characteristics, yet in previous work have never been characterized individually. Our experiments, on an Intel x86-64 multicore show that the proposed approach can reduce the average CPU power consumption by 5.58%/30.05% compared to static undervolting and the nominal voltage level, respectively. Finally, we provide indicative estimates for the gains that could be achieved in future CPU architectures with multiple, per-core voltage domains.",
        "link": "https://dl.acm.org/doi/10.1145/3457388.3458658",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Exploring the cost and performance benefits of AWS step functions using a data processing pipeline",
        "authors": "['Anil Mathew', 'Vasilios Andrikopoulos', 'Frank J. Blaauw']",
        "date": "December 2021",
        "source": "UCC '21: Proceedings of the 14th IEEE/ACM International Conference on Utility and Cloud Computing",
        "abstract": "In traditional cloud computing, dedicated hardware is substituted by dynamically allocated, utility-oriented resources such as virtualized servers. While cloud services are following the pay-as-you-go pricing model, resources are billed based on instance allocation and not on the actual usage, leading the customers to be charged needlessly. In serverless computing, as exemplified by the Function-as-a-Service (FaaS) model where functions are the basic resources, functions are typically not allocated or charged until invoked or triggered. Functions are not applications, however, and to build compelling serverless applications they frequently need to be orchestrated with some kind of application logic. A major issue emerging by the use of orchestration is that it complicates further the already complex billing model used by FaaS providers, which in combination with the lack of granular billing and execution details offered by the providers makes the development and evaluation of serverless applications challenging. Towards shedding some light into this matter, in this work we extensively evaluate the state-of-the-art function orchestrator AWS Step Functions (ASF) with respect to its performance and cost. For this purpose we conduct a series of experiments using a serverless data processing pipeline application developed as both ASF Standard and Express workflows. Our results show that Step Functions using Express workflows are economical when running short-lived tasks with many state transitions. In contrast, Standard workflows are better suited for long-running tasks, offering in addition detailed debugging and logging information. However, even if the behavior of the orchestrated AWS Lambda functions influences both types of workflows, Step Functions realized as Express workflows get impacted the most by the phenomena affecting Lambda functions.",
        "link": "https://dl.acm.org/doi/10.1145/3468737.3494084",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A distributed, decoupled system for losslessly streaming dynamic light probes to thin clients",
        "authors": "['Michael Stengel', 'Zander Majercik', 'Benjamin Boudaoud', 'Morgan McGuire']",
        "date": "June 2021",
        "source": "MMSys '21: Proceedings of the 12th ACM Multimedia Systems Conference",
        "abstract": "We present a networked, high-performance graphics system that combines dynamic, high-quality, ray traced global illumination computed on a server with direct illumination and primary visibility computed on a client. This approach provides many of the image quality benefits of real-time ray tracing on low-power and legacy hardware, while maintaining a low latency response and mobile form factor. As opposed to streaming full frames from rendering servers to end clients, our system distributes the graphics pipeline over a network by computing diffuse global illumination on a remote machine. Diffuse global illumination is computed using a recent irradiance volume representation combined with a new lossless, HEVC-based, hardware-accelerated encoding, and a perceptually-motivated update scheme. Our experimental implementation streams thousands of irradiance probes per second and requires less than 50 Mbps of throughput, reducing the consumed bandwidth by 99.4% when streaming at 60 Hz compared to traditional lossless texture compression. The bandwidth reduction achieved with our approach allows higher quality and lower latency graphics than state-of-the-art remote rendering via video streaming. In addition, our split-rendering solution decouples remote computation from local rendering and so does not limit local display update rate or display resolution.",
        "link": "https://dl.acm.org/doi/10.1145/3458305.3463379",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "QDiff: differential testing of quantum software stacks",
        "authors": "['Jiyuan Wang', 'Qian Zhang', 'Guoqing Harry Xu', 'Miryung Kim']",
        "date": "November 2021",
        "source": "ASE '21: Proceedings of the 36th IEEE/ACM International Conference on Automated Software Engineering",
        "abstract": "Over the past few years, several quantum software stacks (QSS) have been developed in response to rapid hardware advances in quantum computing. A QSS includes a quantum programming language, an optimizing compiler that translates a quantum algorithm written in a high-level language into quantum gate instructions, a quantum simulator that emulates these instructions on a classical device, and a software controller that sends analog signals to a very expensive quantum hardware based on quantum circuits. In comparison to traditional compilers and architecture simulators, QSSes are difficult to tests due to the probabilistic nature of results, the lack of clear hardware specifications, and quantum programming complexity. This work devises a novel differential testing approach for QSSes, named QDIFF with three major innovations: (1) We generate input programs to be tested via semantics-preserving, source to source transformation to explore program variants. (2) We speed up differential testing by filtering out quantum circuits that are not worthwhile to execute on quantum hardware by analyzing static characteristics such as a circuit depth, 2-gate operations, gate error rates, and T1 relaxation time. (3) We design an extensible equivalence checking mechanism via distribution comparison functions such as Kolmogorov-Smirnov test and cross entropy. We evaluate QDiff with three widely-used open source QSSes: Qiskit from IBM, Cirq from Google, and Pyquil from Rigetti. By running QDiff on both real hardware and quantum simulators, we found several critical bugs revealing potential instabilities in these platforms. QDiff's source transformation is effective in producing semantically equivalent yet not-identical circuits (i.e., 34% of trials), and its filtering mechanism can speed up differential testing by 66%.",
        "link": "https://dl.acm.org/doi/10.1109/ASE51524.2021.9678792",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Cache-aware Sparse Patterns for the Factorized Sparse Approximate Inverse Preconditioner",
        "authors": "['Sergi Laut', 'Ricard Borrell', 'Marc Casas']",
        "date": "June 2021",
        "source": "HPDC '21: Proceedings of the 30th International Symposium on High-Performance Parallel and Distributed Computing",
        "abstract": "Conjugate Gradient is a widely used iterative method to solve linear systems Ax=b with matrix A being symmetric and positive definite. Part of its effectiveness relies on finding a suitable preconditioner that accelerates its convergence. Factorized Sparse Approximate Inverse (FSAI) preconditioners are a prominent and easily parallelizable option. An essential element of a FSAI preconditioner is the definition of its sparse pattern, which constraints the approximation of the inverse A-1. This definition is generally based on numerical criteria. In this paper we introduce complementary architecture-aware criteria to increase the numerical effectiveness of the preconditioner without incurring in significant performance costs. In particular, we define cache-aware pattern extensions that do not trigger additional cache misses when accessing vector x in the y=Ax Sparse Matrix-Vector (SpMV) kernel. As a result, we obtain very significant reductions in terms of average solution time ranging between 12.94% and 22.85% on three different architectures - Intel Skylake, POWER9 and A64FX - over a set of 72 test matrices.",
        "link": "https://dl.acm.org/doi/10.1145/3431379.3460642",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "SoundStream: An End-to-End Neural Audio Codec",
        "authors": "['Neil Zeghidour', 'Alejandro Luebs', 'Ahmed Omran', 'Jan Skoglund', 'Marco Tagliasacchi']",
        "date": "None",
        "source": "IEEE/ACM Transactions on Audio, Speech and Language Processing",
        "abstract": "We present <italic>SoundStream</italic>, a novel neural audio codec that can efficiently compress speech, music and general audio at bitrates normally targeted by speech-tailored codecs. <italic>SoundStream</italic> relies on a model architecture composed by a fully convolutional encoder/decoder network and a residual vector quantizer, which are trained jointly end-to-end. Training leverages recent advances in text-to-speech and speech enhancement, which combine adversarial and reconstruction losses to allow the generation of high-quality audio content from quantized embeddings. By training with structured dropout applied to quantizer layers, a single model can operate across variable bitrates from 3 kbps to 18 kbps, with a negligible quality loss when compared with models trained at fixed bitrates. In addition, the model is amenable to a low latency implementation, which supports streamable inference and runs in real time on a smartphone CPU. In subjective evaluations using audio at 24 kHz sampling rate, <italic>SoundStream</italic> at 3 kbps outperforms Opus at 12 kbps and approaches EVS at 9.6 kbps. Moreover, we are able to perform joint compression and enhancement either at the encoder or at the decoder side with no additional latency, which we demonstrate through background noise suppression for speech.",
        "link": "https://dl.acm.org/doi/10.1109/TASLP.2021.3129994",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Threat-modeling-guided Trust-based Task Offloading for Resource-constrained Internet of Things",
        "authors": "['Matthew Bradbury', 'Arshad Jhumka', 'Tim Watson', 'Denys Flores', 'Jonathan Burton', 'Matthew Butler']",
        "date": "None",
        "source": "ACM Transactions on Sensor Networks",
        "abstract": "There is an increasing demand for Internet of Things (IoT) networks consisting of resource-constrained devices executing increasingly complex applications. Due to these resource constraints, IoT devices will not be able to execute expensive tasks. One solution is to offload expensive tasks to resource-rich edge nodes, which requires a framework that facilitates the selection of suitable edge nodes to perform task offloading. Therefore, in this article, we present a novel trust-model-driven system architecture, based on behavioral evidence, that is suitable for resource-constrained IoT devices and supports computation offloading. We demonstrate the viability of the proposed architecture with an example deployment of the Beta Reputation System trust model on real hardware to capture node behaviors. The open environment of edge-based IoT networks means that threats against edge nodes can lead to deviation from expected behavior. Hence, we perform a threat modeling to identify such threats. The proposed system architecture includes threat handling mechanisms that provide security properties such as confidentiality, authentication, and non-repudiation of messages in required scenarios and operate within the resource constraints. We evaluate the efficacy of the threat handling mechanisms and identify future work for the standards used.",
        "link": "https://dl.acm.org/doi/10.1145/3510424",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Integration of parallel I/O library and flash native accelerators: an evaluation of SIONlib with IME",
        "authors": "['Konstantinos Chasapis', 'Jean-Thomas Acquaviva', 'Sebastian Lührs']",
        "date": "April 2021",
        "source": "CHEOPS '21: Proceedings of the Workshop on Challenges and Opportunities of Efficient and Performant Storage Systems",
        "abstract": "As illustrated by the IO500 ranking [9] performance and scalability of storage systems have improved dramatically in the recent years. Most of these progresses are due to novel hardware and architectures. The resulting solutions tend to be more complex and harder to manage from an end-user perspective. This work proposes to address simultaneously the user experience and the performance issue. The solution is based on DDN's Infinite Memory Engine (IME) [12], an all Flash distributed cache that sits between the compute nodes and the parallel file system. IME delivers extreme performance for bulk data and complex I/O patterns but does not accelerate metadata traffic. To shield the end-user from the additional complexity of dealing with an extra storage-tier, IME accesses are embedded within SIONlib. SIONlib is a parallel I/O library, which has the ability to provide a task local like access scheme for a single file in the underlying file system that allows to reduce the number of metadata operations. This combined construction with IME and SIONlib allows end-users to obtain the Flash level of performance transparently for their applications and also to receive a performance boost on metadata operations. In respect of traditional file systems, according to the partest benchmark suite, the solution improves performance over 1.8 times on a single node and up to 4 times for multiple nodes. Moreover, experimenting with SIONlib parameters we observe that the resulting performances are more robust towards different data layout patterns and transfer sizes than with traditional file systems.",
        "link": "https://dl.acm.org/doi/10.1145/3439839.3458736",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "In-depth analyses of unified virtual memory system for GPU accelerated computing",
        "authors": "['Tyler Allen', 'Rong Ge']",
        "date": "November 2021",
        "source": "SC '21: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",
        "abstract": "The abstraction of a shared memory space over separate CPU and GPU memory domains has eased the burden of portability for many HPC codebases. However, users pay for the ease of use provided by systems-managed memory space with a moderate-to-high performance overhead. NVIDIA Unified Virtual Memory (UVM) is presently the primary real-world implementation of such abstraction and offers a functionally equivalent testbed for a novel in-depth performance study for both UVM and future Linux Heterogeneous Memory Management (HMM) compatible systems. The continued advocation for UVM and HMM motivates the improvement of the underlying system. We focus on a UVM-based system and investigate the root causes of the UVM overhead, which is a non-trivial task due to the complex interactions of multiple hardware and software constituents and the requirement of targeted analysis methodology. In this paper, we take a deep dive into the UVM system architecture and the internal behaviors of page fault generation and servicing. We reveal specific GPU hardware limitations using targeted benchmarks to uncover driver functionality as a real-time system when processing the resultant workload. We further provide a quantitative evaluation of fault handling for various applications under different scenarios, including prefetching and oversubscription. We find that the driver workload is dependent on the interactions among application access patterns, GPU hardware constraints, and Host OS components. We determine that the cost of host OS components is significant and present across implementations, warranting close attention. This study serves as a proxy for future shared memory systems such as those that interface with HMM.",
        "link": "https://dl.acm.org/doi/10.1145/3458817.3480855",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Improved Lite Audio-Visual Speech Enhancement",
        "authors": "['Shang-Yi Chuang', 'Hsin-Min Wang', 'Yu Tsao']",
        "date": "None",
        "source": "IEEE/ACM Transactions on Audio, Speech and Language Processing",
        "abstract": "Numerous studies have investigated the effectiveness of audio-visual multimodal learning for speech enhancement (AVSE) tasks, seeking a solution that uses visual data as auxiliary and complementary input to reduce the noise of noisy speech signals. Recently, we proposed a lite audio-visual speech enhancement (LAVSE) algorithm for a car-driving scenario. Compared to conventional AVSE systems, LAVSE requires less online computation and to some extent solves the user privacy problem on facial data. In this study, we extend LAVSE to improve its ability to address three practical issues often encountered in implementing AVSE systems, namely, the additional cost of processing visual data, audio-visual asynchronization, and low-quality visual data. The proposed system is termed improved LAVSE (iLAVSE), which uses a convolutional recurrent neural network architecture as the core AVSE model. We evaluate iLAVSE on the Taiwan Mandarin speech with video dataset. Experimental results confirm that compared to conventional AVSE systems, iLAVSE can effectively overcome the aforementioned three practical issues and can improve enhancement performance. The results also confirm that iLAVSE is suitable for real-world scenarios, where high-quality audio-visual sensors may not always be available.",
        "link": "https://dl.acm.org/doi/10.1109/TASLP.2022.3153265",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A First Look at Energy Consumption of NB-IoT in the Wild: Tools and Large-Scale Measurement",
        "authors": "['Deliang Yang', 'Xuan Huang', 'Jun Huang', 'Xiangmao Chang', 'Guoliang Xing', 'Yang Yang']",
        "date": "None",
        "source": "IEEE/ACM Transactions on Networking",
        "abstract": "Recent years have seen a widespread deployment of NB-IoT networks for massive machine-to-machine communication in the emerging 5G era. Unfortunately, the key aspects of NB-IoT networks, such as radio access performance and power consumption have not been well-understood due to lack of effective tools and closed nature of operational cellular infrastructure. In this paper, we develop NB-Scope - the first hardware NB-IoT diagnostic tool that supports fine-grained fusion of power and protocol traces. We then conduct a large-scale field measurement study consisting of 30 nodes deployed at over 1,200 locations in 4 regions during a period of three months. Our in-depth analysis of the collected 49 GB traces showed that NB-IoT nodes yield significantly imbalanced energy consumption in the wild, up to a ratio of 75:1, which may lead to short battery lifetime and frequent network partition. Such a high performance variance can be attributed to several key factors including diverse network coverage levels, long tail power profile, and excessive control message repetitions. We then explore the optimization of NB-IoT base station settings on a software-defined eNodeB testbed, and suggest several important design aspects that can be considered by future NB-IoT specifications and chipsets.",
        "link": "https://dl.acm.org/doi/10.1109/TNET.2021.3096656",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "NEUROTEC I: neuro-inspired artificial intelligence technologies for the electronics of the future",
        "authors": "['Melvin Galicia', 'Stephan Menzel', 'Farhad Merchant', 'Maximilian Müller', 'Hsin-Yu Chen', 'Qing-Tai Zhao', 'Felix Cüppers', 'Abdur R. Jalil', 'Qi Shu', 'Peter Schüffelgen', 'Gregor Mussler', 'Carsten Funck', 'Christian Lanius', 'Stefan Wiefels', 'Moritz von Witzleben', 'Christopher Bengel', 'Nils Kopperberg', 'Tobias Ziegler', 'R. Walied Ahmad', 'Alexander Krüger', 'Leticia Pöhls', 'Regina Dittmann', 'Susanne Hoffmann-Eifert', 'Vikas Rana', 'Detlev Grützmacher', 'Matthias Wuttig', 'Dirk Wouters', 'Andrei Vescan', 'Tobias Gemmeke', 'Joachim Knoch', 'Max Lemme', 'Rainer Leupers', 'Rainer Waser']",
        "date": "March 2022",
        "source": "DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe",
        "abstract": "The field of neuromorphic computing is approaching an era of rapid adoption driven by the urgent need of a substitute for the von Neumann computing architecture. NEUROTEC I: \"Neuro-inspired Artificial Intelligence Technologies for the Electronics of the Future\" project is an initiative sponsored by the German Federal Ministry of Education and Research (BMBF for its initials in German), that aims to effectively advance the foundations for the utilization and exploitation of neuromorphic computing. NEUROTEC I stands at its successful \"final stage\" driven by the collaboration from more than 8 institutes from the Jülich Research Center and the RWTH Aachen University, as well as collaboration from several high-tech industry partners. The NEUROTEC I project considers the field interplay among materials, circuits, design and simulation tools. This paper provides an overview of the project's overall structure and discusses the scientific achievements of its individual activities.",
        "link": "https://dl.acm.org/doi/10.5555/3539845.3540068",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Tap: an app framework for dynamically composable mobile systems",
        "authors": "['Naser AlDuaij', 'Jason Nieh']",
        "date": "June 2021",
        "source": "MobiSys '21: Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services",
        "abstract": "As smartphones and tablets have become ubiquitous, there is a growing demand for apps that can enable users to collaboratively use multiple mobile systems. We present Tap, a framework that makes it easy for users to dynamically compose collections of mobile systems and developers to write apps that make use of those impromptu collections. Tap users control the composition by simply tapping systems together for discovery and authentication. The physical interaction mimics and supports ephemeral user interactions without the need for tediously exchanging user contact information such as phone numbers or email addresses. Tapping triggers a simple NFC-based mechanism to exchange connectivity information and security credentials that works across heterogeneous networks and requires no user accounts or cloud infrastructure support. Tap makes it possible for apps to use existing mobile platform APIs across multiple mobile systems by virtualizing data sources so that local and remote data sources can be combined together upon tapping. Virtualized data sources can be hardware or software features, including media, clipboard, calendar events, and devices such as cameras and microphones. Leveraging existing mobile platform APIs makes it easy for developers to write apps that use hardware and software features across dynamically composed collections of mobile systems. We have implemented a Tap prototype that allows apps to make use of both unmodified Android and iOS systems. We have modified and implemented various apps using Tap to demonstrate that it is easy to use and can enable apps to provide powerful new functionality by leveraging multiple mobile systems. Our results show that Tap has good performance, even for high-bandwidth features, and is user and developer friendly.",
        "link": "https://dl.acm.org/doi/10.1145/3458864.3467678",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A study of persistent memory bugs in the Linux kernel",
        "authors": "['Duo Zhang', 'Om Rameshwar Gatla', 'Wei Xu', 'Mai Zheng']",
        "date": "June 2021",
        "source": "SYSTOR '21: Proceedings of the 14th ACM International Conference on Systems and Storage",
        "abstract": "Persistent memory (PM) technologies have inspired a wide range of PM-based system optimizations. However, building correct PM-based systems is difficult due to the unique characteristics of PM hardware. To better understand the challenges as well as the opportunities to address them, this paper presents a comprehensive study of PM-related bugs in the Linux kernel. By analyzing 1,350 PM-related kernel patches in depth, we derive multiple insights in terms of PM patch categories, PM bug patterns, consequences, and fix strategies. We hope our results could contribute to the development of effective PM bug detectors and robust PM-based systems.",
        "link": "https://dl.acm.org/doi/10.1145/3456727.3463783",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Emerging ExG-based NUI Inputs in Extended Realities: A Bottom-up Survey",
        "authors": "['Kirill A. Shatilov', 'Dimitris Chatzopoulos', 'Lik-Hang Lee', 'Pan Hui']",
        "date": "None",
        "source": "ACM Transactions on Interactive Intelligent Systems",
        "abstract": "Incremental and quantitative improvements of two-way interactions with extended realities (XR) are contributing toward a qualitative leap into a state of XR ecosystems being efficient, user-friendly, and widely adopted. However, there are multiple barriers on the way toward the omnipresence of XR; among them are the following: computational and power limitations of portable hardware, social acceptance of novel interaction protocols, and usability and efficiency of interfaces. In this article, we overview and analyse novel natural user interfaces based on sensing electrical bio-signals that can be leveraged to tackle the challenges of XR input interactions. Electroencephalography-based brain-machine interfaces that enable thought-only hands-free interaction, myoelectric input methods that track body gestures employing electromyography, and gaze-tracking electrooculography input interfaces are the examples of electrical bio-signal sensing technologies united under a collective concept of ExG. ExG signal acquisition modalities provide a way to interact with computing systems using natural intuitive actions enriching interactions with XR. This survey will provide a bottom-up overview starting from (i) underlying biological aspects and signal acquisition techniques, (ii) ExG hardware solutions, (iii) ExG-enabled applications, (iv) discussion on social acceptance of such applications and technologies, as well as (v) research challenges, application directions, and open problems; evidencing the benefits that ExG-based Natural User Interfaces inputs can introduce to the area of XR.",
        "link": "https://dl.acm.org/doi/10.1145/3457950",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Lucid: a language for control in the data plane",
        "authors": "['John Sonchack', 'Devon Loehr', 'Jennifer Rexford', 'David Walker']",
        "date": "August 2021",
        "source": "SIGCOMM '21: Proceedings of the 2021 ACM SIGCOMM 2021 Conference",
        "abstract": "Programmable switch hardware makes it possible to move fine-grained control logic inside the network data plane, improving performance for a wide range of applications. However, applications with integrated control are inherently hard to write in existing data-plane programming languages such as P4. This paper presents Lucid, a language that raises the level of abstraction for putting control functionality in the data plane. Lucid introduces abstractions that make it easy to write sophisticated data-plane applications with interleaved packet-handling and control logic, specialized type and syntax systems that prevent programmer bugs related to data-plane state, and an open-sourced compiler that translates Lucid programs into P4 optimized for the Intel Tofino. These features make Lucid general and easy to use, as we demonstrate by writing a suite of ten different data-plane applications in Lucid. Working prototypes take well under an hour to write, even for a programmer without prior Tofino experience, have around 10x fewer lines of code compared to P4, and compile efficiently to real hardware. In a stateful firewall written in Lucid, we find that moving control from a switch's CPU to its data-plane processor using Lucid reduces the latency of performance-sensitive operations by over 300X.",
        "link": "https://dl.acm.org/doi/10.1145/3452296.3472903",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "UltraDepth: Exposing High-Resolution Texture from Depth Cameras",
        "authors": "['Zhiyuan Xie', 'Xiaomin Ouyang', 'Xiaoming Liu', 'Guoliang Xing']",
        "date": "November 2021",
        "source": "SenSys '21: Proceedings of the 19th ACM Conference on Embedded Networked Sensor Systems",
        "abstract": "Time-of-flight (ToF) depth cameras have been increasingly adopted in various real-world applications, e.g., used with RGB cameras for advanced computer vision tasks like 3-D mapping or deployed alone in privacy-sensitive applications such as sleep monitoring. In this paper, we propose UltraDepth, the first system that can expose high-resolution texture from depth maps captured by off-the-shelf ToF cameras, simply by introducing a distorting IR source. The exposed texture information can significantly augment depth-based applications. Moreover, such a capability can be used to launch privacy attacks, which poses a major concern due to the prominence of ToF cameras. To design UltraDepth, we present an in-depth analysis on the impact of the distorting IR light on the distance measurement. We further show that, the reflection properties (reflectivity and incidence angle) of the objects will be encoded in the distorted depth map and hence can be leveraged to reveal texture of objects in UltraDepth. We then propose two practical implementations of UltraDepth, i.e., reflection-based and external IR-based implementations. Our extensive real-world experiments show that, the depth maps output by UltraDepth achieve 89.06%, 99.33%, 81.25% mean accuracy in object detection, face recognition and character recognition, respectively, which offers over 10x improvement over the ordinary depth maps and even approaches the performance of RGB and IR images in a number of scenarios. The findings of this work provide key insights for new research on depth-related computer vision and security of depth sensing devices.",
        "link": "https://dl.acm.org/doi/10.1145/3485730.3485927",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Toward confidential cloud computing",
        "authors": "['Mark Russinovich', 'Manuel Costa', 'Cédric Fournet', 'David Chisnall', 'Antoine Delignat-Lavaud', 'Sylvan Clebsch', 'Kapil Vaswani', 'Vikas Bhatia']",
        "date": "June 2021",
        "source": "Communications of the ACM",
        "abstract": "Extending hardware-enforced cryptographic protection to data while in use.",
        "link": "https://dl.acm.org/doi/10.1145/3453930",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Pantomime: Mid-Air Gesture Recognition with Sparse Millimeter-Wave Radar Point Clouds",
        "authors": "['Sameera Palipana', 'Dariush Salami', 'Luis A. Leiva', 'Stephan Sigg']",
        "date": "None",
        "source": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies",
        "abstract": "We introduce Pantomime, a novel mid-air gesture recognition system exploiting spatio-temporal properties of millimeter-wave radio frequency (RF) signals. Pantomime is positioned in a unique region of the RF landscape: mid-resolution mid-range high-frequency sensing, which makes it ideal for motion gesture interaction. We configure a commercial frequency-modulated continuous-wave radar device to promote spatial information over the temporal resolution by means of sparse 3D point clouds and contribute a deep learning architecture that directly consumes the point cloud, enabling real-time performance with low computational demands. Pantomime achieves 95% accuracy and 99% AUC in a challenging set of 21 gestures articulated by 41 participants in two indoor environments, outperforming four state-of-the-art 3D point cloud recognizers. We further analyze the effect of the environment in 5 different indoor environments, the effect of articulation speed, angle, and the distance of the person up to 5m. We have publicly made available the collected mmWave gesture dataset consisting of nearly 22,000 gesture instances along with our radar sensor configuration, trained models, and source code for reproducibility. We conclude that pantomime is resilient to various input conditions and that it may enable novel applications in industrial, vehicular, and smart home scenarios.",
        "link": "https://dl.acm.org/doi/10.1145/3448110",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Get The Best of the Three Worlds: Real-Time Neural Image Compression in a Non-GPU Environment",
        "authors": "['Zekun Zheng', 'Xiaodong Wang', 'Xinye Lin', 'Shaohe Lv']",
        "date": "October 2021",
        "source": "MM '21: Proceedings of the 29th ACM International Conference on Multimedia",
        "abstract": "Lossy image compression always faces a tradeoff between rate-distortion performance and compression/decompression speed. With the advent of neural image compression, hardware (GPU) becomes the new vertex in the tradeoff triangle. By resolving the high GPU dependency and improving the low speed of neural models, this paper proposes two non-GPU models that get the best of the three worlds. First, the CPU-friendly Independent Separable Down-Sampling (ISD) and Up-Sampling (ISU) modules are proposed to lighten the network while ensuring a large receptive field. Second, an asymmetric autoencoder architecture is adopted to boost the decoding speed. At last, the Inverse Quantization Residual (IQR) module is proposed to reduce the error caused by quantization. In terms of rate-distortion performance, our network surpasses the state-of-the-art real-time GPU neural compression work at medium and high bit rates. In terms of speed, our model's compression and decompression speeds surpass all other traditional compression methods except JPEG, using only CPUs. In terms of hardware, the proposed models are CPU friendly and perform stably well in a non-GPU environment. The code is publicly available at https://github.com/kengchikengchi/FasiNet.",
        "link": "https://dl.acm.org/doi/10.1145/3474085.3475667",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Full-credit flow control: a novel technique to implement deadlock-free adaptive routing",
        "authors": "['Yi Dai', 'Kai Lu', 'Sheng Ma', 'Junsheng Chang']",
        "date": "March 2022",
        "source": "DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe",
        "abstract": "Deadlock-free adaptive routing is extensively adopted in interconnection networks to improve communication bandwidth and reduce latency. However, existing deadlock-free flow control schemes either underutilize memory resources due to inefficient buffer management for simple hardware implementations, or rely on complicated coordination and synchronization mechanisms with high hardware complexity. In this work, we solve the deadlock problem from a different perspective by considering the deadlock as a lack of credit. With minor modifications of the credit accumulation procedure, our proposed full-credit flow control (FFC) ensures atomic buffer usage only based on local credit status while making full use of the buffer space. FFC can be easily integrated in the industrial router to achieve deadlock freedom with less area and power consumption, but 112% higher throughput, compared to the critical bubble scheme (CBS). We further propose a credit reservation strategy to eliminate the escape virtual channel (VC) cost for fully adaptive routing implementation. The synthesizing results demonstrate that FFC along with credit reservation (FFC-CR) can reduce the area by 29% and power consumption by 26% compared with CBS.",
        "link": "https://dl.acm.org/doi/10.5555/3539845.3540085",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "The Impact of Terrestrial Radiation on FPGAs in Data Centers",
        "authors": "['Andrew M. Keller', 'Michael J. Wirthlin']",
        "date": "None",
        "source": "ACM Transactions on Reconfigurable Technology and Systems",
        "abstract": "Field programmable gate arrays (FPGAs) are used in large numbers in data centers around the world. They are used for cloud computing and computer networking. The most common type of FPGA used in data centers are re-programmable SRAM-based FPGAs. These devices offer potential performance and power consumption savings. A single device also carries a small susceptibility to radiation-induced soft errors, which can lead to unexpected behavior. This article examines the impact of terrestrial radiation on FPGAs in data centers. Results from artificial fault injection and accelerated radiation testing on several data-center-like FPGA applications are compared. A new fault injection scheme provides results that are more similar to radiation testing. Silent data corruption (SDC) is the most commonly observed failure mode followed by FPGA unavailable and host unresponsive. A hypothetical deployment of 100,000 FPGAs in Denver, Colorado, will experience upsets in configuration memory every half-hour on average and SDC failures every 0.5–11 days on average.",
        "link": "https://dl.acm.org/doi/10.1145/3457198",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "CIB-HIER: Centralized Input Buffer Design in Hierarchical High-radix Routers",
        "authors": "['Cunlu Li', 'Dezun Dong', 'Shazhou Yang', 'Xiangke Liao', 'Guangyu Sun', 'Yongheng Liu']",
        "date": "None",
        "source": "ACM Transactions on Architecture and Code Optimization",
        "abstract": "Hierarchical organization is widely used in high-radix routers to enable efficient scaling to higher switch port count. A general-purpose hierarchical router must be symmetrically designed with the same input buffer depth, resulting in a large amount of unused input buffers due to the different link lengths. Sharing input buffers between different input ports can improve buffer utilization, but the implementation overhead also increases with the number of shared ports. Previous work allowed input buffers to be shared among all router ports, which maximizes the buffer utilization but also introduces higher implementation complexity. Moreover, such design can impair performance when faced with long packets, due to the head-of-line blocking in intermediate buffers.In this work, we explain that sharing unused buffers between a subset of router ports is a more efficient design. Based on this observation, we propose Centralized Input Buffer Design in Hierarchical High-radix Routers (CIB-HIER), a novel centralized input buffer design for hierarchical high-radix routers. CIB-HIER integrates multiple input ports onto a single tile and organizes all unused input buffers in the tile as a centralized input buffer. CIB-HIER only allows the centralized input buffer to be shared between ports on the same tile, without introducing additional intermediate virtual channels or global scheduling circuits. Going beyond the basic design of CIB-HIER, the centralized input buffer can be used to relieve the head-of-line blocking caused by shallow intermediate buffers, by stashing long packets in the centralized input buffer. Experimental results show that CIB-HIER is highly effective and can significantly increase the throughput of high-radix routers.",
        "link": "https://dl.acm.org/doi/10.1145/3468062",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "SoK: Remote Power Analysis",
        "authors": "['Macarena C. Martínez-Rodríguez', 'Ignacio M. Delgado-Lozano', 'Billy Bob Brumley']",
        "date": "August 2021",
        "source": "ARES '21: Proceedings of the 16th International Conference on Availability, Reliability and Security",
        "abstract": "In recent years, numerous attacks have appeared that aim to steal secret information from their victim using the power side-channel vector, yet without direct physical access. These attacks are called Remote Power Attacks or Remote Power Analysis, utilizing resources that are natively present inside the victim environment. However, there is no unified definition about the limitations that a power attack requires to be defined as remote. This paper aims to propose a unified definition and concrete threat models to clearly differentiate remote power attacks from non-remote ones. Additionally, we collect the main remote power attacks performed so far from the literature, and the principal proposed countermeasures to avoid them. The search of such countermeasures denoted a clear gap in preventing remote power attacks at the technical level. Thus, the academic community must face an important challenge to avoid this emerging threat, given the clear room for improvement that should be addressed in terms of defense and security of devices that work with private information.",
        "link": "https://dl.acm.org/doi/10.1145/3465481.3465773",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "CIB-HIER: Centralized Input Buffer Design in Hierarchical High-radix Routers",
        "authors": "['Cunlu Li', 'Dezun Dong', 'Shazhou Yang', 'Xiangke Liao', 'Guangyu Sun', 'Yongheng Liu']",
        "date": "None",
        "source": "ACM Transactions on Architecture and Code Optimization",
        "abstract": "Hierarchical organization is widely used in high-radix routers to enable efficient scaling to higher switch port count. A general-purpose hierarchical router must be symmetrically designed with the same input buffer depth, resulting in a large amount of unused input buffers due to the different link lengths. Sharing input buffers between different input ports can improve buffer utilization, but the implementation overhead also increases with the number of shared ports. Previous work allowed input buffers to be shared among all router ports, which maximizes the buffer utilization but also introduces higher implementation complexity. Moreover, such design can impair performance when faced with long packets, due to the head-of-line blocking in intermediate buffers.In this work, we explain that sharing unused buffers between a subset of router ports is a more efficient design. Based on this observation, we propose Centralized Input Buffer Design in Hierarchical High-radix Routers (CIB-HIER), a novel centralized input buffer design for hierarchical high-radix routers. CIB-HIER integrates multiple input ports onto a single tile and organizes all unused input buffers in the tile as a centralized input buffer. CIB-HIER only allows the centralized input buffer to be shared between ports on the same tile, without introducing additional intermediate virtual channels or global scheduling circuits. Going beyond the basic design of CIB-HIER, the centralized input buffer can be used to relieve the head-of-line blocking caused by shallow intermediate buffers, by stashing long packets in the centralized input buffer. Experimental results show that CIB-HIER is highly effective and can significantly increase the throughput of high-radix routers.",
        "link": "https://dl.acm.org/doi/10.1145/3468062",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A Design of Zynq-based Medical Image Edge Detection Accelerator",
        "authors": "['Bin Li', 'Jingxian Chen', 'Xuejun Zhang', 'Xianfu Xu', 'Yini Wei', 'Deyu Kong']",
        "date": "August 2021",
        "source": "ICBIP '21: Proceedings of the 6th International Conference on Biomedical Signal and Image Processing",
        "abstract": "Edge detection technology plays an important role in medical image processing. Sobel operator edge detection is one of the commonly used edge detection operators. At present, most of the solutions using Sobel operator for edge detection of medical images are based on CPU and GPU. Processing speed can become a serious problem as image data increases. The acceleration effect of FPGA on edge detection is quite significant. However, the traditional Sobel edge detection scheme based on FPGA is developed by hardware description language, which has high requirements for developers and is very unfavorable to debugging. Using the Zynq series of C/C++ programming for acceleration can perfectly solve the above problems. However, the current Zynq-based Sobel operator edge detection research, only horizontal edge and vertical edge detection. In order to extract more edge details from different angles, we proposed an improved Sobel operator based on Zynq to detect edges. The performance of the proposed improved Sobel algorithm and the conventional Sobel algorithm on CPU and Zynq platform is compared and evaluated in detail. Experimental results show that the proposed scheme can extract more edge details and achieve satisfactory acceleration effect.",
        "link": "https://dl.acm.org/doi/10.1145/3484424.3484434",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Unsupervised Digit Recognition Using Cosine Similarity In A Neuromemristive Competitive Learning System",
        "authors": "['Bon Woong Ku', 'Catherine D. Schuman', 'Md Musabbir Adnan', 'Tiffany M. Mintz', 'Raphael Pooser', 'Kathleen E. Hamilton', 'Garrett S. Rose', 'Sung Kyu Lim']",
        "date": "None",
        "source": "ACM Journal on Emerging Technologies in Computing Systems",
        "abstract": "This work addresses how to naturally adopt the l2-norm cosine similarity in the neuromemristive system and studies the unsupervised learning performance on handwritten digit image recognition. Proposed architecture is a two-layer fully connected neural network with a hard winner-take-all (WTA) learning module. For input layer, we propose single-spike temporal code that transforms input stimuli into the set of single spikes with different latencies and voltage levels. For a synapse model, we employ a compound memristor where stochastically switching binary-state memristors connected in parallel, which offers a reliable and scalable multi-state solution for synaptic weight storage. Hardware-friendly synaptic adaptation mechanism is proposed to realize spike-timing-dependent plasticity learning. Input spikes are sent out through those memristive synapses to each and every integrate-and-fire neuron in the fully connected output layer, where the hard WTA network motif introduces the competition based on cosine similarity for the given input stimuli. Finally, we present 92.64% accuracy performance on unsupervised digit recognition with only single-epoch MNIST dataset training via high-level simulations, including extensive analysis on the impact of system parameters.",
        "link": "https://dl.acm.org/doi/10.1145/3473036",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "The semantics of shared memory in Intel CPU/FPGA systems",
        "authors": "['Dan Iorga', 'Alastair F. Donaldson', 'Tyler Sorensen', 'John Wickerson']",
        "date": "October 2021",
        "source": "Proceedings of the ACM on Programming Languages",
        "abstract": "Heterogeneous CPU/FPGA devices, in which a CPU and an FPGA can execute together while sharing memory, are becoming popular in several computing sectors. In this paper, we study the shared-memory semantics of these devices, with a view to providing a firm foundation for reasoning about the programs that run on them. Our focus is on Intel platforms that combine an Intel FPGA with a multicore Xeon CPU. We describe the weak-memory behaviours that are allowed (and observable) on these devices when CPU threads and an FPGA thread access common memory locations in a fine-grained manner through multiple channels. Some of these behaviours are familiar from well-studied CPU and GPU concurrency; others are weaker still. We encode these behaviours in two formal memory models: one operational, one axiomatic. We develop executable implementations of both models, using the CBMC bounded model-checking tool for our operational model and the Alloy modelling language for our axiomatic model. Using these, we cross-check our models against each other via a translator that converts Alloy-generated executions into queries for the CBMC model. We also validate our models against actual hardware by translating 583 Alloy-generated executions into litmus tests that we run on CPU/FPGA devices; when doing this, we avoid the prohibitive cost of synthesising a hardware design per litmus test by creating our own 'litmus-test processor' in hardware. We expect that our models will be useful for low-level programmers, compiler writers, and designers of analysis tools. Indeed, as a demonstration of the utility of our work, we use our operational model to reason about a producer/consumer buffer implemented across the CPU and the FPGA. When the buffer uses insufficient synchronisation -- a situation that our model is able to detect -- we observe that its performance improves at the cost of occasional data corruption.",
        "link": "https://dl.acm.org/doi/10.1145/3485497",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "RedMulE: a compact FP16 matrix-multiplication accelerator for adaptive deep learning on RISC-V-based ultra-low-power SoCs",
        "authors": "['Yvan Tortorella', 'Luca Bertaccini', 'Davide Rossi', 'Luca Benini', 'Francesco Conti']",
        "date": "March 2022",
        "source": "DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe",
        "abstract": "The fast proliferation of extreme-edge applications using Deep Learning (DL) based algorithms required dedicated hardware to satisfy extreme-edge applications' latency, throughput, and precision requirements. While inference is achievable in practical cases, online finetuning and adaptation of general DL models are still highly challenging. One of the key stumbling stones is the need for parallel floating-point operations, which are considered unaffordable on sub-100mW extreme-edge SoCs. We tackle this problem with RedMulE (Reduced-precision matrix Multiplication Engine), a parametric low-power hardware accelerator for FP16 matrix multiplications - the main kernel of DL training and inference - conceived for tight integration within a cluster of tiny RISC-V cores based on the PULP (Parallel Ultra-Low-Power) architecture. In 22 nm technology, a 32-FMA RedMulE instance occupies just 0.07 mm2 (14% of an 8-core RISC-V cluster) and achieves up to 666MHz maximum operating frequency, for a throughput of 31.6 MAC/cycle (98.8% utilization). We reach a cluster-level power consumption of 43.5mW and a full-cluster energy efficiency of 688 16-bit GFLOPS/W. Overall, RedMulE features up to 4.65× higher energy efficiency and 22× speedup over SW execution on 8RISC-V cores.",
        "link": "https://dl.acm.org/doi/10.5555/3539845.3540099",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Low-power Near-data Instruction Execution Leveraging Opcode-based Timing Analysis",
        "authors": "['Tziouvaras Athanasios', 'Dimitriou Georgios', 'Stamoulis Georgios']",
        "date": "None",
        "source": "ACM Transactions on Architecture and Code Optimization",
        "abstract": "Traditional processor architectures utilize an external DRAM for data storage, while they also operate under worst-case timing constraints. Such designs are heavily constrained by the delay costs of the data transfer between the core pipeline and the DRAM, and they are incapable of exploiting the timing variations of their pipeline stages. In this work, we focus on a near-data processing methodology combined with a novel timing analysis technique that enables the adaptive frequency scaling of the core clock and boosts the performance of low-power designs. We propose a near-data processing and better-than-worst-case co-design methodology to efficiently move the instruction execution to the DRAM side and, at the same time, to allow the pipeline to operate at higher clock frequencies compared to the worst-case approach. To this end, we develop a timing analysis technique, which evaluates the timing requirements of individual instructions and we dynamically scale the clock frequency, according to the instructions types that currently occupy the pipeline. We evaluate the proposed methodology on six different RISC-V post-layout implementations using an HMC DRAM to enable the processing-in-memory (PIM) process. Results indicate an average speedup factor of 1.96× with a 1.6× reduction in energy consumption compared to a standard RISC-V PIM baseline implementation.",
        "link": "https://dl.acm.org/doi/10.1145/3504005",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Dual-Path Modeling With Memory Embedding Model for Continuous Speech Separation",
        "authors": "['Chenda Li', 'Zhuo Chen', 'Yanmin Qian']",
        "date": "None",
        "source": "IEEE/ACM Transactions on Audio, Speech and Language Processing",
        "abstract": "Continuous speech separation (CSS) aims at separating overlap-free targets from a long, partially-overlapped recording. Though it has shown promising results, the origin CSS framework does not consider cross-window information and long-span dependency. To alleviate these limitations, this work introduces two novel methods to implicitly and explicitly capture the long-span knowledge, respectively. We firstly apply the dual-path (DP) modeling architecture for the CSS framework, where the within and across window information are jointly modeled by alternating stacked local-global processing modules. Secondly, to further capture the long-span dependency, we introduce a memory-based model for CSS. An additional memory pool is designed to extract embedding from each small window, and the inter-window commutation is established above the memory embedding pool through an attention mechanism. This memory-based model can precisely control what information needs to be transferred across the windows, thus leading to both improved modeling capacity and interpretability. The experimental results on the LibriCSS dataset show that both strategies can well capture the long-span information of the continuous speech and significantly improve system performance. Moreover, further improvements are observed with the integration of these two methods.",
        "link": "https://dl.acm.org/doi/10.1109/TASLP.2022.3165712",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Two Methods to Describe New Shift Registers",
        "authors": "['Tao Wu']",
        "date": "October 2021",
        "source": "CSAE '21: Proceedings of the 5th International Conference on Computer Science and Application Engineering",
        "abstract": "Shift registers are typical digital logic structures for integrated circuits and are widely used in hardware descriptions. They can be used as early units before memories and be replaced by them later. In multi-precision shift registers, the control signal has heavy loads, and the critical path appears if the input is complex. In this paper, two techniques for synthesis are proposed to reduce either the critical path or power consumption. The first method divides the shift register into 2 or 3 parts, while the second method applies word-based registers to act as the shift register. Both techniques localize the control signals and reduce the path delay.",
        "link": "https://dl.acm.org/doi/10.1145/3487075.3487111",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "ASIC Design Principle Course with Combination of Online-MOOC and Offline-Inexpensive FPGA Board",
        "authors": "['Zhixiong Di', 'Yongming Tang', 'Jiahua Lu', 'Zhaoyang Lv']",
        "date": "June 2021",
        "source": "GLSVLSI '21: Proceedings of the 2021 on Great Lakes Symposium on VLSI",
        "abstract": "ASIC Design Principle (ASICDP) is a compulsory course for undergraduate majors in microelectronics and integrated circuits, and the focus of this paper is the teaching methods of online theoretical teaching and offline experimental teaching of this course. As is well known, in order to prevent and control COVID-19, the use of online platforms to carry out online teaching has attracted worldwide attention. In this paper, the teaching strategy \"Online-MOOC + Offline Inexpensive FPGA Board\" in ASICDP in the Spring 2020 semester is demonstrated, in where MOOC means Massive Open Online Course. The theoretical teaching content of ASICDP is entirely replicated from Hardware Acceleration Design Methodology (HADM) released by the present authors on \"China University MOOC,\" the largest MOOC platform in China. Meanwhile, with the support of the \"Xilinx & Ministry of Education University-Industry Collaborative Education Program,\" an FPGA development board called the \"Spartan Edge Accelerator Board\" (SEA Board) designed by the authors was used in the experimental teaching of the ASICDP. This method can be used to establish the linkage between online courses and offline experiments, and cultivate students' practical VLSI design and FPGA prototype verification skills. It is believed that for educators that want to improve courses related to ASIC design and FPGA prototype verification, Online-MOOC + Offline-Inexpensive FPGA Board is an effective method with lower cost that is easily promotable and replicated.",
        "link": "https://dl.acm.org/doi/10.1145/3453688.3461502",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Fast k-NN Graph Construction by GPU based NN-Descent",
        "authors": "['Hui Wang', 'Wan-Lei Zhao', 'Xiangxiang Zeng', 'Jianye Yang']",
        "date": "October 2021",
        "source": "CIKM '21: Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management",
        "abstract": "NN-Descent is a classic k-NN graph construction approach. It is still widely employed in machine learning, computer vision, and information retrieval tasks due to its efficiency and genericness. However, the current design only works well on CPU. In this paper, NN-Descent has been redesigned to adapt to the GPU architecture. A new graph update strategy called selective update is proposed. It reduces the data exchange between GPU cores and GPU global memory significantly, which is the processing bottleneck under GPU computation architecture. This redesign leads to full exploitation of the parallelism of the GPU hardware. In the meantime, the genericness, as well as the simplicity of NN-Descent, are well-preserved. Moreover, a procedure that allows to k-NN graph to be merged efficiently on GPU is proposed. It makes the construction of high-quality k-NN graphs for out-of-GPU-memory datasets tractable. Our approach is 100-250× faster than the single-thread NN-Descent and is 2.5-5× faster than the existing GPU-based approaches as we tested on million as well as billion scale datasets.",
        "link": "https://dl.acm.org/doi/10.1145/3459637.3482344",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Robust and Attack Resilient Logic Locking with a High Application-Level Impact",
        "authors": "['Yuntao Liu', 'Michael Zuzak', 'Yang Xie', 'Abhishek Chakraborty', 'Ankur Srivastava']",
        "date": "None",
        "source": "ACM Journal on Emerging Technologies in Computing Systems",
        "abstract": "Logic locking is a hardware security technique aimed at protecting intellectual property against security threats in the IC supply chain, especially those posed by untrusted fabrication facilities. Such techniques incorporate additional locking circuitry within an integrated circuit (IC) that induces incorrect digital functionality when an incorrect verification key is provided by a user. The amount of error induced by an incorrect key is known as the effectiveness of the locking technique. A family of attacks known as “SAT attacks” provide a strong mathematical formulation to find the correct key of locked circuits. To achieve high SAT resilience (i.e., complexity of SAT attacks), many conventional logic locking schemes fail to inject sufficient error into the circuit when the key is incorrect. For example, in the case of SARLock and Anti-SAT, there are usually very few (or only one) input minterms that cause any error at the circuit output. The state-of-the-art stripped functionality logic locking (SFLL) technique provides a wide spectrum of configurations that introduced a tradeoff between SAT resilience and effectiveness. In this work, we prove that such a tradeoff is universal among all logic locking techniques. To attain high effectiveness of locking without compromising SAT resilience, we propose a novel logic locking scheme, called Strong Anti-SAT (SAS). In addition to SAT attacks, removal-based attacks are another popular kind of attack formulation against logic locking where the attacker tries to identify and remove the locking structure. Based on SAS, we also propose Robust SAS (RSAS) that is resilient to removal attacks and maintains the same SAT resilience and effectiveness as SAS. SAS and RSAS have the following significant improvements over existing techniques. (1) We prove that the SAT resilience of SAS and RSAS against SAT attack is not compromised by increase in effectiveness. (2) In contrast to prior work that focused solely on the circuit-level locking impact, we integrate SAS-locked modules into an 80386 processor and show that SAS has a high application-level impact. (3) Our experiments show that SAS and RSAS exhibit better SAT resilience than SFLL and their effectiveness is similar to SFLL.",
        "link": "https://dl.acm.org/doi/10.1145/3446215",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Novel Approximate Multiplier Designs for Edge Detection Application",
        "authors": "['Yashaswi Mannepalli', 'Viraj Bharadwaj Korede', 'Madhav Rao']",
        "date": "June 2021",
        "source": "GLSVLSI '21: Proceedings of the 2021 on Great Lakes Symposium on VLSI",
        "abstract": "Approximate computing in general has garnered much needed attention in the design community owing to high power saving benefits, and at the same time quick generation of results. Approximate computing as a design technique continues to offer design advantages which is recently ceased by the ever decreasing technology scaling. Approximate computing is mostly applied to arithmetic designs, that has resulted in significant research interests. The paper proposes a reliable and efficient approximate multiplier design, that uses optimized lower part constant OR adder (OLOCA) design and hardware optimized approximate adder with normal error distribution (HOAANED) separately as two variants. The two approximate multipliers derived from OLOCA adder and HOAANED adder were found to be highly power and footprint efficient, and in addition offers performance improvement over other approximate multipliers. The error characteristics for the proposed multiplier designs were evaluated and compared with the existing approximate multiplier design. The proposed multiplier design along with the existing ones were synthesized using 45 nm CMOS technology and results were analyzed. The proposed approximate multipliers were further explored for canny edge detection application, and results for different standard images were found to be highly acceptable showing 99.9% of outcome similar to exact multiplier design.",
        "link": "https://dl.acm.org/doi/10.1145/3453688.3461482",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Toward Polymorphic Internet of Things Receivers Through Real-Time Waveform-Level Deep Learning",
        "authors": "['Francesco Restuccia', 'Tommaso Melodia']",
        "date": "September 2021",
        "source": "GetMobile: Mobile Computing and Communications",
        "abstract": "Wireless systems such as the Internet of Things (IoT) are changing the way we interact with the cyber and the physical world. As IoT systems become more and more pervasive, it is imperative to design wireless protocols that can effectively and efficiently support IoT devices and operations. On the other hand, today's IoT wireless systems are based on inflexible designs, which makes them inefficient and prone to a variety of wireless attacks. In this paper, we introduce the new notion of a deep learning-based polymorphic IoT receiver, able to reconfigure its waveform demodulation strategy itself in real time, based on the inferred waveform parameters. Our key innovation is the introduction of a novel embedded deep learning architecture that enables the solution of waveform inference problems, which is then integrated into a generalized hardware/software architecture with radio components and signal processing. Our polymorphic wireless receiver is prototyped on a custom-made software-defined radio platform. We show through extensive over-the-air experiments that the system achieves throughput within 87% of a perfect-knowledge Oracle system, thus demonstrating for the first time that polymorphic receivers are feasible.",
        "link": "https://dl.acm.org/doi/10.1145/3511285.3511294",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A community-driven approach to democratize access to satellite ground stations",
        "authors": "['Vaibhav Singh', 'Akarsh Prabhakara', 'Diana Zhang', 'Osman Yağan', 'Swarun Kumar']",
        "date": "October 2021",
        "source": "MobiCom '21: Proceedings of the 27th Annual International Conference on Mobile Computing and Networking",
        "abstract": "Should you decide to launch a nano-satellite today in Low-Earth Orbit (LEO), the cost of renting ground station communication infrastructure is likely to significantly exceed your launch costs. While space launch costs have lowered significantly with innovative launch vehicles, private players, and smaller payloads, access to ground infrastructure remains a luxury. This is especially true for smaller LEO satellites that are only visible at any location for a few tens of minutes a day and whose signals are extremely weak, necessitating bulky and expensive ground station infrastructure. In this paper, we present a community-driven distributed reception paradigm for LEO satellite signals where signals received on many tiny handheld receivers (not necessarily deployed on rooftops but also indoors) are coherently combined to recover the desired signal. This is made possible by employing new synchronization and receiver orientation techniques that study satellite trajectories and leverage the presence of other ambient signals. We compare our results with a large commercial receiver deployed on a rooftop and show a 8 dB SNR increase both indoors and outdoors using 8 receivers, costing $38 per RF frontend.",
        "link": "https://dl.acm.org/doi/10.1145/3447993.3448630",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "LED-to-LED based VLC systems: developments and open problems",
        "authors": "['Muhammad Sarmad Mir', 'Behnaz Majlesein', 'Borja Genoves Guzman', 'Julio Rufo', 'Domenico Giustiniano']",
        "date": "June 2021",
        "source": "IoL '21: Proceedings of the Workshop on Internet of Lights",
        "abstract": "Visible light communication (VLC) is an emerging short-range wireless communication technology using the unlicensed light spectrum. Light Emitting Diode (LED) is used as VLC transmitter, while photodiodes or image sensors are used as receiver, depending on the applications and hardware constraints. However, LEDs can be used not only as a transmitter, but also as a receiver in applications where cost is of primary concern. LED as a receiver is sensitive to a narrow band of wavelengths, it is robust to sunlight interference and widely available as compared to other light detectors. This paper surveys the potential and limitations of LED-to-LED communication. It also contributes to identifying the challenges and potential research directions in this rising area of interest.",
        "link": "https://dl.acm.org/doi/10.1145/3469264.3469805",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A golden age for computing frontiers, a dark age for computing education?",
        "authors": "['Christof Teuscher']",
        "date": "May 2021",
        "source": "CF '21: Proceedings of the 18th ACM International Conference on Computing Frontiers",
        "abstract": "There is no doubt that the body of knowledge spanned by the computing disciplines has gone through an unprecedented expansion, both in depth and breadth, over the last century. In this position paper, we argue that this expansion has led to a crisis in computing education: quite literally the vast majority of the topics of interest of this conference are not taught at the undergraduate level and most graduate courses will only scratch the surface of a few selected topics. But alas, industry is increasingly expecting students to be familiar with emerging topics, such as neuromorphic, probabilistic, and quantum computing, AI, and deep learning. We provide evidence for the rapid growth of emerging topics, highlight the decline of traditional areas, muse about the failure of higher education to adapt quickly, and delineate possible ways to avert the crisis by looking at how the field of physics dealt with significant expansions over the last centuries.",
        "link": "https://dl.acm.org/doi/10.1145/3457388.3458673",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Exploiting parallelism with vertex-clustering in processing-in-memory-based GCN accelerators",
        "authors": "['Yu Zhu', 'Zhenhua Zhu', 'Guohao Dai', 'Kai Zhong', 'Huazhong Yang', 'Yu Wang']",
        "date": "March 2022",
        "source": "DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe",
        "abstract": "Recently, Graph Convolutional Networks (GCNs) have shown powerful learning capabilities in graph processing tasks. Computing GCNs with conventional von Neumann architectures usually suffers from limited memory bandwidth due to the irregular memory access. Recent work has proposed Processing-In-Memory (PIM) architectures to overcome the bandwidth bottleneck in Convolutional Neural Networks (CNNs) by performing in-situ matrix-vector multiplication. However, the performance improvement and computation parallelism of existing CNN-oriented PIM architectures is hindered when performing GCNs because of the large scale and sparsity of graphs. To tackle these problems, this paper presents a parallelism enhancement framework for PIM-based GCN architectures. At the software level, we propose a fixed-point quantization method for GCNs, which reduces the PIM computation overhead with little accuracy loss. We also introduce the vertex clustering algorithm to the graph, minimizing the inter-cluster links and realizing cluster-level parallel computing on multi-core systems. At the hardware level, we design a Resistive Random Access Memory (RRAM) based multi-core PIM architecture for GCN, which supports the cluster-level parallelism. Besides, we propose a coarse-grained pipeline dataflow to cover the RRAM write costs and improve the GCN computation throughput. At the software/hardware interface level, we propose a PIM-aware GCN mapping strategy to achieve the optimal tradeoff between resource utilization and computation performance. We also propose edge dropping methods to reduce the inter-core communications with little accuracy loss. We evaluate our framework on typical datasets with multiple widely-used GCN models. Experimental results show that the proposed framework achieves 698×, 89×, and 41× speedup with 7108×, 255×, and 31× energy efficiency enhancement compared with CPUs, GPUs, and ASICs, respectively.",
        "link": "https://dl.acm.org/doi/10.5555/3539845.3540006",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Verification: can wifi backscatter replace RFID?",
        "authors": "['Farzan Dehbashi', 'Ali Abedi', 'Tim Brecht', 'Omid Abari']",
        "date": "October 2021",
        "source": "MobiCom '21: Proceedings of the 27th Annual International Conference on Mobile Computing and Networking",
        "abstract": "WiFi backscatter communication has been proposed to enable battery-free sensors to transmit data using WiFi networks. The main advantage of WiFi backscatter technologies over RFID is that data from their tags can be read using existing WiFi infrastructures instead of specialized readers. This can potentially reduce the complexity and cost of deploying battery-free sensors. Despite extensive work in this area, none of the existing systems are in widespread use today. We hypothesize that this is because WiFi-based backscatter tags do not scale well and their range and capabilities are limited when compared with RFID. To test this hypothesis we conduct several real-world experiments. We compare WiFi backscatter and RFID technologies in terms of RF harvesting capabilities, throughput, range and scalability. Our results show that existing WiFi backscatter tags cannot rely on RF harvesting (as opposed to RFID tags) due to their high power consumption. We find that WiFi backscatter tags must be quite close to a WiFi device to work robustly in non-line-of-sight scenarios, limiting their operating range. Furthermore, our results show that some WiFi backscatter systems can cause significant interference for existing WiFi traffic and be affected by them since they do not perform carrier sensing.",
        "link": "https://dl.acm.org/doi/10.1145/3447993.3448622",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Design and Implementation of the Processing Circuit of the Miniaturized Multi-channel Ultrasonic Signal",
        "authors": "['YouDi Kong', 'YuHua Wu', 'GuangJie Wang', 'JianRui Zhao', 'HaiTao Wang', 'WeiXing Zhang']",
        "date": "October 2021",
        "source": "EITCE '21: Proceedings of the 2021 5th International Conference on Electronic Information Technology and Computer Engineering",
        "abstract": "With the rapid development of the measurement technology and the sensor technology, the processing circuit of the multi-channel ultrasonic signal is more and more widely used in many fields. At present, the miniaturization technology of the circuit is a hot research problem, especially in the academia and industry. In the study, the transmitting and receiving circuit of the miniaturized multi-channel ultrasonic signal based on the integrated transceiver ultrasonic transducer is innovatively designed by combining the phase difference signal of the microcontroller with the new mode of the conventional multiplexer. The method has been tested and validated. It is concluded that the function of the transmission and reception of the ultrasonic signal can be completed through the design. The complexity of the processing circuit of the multi-channel ultrasonic signal is reduced. The miniaturization of the signal processing circuit is realized, and the multi-channel signal transmission of the ultrasonic transducer is ensured. The overall system which has some features such as simple architecture, small volume and light weight has wide prospect in the future market.",
        "link": "https://dl.acm.org/doi/10.1145/3501409.3501485",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Clobber-NVM: log less, re-execute more",
        "authors": "['Yi Xu', 'Joseph Izraelevitz', 'Steven Swanson']",
        "date": "April 2021",
        "source": "ASPLOS '21: Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "Non-volatile memory allows direct access to persistent storage via a load/store interface. However, because the cache is volatile, cached updates to persistent state will be dropped after a power loss. Failure-atomicity NVM libraries provide the means to apply sets of writes to persistent state atomically. Unfortunately, most of these libraries impose significant overhead.  This work proposes Clobber-NVM, a failure-atomicity library that ensures data consistency by reexecution. Clobber-NVM’s novel logging strategy, clobber logging, records only those transaction inputs that are overwritten during transaction execution. Then, after a failure, it recovers to a consistent state by restoring overwritten inputs and reexecuting any interrupted transactions. Clobber-NVM utilizes a clobber logging compiler pass for identifying the minimal set of writes that need to be logged. Based on our experiments, classical undo logging logs up to 42.6X more bytes than Clobber-NVM, and requires 2.4X to 4.7X more expensive ordering instructions (e.g., clflush and sfence). Less logging leads to better performance: Relative to prior art, Clobber-NVM provides up to 2.5X performance improvement over Mnemosyne, 2.6X over Intel’s PMDK, and up to 8.1X over HP’s Atlas.",
        "link": "https://dl.acm.org/doi/10.1145/3445814.3446730",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "SNR: Squeezing Numerical Range Defuses Bit Error Vulnerability Surface in Deep Neural Networks",
        "authors": "['Elbruz Ozen', 'Alex Orailoglu']",
        "date": "None",
        "source": "ACM Transactions on Embedded Computing Systems",
        "abstract": "As deep learning algorithms are widely adopted, an increasing number of them are positioned in embedded application domains with strict reliability constraints. The expenditure of significant resources to satisfy performance requirements in deep neural network accelerators has thinned out the margins for delivering safety in embedded deep learning applications, thus precluding the adoption of conventional fault tolerance methods. The potential of exploiting the inherent resilience characteristics of deep neural networks remains though unexplored, offering a promising low-cost path towards safety in embedded deep learning applications. This work demonstrates the possibility of such exploitation by juxtaposing the reduction of the vulnerability surface through the proper design of the quantization schemes with shaping the parameter distributions at each layer through the guidance offered by appropriate training methods, thus delivering deep neural networks of high resilience merely through algorithmic modifications. Unequaled error resilience characteristics can be thus injected into safety-critical deep learning applications to tolerate bit error rates of up to \\(\\) at absolutely zero hardware, energy, and performance costs while improving the error-free model accuracy even further.",
        "link": "https://dl.acm.org/doi/10.1145/3477007",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A Distributed Real-time Scheduling System for Industrial Wireless Networks",
        "authors": "['Venkata P. Modekurthy', 'Abusayeed Saifullah', 'Sanjay Madria']",
        "date": "None",
        "source": "ACM Transactions on Embedded Computing Systems",
        "abstract": "The concept of Industry 4.0 introduces the unification of industrial Internet-of-Things (IoT), cyber physical systems, and data-driven business modeling to improve production efficiency of the factories. To ensure high production efficiency, Industry 4.0 requires industrial IoT to be adaptable, scalable, real-time, and reliable. Recent successful industrial wireless standards such as WirelessHART appeared as a feasible approach for such industrial IoT. For reliable and real-time communication in highly unreliable environments, they adopt a high degree of redundancy. While a high degree of redundancy is crucial to real-time control, it causes a huge waste of energy, bandwidth, and time under a centralized approach and are therefore less suitable for scalability and handling network dynamics. To address these challenges, we propose DistributedHART—a distributed real-time scheduling system for WirelessHART networks. The essence of our approach is to adopt local (node-level) scheduling through a time window allocation among the nodes that allows each node to schedule its transmissions using a real-time scheduling policy locally and online. DistributedHART obviates the need of creating and disseminating a central global schedule in our approach, thereby significantly reducing resource usage and enhancing the scalability. To our knowledge, it is the first distributed real-time multi-channel scheduler for WirelessHART. We have implemented DistributedHART and experimented on a 130-node testbed. Our testbed experiments as well as simulations show at least 85% less energy consumption in DistributedHART compared to existing centralized approach while ensuring similar schedulability.",
        "link": "https://dl.acm.org/doi/10.1145/3464429",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Research on Machine Translation and Computer Aided Translation Based on Cloud Computing",
        "authors": "['Xiaohui Zhang']",
        "date": "September 2021",
        "source": "ICISCAE 2021: 2021 4th International Conference on Information Systems and Computer Aided Education",
        "abstract": "New breakthroughs have not been made in the principle and technology of machine translation, and high-quality automatic translation has not yet been realized. The core technology of computer-aided translation is translation memory technology. Although computer-aided translation has made considerable progress, the translation memory technology has not yet made a decisive breakthrough. Aiming at the problem that the translation time of machine-aided translation system is relatively long at present, the design of machine-aided translation system based on cloud computing is proposed. A new machine-aided translation system is designed by referring to the cloud computing model. The hardware of the system is divided into four layers: user layer, service layer, computing layer and storage layer. The storage structure, translation structure and retrieval structure are designed respectively. After that, the software and hardware of the system are designed. The test results show that the translation time of the machine-assisted translation system based on cloud computing is shorter than that of the traditional machine translation system.",
        "link": "https://dl.acm.org/doi/10.1145/3482632.3484009",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Development of an online resource integration system for computer aided aesthetic education by big data technology",
        "authors": "['Xinyun Du', 'Tsungshun Hsieh']",
        "date": "September 2021",
        "source": "ICISCAE 2021: 2021 4th International Conference on Information Systems and Computer Aided Education",
        "abstract": "Big data technology is a currently emerging technology that plays an important role in driving online education in depth. The article discusses the development of an online resource integration system for university general aesthetic education based on big data technology, firstly, it introduces the system design and the main functional modules, including online resource management module, information information management module, and system maintenance module, introduces the main functions of the system, such as resource retrieval service, information interaction service, and resource utilization service, and finally, from the perspectives of hardware configuration, system architecture system integration and cost control are proposed.",
        "link": "https://dl.acm.org/doi/10.1145/3482632.3484076",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A flash-based current-mode IC to realize quantized neural networks",
        "authors": "['Kyler R. Scott', 'Cheng-Yen Lee', 'Sunil P. Khatri', 'Sarma Vrudhula']",
        "date": "March 2022",
        "source": "DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe",
        "abstract": "This paper presents a mixed-signal architecture for implementing Quantized Neural Networks (QNNs) using flash transistors to achieve extremely high throughput with extremely low power, energy and memory requirements. Its low resource consumption makes our design especially suited for use in edge devices. The network weights are stored in-memory using flash transistors, and nodes perform operations in the analog current domain. Our design can be programmed with any QNN whose hyperparameters (the number of layers, filters, or filter size, etc) do not exceed the maximum provisioned. Once the flash devices are programmed with a trained model and the IC is given an input, our architecture performs inference with zero access to off-chip memory. We demonstrate the robustness of our design under current-mode non-linearities arising from process and voltage variations. We test validation accuracy on the ImageNet dataset, and show that our IC suffers only 0.6% and 1.0% reduction in classification accuracy for Top-1 and Top-5 outputs, respectively. Our implementation results in a ~50× reduction in latency and energy when compared to a recently published mixed-signal ASIC implementation, with similar power characteristics. Our approach provides layer partitioning and node sharing possibilities, which allow us to trade off latency, power, and area amongst each other.",
        "link": "https://dl.acm.org/doi/10.5555/3539845.3540082",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Exploring planning and operations design space for EV charging stations",
        "authors": "['Sangyoung Park', 'Alma Pröbstl', 'Wanli Chang', 'Anuradha Annaswamy', 'Samarjit Chakraborty']",
        "date": "March 2021",
        "source": "SAC '21: Proceedings of the 36th Annual ACM Symposium on Applied Computing",
        "abstract": "EVs suffer from long charging times and short-drive ranges, limiting EV usage to daily short-range commuting rather than general purpose use. Among the candidates for EV charging infrastructures, the public EV charging station architecture has benefits in that it allows an efficient investment of costly equipments, and a long-range travel with multiple charging cycles. This paper focuses on an EC charging station architecture comprising PV panels, an energy storage system (ESS) and multiple fast-DC charging posts. Systematically deriving the optimal planning, i.e., determining the optimal sizes of these components, is a complicated problem as the EV charging station operations and planning are intertwined. In this paper, we derive EV charging station operation policies by formulating an average reward Markov decision process (MDP) maximization problem to synthesize controllers that maximize the operating income. Then, these controllers are used to evaluate the operating income, for the purpose of EV charging station planning. For efficient exploration of the design space, we perform a mixed search-based technique combining sequential quadratic programming (SQP) with a greedy algorithm. There will be significant gain in terms of long-term operating cost when the costs of ESS and PV panels continue to reduce in the future. Our solution framework is a helpful tool for such reasoning, and for identifying optimal planning and operation policies for public EV charging stations.",
        "link": "https://dl.acm.org/doi/10.1145/3412841.3441896",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Extending Intel-x86 consistency and persistency: formalising the semantics of Intel-x86 memory types and non-temporal stores",
        "authors": "['Azalea Raad', 'Luc Maranget', 'Viktor Vafeiadis']",
        "date": "January 2022",
        "source": "Proceedings of the ACM on Programming Languages",
        "abstract": "Existing semantic formalisations of the Intel-x86 architecture cover only a small fragment of its available features that are relevant for the consistency semantics of multi-threaded programs as well as the persistency semantics of programs interfacing with non-volatile memory. We extend these formalisations to cover: (1) non-temporal writes, which provide higher performance and are used to ensure that updates are flushed to memory; (2) reads and writes to other Intel-x86 memory types, namely uncacheable, write-combined, and write-through; as well as (3) the interaction between these features. We develop our formal model in both operational and declarative styles, and prove that the two characterisations are equivalent. We have empirically validated our formalisation of the consistency semantics of these additional features and their subtle interactions by extensive testing on different Intel-x86 implementations.",
        "link": "https://dl.acm.org/doi/10.1145/3498683",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "EmSBoTScript: A Tiny Virtual Machine-Based Embedded Software Framework",
        "authors": "['Long Peng', 'Hao Xu', 'Jie Yu', 'Xiaodong Liu', 'Fei Guan']",
        "date": "December 2021",
        "source": "CSAI '21: Proceedings of the 2021 5th International Conference on Computer Science and Artificial Intelligence",
        "abstract": "Modern swarm and modular robotic systems can be composed of diverse and miniature hardware components. To deal with heterogeneity, researchers adopt a virtual machine (VM)-based approach to ease software programming and updating for robotic systems. However, current VM-based solutions neither consider resource-constrained devices, nor have limited capabilities. This paper introduces EmSBoTScript, a tiny VM-based robotic software framework that is tailored for heterogeneous and miniature platforms. We endow EmSBoTScript with features of CPU independence, low memory footprint, concurrency and synchronization. We elaborate its programming model, script language and VM architecture to show its novelty in this paper. Implementation details and benchmark results are also provided.",
        "link": "https://dl.acm.org/doi/10.1145/3507548.3507592",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "SecNVM: An Efficient and Write-Friendly Metadata Crash Consistency Scheme for Secure NVM",
        "authors": "['Mengya Lei', 'Fan Li', 'Fang Wang', 'Dan Feng', 'Xiaomin Zou', 'Renzhi Xiao']",
        "date": "None",
        "source": "ACM Transactions on Architecture and Code Optimization",
        "abstract": "Data security is an indispensable part of non-volatile memory (NVM) systems. However, implementing data security efficiently on NVM is challenging, since we have to guarantee the consistency of user data and the related security metadata. Existing consistency schemes ignore the recoverability of the SGX style integrity tree (SIT) and the access correlation between metadata blocks, thereby generating unnecessary NVM write traffic. In this article, we propose SecNVM, an efficient and write-friendly metadata crash consistency scheme for secure NVM. SecNVM utilizes the observation that for a lazily updated SIT, the lost tree nodes after a crash can be recovered by the corresponding child nodes in NVM. It reduces the SIT persistency overhead through a restrained write-back metadata cache and exploits the SIT inter-layer dependency for recovery. Next, leveraging the strong access correlation between the counter and DMAC, SecNVM improves the efficiency of security metadata access through a novel collaborative counter-DMAC scheme. In addition, it adopts a lightweight address tracker to reduce the cost of address tracking for fast recovery. Experiments show that compared to the state-of-the-art schemes, SecNVM improves the performance and decreases write traffic a lot, and achieves an acceptable recovery time.",
        "link": "https://dl.acm.org/doi/10.1145/3488724",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Sherlock: A Multi-Objective Design Space Exploration Framework",
        "authors": "['Quentin Gautier', 'Alric Althoff', 'Christopher L. Crutchfield', 'Ryan Kastner']",
        "date": "None",
        "source": "ACM Transactions on Design Automation of Electronic Systems",
        "abstract": "Design space exploration (DSE) provides intelligent methods to tune the large number of optimization parameters present in modern FPGA high-level synthesis tools. High-level synthesis parameter tuning is a time-consuming process due to lengthy hardware compilation times—synthesizing an FPGA design can take tens of hours. DSE helps find an optimal solution faster than brute-force methods without relying on designer intuition to achieve high-quality results. Sherlock is a DSE framework that can handle multiple conflicting optimization objectives and aggressively focuses on finding Pareto-optimal solutions. Sherlock integrates a model selection process to choose the regression model that helps reach the optimal solution faster. Sherlock designs a strategy based around the multi-armed bandit problem, opting to balance exploration and exploitation based on the learned and expected results. Sherlock can decrease the importance of models that do not provide correct estimates, reaching the optimal design faster. Sherlock is capable of tailoring its choice of regression models to the problem at hand, leading to a model that best reflects the application design space. We have tested the framework on a large dataset of FPGA design problems and found that Sherlock converges toward the set of optimal designs faster than similar frameworks.",
        "link": "https://dl.acm.org/doi/10.1145/3511472",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "The Control System Design for 600kV High Voltage Platform of HIAF Electron Cooler",
        "authors": "['Yunbin Zhou', 'Lijun Mao', 'Wei Zhang', 'Kaiming Yan', 'Xiaoming Ma', 'Mingrui Li']",
        "date": "October 2021",
        "source": "CSAE '21: Proceedings of the 5th International Conference on Computer Science and Application Engineering",
        "abstract": "HIAF is the next generation heavy ion accelerator in China, it contains lots of sub facilities. The electron cooler is one of the most significant facilities in HIAF of Spectrometer Ring (SRing). With the electron cooler, SRing could generate high quality and high intensity ion beam for the experiments. The electron cooler in HIAF was designed to produce 600KeV electron beam. To achieve this design target, high voltage needs to reach 600kV maximum and ripple wave less than 1*10-4. A new control system was designed for the high voltage platform which uses cascaded transformer construction. The system makes use of XGS-PON network as the main means of communication. Zynq 7015 as the CPU of the embedded controller. The controller integrated 2 DAC ports with 100KS/S and 4 ADC ports with 200KS/S for high voltage modules setting and high voltage divider monitor, 8 low-speed ADC ports for auxiliary power supplies, and the environment sensors. The embedded Ubuntu Linux and the EPICS frameworks were programmed in the controller, all of the control parameters were sent through channel access protocol. No need for host computer to participate in control logic, only used for display. The hardware mentioned above interacts with the operating system through the FPGA part of Zynq. The FPGA is programed as a coprocessor for communicating, data processing and interlock control. Corresponding drivers are integrated in the Linux system at the same time.",
        "link": "https://dl.acm.org/doi/10.1145/3487075.3487085",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Separation of Powers in Federated Learning (Poster Paper)",
        "authors": "['Pau-Chen Cheng', 'Kevin Eykholt', 'Zhongshu Gu', 'Hani Jamjoom', 'K. R. Jayaram', 'Enriquillo Valdez', 'Ashish Verma']",
        "date": "October 2021",
        "source": "ResilientFL '21: Proceedings of the First Workshop on Systems Challenges in Reliable and Secure Federated Learning",
        "abstract": "In federated learning (FL), model updates from mutually distrusting parties are aggregated in a centralized fusion server. The concentration of model updates simplifies FL's model building process, but might lead to unforeseeable information leakage. This problem has become acute due to recent FL attacks that can reconstruct large fractions of training data from ostensibly \"sanitized\" model updates. In this paper, we re-examine the current design of FL systems under the new security model of reconstruction attacks. To break down information concentration, we build TRUDA, a new cross-silo FL system, employing a trustworthy and decentralized aggregation architecture. Based on the unique computational properties of model-fusion algorithms, we disassemble all exchanged model updates at the parameter-granularity and re-stitch them to form random partitions designated for multiple hardware-protected aggregators. Thus, each aggregator only has a fragmentary and shuffled view of model updates and is oblivious to the model architecture. The deployed security mechanisms in TRUDA can effectively mitigate training data reconstruction attacks, while still preserving the accuracy of trained models and keeping performance overheads low.",
        "link": "https://dl.acm.org/doi/10.1145/3477114.3488765",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "WaFFLe: Gated Cache-<underline>Wa</underline>ys with Per-Core <underline>F</underline>ine-Grained DV<underline>F</underline>S for Reduced On-Chip Temperature and <underline>Le</underline>akage Consumption",
        "authors": "['Shounak Chakraborty', 'Magnus Själander']",
        "date": "None",
        "source": "ACM Transactions on Architecture and Code Optimization",
        "abstract": "Managing thermal imbalance in contemporary chip multi-processors (CMPs) is crucial in assuring functional correctness of modern mobile as well as server systems. Localized regions with high activity, e.g., register files, ALUs, FPUs, and so on, experience higher temperatures than the average across the chip and are commonly referred to as hotspots. Hotspots affect functional correctness of the underlying circuitry and a noticeable increase in leakage power, which in turn generates heat in a self-reinforced cycle. Techniques that reduce the severity of or completely eliminate hotspots can maintain functional correctness along with improving performance of CMPs. Conventional dynamic thermal management targets the cores to reduce hotspots but often ignores caches, which are known for their high leakage power consumption.This article presents WaFFLe, an approach that targets the leakage power of the last-level cache (LLC) and hotspots occurring at the cores. WaFFLe turns off LLC-ways to reduce leakage power and to generate on-chip thermal buffers. In addition, fine-grained DVFS is applied during long LLC miss induced stalls to reduce core temperature. Our results show that WaFFLe reduces peak and average temperature of a 16-core based homogeneous tiled CMP with up to 8.4 ֯ C and 6.2 ֯ C, respectively, with an average performance degradation of only 2.5 %. We also show that WaFFLe outperforms a state-of-the-art cache-based technique and a greedy DVFS policy.",
        "link": "https://dl.acm.org/doi/10.1145/3471908",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Generic change detection (almost entirely) in the dataplane",
        "authors": "['Gonçalo Matos', 'Salvatore Signorello', 'Fernando M. V. Ramos']",
        "date": "December 2021",
        "source": "ANCS '21: Proceedings of the Symposium on Architectures for Networking and Communications Systems",
        "abstract": "Identifying traffic changes accurately sits at the core of many network tasks, from congestion analysis to intrusion detection. Modern systems leverage sketch-based structures that achieve favourable memory-accuracy tradeoffs by maintaining compact summaries of traffic data. Mainly used to detect heavy-hitters (usually the major source of network congestion), some can be adapted to detect traffic changes, but they fail on generality. As their core data structures track elephant flows, they miss to identify mice traffic that may be the main cause of change (e.g., microbursts or low-volume attacks). We present k-meleon, an in-network online change detection system that identifies heavy-changes - instead of changes amongst heavy-hitters only, a subtle but crucial difference. Our main contribution is a variant of the k-ary sketch (a well-known heavy-change detector) that runs on the data plane of a switch. The challenge was the batch-based design of the original. To address it, k-meleon features a new stream-based design that matches the pipeline computation model and fits its tough constraints. A preliminary evaluation shows that k-meleon achieves the same level of accuracy for online detection as the offline k-ary, detecting changes for any type of flow: be it an elephant, or a mouse.",
        "link": "https://dl.acm.org/doi/10.1145/3493425.3502767",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Comparative Evaluation of Machine Learning Inference Machines on Edge-class Devices",
        "authors": "['Petros Amanatidis', 'George Iosifidis', 'Dimitris Karampatzakis']",
        "date": "November 2021",
        "source": "PCI '21: Proceedings of the 25th Pan-Hellenic Conference on Informatics",
        "abstract": "Computer science and engineering have evolved rapidly over the last decade offering innovative Machine Learning frameworks and high-performance hardware devices. Executing data analytics at the edge promises to transform the mobile computing paradigm by bringing intelligence next to the end user. However, it remains an open question to explore if, and to what extent, today’s Edge-class devices can support ML frameworks and which is the best configuration for efficient task execution. This paper provides a comparative evaluation of Machine Learning inference machines on Edge-class compute engines. The testbed consists of two hardware compute engines (i.e., CPU-based Raspberry Pi 4 and Google Edge TPU accelerator) and two inference machines (i.e., TensorFlow-Lite and Arm NN). Through an extensive set of experiments in our bespoke testbed, we compared three setups using TensorFlow-Lite ML framework, in terms of accuracy, execution time, and energy efficiency. Based on the results, an optimized configuration of the workload parameters can increase accuracy by 10%, and in addition, the class of the Edge compute engine in combination with the inference machine affects execution time by 86% and power consumption by almost 145%.",
        "link": "https://dl.acm.org/doi/10.1145/3503823.3503843",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "FlexPushdownDB: hybrid pushdown and caching in a cloud DBMS",
        "authors": "['Yifei Yang', 'Matt Youill', 'Matthew Woicik', 'Yizhou Liu', 'Xiangyao Yu', 'Marco Serafini', 'Ashraf Aboulnaga', 'Michael Stonebraker']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Modern cloud databases adopt a storage-disaggregation architecture that separates the management of computation and storage. A major bottleneck in such an architecture is the network connecting the computation and storage layers. Two solutions have been explored to mitigate the bottleneck: caching and computation pushdown. While both techniques can significantly reduce network traffic, existing DBMSs consider them as orthogonal techniques and support only one or the other, leaving potential performance benefits unexploited.In this paper we present FlexPushdownDB (FPDB), an OLAP cloud DBMS prototype that supports fine-grained hybrid query execution to combine the benefits of caching and computation pushdown in a storage-disaggregation architecture. We build a hybrid query executor based on a new concept called separable operators to combine the data from the cache and results from the pushdown processing. We also propose a novel Weighted-LFU cache replacement policy that takes into account the cost of pushdown computation. Our experimental evaluation on the Star Schema Benchmark shows that the hybrid execution outperforms both the conventional caching-only architecture and pushdown-only architecture by 2.2X. In the hybrid architecture, our experiments show that Weighted-LFU can outperform the baseline LFU by 37%.",
        "link": "https://dl.acm.org/doi/10.14778/3476249.3476265",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Machine Learning and Soil Humidity Sensing: Signal Strength Approach",
        "authors": "['Lea Dujić Rodić', 'Tomislav Županović', 'Toni Perković', 'Petar Šolić', 'Joel J. P. C. Rodrigues']",
        "date": "None",
        "source": "ACM Transactions on Internet Technology",
        "abstract": "The Internet-of-Things vision of ubiquitous and pervasive computing gives rise to future smart irrigation systems comprising the physical and digital worlds. A smart irrigation ecosystem combined with Machine Learning can provide solutions that successfully solve the soil humidity sensing task in order to ensure optimal water usage. Existing solutions are based on data received from the power hungry/expensive sensors that are transmitting the sensed data over the wireless channel. Over time, the systems become difficult to maintain, especially in remote areas due to the battery replacement issues with a large number of devices. Therefore, a novel solution must provide an alternative, cost- and energy-effective device that has unique advantage over the existing solutions. This work explores the concept of a novel, low-power, LoRa-based, cost-effective system that achieves humidity sensing using Deep Learning techniques that can be employed to sense soil humidity with high accuracy simply by measuring the signal strength of the given underground beacon device.",
        "link": "https://dl.acm.org/doi/10.1145/3418207",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "An embedded controller for the hydraulic walking robot WLBOT",
        "authors": "['Ziqi Liu', 'Bo Jin', 'Shuo Zhai', 'Junkui Dong']",
        "date": "April 2021",
        "source": "ICRSA '21: Proceedings of the 2021 4th International Conference on Robot Systems and Applications",
        "abstract": "This paper presents an embedded controller for the quadruped hydraulic robot WLBOT. First, we give an overview of a WLBOT system. Second, the hardware design and the software architecture of the embedded controller are introduced. The embedded controller takes charge of multi-sensor information processing and signal output of the servo valve, as well as receiving control command and sending processed information via Control Area Network (CAN) bus. What's more, the realization of the 2kHz high-speed control of the embedded controller is illustrated. Finally, the platform is constructed, in which the feasibility of the design and the validity of the control algorithm is verified. It shows that WLBOT can walk properly in a PID controller as expected.",
        "link": "https://dl.acm.org/doi/10.1145/3467691.3467703",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A Methodology for Simulating Multi-chiplet Systems Using Open-source Simulators",
        "authors": "['Haocong Zhi', 'Xianuo Xu', 'Weijian Han', 'Zhilin Gao', 'Xiaohang Wang', 'Maurizio Palesi', 'Amit Kumar Singh', 'Letian Huang']",
        "date": "September 2021",
        "source": "NANOCOM '21: Proceedings of the Eight Annual ACM International Conference on Nanoscale Computing and Communication",
        "abstract": "Multi-chiplet systems are a new design paradigm to mitigate the chip design cost and improve yield for complex SoCs. The design space of multi-chiplet systems is much larger compared to a single chip SoC system. To support early stage design space exploration, simulators are of paramount importance. However, existing open-source multi-/many-core simulators are not suitable for simulating large-scale multi-chiplet systems due to the following reasons: 1) lack of accurate inter-chiplet interconnection model, and 2) incapable of supporting large-scale parallel simulation with accurate interconnection modelling. Therefore, we propose a methodology for building up a simulator for multi-chiplet systems using open-source simulators like gem5, sniper, gpgpu-sim, etc. This simulation methodology mimics the reuse and integration idea of chiplets, that is, these existing open-source simulators are reused to simulate individual chiplets, and an inter-simulator-process communication and synchronization protocol is proposed to simulate inter-chiplet communication. The proposed simulation methodology has the following features: 1) Parallel simulation for large-scale systems is supported with inter- and intra-chiplet interconnection accurately modelled. 2) Both distributed and shared memory models are supported for multi-chiplet systems. We also provide a method to modify the code of the open-source simulators like gem5, sniper, gpgpu-sim, etc. for multi-chiplet simulation, and we have released the source code of multi-chiplet simulators based on gem5, sniper, gpgpu-sim at https://github.com/FCAS-SCUT/chiplet_simulators. In the future we will port more applications/benchmarks and integrate more open-source simulators.",
        "link": "https://dl.acm.org/doi/10.1145/3477206.3477459",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "SpiderWeb: Enabling Through-Screen Visible Light Communication",
        "authors": "['Hanting Ye', 'Qing Wang']",
        "date": "November 2021",
        "source": "SenSys '21: Proceedings of the 19th ACM Conference on Embedded Networked Sensor Systems",
        "abstract": "We are now witnessing a trend of realizing full-screen on electronic devices such as smartphones to maximize their screen-to-body ratio for a better user experience. Thus the bezel/narrow-bezel on today's devices to host various line-of-sight sensors would disappear. This trend not only is forcing sensors like the front cameras to be placed under the screen of devices, but also will challenge the deployment of the emerging Visible Light Communication (VLC) technology, a paradigm for the next-generation wireless communication. In this work, we propose the concept of through-screen VLC with photosensors placed under Organic Light-Emitting Diode (OLED) screen. Though being transparent, an OLED screen greatly attenuates the intensity of passing-through light, degrading the efficiency of intensity-based VLC systems. In this paper, we instead exploit the color domain to build SpiderWeb, a through-screen VLC system. For the first time, we observe that an OLED screen introduces a color-pulling effect at photosensors, affecting the decoding of color-based VLC signals. Motivated by this observation and by the structure of spider's web, we design the SWebCSK Color-Shift Keying modulation scheme and a slope-based demodulation method, which can eliminate the color-pulling effect. We prototype SpiderWeb with off-the-shelf hardware and evaluate its performance thoroughly under various scenarios. The results show that compared to existing solutions, our solutions can reduce the bit error rate by two orders of magnitude and can achieve a 3.4x data rate.",
        "link": "https://dl.acm.org/doi/10.1145/3485730.3485948",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Kard: lightweight data race detection with per-thread memory protection",
        "authors": "['Adil Ahmad', 'Sangho Lee', 'Pedro Fonseca', 'Byoungyoung Lee']",
        "date": "April 2021",
        "source": "ASPLOS '21: Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "Finding data race bugs in multi-threaded programs has proven challenging. A promising direction is to use dynamic detectors that monitor the program’s execution for data races. However, despite extensive work on dynamic data race detection, most proposed systems for commodity hardware incur prohibitive overheads due to expensive compiler instrumentation of memory accesses; hence, they are not efficient enough to be used in all development and testing settings.   KARD is a lightweight system that dynamically detects data races caused by inconsistent lock usage—when a program concurrently accesses the same memory object using different locks or only some of the concurrent accesses are synchronized using a common lock. Unlike existing detectors, KARD does not monitor memory accesses using expensive compiler instrumentation. Instead, KARD leverages commodity per-thread memory protection, Intel Memory Protection Keys (MPK). Using MPK, KARD ensures that a shared object is only accessible to a single thread in its critical section, and captures all violating accesses from other concurrent threads. KARD overcomes various limitations of MPK by introducing key-enforced race detection, employing consolidated unique page allocation, carefully managing protection keys, and automatically pruning out non-racy or redundant violations. Our evaluation shows that KARD detects all data races caused by inconsistent lock usage and has a low geometric mean execution time overhead: 7.0% on PARSEC and SPLASH-2x benchmarks and 5.3% on a set of real-world applications (NGINX, memcached, pigz, and Aget).",
        "link": "https://dl.acm.org/doi/10.1145/3445814.3446727",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Realization of Multi Parameter Measuring Device Based on ATT7022E and STM32",
        "authors": "['Zhaoling Gao', 'Guang Yu']",
        "date": "October 2021",
        "source": "ICCPR '21: Proceedings of the 2021 10th International Conference on Computing and Pattern Recognition",
        "abstract": "This article introduces in detail a multi-parameter measurement device that can measure six-channel voltage, current and other information with the three-phase multi-function measurement chip ATT7022E, and can transmit the measured data to the high-performance ARM_Cortex_M4 microcontroller through the serial port of the STM32 single-chip microcomputer. This controller is equipped with an industrial Ethernet interface to connect to the Internet. In addition to the data acquisition part, it also realizes the remote configuration and precise control of the entire device. This article mainly introduces the content of the data acquisition part, including the acquisition of three-phase input and output Terminal voltage, current, active power, reactive power, apparent power, power factor and other parameters. Describes the design of this part of the hardware schematic diagram: including voltage sampling circuit, current sampling circuit, ATT7022E basic circuit, STM32F103RBT6 microcontroller basic circuit, serial communication interface and other circuit designs. It also describes the software calibration method, the realization of serial communication with the host computer ARM_Cortex_M4 microcontroller and the analysis of the measurement results. After experiments, the voltage is calibrated near the grid voltage, and the voltage value can reach the accuracy requirement within 0.5 level in the technical indicators of the power monitor.",
        "link": "https://dl.acm.org/doi/10.1145/3497623.3497662",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Reprogramming 3D TLC Flash Memory based Solid State Drives",
        "authors": "['Congming Gao', 'Min Ye', 'Chun Jason Xue', 'Youtao Zhang', 'Liang Shi', 'Jiwu Shu', 'Jun Yang']",
        "date": "None",
        "source": "ACM Transactions on Storage",
        "abstract": "NAND flash memory-based SSDs have been widely adopted. The scaling of SSD has evolved from plannar (2D) to 3D stacking. For reliability and other reasons, the technology node in 3D NAND SSD is larger than in 2D, but data density can be increased via increasing bit-per-cell. In this work, we develop a novel reprogramming scheme for TLCs in 3D NAND SSD, such that a cell can be programmed and reprogrammed several times before it is erased. Such reprogramming can improve the endurance of a cell and the speed of programming, and increase the amount of bits written in a cell per program/erase cycle, i.e., effective capacity. Our work is the first to perform a real 3D NAND SSD test to validate the feasibility of the reprogram operation. From the collected data, we derive the restrictions of performing reprogramming due to reliability challenges. Furthermore, a reprogrammable SSD (ReSSD) is designed to structure reprogram operations. ReSSD is evaluated in a case study in RAID 5 system (RSS-RAID). Experimental results show that RSS-RAID can improve the endurance by 35.7%, boost write performance by 15.9%, and increase effective capacity by 7.71%, with negligible overhead compared with conventional 3D SSD-based RAID 5 system.",
        "link": "https://dl.acm.org/doi/10.1145/3487064",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "User-defined cloud",
        "authors": "['Yiying Zhang', 'Ardalan Amiri Sani', 'Guoqing Harry Xu']",
        "date": "June 2021",
        "source": "HotOS '21: Proceedings of the Workshop on Hot Topics in Operating Systems",
        "abstract": "Since its creation, cloud computing has always taken a provider-dictated approach, where cloud providers define and manage the cloud to accommodate the user needs they deem important. We propose \"User-Defined Cloud\", or UDC, a new cloud scheme that allows users to define their own \"clouds\", by defining hardware resource needs, system software features, and security requirements of their applications, and to do so without the need to build or manage low-level systems.",
        "link": "https://dl.acm.org/doi/10.1145/3458336.3465304",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A cross-platform cache timing attack framework via deep learning",
        "authors": "['Ruyi Ding', 'Ziyue Zhang', 'Xiang Zhang', 'Cheng Gongye', 'Yunsi Fei', 'Aidong A. Ding']",
        "date": "March 2022",
        "source": "DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe",
        "abstract": "While deep learning methods have been adopted in power side-channel analysis, they have not been applied to cache timing attacks due to the limited dimension of cache timing data. This paper proposes a persistent cache monitor based on cache line flushing instructions, which runs concurrently to a victim execution and captures detailed memory access patterns in high-dimensional timing traces. We discover a new cache timing side-channel across both inclusive and non-inclusive caches, different from the traditional \"Flush+Flush\" timing leakage. We then propose a non-profiling differential deep learning analysis strategy to exploit the cache timing traces for key recovery. We further propose a framework for cross-platform cache timing attack via deep learning. Knowledge learned from profiling a common reference device can be transferred to build models to attack many other victim devices, even in different processor families. We take the OpenSSL AES-128 encryption algorithm as an example victim and deploy an asynchronous cache attack. We target three different devices from Intel, AMD, and ARM processors. We examine various scenarios for assigning the teacher role to one device and the student role to other devices, and evaluate the cross-platform deep-learning attack framework. Experimental results show that this new attack is easily extendable to victim devices and is more effective than attacks without any prior knowledge.",
        "link": "https://dl.acm.org/doi/10.5555/3539845.3540011",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Use of Speaker Recognition Approaches for Learning and Evaluating Embedding Representations of Musical Instrument Sounds",
        "authors": "['Xuan Shi', 'Erica Cooper', 'Junichi Yamagishi']",
        "date": "None",
        "source": "IEEE/ACM Transactions on Audio, Speech and Language Processing",
        "abstract": "Constructing an embedding space for musical instrument sounds that can meaningfully represent new and unseen instruments is important for downstream music generation tasks such as multi-instrument synthesis and timbre transfer. The framework of Automatic Speaker Verification (ASV) provides us with architectures and evaluation methodologies for verifying the identities of unseen speakers, and these can be repurposed for the task of learning and evaluating a musical instrument sound embedding space that can support unseen instruments. Borrowing from state-of-the-art ASV techniques, we construct a musical instrument recognition model that uses a SincNet front-end, a ResNet architecture, and an angular softmax objective function. Experiments on the NSynth and RWC datasets show our model&#x2019;s effectiveness in terms of equal error rate (EER) for unseen instruments, and ablation studies show the importance of data augmentation and the angular softmax objective. Experiments also show the benefit of using a CQT-based filterbank for initializing SincNet over a Mel filterbank initialization. Further complementary analysis of the learned embedding space is conducted with t-SNE visualizations and probing classification tasks, which show that including instrument family labels as a multi-task learning target can help to regularize the embedding space and incorporate useful structure, and that meaningful information such as playing style, which was not included during training, is contained in the embeddings of unseen instruments.",
        "link": "https://dl.acm.org/doi/10.1109/TASLP.2022.3140549",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "Towards energy-efficient CGRAs via stochastic computing",
        "authors": "['Bo Wang', 'Rong Zhu', 'Jiaxing Shang', 'Dajiang Liu']",
        "date": "March 2022",
        "source": "DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe",
        "abstract": "Stochastic computing (SC) is a promising computing paradigm for low-power and low-cost applications with the added benefit of high error tolerance. Meanwhile, Coarse-Grained Re-configurable Architecture (CGRA) is also a promising platform for domain-specific applications for its combination of energy efficiency and flexibility. Intuitively, introducing SC to CGRA would synergistically reinforce the strengths of both paradigms. Accordingly, this paper proposes an SC-based CGRA by replacing the exact multiplication in traditional CGRA with an SC-based multiplication, where the problem of accuracy and latency are both improved using parallel stochastic sequence generators and leading zero shifters. In addition, with the flexible connections among PEs, the high-accuracy operation can be easily achieved by combing neighbor PEs without switching costs like power-gating. Compared to the state-of-the-art approximate computing design of CGRA, our proposed CGRA has 16% more energy reduction and 34% energy efficiency improvement while keeping high configuration flexibility.",
        "link": "https://dl.acm.org/doi/10.5555/3539845.3539900",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "End-to-End Dereverberation, Beamforming, and Speech Recognition in a Cocktail Party",
        "authors": "['Wangyou Zhang', 'Xuankai Chang', 'Christoph Boeddeker', 'Tomohiro Nakatani', 'Shinji Watanabe', 'Yanmin Qian']",
        "date": "None",
        "source": "IEEE/ACM Transactions on Audio, Speech and Language Processing",
        "abstract": "Far-field multi-speaker automatic speech recognition (ASR) has drawn increasing attention in recent years. Most existing methods feature a signal processing frontend and an ASR backend. In realistic scenarios, these modules are usually trained separately or progressively, which suffers from either inter-module mismatch or a complicated training process. In this paper, we propose an end-to-end multi-channel model that jointly optimizes the speech enhancement (including speech dereverberation, denoising, and separation) frontend and the ASR backend as a single system. To the best of our knowledge, this is the first work that proposes to optimize dereverberation, beamforming, and multi-speaker ASR in a fully end-to-end manner. The frontend module consists of a weighted prediction error (WPE) based submodule for dereverberation and a neural beamformer for denoising and speech separation. For the backend, we adopt a widely used end-to-end (E2E) ASR architecture. It is worth noting that the entire model is differentiable and can be optimized in a fully end-to-end manner using only the ASR criterion, without the need of parallel signal-level labels. We evaluate the proposed model on several multi-speaker benchmark datasets, and experimental results show that the fully E2E ASR model can achieve competitive performance on both noisy and reverberant conditions, with over 30&#x0025; relative word error rate (WER) reduction over the single-channel baseline systems.",
        "link": "https://dl.acm.org/doi/10.1109/TASLP.2022.3209942",
        "category": "Hardware AND Architecture"
    },
    {
        "title": "A Method for Accelerating YOLO by Hybrid Computing Based on ARM and FPGA",
        "authors": "['Qilin Xiong', 'Chun Liao', 'Zhenhong Yang', 'Wanlin Gao']",
        "date": "December 2021",
        "source": "ACAI '21: Proceedings of the 2021 4th International Conference on Algorithms, Computing and Artificial Intelligence",
        "abstract": "CNN has promoted the rapid development of target recognition and detection technology. By comparison with machine learning, it has faster detection speed and higher robustness.  However, the deployment of the CNN network model often needs more computing resources, which hinders the application of artificial intelligence technology.  In this paper, the authors use the hybrid architecture of ARM and FPGA to deploy a You Only Look Once (YOLO) model on the FPGA to improve the efficiency of target recognition and detection under condition of low resources consumption and low power consumption. YOLO is a one-stage real-time detection model and it has high detection speed and remarkable accuracy. High-level Synthesis (HLS) is a fast development and verification technology of FPGA based on C/C++. We use HLS to implement the pipeline mechanism and complete the parallel calculation of convolution, thereby constructing a forward reasoning model of YOLOv3-tiny.  In order to accelerate the forward inference process of YOLO, we combine convolution with batch normalization.  The FPGA we use in the paper is Xilinx Zynq-7035 containing system on chip (SoC). We build the software and hardware co-architecture of ARM and FPGA on Zynq-7035, which makes full use of the logic control advantages of ARM and the logic computing advantages of FPGA.  In the end, we achieve 28.99 GOP/S speed with only 3.715W power consumption.  Finally, compared with the Ryzen 5 3600, we achieve 41.3inference speed at a lower clock rate.",
        "link": "https://dl.acm.org/doi/10.1145/3508546.3508576",
        "category": "Hardware AND Architecture"
    }
]