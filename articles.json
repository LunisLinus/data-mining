[
    {
        "title": "Text mining for incoming tasks based on the urgency/importance factors and task classification using machine learning tools",
        "authors": [
            "Yasser Ali Alshehri"
        ],
        "date": "March 2020",
        "source": "ICCDA 2020: Proceedings of the 2020 the 4th International Conference on Compute and Data Analysis",
        "abstract": "In workplaces, there is a massive amount of unstructured data from different sources. In this paper, we present a case study that explains how can through communications between employees, we can help to prioritize tasks requests to increase the efficiency of their works for both technical and non-technical workers. This involves managing daily incoming tasks based on their level of urgency and importance.To allow all workers to utilize the urgency-importance matrix as a time-management tool, we need to automate this tool. The textual content of incoming tasks are analyzed, and metrics related to urgency and importance are extracted. A third factor (i.e., the response variable) is defined based on the two input variables (urgency and importance). Then, machine learning applied to the data to predict the class of incoming tasks based on data outcome desired. We used ordinal regression, neural networks, and decision tree algorithms to predict the four levels of task priority. We measure the performance of all using recalls, precisions, and F-scores. All classifiers perform higher than 89% in terms of all measures.",
        "link": "https://dl.acm.org/doi/10.1145/3388142.3388153"
    },
    {
        "title": "Text mining for malware classification using multivariate all repeated patterns detection",
        "authors": [
            "Konstantinos F. Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2019",
        "source": "ASONAM '19: Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Mobile phones have become nowadays a commodity to the majority of people. Using them, people are able to access the world of Internet and connect with their friends, their colleagues at work or even unknown people with common interests. This proliferation of the mobile devices has also been seen as an opportunity for the cyber criminals to deceive smartphone users and steel their money directly or indirectly, respectively, by accessing their bank accounts through the smartphones or by blackmailing them or selling their private data such as photos, credit card data, etc. to third parties. This is usually achieved by installing malware to smartphones masking their malevolent payload as a legitimate application and advertise it to the users with the hope that mobile users will install it in their devices. Thus, any existing application can easily be modified by integrating a malware and then presented it as a legitimate one. In response to this, scientists have proposed a number of malware detection and classification methods using a variety of techniques. Even though, several of them achieve relatively high precision in malware classification, there is still space for improvement. In this paper, we propose a text mining all repeated pattern detection method which uses the decompiled files of an application in order to classify a suspicious application into one of the known malware families. Based on the experimental results using a real malware dataset, the methodology tries to correctly classify (without any misclassification) all randomly selected malware applications of 3 categories with 3 different families each.",
        "link": "https://dl.acm.org/doi/10.1145/3341161.3350841"
    },
    {
        "title": "Comparative Study between Traditional Machine Learning and Deep Learning Approaches for Text Classification",
        "authors": [
            "Cannannore Nidhi Kamath",
            "Syed Saqib Bukhari",
            "Andreas Dengel"
        ],
        "date": "August 2018",
        "source": "DocEng '18: Proceedings of the ACM Symposium on Document Engineering 2018",
        "abstract": "In this contemporaneous world, it is an obligation for any organization working with documents to end up with the insipid task of classifying truckload of documents, which is the nascent stage of venturing into the realm of information retrieval and data mining. But classification of such humongous documents into multiple classes, calls for a lot of time and labor. Hence a system which could classify these documents with acceptable accuracy would be of an unfathomable help in document engineering. We have created multiple classifiers for document classification and compared their accuracy on raw and processed data. We have garnered data used in a corporate organization as well as publicly available data for comparison. Data is processed by removing the stop-words and stemming is implemented to produce root words. Multiple traditional machine learning techniques like Naive Bayes, Logistic Regression, Support Vector Machine, Random forest Classifier and Multi-Layer Perceptron are used for classification of documents. Classifiers are applied on raw and processed data separately and their accuracy is noted. Along with this, Deep learning technique such as Convolution Neural Network is also used to classify the data and its accuracy is compared with that of traditional machine learning techniques. We are also exploring hierarchical classifiers for classification of classes and subclasses. The system classifies the data faster and with better accuracy than if done manually. The results are discussed in the results and evaluation section.",
        "link": "https://dl.acm.org/doi/10.1145/3209280.3209526"
    },
    {
        "title": "Identification of Overpricing in the Purchase of Medication by the Federal Government of Brazil, Using Text Mining and Clustering Based on Ontology",
        "authors": [
            "Marco Aurelio O. S. Correa",
            "Adriano Galindo Leal"
        ],
        "date": "August 2018",
        "source": "ICCBDC '18: Proceedings of the 2018 2nd International Conference on Cloud and Big Data Computing",
        "abstract": "Increasing the transparency level in his actions and spending is one of the primary duties of the Brazilian Federal Government. The creation of laws that oblige full disclose of all its expenditures through transparency portals enables citizens to supervise all government entities. However, only the dissemination of these data, without a definite standard or the availability of data analysis tools, does not guarantee that the citizen is empowered to play his role. Therefore hence, the objective of this work is to identify overprice in the acquisition of products purchased by the federal government of Brazil using the unstructured data available on the Transparency Portal. The last two-years' worth of purchasing data, available in the Transparency Portal, were extracted, processed and stored. Due to his diverse nature and high volume of data, this study focused only on medicines purchased by the Ministry of Health. Ontology-based text mining and clustering techniques were applied for automatic identification and classification of products. The processing of this information was done through text mining and clustering, based on the ontology registered in another database of the Brazilian government. Because of this work, a consolidated price base per medication was created to allow the identification of distortions in prices practised, facilitating the identification of cases that merit further investigation to unravel fraud to the treasury.",
        "link": "https://dl.acm.org/doi/10.1145/3264560.3264569"
    },
    {
        "title": "Computational Estimation by Scientific Data Mining with Classical Methods to Automate Learning Strategies of Scientists",
        "authors": [
            "Aparna S. Varde"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Experimental results are often plotted as 2-dimensional graphical plots (aka graphs) in scientific domains depicting dependent versus independent variables to aid visual analysis of processes. Repeatedly performing laboratory experiments consumes significant time and resources, motivating the need for computational estimation. The goals are to estimate the graph obtained in an experiment given its input conditions, and to estimate the conditions that would lead to a desired graph. Existing estimation approaches often do not meet accuracy and efficiency needs of targeted applications. We develop a computational estimation approach called AutoDomainMine that integrates clustering and classification over complex scientific data in a framework so as to automate classical learning methods of scientists. Knowledge discovered thereby from a database of existing experiments serves as the basis for estimation. Challenges include preserving domain semantics in clustering, finding matching strategies in classification, striking a good balance between elaboration and conciseness while displaying estimation results based on needs of targeted users, and deriving objective measures to capture subjective user interests. These and other challenges are addressed in this work. The AutoDomainMine approach is used to build a computational estimation system, rigorously evaluated with real data in Materials Science. Our evaluation confirms that AutoDomainMine provides desired accuracy and efficiency in computational estimation. It is extendable to other science and engineering domains as proved by adaptation of its sub-processes within fields such as Bioinformatics and Nanotechnology.",
        "link": "https://dl.acm.org/doi/10.1145/3502736"
    },
    {
        "title": "Where is the road for issue reports classification based on text mining?",
        "authors": [
            "Qiang Fan",
            "Yue Yu",
            "Gang Yin",
            "Tao Wang",
            "Huaimin Wang"
        ],
        "date": "November 2017",
        "source": "ESEM '17: Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Currently, open source projects receive various kinds of issues daily, because of the extreme openness of Issue Tracking System (ITS) in GitHub. ITS is a labor-intensive and time-consuming task of issue categorization for project managers. However, a contributor is only required a short textual abstract to report an issue in GitHub. Thus, most traditional classification approaches based on detailed and structured data (e.g., priority, severity, software version and so on) are difficult to adopt. In this paper, issue classification approaches on a large-scale dataset, including 80 popular projects and over 252,000 issue reports collected from GitHub, were investigated. First, four traditional text-based classification methods and their performances were discussed. Semantic perplexity (i.e., an issues description confuses bug-related sentences with nonbug-related sentences) is a crucial factor that affects the classification performances based on quantitative and qualitative study. Finally, A two-stage classifier framework based on the novel metrics of semantic perplexity of issue reports was designed. Results show that our two-stage classification can significantly improve issue classification performances.",
        "link": "https://dl.acm.org/doi/10.1109/ESEM.2017.19"
    },
    {
        "title": "Feature-based Facebook reviews process model for e-management using data mining",
        "authors": [
            "Anish Kumar Varudharajulu",
            "Yongsheng Ma"
        ],
        "date": "January 2019",
        "source": "IC4E '19: Proceedings of the 10th International Conference on E-Education, E-Business, E-Management and E-Learning",
        "abstract": "The data generated from online communication acts as potential gold mines for discovering knowledge for researchers. A large amount of data is also generated in the form of web documents, emails, blogs, and feedback, etc. Text analytics is being significantly employed to mine important information. Opinion mining is the process of extracting human thoughts and perceptions from unstructured texts. The showstopper for designing an opinion mining system for analyzing reviews arise from the fact that customer reviews are often noisy. These reviews are informally written. In addition, they are subjected to spelling mistakes, grammatical errors, improper punctuation and irrational capitalization. This paper focuses on analyzing the different classification and clustering algorithms aimed at extracting and consolidating opinions of customers from social media sites like Facebook, Twitter and through surveys, at multiple levels of granularity to monitor and measure customer satisfaction. Ours is an automated approach, in which the system aids in the process of knowledge assimilation for knowledge-based building and also performs the analytics. Domain experts ratify the knowledge base and also provide training datasets for the system to intuitively gather more instances for ratification. The system identifies opinion expressions as phrases containing opinion words, opinionated features and also opinion modifiers. These expressions are categorized as positive, negative or neutral. Opinion expressions are identified and categorized using localized linguistic techniques. Opinions can be congregated at any desired level of specificity i.e. feature level or product level, user level or service level, etc. We have developed a system based on this approach, which provides the user with a platform to analyze opinion expressions crawled from a set of pre-defined datasets.",
        "link": "https://dl.acm.org/doi/10.1145/3306500.3306514"
    },
    {
        "title": "Discriminative Topic Mining via Category-Name Guided Text Embedding",
        "authors": [
            "Yu Meng",
            "Jiaxin Huang",
            "Guangyuan Wang",
            "Zihan Wang",
            "Chao Zhang",
            "Yu Zhang",
            "Jiawei Han"
        ],
        "date": "April 2020",
        "source": "WWW '20: Proceedings of The Web Conference 2020",
        "abstract": "Mining a set of meaningful and distinctive topics automatically from massive text corpora has broad applications. Existing topic models, however, typically work in a purely unsupervised way, which often generate topics that do not fit users’ particular needs and yield suboptimal performance on downstream tasks. We propose a new task, discriminative topic mining, which leverages a set of user-provided category names to mine discriminative topics from text corpora. This new task not only helps a user understand clearly and distinctively the topics he/she is most interested in, but also benefits directly keyword-driven classification tasks. We develop CatE, a novel category-name guided text embedding method for discriminative topic mining, which effectively leverages minimal user guidance to learn a discriminative embedding space and discover category representative terms in an iterative manner. We conduct a comprehensive set of experiments to show that CatE mines high-quality set of topics guided by category names only, and benefits a variety of downstream applications including weakly-supervised classification and lexical entailment direction identification.",
        "link": "https://dl.acm.org/doi/10.1145/3366423.3380278"
    },
    {
        "title": "Comparative Study of Heart Disease Diagnosis Using Top Ten Data Mining Classification Algorithms",
        "authors": [
            "I. Ketut Agung Enriko"
        ],
        "date": "June 2019",
        "source": "ICFET '19: Proceedings of the 5th International Conference on Frontiers of Educational Technologies",
        "abstract": "Data mining has been used for many purposes, especially for prediction system. In healthcare, data mining algorithms often used in disease diagnosis. Meanwhile, heart disease is known as a primary cause of death over the years. Many studies have been performed in heart disease diagnosis using data mining methods. There are some popular data mining algorithms that can be used in heart disease diagnosis, for example, k-Nearest Neighbor, CART, and AdaBoost. The algorithms are used to analyze a sample of cardiovascular patients data and predict the heart disease type that they suffer. Some parameters are taken from the patient, including EKG morphology, blood pressure, and information about the existence of chest pain, shortness of breath, palpitation, and cold sweat. In this study, medical records data are collected from Harapan Kita Hospital and utilized as a dataset sample in this research. Top ten data mining classification algorithms are used in diagnosing heart disease from Harapan Kita Hospital data and examining their performance by checking the accuracy and speed.",
        "link": "https://dl.acm.org/doi/10.1145/3338188.3338220"
    },
    {
        "title": "Text Mining Approach for Identifying Research Trends",
        "authors": [
            "Snezhana Sulova"
        ],
        "date": "June 2021",
        "source": "CompSysTech '21: Proceedings of the 22nd International Conference on Computer Systems and Technologies",
        "abstract": "With the increase of unstructured data, the issues connected with automatic text processing, the categorization of documents and the discovery of topics have become objects of growing interest. In order to improve the process of grouping and processing research publications, we would like to propose a method based upon natural language processing. It is based on text mining technologies which aim to identify key tendencies in documents. It processes the content of publications by clustering and identifies the topics of each identified group. This analysis helps by identifying key tendencies as well as discovering emerging new areas of research. Publications from the research literature database, Scopus, were used to test the approach. The topic of the publications is “the application of digital technologies in the logistics business”. The experiments were completed using the RapidMiner Studio software.",
        "link": "https://dl.acm.org/doi/10.1145/3472410.3472433"
    },
    {
        "title": "Assessment of Vulnerability Severity using Text Mining",
        "authors": [
            "Georgios Spanos",
            "Lefteris Angelis",
            "Dimitrios Toloudis"
        ],
        "date": "September 2017",
        "source": "PCI '17: Proceedings of the 21st Pan-Hellenic Conference on Informatics",
        "abstract": "Software1 vulnerabilities are closely associated with information systems security, a major and critical field in today's technology. Vulnerabilities constitute a constant and increasing threat for various aspects of everyday life, especially for safety and economy, since the social impact from the problems that they cause is complicated and often unpredictable. Although there is an entire research branch in software engineering that deals with the identification and elimination of vulnerabilities, the growing complexity of software products and the variability of software production procedures are factors contributing to the ongoing occurrence of vulnerabilities, Hence, another area that is being developed in parallel focuses on the study and management of the vulnerabilities that have already been reported and registered in databases. The information contained in such databases includes, a textual description and a number of metrics related to vulnerabilities. The purpose of this paper is to investigate to what extend the assessment of the vulnerability severity can be inferred directly from the corresponding textual description, or in other words, to examine the informative power of the description with respect to the vulnerability severity. For this purpose, text mining techniques, i.e. text analysis and three different classification methods (decision trees, neural networks and support vector machines) were employed. The application of text mining to a sample of 70,678 vulnerabilities from a public data source shows that the description itself is a reliable and highly accurate source of information for vulnerability prioritization.",
        "link": "https://dl.acm.org/doi/10.1145/3139367.3139390"
    },
    {
        "title": "Clustering Algorithms for Spatial Data Mining",
        "authors": [
            "Chetashri Bhadane",
            "Ketan Shah"
        ],
        "date": "April 2020",
        "source": "ICGDA '20: Proceedings of the 2020 3rd International Conference on Geoinformatics and Data Analysis",
        "abstract": "With the advances in mobile and wireless technologies, there has been a rise in applications that track and share the users' geospatial data. People use several social networking sites such as Twitter, Facebook and Flickr, where they share their status updates. With the integration of Global Positioning System (GPS) with mobile phones, it is now possible to share one's locations on these social networks. GPS allows us to record and track a person's movement along with the timestamp. The data set obtained from these GPS logs is vast and is widely used to analyze the users' movement patterns. Specifically, we can find out significant locations based on the number of users present at that location and the time spent by them at such places. Once significant places have been identified, it is also possible to identify the semantic importance of these locations. This paper presents an overview of the clustering techniques used to find important places of interest using large GPS based mobility datasets. Four clustering algorithms, K-Means, DBSCAN, OPTICS and Hierarchical, are implemented, and performance is tested using real-time data of 50 users collected over 2--5 years. Performance summary depicts that K-Means and DBSCAN perform well for spatial data.",
        "link": "https://dl.acm.org/doi/10.1145/3397056.3397068"
    },
    {
        "title": "Feature Selection based Arabic Text Classification using Different Machine Learning Algorithms: Comparative Study",
        "authors": [
            "Sakina Rim Bennabi",
            "Zakaria Elberrichi"
        ],
        "date": "June 2020",
        "source": "ICIST '20: Proceedings of the 10th International Conference on Information Systems and Technologies",
        "abstract": "Feature selection is a method of data pre-processing widely used when mining large data, such as textual classification. Several studies have been conducted to compare the different methods of feature selection applied to corpora in English. Unfortunately, a small number of works concern the Arabic language. This article aims to present a comparative study of different feature selection techniques including: Chi2, the ANOVA method and mutual information, applied on a corpus in Arabic language, while also diversifying the machine learning algorithms (Naive Bayes, SVM and KNN). This experimental study has shown in general that reducing dimensionality with feature selection techniques has slightly affected the performance of textual classification, reducing the size of the corpus by up to 1%.",
        "link": "https://dl.acm.org/doi/10.1145/3447568.3448531"
    },
    {
        "title": "Intelligent Data Analysis using Optimized Support Vector Machine Based Data Mining Approach for Tourism Industry",
        "authors": [
            "Ms Promila Sharma",
            "Uma Meena",
            "Girish Kumar Sharma"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Data analysis involves the deployment of sophisticated approaches from data mining methods, information theory, and artificial intelligence in various fields like tourism, hospitality, and so on for the extraction of knowledge from the gathered and preprocessed data. In tourism, pattern analysis or data analysis using classification is significant for finding the patterns that represent new and potentially useful information or knowledge about the destination and other data. Several data mining techniques are introduced for the classification of data or patterns. However, overfitting, less accuracy, local minima, sensitive to noise are the drawbacks in some existing data mining classification methods. To overcome these challenges, Support vector machine with Red deer optimization (SVM-RDO) based data mining strategy is proposed in this article. Extended Kalman filter (EKF) is utilized in the first phase, i.e., data cleaning to remove the noise and missing values from the input data. Mantaray foraging algorithm (MaFA) is used in the data selection phase, in which the significant data are selected for the further process to reduce the computational complexity. The final phase is the classification, in which SVM-RDO is proposed to access the useful pattern from the selected data. PYTHON is the implementation tool used for the experiment of the proposed model. The experimental analysis is done to show the efficacy of the proposed work. From the experimental results, the proposed SVM-RDO achieved better accuracy, precision, recall, and F1 score than the existing methods for the tourism dataset. Thus, it is showed the effectiveness of the proposed SVM-RDO for pattern analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3494566"
    },
    {
        "title": "Applying text mining to predict learners' cognitive engagement",
        "authors": [
            "Hayati Hind",
            "Mohammed Khalidi Idrissi",
            "Samir Bennani"
        ],
        "date": "October 2017",
        "source": "SCAMS '17: Proceedings of the Mediterranean Symposium on Smart City Application",
        "abstract": "In academic process, engagement represent one of the important success' factor. This paper proposes a predictive system for learners' cognitive engagement based on their online discussion forums participation. In the first level the ontology OWL and the LSA method are used to perform a semantic classification system of the threads according to a specific context chosen by the tutor. Then the Text mining, as a predictive method, is applied to the classified threads and learners' participation in the forums.",
        "link": "https://dl.acm.org/doi/10.1145/3175628.3175655"
    },
    {
        "title": "Investigate Transitions into Drug Addiction through Text Mining of Reddit Data",
        "authors": [
            "John Lu",
            "Sumati Sridhar",
            "Ritika Pandey",
            "Mohammad Al Hasan",
            "Georege Mohler"
        ],
        "date": "July 2019",
        "source": "KDD '19: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Increasing rates of opioid drug abuse and heightened prevalence of online support communities underscore the necessity of employing data mining techniques to better understand drug addiction using these rapidly developing online resources. In this work, we obtained data from Reddit, an online collection of forums, to gather insight into drug use/misuse using text snippets from users narratives. Specifically, using users' posts, we trained a binary classifier which predicts a user's transitions from casual drug discussion forums to drug recovery forums. We also proposed a Cox regression model that outputs likelihoods of such transitions. In doing so, we found that utterances of select drugs and certain linguistic features contained in one's posts can help predict these transitions. Using unfiltered drug-related posts, our research delineates drugs that are associated with higher rates of transitions from recreational drug discussion to support/recovery discussion, offers insight into modern drug culture, and provides tools with potential applications in combating the opioid crisis.",
        "link": "https://dl.acm.org/doi/10.1145/3292500.3330737"
    },
    {
        "title": "Analysing digital banking reviews using text mining",
        "authors": [
            "Li Chen Cheng",
            "Legaspi Rhea Sharmayne"
        ],
        "date": "December 2020",
        "source": "ASONAM '20: Proceedings of the 12th IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Digital banks are new entrants in the banking industry in the Philippines as they only started late 2018. Since then, a handful of players have and are still emerging. With more and more people becoming technologically savvy, it is very critical for financial institutions to develop a digital banking application that will stand out from the competition. This paper aims to use text mining methods to analyse digital banking application reviews. This study will perform topic modelling using LDA to explore customer concerns and will mine association rules between the digital banking features with the review score. The results will reveal which areas the digital banking application can further optimize for customer satisfaction and retention.",
        "link": "https://dl.acm.org/doi/10.1109/ASONAM49781.2020.9381429"
    },
    {
        "title": "Mining Moocs Videos Metadata Using Classification Techniques",
        "authors": [
            "El Harrak Othman",
            "Ghadi Abderrahim",
            "El Bouhdidi Jaber"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "In few years, the internet has become the first source of information for most people, Today MOOCs makes it possible for everyone to access to the education over the world, it's represents an emerging methodology of online teaching and an important development in open education. Due to the rapid development of Moocs and the rapid growth of digital data and video database over the Internet, it is becoming very difficult for learners to browsing and choosing the best training for them. The scientific community has increased the amount of research into new technologies, with a view to improve the digital video utilization: its archiving, indexing, accessibility, acquisition, store and even its process and usability. All these parts of the video utilization entail the necessity of the extraction of all important information of a video, especially in cases of lack of metadata information. Web video mining is retrieving the content using data mining techniques from World Wide Web. There are two approaches for web video mining using traditional image processing (signal processing) and metadata based approach. In this paper, we present the various research makes in this subject and we propose an effective methodology to extract the metadata from Moocs videos and classify them based on the extracted metadata by applying data mining techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090450"
    },
    {
        "title": "Text mining with HathiTrust",
        "authors": [
            "Eleanor Dickson Koehl",
            "Ryan Dubnicek"
        ],
        "date": "June 2019",
        "source": "JCDL '19: Proceedings of the 18th Joint Conference on Digital Libraries",
        "abstract": "This tutorial will introduce attendees to the HathiTrust Research Center and its tools and services for computational text analysis research. HTRC leverages the scope and scale of the HathiTrust Digital Library collection to create opportunities for researchers to perform text data mining on subsets of the corpus. Attendees will develop skills that will allow them to conduct basic text analysis research using HathiTrust data, learn techniques for engaging with scholars on their text data mining research, and explore strategies for computational access to large-scale digital collections.",
        "link": "https://dl.acm.org/doi/10.1109/JCDL.2019.00115"
    },
    {
        "title": "Text mining for plagiarism detection: multivariate pattern detection for recognition of text similarities",
        "authors": [
            "Konstantinos Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2018",
        "source": "ASONAM '18: Proceedings of the 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "The problem of plagiarism the recent years has been intensified by the availability of information in digital form and the accessibility of the electronic libraries through the Internet. As a result, plagiarism detection has been transformed into a big data analytics problem since the number of digital sources is extravagant and a new document needs to be compared with millions of other existing documents. In this paper, a text mining methodology is proposed that can detect all common patterns between a document and the documents in a reference database. The technique is based on a pattern detection algorithm and the corresponding data structure that enables the algorithm to detect all common patterns. The methodology has been applied in a well-defined dataset providing very promising results identifying difficult cases of plagiarism such as technical disguise.",
        "link": "https://dl.acm.org/doi/10.5555/3382225.3382424"
    },
    {
        "title": "Petroleum Engineering Data Text Classification Using Convolutional Neural Network Based Classifier",
        "authors": [
            "Tzuhan Hsu",
            "Yaoqin Zhang"
        ],
        "date": "May 2018",
        "source": "ICMLT '18: Proceedings of the 2018 International Conference on Machine Learning Technologies",
        "abstract": "In this paper, authors introduce a convolutional neural network (CNN) based classification approach for petroleum engineering event. Recently, text classification problem has raised substantial attention in the field of machine learning. The aim of text classification process is to assign correct label to a text on the basis of its content. However, due to the imprecise of feature representation, most of former text classification methods cannot export satisfying result. To solve this problem, authors propose a model based on convolutional neural network. Authors conduct experiments with real world petroleum engineering data text, artificially labelled into 6 categories. Beside purposed CNN-based model, the authors also implement other baseline machine learning approaches: k nearest neighbor, random forest, support vector machines, recurrent neural network, gated recurrent units, long short-term memory. Among all these models, our CNN-based model reach best performance in text classification task. In the last part of this paper, authors compare and analysis their purpose and baseline model.",
        "link": "https://dl.acm.org/doi/10.1145/3231884.3231898"
    },
    {
        "title": "Classifying Indonesian Online Articles as Advertisement Placement Base Using Text Mining",
        "authors": [
            "Nadhira Tasya",
            "Arian Dhini"
        ],
        "date": "July 2017",
        "source": "ICBIM 2017: Proceedings of the International Conference on Business and Information Management",
        "abstract": "Rapid development in technological aspect resulting in growing level of human needs for the latest news, so that emerged a new trend of publishing and accessing news through online media or usually called online journalism. In addition, the number of people who sell and purchase through online sites also continues to increase and this opportunity is utilized by the company and the advertiser by implementing targeted web advertising. However, the high number of articles that have been published and accessed leads to great opportunities for errors in determining where to place the ads. Therefore, it needs a system that can categorize articles accessed by users as the basis of advertisement placement by the company and this classification system can be done by applying the method of Data Mining and Text Mining. This research uses document data in the form of article content that will be categorized into twenty categories of class of advertisement by using Text Mining technique with Support Vector Machine algorithm. The results of this study may be used by companies or advertisers as a basis for placement of ads on selected online media sites.",
        "link": "https://dl.acm.org/doi/10.1145/3134271.3134288"
    },
    {
        "title": "Mining Free-Text Medical Notes for Suicide Risk Assessment",
        "authors": [
            "Marios Adamou",
            "Grigoris Antoniou",
            "Elissavet Greasidou",
            "Vincenzo Lagani",
            "Paulos Charonyktakis",
            "Ioannis Tsamardinos"
        ],
        "date": "July 2018",
        "source": "SETN '18: Proceedings of the 10th Hellenic Conference on Artificial Intelligence",
        "abstract": "Suicide has been considered as an important public health issue for a very long time, and is one of the main causes of death worldwide. Despite suicide prevention strategies being applied, the rate of suicide has not changed substantially over the past decades. Advances in machine learning make it possible to attempt to predict suicide based on the analysis of relevant data to inform clinical practice. This paper reports on findings from the analysis of data of patients who died by suicide in the period 2013-2016 and made use of both structured data and free-text medical notes. We focus on examining various text-mining approaches to support risk assessment. The results show that using advance machine learning and text-mining techniques, it is possible to predict within a specified period which people are most at risk of taking their own life at the time of referral to a mental health service.",
        "link": "https://dl.acm.org/doi/10.1145/3200947.3201020"
    },
    {
        "title": "Integration of data mining techniques in e-learning systems: Clustering Profil of Lerners and Recommender Course System",
        "authors": [
            "A. El Moustamid",
            "E. En-Naimi",
            "J. El Bouhdidi"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "Data mining aims at the extraction of knowledge from large quantities of data by automatic or semi-automatic methods. In the field of education and training e-learning, Data Mining techniques as clustering (The aim of this technique is to identify groups that are similar in some aspect) [21], prediction (is used for developing model where combination of data viewed from different aspects can effect single viewed data. Here, is important to watch how predictors variables are effecting predicted variable)[21], classification are applied in e-learning to improve the level of learners by providing learning according to the learners' requirement) are used to analyze learners profile, to predict and improve student performance. The aim of our project is to develop a system capable of analyzing learners profile and indexing web videos in order to offer learners a rich database with courses that correspond to their levels.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090453"
    },
    {
        "title": "Multi-level mining and visualization of scientific text collections: Exploring a bi-lingual scientific repository",
        "authors": [
            "Pablo Accuosto",
            "Francesco Ronzano",
            "Daniel Ferrés",
            "Horacio Saggion"
        ],
        "date": "December 2017",
        "source": "WOSP 2017: Proceedings of the 6th International Workshop on Mining Scientific Publications",
        "abstract": "We present a system to mine and visualize collections of scientific documents by semantically browsing information extracted from single publications or aggregated throughout corpora of articles. The text mining tool performs deep analysis of document collections allowing the extraction and interpretation of research paper's contents. In addition to the extraction and enrichment of documents with metadata (titles, authors, affiliations, etc), the deep analysis performed comprises semantic interpretation, rhetorical analysis of sentences, triple-based information extraction, and text summarization. The visualization components allow geographical-based exploration of collections, topic-evolution interpretation, and collaborative network analysis among others. The paper presents a case study of a bi-lingual collection in the field of Natural Language Processing (NLP).",
        "link": "https://dl.acm.org/doi/10.1145/3127526.3127529"
    },
    {
        "title": "Hierarchical Topic Mining via Joint Spherical Tree and Text Embedding",
        "authors": [
            "Yu Meng",
            "Yunyi Zhang",
            "Jiaxin Huang",
            "Yu Zhang",
            "Chao Zhang",
            "Jiawei Han"
        ],
        "date": "August 2020",
        "source": "KDD '20: Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Mining a set of meaningful topics organized into a hierarchy is intuitively appealing since topic correlations are ubiquitous in massive text corpora. To account for potential hierarchical topic structures, hierarchical topic models generalize flat topic models by incorporating latent topic hierarchies into their generative modeling process. However, due to their purely unsupervised nature, the learned topic hierarchy often deviates from users' particular needs or interests. To guide the hierarchical topic discovery process with minimal user supervision, we propose a new task, Hierarchical Topic Mining, which takes a category tree described by category names only, and aims to mine a set of representative terms for each category from a text corpus to help a user comprehend his/her interested topics. We develop a novel joint tree and text embedding method along with a principled optimization procedure that allows simultaneous modeling of the category tree structure and the corpus generative process in the spherical space for effective category-representative term discovery. Our comprehensive experiments show that our model, named JoSH, mines a high-quality set of hierarchical topics with high efficiency and benefits weakly-supervised hierarchical text classification tasks.",
        "link": "https://dl.acm.org/doi/10.1145/3394486.3403242"
    },
    {
        "title": "Unsupervised behavioural mining and clustering for malware family identification",
        "authors": [
            "Khanh Huu The Dam",
            "Thomas Given-Wilson",
            "Axel Legay"
        ],
        "date": "March 2021",
        "source": "SAC '21: Proceedings of the 36th Annual ACM Symposium on Applied Computing",
        "abstract": "More accurate and advanced detection and classification of malware requires exploiting program behaviour and not merely syntactic or static features. One approach is to use system call dependency graphs (SCDGs) that represent the program behaviour by interactions with the system, and the relations between these interactions. These SCDGs have been used with supervised learning techniques to very accurately detect and classify malware. This works considers the unsupervised learning challenge of mining for common clusters of behaviour without a priori knowledge. This allows for clustering of similar programs by behaviour, that can then be used for either classification or further analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3412841.3441919"
    },
    {
        "title": "Text Mining for Evaluating Authors' Birth and Death Years",
        "authors": [
            "Dror Moghaz",
            "Yaakov Hacohen-Kerner",
            "Dov Gabbay"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "This article presents a unique method in text and data mining for finding the era, i.e., mining temporal data, in which an anonymous author was living. Finding this era can assist in the examination of a fake document or extracting the time period in which a writer lived. The study and the experiments concern Hebrew, and in some parts, Aramaic and Yiddish rabbinic texts. The rabbinic texts are undated and contain no bibliographic sections, posing an interesting challenge. This work proposes algorithms using key phrases and key words that allow the temporal organization of citations together with linguistic patterns. Based on these key phrases, key words, and the references, we established several types of “Iron-clad,” Heuristic and Greedy rules for estimating the years of birth and death of a writer in an interesting classification task. Experiments were conducted on corpora, including documents authored by 12, 24, and 36 rabbinic writers and demonstrated promising results.",
        "link": "https://dl.acm.org/doi/10.1145/3281631"
    },
    {
        "title": "Transfer Learning to Timed Text Based Video Classification Using CNN",
        "authors": [
            "Zenun Kastrati",
            "Ali Shariq Imran",
            "Arianit Kurti"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "Open educational video resources are gaining popularity with a growing number of massive open online courses (MOOCs). This has created a niche for content providers to adopt effective solutions in automatically organizing and structuring of educational resources for maximum visibility. Recent advances in deep learning techniques are proving useful in managing and classifying resources into appropriate categories. This paper proposes one such convolutional neural network (CNN) model for classifying video lectures in a MOOC setting using a transfer learning approach. The model uses a time-aligned text transcripts corresponding to video lectures from six broader subject categories. Video lectures and their corresponding transcript dataset is gathered from the Coursera MOOC platform. Two different CNN models are proposed: i) CNN based classification using embeddings learned from our MOOC dataset, ii) CNN based classification using transfer learning. Word embeddings generated from two well known state-of-the-art pre-trained models Word2Vec and GloVe, are used in the transfer learning approach for the second case. The proposed CNN models are evaluated using precision, recall, and F1 score and the obtained performance is compared with both conventional and deep learning classifiers. The proposed CNN models have an F1 score improvement of 10-22 percentage points over DNN and conventional classifiers",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326483"
    },
    {
        "title": "Evolutionary role mining in complex networks by ensemble clustering",
        "authors": [
            "Sarvenaz Choobdar",
            "Pedro Ribeiro",
            "Fernando Silva"
        ],
        "date": "April 2017",
        "source": "SAC '17: Proceedings of the Symposium on Applied Computing",
        "abstract": "The structural patterns in the neighborhood of nodes assign unique roles to the nodes. Mining the set of existing roles in a network provides a descriptive profile of the network and draws its general picture. This paper proposes a new method to determine structural roles in a dynamic network based on the current position of nodes and their historic behavior. We develop a temporal ensemble clustering technique to dynamically find groups of nodes, holding similar tempo-structural roles. We compare two weighting functions, based on age and distribution of data, to incorporate temporal behavior of nodes in the role discovery. To evaluate the performance of the proposed method, we assess the results from two points of view: 1) goodness of fit to current structure of the network; 2) consistency with historic data. We conduct the evaluation using different ensemble clustering techniques. The results on real world networks demonstrate that our method can detect tempo-structural roles that simultaneously depict the topology of a network and reflect its dynamics with high accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3019612.3019815"
    },
    {
        "title": "Analyzing used-car web listings via text mining",
        "authors": [
            "Ayhan Demiriz",
            "Fatma Cantaş"
        ],
        "date": "October 2017",
        "source": "IML '17: Proceedings of the 1st International Conference on Internet of Things and Machine Learning",
        "abstract": "Used car trade is one of the major components of the world economies. It is not uncommon to sell a car by placing an internet advertisement irrespective of the geography in these days. A typical content of an advertisement is usually composed of two parts namely the structured and the free text data. The structured data may include some information about the asking price, make, model, year, mileage of the car and the contact info. In most cases, seller may give important clues about the car's current conditions in the free text data where the title (head) of the advertisement can be included as free text too. This paper reports preliminary results from a text mining study conducted on 75K used car internet listings collected from two major car listing web sites in Turkey. As expected, the words and the phrases related to the description of the car are observed to be frequent. The leading concepts in the free text are found to be regarding how to describe the current condition of a car, for example \"no crash history\".",
        "link": "https://dl.acm.org/doi/10.1145/3109761.3109782"
    },
    {
        "title": "Text classification of Diseases Treated by Traditional Chinese Medicine Prescription based on machine learning",
        "authors": [
            "Hanqing Zhao",
            "Bo Han",
            "Chen Li"
        ],
        "date": "September 2020",
        "source": "ISAIMS '20: Proceedings of the 1st International Symposium on Artificial Intelligence in Medical Sciences",
        "abstract": "OBJECTIVE: To explore the application of machine learning in th e identification of diseases treated by traditional Chinese medicine prescriptions. METHODS: Based on the composition of the text document of Chinese medicine prescriptions, the prescriptions were divided into cough, headache and diarrhea. THUTCT were introduced to establish and train two machine learning text classification models, LibSVM and LibLinear, and the prescriptions to be deter mined were put into the model for classification and prediction. R ESULTS: The Precision rate of LibSVM model and Liblinear mod el were 0.7283 and 0.6690. Seven prescriptions were classified an d predicted, and the results were in line with expectations. CONCLUSION: THUCTC has good universality for the content of TCM prescriptions, high classification accuracy and fast testing speed, which is suitable for the text classification and discrimination research of TCM prescriptions for diseases.",
        "link": "https://dl.acm.org/doi/10.1145/3429889.3429895"
    },
    {
        "title": "Evaluation of retweet clustering method classification method using retweets on Twitter without text data",
        "authors": [
            "K. Uchida",
            "F. Toriumi",
            "T. Sakaki"
        ],
        "date": "August 2017",
        "source": "WI '17: Proceedings of the International Conference on Web Intelligence",
        "abstract": "Burst phenomena, which frequently occur on social media, are caused by such social events as flaming on the internet, elections, and natural disasters. To understand people's thoughts and feelings, we must classify their opinions from burst phenomena. Therefore, classification methods that categorize tweets are critical. However, since most classification methods focus on text mining, they cannot group tweets by topics because each tweet has poor linguistic similarities. We used a non-text-based classification method proposed by Baba et al. that groups tweets by topics, even if they have poor linguistic similarities, and verified its validity by comparing it with a text-based classification method in two different evaluations: qualitative and quantitative. In the qualitative evaluation part, we did a questionnaire survey and validated the suitability of the topic clusters created using both the non-and text-based methods. Since evaluating the similarity of every pair of tweets in each topic is difficult, we evaluated the similarity between sampled pairs in the survey and acquired more appropriate topic clustering results using the non-text-based method than the text-based method. In the quantitative evaluation part, we focused on the robustness of each method against data reduction. Many approaches analyze social media data, especially because collecting data from social media is comparatively easy. However, since collecting the whole data of burst phenomena is very costly due to the vast amounts of available social media data, robustness against data reduction is an important index to evaluate classification methods. With the non-text-based method, over 55% of the pairs of tweets in the same cluster were also included in the same cluster even when the data were reduced to 10% in all three of our example cases. In this paper, as a source we focus on Twitter, one of the most popular microblogging services. Using clustering to conduct detailed case analyses, we scrutinized three burst cases that include natural disasters and flaming on the internet and found that a non-text-based method more effectively classified tweets in burst phenomena than a text-based method.",
        "link": "https://dl.acm.org/doi/10.1145/3106426.3106451"
    },
    {
        "title": "Using Spark for Text Mining on Large Scale Liver Cancer Literature",
        "authors": [
            "Ming-Yen Lin",
            "Yu-Ju Lin",
            "Sue-Chen Hsueh"
        ],
        "date": "January 2021",
        "source": "BDET 2021: 2021 the 3rd International Conference on Big Data Engineering and Technology (BDET)",
        "abstract": "Cancer is one of the main causes of death. The number of known cancer to-date is more than one hundred. Liver cancer, ranked after trachea and lung cancer, has long been high in the cancer leading cause in Taiwan and even ranked as second in 2016. The number of scientific articles related to cancer proliferates every year. The number reaches as high as 20 million in PubMed so that discovering useful information from the massive collection is very difficult. In addition, using a single machine to sift through these articles is very time-consuming. Therefore, we present a big data analytic framework using the distributed Apache Spark platform for text mining in PubMed literature. We establish a prediction model for liver cancer articles so that researchers may effectively validate whether an article is related to liver cancer or not. Classification models in Spark MLlib including Linear Support Vector Machines (SVM), Logistic Regression, are used in our experiments. Relevancy to liver cancer is further confirmed by using MeSH (Medical Subject Headings) terms. Logistic regression is about 3 times faster than SVMs and the accuracy of both methods is close to 95% in the experiments using hold-out validation. When max_features is 500 and min_df ≤ 0.1 (or min_df = 1), the accuracy may reach 96%. In the experiments with K-fold cross-validation, the accuracy of SVMs methods is 96%. The experimental results show that that our prediction model may effectively classify liver cancer articles.",
        "link": "https://dl.acm.org/doi/10.1145/3474944.3474958"
    },
    {
        "title": "Automatic topic classification of test cases using text mining at an Android smartphone vendor",
        "authors": [
            "Junji Shimagaki",
            "Yasutaka Kamei",
            "Naoyasu Ubayashi",
            "Abram Hindle"
        ],
        "date": "October 2018",
        "source": "ESEM '18: Proceedings of the 12th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Background: An Android smartphone is an ecosystem of applications, drivers, operating system components, and assets. The volume of the software is large and the number of test cases needed to cover the functionality of an Android system is substantial. Enormous effort has been already taken to properly quantify \"what features and apps were tested and verified?\". This insight is provided by dashboards that summarize test coverage and results per feature. One method to achieve this is to manually tag or label test cases with the topic or function they cover, much like function points. At the studied Android smartphone vendor, tests are labelled with manually defined tags, so-called \"feature labels (FLs)\", and the FLs serve to categorize 100s to 1000s test cases into 10 to 50 groups. Aim: Unfortunately for developers, manual assignment of FLs to 1000s of test cases is a time consuming task, leading to inaccurately labeled test cases, which will render the dashboard useless. We created an automated system that suggests tags/labels to the developers for their test cases rather than manual labeling. Method: We use machine learning models to predict and label the functionality tested by 10,000 test cases developed at the company. Results: Through the quantitative experiments, our models achieved acceptable F-1 performance of 0.3 to 0.88. Also through the qualitative studies with expert teams, we showed that the hierarchy and path of tests was a good predictor of a feature's label. Conclusions: We find that this method can reduce tedious manual effort that software developers spent classifying test cases, while providing more accurate classification results.",
        "link": "https://dl.acm.org/doi/10.1145/3239235.3268927"
    },
    {
        "title": "To apply Data Mining for Classification of Crowd sourced Software Requirements",
        "authors": [
            "Soonh Taj",
            "Qasim Arain",
            "Imran Memon",
            "Asma Zubedi"
        ],
        "date": "April 2019",
        "source": "ICSIE '19: Proceedings of the 8th International Conference on Software and Information Engineering",
        "abstract": "Now a day's main focus of developers is to build quality software that works according to customer needs and for this reason it is necessary to gather right requirements as requirement elicitation is the critical step that impacts on the success of software project as misinterpreted requirements leads to the failure of software project. By keeping this in mind a research is carried out on improving requirements elicitation process and automating the process of classifying requirements. In this research, a model is proposed which will help in this scenario for requirements elicitation and requirement classification. This paper presents a model in which crowd sourcing approach is used so that customers, end users, stakeholders, developers and software engineers can make active participation for requirement elicitation process and requirements gathered using crowdsourcing approach are used by model for classification process i.e. classification of requirements into functional and non-functional requirements. For the proof of proposed model a case study is conducted. Results of case study provided the usefulness and efficiency of proposed model for classification of crowd sourced software requirements.",
        "link": "https://dl.acm.org/doi/10.1145/3328833.3328837"
    },
    {
        "title": "Text classification using Fuzzy TF-IDF and Machine Learning Models",
        "authors": [
            "Mariem Bounabi",
            "Karim El Moutaouakil",
            "Khalid Satori"
        ],
        "date": "October 2019",
        "source": "BDIoT'19: Proceedings of the 4th International Conference on Big Data and Internet of Things",
        "abstract": "The representation of the information has an important impact on the text classification task. Several weighting methods were proposed in the literature, and the term frequency-inverse term frequency (TFIDF), the most know on the text treatment field. The FTF-IDF is a vector representation where the components of the TFIDF are presented as inputs to the Fuzzy Inference System (FIS). In this work, we compare several Machin Learning algorithms such as Naïve Bayes and its derivatives, SVM and Random forest classifiers, using the FTF-IDF representation. To improve the quality of the used classifiers, we call sum attribute selection methods. The recognition rate, for the tested systems, is satisfied, where the system based on naïve Bayes classifier, the FTF-IDF weighting terms, and the info gain select attributes method gives 98.7% as accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3372938.3372956"
    },
    {
        "title": "Text Mining in Cybersecurity: A Systematic Literature Review",
        "authors": [
            "Luciano Ignaczak",
            "Guilherme Goldschmidt",
            "Cristiano André Da Costa",
            "Rodrigo Da Rosa Righi"
        ],
        "date": null,
        "source": "ACM Computing Surveys",
        "abstract": "The growth of data volume has changed cybersecurity activities, demanding a higher level of automation. In this new cybersecurity landscape, text mining emerged as an alternative to improve the efficiency of the activities involving unstructured data. This article proposes a Systematic Literature Review (SLR) to present the application of text mining in the cybersecurity domain. Using a systematic protocol, we identified 2,196 studies, out of which 83 were summarized. As a contribution, we propose a taxonomy to demonstrate the different activities in the cybersecurity domain supported by text mining. We also detail the strategies evaluated in the application of text mining tasks and the use of neural networks to support activities involving unstructured data. The work also discusses text classification performance aiming its application in real-world solutions. The SLR also highlights open gaps for future research, such as the analysis of non-English content and the intensification in the usage of neural networks.",
        "link": "https://dl.acm.org/doi/10.1145/3462477"
    },
    {
        "title": "Using Text Mining to Analyze the Financial Patents",
        "authors": [
            "Yung-Feng Lu",
            "Jia-Lang Xu",
            "Mu-Yen Chen"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "With the advancement and development of science and technology, Nowadays society is the development of information technology, cloud computing, mobile Internet, Internet of Things, block chain and information technology.The combination of finance and technology has made financial technology terms gradually emerging.. This research analyzes the patents of financial technology. However, Taiwan's financial technology patents are mostly G06Q-20 (payment plan, architecture or agreement), G06Q-30 (commercial, such as marketing, shopping, payment, auction or e-commerce). G06Q-40 (financial, investment or tax treatment, insurance).The research use text mining technology to analyze and understand Taiwan's trends and developments in financial technology. The research results show that G06Q-20 patents are biased towards information security and mobile payment. G06Q-30 patents favor shopping websites and advertising. G06Q-40 patents are biased towards diversity, such as securities, insurance, finance, and stocks.",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326478"
    },
    {
        "title": "Identifying Online Profiles of Distance Learning Students Using Data Mining Techniques",
        "authors": [
            "Osama Islam",
            "Muazzam Siddiqui",
            "Naif Radi Aljohani"
        ],
        "date": "October 2019",
        "source": "ICDTE '19: Proceedings of the 3rd International Conference on Digital Technology in Education",
        "abstract": "Educational data has grown over the years with the increased use of technology within educational environments. This has led to a huge amount of data being stored in various data sources representing the student, his/her activities, and other aspects relevant to the learning process. To meet this analytical need, Educational Data Mining (EDM) has emerged to assist educational institutions in identifying key benefits such as students at risk, the level of student engagement or predicting student performance. The aim of this research was to explore the various aspects of student interaction data using data mining techniques to identify relevant patterns of behaviors and possible key attributes that have higher degrees of influence on distance learning students. The main findings identified several patterns of user online profiles based on a set of adopted learning strategies, the research proposed a framework for analyzing such interaction data based on R and Hadoop platforms to correlate online profiles with student performance.",
        "link": "https://dl.acm.org/doi/10.1145/3369199.3369249"
    },
    {
        "title": "Extremely Fast Decision Tree Mining for Evolving Data Streams",
        "authors": [
            "Albert Bifet",
            "Jiajin Zhang",
            "Wei Fan",
            "Cheng He",
            "Jianfeng Zhang",
            "Jianfeng Qian",
            "Geoff Holmes",
            "Bernhard Pfahringer"
        ],
        "date": "August 2017",
        "source": "KDD '17: Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
        "abstract": "Nowadays real-time industrial applications are generating a huge amount of data continuously every day. To process these large data streams, we need fast and efficient methodologies and systems. A useful feature desired for data scientists and analysts is to have easy to visualize and understand machine learning models. Decision trees are preferred in many real-time applications for this reason, and also, because combined in an ensemble, they are one of the most powerful methods in machine learning. In this paper, we present a new system called STREAMDM-C++, that implements decision trees for data streams in C++, and that has been used extensively at Huawei. Streaming decision trees adapt to changes on streams, a huge advantage since standard decision trees are built using a snapshot of data, and can not evolve over time. STREAMDM-C++ is easy to extend, and contains more powerful ensemble methods, and a more efficient and easy to use adaptive decision trees. We compare our new implementation with VFML, the current state of the art implementation in C, and show how our new system outperforms VFML in speed using less resources.",
        "link": "https://dl.acm.org/doi/10.1145/3097983.3098139"
    },
    {
        "title": "Data miners' little helper: data transformation activity cues for cluster analysis on document collections",
        "authors": [
            "Tania Cerquitelli",
            "Evelina Di Corso",
            "Francesco Ventura",
            "Silvia Chiusano"
        ],
        "date": "June 2017",
        "source": "WIMS '17: Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "In this paper we propose a new self-learning engine to streamline the analytics process, as it enables analysts to mine massive data repositories with minimal user intervention. In the context of cluster analysis on a collection of documents this new system, named SELF-DATA (SELF-learning DAta TrAnsformation), suggests to the analyst how to configure the whole mining process for a given dataset. SELF-DATA relies on an engine exploring different data weighting schemas (e.g., normalized term frequencies) and data transformation methods (e.g., PCA) before applying the cluster analysis, evaluating and comparing solutions through different quality indices (e.g., weighted Silhouette), and presenting the k-top solutions to the analyst. SELF-DATA will also include a knowledge base storing results of experiments on previously processed datasets, and a classification algorithm trained on the knowledge base content to forecast the best configuration for the whole mining process for an unexplored dataset. The first development of SELF-DATA running on Apache Spark has been validated on 5 collections of documents. Experimental results highlight that TF-IDF and logarithmic entropy are effective to measure item relevance with sparse datasets, and the LSI method outperforms PCA with a large dictionary.",
        "link": "https://dl.acm.org/doi/10.1145/3102254.3102288"
    },
    {
        "title": "Interactive Visual Graph Mining and Learning",
        "authors": [
            "Ryan A. Rossi",
            "Nesreen K. Ahmed",
            "Rong Zhou",
            "Hoda Eldardiry"
        ],
        "date": null,
        "source": "ACM Transactions on Intelligent Systems and Technology",
        "abstract": "This article presents a platform for interactive graph mining and relational machine learning called GraphVis. The platform combines interactive visual representations with state-of-the-art graph mining and relational machine learning techniques to aid in revealing important insights quickly as well as learning an appropriate and highly predictive model for a particular task (e.g., classification, link prediction, discovering the roles of nodes, and finding influential nodes). Visual representations and interaction techniques and tools are developed for simple, fast, and intuitive real-time interactive exploration, mining, and modeling of graph data. In particular, we propose techniques for interactive relational learning (e.g., node/link classification), interactive link prediction and weighting, role discovery and community detection, higher-order network analysis (via graphlets, network motifs), among others. GraphVis also allows for the refinement and tuning of graph mining and relational learning methods for specific application domains and constraints via an end-to-end interactive visual analytic pipeline that learns, infers, and provides rapid interactive visualization with immediate feedback at each change/prediction in real-time. Other key aspects include interactive filtering, querying, ranking, manipulating, exporting, as well as tools for dynamic network analysis and visualization, interactive graph generators (including new block model approaches), and a variety of multi-level network analysis techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3200764"
    },
    {
        "title": "Data Mining and Opinion Mining: A Tool in Educational Context",
        "authors": [
            "Myriam Peñafiel",
            "Stefanie Vásquez",
            "Diego Vásquez",
            "Juan Zaldumbide",
            "Sergio Luján-Mora"
        ],
        "date": "July 2018",
        "source": "ICoMS '18: Proceedings of the 2018 1st International Conference on Mathematics and Statistics",
        "abstract": "The use of the web as a universal communication platform generates large volumes of data (Big data), which in many cases, need to be processed so that they can become useful knowledge in face of the sceptics who have doubts about the credibility of such information. The use of web data that comes from educational contexts needs to be addressed, since that large amount of unstructured information is not being valued, losing valuable information that can be used. To solve this problem, we propose the use of data mining techniques such as sentiment analysis to validate the information that comes from the educational platforms. The objective of this research is to propose a methodology that allows the user to apply sentiment analysis in a simple way, because although some researchers have done it, very few do with data in the educational context. The results obtained prove that the proposal can be used in similar cases.",
        "link": "https://dl.acm.org/doi/10.1145/3274250.3274263"
    },
    {
        "title": "Research Hotspots Mining and Visualized Analysis Based on Linking Cluster and K-Core Decomposition",
        "authors": [
            "Ying Cai",
            "Fang Huang",
            "Shuai Wang",
            "Hao Zhang",
            "Chunxiu Du"
        ],
        "date": "May 2018",
        "source": "ICDPA 2018: Proceedings of the International Conference on Data Processing and Applications",
        "abstract": "In this paper, we focus on research hotspots mining with knowledge semantic features on discipline topic word networks, which are extracted from scientific and technical documents. For the topic networks, a novel method of combining linking community clustering with k-core decomposition is proposed to mine the hotspot community with knowledge hierarchy structure. And as a post-processing, the hierarchical community identification is implemented by k-core decomposition in order to discover the internal connection with knowledge semantic hierarchy in the research hotspot. Finally, the community density and visualization method is utilized to analyze the core and sub- problem in the research hotspot community. In experiments, the topic networks for a single discipline and interdisciplinary research hotspot are taken as examples of analysis, and the results show the proposed approach can effectively realize the objective of mining and analyzing the discipline research hotspot on the academic topic word network.",
        "link": "https://dl.acm.org/doi/10.1145/3224207.3224216"
    },
    {
        "title": "The Application of Data Mining Algorithms in the Construction of Travel Recommendation System",
        "authors": [
            "Chao Lou",
            "Zhaonan Mu",
            "Mengzhu Liu"
        ],
        "date": "January 2021",
        "source": "CONF-CDS 2021: The 2nd International Conference on Computing and Data Science",
        "abstract": "Photos shared by users of online social network often associated with tags, texts, geographic and time information. These data are ideal research resource which can be used by researchers to observe people's interests and behaviors. They can also be used to analyze people's travel behaviors and interests because it can reflect people's travel activities and experiences directly. This project aimed to consolidate people into groups based on their travel preferences of travel season, travel time span, scenery type of travel location and then analyze popular travel places for different people groups. The travel preferences were analyzed from extracted data applying PHP language and Naive Bayes classification approach. According to the analyzed data, people were divided into groups. Then, DBSCAN clustering was used to get popular locations based on the different groups. As a result, 80 thousand photo data of 5 cities have been collected from Flickr by Phython. It has been showed that people's preference of season in most of the mined cities was spring and summer and the preference of time span was short. Moreover, mined people has been divided into 16 groups according to their preference and based on the groups popular locations have been calculated by DBSCAN clustering. Theses locations have been provided in a recommendation website which was built in this project. For the future work, mining Flickr data associated with other travel social network sites could be a good way to build a more effective recommendation system.",
        "link": "https://dl.acm.org/doi/10.1145/3448734.3450477"
    },
    {
        "title": "Compare Machine Learning Models in Text Classification Using Steam User Reviews",
        "authors": [
            "Youchen Miao",
            "Zeyu Jin",
            "Yumeng Zhang",
            "Yuchen Chen",
            "Junren Lai"
        ],
        "date": "November 2021",
        "source": "ICSED '21: Proceedings of the 2021 3rd International Conference on Software Engineering and Development",
        "abstract": "Text Classification and Sentiment Analysis of game reviews are viewed as important parts in not only academic fields but also in game studies. In this paper, with more than 400 thousand game reviews on Steam platform, we preprocess the data using different libraries (sklearn, nltk, and spaCy) and use them as inputs to build three sentiment classification models based on different algorithms (Naive Bayes, SVM, and Random Forest). In contrast to previous studies that only focus on different sentiment analysis models, our paper also highlights the use of different APIs to preprocess the data and their corresponding model performance. The results show that no matter which API we choose, Random Forest models always perform the best. However, in terms of training time, Naive Bayes is the fastest. This work can be used to apply grid search for researchers to automatically find the optimum API before conducting sentiment analysis in the future.",
        "link": "https://dl.acm.org/doi/10.1145/3507473.3507480"
    },
    {
        "title": "Lost in Transduction: Transductive Transfer Learning in Text Classification",
        "authors": [
            "Alejandro Moreo",
            "Andrea Esuli",
            "Fabrizio Sebastiani"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Obtaining high-quality labelled data for training a classifier in a new application domain is often costly. Transfer Learning\n(a.k.a. “Inductive Transfer”) tries to alleviate these costs by transferring, to the “target” domain of interest, knowledge available from a different “source” domain. In transfer learning the lack of labelled information from the target domain is compensated by the availability at training time of a set of unlabelled examples from the target distribution. Transductive Transfer Learning denotes the transfer learning setting in which the only set of target documents that we are interested in classifying is known and available at training time. Although this definition is indeed in line with Vapnik’s original definition of “transduction”, current terminology in the field is confused. In this article, we discuss how the term “transduction” has been misused in the transfer learning literature, and propose a clarification consistent with the original characterization of this term given by Vapnik. We go on to observe that the above terminology misuse has brought about misleading experimental comparisons, with inductive transfer learning methods that have been incorrectly compared with transductive transfer learning methods. We then, give empirical evidence that the difference in performance between the inductive version and the transductive version of a transfer learning method can indeed be statistically significant (i.e., that knowing at training time the only data one needs to classify indeed gives an advantage). Our clarification allows a reassessment of the field, and of the relative merits of the major, state-of-the-art algorithms for transfer learning in text classification.",
        "link": "https://dl.acm.org/doi/10.1145/3453146"
    },
    {
        "title": "Machine Learning on Mining Potential Adverse Drug Reactions for Pharmacovigilance",
        "authors": [
            "Ke Jia"
        ],
        "date": "July 2021",
        "source": "DSIT 2021: 2021 4th International Conference on Data Science and Information Technology",
        "abstract": "Drugs are related to the lives and health of patients. Mining and discovering potential adverse drug reactions from a large number of unstructured texts play an important role in drug research and pharmacovigilance. It is a hot research issue in the field of biomedicine in recent years. A large number of scholars have discovered a large amount of information on adverse drug reactions from different data sources through effective machine learning methods. This paper reviews the research progress in recent years to provide a reference for future research.",
        "link": "https://dl.acm.org/doi/10.1145/3478905.3478964"
    },
    {
        "title": "TACAM: Topic And Context Aware Argument Mining",
        "authors": [
            "Michael Fromm",
            "Evgeniy Faerman",
            "Thomas Seidl"
        ],
        "date": "October 2019",
        "source": "WI '19: IEEE/WIC/ACM International Conference on Web Intelligence",
        "abstract": "In this work we address the problem of argument search. The purpose of argument search is the distillation of pro and contra arguments for requested topics from large text corpora. In previous works, the usual approach is to use a standard search engine to extract text parts which are relevant to the given topic and subsequently use an argument recognition algorithm to select arguments from them. The main challenge in the argument recognition task, which is also known as argument mining, is that often sentences containing arguments are structurally similar to purely informative sentences without any stance about the topic. In fact, they only differ semantically. Most approaches use topic or search term information only for the first search step and therefore assume that arguments can be classified independently of a topic. We argue that topic information is crucial for argument mining, since the topic defines the semantic context of an argument. Precisely, we propose different models for the classification of arguments, which take information about a topic of an argument into account. Moreover, to enrich the context of a topic and to let models understand the context of the potential argument better, we integrate information from different external sources such as Knowledge Graphs or pre-trained NLP models. Our evaluation shows that considering topic information, especially in connection with external information, provides a significant performance boost for the argument mining task.",
        "link": "https://dl.acm.org/doi/10.1145/3350546.3352506"
    },
    {
        "title": "Text mining for incoming tasks based on the urgency/importance factors and task classification using machine learning tools",
        "authors": [
            "Yasser Ali Alshehri"
        ],
        "date": "March 2020",
        "source": "ICCDA 2020: Proceedings of the 2020 the 4th International Conference on Compute and Data Analysis",
        "abstract": "In workplaces, there is a massive amount of unstructured data from different sources. In this paper, we present a case study that explains how can through communications between employees, we can help to prioritize tasks requests to increase the efficiency of their works for both technical and non-technical workers. This involves managing daily incoming tasks based on their level of urgency and importance.To allow all workers to utilize the urgency-importance matrix as a time-management tool, we need to automate this tool. The textual content of incoming tasks are analyzed, and metrics related to urgency and importance are extracted. A third factor (i.e., the response variable) is defined based on the two input variables (urgency and importance). Then, machine learning applied to the data to predict the class of incoming tasks based on data outcome desired. We used ordinal regression, neural networks, and decision tree algorithms to predict the four levels of task priority. We measure the performance of all using recalls, precisions, and F-scores. All classifiers perform higher than 89% in terms of all measures.",
        "link": "https://dl.acm.org/doi/10.1145/3388142.3388153"
    },
    {
        "title": "Text mining for malware classification using multivariate all repeated patterns detection",
        "authors": [
            "Konstantinos F. Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2019",
        "source": "ASONAM '19: Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Mobile phones have become nowadays a commodity to the majority of people. Using them, people are able to access the world of Internet and connect with their friends, their colleagues at work or even unknown people with common interests. This proliferation of the mobile devices has also been seen as an opportunity for the cyber criminals to deceive smartphone users and steel their money directly or indirectly, respectively, by accessing their bank accounts through the smartphones or by blackmailing them or selling their private data such as photos, credit card data, etc. to third parties. This is usually achieved by installing malware to smartphones masking their malevolent payload as a legitimate application and advertise it to the users with the hope that mobile users will install it in their devices. Thus, any existing application can easily be modified by integrating a malware and then presented it as a legitimate one. In response to this, scientists have proposed a number of malware detection and classification methods using a variety of techniques. Even though, several of them achieve relatively high precision in malware classification, there is still space for improvement. In this paper, we propose a text mining all repeated pattern detection method which uses the decompiled files of an application in order to classify a suspicious application into one of the known malware families. Based on the experimental results using a real malware dataset, the methodology tries to correctly classify (without any misclassification) all randomly selected malware applications of 3 categories with 3 different families each.",
        "link": "https://dl.acm.org/doi/10.1145/3341161.3350841"
    },
    {
        "title": "Comparative Study between Traditional Machine Learning and Deep Learning Approaches for Text Classification",
        "authors": [
            "Cannannore Nidhi Kamath",
            "Syed Saqib Bukhari",
            "Andreas Dengel"
        ],
        "date": "August 2018",
        "source": "DocEng '18: Proceedings of the ACM Symposium on Document Engineering 2018",
        "abstract": "In this contemporaneous world, it is an obligation for any organization working with documents to end up with the insipid task of classifying truckload of documents, which is the nascent stage of venturing into the realm of information retrieval and data mining. But classification of such humongous documents into multiple classes, calls for a lot of time and labor. Hence a system which could classify these documents with acceptable accuracy would be of an unfathomable help in document engineering. We have created multiple classifiers for document classification and compared their accuracy on raw and processed data. We have garnered data used in a corporate organization as well as publicly available data for comparison. Data is processed by removing the stop-words and stemming is implemented to produce root words. Multiple traditional machine learning techniques like Naive Bayes, Logistic Regression, Support Vector Machine, Random forest Classifier and Multi-Layer Perceptron are used for classification of documents. Classifiers are applied on raw and processed data separately and their accuracy is noted. Along with this, Deep learning technique such as Convolution Neural Network is also used to classify the data and its accuracy is compared with that of traditional machine learning techniques. We are also exploring hierarchical classifiers for classification of classes and subclasses. The system classifies the data faster and with better accuracy than if done manually. The results are discussed in the results and evaluation section.",
        "link": "https://dl.acm.org/doi/10.1145/3209280.3209526"
    },
    {
        "title": "Identification of Overpricing in the Purchase of Medication by the Federal Government of Brazil, Using Text Mining and Clustering Based on Ontology",
        "authors": [
            "Marco Aurelio O. S. Correa",
            "Adriano Galindo Leal"
        ],
        "date": "August 2018",
        "source": "ICCBDC '18: Proceedings of the 2018 2nd International Conference on Cloud and Big Data Computing",
        "abstract": "Increasing the transparency level in his actions and spending is one of the primary duties of the Brazilian Federal Government. The creation of laws that oblige full disclose of all its expenditures through transparency portals enables citizens to supervise all government entities. However, only the dissemination of these data, without a definite standard or the availability of data analysis tools, does not guarantee that the citizen is empowered to play his role. Therefore hence, the objective of this work is to identify overprice in the acquisition of products purchased by the federal government of Brazil using the unstructured data available on the Transparency Portal. The last two-years' worth of purchasing data, available in the Transparency Portal, were extracted, processed and stored. Due to his diverse nature and high volume of data, this study focused only on medicines purchased by the Ministry of Health. Ontology-based text mining and clustering techniques were applied for automatic identification and classification of products. The processing of this information was done through text mining and clustering, based on the ontology registered in another database of the Brazilian government. Because of this work, a consolidated price base per medication was created to allow the identification of distortions in prices practised, facilitating the identification of cases that merit further investigation to unravel fraud to the treasury.",
        "link": "https://dl.acm.org/doi/10.1145/3264560.3264569"
    },
    {
        "title": "Computational Estimation by Scientific Data Mining with Classical Methods to Automate Learning Strategies of Scientists",
        "authors": [
            "Aparna S. Varde"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Experimental results are often plotted as 2-dimensional graphical plots (aka graphs) in scientific domains depicting dependent versus independent variables to aid visual analysis of processes. Repeatedly performing laboratory experiments consumes significant time and resources, motivating the need for computational estimation. The goals are to estimate the graph obtained in an experiment given its input conditions, and to estimate the conditions that would lead to a desired graph. Existing estimation approaches often do not meet accuracy and efficiency needs of targeted applications. We develop a computational estimation approach called AutoDomainMine that integrates clustering and classification over complex scientific data in a framework so as to automate classical learning methods of scientists. Knowledge discovered thereby from a database of existing experiments serves as the basis for estimation. Challenges include preserving domain semantics in clustering, finding matching strategies in classification, striking a good balance between elaboration and conciseness while displaying estimation results based on needs of targeted users, and deriving objective measures to capture subjective user interests. These and other challenges are addressed in this work. The AutoDomainMine approach is used to build a computational estimation system, rigorously evaluated with real data in Materials Science. Our evaluation confirms that AutoDomainMine provides desired accuracy and efficiency in computational estimation. It is extendable to other science and engineering domains as proved by adaptation of its sub-processes within fields such as Bioinformatics and Nanotechnology.",
        "link": "https://dl.acm.org/doi/10.1145/3502736"
    },
    {
        "title": "Where is the road for issue reports classification based on text mining?",
        "authors": [
            "Qiang Fan",
            "Yue Yu",
            "Gang Yin",
            "Tao Wang",
            "Huaimin Wang"
        ],
        "date": "November 2017",
        "source": "ESEM '17: Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Currently, open source projects receive various kinds of issues daily, because of the extreme openness of Issue Tracking System (ITS) in GitHub. ITS is a labor-intensive and time-consuming task of issue categorization for project managers. However, a contributor is only required a short textual abstract to report an issue in GitHub. Thus, most traditional classification approaches based on detailed and structured data (e.g., priority, severity, software version and so on) are difficult to adopt. In this paper, issue classification approaches on a large-scale dataset, including 80 popular projects and over 252,000 issue reports collected from GitHub, were investigated. First, four traditional text-based classification methods and their performances were discussed. Semantic perplexity (i.e., an issues description confuses bug-related sentences with nonbug-related sentences) is a crucial factor that affects the classification performances based on quantitative and qualitative study. Finally, A two-stage classifier framework based on the novel metrics of semantic perplexity of issue reports was designed. Results show that our two-stage classification can significantly improve issue classification performances.",
        "link": "https://dl.acm.org/doi/10.1109/ESEM.2017.19"
    },
    {
        "title": "Feature-based Facebook reviews process model for e-management using data mining",
        "authors": [
            "Anish Kumar Varudharajulu",
            "Yongsheng Ma"
        ],
        "date": "January 2019",
        "source": "IC4E '19: Proceedings of the 10th International Conference on E-Education, E-Business, E-Management and E-Learning",
        "abstract": "The data generated from online communication acts as potential gold mines for discovering knowledge for researchers. A large amount of data is also generated in the form of web documents, emails, blogs, and feedback, etc. Text analytics is being significantly employed to mine important information. Opinion mining is the process of extracting human thoughts and perceptions from unstructured texts. The showstopper for designing an opinion mining system for analyzing reviews arise from the fact that customer reviews are often noisy. These reviews are informally written. In addition, they are subjected to spelling mistakes, grammatical errors, improper punctuation and irrational capitalization. This paper focuses on analyzing the different classification and clustering algorithms aimed at extracting and consolidating opinions of customers from social media sites like Facebook, Twitter and through surveys, at multiple levels of granularity to monitor and measure customer satisfaction. Ours is an automated approach, in which the system aids in the process of knowledge assimilation for knowledge-based building and also performs the analytics. Domain experts ratify the knowledge base and also provide training datasets for the system to intuitively gather more instances for ratification. The system identifies opinion expressions as phrases containing opinion words, opinionated features and also opinion modifiers. These expressions are categorized as positive, negative or neutral. Opinion expressions are identified and categorized using localized linguistic techniques. Opinions can be congregated at any desired level of specificity i.e. feature level or product level, user level or service level, etc. We have developed a system based on this approach, which provides the user with a platform to analyze opinion expressions crawled from a set of pre-defined datasets.",
        "link": "https://dl.acm.org/doi/10.1145/3306500.3306514"
    },
    {
        "title": "Discriminative Topic Mining via Category-Name Guided Text Embedding",
        "authors": [
            "Yu Meng",
            "Jiaxin Huang",
            "Guangyuan Wang",
            "Zihan Wang",
            "Chao Zhang",
            "Yu Zhang",
            "Jiawei Han"
        ],
        "date": "April 2020",
        "source": "WWW '20: Proceedings of The Web Conference 2020",
        "abstract": "Mining a set of meaningful and distinctive topics automatically from massive text corpora has broad applications. Existing topic models, however, typically work in a purely unsupervised way, which often generate topics that do not fit users’ particular needs and yield suboptimal performance on downstream tasks. We propose a new task, discriminative topic mining, which leverages a set of user-provided category names to mine discriminative topics from text corpora. This new task not only helps a user understand clearly and distinctively the topics he/she is most interested in, but also benefits directly keyword-driven classification tasks. We develop CatE, a novel category-name guided text embedding method for discriminative topic mining, which effectively leverages minimal user guidance to learn a discriminative embedding space and discover category representative terms in an iterative manner. We conduct a comprehensive set of experiments to show that CatE mines high-quality set of topics guided by category names only, and benefits a variety of downstream applications including weakly-supervised classification and lexical entailment direction identification.",
        "link": "https://dl.acm.org/doi/10.1145/3366423.3380278"
    },
    {
        "title": "Comparative Study of Heart Disease Diagnosis Using Top Ten Data Mining Classification Algorithms",
        "authors": [
            "I. Ketut Agung Enriko"
        ],
        "date": "June 2019",
        "source": "ICFET '19: Proceedings of the 5th International Conference on Frontiers of Educational Technologies",
        "abstract": "Data mining has been used for many purposes, especially for prediction system. In healthcare, data mining algorithms often used in disease diagnosis. Meanwhile, heart disease is known as a primary cause of death over the years. Many studies have been performed in heart disease diagnosis using data mining methods. There are some popular data mining algorithms that can be used in heart disease diagnosis, for example, k-Nearest Neighbor, CART, and AdaBoost. The algorithms are used to analyze a sample of cardiovascular patients data and predict the heart disease type that they suffer. Some parameters are taken from the patient, including EKG morphology, blood pressure, and information about the existence of chest pain, shortness of breath, palpitation, and cold sweat. In this study, medical records data are collected from Harapan Kita Hospital and utilized as a dataset sample in this research. Top ten data mining classification algorithms are used in diagnosing heart disease from Harapan Kita Hospital data and examining their performance by checking the accuracy and speed.",
        "link": "https://dl.acm.org/doi/10.1145/3338188.3338220"
    },
    {
        "title": "Text Mining Approach for Identifying Research Trends",
        "authors": [
            "Snezhana Sulova"
        ],
        "date": "June 2021",
        "source": "CompSysTech '21: Proceedings of the 22nd International Conference on Computer Systems and Technologies",
        "abstract": "With the increase of unstructured data, the issues connected with automatic text processing, the categorization of documents and the discovery of topics have become objects of growing interest. In order to improve the process of grouping and processing research publications, we would like to propose a method based upon natural language processing. It is based on text mining technologies which aim to identify key tendencies in documents. It processes the content of publications by clustering and identifies the topics of each identified group. This analysis helps by identifying key tendencies as well as discovering emerging new areas of research. Publications from the research literature database, Scopus, were used to test the approach. The topic of the publications is “the application of digital technologies in the logistics business”. The experiments were completed using the RapidMiner Studio software.",
        "link": "https://dl.acm.org/doi/10.1145/3472410.3472433"
    },
    {
        "title": "Assessment of Vulnerability Severity using Text Mining",
        "authors": [
            "Georgios Spanos",
            "Lefteris Angelis",
            "Dimitrios Toloudis"
        ],
        "date": "September 2017",
        "source": "PCI '17: Proceedings of the 21st Pan-Hellenic Conference on Informatics",
        "abstract": "Software1 vulnerabilities are closely associated with information systems security, a major and critical field in today's technology. Vulnerabilities constitute a constant and increasing threat for various aspects of everyday life, especially for safety and economy, since the social impact from the problems that they cause is complicated and often unpredictable. Although there is an entire research branch in software engineering that deals with the identification and elimination of vulnerabilities, the growing complexity of software products and the variability of software production procedures are factors contributing to the ongoing occurrence of vulnerabilities, Hence, another area that is being developed in parallel focuses on the study and management of the vulnerabilities that have already been reported and registered in databases. The information contained in such databases includes, a textual description and a number of metrics related to vulnerabilities. The purpose of this paper is to investigate to what extend the assessment of the vulnerability severity can be inferred directly from the corresponding textual description, or in other words, to examine the informative power of the description with respect to the vulnerability severity. For this purpose, text mining techniques, i.e. text analysis and three different classification methods (decision trees, neural networks and support vector machines) were employed. The application of text mining to a sample of 70,678 vulnerabilities from a public data source shows that the description itself is a reliable and highly accurate source of information for vulnerability prioritization.",
        "link": "https://dl.acm.org/doi/10.1145/3139367.3139390"
    },
    {
        "title": "Clustering Algorithms for Spatial Data Mining",
        "authors": [
            "Chetashri Bhadane",
            "Ketan Shah"
        ],
        "date": "April 2020",
        "source": "ICGDA '20: Proceedings of the 2020 3rd International Conference on Geoinformatics and Data Analysis",
        "abstract": "With the advances in mobile and wireless technologies, there has been a rise in applications that track and share the users' geospatial data. People use several social networking sites such as Twitter, Facebook and Flickr, where they share their status updates. With the integration of Global Positioning System (GPS) with mobile phones, it is now possible to share one's locations on these social networks. GPS allows us to record and track a person's movement along with the timestamp. The data set obtained from these GPS logs is vast and is widely used to analyze the users' movement patterns. Specifically, we can find out significant locations based on the number of users present at that location and the time spent by them at such places. Once significant places have been identified, it is also possible to identify the semantic importance of these locations. This paper presents an overview of the clustering techniques used to find important places of interest using large GPS based mobility datasets. Four clustering algorithms, K-Means, DBSCAN, OPTICS and Hierarchical, are implemented, and performance is tested using real-time data of 50 users collected over 2--5 years. Performance summary depicts that K-Means and DBSCAN perform well for spatial data.",
        "link": "https://dl.acm.org/doi/10.1145/3397056.3397068"
    },
    {
        "title": "Feature Selection based Arabic Text Classification using Different Machine Learning Algorithms: Comparative Study",
        "authors": [
            "Sakina Rim Bennabi",
            "Zakaria Elberrichi"
        ],
        "date": "June 2020",
        "source": "ICIST '20: Proceedings of the 10th International Conference on Information Systems and Technologies",
        "abstract": "Feature selection is a method of data pre-processing widely used when mining large data, such as textual classification. Several studies have been conducted to compare the different methods of feature selection applied to corpora in English. Unfortunately, a small number of works concern the Arabic language. This article aims to present a comparative study of different feature selection techniques including: Chi2, the ANOVA method and mutual information, applied on a corpus in Arabic language, while also diversifying the machine learning algorithms (Naive Bayes, SVM and KNN). This experimental study has shown in general that reducing dimensionality with feature selection techniques has slightly affected the performance of textual classification, reducing the size of the corpus by up to 1%.",
        "link": "https://dl.acm.org/doi/10.1145/3447568.3448531"
    },
    {
        "title": "Intelligent Data Analysis using Optimized Support Vector Machine Based Data Mining Approach for Tourism Industry",
        "authors": [
            "Ms Promila Sharma",
            "Uma Meena",
            "Girish Kumar Sharma"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Data analysis involves the deployment of sophisticated approaches from data mining methods, information theory, and artificial intelligence in various fields like tourism, hospitality, and so on for the extraction of knowledge from the gathered and preprocessed data. In tourism, pattern analysis or data analysis using classification is significant for finding the patterns that represent new and potentially useful information or knowledge about the destination and other data. Several data mining techniques are introduced for the classification of data or patterns. However, overfitting, less accuracy, local minima, sensitive to noise are the drawbacks in some existing data mining classification methods. To overcome these challenges, Support vector machine with Red deer optimization (SVM-RDO) based data mining strategy is proposed in this article. Extended Kalman filter (EKF) is utilized in the first phase, i.e., data cleaning to remove the noise and missing values from the input data. Mantaray foraging algorithm (MaFA) is used in the data selection phase, in which the significant data are selected for the further process to reduce the computational complexity. The final phase is the classification, in which SVM-RDO is proposed to access the useful pattern from the selected data. PYTHON is the implementation tool used for the experiment of the proposed model. The experimental analysis is done to show the efficacy of the proposed work. From the experimental results, the proposed SVM-RDO achieved better accuracy, precision, recall, and F1 score than the existing methods for the tourism dataset. Thus, it is showed the effectiveness of the proposed SVM-RDO for pattern analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3494566"
    },
    {
        "title": "Applying text mining to predict learners' cognitive engagement",
        "authors": [
            "Hayati Hind",
            "Mohammed Khalidi Idrissi",
            "Samir Bennani"
        ],
        "date": "October 2017",
        "source": "SCAMS '17: Proceedings of the Mediterranean Symposium on Smart City Application",
        "abstract": "In academic process, engagement represent one of the important success' factor. This paper proposes a predictive system for learners' cognitive engagement based on their online discussion forums participation. In the first level the ontology OWL and the LSA method are used to perform a semantic classification system of the threads according to a specific context chosen by the tutor. Then the Text mining, as a predictive method, is applied to the classified threads and learners' participation in the forums.",
        "link": "https://dl.acm.org/doi/10.1145/3175628.3175655"
    },
    {
        "title": "Investigate Transitions into Drug Addiction through Text Mining of Reddit Data",
        "authors": [
            "John Lu",
            "Sumati Sridhar",
            "Ritika Pandey",
            "Mohammad Al Hasan",
            "Georege Mohler"
        ],
        "date": "July 2019",
        "source": "KDD '19: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Increasing rates of opioid drug abuse and heightened prevalence of online support communities underscore the necessity of employing data mining techniques to better understand drug addiction using these rapidly developing online resources. In this work, we obtained data from Reddit, an online collection of forums, to gather insight into drug use/misuse using text snippets from users narratives. Specifically, using users' posts, we trained a binary classifier which predicts a user's transitions from casual drug discussion forums to drug recovery forums. We also proposed a Cox regression model that outputs likelihoods of such transitions. In doing so, we found that utterances of select drugs and certain linguistic features contained in one's posts can help predict these transitions. Using unfiltered drug-related posts, our research delineates drugs that are associated with higher rates of transitions from recreational drug discussion to support/recovery discussion, offers insight into modern drug culture, and provides tools with potential applications in combating the opioid crisis.",
        "link": "https://dl.acm.org/doi/10.1145/3292500.3330737"
    },
    {
        "title": "Analysing digital banking reviews using text mining",
        "authors": [
            "Li Chen Cheng",
            "Legaspi Rhea Sharmayne"
        ],
        "date": "December 2020",
        "source": "ASONAM '20: Proceedings of the 12th IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Digital banks are new entrants in the banking industry in the Philippines as they only started late 2018. Since then, a handful of players have and are still emerging. With more and more people becoming technologically savvy, it is very critical for financial institutions to develop a digital banking application that will stand out from the competition. This paper aims to use text mining methods to analyse digital banking application reviews. This study will perform topic modelling using LDA to explore customer concerns and will mine association rules between the digital banking features with the review score. The results will reveal which areas the digital banking application can further optimize for customer satisfaction and retention.",
        "link": "https://dl.acm.org/doi/10.1109/ASONAM49781.2020.9381429"
    },
    {
        "title": "Mining Moocs Videos Metadata Using Classification Techniques",
        "authors": [
            "El Harrak Othman",
            "Ghadi Abderrahim",
            "El Bouhdidi Jaber"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "In few years, the internet has become the first source of information for most people, Today MOOCs makes it possible for everyone to access to the education over the world, it's represents an emerging methodology of online teaching and an important development in open education. Due to the rapid development of Moocs and the rapid growth of digital data and video database over the Internet, it is becoming very difficult for learners to browsing and choosing the best training for them. The scientific community has increased the amount of research into new technologies, with a view to improve the digital video utilization: its archiving, indexing, accessibility, acquisition, store and even its process and usability. All these parts of the video utilization entail the necessity of the extraction of all important information of a video, especially in cases of lack of metadata information. Web video mining is retrieving the content using data mining techniques from World Wide Web. There are two approaches for web video mining using traditional image processing (signal processing) and metadata based approach. In this paper, we present the various research makes in this subject and we propose an effective methodology to extract the metadata from Moocs videos and classify them based on the extracted metadata by applying data mining techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090450"
    },
    {
        "title": "Text mining with HathiTrust",
        "authors": [
            "Eleanor Dickson Koehl",
            "Ryan Dubnicek"
        ],
        "date": "June 2019",
        "source": "JCDL '19: Proceedings of the 18th Joint Conference on Digital Libraries",
        "abstract": "This tutorial will introduce attendees to the HathiTrust Research Center and its tools and services for computational text analysis research. HTRC leverages the scope and scale of the HathiTrust Digital Library collection to create opportunities for researchers to perform text data mining on subsets of the corpus. Attendees will develop skills that will allow them to conduct basic text analysis research using HathiTrust data, learn techniques for engaging with scholars on their text data mining research, and explore strategies for computational access to large-scale digital collections.",
        "link": "https://dl.acm.org/doi/10.1109/JCDL.2019.00115"
    },
    {
        "title": "Text mining for plagiarism detection: multivariate pattern detection for recognition of text similarities",
        "authors": [
            "Konstantinos Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2018",
        "source": "ASONAM '18: Proceedings of the 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "The problem of plagiarism the recent years has been intensified by the availability of information in digital form and the accessibility of the electronic libraries through the Internet. As a result, plagiarism detection has been transformed into a big data analytics problem since the number of digital sources is extravagant and a new document needs to be compared with millions of other existing documents. In this paper, a text mining methodology is proposed that can detect all common patterns between a document and the documents in a reference database. The technique is based on a pattern detection algorithm and the corresponding data structure that enables the algorithm to detect all common patterns. The methodology has been applied in a well-defined dataset providing very promising results identifying difficult cases of plagiarism such as technical disguise.",
        "link": "https://dl.acm.org/doi/10.5555/3382225.3382424"
    },
    {
        "title": "Petroleum Engineering Data Text Classification Using Convolutional Neural Network Based Classifier",
        "authors": [
            "Tzuhan Hsu",
            "Yaoqin Zhang"
        ],
        "date": "May 2018",
        "source": "ICMLT '18: Proceedings of the 2018 International Conference on Machine Learning Technologies",
        "abstract": "In this paper, authors introduce a convolutional neural network (CNN) based classification approach for petroleum engineering event. Recently, text classification problem has raised substantial attention in the field of machine learning. The aim of text classification process is to assign correct label to a text on the basis of its content. However, due to the imprecise of feature representation, most of former text classification methods cannot export satisfying result. To solve this problem, authors propose a model based on convolutional neural network. Authors conduct experiments with real world petroleum engineering data text, artificially labelled into 6 categories. Beside purposed CNN-based model, the authors also implement other baseline machine learning approaches: k nearest neighbor, random forest, support vector machines, recurrent neural network, gated recurrent units, long short-term memory. Among all these models, our CNN-based model reach best performance in text classification task. In the last part of this paper, authors compare and analysis their purpose and baseline model.",
        "link": "https://dl.acm.org/doi/10.1145/3231884.3231898"
    },
    {
        "title": "Classifying Indonesian Online Articles as Advertisement Placement Base Using Text Mining",
        "authors": [
            "Nadhira Tasya",
            "Arian Dhini"
        ],
        "date": "July 2017",
        "source": "ICBIM 2017: Proceedings of the International Conference on Business and Information Management",
        "abstract": "Rapid development in technological aspect resulting in growing level of human needs for the latest news, so that emerged a new trend of publishing and accessing news through online media or usually called online journalism. In addition, the number of people who sell and purchase through online sites also continues to increase and this opportunity is utilized by the company and the advertiser by implementing targeted web advertising. However, the high number of articles that have been published and accessed leads to great opportunities for errors in determining where to place the ads. Therefore, it needs a system that can categorize articles accessed by users as the basis of advertisement placement by the company and this classification system can be done by applying the method of Data Mining and Text Mining. This research uses document data in the form of article content that will be categorized into twenty categories of class of advertisement by using Text Mining technique with Support Vector Machine algorithm. The results of this study may be used by companies or advertisers as a basis for placement of ads on selected online media sites.",
        "link": "https://dl.acm.org/doi/10.1145/3134271.3134288"
    },
    {
        "title": "Mining Free-Text Medical Notes for Suicide Risk Assessment",
        "authors": [
            "Marios Adamou",
            "Grigoris Antoniou",
            "Elissavet Greasidou",
            "Vincenzo Lagani",
            "Paulos Charonyktakis",
            "Ioannis Tsamardinos"
        ],
        "date": "July 2018",
        "source": "SETN '18: Proceedings of the 10th Hellenic Conference on Artificial Intelligence",
        "abstract": "Suicide has been considered as an important public health issue for a very long time, and is one of the main causes of death worldwide. Despite suicide prevention strategies being applied, the rate of suicide has not changed substantially over the past decades. Advances in machine learning make it possible to attempt to predict suicide based on the analysis of relevant data to inform clinical practice. This paper reports on findings from the analysis of data of patients who died by suicide in the period 2013-2016 and made use of both structured data and free-text medical notes. We focus on examining various text-mining approaches to support risk assessment. The results show that using advance machine learning and text-mining techniques, it is possible to predict within a specified period which people are most at risk of taking their own life at the time of referral to a mental health service.",
        "link": "https://dl.acm.org/doi/10.1145/3200947.3201020"
    },
    {
        "title": "Integration of data mining techniques in e-learning systems: Clustering Profil of Lerners and Recommender Course System",
        "authors": [
            "A. El Moustamid",
            "E. En-Naimi",
            "J. El Bouhdidi"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "Data mining aims at the extraction of knowledge from large quantities of data by automatic or semi-automatic methods. In the field of education and training e-learning, Data Mining techniques as clustering (The aim of this technique is to identify groups that are similar in some aspect) [21], prediction (is used for developing model where combination of data viewed from different aspects can effect single viewed data. Here, is important to watch how predictors variables are effecting predicted variable)[21], classification are applied in e-learning to improve the level of learners by providing learning according to the learners' requirement) are used to analyze learners profile, to predict and improve student performance. The aim of our project is to develop a system capable of analyzing learners profile and indexing web videos in order to offer learners a rich database with courses that correspond to their levels.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090453"
    },
    {
        "title": "Multi-level mining and visualization of scientific text collections: Exploring a bi-lingual scientific repository",
        "authors": [
            "Pablo Accuosto",
            "Francesco Ronzano",
            "Daniel Ferrés",
            "Horacio Saggion"
        ],
        "date": "December 2017",
        "source": "WOSP 2017: Proceedings of the 6th International Workshop on Mining Scientific Publications",
        "abstract": "We present a system to mine and visualize collections of scientific documents by semantically browsing information extracted from single publications or aggregated throughout corpora of articles. The text mining tool performs deep analysis of document collections allowing the extraction and interpretation of research paper's contents. In addition to the extraction and enrichment of documents with metadata (titles, authors, affiliations, etc), the deep analysis performed comprises semantic interpretation, rhetorical analysis of sentences, triple-based information extraction, and text summarization. The visualization components allow geographical-based exploration of collections, topic-evolution interpretation, and collaborative network analysis among others. The paper presents a case study of a bi-lingual collection in the field of Natural Language Processing (NLP).",
        "link": "https://dl.acm.org/doi/10.1145/3127526.3127529"
    },
    {
        "title": "Hierarchical Topic Mining via Joint Spherical Tree and Text Embedding",
        "authors": [
            "Yu Meng",
            "Yunyi Zhang",
            "Jiaxin Huang",
            "Yu Zhang",
            "Chao Zhang",
            "Jiawei Han"
        ],
        "date": "August 2020",
        "source": "KDD '20: Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Mining a set of meaningful topics organized into a hierarchy is intuitively appealing since topic correlations are ubiquitous in massive text corpora. To account for potential hierarchical topic structures, hierarchical topic models generalize flat topic models by incorporating latent topic hierarchies into their generative modeling process. However, due to their purely unsupervised nature, the learned topic hierarchy often deviates from users' particular needs or interests. To guide the hierarchical topic discovery process with minimal user supervision, we propose a new task, Hierarchical Topic Mining, which takes a category tree described by category names only, and aims to mine a set of representative terms for each category from a text corpus to help a user comprehend his/her interested topics. We develop a novel joint tree and text embedding method along with a principled optimization procedure that allows simultaneous modeling of the category tree structure and the corpus generative process in the spherical space for effective category-representative term discovery. Our comprehensive experiments show that our model, named JoSH, mines a high-quality set of hierarchical topics with high efficiency and benefits weakly-supervised hierarchical text classification tasks.",
        "link": "https://dl.acm.org/doi/10.1145/3394486.3403242"
    },
    {
        "title": "Unsupervised behavioural mining and clustering for malware family identification",
        "authors": [
            "Khanh Huu The Dam",
            "Thomas Given-Wilson",
            "Axel Legay"
        ],
        "date": "March 2021",
        "source": "SAC '21: Proceedings of the 36th Annual ACM Symposium on Applied Computing",
        "abstract": "More accurate and advanced detection and classification of malware requires exploiting program behaviour and not merely syntactic or static features. One approach is to use system call dependency graphs (SCDGs) that represent the program behaviour by interactions with the system, and the relations between these interactions. These SCDGs have been used with supervised learning techniques to very accurately detect and classify malware. This works considers the unsupervised learning challenge of mining for common clusters of behaviour without a priori knowledge. This allows for clustering of similar programs by behaviour, that can then be used for either classification or further analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3412841.3441919"
    },
    {
        "title": "Text Mining for Evaluating Authors' Birth and Death Years",
        "authors": [
            "Dror Moghaz",
            "Yaakov Hacohen-Kerner",
            "Dov Gabbay"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "This article presents a unique method in text and data mining for finding the era, i.e., mining temporal data, in which an anonymous author was living. Finding this era can assist in the examination of a fake document or extracting the time period in which a writer lived. The study and the experiments concern Hebrew, and in some parts, Aramaic and Yiddish rabbinic texts. The rabbinic texts are undated and contain no bibliographic sections, posing an interesting challenge. This work proposes algorithms using key phrases and key words that allow the temporal organization of citations together with linguistic patterns. Based on these key phrases, key words, and the references, we established several types of “Iron-clad,” Heuristic and Greedy rules for estimating the years of birth and death of a writer in an interesting classification task. Experiments were conducted on corpora, including documents authored by 12, 24, and 36 rabbinic writers and demonstrated promising results.",
        "link": "https://dl.acm.org/doi/10.1145/3281631"
    },
    {
        "title": "Transfer Learning to Timed Text Based Video Classification Using CNN",
        "authors": [
            "Zenun Kastrati",
            "Ali Shariq Imran",
            "Arianit Kurti"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "Open educational video resources are gaining popularity with a growing number of massive open online courses (MOOCs). This has created a niche for content providers to adopt effective solutions in automatically organizing and structuring of educational resources for maximum visibility. Recent advances in deep learning techniques are proving useful in managing and classifying resources into appropriate categories. This paper proposes one such convolutional neural network (CNN) model for classifying video lectures in a MOOC setting using a transfer learning approach. The model uses a time-aligned text transcripts corresponding to video lectures from six broader subject categories. Video lectures and their corresponding transcript dataset is gathered from the Coursera MOOC platform. Two different CNN models are proposed: i) CNN based classification using embeddings learned from our MOOC dataset, ii) CNN based classification using transfer learning. Word embeddings generated from two well known state-of-the-art pre-trained models Word2Vec and GloVe, are used in the transfer learning approach for the second case. The proposed CNN models are evaluated using precision, recall, and F1 score and the obtained performance is compared with both conventional and deep learning classifiers. The proposed CNN models have an F1 score improvement of 10-22 percentage points over DNN and conventional classifiers",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326483"
    },
    {
        "title": "Evolutionary role mining in complex networks by ensemble clustering",
        "authors": [
            "Sarvenaz Choobdar",
            "Pedro Ribeiro",
            "Fernando Silva"
        ],
        "date": "April 2017",
        "source": "SAC '17: Proceedings of the Symposium on Applied Computing",
        "abstract": "The structural patterns in the neighborhood of nodes assign unique roles to the nodes. Mining the set of existing roles in a network provides a descriptive profile of the network and draws its general picture. This paper proposes a new method to determine structural roles in a dynamic network based on the current position of nodes and their historic behavior. We develop a temporal ensemble clustering technique to dynamically find groups of nodes, holding similar tempo-structural roles. We compare two weighting functions, based on age and distribution of data, to incorporate temporal behavior of nodes in the role discovery. To evaluate the performance of the proposed method, we assess the results from two points of view: 1) goodness of fit to current structure of the network; 2) consistency with historic data. We conduct the evaluation using different ensemble clustering techniques. The results on real world networks demonstrate that our method can detect tempo-structural roles that simultaneously depict the topology of a network and reflect its dynamics with high accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3019612.3019815"
    },
    {
        "title": "Analyzing used-car web listings via text mining",
        "authors": [
            "Ayhan Demiriz",
            "Fatma Cantaş"
        ],
        "date": "October 2017",
        "source": "IML '17: Proceedings of the 1st International Conference on Internet of Things and Machine Learning",
        "abstract": "Used car trade is one of the major components of the world economies. It is not uncommon to sell a car by placing an internet advertisement irrespective of the geography in these days. A typical content of an advertisement is usually composed of two parts namely the structured and the free text data. The structured data may include some information about the asking price, make, model, year, mileage of the car and the contact info. In most cases, seller may give important clues about the car's current conditions in the free text data where the title (head) of the advertisement can be included as free text too. This paper reports preliminary results from a text mining study conducted on 75K used car internet listings collected from two major car listing web sites in Turkey. As expected, the words and the phrases related to the description of the car are observed to be frequent. The leading concepts in the free text are found to be regarding how to describe the current condition of a car, for example \"no crash history\".",
        "link": "https://dl.acm.org/doi/10.1145/3109761.3109782"
    },
    {
        "title": "Text classification of Diseases Treated by Traditional Chinese Medicine Prescription based on machine learning",
        "authors": [
            "Hanqing Zhao",
            "Bo Han",
            "Chen Li"
        ],
        "date": "September 2020",
        "source": "ISAIMS '20: Proceedings of the 1st International Symposium on Artificial Intelligence in Medical Sciences",
        "abstract": "OBJECTIVE: To explore the application of machine learning in th e identification of diseases treated by traditional Chinese medicine prescriptions. METHODS: Based on the composition of the text document of Chinese medicine prescriptions, the prescriptions were divided into cough, headache and diarrhea. THUTCT were introduced to establish and train two machine learning text classification models, LibSVM and LibLinear, and the prescriptions to be deter mined were put into the model for classification and prediction. R ESULTS: The Precision rate of LibSVM model and Liblinear mod el were 0.7283 and 0.6690. Seven prescriptions were classified an d predicted, and the results were in line with expectations. CONCLUSION: THUCTC has good universality for the content of TCM prescriptions, high classification accuracy and fast testing speed, which is suitable for the text classification and discrimination research of TCM prescriptions for diseases.",
        "link": "https://dl.acm.org/doi/10.1145/3429889.3429895"
    },
    {
        "title": "Evaluation of retweet clustering method classification method using retweets on Twitter without text data",
        "authors": [
            "K. Uchida",
            "F. Toriumi",
            "T. Sakaki"
        ],
        "date": "August 2017",
        "source": "WI '17: Proceedings of the International Conference on Web Intelligence",
        "abstract": "Burst phenomena, which frequently occur on social media, are caused by such social events as flaming on the internet, elections, and natural disasters. To understand people's thoughts and feelings, we must classify their opinions from burst phenomena. Therefore, classification methods that categorize tweets are critical. However, since most classification methods focus on text mining, they cannot group tweets by topics because each tweet has poor linguistic similarities. We used a non-text-based classification method proposed by Baba et al. that groups tweets by topics, even if they have poor linguistic similarities, and verified its validity by comparing it with a text-based classification method in two different evaluations: qualitative and quantitative. In the qualitative evaluation part, we did a questionnaire survey and validated the suitability of the topic clusters created using both the non-and text-based methods. Since evaluating the similarity of every pair of tweets in each topic is difficult, we evaluated the similarity between sampled pairs in the survey and acquired more appropriate topic clustering results using the non-text-based method than the text-based method. In the quantitative evaluation part, we focused on the robustness of each method against data reduction. Many approaches analyze social media data, especially because collecting data from social media is comparatively easy. However, since collecting the whole data of burst phenomena is very costly due to the vast amounts of available social media data, robustness against data reduction is an important index to evaluate classification methods. With the non-text-based method, over 55% of the pairs of tweets in the same cluster were also included in the same cluster even when the data were reduced to 10% in all three of our example cases. In this paper, as a source we focus on Twitter, one of the most popular microblogging services. Using clustering to conduct detailed case analyses, we scrutinized three burst cases that include natural disasters and flaming on the internet and found that a non-text-based method more effectively classified tweets in burst phenomena than a text-based method.",
        "link": "https://dl.acm.org/doi/10.1145/3106426.3106451"
    },
    {
        "title": "Using Spark for Text Mining on Large Scale Liver Cancer Literature",
        "authors": [
            "Ming-Yen Lin",
            "Yu-Ju Lin",
            "Sue-Chen Hsueh"
        ],
        "date": "January 2021",
        "source": "BDET 2021: 2021 the 3rd International Conference on Big Data Engineering and Technology (BDET)",
        "abstract": "Cancer is one of the main causes of death. The number of known cancer to-date is more than one hundred. Liver cancer, ranked after trachea and lung cancer, has long been high in the cancer leading cause in Taiwan and even ranked as second in 2016. The number of scientific articles related to cancer proliferates every year. The number reaches as high as 20 million in PubMed so that discovering useful information from the massive collection is very difficult. In addition, using a single machine to sift through these articles is very time-consuming. Therefore, we present a big data analytic framework using the distributed Apache Spark platform for text mining in PubMed literature. We establish a prediction model for liver cancer articles so that researchers may effectively validate whether an article is related to liver cancer or not. Classification models in Spark MLlib including Linear Support Vector Machines (SVM), Logistic Regression, are used in our experiments. Relevancy to liver cancer is further confirmed by using MeSH (Medical Subject Headings) terms. Logistic regression is about 3 times faster than SVMs and the accuracy of both methods is close to 95% in the experiments using hold-out validation. When max_features is 500 and min_df ≤ 0.1 (or min_df = 1), the accuracy may reach 96%. In the experiments with K-fold cross-validation, the accuracy of SVMs methods is 96%. The experimental results show that that our prediction model may effectively classify liver cancer articles.",
        "link": "https://dl.acm.org/doi/10.1145/3474944.3474958"
    },
    {
        "title": "Automatic topic classification of test cases using text mining at an Android smartphone vendor",
        "authors": [
            "Junji Shimagaki",
            "Yasutaka Kamei",
            "Naoyasu Ubayashi",
            "Abram Hindle"
        ],
        "date": "October 2018",
        "source": "ESEM '18: Proceedings of the 12th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Background: An Android smartphone is an ecosystem of applications, drivers, operating system components, and assets. The volume of the software is large and the number of test cases needed to cover the functionality of an Android system is substantial. Enormous effort has been already taken to properly quantify \"what features and apps were tested and verified?\". This insight is provided by dashboards that summarize test coverage and results per feature. One method to achieve this is to manually tag or label test cases with the topic or function they cover, much like function points. At the studied Android smartphone vendor, tests are labelled with manually defined tags, so-called \"feature labels (FLs)\", and the FLs serve to categorize 100s to 1000s test cases into 10 to 50 groups. Aim: Unfortunately for developers, manual assignment of FLs to 1000s of test cases is a time consuming task, leading to inaccurately labeled test cases, which will render the dashboard useless. We created an automated system that suggests tags/labels to the developers for their test cases rather than manual labeling. Method: We use machine learning models to predict and label the functionality tested by 10,000 test cases developed at the company. Results: Through the quantitative experiments, our models achieved acceptable F-1 performance of 0.3 to 0.88. Also through the qualitative studies with expert teams, we showed that the hierarchy and path of tests was a good predictor of a feature's label. Conclusions: We find that this method can reduce tedious manual effort that software developers spent classifying test cases, while providing more accurate classification results.",
        "link": "https://dl.acm.org/doi/10.1145/3239235.3268927"
    },
    {
        "title": "To apply Data Mining for Classification of Crowd sourced Software Requirements",
        "authors": [
            "Soonh Taj",
            "Qasim Arain",
            "Imran Memon",
            "Asma Zubedi"
        ],
        "date": "April 2019",
        "source": "ICSIE '19: Proceedings of the 8th International Conference on Software and Information Engineering",
        "abstract": "Now a day's main focus of developers is to build quality software that works according to customer needs and for this reason it is necessary to gather right requirements as requirement elicitation is the critical step that impacts on the success of software project as misinterpreted requirements leads to the failure of software project. By keeping this in mind a research is carried out on improving requirements elicitation process and automating the process of classifying requirements. In this research, a model is proposed which will help in this scenario for requirements elicitation and requirement classification. This paper presents a model in which crowd sourcing approach is used so that customers, end users, stakeholders, developers and software engineers can make active participation for requirement elicitation process and requirements gathered using crowdsourcing approach are used by model for classification process i.e. classification of requirements into functional and non-functional requirements. For the proof of proposed model a case study is conducted. Results of case study provided the usefulness and efficiency of proposed model for classification of crowd sourced software requirements.",
        "link": "https://dl.acm.org/doi/10.1145/3328833.3328837"
    },
    {
        "title": "Text classification using Fuzzy TF-IDF and Machine Learning Models",
        "authors": [
            "Mariem Bounabi",
            "Karim El Moutaouakil",
            "Khalid Satori"
        ],
        "date": "October 2019",
        "source": "BDIoT'19: Proceedings of the 4th International Conference on Big Data and Internet of Things",
        "abstract": "The representation of the information has an important impact on the text classification task. Several weighting methods were proposed in the literature, and the term frequency-inverse term frequency (TFIDF), the most know on the text treatment field. The FTF-IDF is a vector representation where the components of the TFIDF are presented as inputs to the Fuzzy Inference System (FIS). In this work, we compare several Machin Learning algorithms such as Naïve Bayes and its derivatives, SVM and Random forest classifiers, using the FTF-IDF representation. To improve the quality of the used classifiers, we call sum attribute selection methods. The recognition rate, for the tested systems, is satisfied, where the system based on naïve Bayes classifier, the FTF-IDF weighting terms, and the info gain select attributes method gives 98.7% as accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3372938.3372956"
    },
    {
        "title": "Text Mining in Cybersecurity: A Systematic Literature Review",
        "authors": [
            "Luciano Ignaczak",
            "Guilherme Goldschmidt",
            "Cristiano André Da Costa",
            "Rodrigo Da Rosa Righi"
        ],
        "date": null,
        "source": "ACM Computing Surveys",
        "abstract": "The growth of data volume has changed cybersecurity activities, demanding a higher level of automation. In this new cybersecurity landscape, text mining emerged as an alternative to improve the efficiency of the activities involving unstructured data. This article proposes a Systematic Literature Review (SLR) to present the application of text mining in the cybersecurity domain. Using a systematic protocol, we identified 2,196 studies, out of which 83 were summarized. As a contribution, we propose a taxonomy to demonstrate the different activities in the cybersecurity domain supported by text mining. We also detail the strategies evaluated in the application of text mining tasks and the use of neural networks to support activities involving unstructured data. The work also discusses text classification performance aiming its application in real-world solutions. The SLR also highlights open gaps for future research, such as the analysis of non-English content and the intensification in the usage of neural networks.",
        "link": "https://dl.acm.org/doi/10.1145/3462477"
    },
    {
        "title": "Using Text Mining to Analyze the Financial Patents",
        "authors": [
            "Yung-Feng Lu",
            "Jia-Lang Xu",
            "Mu-Yen Chen"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "With the advancement and development of science and technology, Nowadays society is the development of information technology, cloud computing, mobile Internet, Internet of Things, block chain and information technology.The combination of finance and technology has made financial technology terms gradually emerging.. This research analyzes the patents of financial technology. However, Taiwan's financial technology patents are mostly G06Q-20 (payment plan, architecture or agreement), G06Q-30 (commercial, such as marketing, shopping, payment, auction or e-commerce). G06Q-40 (financial, investment or tax treatment, insurance).The research use text mining technology to analyze and understand Taiwan's trends and developments in financial technology. The research results show that G06Q-20 patents are biased towards information security and mobile payment. G06Q-30 patents favor shopping websites and advertising. G06Q-40 patents are biased towards diversity, such as securities, insurance, finance, and stocks.",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326478"
    },
    {
        "title": "Identifying Online Profiles of Distance Learning Students Using Data Mining Techniques",
        "authors": [
            "Osama Islam",
            "Muazzam Siddiqui",
            "Naif Radi Aljohani"
        ],
        "date": "October 2019",
        "source": "ICDTE '19: Proceedings of the 3rd International Conference on Digital Technology in Education",
        "abstract": "Educational data has grown over the years with the increased use of technology within educational environments. This has led to a huge amount of data being stored in various data sources representing the student, his/her activities, and other aspects relevant to the learning process. To meet this analytical need, Educational Data Mining (EDM) has emerged to assist educational institutions in identifying key benefits such as students at risk, the level of student engagement or predicting student performance. The aim of this research was to explore the various aspects of student interaction data using data mining techniques to identify relevant patterns of behaviors and possible key attributes that have higher degrees of influence on distance learning students. The main findings identified several patterns of user online profiles based on a set of adopted learning strategies, the research proposed a framework for analyzing such interaction data based on R and Hadoop platforms to correlate online profiles with student performance.",
        "link": "https://dl.acm.org/doi/10.1145/3369199.3369249"
    },
    {
        "title": "Extremely Fast Decision Tree Mining for Evolving Data Streams",
        "authors": [
            "Albert Bifet",
            "Jiajin Zhang",
            "Wei Fan",
            "Cheng He",
            "Jianfeng Zhang",
            "Jianfeng Qian",
            "Geoff Holmes",
            "Bernhard Pfahringer"
        ],
        "date": "August 2017",
        "source": "KDD '17: Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
        "abstract": "Nowadays real-time industrial applications are generating a huge amount of data continuously every day. To process these large data streams, we need fast and efficient methodologies and systems. A useful feature desired for data scientists and analysts is to have easy to visualize and understand machine learning models. Decision trees are preferred in many real-time applications for this reason, and also, because combined in an ensemble, they are one of the most powerful methods in machine learning. In this paper, we present a new system called STREAMDM-C++, that implements decision trees for data streams in C++, and that has been used extensively at Huawei. Streaming decision trees adapt to changes on streams, a huge advantage since standard decision trees are built using a snapshot of data, and can not evolve over time. STREAMDM-C++ is easy to extend, and contains more powerful ensemble methods, and a more efficient and easy to use adaptive decision trees. We compare our new implementation with VFML, the current state of the art implementation in C, and show how our new system outperforms VFML in speed using less resources.",
        "link": "https://dl.acm.org/doi/10.1145/3097983.3098139"
    },
    {
        "title": "Data miners' little helper: data transformation activity cues for cluster analysis on document collections",
        "authors": [
            "Tania Cerquitelli",
            "Evelina Di Corso",
            "Francesco Ventura",
            "Silvia Chiusano"
        ],
        "date": "June 2017",
        "source": "WIMS '17: Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "In this paper we propose a new self-learning engine to streamline the analytics process, as it enables analysts to mine massive data repositories with minimal user intervention. In the context of cluster analysis on a collection of documents this new system, named SELF-DATA (SELF-learning DAta TrAnsformation), suggests to the analyst how to configure the whole mining process for a given dataset. SELF-DATA relies on an engine exploring different data weighting schemas (e.g., normalized term frequencies) and data transformation methods (e.g., PCA) before applying the cluster analysis, evaluating and comparing solutions through different quality indices (e.g., weighted Silhouette), and presenting the k-top solutions to the analyst. SELF-DATA will also include a knowledge base storing results of experiments on previously processed datasets, and a classification algorithm trained on the knowledge base content to forecast the best configuration for the whole mining process for an unexplored dataset. The first development of SELF-DATA running on Apache Spark has been validated on 5 collections of documents. Experimental results highlight that TF-IDF and logarithmic entropy are effective to measure item relevance with sparse datasets, and the LSI method outperforms PCA with a large dictionary.",
        "link": "https://dl.acm.org/doi/10.1145/3102254.3102288"
    },
    {
        "title": "Interactive Visual Graph Mining and Learning",
        "authors": [
            "Ryan A. Rossi",
            "Nesreen K. Ahmed",
            "Rong Zhou",
            "Hoda Eldardiry"
        ],
        "date": null,
        "source": "ACM Transactions on Intelligent Systems and Technology",
        "abstract": "This article presents a platform for interactive graph mining and relational machine learning called GraphVis. The platform combines interactive visual representations with state-of-the-art graph mining and relational machine learning techniques to aid in revealing important insights quickly as well as learning an appropriate and highly predictive model for a particular task (e.g., classification, link prediction, discovering the roles of nodes, and finding influential nodes). Visual representations and interaction techniques and tools are developed for simple, fast, and intuitive real-time interactive exploration, mining, and modeling of graph data. In particular, we propose techniques for interactive relational learning (e.g., node/link classification), interactive link prediction and weighting, role discovery and community detection, higher-order network analysis (via graphlets, network motifs), among others. GraphVis also allows for the refinement and tuning of graph mining and relational learning methods for specific application domains and constraints via an end-to-end interactive visual analytic pipeline that learns, infers, and provides rapid interactive visualization with immediate feedback at each change/prediction in real-time. Other key aspects include interactive filtering, querying, ranking, manipulating, exporting, as well as tools for dynamic network analysis and visualization, interactive graph generators (including new block model approaches), and a variety of multi-level network analysis techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3200764"
    },
    {
        "title": "Data Mining and Opinion Mining: A Tool in Educational Context",
        "authors": [
            "Myriam Peñafiel",
            "Stefanie Vásquez",
            "Diego Vásquez",
            "Juan Zaldumbide",
            "Sergio Luján-Mora"
        ],
        "date": "July 2018",
        "source": "ICoMS '18: Proceedings of the 2018 1st International Conference on Mathematics and Statistics",
        "abstract": "The use of the web as a universal communication platform generates large volumes of data (Big data), which in many cases, need to be processed so that they can become useful knowledge in face of the sceptics who have doubts about the credibility of such information. The use of web data that comes from educational contexts needs to be addressed, since that large amount of unstructured information is not being valued, losing valuable information that can be used. To solve this problem, we propose the use of data mining techniques such as sentiment analysis to validate the information that comes from the educational platforms. The objective of this research is to propose a methodology that allows the user to apply sentiment analysis in a simple way, because although some researchers have done it, very few do with data in the educational context. The results obtained prove that the proposal can be used in similar cases.",
        "link": "https://dl.acm.org/doi/10.1145/3274250.3274263"
    },
    {
        "title": "Research Hotspots Mining and Visualized Analysis Based on Linking Cluster and K-Core Decomposition",
        "authors": [
            "Ying Cai",
            "Fang Huang",
            "Shuai Wang",
            "Hao Zhang",
            "Chunxiu Du"
        ],
        "date": "May 2018",
        "source": "ICDPA 2018: Proceedings of the International Conference on Data Processing and Applications",
        "abstract": "In this paper, we focus on research hotspots mining with knowledge semantic features on discipline topic word networks, which are extracted from scientific and technical documents. For the topic networks, a novel method of combining linking community clustering with k-core decomposition is proposed to mine the hotspot community with knowledge hierarchy structure. And as a post-processing, the hierarchical community identification is implemented by k-core decomposition in order to discover the internal connection with knowledge semantic hierarchy in the research hotspot. Finally, the community density and visualization method is utilized to analyze the core and sub- problem in the research hotspot community. In experiments, the topic networks for a single discipline and interdisciplinary research hotspot are taken as examples of analysis, and the results show the proposed approach can effectively realize the objective of mining and analyzing the discipline research hotspot on the academic topic word network.",
        "link": "https://dl.acm.org/doi/10.1145/3224207.3224216"
    },
    {
        "title": "The Application of Data Mining Algorithms in the Construction of Travel Recommendation System",
        "authors": [
            "Chao Lou",
            "Zhaonan Mu",
            "Mengzhu Liu"
        ],
        "date": "January 2021",
        "source": "CONF-CDS 2021: The 2nd International Conference on Computing and Data Science",
        "abstract": "Photos shared by users of online social network often associated with tags, texts, geographic and time information. These data are ideal research resource which can be used by researchers to observe people's interests and behaviors. They can also be used to analyze people's travel behaviors and interests because it can reflect people's travel activities and experiences directly. This project aimed to consolidate people into groups based on their travel preferences of travel season, travel time span, scenery type of travel location and then analyze popular travel places for different people groups. The travel preferences were analyzed from extracted data applying PHP language and Naive Bayes classification approach. According to the analyzed data, people were divided into groups. Then, DBSCAN clustering was used to get popular locations based on the different groups. As a result, 80 thousand photo data of 5 cities have been collected from Flickr by Phython. It has been showed that people's preference of season in most of the mined cities was spring and summer and the preference of time span was short. Moreover, mined people has been divided into 16 groups according to their preference and based on the groups popular locations have been calculated by DBSCAN clustering. Theses locations have been provided in a recommendation website which was built in this project. For the future work, mining Flickr data associated with other travel social network sites could be a good way to build a more effective recommendation system.",
        "link": "https://dl.acm.org/doi/10.1145/3448734.3450477"
    },
    {
        "title": "Compare Machine Learning Models in Text Classification Using Steam User Reviews",
        "authors": [
            "Youchen Miao",
            "Zeyu Jin",
            "Yumeng Zhang",
            "Yuchen Chen",
            "Junren Lai"
        ],
        "date": "November 2021",
        "source": "ICSED '21: Proceedings of the 2021 3rd International Conference on Software Engineering and Development",
        "abstract": "Text Classification and Sentiment Analysis of game reviews are viewed as important parts in not only academic fields but also in game studies. In this paper, with more than 400 thousand game reviews on Steam platform, we preprocess the data using different libraries (sklearn, nltk, and spaCy) and use them as inputs to build three sentiment classification models based on different algorithms (Naive Bayes, SVM, and Random Forest). In contrast to previous studies that only focus on different sentiment analysis models, our paper also highlights the use of different APIs to preprocess the data and their corresponding model performance. The results show that no matter which API we choose, Random Forest models always perform the best. However, in terms of training time, Naive Bayes is the fastest. This work can be used to apply grid search for researchers to automatically find the optimum API before conducting sentiment analysis in the future.",
        "link": "https://dl.acm.org/doi/10.1145/3507473.3507480"
    },
    {
        "title": "Lost in Transduction: Transductive Transfer Learning in Text Classification",
        "authors": [
            "Alejandro Moreo",
            "Andrea Esuli",
            "Fabrizio Sebastiani"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Obtaining high-quality labelled data for training a classifier in a new application domain is often costly. Transfer Learning\n(a.k.a. “Inductive Transfer”) tries to alleviate these costs by transferring, to the “target” domain of interest, knowledge available from a different “source” domain. In transfer learning the lack of labelled information from the target domain is compensated by the availability at training time of a set of unlabelled examples from the target distribution. Transductive Transfer Learning denotes the transfer learning setting in which the only set of target documents that we are interested in classifying is known and available at training time. Although this definition is indeed in line with Vapnik’s original definition of “transduction”, current terminology in the field is confused. In this article, we discuss how the term “transduction” has been misused in the transfer learning literature, and propose a clarification consistent with the original characterization of this term given by Vapnik. We go on to observe that the above terminology misuse has brought about misleading experimental comparisons, with inductive transfer learning methods that have been incorrectly compared with transductive transfer learning methods. We then, give empirical evidence that the difference in performance between the inductive version and the transductive version of a transfer learning method can indeed be statistically significant (i.e., that knowing at training time the only data one needs to classify indeed gives an advantage). Our clarification allows a reassessment of the field, and of the relative merits of the major, state-of-the-art algorithms for transfer learning in text classification.",
        "link": "https://dl.acm.org/doi/10.1145/3453146"
    },
    {
        "title": "Machine Learning on Mining Potential Adverse Drug Reactions for Pharmacovigilance",
        "authors": [
            "Ke Jia"
        ],
        "date": "July 2021",
        "source": "DSIT 2021: 2021 4th International Conference on Data Science and Information Technology",
        "abstract": "Drugs are related to the lives and health of patients. Mining and discovering potential adverse drug reactions from a large number of unstructured texts play an important role in drug research and pharmacovigilance. It is a hot research issue in the field of biomedicine in recent years. A large number of scholars have discovered a large amount of information on adverse drug reactions from different data sources through effective machine learning methods. This paper reviews the research progress in recent years to provide a reference for future research.",
        "link": "https://dl.acm.org/doi/10.1145/3478905.3478964"
    },
    {
        "title": "TACAM: Topic And Context Aware Argument Mining",
        "authors": [
            "Michael Fromm",
            "Evgeniy Faerman",
            "Thomas Seidl"
        ],
        "date": "October 2019",
        "source": "WI '19: IEEE/WIC/ACM International Conference on Web Intelligence",
        "abstract": "In this work we address the problem of argument search. The purpose of argument search is the distillation of pro and contra arguments for requested topics from large text corpora. In previous works, the usual approach is to use a standard search engine to extract text parts which are relevant to the given topic and subsequently use an argument recognition algorithm to select arguments from them. The main challenge in the argument recognition task, which is also known as argument mining, is that often sentences containing arguments are structurally similar to purely informative sentences without any stance about the topic. In fact, they only differ semantically. Most approaches use topic or search term information only for the first search step and therefore assume that arguments can be classified independently of a topic. We argue that topic information is crucial for argument mining, since the topic defines the semantic context of an argument. Precisely, we propose different models for the classification of arguments, which take information about a topic of an argument into account. Moreover, to enrich the context of a topic and to let models understand the context of the potential argument better, we integrate information from different external sources such as Knowledge Graphs or pre-trained NLP models. Our evaluation shows that considering topic information, especially in connection with external information, provides a significant performance boost for the argument mining task.",
        "link": "https://dl.acm.org/doi/10.1145/3350546.3352506"
    },
    {
        "title": "Text mining for incoming tasks based on the urgency/importance factors and task classification using machine learning tools",
        "authors": [
            "Yasser Ali Alshehri"
        ],
        "date": "March 2020",
        "source": "ICCDA 2020: Proceedings of the 2020 the 4th International Conference on Compute and Data Analysis",
        "abstract": "In workplaces, there is a massive amount of unstructured data from different sources. In this paper, we present a case study that explains how can through communications between employees, we can help to prioritize tasks requests to increase the efficiency of their works for both technical and non-technical workers. This involves managing daily incoming tasks based on their level of urgency and importance.To allow all workers to utilize the urgency-importance matrix as a time-management tool, we need to automate this tool. The textual content of incoming tasks are analyzed, and metrics related to urgency and importance are extracted. A third factor (i.e., the response variable) is defined based on the two input variables (urgency and importance). Then, machine learning applied to the data to predict the class of incoming tasks based on data outcome desired. We used ordinal regression, neural networks, and decision tree algorithms to predict the four levels of task priority. We measure the performance of all using recalls, precisions, and F-scores. All classifiers perform higher than 89% in terms of all measures.",
        "link": "https://dl.acm.org/doi/10.1145/3388142.3388153"
    },
    {
        "title": "Text mining for malware classification using multivariate all repeated patterns detection",
        "authors": [
            "Konstantinos F. Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2019",
        "source": "ASONAM '19: Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Mobile phones have become nowadays a commodity to the majority of people. Using them, people are able to access the world of Internet and connect with their friends, their colleagues at work or even unknown people with common interests. This proliferation of the mobile devices has also been seen as an opportunity for the cyber criminals to deceive smartphone users and steel their money directly or indirectly, respectively, by accessing their bank accounts through the smartphones or by blackmailing them or selling their private data such as photos, credit card data, etc. to third parties. This is usually achieved by installing malware to smartphones masking their malevolent payload as a legitimate application and advertise it to the users with the hope that mobile users will install it in their devices. Thus, any existing application can easily be modified by integrating a malware and then presented it as a legitimate one. In response to this, scientists have proposed a number of malware detection and classification methods using a variety of techniques. Even though, several of them achieve relatively high precision in malware classification, there is still space for improvement. In this paper, we propose a text mining all repeated pattern detection method which uses the decompiled files of an application in order to classify a suspicious application into one of the known malware families. Based on the experimental results using a real malware dataset, the methodology tries to correctly classify (without any misclassification) all randomly selected malware applications of 3 categories with 3 different families each.",
        "link": "https://dl.acm.org/doi/10.1145/3341161.3350841"
    },
    {
        "title": "Comparative Study between Traditional Machine Learning and Deep Learning Approaches for Text Classification",
        "authors": [
            "Cannannore Nidhi Kamath",
            "Syed Saqib Bukhari",
            "Andreas Dengel"
        ],
        "date": "August 2018",
        "source": "DocEng '18: Proceedings of the ACM Symposium on Document Engineering 2018",
        "abstract": "In this contemporaneous world, it is an obligation for any organization working with documents to end up with the insipid task of classifying truckload of documents, which is the nascent stage of venturing into the realm of information retrieval and data mining. But classification of such humongous documents into multiple classes, calls for a lot of time and labor. Hence a system which could classify these documents with acceptable accuracy would be of an unfathomable help in document engineering. We have created multiple classifiers for document classification and compared their accuracy on raw and processed data. We have garnered data used in a corporate organization as well as publicly available data for comparison. Data is processed by removing the stop-words and stemming is implemented to produce root words. Multiple traditional machine learning techniques like Naive Bayes, Logistic Regression, Support Vector Machine, Random forest Classifier and Multi-Layer Perceptron are used for classification of documents. Classifiers are applied on raw and processed data separately and their accuracy is noted. Along with this, Deep learning technique such as Convolution Neural Network is also used to classify the data and its accuracy is compared with that of traditional machine learning techniques. We are also exploring hierarchical classifiers for classification of classes and subclasses. The system classifies the data faster and with better accuracy than if done manually. The results are discussed in the results and evaluation section.",
        "link": "https://dl.acm.org/doi/10.1145/3209280.3209526"
    },
    {
        "title": "Identification of Overpricing in the Purchase of Medication by the Federal Government of Brazil, Using Text Mining and Clustering Based on Ontology",
        "authors": [
            "Marco Aurelio O. S. Correa",
            "Adriano Galindo Leal"
        ],
        "date": "August 2018",
        "source": "ICCBDC '18: Proceedings of the 2018 2nd International Conference on Cloud and Big Data Computing",
        "abstract": "Increasing the transparency level in his actions and spending is one of the primary duties of the Brazilian Federal Government. The creation of laws that oblige full disclose of all its expenditures through transparency portals enables citizens to supervise all government entities. However, only the dissemination of these data, without a definite standard or the availability of data analysis tools, does not guarantee that the citizen is empowered to play his role. Therefore hence, the objective of this work is to identify overprice in the acquisition of products purchased by the federal government of Brazil using the unstructured data available on the Transparency Portal. The last two-years' worth of purchasing data, available in the Transparency Portal, were extracted, processed and stored. Due to his diverse nature and high volume of data, this study focused only on medicines purchased by the Ministry of Health. Ontology-based text mining and clustering techniques were applied for automatic identification and classification of products. The processing of this information was done through text mining and clustering, based on the ontology registered in another database of the Brazilian government. Because of this work, a consolidated price base per medication was created to allow the identification of distortions in prices practised, facilitating the identification of cases that merit further investigation to unravel fraud to the treasury.",
        "link": "https://dl.acm.org/doi/10.1145/3264560.3264569"
    },
    {
        "title": "Computational Estimation by Scientific Data Mining with Classical Methods to Automate Learning Strategies of Scientists",
        "authors": [
            "Aparna S. Varde"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Experimental results are often plotted as 2-dimensional graphical plots (aka graphs) in scientific domains depicting dependent versus independent variables to aid visual analysis of processes. Repeatedly performing laboratory experiments consumes significant time and resources, motivating the need for computational estimation. The goals are to estimate the graph obtained in an experiment given its input conditions, and to estimate the conditions that would lead to a desired graph. Existing estimation approaches often do not meet accuracy and efficiency needs of targeted applications. We develop a computational estimation approach called AutoDomainMine that integrates clustering and classification over complex scientific data in a framework so as to automate classical learning methods of scientists. Knowledge discovered thereby from a database of existing experiments serves as the basis for estimation. Challenges include preserving domain semantics in clustering, finding matching strategies in classification, striking a good balance between elaboration and conciseness while displaying estimation results based on needs of targeted users, and deriving objective measures to capture subjective user interests. These and other challenges are addressed in this work. The AutoDomainMine approach is used to build a computational estimation system, rigorously evaluated with real data in Materials Science. Our evaluation confirms that AutoDomainMine provides desired accuracy and efficiency in computational estimation. It is extendable to other science and engineering domains as proved by adaptation of its sub-processes within fields such as Bioinformatics and Nanotechnology.",
        "link": "https://dl.acm.org/doi/10.1145/3502736"
    },
    {
        "title": "Where is the road for issue reports classification based on text mining?",
        "authors": [
            "Qiang Fan",
            "Yue Yu",
            "Gang Yin",
            "Tao Wang",
            "Huaimin Wang"
        ],
        "date": "November 2017",
        "source": "ESEM '17: Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Currently, open source projects receive various kinds of issues daily, because of the extreme openness of Issue Tracking System (ITS) in GitHub. ITS is a labor-intensive and time-consuming task of issue categorization for project managers. However, a contributor is only required a short textual abstract to report an issue in GitHub. Thus, most traditional classification approaches based on detailed and structured data (e.g., priority, severity, software version and so on) are difficult to adopt. In this paper, issue classification approaches on a large-scale dataset, including 80 popular projects and over 252,000 issue reports collected from GitHub, were investigated. First, four traditional text-based classification methods and their performances were discussed. Semantic perplexity (i.e., an issues description confuses bug-related sentences with nonbug-related sentences) is a crucial factor that affects the classification performances based on quantitative and qualitative study. Finally, A two-stage classifier framework based on the novel metrics of semantic perplexity of issue reports was designed. Results show that our two-stage classification can significantly improve issue classification performances.",
        "link": "https://dl.acm.org/doi/10.1109/ESEM.2017.19"
    },
    {
        "title": "Feature-based Facebook reviews process model for e-management using data mining",
        "authors": [
            "Anish Kumar Varudharajulu",
            "Yongsheng Ma"
        ],
        "date": "January 2019",
        "source": "IC4E '19: Proceedings of the 10th International Conference on E-Education, E-Business, E-Management and E-Learning",
        "abstract": "The data generated from online communication acts as potential gold mines for discovering knowledge for researchers. A large amount of data is also generated in the form of web documents, emails, blogs, and feedback, etc. Text analytics is being significantly employed to mine important information. Opinion mining is the process of extracting human thoughts and perceptions from unstructured texts. The showstopper for designing an opinion mining system for analyzing reviews arise from the fact that customer reviews are often noisy. These reviews are informally written. In addition, they are subjected to spelling mistakes, grammatical errors, improper punctuation and irrational capitalization. This paper focuses on analyzing the different classification and clustering algorithms aimed at extracting and consolidating opinions of customers from social media sites like Facebook, Twitter and through surveys, at multiple levels of granularity to monitor and measure customer satisfaction. Ours is an automated approach, in which the system aids in the process of knowledge assimilation for knowledge-based building and also performs the analytics. Domain experts ratify the knowledge base and also provide training datasets for the system to intuitively gather more instances for ratification. The system identifies opinion expressions as phrases containing opinion words, opinionated features and also opinion modifiers. These expressions are categorized as positive, negative or neutral. Opinion expressions are identified and categorized using localized linguistic techniques. Opinions can be congregated at any desired level of specificity i.e. feature level or product level, user level or service level, etc. We have developed a system based on this approach, which provides the user with a platform to analyze opinion expressions crawled from a set of pre-defined datasets.",
        "link": "https://dl.acm.org/doi/10.1145/3306500.3306514"
    },
    {
        "title": "Discriminative Topic Mining via Category-Name Guided Text Embedding",
        "authors": [
            "Yu Meng",
            "Jiaxin Huang",
            "Guangyuan Wang",
            "Zihan Wang",
            "Chao Zhang",
            "Yu Zhang",
            "Jiawei Han"
        ],
        "date": "April 2020",
        "source": "WWW '20: Proceedings of The Web Conference 2020",
        "abstract": "Mining a set of meaningful and distinctive topics automatically from massive text corpora has broad applications. Existing topic models, however, typically work in a purely unsupervised way, which often generate topics that do not fit users’ particular needs and yield suboptimal performance on downstream tasks. We propose a new task, discriminative topic mining, which leverages a set of user-provided category names to mine discriminative topics from text corpora. This new task not only helps a user understand clearly and distinctively the topics he/she is most interested in, but also benefits directly keyword-driven classification tasks. We develop CatE, a novel category-name guided text embedding method for discriminative topic mining, which effectively leverages minimal user guidance to learn a discriminative embedding space and discover category representative terms in an iterative manner. We conduct a comprehensive set of experiments to show that CatE mines high-quality set of topics guided by category names only, and benefits a variety of downstream applications including weakly-supervised classification and lexical entailment direction identification.",
        "link": "https://dl.acm.org/doi/10.1145/3366423.3380278"
    },
    {
        "title": "Comparative Study of Heart Disease Diagnosis Using Top Ten Data Mining Classification Algorithms",
        "authors": [
            "I. Ketut Agung Enriko"
        ],
        "date": "June 2019",
        "source": "ICFET '19: Proceedings of the 5th International Conference on Frontiers of Educational Technologies",
        "abstract": "Data mining has been used for many purposes, especially for prediction system. In healthcare, data mining algorithms often used in disease diagnosis. Meanwhile, heart disease is known as a primary cause of death over the years. Many studies have been performed in heart disease diagnosis using data mining methods. There are some popular data mining algorithms that can be used in heart disease diagnosis, for example, k-Nearest Neighbor, CART, and AdaBoost. The algorithms are used to analyze a sample of cardiovascular patients data and predict the heart disease type that they suffer. Some parameters are taken from the patient, including EKG morphology, blood pressure, and information about the existence of chest pain, shortness of breath, palpitation, and cold sweat. In this study, medical records data are collected from Harapan Kita Hospital and utilized as a dataset sample in this research. Top ten data mining classification algorithms are used in diagnosing heart disease from Harapan Kita Hospital data and examining their performance by checking the accuracy and speed.",
        "link": "https://dl.acm.org/doi/10.1145/3338188.3338220"
    },
    {
        "title": "Text Mining Approach for Identifying Research Trends",
        "authors": [
            "Snezhana Sulova"
        ],
        "date": "June 2021",
        "source": "CompSysTech '21: Proceedings of the 22nd International Conference on Computer Systems and Technologies",
        "abstract": "With the increase of unstructured data, the issues connected with automatic text processing, the categorization of documents and the discovery of topics have become objects of growing interest. In order to improve the process of grouping and processing research publications, we would like to propose a method based upon natural language processing. It is based on text mining technologies which aim to identify key tendencies in documents. It processes the content of publications by clustering and identifies the topics of each identified group. This analysis helps by identifying key tendencies as well as discovering emerging new areas of research. Publications from the research literature database, Scopus, were used to test the approach. The topic of the publications is “the application of digital technologies in the logistics business”. The experiments were completed using the RapidMiner Studio software.",
        "link": "https://dl.acm.org/doi/10.1145/3472410.3472433"
    },
    {
        "title": "Assessment of Vulnerability Severity using Text Mining",
        "authors": [
            "Georgios Spanos",
            "Lefteris Angelis",
            "Dimitrios Toloudis"
        ],
        "date": "September 2017",
        "source": "PCI '17: Proceedings of the 21st Pan-Hellenic Conference on Informatics",
        "abstract": "Software1 vulnerabilities are closely associated with information systems security, a major and critical field in today's technology. Vulnerabilities constitute a constant and increasing threat for various aspects of everyday life, especially for safety and economy, since the social impact from the problems that they cause is complicated and often unpredictable. Although there is an entire research branch in software engineering that deals with the identification and elimination of vulnerabilities, the growing complexity of software products and the variability of software production procedures are factors contributing to the ongoing occurrence of vulnerabilities, Hence, another area that is being developed in parallel focuses on the study and management of the vulnerabilities that have already been reported and registered in databases. The information contained in such databases includes, a textual description and a number of metrics related to vulnerabilities. The purpose of this paper is to investigate to what extend the assessment of the vulnerability severity can be inferred directly from the corresponding textual description, or in other words, to examine the informative power of the description with respect to the vulnerability severity. For this purpose, text mining techniques, i.e. text analysis and three different classification methods (decision trees, neural networks and support vector machines) were employed. The application of text mining to a sample of 70,678 vulnerabilities from a public data source shows that the description itself is a reliable and highly accurate source of information for vulnerability prioritization.",
        "link": "https://dl.acm.org/doi/10.1145/3139367.3139390"
    },
    {
        "title": "Clustering Algorithms for Spatial Data Mining",
        "authors": [
            "Chetashri Bhadane",
            "Ketan Shah"
        ],
        "date": "April 2020",
        "source": "ICGDA '20: Proceedings of the 2020 3rd International Conference on Geoinformatics and Data Analysis",
        "abstract": "With the advances in mobile and wireless technologies, there has been a rise in applications that track and share the users' geospatial data. People use several social networking sites such as Twitter, Facebook and Flickr, where they share their status updates. With the integration of Global Positioning System (GPS) with mobile phones, it is now possible to share one's locations on these social networks. GPS allows us to record and track a person's movement along with the timestamp. The data set obtained from these GPS logs is vast and is widely used to analyze the users' movement patterns. Specifically, we can find out significant locations based on the number of users present at that location and the time spent by them at such places. Once significant places have been identified, it is also possible to identify the semantic importance of these locations. This paper presents an overview of the clustering techniques used to find important places of interest using large GPS based mobility datasets. Four clustering algorithms, K-Means, DBSCAN, OPTICS and Hierarchical, are implemented, and performance is tested using real-time data of 50 users collected over 2--5 years. Performance summary depicts that K-Means and DBSCAN perform well for spatial data.",
        "link": "https://dl.acm.org/doi/10.1145/3397056.3397068"
    },
    {
        "title": "Feature Selection based Arabic Text Classification using Different Machine Learning Algorithms: Comparative Study",
        "authors": [
            "Sakina Rim Bennabi",
            "Zakaria Elberrichi"
        ],
        "date": "June 2020",
        "source": "ICIST '20: Proceedings of the 10th International Conference on Information Systems and Technologies",
        "abstract": "Feature selection is a method of data pre-processing widely used when mining large data, such as textual classification. Several studies have been conducted to compare the different methods of feature selection applied to corpora in English. Unfortunately, a small number of works concern the Arabic language. This article aims to present a comparative study of different feature selection techniques including: Chi2, the ANOVA method and mutual information, applied on a corpus in Arabic language, while also diversifying the machine learning algorithms (Naive Bayes, SVM and KNN). This experimental study has shown in general that reducing dimensionality with feature selection techniques has slightly affected the performance of textual classification, reducing the size of the corpus by up to 1%.",
        "link": "https://dl.acm.org/doi/10.1145/3447568.3448531"
    },
    {
        "title": "Intelligent Data Analysis using Optimized Support Vector Machine Based Data Mining Approach for Tourism Industry",
        "authors": [
            "Ms Promila Sharma",
            "Uma Meena",
            "Girish Kumar Sharma"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Data analysis involves the deployment of sophisticated approaches from data mining methods, information theory, and artificial intelligence in various fields like tourism, hospitality, and so on for the extraction of knowledge from the gathered and preprocessed data. In tourism, pattern analysis or data analysis using classification is significant for finding the patterns that represent new and potentially useful information or knowledge about the destination and other data. Several data mining techniques are introduced for the classification of data or patterns. However, overfitting, less accuracy, local minima, sensitive to noise are the drawbacks in some existing data mining classification methods. To overcome these challenges, Support vector machine with Red deer optimization (SVM-RDO) based data mining strategy is proposed in this article. Extended Kalman filter (EKF) is utilized in the first phase, i.e., data cleaning to remove the noise and missing values from the input data. Mantaray foraging algorithm (MaFA) is used in the data selection phase, in which the significant data are selected for the further process to reduce the computational complexity. The final phase is the classification, in which SVM-RDO is proposed to access the useful pattern from the selected data. PYTHON is the implementation tool used for the experiment of the proposed model. The experimental analysis is done to show the efficacy of the proposed work. From the experimental results, the proposed SVM-RDO achieved better accuracy, precision, recall, and F1 score than the existing methods for the tourism dataset. Thus, it is showed the effectiveness of the proposed SVM-RDO for pattern analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3494566"
    },
    {
        "title": "Applying text mining to predict learners' cognitive engagement",
        "authors": [
            "Hayati Hind",
            "Mohammed Khalidi Idrissi",
            "Samir Bennani"
        ],
        "date": "October 2017",
        "source": "SCAMS '17: Proceedings of the Mediterranean Symposium on Smart City Application",
        "abstract": "In academic process, engagement represent one of the important success' factor. This paper proposes a predictive system for learners' cognitive engagement based on their online discussion forums participation. In the first level the ontology OWL and the LSA method are used to perform a semantic classification system of the threads according to a specific context chosen by the tutor. Then the Text mining, as a predictive method, is applied to the classified threads and learners' participation in the forums.",
        "link": "https://dl.acm.org/doi/10.1145/3175628.3175655"
    },
    {
        "title": "Investigate Transitions into Drug Addiction through Text Mining of Reddit Data",
        "authors": [
            "John Lu",
            "Sumati Sridhar",
            "Ritika Pandey",
            "Mohammad Al Hasan",
            "Georege Mohler"
        ],
        "date": "July 2019",
        "source": "KDD '19: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Increasing rates of opioid drug abuse and heightened prevalence of online support communities underscore the necessity of employing data mining techniques to better understand drug addiction using these rapidly developing online resources. In this work, we obtained data from Reddit, an online collection of forums, to gather insight into drug use/misuse using text snippets from users narratives. Specifically, using users' posts, we trained a binary classifier which predicts a user's transitions from casual drug discussion forums to drug recovery forums. We also proposed a Cox regression model that outputs likelihoods of such transitions. In doing so, we found that utterances of select drugs and certain linguistic features contained in one's posts can help predict these transitions. Using unfiltered drug-related posts, our research delineates drugs that are associated with higher rates of transitions from recreational drug discussion to support/recovery discussion, offers insight into modern drug culture, and provides tools with potential applications in combating the opioid crisis.",
        "link": "https://dl.acm.org/doi/10.1145/3292500.3330737"
    },
    {
        "title": "Analysing digital banking reviews using text mining",
        "authors": [
            "Li Chen Cheng",
            "Legaspi Rhea Sharmayne"
        ],
        "date": "December 2020",
        "source": "ASONAM '20: Proceedings of the 12th IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Digital banks are new entrants in the banking industry in the Philippines as they only started late 2018. Since then, a handful of players have and are still emerging. With more and more people becoming technologically savvy, it is very critical for financial institutions to develop a digital banking application that will stand out from the competition. This paper aims to use text mining methods to analyse digital banking application reviews. This study will perform topic modelling using LDA to explore customer concerns and will mine association rules between the digital banking features with the review score. The results will reveal which areas the digital banking application can further optimize for customer satisfaction and retention.",
        "link": "https://dl.acm.org/doi/10.1109/ASONAM49781.2020.9381429"
    },
    {
        "title": "Mining Moocs Videos Metadata Using Classification Techniques",
        "authors": [
            "El Harrak Othman",
            "Ghadi Abderrahim",
            "El Bouhdidi Jaber"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "In few years, the internet has become the first source of information for most people, Today MOOCs makes it possible for everyone to access to the education over the world, it's represents an emerging methodology of online teaching and an important development in open education. Due to the rapid development of Moocs and the rapid growth of digital data and video database over the Internet, it is becoming very difficult for learners to browsing and choosing the best training for them. The scientific community has increased the amount of research into new technologies, with a view to improve the digital video utilization: its archiving, indexing, accessibility, acquisition, store and even its process and usability. All these parts of the video utilization entail the necessity of the extraction of all important information of a video, especially in cases of lack of metadata information. Web video mining is retrieving the content using data mining techniques from World Wide Web. There are two approaches for web video mining using traditional image processing (signal processing) and metadata based approach. In this paper, we present the various research makes in this subject and we propose an effective methodology to extract the metadata from Moocs videos and classify them based on the extracted metadata by applying data mining techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090450"
    },
    {
        "title": "Text mining with HathiTrust",
        "authors": [
            "Eleanor Dickson Koehl",
            "Ryan Dubnicek"
        ],
        "date": "June 2019",
        "source": "JCDL '19: Proceedings of the 18th Joint Conference on Digital Libraries",
        "abstract": "This tutorial will introduce attendees to the HathiTrust Research Center and its tools and services for computational text analysis research. HTRC leverages the scope and scale of the HathiTrust Digital Library collection to create opportunities for researchers to perform text data mining on subsets of the corpus. Attendees will develop skills that will allow them to conduct basic text analysis research using HathiTrust data, learn techniques for engaging with scholars on their text data mining research, and explore strategies for computational access to large-scale digital collections.",
        "link": "https://dl.acm.org/doi/10.1109/JCDL.2019.00115"
    },
    {
        "title": "Text mining for plagiarism detection: multivariate pattern detection for recognition of text similarities",
        "authors": [
            "Konstantinos Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2018",
        "source": "ASONAM '18: Proceedings of the 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "The problem of plagiarism the recent years has been intensified by the availability of information in digital form and the accessibility of the electronic libraries through the Internet. As a result, plagiarism detection has been transformed into a big data analytics problem since the number of digital sources is extravagant and a new document needs to be compared with millions of other existing documents. In this paper, a text mining methodology is proposed that can detect all common patterns between a document and the documents in a reference database. The technique is based on a pattern detection algorithm and the corresponding data structure that enables the algorithm to detect all common patterns. The methodology has been applied in a well-defined dataset providing very promising results identifying difficult cases of plagiarism such as technical disguise.",
        "link": "https://dl.acm.org/doi/10.5555/3382225.3382424"
    },
    {
        "title": "Petroleum Engineering Data Text Classification Using Convolutional Neural Network Based Classifier",
        "authors": [
            "Tzuhan Hsu",
            "Yaoqin Zhang"
        ],
        "date": "May 2018",
        "source": "ICMLT '18: Proceedings of the 2018 International Conference on Machine Learning Technologies",
        "abstract": "In this paper, authors introduce a convolutional neural network (CNN) based classification approach for petroleum engineering event. Recently, text classification problem has raised substantial attention in the field of machine learning. The aim of text classification process is to assign correct label to a text on the basis of its content. However, due to the imprecise of feature representation, most of former text classification methods cannot export satisfying result. To solve this problem, authors propose a model based on convolutional neural network. Authors conduct experiments with real world petroleum engineering data text, artificially labelled into 6 categories. Beside purposed CNN-based model, the authors also implement other baseline machine learning approaches: k nearest neighbor, random forest, support vector machines, recurrent neural network, gated recurrent units, long short-term memory. Among all these models, our CNN-based model reach best performance in text classification task. In the last part of this paper, authors compare and analysis their purpose and baseline model.",
        "link": "https://dl.acm.org/doi/10.1145/3231884.3231898"
    },
    {
        "title": "Classifying Indonesian Online Articles as Advertisement Placement Base Using Text Mining",
        "authors": [
            "Nadhira Tasya",
            "Arian Dhini"
        ],
        "date": "July 2017",
        "source": "ICBIM 2017: Proceedings of the International Conference on Business and Information Management",
        "abstract": "Rapid development in technological aspect resulting in growing level of human needs for the latest news, so that emerged a new trend of publishing and accessing news through online media or usually called online journalism. In addition, the number of people who sell and purchase through online sites also continues to increase and this opportunity is utilized by the company and the advertiser by implementing targeted web advertising. However, the high number of articles that have been published and accessed leads to great opportunities for errors in determining where to place the ads. Therefore, it needs a system that can categorize articles accessed by users as the basis of advertisement placement by the company and this classification system can be done by applying the method of Data Mining and Text Mining. This research uses document data in the form of article content that will be categorized into twenty categories of class of advertisement by using Text Mining technique with Support Vector Machine algorithm. The results of this study may be used by companies or advertisers as a basis for placement of ads on selected online media sites.",
        "link": "https://dl.acm.org/doi/10.1145/3134271.3134288"
    },
    {
        "title": "Mining Free-Text Medical Notes for Suicide Risk Assessment",
        "authors": [
            "Marios Adamou",
            "Grigoris Antoniou",
            "Elissavet Greasidou",
            "Vincenzo Lagani",
            "Paulos Charonyktakis",
            "Ioannis Tsamardinos"
        ],
        "date": "July 2018",
        "source": "SETN '18: Proceedings of the 10th Hellenic Conference on Artificial Intelligence",
        "abstract": "Suicide has been considered as an important public health issue for a very long time, and is one of the main causes of death worldwide. Despite suicide prevention strategies being applied, the rate of suicide has not changed substantially over the past decades. Advances in machine learning make it possible to attempt to predict suicide based on the analysis of relevant data to inform clinical practice. This paper reports on findings from the analysis of data of patients who died by suicide in the period 2013-2016 and made use of both structured data and free-text medical notes. We focus on examining various text-mining approaches to support risk assessment. The results show that using advance machine learning and text-mining techniques, it is possible to predict within a specified period which people are most at risk of taking their own life at the time of referral to a mental health service.",
        "link": "https://dl.acm.org/doi/10.1145/3200947.3201020"
    },
    {
        "title": "Integration of data mining techniques in e-learning systems: Clustering Profil of Lerners and Recommender Course System",
        "authors": [
            "A. El Moustamid",
            "E. En-Naimi",
            "J. El Bouhdidi"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "Data mining aims at the extraction of knowledge from large quantities of data by automatic or semi-automatic methods. In the field of education and training e-learning, Data Mining techniques as clustering (The aim of this technique is to identify groups that are similar in some aspect) [21], prediction (is used for developing model where combination of data viewed from different aspects can effect single viewed data. Here, is important to watch how predictors variables are effecting predicted variable)[21], classification are applied in e-learning to improve the level of learners by providing learning according to the learners' requirement) are used to analyze learners profile, to predict and improve student performance. The aim of our project is to develop a system capable of analyzing learners profile and indexing web videos in order to offer learners a rich database with courses that correspond to their levels.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090453"
    },
    {
        "title": "Multi-level mining and visualization of scientific text collections: Exploring a bi-lingual scientific repository",
        "authors": [
            "Pablo Accuosto",
            "Francesco Ronzano",
            "Daniel Ferrés",
            "Horacio Saggion"
        ],
        "date": "December 2017",
        "source": "WOSP 2017: Proceedings of the 6th International Workshop on Mining Scientific Publications",
        "abstract": "We present a system to mine and visualize collections of scientific documents by semantically browsing information extracted from single publications or aggregated throughout corpora of articles. The text mining tool performs deep analysis of document collections allowing the extraction and interpretation of research paper's contents. In addition to the extraction and enrichment of documents with metadata (titles, authors, affiliations, etc), the deep analysis performed comprises semantic interpretation, rhetorical analysis of sentences, triple-based information extraction, and text summarization. The visualization components allow geographical-based exploration of collections, topic-evolution interpretation, and collaborative network analysis among others. The paper presents a case study of a bi-lingual collection in the field of Natural Language Processing (NLP).",
        "link": "https://dl.acm.org/doi/10.1145/3127526.3127529"
    },
    {
        "title": "Hierarchical Topic Mining via Joint Spherical Tree and Text Embedding",
        "authors": [
            "Yu Meng",
            "Yunyi Zhang",
            "Jiaxin Huang",
            "Yu Zhang",
            "Chao Zhang",
            "Jiawei Han"
        ],
        "date": "August 2020",
        "source": "KDD '20: Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Mining a set of meaningful topics organized into a hierarchy is intuitively appealing since topic correlations are ubiquitous in massive text corpora. To account for potential hierarchical topic structures, hierarchical topic models generalize flat topic models by incorporating latent topic hierarchies into their generative modeling process. However, due to their purely unsupervised nature, the learned topic hierarchy often deviates from users' particular needs or interests. To guide the hierarchical topic discovery process with minimal user supervision, we propose a new task, Hierarchical Topic Mining, which takes a category tree described by category names only, and aims to mine a set of representative terms for each category from a text corpus to help a user comprehend his/her interested topics. We develop a novel joint tree and text embedding method along with a principled optimization procedure that allows simultaneous modeling of the category tree structure and the corpus generative process in the spherical space for effective category-representative term discovery. Our comprehensive experiments show that our model, named JoSH, mines a high-quality set of hierarchical topics with high efficiency and benefits weakly-supervised hierarchical text classification tasks.",
        "link": "https://dl.acm.org/doi/10.1145/3394486.3403242"
    },
    {
        "title": "Unsupervised behavioural mining and clustering for malware family identification",
        "authors": [
            "Khanh Huu The Dam",
            "Thomas Given-Wilson",
            "Axel Legay"
        ],
        "date": "March 2021",
        "source": "SAC '21: Proceedings of the 36th Annual ACM Symposium on Applied Computing",
        "abstract": "More accurate and advanced detection and classification of malware requires exploiting program behaviour and not merely syntactic or static features. One approach is to use system call dependency graphs (SCDGs) that represent the program behaviour by interactions with the system, and the relations between these interactions. These SCDGs have been used with supervised learning techniques to very accurately detect and classify malware. This works considers the unsupervised learning challenge of mining for common clusters of behaviour without a priori knowledge. This allows for clustering of similar programs by behaviour, that can then be used for either classification or further analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3412841.3441919"
    },
    {
        "title": "Text Mining for Evaluating Authors' Birth and Death Years",
        "authors": [
            "Dror Moghaz",
            "Yaakov Hacohen-Kerner",
            "Dov Gabbay"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "This article presents a unique method in text and data mining for finding the era, i.e., mining temporal data, in which an anonymous author was living. Finding this era can assist in the examination of a fake document or extracting the time period in which a writer lived. The study and the experiments concern Hebrew, and in some parts, Aramaic and Yiddish rabbinic texts. The rabbinic texts are undated and contain no bibliographic sections, posing an interesting challenge. This work proposes algorithms using key phrases and key words that allow the temporal organization of citations together with linguistic patterns. Based on these key phrases, key words, and the references, we established several types of “Iron-clad,” Heuristic and Greedy rules for estimating the years of birth and death of a writer in an interesting classification task. Experiments were conducted on corpora, including documents authored by 12, 24, and 36 rabbinic writers and demonstrated promising results.",
        "link": "https://dl.acm.org/doi/10.1145/3281631"
    },
    {
        "title": "Transfer Learning to Timed Text Based Video Classification Using CNN",
        "authors": [
            "Zenun Kastrati",
            "Ali Shariq Imran",
            "Arianit Kurti"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "Open educational video resources are gaining popularity with a growing number of massive open online courses (MOOCs). This has created a niche for content providers to adopt effective solutions in automatically organizing and structuring of educational resources for maximum visibility. Recent advances in deep learning techniques are proving useful in managing and classifying resources into appropriate categories. This paper proposes one such convolutional neural network (CNN) model for classifying video lectures in a MOOC setting using a transfer learning approach. The model uses a time-aligned text transcripts corresponding to video lectures from six broader subject categories. Video lectures and their corresponding transcript dataset is gathered from the Coursera MOOC platform. Two different CNN models are proposed: i) CNN based classification using embeddings learned from our MOOC dataset, ii) CNN based classification using transfer learning. Word embeddings generated from two well known state-of-the-art pre-trained models Word2Vec and GloVe, are used in the transfer learning approach for the second case. The proposed CNN models are evaluated using precision, recall, and F1 score and the obtained performance is compared with both conventional and deep learning classifiers. The proposed CNN models have an F1 score improvement of 10-22 percentage points over DNN and conventional classifiers",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326483"
    },
    {
        "title": "Evolutionary role mining in complex networks by ensemble clustering",
        "authors": [
            "Sarvenaz Choobdar",
            "Pedro Ribeiro",
            "Fernando Silva"
        ],
        "date": "April 2017",
        "source": "SAC '17: Proceedings of the Symposium on Applied Computing",
        "abstract": "The structural patterns in the neighborhood of nodes assign unique roles to the nodes. Mining the set of existing roles in a network provides a descriptive profile of the network and draws its general picture. This paper proposes a new method to determine structural roles in a dynamic network based on the current position of nodes and their historic behavior. We develop a temporal ensemble clustering technique to dynamically find groups of nodes, holding similar tempo-structural roles. We compare two weighting functions, based on age and distribution of data, to incorporate temporal behavior of nodes in the role discovery. To evaluate the performance of the proposed method, we assess the results from two points of view: 1) goodness of fit to current structure of the network; 2) consistency with historic data. We conduct the evaluation using different ensemble clustering techniques. The results on real world networks demonstrate that our method can detect tempo-structural roles that simultaneously depict the topology of a network and reflect its dynamics with high accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3019612.3019815"
    },
    {
        "title": "Analyzing used-car web listings via text mining",
        "authors": [
            "Ayhan Demiriz",
            "Fatma Cantaş"
        ],
        "date": "October 2017",
        "source": "IML '17: Proceedings of the 1st International Conference on Internet of Things and Machine Learning",
        "abstract": "Used car trade is one of the major components of the world economies. It is not uncommon to sell a car by placing an internet advertisement irrespective of the geography in these days. A typical content of an advertisement is usually composed of two parts namely the structured and the free text data. The structured data may include some information about the asking price, make, model, year, mileage of the car and the contact info. In most cases, seller may give important clues about the car's current conditions in the free text data where the title (head) of the advertisement can be included as free text too. This paper reports preliminary results from a text mining study conducted on 75K used car internet listings collected from two major car listing web sites in Turkey. As expected, the words and the phrases related to the description of the car are observed to be frequent. The leading concepts in the free text are found to be regarding how to describe the current condition of a car, for example \"no crash history\".",
        "link": "https://dl.acm.org/doi/10.1145/3109761.3109782"
    },
    {
        "title": "Text classification of Diseases Treated by Traditional Chinese Medicine Prescription based on machine learning",
        "authors": [
            "Hanqing Zhao",
            "Bo Han",
            "Chen Li"
        ],
        "date": "September 2020",
        "source": "ISAIMS '20: Proceedings of the 1st International Symposium on Artificial Intelligence in Medical Sciences",
        "abstract": "OBJECTIVE: To explore the application of machine learning in th e identification of diseases treated by traditional Chinese medicine prescriptions. METHODS: Based on the composition of the text document of Chinese medicine prescriptions, the prescriptions were divided into cough, headache and diarrhea. THUTCT were introduced to establish and train two machine learning text classification models, LibSVM and LibLinear, and the prescriptions to be deter mined were put into the model for classification and prediction. R ESULTS: The Precision rate of LibSVM model and Liblinear mod el were 0.7283 and 0.6690. Seven prescriptions were classified an d predicted, and the results were in line with expectations. CONCLUSION: THUCTC has good universality for the content of TCM prescriptions, high classification accuracy and fast testing speed, which is suitable for the text classification and discrimination research of TCM prescriptions for diseases.",
        "link": "https://dl.acm.org/doi/10.1145/3429889.3429895"
    },
    {
        "title": "Evaluation of retweet clustering method classification method using retweets on Twitter without text data",
        "authors": [
            "K. Uchida",
            "F. Toriumi",
            "T. Sakaki"
        ],
        "date": "August 2017",
        "source": "WI '17: Proceedings of the International Conference on Web Intelligence",
        "abstract": "Burst phenomena, which frequently occur on social media, are caused by such social events as flaming on the internet, elections, and natural disasters. To understand people's thoughts and feelings, we must classify their opinions from burst phenomena. Therefore, classification methods that categorize tweets are critical. However, since most classification methods focus on text mining, they cannot group tweets by topics because each tweet has poor linguistic similarities. We used a non-text-based classification method proposed by Baba et al. that groups tweets by topics, even if they have poor linguistic similarities, and verified its validity by comparing it with a text-based classification method in two different evaluations: qualitative and quantitative. In the qualitative evaluation part, we did a questionnaire survey and validated the suitability of the topic clusters created using both the non-and text-based methods. Since evaluating the similarity of every pair of tweets in each topic is difficult, we evaluated the similarity between sampled pairs in the survey and acquired more appropriate topic clustering results using the non-text-based method than the text-based method. In the quantitative evaluation part, we focused on the robustness of each method against data reduction. Many approaches analyze social media data, especially because collecting data from social media is comparatively easy. However, since collecting the whole data of burst phenomena is very costly due to the vast amounts of available social media data, robustness against data reduction is an important index to evaluate classification methods. With the non-text-based method, over 55% of the pairs of tweets in the same cluster were also included in the same cluster even when the data were reduced to 10% in all three of our example cases. In this paper, as a source we focus on Twitter, one of the most popular microblogging services. Using clustering to conduct detailed case analyses, we scrutinized three burst cases that include natural disasters and flaming on the internet and found that a non-text-based method more effectively classified tweets in burst phenomena than a text-based method.",
        "link": "https://dl.acm.org/doi/10.1145/3106426.3106451"
    },
    {
        "title": "Using Spark for Text Mining on Large Scale Liver Cancer Literature",
        "authors": [
            "Ming-Yen Lin",
            "Yu-Ju Lin",
            "Sue-Chen Hsueh"
        ],
        "date": "January 2021",
        "source": "BDET 2021: 2021 the 3rd International Conference on Big Data Engineering and Technology (BDET)",
        "abstract": "Cancer is one of the main causes of death. The number of known cancer to-date is more than one hundred. Liver cancer, ranked after trachea and lung cancer, has long been high in the cancer leading cause in Taiwan and even ranked as second in 2016. The number of scientific articles related to cancer proliferates every year. The number reaches as high as 20 million in PubMed so that discovering useful information from the massive collection is very difficult. In addition, using a single machine to sift through these articles is very time-consuming. Therefore, we present a big data analytic framework using the distributed Apache Spark platform for text mining in PubMed literature. We establish a prediction model for liver cancer articles so that researchers may effectively validate whether an article is related to liver cancer or not. Classification models in Spark MLlib including Linear Support Vector Machines (SVM), Logistic Regression, are used in our experiments. Relevancy to liver cancer is further confirmed by using MeSH (Medical Subject Headings) terms. Logistic regression is about 3 times faster than SVMs and the accuracy of both methods is close to 95% in the experiments using hold-out validation. When max_features is 500 and min_df ≤ 0.1 (or min_df = 1), the accuracy may reach 96%. In the experiments with K-fold cross-validation, the accuracy of SVMs methods is 96%. The experimental results show that that our prediction model may effectively classify liver cancer articles.",
        "link": "https://dl.acm.org/doi/10.1145/3474944.3474958"
    },
    {
        "title": "Automatic topic classification of test cases using text mining at an Android smartphone vendor",
        "authors": [
            "Junji Shimagaki",
            "Yasutaka Kamei",
            "Naoyasu Ubayashi",
            "Abram Hindle"
        ],
        "date": "October 2018",
        "source": "ESEM '18: Proceedings of the 12th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Background: An Android smartphone is an ecosystem of applications, drivers, operating system components, and assets. The volume of the software is large and the number of test cases needed to cover the functionality of an Android system is substantial. Enormous effort has been already taken to properly quantify \"what features and apps were tested and verified?\". This insight is provided by dashboards that summarize test coverage and results per feature. One method to achieve this is to manually tag or label test cases with the topic or function they cover, much like function points. At the studied Android smartphone vendor, tests are labelled with manually defined tags, so-called \"feature labels (FLs)\", and the FLs serve to categorize 100s to 1000s test cases into 10 to 50 groups. Aim: Unfortunately for developers, manual assignment of FLs to 1000s of test cases is a time consuming task, leading to inaccurately labeled test cases, which will render the dashboard useless. We created an automated system that suggests tags/labels to the developers for their test cases rather than manual labeling. Method: We use machine learning models to predict and label the functionality tested by 10,000 test cases developed at the company. Results: Through the quantitative experiments, our models achieved acceptable F-1 performance of 0.3 to 0.88. Also through the qualitative studies with expert teams, we showed that the hierarchy and path of tests was a good predictor of a feature's label. Conclusions: We find that this method can reduce tedious manual effort that software developers spent classifying test cases, while providing more accurate classification results.",
        "link": "https://dl.acm.org/doi/10.1145/3239235.3268927"
    },
    {
        "title": "To apply Data Mining for Classification of Crowd sourced Software Requirements",
        "authors": [
            "Soonh Taj",
            "Qasim Arain",
            "Imran Memon",
            "Asma Zubedi"
        ],
        "date": "April 2019",
        "source": "ICSIE '19: Proceedings of the 8th International Conference on Software and Information Engineering",
        "abstract": "Now a day's main focus of developers is to build quality software that works according to customer needs and for this reason it is necessary to gather right requirements as requirement elicitation is the critical step that impacts on the success of software project as misinterpreted requirements leads to the failure of software project. By keeping this in mind a research is carried out on improving requirements elicitation process and automating the process of classifying requirements. In this research, a model is proposed which will help in this scenario for requirements elicitation and requirement classification. This paper presents a model in which crowd sourcing approach is used so that customers, end users, stakeholders, developers and software engineers can make active participation for requirement elicitation process and requirements gathered using crowdsourcing approach are used by model for classification process i.e. classification of requirements into functional and non-functional requirements. For the proof of proposed model a case study is conducted. Results of case study provided the usefulness and efficiency of proposed model for classification of crowd sourced software requirements.",
        "link": "https://dl.acm.org/doi/10.1145/3328833.3328837"
    },
    {
        "title": "Text classification using Fuzzy TF-IDF and Machine Learning Models",
        "authors": [
            "Mariem Bounabi",
            "Karim El Moutaouakil",
            "Khalid Satori"
        ],
        "date": "October 2019",
        "source": "BDIoT'19: Proceedings of the 4th International Conference on Big Data and Internet of Things",
        "abstract": "The representation of the information has an important impact on the text classification task. Several weighting methods were proposed in the literature, and the term frequency-inverse term frequency (TFIDF), the most know on the text treatment field. The FTF-IDF is a vector representation where the components of the TFIDF are presented as inputs to the Fuzzy Inference System (FIS). In this work, we compare several Machin Learning algorithms such as Naïve Bayes and its derivatives, SVM and Random forest classifiers, using the FTF-IDF representation. To improve the quality of the used classifiers, we call sum attribute selection methods. The recognition rate, for the tested systems, is satisfied, where the system based on naïve Bayes classifier, the FTF-IDF weighting terms, and the info gain select attributes method gives 98.7% as accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3372938.3372956"
    },
    {
        "title": "Text Mining in Cybersecurity: A Systematic Literature Review",
        "authors": [
            "Luciano Ignaczak",
            "Guilherme Goldschmidt",
            "Cristiano André Da Costa",
            "Rodrigo Da Rosa Righi"
        ],
        "date": null,
        "source": "ACM Computing Surveys",
        "abstract": "The growth of data volume has changed cybersecurity activities, demanding a higher level of automation. In this new cybersecurity landscape, text mining emerged as an alternative to improve the efficiency of the activities involving unstructured data. This article proposes a Systematic Literature Review (SLR) to present the application of text mining in the cybersecurity domain. Using a systematic protocol, we identified 2,196 studies, out of which 83 were summarized. As a contribution, we propose a taxonomy to demonstrate the different activities in the cybersecurity domain supported by text mining. We also detail the strategies evaluated in the application of text mining tasks and the use of neural networks to support activities involving unstructured data. The work also discusses text classification performance aiming its application in real-world solutions. The SLR also highlights open gaps for future research, such as the analysis of non-English content and the intensification in the usage of neural networks.",
        "link": "https://dl.acm.org/doi/10.1145/3462477"
    },
    {
        "title": "Using Text Mining to Analyze the Financial Patents",
        "authors": [
            "Yung-Feng Lu",
            "Jia-Lang Xu",
            "Mu-Yen Chen"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "With the advancement and development of science and technology, Nowadays society is the development of information technology, cloud computing, mobile Internet, Internet of Things, block chain and information technology.The combination of finance and technology has made financial technology terms gradually emerging.. This research analyzes the patents of financial technology. However, Taiwan's financial technology patents are mostly G06Q-20 (payment plan, architecture or agreement), G06Q-30 (commercial, such as marketing, shopping, payment, auction or e-commerce). G06Q-40 (financial, investment or tax treatment, insurance).The research use text mining technology to analyze and understand Taiwan's trends and developments in financial technology. The research results show that G06Q-20 patents are biased towards information security and mobile payment. G06Q-30 patents favor shopping websites and advertising. G06Q-40 patents are biased towards diversity, such as securities, insurance, finance, and stocks.",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326478"
    },
    {
        "title": "Identifying Online Profiles of Distance Learning Students Using Data Mining Techniques",
        "authors": [
            "Osama Islam",
            "Muazzam Siddiqui",
            "Naif Radi Aljohani"
        ],
        "date": "October 2019",
        "source": "ICDTE '19: Proceedings of the 3rd International Conference on Digital Technology in Education",
        "abstract": "Educational data has grown over the years with the increased use of technology within educational environments. This has led to a huge amount of data being stored in various data sources representing the student, his/her activities, and other aspects relevant to the learning process. To meet this analytical need, Educational Data Mining (EDM) has emerged to assist educational institutions in identifying key benefits such as students at risk, the level of student engagement or predicting student performance. The aim of this research was to explore the various aspects of student interaction data using data mining techniques to identify relevant patterns of behaviors and possible key attributes that have higher degrees of influence on distance learning students. The main findings identified several patterns of user online profiles based on a set of adopted learning strategies, the research proposed a framework for analyzing such interaction data based on R and Hadoop platforms to correlate online profiles with student performance.",
        "link": "https://dl.acm.org/doi/10.1145/3369199.3369249"
    },
    {
        "title": "Extremely Fast Decision Tree Mining for Evolving Data Streams",
        "authors": [
            "Albert Bifet",
            "Jiajin Zhang",
            "Wei Fan",
            "Cheng He",
            "Jianfeng Zhang",
            "Jianfeng Qian",
            "Geoff Holmes",
            "Bernhard Pfahringer"
        ],
        "date": "August 2017",
        "source": "KDD '17: Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
        "abstract": "Nowadays real-time industrial applications are generating a huge amount of data continuously every day. To process these large data streams, we need fast and efficient methodologies and systems. A useful feature desired for data scientists and analysts is to have easy to visualize and understand machine learning models. Decision trees are preferred in many real-time applications for this reason, and also, because combined in an ensemble, they are one of the most powerful methods in machine learning. In this paper, we present a new system called STREAMDM-C++, that implements decision trees for data streams in C++, and that has been used extensively at Huawei. Streaming decision trees adapt to changes on streams, a huge advantage since standard decision trees are built using a snapshot of data, and can not evolve over time. STREAMDM-C++ is easy to extend, and contains more powerful ensemble methods, and a more efficient and easy to use adaptive decision trees. We compare our new implementation with VFML, the current state of the art implementation in C, and show how our new system outperforms VFML in speed using less resources.",
        "link": "https://dl.acm.org/doi/10.1145/3097983.3098139"
    },
    {
        "title": "Data miners' little helper: data transformation activity cues for cluster analysis on document collections",
        "authors": [
            "Tania Cerquitelli",
            "Evelina Di Corso",
            "Francesco Ventura",
            "Silvia Chiusano"
        ],
        "date": "June 2017",
        "source": "WIMS '17: Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "In this paper we propose a new self-learning engine to streamline the analytics process, as it enables analysts to mine massive data repositories with minimal user intervention. In the context of cluster analysis on a collection of documents this new system, named SELF-DATA (SELF-learning DAta TrAnsformation), suggests to the analyst how to configure the whole mining process for a given dataset. SELF-DATA relies on an engine exploring different data weighting schemas (e.g., normalized term frequencies) and data transformation methods (e.g., PCA) before applying the cluster analysis, evaluating and comparing solutions through different quality indices (e.g., weighted Silhouette), and presenting the k-top solutions to the analyst. SELF-DATA will also include a knowledge base storing results of experiments on previously processed datasets, and a classification algorithm trained on the knowledge base content to forecast the best configuration for the whole mining process for an unexplored dataset. The first development of SELF-DATA running on Apache Spark has been validated on 5 collections of documents. Experimental results highlight that TF-IDF and logarithmic entropy are effective to measure item relevance with sparse datasets, and the LSI method outperforms PCA with a large dictionary.",
        "link": "https://dl.acm.org/doi/10.1145/3102254.3102288"
    },
    {
        "title": "Interactive Visual Graph Mining and Learning",
        "authors": [
            "Ryan A. Rossi",
            "Nesreen K. Ahmed",
            "Rong Zhou",
            "Hoda Eldardiry"
        ],
        "date": null,
        "source": "ACM Transactions on Intelligent Systems and Technology",
        "abstract": "This article presents a platform for interactive graph mining and relational machine learning called GraphVis. The platform combines interactive visual representations with state-of-the-art graph mining and relational machine learning techniques to aid in revealing important insights quickly as well as learning an appropriate and highly predictive model for a particular task (e.g., classification, link prediction, discovering the roles of nodes, and finding influential nodes). Visual representations and interaction techniques and tools are developed for simple, fast, and intuitive real-time interactive exploration, mining, and modeling of graph data. In particular, we propose techniques for interactive relational learning (e.g., node/link classification), interactive link prediction and weighting, role discovery and community detection, higher-order network analysis (via graphlets, network motifs), among others. GraphVis also allows for the refinement and tuning of graph mining and relational learning methods for specific application domains and constraints via an end-to-end interactive visual analytic pipeline that learns, infers, and provides rapid interactive visualization with immediate feedback at each change/prediction in real-time. Other key aspects include interactive filtering, querying, ranking, manipulating, exporting, as well as tools for dynamic network analysis and visualization, interactive graph generators (including new block model approaches), and a variety of multi-level network analysis techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3200764"
    },
    {
        "title": "Data Mining and Opinion Mining: A Tool in Educational Context",
        "authors": [
            "Myriam Peñafiel",
            "Stefanie Vásquez",
            "Diego Vásquez",
            "Juan Zaldumbide",
            "Sergio Luján-Mora"
        ],
        "date": "July 2018",
        "source": "ICoMS '18: Proceedings of the 2018 1st International Conference on Mathematics and Statistics",
        "abstract": "The use of the web as a universal communication platform generates large volumes of data (Big data), which in many cases, need to be processed so that they can become useful knowledge in face of the sceptics who have doubts about the credibility of such information. The use of web data that comes from educational contexts needs to be addressed, since that large amount of unstructured information is not being valued, losing valuable information that can be used. To solve this problem, we propose the use of data mining techniques such as sentiment analysis to validate the information that comes from the educational platforms. The objective of this research is to propose a methodology that allows the user to apply sentiment analysis in a simple way, because although some researchers have done it, very few do with data in the educational context. The results obtained prove that the proposal can be used in similar cases.",
        "link": "https://dl.acm.org/doi/10.1145/3274250.3274263"
    },
    {
        "title": "Research Hotspots Mining and Visualized Analysis Based on Linking Cluster and K-Core Decomposition",
        "authors": [
            "Ying Cai",
            "Fang Huang",
            "Shuai Wang",
            "Hao Zhang",
            "Chunxiu Du"
        ],
        "date": "May 2018",
        "source": "ICDPA 2018: Proceedings of the International Conference on Data Processing and Applications",
        "abstract": "In this paper, we focus on research hotspots mining with knowledge semantic features on discipline topic word networks, which are extracted from scientific and technical documents. For the topic networks, a novel method of combining linking community clustering with k-core decomposition is proposed to mine the hotspot community with knowledge hierarchy structure. And as a post-processing, the hierarchical community identification is implemented by k-core decomposition in order to discover the internal connection with knowledge semantic hierarchy in the research hotspot. Finally, the community density and visualization method is utilized to analyze the core and sub- problem in the research hotspot community. In experiments, the topic networks for a single discipline and interdisciplinary research hotspot are taken as examples of analysis, and the results show the proposed approach can effectively realize the objective of mining and analyzing the discipline research hotspot on the academic topic word network.",
        "link": "https://dl.acm.org/doi/10.1145/3224207.3224216"
    },
    {
        "title": "The Application of Data Mining Algorithms in the Construction of Travel Recommendation System",
        "authors": [
            "Chao Lou",
            "Zhaonan Mu",
            "Mengzhu Liu"
        ],
        "date": "January 2021",
        "source": "CONF-CDS 2021: The 2nd International Conference on Computing and Data Science",
        "abstract": "Photos shared by users of online social network often associated with tags, texts, geographic and time information. These data are ideal research resource which can be used by researchers to observe people's interests and behaviors. They can also be used to analyze people's travel behaviors and interests because it can reflect people's travel activities and experiences directly. This project aimed to consolidate people into groups based on their travel preferences of travel season, travel time span, scenery type of travel location and then analyze popular travel places for different people groups. The travel preferences were analyzed from extracted data applying PHP language and Naive Bayes classification approach. According to the analyzed data, people were divided into groups. Then, DBSCAN clustering was used to get popular locations based on the different groups. As a result, 80 thousand photo data of 5 cities have been collected from Flickr by Phython. It has been showed that people's preference of season in most of the mined cities was spring and summer and the preference of time span was short. Moreover, mined people has been divided into 16 groups according to their preference and based on the groups popular locations have been calculated by DBSCAN clustering. Theses locations have been provided in a recommendation website which was built in this project. For the future work, mining Flickr data associated with other travel social network sites could be a good way to build a more effective recommendation system.",
        "link": "https://dl.acm.org/doi/10.1145/3448734.3450477"
    },
    {
        "title": "Compare Machine Learning Models in Text Classification Using Steam User Reviews",
        "authors": [
            "Youchen Miao",
            "Zeyu Jin",
            "Yumeng Zhang",
            "Yuchen Chen",
            "Junren Lai"
        ],
        "date": "November 2021",
        "source": "ICSED '21: Proceedings of the 2021 3rd International Conference on Software Engineering and Development",
        "abstract": "Text Classification and Sentiment Analysis of game reviews are viewed as important parts in not only academic fields but also in game studies. In this paper, with more than 400 thousand game reviews on Steam platform, we preprocess the data using different libraries (sklearn, nltk, and spaCy) and use them as inputs to build three sentiment classification models based on different algorithms (Naive Bayes, SVM, and Random Forest). In contrast to previous studies that only focus on different sentiment analysis models, our paper also highlights the use of different APIs to preprocess the data and their corresponding model performance. The results show that no matter which API we choose, Random Forest models always perform the best. However, in terms of training time, Naive Bayes is the fastest. This work can be used to apply grid search for researchers to automatically find the optimum API before conducting sentiment analysis in the future.",
        "link": "https://dl.acm.org/doi/10.1145/3507473.3507480"
    },
    {
        "title": "Lost in Transduction: Transductive Transfer Learning in Text Classification",
        "authors": [
            "Alejandro Moreo",
            "Andrea Esuli",
            "Fabrizio Sebastiani"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Obtaining high-quality labelled data for training a classifier in a new application domain is often costly. Transfer Learning\n(a.k.a. “Inductive Transfer”) tries to alleviate these costs by transferring, to the “target” domain of interest, knowledge available from a different “source” domain. In transfer learning the lack of labelled information from the target domain is compensated by the availability at training time of a set of unlabelled examples from the target distribution. Transductive Transfer Learning denotes the transfer learning setting in which the only set of target documents that we are interested in classifying is known and available at training time. Although this definition is indeed in line with Vapnik’s original definition of “transduction”, current terminology in the field is confused. In this article, we discuss how the term “transduction” has been misused in the transfer learning literature, and propose a clarification consistent with the original characterization of this term given by Vapnik. We go on to observe that the above terminology misuse has brought about misleading experimental comparisons, with inductive transfer learning methods that have been incorrectly compared with transductive transfer learning methods. We then, give empirical evidence that the difference in performance between the inductive version and the transductive version of a transfer learning method can indeed be statistically significant (i.e., that knowing at training time the only data one needs to classify indeed gives an advantage). Our clarification allows a reassessment of the field, and of the relative merits of the major, state-of-the-art algorithms for transfer learning in text classification.",
        "link": "https://dl.acm.org/doi/10.1145/3453146"
    },
    {
        "title": "Machine Learning on Mining Potential Adverse Drug Reactions for Pharmacovigilance",
        "authors": [
            "Ke Jia"
        ],
        "date": "July 2021",
        "source": "DSIT 2021: 2021 4th International Conference on Data Science and Information Technology",
        "abstract": "Drugs are related to the lives and health of patients. Mining and discovering potential adverse drug reactions from a large number of unstructured texts play an important role in drug research and pharmacovigilance. It is a hot research issue in the field of biomedicine in recent years. A large number of scholars have discovered a large amount of information on adverse drug reactions from different data sources through effective machine learning methods. This paper reviews the research progress in recent years to provide a reference for future research.",
        "link": "https://dl.acm.org/doi/10.1145/3478905.3478964"
    },
    {
        "title": "TACAM: Topic And Context Aware Argument Mining",
        "authors": [
            "Michael Fromm",
            "Evgeniy Faerman",
            "Thomas Seidl"
        ],
        "date": "October 2019",
        "source": "WI '19: IEEE/WIC/ACM International Conference on Web Intelligence",
        "abstract": "In this work we address the problem of argument search. The purpose of argument search is the distillation of pro and contra arguments for requested topics from large text corpora. In previous works, the usual approach is to use a standard search engine to extract text parts which are relevant to the given topic and subsequently use an argument recognition algorithm to select arguments from them. The main challenge in the argument recognition task, which is also known as argument mining, is that often sentences containing arguments are structurally similar to purely informative sentences without any stance about the topic. In fact, they only differ semantically. Most approaches use topic or search term information only for the first search step and therefore assume that arguments can be classified independently of a topic. We argue that topic information is crucial for argument mining, since the topic defines the semantic context of an argument. Precisely, we propose different models for the classification of arguments, which take information about a topic of an argument into account. Moreover, to enrich the context of a topic and to let models understand the context of the potential argument better, we integrate information from different external sources such as Knowledge Graphs or pre-trained NLP models. Our evaluation shows that considering topic information, especially in connection with external information, provides a significant performance boost for the argument mining task.",
        "link": "https://dl.acm.org/doi/10.1145/3350546.3352506"
    },
    {
        "title": "Text mining for incoming tasks based on the urgency/importance factors and task classification using machine learning tools",
        "authors": [
            "Yasser Ali Alshehri"
        ],
        "date": "March 2020",
        "source": "ICCDA 2020: Proceedings of the 2020 the 4th International Conference on Compute and Data Analysis",
        "abstract": "In workplaces, there is a massive amount of unstructured data from different sources. In this paper, we present a case study that explains how can through communications between employees, we can help to prioritize tasks requests to increase the efficiency of their works for both technical and non-technical workers. This involves managing daily incoming tasks based on their level of urgency and importance.To allow all workers to utilize the urgency-importance matrix as a time-management tool, we need to automate this tool. The textual content of incoming tasks are analyzed, and metrics related to urgency and importance are extracted. A third factor (i.e., the response variable) is defined based on the two input variables (urgency and importance). Then, machine learning applied to the data to predict the class of incoming tasks based on data outcome desired. We used ordinal regression, neural networks, and decision tree algorithms to predict the four levels of task priority. We measure the performance of all using recalls, precisions, and F-scores. All classifiers perform higher than 89% in terms of all measures.",
        "link": "https://dl.acm.org/doi/10.1145/3388142.3388153"
    },
    {
        "title": "Text mining for malware classification using multivariate all repeated patterns detection",
        "authors": [
            "Konstantinos F. Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2019",
        "source": "ASONAM '19: Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Mobile phones have become nowadays a commodity to the majority of people. Using them, people are able to access the world of Internet and connect with their friends, their colleagues at work or even unknown people with common interests. This proliferation of the mobile devices has also been seen as an opportunity for the cyber criminals to deceive smartphone users and steel their money directly or indirectly, respectively, by accessing their bank accounts through the smartphones or by blackmailing them or selling their private data such as photos, credit card data, etc. to third parties. This is usually achieved by installing malware to smartphones masking their malevolent payload as a legitimate application and advertise it to the users with the hope that mobile users will install it in their devices. Thus, any existing application can easily be modified by integrating a malware and then presented it as a legitimate one. In response to this, scientists have proposed a number of malware detection and classification methods using a variety of techniques. Even though, several of them achieve relatively high precision in malware classification, there is still space for improvement. In this paper, we propose a text mining all repeated pattern detection method which uses the decompiled files of an application in order to classify a suspicious application into one of the known malware families. Based on the experimental results using a real malware dataset, the methodology tries to correctly classify (without any misclassification) all randomly selected malware applications of 3 categories with 3 different families each.",
        "link": "https://dl.acm.org/doi/10.1145/3341161.3350841"
    },
    {
        "title": "Comparative Study between Traditional Machine Learning and Deep Learning Approaches for Text Classification",
        "authors": [
            "Cannannore Nidhi Kamath",
            "Syed Saqib Bukhari",
            "Andreas Dengel"
        ],
        "date": "August 2018",
        "source": "DocEng '18: Proceedings of the ACM Symposium on Document Engineering 2018",
        "abstract": "In this contemporaneous world, it is an obligation for any organization working with documents to end up with the insipid task of classifying truckload of documents, which is the nascent stage of venturing into the realm of information retrieval and data mining. But classification of such humongous documents into multiple classes, calls for a lot of time and labor. Hence a system which could classify these documents with acceptable accuracy would be of an unfathomable help in document engineering. We have created multiple classifiers for document classification and compared their accuracy on raw and processed data. We have garnered data used in a corporate organization as well as publicly available data for comparison. Data is processed by removing the stop-words and stemming is implemented to produce root words. Multiple traditional machine learning techniques like Naive Bayes, Logistic Regression, Support Vector Machine, Random forest Classifier and Multi-Layer Perceptron are used for classification of documents. Classifiers are applied on raw and processed data separately and their accuracy is noted. Along with this, Deep learning technique such as Convolution Neural Network is also used to classify the data and its accuracy is compared with that of traditional machine learning techniques. We are also exploring hierarchical classifiers for classification of classes and subclasses. The system classifies the data faster and with better accuracy than if done manually. The results are discussed in the results and evaluation section.",
        "link": "https://dl.acm.org/doi/10.1145/3209280.3209526"
    },
    {
        "title": "Identification of Overpricing in the Purchase of Medication by the Federal Government of Brazil, Using Text Mining and Clustering Based on Ontology",
        "authors": [
            "Marco Aurelio O. S. Correa",
            "Adriano Galindo Leal"
        ],
        "date": "August 2018",
        "source": "ICCBDC '18: Proceedings of the 2018 2nd International Conference on Cloud and Big Data Computing",
        "abstract": "Increasing the transparency level in his actions and spending is one of the primary duties of the Brazilian Federal Government. The creation of laws that oblige full disclose of all its expenditures through transparency portals enables citizens to supervise all government entities. However, only the dissemination of these data, without a definite standard or the availability of data analysis tools, does not guarantee that the citizen is empowered to play his role. Therefore hence, the objective of this work is to identify overprice in the acquisition of products purchased by the federal government of Brazil using the unstructured data available on the Transparency Portal. The last two-years' worth of purchasing data, available in the Transparency Portal, were extracted, processed and stored. Due to his diverse nature and high volume of data, this study focused only on medicines purchased by the Ministry of Health. Ontology-based text mining and clustering techniques were applied for automatic identification and classification of products. The processing of this information was done through text mining and clustering, based on the ontology registered in another database of the Brazilian government. Because of this work, a consolidated price base per medication was created to allow the identification of distortions in prices practised, facilitating the identification of cases that merit further investigation to unravel fraud to the treasury.",
        "link": "https://dl.acm.org/doi/10.1145/3264560.3264569"
    },
    {
        "title": "Computational Estimation by Scientific Data Mining with Classical Methods to Automate Learning Strategies of Scientists",
        "authors": [
            "Aparna S. Varde"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Experimental results are often plotted as 2-dimensional graphical plots (aka graphs) in scientific domains depicting dependent versus independent variables to aid visual analysis of processes. Repeatedly performing laboratory experiments consumes significant time and resources, motivating the need for computational estimation. The goals are to estimate the graph obtained in an experiment given its input conditions, and to estimate the conditions that would lead to a desired graph. Existing estimation approaches often do not meet accuracy and efficiency needs of targeted applications. We develop a computational estimation approach called AutoDomainMine that integrates clustering and classification over complex scientific data in a framework so as to automate classical learning methods of scientists. Knowledge discovered thereby from a database of existing experiments serves as the basis for estimation. Challenges include preserving domain semantics in clustering, finding matching strategies in classification, striking a good balance between elaboration and conciseness while displaying estimation results based on needs of targeted users, and deriving objective measures to capture subjective user interests. These and other challenges are addressed in this work. The AutoDomainMine approach is used to build a computational estimation system, rigorously evaluated with real data in Materials Science. Our evaluation confirms that AutoDomainMine provides desired accuracy and efficiency in computational estimation. It is extendable to other science and engineering domains as proved by adaptation of its sub-processes within fields such as Bioinformatics and Nanotechnology.",
        "link": "https://dl.acm.org/doi/10.1145/3502736"
    },
    {
        "title": "Where is the road for issue reports classification based on text mining?",
        "authors": [
            "Qiang Fan",
            "Yue Yu",
            "Gang Yin",
            "Tao Wang",
            "Huaimin Wang"
        ],
        "date": "November 2017",
        "source": "ESEM '17: Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Currently, open source projects receive various kinds of issues daily, because of the extreme openness of Issue Tracking System (ITS) in GitHub. ITS is a labor-intensive and time-consuming task of issue categorization for project managers. However, a contributor is only required a short textual abstract to report an issue in GitHub. Thus, most traditional classification approaches based on detailed and structured data (e.g., priority, severity, software version and so on) are difficult to adopt. In this paper, issue classification approaches on a large-scale dataset, including 80 popular projects and over 252,000 issue reports collected from GitHub, were investigated. First, four traditional text-based classification methods and their performances were discussed. Semantic perplexity (i.e., an issues description confuses bug-related sentences with nonbug-related sentences) is a crucial factor that affects the classification performances based on quantitative and qualitative study. Finally, A two-stage classifier framework based on the novel metrics of semantic perplexity of issue reports was designed. Results show that our two-stage classification can significantly improve issue classification performances.",
        "link": "https://dl.acm.org/doi/10.1109/ESEM.2017.19"
    },
    {
        "title": "Feature-based Facebook reviews process model for e-management using data mining",
        "authors": [
            "Anish Kumar Varudharajulu",
            "Yongsheng Ma"
        ],
        "date": "January 2019",
        "source": "IC4E '19: Proceedings of the 10th International Conference on E-Education, E-Business, E-Management and E-Learning",
        "abstract": "The data generated from online communication acts as potential gold mines for discovering knowledge for researchers. A large amount of data is also generated in the form of web documents, emails, blogs, and feedback, etc. Text analytics is being significantly employed to mine important information. Opinion mining is the process of extracting human thoughts and perceptions from unstructured texts. The showstopper for designing an opinion mining system for analyzing reviews arise from the fact that customer reviews are often noisy. These reviews are informally written. In addition, they are subjected to spelling mistakes, grammatical errors, improper punctuation and irrational capitalization. This paper focuses on analyzing the different classification and clustering algorithms aimed at extracting and consolidating opinions of customers from social media sites like Facebook, Twitter and through surveys, at multiple levels of granularity to monitor and measure customer satisfaction. Ours is an automated approach, in which the system aids in the process of knowledge assimilation for knowledge-based building and also performs the analytics. Domain experts ratify the knowledge base and also provide training datasets for the system to intuitively gather more instances for ratification. The system identifies opinion expressions as phrases containing opinion words, opinionated features and also opinion modifiers. These expressions are categorized as positive, negative or neutral. Opinion expressions are identified and categorized using localized linguistic techniques. Opinions can be congregated at any desired level of specificity i.e. feature level or product level, user level or service level, etc. We have developed a system based on this approach, which provides the user with a platform to analyze opinion expressions crawled from a set of pre-defined datasets.",
        "link": "https://dl.acm.org/doi/10.1145/3306500.3306514"
    },
    {
        "title": "Discriminative Topic Mining via Category-Name Guided Text Embedding",
        "authors": [
            "Yu Meng",
            "Jiaxin Huang",
            "Guangyuan Wang",
            "Zihan Wang",
            "Chao Zhang",
            "Yu Zhang",
            "Jiawei Han"
        ],
        "date": "April 2020",
        "source": "WWW '20: Proceedings of The Web Conference 2020",
        "abstract": "Mining a set of meaningful and distinctive topics automatically from massive text corpora has broad applications. Existing topic models, however, typically work in a purely unsupervised way, which often generate topics that do not fit users’ particular needs and yield suboptimal performance on downstream tasks. We propose a new task, discriminative topic mining, which leverages a set of user-provided category names to mine discriminative topics from text corpora. This new task not only helps a user understand clearly and distinctively the topics he/she is most interested in, but also benefits directly keyword-driven classification tasks. We develop CatE, a novel category-name guided text embedding method for discriminative topic mining, which effectively leverages minimal user guidance to learn a discriminative embedding space and discover category representative terms in an iterative manner. We conduct a comprehensive set of experiments to show that CatE mines high-quality set of topics guided by category names only, and benefits a variety of downstream applications including weakly-supervised classification and lexical entailment direction identification.",
        "link": "https://dl.acm.org/doi/10.1145/3366423.3380278"
    },
    {
        "title": "Comparative Study of Heart Disease Diagnosis Using Top Ten Data Mining Classification Algorithms",
        "authors": [
            "I. Ketut Agung Enriko"
        ],
        "date": "June 2019",
        "source": "ICFET '19: Proceedings of the 5th International Conference on Frontiers of Educational Technologies",
        "abstract": "Data mining has been used for many purposes, especially for prediction system. In healthcare, data mining algorithms often used in disease diagnosis. Meanwhile, heart disease is known as a primary cause of death over the years. Many studies have been performed in heart disease diagnosis using data mining methods. There are some popular data mining algorithms that can be used in heart disease diagnosis, for example, k-Nearest Neighbor, CART, and AdaBoost. The algorithms are used to analyze a sample of cardiovascular patients data and predict the heart disease type that they suffer. Some parameters are taken from the patient, including EKG morphology, blood pressure, and information about the existence of chest pain, shortness of breath, palpitation, and cold sweat. In this study, medical records data are collected from Harapan Kita Hospital and utilized as a dataset sample in this research. Top ten data mining classification algorithms are used in diagnosing heart disease from Harapan Kita Hospital data and examining their performance by checking the accuracy and speed.",
        "link": "https://dl.acm.org/doi/10.1145/3338188.3338220"
    },
    {
        "title": "Text Mining Approach for Identifying Research Trends",
        "authors": [
            "Snezhana Sulova"
        ],
        "date": "June 2021",
        "source": "CompSysTech '21: Proceedings of the 22nd International Conference on Computer Systems and Technologies",
        "abstract": "With the increase of unstructured data, the issues connected with automatic text processing, the categorization of documents and the discovery of topics have become objects of growing interest. In order to improve the process of grouping and processing research publications, we would like to propose a method based upon natural language processing. It is based on text mining technologies which aim to identify key tendencies in documents. It processes the content of publications by clustering and identifies the topics of each identified group. This analysis helps by identifying key tendencies as well as discovering emerging new areas of research. Publications from the research literature database, Scopus, were used to test the approach. The topic of the publications is “the application of digital technologies in the logistics business”. The experiments were completed using the RapidMiner Studio software.",
        "link": "https://dl.acm.org/doi/10.1145/3472410.3472433"
    },
    {
        "title": "Assessment of Vulnerability Severity using Text Mining",
        "authors": [
            "Georgios Spanos",
            "Lefteris Angelis",
            "Dimitrios Toloudis"
        ],
        "date": "September 2017",
        "source": "PCI '17: Proceedings of the 21st Pan-Hellenic Conference on Informatics",
        "abstract": "Software1 vulnerabilities are closely associated with information systems security, a major and critical field in today's technology. Vulnerabilities constitute a constant and increasing threat for various aspects of everyday life, especially for safety and economy, since the social impact from the problems that they cause is complicated and often unpredictable. Although there is an entire research branch in software engineering that deals with the identification and elimination of vulnerabilities, the growing complexity of software products and the variability of software production procedures are factors contributing to the ongoing occurrence of vulnerabilities, Hence, another area that is being developed in parallel focuses on the study and management of the vulnerabilities that have already been reported and registered in databases. The information contained in such databases includes, a textual description and a number of metrics related to vulnerabilities. The purpose of this paper is to investigate to what extend the assessment of the vulnerability severity can be inferred directly from the corresponding textual description, or in other words, to examine the informative power of the description with respect to the vulnerability severity. For this purpose, text mining techniques, i.e. text analysis and three different classification methods (decision trees, neural networks and support vector machines) were employed. The application of text mining to a sample of 70,678 vulnerabilities from a public data source shows that the description itself is a reliable and highly accurate source of information for vulnerability prioritization.",
        "link": "https://dl.acm.org/doi/10.1145/3139367.3139390"
    },
    {
        "title": "Clustering Algorithms for Spatial Data Mining",
        "authors": [
            "Chetashri Bhadane",
            "Ketan Shah"
        ],
        "date": "April 2020",
        "source": "ICGDA '20: Proceedings of the 2020 3rd International Conference on Geoinformatics and Data Analysis",
        "abstract": "With the advances in mobile and wireless technologies, there has been a rise in applications that track and share the users' geospatial data. People use several social networking sites such as Twitter, Facebook and Flickr, where they share their status updates. With the integration of Global Positioning System (GPS) with mobile phones, it is now possible to share one's locations on these social networks. GPS allows us to record and track a person's movement along with the timestamp. The data set obtained from these GPS logs is vast and is widely used to analyze the users' movement patterns. Specifically, we can find out significant locations based on the number of users present at that location and the time spent by them at such places. Once significant places have been identified, it is also possible to identify the semantic importance of these locations. This paper presents an overview of the clustering techniques used to find important places of interest using large GPS based mobility datasets. Four clustering algorithms, K-Means, DBSCAN, OPTICS and Hierarchical, are implemented, and performance is tested using real-time data of 50 users collected over 2--5 years. Performance summary depicts that K-Means and DBSCAN perform well for spatial data.",
        "link": "https://dl.acm.org/doi/10.1145/3397056.3397068"
    },
    {
        "title": "Feature Selection based Arabic Text Classification using Different Machine Learning Algorithms: Comparative Study",
        "authors": [
            "Sakina Rim Bennabi",
            "Zakaria Elberrichi"
        ],
        "date": "June 2020",
        "source": "ICIST '20: Proceedings of the 10th International Conference on Information Systems and Technologies",
        "abstract": "Feature selection is a method of data pre-processing widely used when mining large data, such as textual classification. Several studies have been conducted to compare the different methods of feature selection applied to corpora in English. Unfortunately, a small number of works concern the Arabic language. This article aims to present a comparative study of different feature selection techniques including: Chi2, the ANOVA method and mutual information, applied on a corpus in Arabic language, while also diversifying the machine learning algorithms (Naive Bayes, SVM and KNN). This experimental study has shown in general that reducing dimensionality with feature selection techniques has slightly affected the performance of textual classification, reducing the size of the corpus by up to 1%.",
        "link": "https://dl.acm.org/doi/10.1145/3447568.3448531"
    },
    {
        "title": "Intelligent Data Analysis using Optimized Support Vector Machine Based Data Mining Approach for Tourism Industry",
        "authors": [
            "Ms Promila Sharma",
            "Uma Meena",
            "Girish Kumar Sharma"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Data analysis involves the deployment of sophisticated approaches from data mining methods, information theory, and artificial intelligence in various fields like tourism, hospitality, and so on for the extraction of knowledge from the gathered and preprocessed data. In tourism, pattern analysis or data analysis using classification is significant for finding the patterns that represent new and potentially useful information or knowledge about the destination and other data. Several data mining techniques are introduced for the classification of data or patterns. However, overfitting, less accuracy, local minima, sensitive to noise are the drawbacks in some existing data mining classification methods. To overcome these challenges, Support vector machine with Red deer optimization (SVM-RDO) based data mining strategy is proposed in this article. Extended Kalman filter (EKF) is utilized in the first phase, i.e., data cleaning to remove the noise and missing values from the input data. Mantaray foraging algorithm (MaFA) is used in the data selection phase, in which the significant data are selected for the further process to reduce the computational complexity. The final phase is the classification, in which SVM-RDO is proposed to access the useful pattern from the selected data. PYTHON is the implementation tool used for the experiment of the proposed model. The experimental analysis is done to show the efficacy of the proposed work. From the experimental results, the proposed SVM-RDO achieved better accuracy, precision, recall, and F1 score than the existing methods for the tourism dataset. Thus, it is showed the effectiveness of the proposed SVM-RDO for pattern analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3494566"
    },
    {
        "title": "Applying text mining to predict learners' cognitive engagement",
        "authors": [
            "Hayati Hind",
            "Mohammed Khalidi Idrissi",
            "Samir Bennani"
        ],
        "date": "October 2017",
        "source": "SCAMS '17: Proceedings of the Mediterranean Symposium on Smart City Application",
        "abstract": "In academic process, engagement represent one of the important success' factor. This paper proposes a predictive system for learners' cognitive engagement based on their online discussion forums participation. In the first level the ontology OWL and the LSA method are used to perform a semantic classification system of the threads according to a specific context chosen by the tutor. Then the Text mining, as a predictive method, is applied to the classified threads and learners' participation in the forums.",
        "link": "https://dl.acm.org/doi/10.1145/3175628.3175655"
    },
    {
        "title": "Investigate Transitions into Drug Addiction through Text Mining of Reddit Data",
        "authors": [
            "John Lu",
            "Sumati Sridhar",
            "Ritika Pandey",
            "Mohammad Al Hasan",
            "Georege Mohler"
        ],
        "date": "July 2019",
        "source": "KDD '19: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Increasing rates of opioid drug abuse and heightened prevalence of online support communities underscore the necessity of employing data mining techniques to better understand drug addiction using these rapidly developing online resources. In this work, we obtained data from Reddit, an online collection of forums, to gather insight into drug use/misuse using text snippets from users narratives. Specifically, using users' posts, we trained a binary classifier which predicts a user's transitions from casual drug discussion forums to drug recovery forums. We also proposed a Cox regression model that outputs likelihoods of such transitions. In doing so, we found that utterances of select drugs and certain linguistic features contained in one's posts can help predict these transitions. Using unfiltered drug-related posts, our research delineates drugs that are associated with higher rates of transitions from recreational drug discussion to support/recovery discussion, offers insight into modern drug culture, and provides tools with potential applications in combating the opioid crisis.",
        "link": "https://dl.acm.org/doi/10.1145/3292500.3330737"
    },
    {
        "title": "Analysing digital banking reviews using text mining",
        "authors": [
            "Li Chen Cheng",
            "Legaspi Rhea Sharmayne"
        ],
        "date": "December 2020",
        "source": "ASONAM '20: Proceedings of the 12th IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Digital banks are new entrants in the banking industry in the Philippines as they only started late 2018. Since then, a handful of players have and are still emerging. With more and more people becoming technologically savvy, it is very critical for financial institutions to develop a digital banking application that will stand out from the competition. This paper aims to use text mining methods to analyse digital banking application reviews. This study will perform topic modelling using LDA to explore customer concerns and will mine association rules between the digital banking features with the review score. The results will reveal which areas the digital banking application can further optimize for customer satisfaction and retention.",
        "link": "https://dl.acm.org/doi/10.1109/ASONAM49781.2020.9381429"
    },
    {
        "title": "Mining Moocs Videos Metadata Using Classification Techniques",
        "authors": [
            "El Harrak Othman",
            "Ghadi Abderrahim",
            "El Bouhdidi Jaber"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "In few years, the internet has become the first source of information for most people, Today MOOCs makes it possible for everyone to access to the education over the world, it's represents an emerging methodology of online teaching and an important development in open education. Due to the rapid development of Moocs and the rapid growth of digital data and video database over the Internet, it is becoming very difficult for learners to browsing and choosing the best training for them. The scientific community has increased the amount of research into new technologies, with a view to improve the digital video utilization: its archiving, indexing, accessibility, acquisition, store and even its process and usability. All these parts of the video utilization entail the necessity of the extraction of all important information of a video, especially in cases of lack of metadata information. Web video mining is retrieving the content using data mining techniques from World Wide Web. There are two approaches for web video mining using traditional image processing (signal processing) and metadata based approach. In this paper, we present the various research makes in this subject and we propose an effective methodology to extract the metadata from Moocs videos and classify them based on the extracted metadata by applying data mining techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090450"
    },
    {
        "title": "Text mining with HathiTrust",
        "authors": [
            "Eleanor Dickson Koehl",
            "Ryan Dubnicek"
        ],
        "date": "June 2019",
        "source": "JCDL '19: Proceedings of the 18th Joint Conference on Digital Libraries",
        "abstract": "This tutorial will introduce attendees to the HathiTrust Research Center and its tools and services for computational text analysis research. HTRC leverages the scope and scale of the HathiTrust Digital Library collection to create opportunities for researchers to perform text data mining on subsets of the corpus. Attendees will develop skills that will allow them to conduct basic text analysis research using HathiTrust data, learn techniques for engaging with scholars on their text data mining research, and explore strategies for computational access to large-scale digital collections.",
        "link": "https://dl.acm.org/doi/10.1109/JCDL.2019.00115"
    },
    {
        "title": "Text mining for plagiarism detection: multivariate pattern detection for recognition of text similarities",
        "authors": [
            "Konstantinos Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2018",
        "source": "ASONAM '18: Proceedings of the 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "The problem of plagiarism the recent years has been intensified by the availability of information in digital form and the accessibility of the electronic libraries through the Internet. As a result, plagiarism detection has been transformed into a big data analytics problem since the number of digital sources is extravagant and a new document needs to be compared with millions of other existing documents. In this paper, a text mining methodology is proposed that can detect all common patterns between a document and the documents in a reference database. The technique is based on a pattern detection algorithm and the corresponding data structure that enables the algorithm to detect all common patterns. The methodology has been applied in a well-defined dataset providing very promising results identifying difficult cases of plagiarism such as technical disguise.",
        "link": "https://dl.acm.org/doi/10.5555/3382225.3382424"
    },
    {
        "title": "Petroleum Engineering Data Text Classification Using Convolutional Neural Network Based Classifier",
        "authors": [
            "Tzuhan Hsu",
            "Yaoqin Zhang"
        ],
        "date": "May 2018",
        "source": "ICMLT '18: Proceedings of the 2018 International Conference on Machine Learning Technologies",
        "abstract": "In this paper, authors introduce a convolutional neural network (CNN) based classification approach for petroleum engineering event. Recently, text classification problem has raised substantial attention in the field of machine learning. The aim of text classification process is to assign correct label to a text on the basis of its content. However, due to the imprecise of feature representation, most of former text classification methods cannot export satisfying result. To solve this problem, authors propose a model based on convolutional neural network. Authors conduct experiments with real world petroleum engineering data text, artificially labelled into 6 categories. Beside purposed CNN-based model, the authors also implement other baseline machine learning approaches: k nearest neighbor, random forest, support vector machines, recurrent neural network, gated recurrent units, long short-term memory. Among all these models, our CNN-based model reach best performance in text classification task. In the last part of this paper, authors compare and analysis their purpose and baseline model.",
        "link": "https://dl.acm.org/doi/10.1145/3231884.3231898"
    },
    {
        "title": "Classifying Indonesian Online Articles as Advertisement Placement Base Using Text Mining",
        "authors": [
            "Nadhira Tasya",
            "Arian Dhini"
        ],
        "date": "July 2017",
        "source": "ICBIM 2017: Proceedings of the International Conference on Business and Information Management",
        "abstract": "Rapid development in technological aspect resulting in growing level of human needs for the latest news, so that emerged a new trend of publishing and accessing news through online media or usually called online journalism. In addition, the number of people who sell and purchase through online sites also continues to increase and this opportunity is utilized by the company and the advertiser by implementing targeted web advertising. However, the high number of articles that have been published and accessed leads to great opportunities for errors in determining where to place the ads. Therefore, it needs a system that can categorize articles accessed by users as the basis of advertisement placement by the company and this classification system can be done by applying the method of Data Mining and Text Mining. This research uses document data in the form of article content that will be categorized into twenty categories of class of advertisement by using Text Mining technique with Support Vector Machine algorithm. The results of this study may be used by companies or advertisers as a basis for placement of ads on selected online media sites.",
        "link": "https://dl.acm.org/doi/10.1145/3134271.3134288"
    },
    {
        "title": "Mining Free-Text Medical Notes for Suicide Risk Assessment",
        "authors": [
            "Marios Adamou",
            "Grigoris Antoniou",
            "Elissavet Greasidou",
            "Vincenzo Lagani",
            "Paulos Charonyktakis",
            "Ioannis Tsamardinos"
        ],
        "date": "July 2018",
        "source": "SETN '18: Proceedings of the 10th Hellenic Conference on Artificial Intelligence",
        "abstract": "Suicide has been considered as an important public health issue for a very long time, and is one of the main causes of death worldwide. Despite suicide prevention strategies being applied, the rate of suicide has not changed substantially over the past decades. Advances in machine learning make it possible to attempt to predict suicide based on the analysis of relevant data to inform clinical practice. This paper reports on findings from the analysis of data of patients who died by suicide in the period 2013-2016 and made use of both structured data and free-text medical notes. We focus on examining various text-mining approaches to support risk assessment. The results show that using advance machine learning and text-mining techniques, it is possible to predict within a specified period which people are most at risk of taking their own life at the time of referral to a mental health service.",
        "link": "https://dl.acm.org/doi/10.1145/3200947.3201020"
    },
    {
        "title": "Integration of data mining techniques in e-learning systems: Clustering Profil of Lerners and Recommender Course System",
        "authors": [
            "A. El Moustamid",
            "E. En-Naimi",
            "J. El Bouhdidi"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "Data mining aims at the extraction of knowledge from large quantities of data by automatic or semi-automatic methods. In the field of education and training e-learning, Data Mining techniques as clustering (The aim of this technique is to identify groups that are similar in some aspect) [21], prediction (is used for developing model where combination of data viewed from different aspects can effect single viewed data. Here, is important to watch how predictors variables are effecting predicted variable)[21], classification are applied in e-learning to improve the level of learners by providing learning according to the learners' requirement) are used to analyze learners profile, to predict and improve student performance. The aim of our project is to develop a system capable of analyzing learners profile and indexing web videos in order to offer learners a rich database with courses that correspond to their levels.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090453"
    },
    {
        "title": "Multi-level mining and visualization of scientific text collections: Exploring a bi-lingual scientific repository",
        "authors": [
            "Pablo Accuosto",
            "Francesco Ronzano",
            "Daniel Ferrés",
            "Horacio Saggion"
        ],
        "date": "December 2017",
        "source": "WOSP 2017: Proceedings of the 6th International Workshop on Mining Scientific Publications",
        "abstract": "We present a system to mine and visualize collections of scientific documents by semantically browsing information extracted from single publications or aggregated throughout corpora of articles. The text mining tool performs deep analysis of document collections allowing the extraction and interpretation of research paper's contents. In addition to the extraction and enrichment of documents with metadata (titles, authors, affiliations, etc), the deep analysis performed comprises semantic interpretation, rhetorical analysis of sentences, triple-based information extraction, and text summarization. The visualization components allow geographical-based exploration of collections, topic-evolution interpretation, and collaborative network analysis among others. The paper presents a case study of a bi-lingual collection in the field of Natural Language Processing (NLP).",
        "link": "https://dl.acm.org/doi/10.1145/3127526.3127529"
    },
    {
        "title": "Hierarchical Topic Mining via Joint Spherical Tree and Text Embedding",
        "authors": [
            "Yu Meng",
            "Yunyi Zhang",
            "Jiaxin Huang",
            "Yu Zhang",
            "Chao Zhang",
            "Jiawei Han"
        ],
        "date": "August 2020",
        "source": "KDD '20: Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Mining a set of meaningful topics organized into a hierarchy is intuitively appealing since topic correlations are ubiquitous in massive text corpora. To account for potential hierarchical topic structures, hierarchical topic models generalize flat topic models by incorporating latent topic hierarchies into their generative modeling process. However, due to their purely unsupervised nature, the learned topic hierarchy often deviates from users' particular needs or interests. To guide the hierarchical topic discovery process with minimal user supervision, we propose a new task, Hierarchical Topic Mining, which takes a category tree described by category names only, and aims to mine a set of representative terms for each category from a text corpus to help a user comprehend his/her interested topics. We develop a novel joint tree and text embedding method along with a principled optimization procedure that allows simultaneous modeling of the category tree structure and the corpus generative process in the spherical space for effective category-representative term discovery. Our comprehensive experiments show that our model, named JoSH, mines a high-quality set of hierarchical topics with high efficiency and benefits weakly-supervised hierarchical text classification tasks.",
        "link": "https://dl.acm.org/doi/10.1145/3394486.3403242"
    },
    {
        "title": "Unsupervised behavioural mining and clustering for malware family identification",
        "authors": [
            "Khanh Huu The Dam",
            "Thomas Given-Wilson",
            "Axel Legay"
        ],
        "date": "March 2021",
        "source": "SAC '21: Proceedings of the 36th Annual ACM Symposium on Applied Computing",
        "abstract": "More accurate and advanced detection and classification of malware requires exploiting program behaviour and not merely syntactic or static features. One approach is to use system call dependency graphs (SCDGs) that represent the program behaviour by interactions with the system, and the relations between these interactions. These SCDGs have been used with supervised learning techniques to very accurately detect and classify malware. This works considers the unsupervised learning challenge of mining for common clusters of behaviour without a priori knowledge. This allows for clustering of similar programs by behaviour, that can then be used for either classification or further analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3412841.3441919"
    },
    {
        "title": "Text Mining for Evaluating Authors' Birth and Death Years",
        "authors": [
            "Dror Moghaz",
            "Yaakov Hacohen-Kerner",
            "Dov Gabbay"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "This article presents a unique method in text and data mining for finding the era, i.e., mining temporal data, in which an anonymous author was living. Finding this era can assist in the examination of a fake document or extracting the time period in which a writer lived. The study and the experiments concern Hebrew, and in some parts, Aramaic and Yiddish rabbinic texts. The rabbinic texts are undated and contain no bibliographic sections, posing an interesting challenge. This work proposes algorithms using key phrases and key words that allow the temporal organization of citations together with linguistic patterns. Based on these key phrases, key words, and the references, we established several types of “Iron-clad,” Heuristic and Greedy rules for estimating the years of birth and death of a writer in an interesting classification task. Experiments were conducted on corpora, including documents authored by 12, 24, and 36 rabbinic writers and demonstrated promising results.",
        "link": "https://dl.acm.org/doi/10.1145/3281631"
    },
    {
        "title": "Transfer Learning to Timed Text Based Video Classification Using CNN",
        "authors": [
            "Zenun Kastrati",
            "Ali Shariq Imran",
            "Arianit Kurti"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "Open educational video resources are gaining popularity with a growing number of massive open online courses (MOOCs). This has created a niche for content providers to adopt effective solutions in automatically organizing and structuring of educational resources for maximum visibility. Recent advances in deep learning techniques are proving useful in managing and classifying resources into appropriate categories. This paper proposes one such convolutional neural network (CNN) model for classifying video lectures in a MOOC setting using a transfer learning approach. The model uses a time-aligned text transcripts corresponding to video lectures from six broader subject categories. Video lectures and their corresponding transcript dataset is gathered from the Coursera MOOC platform. Two different CNN models are proposed: i) CNN based classification using embeddings learned from our MOOC dataset, ii) CNN based classification using transfer learning. Word embeddings generated from two well known state-of-the-art pre-trained models Word2Vec and GloVe, are used in the transfer learning approach for the second case. The proposed CNN models are evaluated using precision, recall, and F1 score and the obtained performance is compared with both conventional and deep learning classifiers. The proposed CNN models have an F1 score improvement of 10-22 percentage points over DNN and conventional classifiers",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326483"
    },
    {
        "title": "Evolutionary role mining in complex networks by ensemble clustering",
        "authors": [
            "Sarvenaz Choobdar",
            "Pedro Ribeiro",
            "Fernando Silva"
        ],
        "date": "April 2017",
        "source": "SAC '17: Proceedings of the Symposium on Applied Computing",
        "abstract": "The structural patterns in the neighborhood of nodes assign unique roles to the nodes. Mining the set of existing roles in a network provides a descriptive profile of the network and draws its general picture. This paper proposes a new method to determine structural roles in a dynamic network based on the current position of nodes and their historic behavior. We develop a temporal ensemble clustering technique to dynamically find groups of nodes, holding similar tempo-structural roles. We compare two weighting functions, based on age and distribution of data, to incorporate temporal behavior of nodes in the role discovery. To evaluate the performance of the proposed method, we assess the results from two points of view: 1) goodness of fit to current structure of the network; 2) consistency with historic data. We conduct the evaluation using different ensemble clustering techniques. The results on real world networks demonstrate that our method can detect tempo-structural roles that simultaneously depict the topology of a network and reflect its dynamics with high accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3019612.3019815"
    },
    {
        "title": "Analyzing used-car web listings via text mining",
        "authors": [
            "Ayhan Demiriz",
            "Fatma Cantaş"
        ],
        "date": "October 2017",
        "source": "IML '17: Proceedings of the 1st International Conference on Internet of Things and Machine Learning",
        "abstract": "Used car trade is one of the major components of the world economies. It is not uncommon to sell a car by placing an internet advertisement irrespective of the geography in these days. A typical content of an advertisement is usually composed of two parts namely the structured and the free text data. The structured data may include some information about the asking price, make, model, year, mileage of the car and the contact info. In most cases, seller may give important clues about the car's current conditions in the free text data where the title (head) of the advertisement can be included as free text too. This paper reports preliminary results from a text mining study conducted on 75K used car internet listings collected from two major car listing web sites in Turkey. As expected, the words and the phrases related to the description of the car are observed to be frequent. The leading concepts in the free text are found to be regarding how to describe the current condition of a car, for example \"no crash history\".",
        "link": "https://dl.acm.org/doi/10.1145/3109761.3109782"
    },
    {
        "title": "Text classification of Diseases Treated by Traditional Chinese Medicine Prescription based on machine learning",
        "authors": [
            "Hanqing Zhao",
            "Bo Han",
            "Chen Li"
        ],
        "date": "September 2020",
        "source": "ISAIMS '20: Proceedings of the 1st International Symposium on Artificial Intelligence in Medical Sciences",
        "abstract": "OBJECTIVE: To explore the application of machine learning in th e identification of diseases treated by traditional Chinese medicine prescriptions. METHODS: Based on the composition of the text document of Chinese medicine prescriptions, the prescriptions were divided into cough, headache and diarrhea. THUTCT were introduced to establish and train two machine learning text classification models, LibSVM and LibLinear, and the prescriptions to be deter mined were put into the model for classification and prediction. R ESULTS: The Precision rate of LibSVM model and Liblinear mod el were 0.7283 and 0.6690. Seven prescriptions were classified an d predicted, and the results were in line with expectations. CONCLUSION: THUCTC has good universality for the content of TCM prescriptions, high classification accuracy and fast testing speed, which is suitable for the text classification and discrimination research of TCM prescriptions for diseases.",
        "link": "https://dl.acm.org/doi/10.1145/3429889.3429895"
    },
    {
        "title": "Evaluation of retweet clustering method classification method using retweets on Twitter without text data",
        "authors": [
            "K. Uchida",
            "F. Toriumi",
            "T. Sakaki"
        ],
        "date": "August 2017",
        "source": "WI '17: Proceedings of the International Conference on Web Intelligence",
        "abstract": "Burst phenomena, which frequently occur on social media, are caused by such social events as flaming on the internet, elections, and natural disasters. To understand people's thoughts and feelings, we must classify their opinions from burst phenomena. Therefore, classification methods that categorize tweets are critical. However, since most classification methods focus on text mining, they cannot group tweets by topics because each tweet has poor linguistic similarities. We used a non-text-based classification method proposed by Baba et al. that groups tweets by topics, even if they have poor linguistic similarities, and verified its validity by comparing it with a text-based classification method in two different evaluations: qualitative and quantitative. In the qualitative evaluation part, we did a questionnaire survey and validated the suitability of the topic clusters created using both the non-and text-based methods. Since evaluating the similarity of every pair of tweets in each topic is difficult, we evaluated the similarity between sampled pairs in the survey and acquired more appropriate topic clustering results using the non-text-based method than the text-based method. In the quantitative evaluation part, we focused on the robustness of each method against data reduction. Many approaches analyze social media data, especially because collecting data from social media is comparatively easy. However, since collecting the whole data of burst phenomena is very costly due to the vast amounts of available social media data, robustness against data reduction is an important index to evaluate classification methods. With the non-text-based method, over 55% of the pairs of tweets in the same cluster were also included in the same cluster even when the data were reduced to 10% in all three of our example cases. In this paper, as a source we focus on Twitter, one of the most popular microblogging services. Using clustering to conduct detailed case analyses, we scrutinized three burst cases that include natural disasters and flaming on the internet and found that a non-text-based method more effectively classified tweets in burst phenomena than a text-based method.",
        "link": "https://dl.acm.org/doi/10.1145/3106426.3106451"
    },
    {
        "title": "Using Spark for Text Mining on Large Scale Liver Cancer Literature",
        "authors": [
            "Ming-Yen Lin",
            "Yu-Ju Lin",
            "Sue-Chen Hsueh"
        ],
        "date": "January 2021",
        "source": "BDET 2021: 2021 the 3rd International Conference on Big Data Engineering and Technology (BDET)",
        "abstract": "Cancer is one of the main causes of death. The number of known cancer to-date is more than one hundred. Liver cancer, ranked after trachea and lung cancer, has long been high in the cancer leading cause in Taiwan and even ranked as second in 2016. The number of scientific articles related to cancer proliferates every year. The number reaches as high as 20 million in PubMed so that discovering useful information from the massive collection is very difficult. In addition, using a single machine to sift through these articles is very time-consuming. Therefore, we present a big data analytic framework using the distributed Apache Spark platform for text mining in PubMed literature. We establish a prediction model for liver cancer articles so that researchers may effectively validate whether an article is related to liver cancer or not. Classification models in Spark MLlib including Linear Support Vector Machines (SVM), Logistic Regression, are used in our experiments. Relevancy to liver cancer is further confirmed by using MeSH (Medical Subject Headings) terms. Logistic regression is about 3 times faster than SVMs and the accuracy of both methods is close to 95% in the experiments using hold-out validation. When max_features is 500 and min_df ≤ 0.1 (or min_df = 1), the accuracy may reach 96%. In the experiments with K-fold cross-validation, the accuracy of SVMs methods is 96%. The experimental results show that that our prediction model may effectively classify liver cancer articles.",
        "link": "https://dl.acm.org/doi/10.1145/3474944.3474958"
    },
    {
        "title": "Automatic topic classification of test cases using text mining at an Android smartphone vendor",
        "authors": [
            "Junji Shimagaki",
            "Yasutaka Kamei",
            "Naoyasu Ubayashi",
            "Abram Hindle"
        ],
        "date": "October 2018",
        "source": "ESEM '18: Proceedings of the 12th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Background: An Android smartphone is an ecosystem of applications, drivers, operating system components, and assets. The volume of the software is large and the number of test cases needed to cover the functionality of an Android system is substantial. Enormous effort has been already taken to properly quantify \"what features and apps were tested and verified?\". This insight is provided by dashboards that summarize test coverage and results per feature. One method to achieve this is to manually tag or label test cases with the topic or function they cover, much like function points. At the studied Android smartphone vendor, tests are labelled with manually defined tags, so-called \"feature labels (FLs)\", and the FLs serve to categorize 100s to 1000s test cases into 10 to 50 groups. Aim: Unfortunately for developers, manual assignment of FLs to 1000s of test cases is a time consuming task, leading to inaccurately labeled test cases, which will render the dashboard useless. We created an automated system that suggests tags/labels to the developers for their test cases rather than manual labeling. Method: We use machine learning models to predict and label the functionality tested by 10,000 test cases developed at the company. Results: Through the quantitative experiments, our models achieved acceptable F-1 performance of 0.3 to 0.88. Also through the qualitative studies with expert teams, we showed that the hierarchy and path of tests was a good predictor of a feature's label. Conclusions: We find that this method can reduce tedious manual effort that software developers spent classifying test cases, while providing more accurate classification results.",
        "link": "https://dl.acm.org/doi/10.1145/3239235.3268927"
    },
    {
        "title": "To apply Data Mining for Classification of Crowd sourced Software Requirements",
        "authors": [
            "Soonh Taj",
            "Qasim Arain",
            "Imran Memon",
            "Asma Zubedi"
        ],
        "date": "April 2019",
        "source": "ICSIE '19: Proceedings of the 8th International Conference on Software and Information Engineering",
        "abstract": "Now a day's main focus of developers is to build quality software that works according to customer needs and for this reason it is necessary to gather right requirements as requirement elicitation is the critical step that impacts on the success of software project as misinterpreted requirements leads to the failure of software project. By keeping this in mind a research is carried out on improving requirements elicitation process and automating the process of classifying requirements. In this research, a model is proposed which will help in this scenario for requirements elicitation and requirement classification. This paper presents a model in which crowd sourcing approach is used so that customers, end users, stakeholders, developers and software engineers can make active participation for requirement elicitation process and requirements gathered using crowdsourcing approach are used by model for classification process i.e. classification of requirements into functional and non-functional requirements. For the proof of proposed model a case study is conducted. Results of case study provided the usefulness and efficiency of proposed model for classification of crowd sourced software requirements.",
        "link": "https://dl.acm.org/doi/10.1145/3328833.3328837"
    },
    {
        "title": "Text classification using Fuzzy TF-IDF and Machine Learning Models",
        "authors": [
            "Mariem Bounabi",
            "Karim El Moutaouakil",
            "Khalid Satori"
        ],
        "date": "October 2019",
        "source": "BDIoT'19: Proceedings of the 4th International Conference on Big Data and Internet of Things",
        "abstract": "The representation of the information has an important impact on the text classification task. Several weighting methods were proposed in the literature, and the term frequency-inverse term frequency (TFIDF), the most know on the text treatment field. The FTF-IDF is a vector representation where the components of the TFIDF are presented as inputs to the Fuzzy Inference System (FIS). In this work, we compare several Machin Learning algorithms such as Naïve Bayes and its derivatives, SVM and Random forest classifiers, using the FTF-IDF representation. To improve the quality of the used classifiers, we call sum attribute selection methods. The recognition rate, for the tested systems, is satisfied, where the system based on naïve Bayes classifier, the FTF-IDF weighting terms, and the info gain select attributes method gives 98.7% as accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3372938.3372956"
    },
    {
        "title": "Text Mining in Cybersecurity: A Systematic Literature Review",
        "authors": [
            "Luciano Ignaczak",
            "Guilherme Goldschmidt",
            "Cristiano André Da Costa",
            "Rodrigo Da Rosa Righi"
        ],
        "date": null,
        "source": "ACM Computing Surveys",
        "abstract": "The growth of data volume has changed cybersecurity activities, demanding a higher level of automation. In this new cybersecurity landscape, text mining emerged as an alternative to improve the efficiency of the activities involving unstructured data. This article proposes a Systematic Literature Review (SLR) to present the application of text mining in the cybersecurity domain. Using a systematic protocol, we identified 2,196 studies, out of which 83 were summarized. As a contribution, we propose a taxonomy to demonstrate the different activities in the cybersecurity domain supported by text mining. We also detail the strategies evaluated in the application of text mining tasks and the use of neural networks to support activities involving unstructured data. The work also discusses text classification performance aiming its application in real-world solutions. The SLR also highlights open gaps for future research, such as the analysis of non-English content and the intensification in the usage of neural networks.",
        "link": "https://dl.acm.org/doi/10.1145/3462477"
    },
    {
        "title": "Using Text Mining to Analyze the Financial Patents",
        "authors": [
            "Yung-Feng Lu",
            "Jia-Lang Xu",
            "Mu-Yen Chen"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "With the advancement and development of science and technology, Nowadays society is the development of information technology, cloud computing, mobile Internet, Internet of Things, block chain and information technology.The combination of finance and technology has made financial technology terms gradually emerging.. This research analyzes the patents of financial technology. However, Taiwan's financial technology patents are mostly G06Q-20 (payment plan, architecture or agreement), G06Q-30 (commercial, such as marketing, shopping, payment, auction or e-commerce). G06Q-40 (financial, investment or tax treatment, insurance).The research use text mining technology to analyze and understand Taiwan's trends and developments in financial technology. The research results show that G06Q-20 patents are biased towards information security and mobile payment. G06Q-30 patents favor shopping websites and advertising. G06Q-40 patents are biased towards diversity, such as securities, insurance, finance, and stocks.",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326478"
    },
    {
        "title": "Identifying Online Profiles of Distance Learning Students Using Data Mining Techniques",
        "authors": [
            "Osama Islam",
            "Muazzam Siddiqui",
            "Naif Radi Aljohani"
        ],
        "date": "October 2019",
        "source": "ICDTE '19: Proceedings of the 3rd International Conference on Digital Technology in Education",
        "abstract": "Educational data has grown over the years with the increased use of technology within educational environments. This has led to a huge amount of data being stored in various data sources representing the student, his/her activities, and other aspects relevant to the learning process. To meet this analytical need, Educational Data Mining (EDM) has emerged to assist educational institutions in identifying key benefits such as students at risk, the level of student engagement or predicting student performance. The aim of this research was to explore the various aspects of student interaction data using data mining techniques to identify relevant patterns of behaviors and possible key attributes that have higher degrees of influence on distance learning students. The main findings identified several patterns of user online profiles based on a set of adopted learning strategies, the research proposed a framework for analyzing such interaction data based on R and Hadoop platforms to correlate online profiles with student performance.",
        "link": "https://dl.acm.org/doi/10.1145/3369199.3369249"
    },
    {
        "title": "Extremely Fast Decision Tree Mining for Evolving Data Streams",
        "authors": [
            "Albert Bifet",
            "Jiajin Zhang",
            "Wei Fan",
            "Cheng He",
            "Jianfeng Zhang",
            "Jianfeng Qian",
            "Geoff Holmes",
            "Bernhard Pfahringer"
        ],
        "date": "August 2017",
        "source": "KDD '17: Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
        "abstract": "Nowadays real-time industrial applications are generating a huge amount of data continuously every day. To process these large data streams, we need fast and efficient methodologies and systems. A useful feature desired for data scientists and analysts is to have easy to visualize and understand machine learning models. Decision trees are preferred in many real-time applications for this reason, and also, because combined in an ensemble, they are one of the most powerful methods in machine learning. In this paper, we present a new system called STREAMDM-C++, that implements decision trees for data streams in C++, and that has been used extensively at Huawei. Streaming decision trees adapt to changes on streams, a huge advantage since standard decision trees are built using a snapshot of data, and can not evolve over time. STREAMDM-C++ is easy to extend, and contains more powerful ensemble methods, and a more efficient and easy to use adaptive decision trees. We compare our new implementation with VFML, the current state of the art implementation in C, and show how our new system outperforms VFML in speed using less resources.",
        "link": "https://dl.acm.org/doi/10.1145/3097983.3098139"
    },
    {
        "title": "Data miners' little helper: data transformation activity cues for cluster analysis on document collections",
        "authors": [
            "Tania Cerquitelli",
            "Evelina Di Corso",
            "Francesco Ventura",
            "Silvia Chiusano"
        ],
        "date": "June 2017",
        "source": "WIMS '17: Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "In this paper we propose a new self-learning engine to streamline the analytics process, as it enables analysts to mine massive data repositories with minimal user intervention. In the context of cluster analysis on a collection of documents this new system, named SELF-DATA (SELF-learning DAta TrAnsformation), suggests to the analyst how to configure the whole mining process for a given dataset. SELF-DATA relies on an engine exploring different data weighting schemas (e.g., normalized term frequencies) and data transformation methods (e.g., PCA) before applying the cluster analysis, evaluating and comparing solutions through different quality indices (e.g., weighted Silhouette), and presenting the k-top solutions to the analyst. SELF-DATA will also include a knowledge base storing results of experiments on previously processed datasets, and a classification algorithm trained on the knowledge base content to forecast the best configuration for the whole mining process for an unexplored dataset. The first development of SELF-DATA running on Apache Spark has been validated on 5 collections of documents. Experimental results highlight that TF-IDF and logarithmic entropy are effective to measure item relevance with sparse datasets, and the LSI method outperforms PCA with a large dictionary.",
        "link": "https://dl.acm.org/doi/10.1145/3102254.3102288"
    },
    {
        "title": "Interactive Visual Graph Mining and Learning",
        "authors": [
            "Ryan A. Rossi",
            "Nesreen K. Ahmed",
            "Rong Zhou",
            "Hoda Eldardiry"
        ],
        "date": null,
        "source": "ACM Transactions on Intelligent Systems and Technology",
        "abstract": "This article presents a platform for interactive graph mining and relational machine learning called GraphVis. The platform combines interactive visual representations with state-of-the-art graph mining and relational machine learning techniques to aid in revealing important insights quickly as well as learning an appropriate and highly predictive model for a particular task (e.g., classification, link prediction, discovering the roles of nodes, and finding influential nodes). Visual representations and interaction techniques and tools are developed for simple, fast, and intuitive real-time interactive exploration, mining, and modeling of graph data. In particular, we propose techniques for interactive relational learning (e.g., node/link classification), interactive link prediction and weighting, role discovery and community detection, higher-order network analysis (via graphlets, network motifs), among others. GraphVis also allows for the refinement and tuning of graph mining and relational learning methods for specific application domains and constraints via an end-to-end interactive visual analytic pipeline that learns, infers, and provides rapid interactive visualization with immediate feedback at each change/prediction in real-time. Other key aspects include interactive filtering, querying, ranking, manipulating, exporting, as well as tools for dynamic network analysis and visualization, interactive graph generators (including new block model approaches), and a variety of multi-level network analysis techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3200764"
    },
    {
        "title": "Data Mining and Opinion Mining: A Tool in Educational Context",
        "authors": [
            "Myriam Peñafiel",
            "Stefanie Vásquez",
            "Diego Vásquez",
            "Juan Zaldumbide",
            "Sergio Luján-Mora"
        ],
        "date": "July 2018",
        "source": "ICoMS '18: Proceedings of the 2018 1st International Conference on Mathematics and Statistics",
        "abstract": "The use of the web as a universal communication platform generates large volumes of data (Big data), which in many cases, need to be processed so that they can become useful knowledge in face of the sceptics who have doubts about the credibility of such information. The use of web data that comes from educational contexts needs to be addressed, since that large amount of unstructured information is not being valued, losing valuable information that can be used. To solve this problem, we propose the use of data mining techniques such as sentiment analysis to validate the information that comes from the educational platforms. The objective of this research is to propose a methodology that allows the user to apply sentiment analysis in a simple way, because although some researchers have done it, very few do with data in the educational context. The results obtained prove that the proposal can be used in similar cases.",
        "link": "https://dl.acm.org/doi/10.1145/3274250.3274263"
    },
    {
        "title": "Research Hotspots Mining and Visualized Analysis Based on Linking Cluster and K-Core Decomposition",
        "authors": [
            "Ying Cai",
            "Fang Huang",
            "Shuai Wang",
            "Hao Zhang",
            "Chunxiu Du"
        ],
        "date": "May 2018",
        "source": "ICDPA 2018: Proceedings of the International Conference on Data Processing and Applications",
        "abstract": "In this paper, we focus on research hotspots mining with knowledge semantic features on discipline topic word networks, which are extracted from scientific and technical documents. For the topic networks, a novel method of combining linking community clustering with k-core decomposition is proposed to mine the hotspot community with knowledge hierarchy structure. And as a post-processing, the hierarchical community identification is implemented by k-core decomposition in order to discover the internal connection with knowledge semantic hierarchy in the research hotspot. Finally, the community density and visualization method is utilized to analyze the core and sub- problem in the research hotspot community. In experiments, the topic networks for a single discipline and interdisciplinary research hotspot are taken as examples of analysis, and the results show the proposed approach can effectively realize the objective of mining and analyzing the discipline research hotspot on the academic topic word network.",
        "link": "https://dl.acm.org/doi/10.1145/3224207.3224216"
    },
    {
        "title": "The Application of Data Mining Algorithms in the Construction of Travel Recommendation System",
        "authors": [
            "Chao Lou",
            "Zhaonan Mu",
            "Mengzhu Liu"
        ],
        "date": "January 2021",
        "source": "CONF-CDS 2021: The 2nd International Conference on Computing and Data Science",
        "abstract": "Photos shared by users of online social network often associated with tags, texts, geographic and time information. These data are ideal research resource which can be used by researchers to observe people's interests and behaviors. They can also be used to analyze people's travel behaviors and interests because it can reflect people's travel activities and experiences directly. This project aimed to consolidate people into groups based on their travel preferences of travel season, travel time span, scenery type of travel location and then analyze popular travel places for different people groups. The travel preferences were analyzed from extracted data applying PHP language and Naive Bayes classification approach. According to the analyzed data, people were divided into groups. Then, DBSCAN clustering was used to get popular locations based on the different groups. As a result, 80 thousand photo data of 5 cities have been collected from Flickr by Phython. It has been showed that people's preference of season in most of the mined cities was spring and summer and the preference of time span was short. Moreover, mined people has been divided into 16 groups according to their preference and based on the groups popular locations have been calculated by DBSCAN clustering. Theses locations have been provided in a recommendation website which was built in this project. For the future work, mining Flickr data associated with other travel social network sites could be a good way to build a more effective recommendation system.",
        "link": "https://dl.acm.org/doi/10.1145/3448734.3450477"
    },
    {
        "title": "Compare Machine Learning Models in Text Classification Using Steam User Reviews",
        "authors": [
            "Youchen Miao",
            "Zeyu Jin",
            "Yumeng Zhang",
            "Yuchen Chen",
            "Junren Lai"
        ],
        "date": "November 2021",
        "source": "ICSED '21: Proceedings of the 2021 3rd International Conference on Software Engineering and Development",
        "abstract": "Text Classification and Sentiment Analysis of game reviews are viewed as important parts in not only academic fields but also in game studies. In this paper, with more than 400 thousand game reviews on Steam platform, we preprocess the data using different libraries (sklearn, nltk, and spaCy) and use them as inputs to build three sentiment classification models based on different algorithms (Naive Bayes, SVM, and Random Forest). In contrast to previous studies that only focus on different sentiment analysis models, our paper also highlights the use of different APIs to preprocess the data and their corresponding model performance. The results show that no matter which API we choose, Random Forest models always perform the best. However, in terms of training time, Naive Bayes is the fastest. This work can be used to apply grid search for researchers to automatically find the optimum API before conducting sentiment analysis in the future.",
        "link": "https://dl.acm.org/doi/10.1145/3507473.3507480"
    },
    {
        "title": "Lost in Transduction: Transductive Transfer Learning in Text Classification",
        "authors": [
            "Alejandro Moreo",
            "Andrea Esuli",
            "Fabrizio Sebastiani"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Obtaining high-quality labelled data for training a classifier in a new application domain is often costly. Transfer Learning\n(a.k.a. “Inductive Transfer”) tries to alleviate these costs by transferring, to the “target” domain of interest, knowledge available from a different “source” domain. In transfer learning the lack of labelled information from the target domain is compensated by the availability at training time of a set of unlabelled examples from the target distribution. Transductive Transfer Learning denotes the transfer learning setting in which the only set of target documents that we are interested in classifying is known and available at training time. Although this definition is indeed in line with Vapnik’s original definition of “transduction”, current terminology in the field is confused. In this article, we discuss how the term “transduction” has been misused in the transfer learning literature, and propose a clarification consistent with the original characterization of this term given by Vapnik. We go on to observe that the above terminology misuse has brought about misleading experimental comparisons, with inductive transfer learning methods that have been incorrectly compared with transductive transfer learning methods. We then, give empirical evidence that the difference in performance between the inductive version and the transductive version of a transfer learning method can indeed be statistically significant (i.e., that knowing at training time the only data one needs to classify indeed gives an advantage). Our clarification allows a reassessment of the field, and of the relative merits of the major, state-of-the-art algorithms for transfer learning in text classification.",
        "link": "https://dl.acm.org/doi/10.1145/3453146"
    },
    {
        "title": "Machine Learning on Mining Potential Adverse Drug Reactions for Pharmacovigilance",
        "authors": [
            "Ke Jia"
        ],
        "date": "July 2021",
        "source": "DSIT 2021: 2021 4th International Conference on Data Science and Information Technology",
        "abstract": "Drugs are related to the lives and health of patients. Mining and discovering potential adverse drug reactions from a large number of unstructured texts play an important role in drug research and pharmacovigilance. It is a hot research issue in the field of biomedicine in recent years. A large number of scholars have discovered a large amount of information on adverse drug reactions from different data sources through effective machine learning methods. This paper reviews the research progress in recent years to provide a reference for future research.",
        "link": "https://dl.acm.org/doi/10.1145/3478905.3478964"
    },
    {
        "title": "TACAM: Topic And Context Aware Argument Mining",
        "authors": [
            "Michael Fromm",
            "Evgeniy Faerman",
            "Thomas Seidl"
        ],
        "date": "October 2019",
        "source": "WI '19: IEEE/WIC/ACM International Conference on Web Intelligence",
        "abstract": "In this work we address the problem of argument search. The purpose of argument search is the distillation of pro and contra arguments for requested topics from large text corpora. In previous works, the usual approach is to use a standard search engine to extract text parts which are relevant to the given topic and subsequently use an argument recognition algorithm to select arguments from them. The main challenge in the argument recognition task, which is also known as argument mining, is that often sentences containing arguments are structurally similar to purely informative sentences without any stance about the topic. In fact, they only differ semantically. Most approaches use topic or search term information only for the first search step and therefore assume that arguments can be classified independently of a topic. We argue that topic information is crucial for argument mining, since the topic defines the semantic context of an argument. Precisely, we propose different models for the classification of arguments, which take information about a topic of an argument into account. Moreover, to enrich the context of a topic and to let models understand the context of the potential argument better, we integrate information from different external sources such as Knowledge Graphs or pre-trained NLP models. Our evaluation shows that considering topic information, especially in connection with external information, provides a significant performance boost for the argument mining task.",
        "link": "https://dl.acm.org/doi/10.1145/3350546.3352506"
    },
    {
        "title": "Text mining for incoming tasks based on the urgency/importance factors and task classification using machine learning tools",
        "authors": [
            "Yasser Ali Alshehri"
        ],
        "date": "March 2020",
        "source": "ICCDA 2020: Proceedings of the 2020 the 4th International Conference on Compute and Data Analysis",
        "abstract": "In workplaces, there is a massive amount of unstructured data from different sources. In this paper, we present a case study that explains how can through communications between employees, we can help to prioritize tasks requests to increase the efficiency of their works for both technical and non-technical workers. This involves managing daily incoming tasks based on their level of urgency and importance.To allow all workers to utilize the urgency-importance matrix as a time-management tool, we need to automate this tool. The textual content of incoming tasks are analyzed, and metrics related to urgency and importance are extracted. A third factor (i.e., the response variable) is defined based on the two input variables (urgency and importance). Then, machine learning applied to the data to predict the class of incoming tasks based on data outcome desired. We used ordinal regression, neural networks, and decision tree algorithms to predict the four levels of task priority. We measure the performance of all using recalls, precisions, and F-scores. All classifiers perform higher than 89% in terms of all measures.",
        "link": "https://dl.acm.org/doi/10.1145/3388142.3388153"
    },
    {
        "title": "Text mining for malware classification using multivariate all repeated patterns detection",
        "authors": [
            "Konstantinos F. Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2019",
        "source": "ASONAM '19: Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Mobile phones have become nowadays a commodity to the majority of people. Using them, people are able to access the world of Internet and connect with their friends, their colleagues at work or even unknown people with common interests. This proliferation of the mobile devices has also been seen as an opportunity for the cyber criminals to deceive smartphone users and steel their money directly or indirectly, respectively, by accessing their bank accounts through the smartphones or by blackmailing them or selling their private data such as photos, credit card data, etc. to third parties. This is usually achieved by installing malware to smartphones masking their malevolent payload as a legitimate application and advertise it to the users with the hope that mobile users will install it in their devices. Thus, any existing application can easily be modified by integrating a malware and then presented it as a legitimate one. In response to this, scientists have proposed a number of malware detection and classification methods using a variety of techniques. Even though, several of them achieve relatively high precision in malware classification, there is still space for improvement. In this paper, we propose a text mining all repeated pattern detection method which uses the decompiled files of an application in order to classify a suspicious application into one of the known malware families. Based on the experimental results using a real malware dataset, the methodology tries to correctly classify (without any misclassification) all randomly selected malware applications of 3 categories with 3 different families each.",
        "link": "https://dl.acm.org/doi/10.1145/3341161.3350841"
    },
    {
        "title": "Comparative Study between Traditional Machine Learning and Deep Learning Approaches for Text Classification",
        "authors": [
            "Cannannore Nidhi Kamath",
            "Syed Saqib Bukhari",
            "Andreas Dengel"
        ],
        "date": "August 2018",
        "source": "DocEng '18: Proceedings of the ACM Symposium on Document Engineering 2018",
        "abstract": "In this contemporaneous world, it is an obligation for any organization working with documents to end up with the insipid task of classifying truckload of documents, which is the nascent stage of venturing into the realm of information retrieval and data mining. But classification of such humongous documents into multiple classes, calls for a lot of time and labor. Hence a system which could classify these documents with acceptable accuracy would be of an unfathomable help in document engineering. We have created multiple classifiers for document classification and compared their accuracy on raw and processed data. We have garnered data used in a corporate organization as well as publicly available data for comparison. Data is processed by removing the stop-words and stemming is implemented to produce root words. Multiple traditional machine learning techniques like Naive Bayes, Logistic Regression, Support Vector Machine, Random forest Classifier and Multi-Layer Perceptron are used for classification of documents. Classifiers are applied on raw and processed data separately and their accuracy is noted. Along with this, Deep learning technique such as Convolution Neural Network is also used to classify the data and its accuracy is compared with that of traditional machine learning techniques. We are also exploring hierarchical classifiers for classification of classes and subclasses. The system classifies the data faster and with better accuracy than if done manually. The results are discussed in the results and evaluation section.",
        "link": "https://dl.acm.org/doi/10.1145/3209280.3209526"
    },
    {
        "title": "Identification of Overpricing in the Purchase of Medication by the Federal Government of Brazil, Using Text Mining and Clustering Based on Ontology",
        "authors": [
            "Marco Aurelio O. S. Correa",
            "Adriano Galindo Leal"
        ],
        "date": "August 2018",
        "source": "ICCBDC '18: Proceedings of the 2018 2nd International Conference on Cloud and Big Data Computing",
        "abstract": "Increasing the transparency level in his actions and spending is one of the primary duties of the Brazilian Federal Government. The creation of laws that oblige full disclose of all its expenditures through transparency portals enables citizens to supervise all government entities. However, only the dissemination of these data, without a definite standard or the availability of data analysis tools, does not guarantee that the citizen is empowered to play his role. Therefore hence, the objective of this work is to identify overprice in the acquisition of products purchased by the federal government of Brazil using the unstructured data available on the Transparency Portal. The last two-years' worth of purchasing data, available in the Transparency Portal, were extracted, processed and stored. Due to his diverse nature and high volume of data, this study focused only on medicines purchased by the Ministry of Health. Ontology-based text mining and clustering techniques were applied for automatic identification and classification of products. The processing of this information was done through text mining and clustering, based on the ontology registered in another database of the Brazilian government. Because of this work, a consolidated price base per medication was created to allow the identification of distortions in prices practised, facilitating the identification of cases that merit further investigation to unravel fraud to the treasury.",
        "link": "https://dl.acm.org/doi/10.1145/3264560.3264569"
    },
    {
        "title": "Computational Estimation by Scientific Data Mining with Classical Methods to Automate Learning Strategies of Scientists",
        "authors": [
            "Aparna S. Varde"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Experimental results are often plotted as 2-dimensional graphical plots (aka graphs) in scientific domains depicting dependent versus independent variables to aid visual analysis of processes. Repeatedly performing laboratory experiments consumes significant time and resources, motivating the need for computational estimation. The goals are to estimate the graph obtained in an experiment given its input conditions, and to estimate the conditions that would lead to a desired graph. Existing estimation approaches often do not meet accuracy and efficiency needs of targeted applications. We develop a computational estimation approach called AutoDomainMine that integrates clustering and classification over complex scientific data in a framework so as to automate classical learning methods of scientists. Knowledge discovered thereby from a database of existing experiments serves as the basis for estimation. Challenges include preserving domain semantics in clustering, finding matching strategies in classification, striking a good balance between elaboration and conciseness while displaying estimation results based on needs of targeted users, and deriving objective measures to capture subjective user interests. These and other challenges are addressed in this work. The AutoDomainMine approach is used to build a computational estimation system, rigorously evaluated with real data in Materials Science. Our evaluation confirms that AutoDomainMine provides desired accuracy and efficiency in computational estimation. It is extendable to other science and engineering domains as proved by adaptation of its sub-processes within fields such as Bioinformatics and Nanotechnology.",
        "link": "https://dl.acm.org/doi/10.1145/3502736"
    },
    {
        "title": "Where is the road for issue reports classification based on text mining?",
        "authors": [
            "Qiang Fan",
            "Yue Yu",
            "Gang Yin",
            "Tao Wang",
            "Huaimin Wang"
        ],
        "date": "November 2017",
        "source": "ESEM '17: Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Currently, open source projects receive various kinds of issues daily, because of the extreme openness of Issue Tracking System (ITS) in GitHub. ITS is a labor-intensive and time-consuming task of issue categorization for project managers. However, a contributor is only required a short textual abstract to report an issue in GitHub. Thus, most traditional classification approaches based on detailed and structured data (e.g., priority, severity, software version and so on) are difficult to adopt. In this paper, issue classification approaches on a large-scale dataset, including 80 popular projects and over 252,000 issue reports collected from GitHub, were investigated. First, four traditional text-based classification methods and their performances were discussed. Semantic perplexity (i.e., an issues description confuses bug-related sentences with nonbug-related sentences) is a crucial factor that affects the classification performances based on quantitative and qualitative study. Finally, A two-stage classifier framework based on the novel metrics of semantic perplexity of issue reports was designed. Results show that our two-stage classification can significantly improve issue classification performances.",
        "link": "https://dl.acm.org/doi/10.1109/ESEM.2017.19"
    },
    {
        "title": "Feature-based Facebook reviews process model for e-management using data mining",
        "authors": [
            "Anish Kumar Varudharajulu",
            "Yongsheng Ma"
        ],
        "date": "January 2019",
        "source": "IC4E '19: Proceedings of the 10th International Conference on E-Education, E-Business, E-Management and E-Learning",
        "abstract": "The data generated from online communication acts as potential gold mines for discovering knowledge for researchers. A large amount of data is also generated in the form of web documents, emails, blogs, and feedback, etc. Text analytics is being significantly employed to mine important information. Opinion mining is the process of extracting human thoughts and perceptions from unstructured texts. The showstopper for designing an opinion mining system for analyzing reviews arise from the fact that customer reviews are often noisy. These reviews are informally written. In addition, they are subjected to spelling mistakes, grammatical errors, improper punctuation and irrational capitalization. This paper focuses on analyzing the different classification and clustering algorithms aimed at extracting and consolidating opinions of customers from social media sites like Facebook, Twitter and through surveys, at multiple levels of granularity to monitor and measure customer satisfaction. Ours is an automated approach, in which the system aids in the process of knowledge assimilation for knowledge-based building and also performs the analytics. Domain experts ratify the knowledge base and also provide training datasets for the system to intuitively gather more instances for ratification. The system identifies opinion expressions as phrases containing opinion words, opinionated features and also opinion modifiers. These expressions are categorized as positive, negative or neutral. Opinion expressions are identified and categorized using localized linguistic techniques. Opinions can be congregated at any desired level of specificity i.e. feature level or product level, user level or service level, etc. We have developed a system based on this approach, which provides the user with a platform to analyze opinion expressions crawled from a set of pre-defined datasets.",
        "link": "https://dl.acm.org/doi/10.1145/3306500.3306514"
    },
    {
        "title": "Discriminative Topic Mining via Category-Name Guided Text Embedding",
        "authors": [
            "Yu Meng",
            "Jiaxin Huang",
            "Guangyuan Wang",
            "Zihan Wang",
            "Chao Zhang",
            "Yu Zhang",
            "Jiawei Han"
        ],
        "date": "April 2020",
        "source": "WWW '20: Proceedings of The Web Conference 2020",
        "abstract": "Mining a set of meaningful and distinctive topics automatically from massive text corpora has broad applications. Existing topic models, however, typically work in a purely unsupervised way, which often generate topics that do not fit users’ particular needs and yield suboptimal performance on downstream tasks. We propose a new task, discriminative topic mining, which leverages a set of user-provided category names to mine discriminative topics from text corpora. This new task not only helps a user understand clearly and distinctively the topics he/she is most interested in, but also benefits directly keyword-driven classification tasks. We develop CatE, a novel category-name guided text embedding method for discriminative topic mining, which effectively leverages minimal user guidance to learn a discriminative embedding space and discover category representative terms in an iterative manner. We conduct a comprehensive set of experiments to show that CatE mines high-quality set of topics guided by category names only, and benefits a variety of downstream applications including weakly-supervised classification and lexical entailment direction identification.",
        "link": "https://dl.acm.org/doi/10.1145/3366423.3380278"
    },
    {
        "title": "Comparative Study of Heart Disease Diagnosis Using Top Ten Data Mining Classification Algorithms",
        "authors": [
            "I. Ketut Agung Enriko"
        ],
        "date": "June 2019",
        "source": "ICFET '19: Proceedings of the 5th International Conference on Frontiers of Educational Technologies",
        "abstract": "Data mining has been used for many purposes, especially for prediction system. In healthcare, data mining algorithms often used in disease diagnosis. Meanwhile, heart disease is known as a primary cause of death over the years. Many studies have been performed in heart disease diagnosis using data mining methods. There are some popular data mining algorithms that can be used in heart disease diagnosis, for example, k-Nearest Neighbor, CART, and AdaBoost. The algorithms are used to analyze a sample of cardiovascular patients data and predict the heart disease type that they suffer. Some parameters are taken from the patient, including EKG morphology, blood pressure, and information about the existence of chest pain, shortness of breath, palpitation, and cold sweat. In this study, medical records data are collected from Harapan Kita Hospital and utilized as a dataset sample in this research. Top ten data mining classification algorithms are used in diagnosing heart disease from Harapan Kita Hospital data and examining their performance by checking the accuracy and speed.",
        "link": "https://dl.acm.org/doi/10.1145/3338188.3338220"
    },
    {
        "title": "Text Mining Approach for Identifying Research Trends",
        "authors": [
            "Snezhana Sulova"
        ],
        "date": "June 2021",
        "source": "CompSysTech '21: Proceedings of the 22nd International Conference on Computer Systems and Technologies",
        "abstract": "With the increase of unstructured data, the issues connected with automatic text processing, the categorization of documents and the discovery of topics have become objects of growing interest. In order to improve the process of grouping and processing research publications, we would like to propose a method based upon natural language processing. It is based on text mining technologies which aim to identify key tendencies in documents. It processes the content of publications by clustering and identifies the topics of each identified group. This analysis helps by identifying key tendencies as well as discovering emerging new areas of research. Publications from the research literature database, Scopus, were used to test the approach. The topic of the publications is “the application of digital technologies in the logistics business”. The experiments were completed using the RapidMiner Studio software.",
        "link": "https://dl.acm.org/doi/10.1145/3472410.3472433"
    },
    {
        "title": "Assessment of Vulnerability Severity using Text Mining",
        "authors": [
            "Georgios Spanos",
            "Lefteris Angelis",
            "Dimitrios Toloudis"
        ],
        "date": "September 2017",
        "source": "PCI '17: Proceedings of the 21st Pan-Hellenic Conference on Informatics",
        "abstract": "Software1 vulnerabilities are closely associated with information systems security, a major and critical field in today's technology. Vulnerabilities constitute a constant and increasing threat for various aspects of everyday life, especially for safety and economy, since the social impact from the problems that they cause is complicated and often unpredictable. Although there is an entire research branch in software engineering that deals with the identification and elimination of vulnerabilities, the growing complexity of software products and the variability of software production procedures are factors contributing to the ongoing occurrence of vulnerabilities, Hence, another area that is being developed in parallel focuses on the study and management of the vulnerabilities that have already been reported and registered in databases. The information contained in such databases includes, a textual description and a number of metrics related to vulnerabilities. The purpose of this paper is to investigate to what extend the assessment of the vulnerability severity can be inferred directly from the corresponding textual description, or in other words, to examine the informative power of the description with respect to the vulnerability severity. For this purpose, text mining techniques, i.e. text analysis and three different classification methods (decision trees, neural networks and support vector machines) were employed. The application of text mining to a sample of 70,678 vulnerabilities from a public data source shows that the description itself is a reliable and highly accurate source of information for vulnerability prioritization.",
        "link": "https://dl.acm.org/doi/10.1145/3139367.3139390"
    },
    {
        "title": "Clustering Algorithms for Spatial Data Mining",
        "authors": [
            "Chetashri Bhadane",
            "Ketan Shah"
        ],
        "date": "April 2020",
        "source": "ICGDA '20: Proceedings of the 2020 3rd International Conference on Geoinformatics and Data Analysis",
        "abstract": "With the advances in mobile and wireless technologies, there has been a rise in applications that track and share the users' geospatial data. People use several social networking sites such as Twitter, Facebook and Flickr, where they share their status updates. With the integration of Global Positioning System (GPS) with mobile phones, it is now possible to share one's locations on these social networks. GPS allows us to record and track a person's movement along with the timestamp. The data set obtained from these GPS logs is vast and is widely used to analyze the users' movement patterns. Specifically, we can find out significant locations based on the number of users present at that location and the time spent by them at such places. Once significant places have been identified, it is also possible to identify the semantic importance of these locations. This paper presents an overview of the clustering techniques used to find important places of interest using large GPS based mobility datasets. Four clustering algorithms, K-Means, DBSCAN, OPTICS and Hierarchical, are implemented, and performance is tested using real-time data of 50 users collected over 2--5 years. Performance summary depicts that K-Means and DBSCAN perform well for spatial data.",
        "link": "https://dl.acm.org/doi/10.1145/3397056.3397068"
    },
    {
        "title": "Feature Selection based Arabic Text Classification using Different Machine Learning Algorithms: Comparative Study",
        "authors": [
            "Sakina Rim Bennabi",
            "Zakaria Elberrichi"
        ],
        "date": "June 2020",
        "source": "ICIST '20: Proceedings of the 10th International Conference on Information Systems and Technologies",
        "abstract": "Feature selection is a method of data pre-processing widely used when mining large data, such as textual classification. Several studies have been conducted to compare the different methods of feature selection applied to corpora in English. Unfortunately, a small number of works concern the Arabic language. This article aims to present a comparative study of different feature selection techniques including: Chi2, the ANOVA method and mutual information, applied on a corpus in Arabic language, while also diversifying the machine learning algorithms (Naive Bayes, SVM and KNN). This experimental study has shown in general that reducing dimensionality with feature selection techniques has slightly affected the performance of textual classification, reducing the size of the corpus by up to 1%.",
        "link": "https://dl.acm.org/doi/10.1145/3447568.3448531"
    },
    {
        "title": "Intelligent Data Analysis using Optimized Support Vector Machine Based Data Mining Approach for Tourism Industry",
        "authors": [
            "Ms Promila Sharma",
            "Uma Meena",
            "Girish Kumar Sharma"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Data analysis involves the deployment of sophisticated approaches from data mining methods, information theory, and artificial intelligence in various fields like tourism, hospitality, and so on for the extraction of knowledge from the gathered and preprocessed data. In tourism, pattern analysis or data analysis using classification is significant for finding the patterns that represent new and potentially useful information or knowledge about the destination and other data. Several data mining techniques are introduced for the classification of data or patterns. However, overfitting, less accuracy, local minima, sensitive to noise are the drawbacks in some existing data mining classification methods. To overcome these challenges, Support vector machine with Red deer optimization (SVM-RDO) based data mining strategy is proposed in this article. Extended Kalman filter (EKF) is utilized in the first phase, i.e., data cleaning to remove the noise and missing values from the input data. Mantaray foraging algorithm (MaFA) is used in the data selection phase, in which the significant data are selected for the further process to reduce the computational complexity. The final phase is the classification, in which SVM-RDO is proposed to access the useful pattern from the selected data. PYTHON is the implementation tool used for the experiment of the proposed model. The experimental analysis is done to show the efficacy of the proposed work. From the experimental results, the proposed SVM-RDO achieved better accuracy, precision, recall, and F1 score than the existing methods for the tourism dataset. Thus, it is showed the effectiveness of the proposed SVM-RDO for pattern analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3494566"
    },
    {
        "title": "Applying text mining to predict learners' cognitive engagement",
        "authors": [
            "Hayati Hind",
            "Mohammed Khalidi Idrissi",
            "Samir Bennani"
        ],
        "date": "October 2017",
        "source": "SCAMS '17: Proceedings of the Mediterranean Symposium on Smart City Application",
        "abstract": "In academic process, engagement represent one of the important success' factor. This paper proposes a predictive system for learners' cognitive engagement based on their online discussion forums participation. In the first level the ontology OWL and the LSA method are used to perform a semantic classification system of the threads according to a specific context chosen by the tutor. Then the Text mining, as a predictive method, is applied to the classified threads and learners' participation in the forums.",
        "link": "https://dl.acm.org/doi/10.1145/3175628.3175655"
    },
    {
        "title": "Investigate Transitions into Drug Addiction through Text Mining of Reddit Data",
        "authors": [
            "John Lu",
            "Sumati Sridhar",
            "Ritika Pandey",
            "Mohammad Al Hasan",
            "Georege Mohler"
        ],
        "date": "July 2019",
        "source": "KDD '19: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Increasing rates of opioid drug abuse and heightened prevalence of online support communities underscore the necessity of employing data mining techniques to better understand drug addiction using these rapidly developing online resources. In this work, we obtained data from Reddit, an online collection of forums, to gather insight into drug use/misuse using text snippets from users narratives. Specifically, using users' posts, we trained a binary classifier which predicts a user's transitions from casual drug discussion forums to drug recovery forums. We also proposed a Cox regression model that outputs likelihoods of such transitions. In doing so, we found that utterances of select drugs and certain linguistic features contained in one's posts can help predict these transitions. Using unfiltered drug-related posts, our research delineates drugs that are associated with higher rates of transitions from recreational drug discussion to support/recovery discussion, offers insight into modern drug culture, and provides tools with potential applications in combating the opioid crisis.",
        "link": "https://dl.acm.org/doi/10.1145/3292500.3330737"
    },
    {
        "title": "Analysing digital banking reviews using text mining",
        "authors": [
            "Li Chen Cheng",
            "Legaspi Rhea Sharmayne"
        ],
        "date": "December 2020",
        "source": "ASONAM '20: Proceedings of the 12th IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Digital banks are new entrants in the banking industry in the Philippines as they only started late 2018. Since then, a handful of players have and are still emerging. With more and more people becoming technologically savvy, it is very critical for financial institutions to develop a digital banking application that will stand out from the competition. This paper aims to use text mining methods to analyse digital banking application reviews. This study will perform topic modelling using LDA to explore customer concerns and will mine association rules between the digital banking features with the review score. The results will reveal which areas the digital banking application can further optimize for customer satisfaction and retention.",
        "link": "https://dl.acm.org/doi/10.1109/ASONAM49781.2020.9381429"
    },
    {
        "title": "Mining Moocs Videos Metadata Using Classification Techniques",
        "authors": [
            "El Harrak Othman",
            "Ghadi Abderrahim",
            "El Bouhdidi Jaber"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "In few years, the internet has become the first source of information for most people, Today MOOCs makes it possible for everyone to access to the education over the world, it's represents an emerging methodology of online teaching and an important development in open education. Due to the rapid development of Moocs and the rapid growth of digital data and video database over the Internet, it is becoming very difficult for learners to browsing and choosing the best training for them. The scientific community has increased the amount of research into new technologies, with a view to improve the digital video utilization: its archiving, indexing, accessibility, acquisition, store and even its process and usability. All these parts of the video utilization entail the necessity of the extraction of all important information of a video, especially in cases of lack of metadata information. Web video mining is retrieving the content using data mining techniques from World Wide Web. There are two approaches for web video mining using traditional image processing (signal processing) and metadata based approach. In this paper, we present the various research makes in this subject and we propose an effective methodology to extract the metadata from Moocs videos and classify them based on the extracted metadata by applying data mining techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090450"
    },
    {
        "title": "Text mining with HathiTrust",
        "authors": [
            "Eleanor Dickson Koehl",
            "Ryan Dubnicek"
        ],
        "date": "June 2019",
        "source": "JCDL '19: Proceedings of the 18th Joint Conference on Digital Libraries",
        "abstract": "This tutorial will introduce attendees to the HathiTrust Research Center and its tools and services for computational text analysis research. HTRC leverages the scope and scale of the HathiTrust Digital Library collection to create opportunities for researchers to perform text data mining on subsets of the corpus. Attendees will develop skills that will allow them to conduct basic text analysis research using HathiTrust data, learn techniques for engaging with scholars on their text data mining research, and explore strategies for computational access to large-scale digital collections.",
        "link": "https://dl.acm.org/doi/10.1109/JCDL.2019.00115"
    },
    {
        "title": "Text mining for plagiarism detection: multivariate pattern detection for recognition of text similarities",
        "authors": [
            "Konstantinos Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2018",
        "source": "ASONAM '18: Proceedings of the 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "The problem of plagiarism the recent years has been intensified by the availability of information in digital form and the accessibility of the electronic libraries through the Internet. As a result, plagiarism detection has been transformed into a big data analytics problem since the number of digital sources is extravagant and a new document needs to be compared with millions of other existing documents. In this paper, a text mining methodology is proposed that can detect all common patterns between a document and the documents in a reference database. The technique is based on a pattern detection algorithm and the corresponding data structure that enables the algorithm to detect all common patterns. The methodology has been applied in a well-defined dataset providing very promising results identifying difficult cases of plagiarism such as technical disguise.",
        "link": "https://dl.acm.org/doi/10.5555/3382225.3382424"
    },
    {
        "title": "Petroleum Engineering Data Text Classification Using Convolutional Neural Network Based Classifier",
        "authors": [
            "Tzuhan Hsu",
            "Yaoqin Zhang"
        ],
        "date": "May 2018",
        "source": "ICMLT '18: Proceedings of the 2018 International Conference on Machine Learning Technologies",
        "abstract": "In this paper, authors introduce a convolutional neural network (CNN) based classification approach for petroleum engineering event. Recently, text classification problem has raised substantial attention in the field of machine learning. The aim of text classification process is to assign correct label to a text on the basis of its content. However, due to the imprecise of feature representation, most of former text classification methods cannot export satisfying result. To solve this problem, authors propose a model based on convolutional neural network. Authors conduct experiments with real world petroleum engineering data text, artificially labelled into 6 categories. Beside purposed CNN-based model, the authors also implement other baseline machine learning approaches: k nearest neighbor, random forest, support vector machines, recurrent neural network, gated recurrent units, long short-term memory. Among all these models, our CNN-based model reach best performance in text classification task. In the last part of this paper, authors compare and analysis their purpose and baseline model.",
        "link": "https://dl.acm.org/doi/10.1145/3231884.3231898"
    },
    {
        "title": "Classifying Indonesian Online Articles as Advertisement Placement Base Using Text Mining",
        "authors": [
            "Nadhira Tasya",
            "Arian Dhini"
        ],
        "date": "July 2017",
        "source": "ICBIM 2017: Proceedings of the International Conference on Business and Information Management",
        "abstract": "Rapid development in technological aspect resulting in growing level of human needs for the latest news, so that emerged a new trend of publishing and accessing news through online media or usually called online journalism. In addition, the number of people who sell and purchase through online sites also continues to increase and this opportunity is utilized by the company and the advertiser by implementing targeted web advertising. However, the high number of articles that have been published and accessed leads to great opportunities for errors in determining where to place the ads. Therefore, it needs a system that can categorize articles accessed by users as the basis of advertisement placement by the company and this classification system can be done by applying the method of Data Mining and Text Mining. This research uses document data in the form of article content that will be categorized into twenty categories of class of advertisement by using Text Mining technique with Support Vector Machine algorithm. The results of this study may be used by companies or advertisers as a basis for placement of ads on selected online media sites.",
        "link": "https://dl.acm.org/doi/10.1145/3134271.3134288"
    },
    {
        "title": "Mining Free-Text Medical Notes for Suicide Risk Assessment",
        "authors": [
            "Marios Adamou",
            "Grigoris Antoniou",
            "Elissavet Greasidou",
            "Vincenzo Lagani",
            "Paulos Charonyktakis",
            "Ioannis Tsamardinos"
        ],
        "date": "July 2018",
        "source": "SETN '18: Proceedings of the 10th Hellenic Conference on Artificial Intelligence",
        "abstract": "Suicide has been considered as an important public health issue for a very long time, and is one of the main causes of death worldwide. Despite suicide prevention strategies being applied, the rate of suicide has not changed substantially over the past decades. Advances in machine learning make it possible to attempt to predict suicide based on the analysis of relevant data to inform clinical practice. This paper reports on findings from the analysis of data of patients who died by suicide in the period 2013-2016 and made use of both structured data and free-text medical notes. We focus on examining various text-mining approaches to support risk assessment. The results show that using advance machine learning and text-mining techniques, it is possible to predict within a specified period which people are most at risk of taking their own life at the time of referral to a mental health service.",
        "link": "https://dl.acm.org/doi/10.1145/3200947.3201020"
    },
    {
        "title": "Integration of data mining techniques in e-learning systems: Clustering Profil of Lerners and Recommender Course System",
        "authors": [
            "A. El Moustamid",
            "E. En-Naimi",
            "J. El Bouhdidi"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "Data mining aims at the extraction of knowledge from large quantities of data by automatic or semi-automatic methods. In the field of education and training e-learning, Data Mining techniques as clustering (The aim of this technique is to identify groups that are similar in some aspect) [21], prediction (is used for developing model where combination of data viewed from different aspects can effect single viewed data. Here, is important to watch how predictors variables are effecting predicted variable)[21], classification are applied in e-learning to improve the level of learners by providing learning according to the learners' requirement) are used to analyze learners profile, to predict and improve student performance. The aim of our project is to develop a system capable of analyzing learners profile and indexing web videos in order to offer learners a rich database with courses that correspond to their levels.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090453"
    },
    {
        "title": "Multi-level mining and visualization of scientific text collections: Exploring a bi-lingual scientific repository",
        "authors": [
            "Pablo Accuosto",
            "Francesco Ronzano",
            "Daniel Ferrés",
            "Horacio Saggion"
        ],
        "date": "December 2017",
        "source": "WOSP 2017: Proceedings of the 6th International Workshop on Mining Scientific Publications",
        "abstract": "We present a system to mine and visualize collections of scientific documents by semantically browsing information extracted from single publications or aggregated throughout corpora of articles. The text mining tool performs deep analysis of document collections allowing the extraction and interpretation of research paper's contents. In addition to the extraction and enrichment of documents with metadata (titles, authors, affiliations, etc), the deep analysis performed comprises semantic interpretation, rhetorical analysis of sentences, triple-based information extraction, and text summarization. The visualization components allow geographical-based exploration of collections, topic-evolution interpretation, and collaborative network analysis among others. The paper presents a case study of a bi-lingual collection in the field of Natural Language Processing (NLP).",
        "link": "https://dl.acm.org/doi/10.1145/3127526.3127529"
    },
    {
        "title": "Hierarchical Topic Mining via Joint Spherical Tree and Text Embedding",
        "authors": [
            "Yu Meng",
            "Yunyi Zhang",
            "Jiaxin Huang",
            "Yu Zhang",
            "Chao Zhang",
            "Jiawei Han"
        ],
        "date": "August 2020",
        "source": "KDD '20: Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Mining a set of meaningful topics organized into a hierarchy is intuitively appealing since topic correlations are ubiquitous in massive text corpora. To account for potential hierarchical topic structures, hierarchical topic models generalize flat topic models by incorporating latent topic hierarchies into their generative modeling process. However, due to their purely unsupervised nature, the learned topic hierarchy often deviates from users' particular needs or interests. To guide the hierarchical topic discovery process with minimal user supervision, we propose a new task, Hierarchical Topic Mining, which takes a category tree described by category names only, and aims to mine a set of representative terms for each category from a text corpus to help a user comprehend his/her interested topics. We develop a novel joint tree and text embedding method along with a principled optimization procedure that allows simultaneous modeling of the category tree structure and the corpus generative process in the spherical space for effective category-representative term discovery. Our comprehensive experiments show that our model, named JoSH, mines a high-quality set of hierarchical topics with high efficiency and benefits weakly-supervised hierarchical text classification tasks.",
        "link": "https://dl.acm.org/doi/10.1145/3394486.3403242"
    },
    {
        "title": "Unsupervised behavioural mining and clustering for malware family identification",
        "authors": [
            "Khanh Huu The Dam",
            "Thomas Given-Wilson",
            "Axel Legay"
        ],
        "date": "March 2021",
        "source": "SAC '21: Proceedings of the 36th Annual ACM Symposium on Applied Computing",
        "abstract": "More accurate and advanced detection and classification of malware requires exploiting program behaviour and not merely syntactic or static features. One approach is to use system call dependency graphs (SCDGs) that represent the program behaviour by interactions with the system, and the relations between these interactions. These SCDGs have been used with supervised learning techniques to very accurately detect and classify malware. This works considers the unsupervised learning challenge of mining for common clusters of behaviour without a priori knowledge. This allows for clustering of similar programs by behaviour, that can then be used for either classification or further analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3412841.3441919"
    },
    {
        "title": "Text Mining for Evaluating Authors' Birth and Death Years",
        "authors": [
            "Dror Moghaz",
            "Yaakov Hacohen-Kerner",
            "Dov Gabbay"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "This article presents a unique method in text and data mining for finding the era, i.e., mining temporal data, in which an anonymous author was living. Finding this era can assist in the examination of a fake document or extracting the time period in which a writer lived. The study and the experiments concern Hebrew, and in some parts, Aramaic and Yiddish rabbinic texts. The rabbinic texts are undated and contain no bibliographic sections, posing an interesting challenge. This work proposes algorithms using key phrases and key words that allow the temporal organization of citations together with linguistic patterns. Based on these key phrases, key words, and the references, we established several types of “Iron-clad,” Heuristic and Greedy rules for estimating the years of birth and death of a writer in an interesting classification task. Experiments were conducted on corpora, including documents authored by 12, 24, and 36 rabbinic writers and demonstrated promising results.",
        "link": "https://dl.acm.org/doi/10.1145/3281631"
    },
    {
        "title": "Transfer Learning to Timed Text Based Video Classification Using CNN",
        "authors": [
            "Zenun Kastrati",
            "Ali Shariq Imran",
            "Arianit Kurti"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "Open educational video resources are gaining popularity with a growing number of massive open online courses (MOOCs). This has created a niche for content providers to adopt effective solutions in automatically organizing and structuring of educational resources for maximum visibility. Recent advances in deep learning techniques are proving useful in managing and classifying resources into appropriate categories. This paper proposes one such convolutional neural network (CNN) model for classifying video lectures in a MOOC setting using a transfer learning approach. The model uses a time-aligned text transcripts corresponding to video lectures from six broader subject categories. Video lectures and their corresponding transcript dataset is gathered from the Coursera MOOC platform. Two different CNN models are proposed: i) CNN based classification using embeddings learned from our MOOC dataset, ii) CNN based classification using transfer learning. Word embeddings generated from two well known state-of-the-art pre-trained models Word2Vec and GloVe, are used in the transfer learning approach for the second case. The proposed CNN models are evaluated using precision, recall, and F1 score and the obtained performance is compared with both conventional and deep learning classifiers. The proposed CNN models have an F1 score improvement of 10-22 percentage points over DNN and conventional classifiers",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326483"
    },
    {
        "title": "Evolutionary role mining in complex networks by ensemble clustering",
        "authors": [
            "Sarvenaz Choobdar",
            "Pedro Ribeiro",
            "Fernando Silva"
        ],
        "date": "April 2017",
        "source": "SAC '17: Proceedings of the Symposium on Applied Computing",
        "abstract": "The structural patterns in the neighborhood of nodes assign unique roles to the nodes. Mining the set of existing roles in a network provides a descriptive profile of the network and draws its general picture. This paper proposes a new method to determine structural roles in a dynamic network based on the current position of nodes and their historic behavior. We develop a temporal ensemble clustering technique to dynamically find groups of nodes, holding similar tempo-structural roles. We compare two weighting functions, based on age and distribution of data, to incorporate temporal behavior of nodes in the role discovery. To evaluate the performance of the proposed method, we assess the results from two points of view: 1) goodness of fit to current structure of the network; 2) consistency with historic data. We conduct the evaluation using different ensemble clustering techniques. The results on real world networks demonstrate that our method can detect tempo-structural roles that simultaneously depict the topology of a network and reflect its dynamics with high accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3019612.3019815"
    },
    {
        "title": "Analyzing used-car web listings via text mining",
        "authors": [
            "Ayhan Demiriz",
            "Fatma Cantaş"
        ],
        "date": "October 2017",
        "source": "IML '17: Proceedings of the 1st International Conference on Internet of Things and Machine Learning",
        "abstract": "Used car trade is one of the major components of the world economies. It is not uncommon to sell a car by placing an internet advertisement irrespective of the geography in these days. A typical content of an advertisement is usually composed of two parts namely the structured and the free text data. The structured data may include some information about the asking price, make, model, year, mileage of the car and the contact info. In most cases, seller may give important clues about the car's current conditions in the free text data where the title (head) of the advertisement can be included as free text too. This paper reports preliminary results from a text mining study conducted on 75K used car internet listings collected from two major car listing web sites in Turkey. As expected, the words and the phrases related to the description of the car are observed to be frequent. The leading concepts in the free text are found to be regarding how to describe the current condition of a car, for example \"no crash history\".",
        "link": "https://dl.acm.org/doi/10.1145/3109761.3109782"
    },
    {
        "title": "Text classification of Diseases Treated by Traditional Chinese Medicine Prescription based on machine learning",
        "authors": [
            "Hanqing Zhao",
            "Bo Han",
            "Chen Li"
        ],
        "date": "September 2020",
        "source": "ISAIMS '20: Proceedings of the 1st International Symposium on Artificial Intelligence in Medical Sciences",
        "abstract": "OBJECTIVE: To explore the application of machine learning in th e identification of diseases treated by traditional Chinese medicine prescriptions. METHODS: Based on the composition of the text document of Chinese medicine prescriptions, the prescriptions were divided into cough, headache and diarrhea. THUTCT were introduced to establish and train two machine learning text classification models, LibSVM and LibLinear, and the prescriptions to be deter mined were put into the model for classification and prediction. R ESULTS: The Precision rate of LibSVM model and Liblinear mod el were 0.7283 and 0.6690. Seven prescriptions were classified an d predicted, and the results were in line with expectations. CONCLUSION: THUCTC has good universality for the content of TCM prescriptions, high classification accuracy and fast testing speed, which is suitable for the text classification and discrimination research of TCM prescriptions for diseases.",
        "link": "https://dl.acm.org/doi/10.1145/3429889.3429895"
    },
    {
        "title": "Evaluation of retweet clustering method classification method using retweets on Twitter without text data",
        "authors": [
            "K. Uchida",
            "F. Toriumi",
            "T. Sakaki"
        ],
        "date": "August 2017",
        "source": "WI '17: Proceedings of the International Conference on Web Intelligence",
        "abstract": "Burst phenomena, which frequently occur on social media, are caused by such social events as flaming on the internet, elections, and natural disasters. To understand people's thoughts and feelings, we must classify their opinions from burst phenomena. Therefore, classification methods that categorize tweets are critical. However, since most classification methods focus on text mining, they cannot group tweets by topics because each tweet has poor linguistic similarities. We used a non-text-based classification method proposed by Baba et al. that groups tweets by topics, even if they have poor linguistic similarities, and verified its validity by comparing it with a text-based classification method in two different evaluations: qualitative and quantitative. In the qualitative evaluation part, we did a questionnaire survey and validated the suitability of the topic clusters created using both the non-and text-based methods. Since evaluating the similarity of every pair of tweets in each topic is difficult, we evaluated the similarity between sampled pairs in the survey and acquired more appropriate topic clustering results using the non-text-based method than the text-based method. In the quantitative evaluation part, we focused on the robustness of each method against data reduction. Many approaches analyze social media data, especially because collecting data from social media is comparatively easy. However, since collecting the whole data of burst phenomena is very costly due to the vast amounts of available social media data, robustness against data reduction is an important index to evaluate classification methods. With the non-text-based method, over 55% of the pairs of tweets in the same cluster were also included in the same cluster even when the data were reduced to 10% in all three of our example cases. In this paper, as a source we focus on Twitter, one of the most popular microblogging services. Using clustering to conduct detailed case analyses, we scrutinized three burst cases that include natural disasters and flaming on the internet and found that a non-text-based method more effectively classified tweets in burst phenomena than a text-based method.",
        "link": "https://dl.acm.org/doi/10.1145/3106426.3106451"
    },
    {
        "title": "Using Spark for Text Mining on Large Scale Liver Cancer Literature",
        "authors": [
            "Ming-Yen Lin",
            "Yu-Ju Lin",
            "Sue-Chen Hsueh"
        ],
        "date": "January 2021",
        "source": "BDET 2021: 2021 the 3rd International Conference on Big Data Engineering and Technology (BDET)",
        "abstract": "Cancer is one of the main causes of death. The number of known cancer to-date is more than one hundred. Liver cancer, ranked after trachea and lung cancer, has long been high in the cancer leading cause in Taiwan and even ranked as second in 2016. The number of scientific articles related to cancer proliferates every year. The number reaches as high as 20 million in PubMed so that discovering useful information from the massive collection is very difficult. In addition, using a single machine to sift through these articles is very time-consuming. Therefore, we present a big data analytic framework using the distributed Apache Spark platform for text mining in PubMed literature. We establish a prediction model for liver cancer articles so that researchers may effectively validate whether an article is related to liver cancer or not. Classification models in Spark MLlib including Linear Support Vector Machines (SVM), Logistic Regression, are used in our experiments. Relevancy to liver cancer is further confirmed by using MeSH (Medical Subject Headings) terms. Logistic regression is about 3 times faster than SVMs and the accuracy of both methods is close to 95% in the experiments using hold-out validation. When max_features is 500 and min_df ≤ 0.1 (or min_df = 1), the accuracy may reach 96%. In the experiments with K-fold cross-validation, the accuracy of SVMs methods is 96%. The experimental results show that that our prediction model may effectively classify liver cancer articles.",
        "link": "https://dl.acm.org/doi/10.1145/3474944.3474958"
    },
    {
        "title": "Automatic topic classification of test cases using text mining at an Android smartphone vendor",
        "authors": [
            "Junji Shimagaki",
            "Yasutaka Kamei",
            "Naoyasu Ubayashi",
            "Abram Hindle"
        ],
        "date": "October 2018",
        "source": "ESEM '18: Proceedings of the 12th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Background: An Android smartphone is an ecosystem of applications, drivers, operating system components, and assets. The volume of the software is large and the number of test cases needed to cover the functionality of an Android system is substantial. Enormous effort has been already taken to properly quantify \"what features and apps were tested and verified?\". This insight is provided by dashboards that summarize test coverage and results per feature. One method to achieve this is to manually tag or label test cases with the topic or function they cover, much like function points. At the studied Android smartphone vendor, tests are labelled with manually defined tags, so-called \"feature labels (FLs)\", and the FLs serve to categorize 100s to 1000s test cases into 10 to 50 groups. Aim: Unfortunately for developers, manual assignment of FLs to 1000s of test cases is a time consuming task, leading to inaccurately labeled test cases, which will render the dashboard useless. We created an automated system that suggests tags/labels to the developers for their test cases rather than manual labeling. Method: We use machine learning models to predict and label the functionality tested by 10,000 test cases developed at the company. Results: Through the quantitative experiments, our models achieved acceptable F-1 performance of 0.3 to 0.88. Also through the qualitative studies with expert teams, we showed that the hierarchy and path of tests was a good predictor of a feature's label. Conclusions: We find that this method can reduce tedious manual effort that software developers spent classifying test cases, while providing more accurate classification results.",
        "link": "https://dl.acm.org/doi/10.1145/3239235.3268927"
    },
    {
        "title": "To apply Data Mining for Classification of Crowd sourced Software Requirements",
        "authors": [
            "Soonh Taj",
            "Qasim Arain",
            "Imran Memon",
            "Asma Zubedi"
        ],
        "date": "April 2019",
        "source": "ICSIE '19: Proceedings of the 8th International Conference on Software and Information Engineering",
        "abstract": "Now a day's main focus of developers is to build quality software that works according to customer needs and for this reason it is necessary to gather right requirements as requirement elicitation is the critical step that impacts on the success of software project as misinterpreted requirements leads to the failure of software project. By keeping this in mind a research is carried out on improving requirements elicitation process and automating the process of classifying requirements. In this research, a model is proposed which will help in this scenario for requirements elicitation and requirement classification. This paper presents a model in which crowd sourcing approach is used so that customers, end users, stakeholders, developers and software engineers can make active participation for requirement elicitation process and requirements gathered using crowdsourcing approach are used by model for classification process i.e. classification of requirements into functional and non-functional requirements. For the proof of proposed model a case study is conducted. Results of case study provided the usefulness and efficiency of proposed model for classification of crowd sourced software requirements.",
        "link": "https://dl.acm.org/doi/10.1145/3328833.3328837"
    },
    {
        "title": "Text classification using Fuzzy TF-IDF and Machine Learning Models",
        "authors": [
            "Mariem Bounabi",
            "Karim El Moutaouakil",
            "Khalid Satori"
        ],
        "date": "October 2019",
        "source": "BDIoT'19: Proceedings of the 4th International Conference on Big Data and Internet of Things",
        "abstract": "The representation of the information has an important impact on the text classification task. Several weighting methods were proposed in the literature, and the term frequency-inverse term frequency (TFIDF), the most know on the text treatment field. The FTF-IDF is a vector representation where the components of the TFIDF are presented as inputs to the Fuzzy Inference System (FIS). In this work, we compare several Machin Learning algorithms such as Naïve Bayes and its derivatives, SVM and Random forest classifiers, using the FTF-IDF representation. To improve the quality of the used classifiers, we call sum attribute selection methods. The recognition rate, for the tested systems, is satisfied, where the system based on naïve Bayes classifier, the FTF-IDF weighting terms, and the info gain select attributes method gives 98.7% as accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3372938.3372956"
    },
    {
        "title": "Text Mining in Cybersecurity: A Systematic Literature Review",
        "authors": [
            "Luciano Ignaczak",
            "Guilherme Goldschmidt",
            "Cristiano André Da Costa",
            "Rodrigo Da Rosa Righi"
        ],
        "date": null,
        "source": "ACM Computing Surveys",
        "abstract": "The growth of data volume has changed cybersecurity activities, demanding a higher level of automation. In this new cybersecurity landscape, text mining emerged as an alternative to improve the efficiency of the activities involving unstructured data. This article proposes a Systematic Literature Review (SLR) to present the application of text mining in the cybersecurity domain. Using a systematic protocol, we identified 2,196 studies, out of which 83 were summarized. As a contribution, we propose a taxonomy to demonstrate the different activities in the cybersecurity domain supported by text mining. We also detail the strategies evaluated in the application of text mining tasks and the use of neural networks to support activities involving unstructured data. The work also discusses text classification performance aiming its application in real-world solutions. The SLR also highlights open gaps for future research, such as the analysis of non-English content and the intensification in the usage of neural networks.",
        "link": "https://dl.acm.org/doi/10.1145/3462477"
    },
    {
        "title": "Using Text Mining to Analyze the Financial Patents",
        "authors": [
            "Yung-Feng Lu",
            "Jia-Lang Xu",
            "Mu-Yen Chen"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "With the advancement and development of science and technology, Nowadays society is the development of information technology, cloud computing, mobile Internet, Internet of Things, block chain and information technology.The combination of finance and technology has made financial technology terms gradually emerging.. This research analyzes the patents of financial technology. However, Taiwan's financial technology patents are mostly G06Q-20 (payment plan, architecture or agreement), G06Q-30 (commercial, such as marketing, shopping, payment, auction or e-commerce). G06Q-40 (financial, investment or tax treatment, insurance).The research use text mining technology to analyze and understand Taiwan's trends and developments in financial technology. The research results show that G06Q-20 patents are biased towards information security and mobile payment. G06Q-30 patents favor shopping websites and advertising. G06Q-40 patents are biased towards diversity, such as securities, insurance, finance, and stocks.",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326478"
    },
    {
        "title": "Identifying Online Profiles of Distance Learning Students Using Data Mining Techniques",
        "authors": [
            "Osama Islam",
            "Muazzam Siddiqui",
            "Naif Radi Aljohani"
        ],
        "date": "October 2019",
        "source": "ICDTE '19: Proceedings of the 3rd International Conference on Digital Technology in Education",
        "abstract": "Educational data has grown over the years with the increased use of technology within educational environments. This has led to a huge amount of data being stored in various data sources representing the student, his/her activities, and other aspects relevant to the learning process. To meet this analytical need, Educational Data Mining (EDM) has emerged to assist educational institutions in identifying key benefits such as students at risk, the level of student engagement or predicting student performance. The aim of this research was to explore the various aspects of student interaction data using data mining techniques to identify relevant patterns of behaviors and possible key attributes that have higher degrees of influence on distance learning students. The main findings identified several patterns of user online profiles based on a set of adopted learning strategies, the research proposed a framework for analyzing such interaction data based on R and Hadoop platforms to correlate online profiles with student performance.",
        "link": "https://dl.acm.org/doi/10.1145/3369199.3369249"
    },
    {
        "title": "Extremely Fast Decision Tree Mining for Evolving Data Streams",
        "authors": [
            "Albert Bifet",
            "Jiajin Zhang",
            "Wei Fan",
            "Cheng He",
            "Jianfeng Zhang",
            "Jianfeng Qian",
            "Geoff Holmes",
            "Bernhard Pfahringer"
        ],
        "date": "August 2017",
        "source": "KDD '17: Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
        "abstract": "Nowadays real-time industrial applications are generating a huge amount of data continuously every day. To process these large data streams, we need fast and efficient methodologies and systems. A useful feature desired for data scientists and analysts is to have easy to visualize and understand machine learning models. Decision trees are preferred in many real-time applications for this reason, and also, because combined in an ensemble, they are one of the most powerful methods in machine learning. In this paper, we present a new system called STREAMDM-C++, that implements decision trees for data streams in C++, and that has been used extensively at Huawei. Streaming decision trees adapt to changes on streams, a huge advantage since standard decision trees are built using a snapshot of data, and can not evolve over time. STREAMDM-C++ is easy to extend, and contains more powerful ensemble methods, and a more efficient and easy to use adaptive decision trees. We compare our new implementation with VFML, the current state of the art implementation in C, and show how our new system outperforms VFML in speed using less resources.",
        "link": "https://dl.acm.org/doi/10.1145/3097983.3098139"
    },
    {
        "title": "Data miners' little helper: data transformation activity cues for cluster analysis on document collections",
        "authors": [
            "Tania Cerquitelli",
            "Evelina Di Corso",
            "Francesco Ventura",
            "Silvia Chiusano"
        ],
        "date": "June 2017",
        "source": "WIMS '17: Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "In this paper we propose a new self-learning engine to streamline the analytics process, as it enables analysts to mine massive data repositories with minimal user intervention. In the context of cluster analysis on a collection of documents this new system, named SELF-DATA (SELF-learning DAta TrAnsformation), suggests to the analyst how to configure the whole mining process for a given dataset. SELF-DATA relies on an engine exploring different data weighting schemas (e.g., normalized term frequencies) and data transformation methods (e.g., PCA) before applying the cluster analysis, evaluating and comparing solutions through different quality indices (e.g., weighted Silhouette), and presenting the k-top solutions to the analyst. SELF-DATA will also include a knowledge base storing results of experiments on previously processed datasets, and a classification algorithm trained on the knowledge base content to forecast the best configuration for the whole mining process for an unexplored dataset. The first development of SELF-DATA running on Apache Spark has been validated on 5 collections of documents. Experimental results highlight that TF-IDF and logarithmic entropy are effective to measure item relevance with sparse datasets, and the LSI method outperforms PCA with a large dictionary.",
        "link": "https://dl.acm.org/doi/10.1145/3102254.3102288"
    },
    {
        "title": "Interactive Visual Graph Mining and Learning",
        "authors": [
            "Ryan A. Rossi",
            "Nesreen K. Ahmed",
            "Rong Zhou",
            "Hoda Eldardiry"
        ],
        "date": null,
        "source": "ACM Transactions on Intelligent Systems and Technology",
        "abstract": "This article presents a platform for interactive graph mining and relational machine learning called GraphVis. The platform combines interactive visual representations with state-of-the-art graph mining and relational machine learning techniques to aid in revealing important insights quickly as well as learning an appropriate and highly predictive model for a particular task (e.g., classification, link prediction, discovering the roles of nodes, and finding influential nodes). Visual representations and interaction techniques and tools are developed for simple, fast, and intuitive real-time interactive exploration, mining, and modeling of graph data. In particular, we propose techniques for interactive relational learning (e.g., node/link classification), interactive link prediction and weighting, role discovery and community detection, higher-order network analysis (via graphlets, network motifs), among others. GraphVis also allows for the refinement and tuning of graph mining and relational learning methods for specific application domains and constraints via an end-to-end interactive visual analytic pipeline that learns, infers, and provides rapid interactive visualization with immediate feedback at each change/prediction in real-time. Other key aspects include interactive filtering, querying, ranking, manipulating, exporting, as well as tools for dynamic network analysis and visualization, interactive graph generators (including new block model approaches), and a variety of multi-level network analysis techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3200764"
    },
    {
        "title": "Data Mining and Opinion Mining: A Tool in Educational Context",
        "authors": [
            "Myriam Peñafiel",
            "Stefanie Vásquez",
            "Diego Vásquez",
            "Juan Zaldumbide",
            "Sergio Luján-Mora"
        ],
        "date": "July 2018",
        "source": "ICoMS '18: Proceedings of the 2018 1st International Conference on Mathematics and Statistics",
        "abstract": "The use of the web as a universal communication platform generates large volumes of data (Big data), which in many cases, need to be processed so that they can become useful knowledge in face of the sceptics who have doubts about the credibility of such information. The use of web data that comes from educational contexts needs to be addressed, since that large amount of unstructured information is not being valued, losing valuable information that can be used. To solve this problem, we propose the use of data mining techniques such as sentiment analysis to validate the information that comes from the educational platforms. The objective of this research is to propose a methodology that allows the user to apply sentiment analysis in a simple way, because although some researchers have done it, very few do with data in the educational context. The results obtained prove that the proposal can be used in similar cases.",
        "link": "https://dl.acm.org/doi/10.1145/3274250.3274263"
    },
    {
        "title": "Research Hotspots Mining and Visualized Analysis Based on Linking Cluster and K-Core Decomposition",
        "authors": [
            "Ying Cai",
            "Fang Huang",
            "Shuai Wang",
            "Hao Zhang",
            "Chunxiu Du"
        ],
        "date": "May 2018",
        "source": "ICDPA 2018: Proceedings of the International Conference on Data Processing and Applications",
        "abstract": "In this paper, we focus on research hotspots mining with knowledge semantic features on discipline topic word networks, which are extracted from scientific and technical documents. For the topic networks, a novel method of combining linking community clustering with k-core decomposition is proposed to mine the hotspot community with knowledge hierarchy structure. And as a post-processing, the hierarchical community identification is implemented by k-core decomposition in order to discover the internal connection with knowledge semantic hierarchy in the research hotspot. Finally, the community density and visualization method is utilized to analyze the core and sub- problem in the research hotspot community. In experiments, the topic networks for a single discipline and interdisciplinary research hotspot are taken as examples of analysis, and the results show the proposed approach can effectively realize the objective of mining and analyzing the discipline research hotspot on the academic topic word network.",
        "link": "https://dl.acm.org/doi/10.1145/3224207.3224216"
    },
    {
        "title": "The Application of Data Mining Algorithms in the Construction of Travel Recommendation System",
        "authors": [
            "Chao Lou",
            "Zhaonan Mu",
            "Mengzhu Liu"
        ],
        "date": "January 2021",
        "source": "CONF-CDS 2021: The 2nd International Conference on Computing and Data Science",
        "abstract": "Photos shared by users of online social network often associated with tags, texts, geographic and time information. These data are ideal research resource which can be used by researchers to observe people's interests and behaviors. They can also be used to analyze people's travel behaviors and interests because it can reflect people's travel activities and experiences directly. This project aimed to consolidate people into groups based on their travel preferences of travel season, travel time span, scenery type of travel location and then analyze popular travel places for different people groups. The travel preferences were analyzed from extracted data applying PHP language and Naive Bayes classification approach. According to the analyzed data, people were divided into groups. Then, DBSCAN clustering was used to get popular locations based on the different groups. As a result, 80 thousand photo data of 5 cities have been collected from Flickr by Phython. It has been showed that people's preference of season in most of the mined cities was spring and summer and the preference of time span was short. Moreover, mined people has been divided into 16 groups according to their preference and based on the groups popular locations have been calculated by DBSCAN clustering. Theses locations have been provided in a recommendation website which was built in this project. For the future work, mining Flickr data associated with other travel social network sites could be a good way to build a more effective recommendation system.",
        "link": "https://dl.acm.org/doi/10.1145/3448734.3450477"
    },
    {
        "title": "Compare Machine Learning Models in Text Classification Using Steam User Reviews",
        "authors": [
            "Youchen Miao",
            "Zeyu Jin",
            "Yumeng Zhang",
            "Yuchen Chen",
            "Junren Lai"
        ],
        "date": "November 2021",
        "source": "ICSED '21: Proceedings of the 2021 3rd International Conference on Software Engineering and Development",
        "abstract": "Text Classification and Sentiment Analysis of game reviews are viewed as important parts in not only academic fields but also in game studies. In this paper, with more than 400 thousand game reviews on Steam platform, we preprocess the data using different libraries (sklearn, nltk, and spaCy) and use them as inputs to build three sentiment classification models based on different algorithms (Naive Bayes, SVM, and Random Forest). In contrast to previous studies that only focus on different sentiment analysis models, our paper also highlights the use of different APIs to preprocess the data and their corresponding model performance. The results show that no matter which API we choose, Random Forest models always perform the best. However, in terms of training time, Naive Bayes is the fastest. This work can be used to apply grid search for researchers to automatically find the optimum API before conducting sentiment analysis in the future.",
        "link": "https://dl.acm.org/doi/10.1145/3507473.3507480"
    },
    {
        "title": "Lost in Transduction: Transductive Transfer Learning in Text Classification",
        "authors": [
            "Alejandro Moreo",
            "Andrea Esuli",
            "Fabrizio Sebastiani"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Obtaining high-quality labelled data for training a classifier in a new application domain is often costly. Transfer Learning\n(a.k.a. “Inductive Transfer”) tries to alleviate these costs by transferring, to the “target” domain of interest, knowledge available from a different “source” domain. In transfer learning the lack of labelled information from the target domain is compensated by the availability at training time of a set of unlabelled examples from the target distribution. Transductive Transfer Learning denotes the transfer learning setting in which the only set of target documents that we are interested in classifying is known and available at training time. Although this definition is indeed in line with Vapnik’s original definition of “transduction”, current terminology in the field is confused. In this article, we discuss how the term “transduction” has been misused in the transfer learning literature, and propose a clarification consistent with the original characterization of this term given by Vapnik. We go on to observe that the above terminology misuse has brought about misleading experimental comparisons, with inductive transfer learning methods that have been incorrectly compared with transductive transfer learning methods. We then, give empirical evidence that the difference in performance between the inductive version and the transductive version of a transfer learning method can indeed be statistically significant (i.e., that knowing at training time the only data one needs to classify indeed gives an advantage). Our clarification allows a reassessment of the field, and of the relative merits of the major, state-of-the-art algorithms for transfer learning in text classification.",
        "link": "https://dl.acm.org/doi/10.1145/3453146"
    },
    {
        "title": "Machine Learning on Mining Potential Adverse Drug Reactions for Pharmacovigilance",
        "authors": [
            "Ke Jia"
        ],
        "date": "July 2021",
        "source": "DSIT 2021: 2021 4th International Conference on Data Science and Information Technology",
        "abstract": "Drugs are related to the lives and health of patients. Mining and discovering potential adverse drug reactions from a large number of unstructured texts play an important role in drug research and pharmacovigilance. It is a hot research issue in the field of biomedicine in recent years. A large number of scholars have discovered a large amount of information on adverse drug reactions from different data sources through effective machine learning methods. This paper reviews the research progress in recent years to provide a reference for future research.",
        "link": "https://dl.acm.org/doi/10.1145/3478905.3478964"
    },
    {
        "title": "TACAM: Topic And Context Aware Argument Mining",
        "authors": [
            "Michael Fromm",
            "Evgeniy Faerman",
            "Thomas Seidl"
        ],
        "date": "October 2019",
        "source": "WI '19: IEEE/WIC/ACM International Conference on Web Intelligence",
        "abstract": "In this work we address the problem of argument search. The purpose of argument search is the distillation of pro and contra arguments for requested topics from large text corpora. In previous works, the usual approach is to use a standard search engine to extract text parts which are relevant to the given topic and subsequently use an argument recognition algorithm to select arguments from them. The main challenge in the argument recognition task, which is also known as argument mining, is that often sentences containing arguments are structurally similar to purely informative sentences without any stance about the topic. In fact, they only differ semantically. Most approaches use topic or search term information only for the first search step and therefore assume that arguments can be classified independently of a topic. We argue that topic information is crucial for argument mining, since the topic defines the semantic context of an argument. Precisely, we propose different models for the classification of arguments, which take information about a topic of an argument into account. Moreover, to enrich the context of a topic and to let models understand the context of the potential argument better, we integrate information from different external sources such as Knowledge Graphs or pre-trained NLP models. Our evaluation shows that considering topic information, especially in connection with external information, provides a significant performance boost for the argument mining task.",
        "link": "https://dl.acm.org/doi/10.1145/3350546.3352506"
    },
    {
        "title": "Text mining for incoming tasks based on the urgency/importance factors and task classification using machine learning tools",
        "authors": [
            "Yasser Ali Alshehri"
        ],
        "date": "March 2020",
        "source": "ICCDA 2020: Proceedings of the 2020 the 4th International Conference on Compute and Data Analysis",
        "abstract": "In workplaces, there is a massive amount of unstructured data from different sources. In this paper, we present a case study that explains how can through communications between employees, we can help to prioritize tasks requests to increase the efficiency of their works for both technical and non-technical workers. This involves managing daily incoming tasks based on their level of urgency and importance.To allow all workers to utilize the urgency-importance matrix as a time-management tool, we need to automate this tool. The textual content of incoming tasks are analyzed, and metrics related to urgency and importance are extracted. A third factor (i.e., the response variable) is defined based on the two input variables (urgency and importance). Then, machine learning applied to the data to predict the class of incoming tasks based on data outcome desired. We used ordinal regression, neural networks, and decision tree algorithms to predict the four levels of task priority. We measure the performance of all using recalls, precisions, and F-scores. All classifiers perform higher than 89% in terms of all measures.",
        "link": "https://dl.acm.org/doi/10.1145/3388142.3388153"
    },
    {
        "title": "Text mining for malware classification using multivariate all repeated patterns detection",
        "authors": [
            "Konstantinos F. Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2019",
        "source": "ASONAM '19: Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Mobile phones have become nowadays a commodity to the majority of people. Using them, people are able to access the world of Internet and connect with their friends, their colleagues at work or even unknown people with common interests. This proliferation of the mobile devices has also been seen as an opportunity for the cyber criminals to deceive smartphone users and steel their money directly or indirectly, respectively, by accessing their bank accounts through the smartphones or by blackmailing them or selling their private data such as photos, credit card data, etc. to third parties. This is usually achieved by installing malware to smartphones masking their malevolent payload as a legitimate application and advertise it to the users with the hope that mobile users will install it in their devices. Thus, any existing application can easily be modified by integrating a malware and then presented it as a legitimate one. In response to this, scientists have proposed a number of malware detection and classification methods using a variety of techniques. Even though, several of them achieve relatively high precision in malware classification, there is still space for improvement. In this paper, we propose a text mining all repeated pattern detection method which uses the decompiled files of an application in order to classify a suspicious application into one of the known malware families. Based on the experimental results using a real malware dataset, the methodology tries to correctly classify (without any misclassification) all randomly selected malware applications of 3 categories with 3 different families each.",
        "link": "https://dl.acm.org/doi/10.1145/3341161.3350841"
    },
    {
        "title": "Comparative Study between Traditional Machine Learning and Deep Learning Approaches for Text Classification",
        "authors": [
            "Cannannore Nidhi Kamath",
            "Syed Saqib Bukhari",
            "Andreas Dengel"
        ],
        "date": "August 2018",
        "source": "DocEng '18: Proceedings of the ACM Symposium on Document Engineering 2018",
        "abstract": "In this contemporaneous world, it is an obligation for any organization working with documents to end up with the insipid task of classifying truckload of documents, which is the nascent stage of venturing into the realm of information retrieval and data mining. But classification of such humongous documents into multiple classes, calls for a lot of time and labor. Hence a system which could classify these documents with acceptable accuracy would be of an unfathomable help in document engineering. We have created multiple classifiers for document classification and compared their accuracy on raw and processed data. We have garnered data used in a corporate organization as well as publicly available data for comparison. Data is processed by removing the stop-words and stemming is implemented to produce root words. Multiple traditional machine learning techniques like Naive Bayes, Logistic Regression, Support Vector Machine, Random forest Classifier and Multi-Layer Perceptron are used for classification of documents. Classifiers are applied on raw and processed data separately and their accuracy is noted. Along with this, Deep learning technique such as Convolution Neural Network is also used to classify the data and its accuracy is compared with that of traditional machine learning techniques. We are also exploring hierarchical classifiers for classification of classes and subclasses. The system classifies the data faster and with better accuracy than if done manually. The results are discussed in the results and evaluation section.",
        "link": "https://dl.acm.org/doi/10.1145/3209280.3209526"
    },
    {
        "title": "Identification of Overpricing in the Purchase of Medication by the Federal Government of Brazil, Using Text Mining and Clustering Based on Ontology",
        "authors": [
            "Marco Aurelio O. S. Correa",
            "Adriano Galindo Leal"
        ],
        "date": "August 2018",
        "source": "ICCBDC '18: Proceedings of the 2018 2nd International Conference on Cloud and Big Data Computing",
        "abstract": "Increasing the transparency level in his actions and spending is one of the primary duties of the Brazilian Federal Government. The creation of laws that oblige full disclose of all its expenditures through transparency portals enables citizens to supervise all government entities. However, only the dissemination of these data, without a definite standard or the availability of data analysis tools, does not guarantee that the citizen is empowered to play his role. Therefore hence, the objective of this work is to identify overprice in the acquisition of products purchased by the federal government of Brazil using the unstructured data available on the Transparency Portal. The last two-years' worth of purchasing data, available in the Transparency Portal, were extracted, processed and stored. Due to his diverse nature and high volume of data, this study focused only on medicines purchased by the Ministry of Health. Ontology-based text mining and clustering techniques were applied for automatic identification and classification of products. The processing of this information was done through text mining and clustering, based on the ontology registered in another database of the Brazilian government. Because of this work, a consolidated price base per medication was created to allow the identification of distortions in prices practised, facilitating the identification of cases that merit further investigation to unravel fraud to the treasury.",
        "link": "https://dl.acm.org/doi/10.1145/3264560.3264569"
    },
    {
        "title": "Computational Estimation by Scientific Data Mining with Classical Methods to Automate Learning Strategies of Scientists",
        "authors": [
            "Aparna S. Varde"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Experimental results are often plotted as 2-dimensional graphical plots (aka graphs) in scientific domains depicting dependent versus independent variables to aid visual analysis of processes. Repeatedly performing laboratory experiments consumes significant time and resources, motivating the need for computational estimation. The goals are to estimate the graph obtained in an experiment given its input conditions, and to estimate the conditions that would lead to a desired graph. Existing estimation approaches often do not meet accuracy and efficiency needs of targeted applications. We develop a computational estimation approach called AutoDomainMine that integrates clustering and classification over complex scientific data in a framework so as to automate classical learning methods of scientists. Knowledge discovered thereby from a database of existing experiments serves as the basis for estimation. Challenges include preserving domain semantics in clustering, finding matching strategies in classification, striking a good balance between elaboration and conciseness while displaying estimation results based on needs of targeted users, and deriving objective measures to capture subjective user interests. These and other challenges are addressed in this work. The AutoDomainMine approach is used to build a computational estimation system, rigorously evaluated with real data in Materials Science. Our evaluation confirms that AutoDomainMine provides desired accuracy and efficiency in computational estimation. It is extendable to other science and engineering domains as proved by adaptation of its sub-processes within fields such as Bioinformatics and Nanotechnology.",
        "link": "https://dl.acm.org/doi/10.1145/3502736"
    },
    {
        "title": "Where is the road for issue reports classification based on text mining?",
        "authors": [
            "Qiang Fan",
            "Yue Yu",
            "Gang Yin",
            "Tao Wang",
            "Huaimin Wang"
        ],
        "date": "November 2017",
        "source": "ESEM '17: Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Currently, open source projects receive various kinds of issues daily, because of the extreme openness of Issue Tracking System (ITS) in GitHub. ITS is a labor-intensive and time-consuming task of issue categorization for project managers. However, a contributor is only required a short textual abstract to report an issue in GitHub. Thus, most traditional classification approaches based on detailed and structured data (e.g., priority, severity, software version and so on) are difficult to adopt. In this paper, issue classification approaches on a large-scale dataset, including 80 popular projects and over 252,000 issue reports collected from GitHub, were investigated. First, four traditional text-based classification methods and their performances were discussed. Semantic perplexity (i.e., an issues description confuses bug-related sentences with nonbug-related sentences) is a crucial factor that affects the classification performances based on quantitative and qualitative study. Finally, A two-stage classifier framework based on the novel metrics of semantic perplexity of issue reports was designed. Results show that our two-stage classification can significantly improve issue classification performances.",
        "link": "https://dl.acm.org/doi/10.1109/ESEM.2017.19"
    },
    {
        "title": "Feature-based Facebook reviews process model for e-management using data mining",
        "authors": [
            "Anish Kumar Varudharajulu",
            "Yongsheng Ma"
        ],
        "date": "January 2019",
        "source": "IC4E '19: Proceedings of the 10th International Conference on E-Education, E-Business, E-Management and E-Learning",
        "abstract": "The data generated from online communication acts as potential gold mines for discovering knowledge for researchers. A large amount of data is also generated in the form of web documents, emails, blogs, and feedback, etc. Text analytics is being significantly employed to mine important information. Opinion mining is the process of extracting human thoughts and perceptions from unstructured texts. The showstopper for designing an opinion mining system for analyzing reviews arise from the fact that customer reviews are often noisy. These reviews are informally written. In addition, they are subjected to spelling mistakes, grammatical errors, improper punctuation and irrational capitalization. This paper focuses on analyzing the different classification and clustering algorithms aimed at extracting and consolidating opinions of customers from social media sites like Facebook, Twitter and through surveys, at multiple levels of granularity to monitor and measure customer satisfaction. Ours is an automated approach, in which the system aids in the process of knowledge assimilation for knowledge-based building and also performs the analytics. Domain experts ratify the knowledge base and also provide training datasets for the system to intuitively gather more instances for ratification. The system identifies opinion expressions as phrases containing opinion words, opinionated features and also opinion modifiers. These expressions are categorized as positive, negative or neutral. Opinion expressions are identified and categorized using localized linguistic techniques. Opinions can be congregated at any desired level of specificity i.e. feature level or product level, user level or service level, etc. We have developed a system based on this approach, which provides the user with a platform to analyze opinion expressions crawled from a set of pre-defined datasets.",
        "link": "https://dl.acm.org/doi/10.1145/3306500.3306514"
    },
    {
        "title": "Discriminative Topic Mining via Category-Name Guided Text Embedding",
        "authors": [
            "Yu Meng",
            "Jiaxin Huang",
            "Guangyuan Wang",
            "Zihan Wang",
            "Chao Zhang",
            "Yu Zhang",
            "Jiawei Han"
        ],
        "date": "April 2020",
        "source": "WWW '20: Proceedings of The Web Conference 2020",
        "abstract": "Mining a set of meaningful and distinctive topics automatically from massive text corpora has broad applications. Existing topic models, however, typically work in a purely unsupervised way, which often generate topics that do not fit users’ particular needs and yield suboptimal performance on downstream tasks. We propose a new task, discriminative topic mining, which leverages a set of user-provided category names to mine discriminative topics from text corpora. This new task not only helps a user understand clearly and distinctively the topics he/she is most interested in, but also benefits directly keyword-driven classification tasks. We develop CatE, a novel category-name guided text embedding method for discriminative topic mining, which effectively leverages minimal user guidance to learn a discriminative embedding space and discover category representative terms in an iterative manner. We conduct a comprehensive set of experiments to show that CatE mines high-quality set of topics guided by category names only, and benefits a variety of downstream applications including weakly-supervised classification and lexical entailment direction identification.",
        "link": "https://dl.acm.org/doi/10.1145/3366423.3380278"
    },
    {
        "title": "Comparative Study of Heart Disease Diagnosis Using Top Ten Data Mining Classification Algorithms",
        "authors": [
            "I. Ketut Agung Enriko"
        ],
        "date": "June 2019",
        "source": "ICFET '19: Proceedings of the 5th International Conference on Frontiers of Educational Technologies",
        "abstract": "Data mining has been used for many purposes, especially for prediction system. In healthcare, data mining algorithms often used in disease diagnosis. Meanwhile, heart disease is known as a primary cause of death over the years. Many studies have been performed in heart disease diagnosis using data mining methods. There are some popular data mining algorithms that can be used in heart disease diagnosis, for example, k-Nearest Neighbor, CART, and AdaBoost. The algorithms are used to analyze a sample of cardiovascular patients data and predict the heart disease type that they suffer. Some parameters are taken from the patient, including EKG morphology, blood pressure, and information about the existence of chest pain, shortness of breath, palpitation, and cold sweat. In this study, medical records data are collected from Harapan Kita Hospital and utilized as a dataset sample in this research. Top ten data mining classification algorithms are used in diagnosing heart disease from Harapan Kita Hospital data and examining their performance by checking the accuracy and speed.",
        "link": "https://dl.acm.org/doi/10.1145/3338188.3338220"
    },
    {
        "title": "Text Mining Approach for Identifying Research Trends",
        "authors": [
            "Snezhana Sulova"
        ],
        "date": "June 2021",
        "source": "CompSysTech '21: Proceedings of the 22nd International Conference on Computer Systems and Technologies",
        "abstract": "With the increase of unstructured data, the issues connected with automatic text processing, the categorization of documents and the discovery of topics have become objects of growing interest. In order to improve the process of grouping and processing research publications, we would like to propose a method based upon natural language processing. It is based on text mining technologies which aim to identify key tendencies in documents. It processes the content of publications by clustering and identifies the topics of each identified group. This analysis helps by identifying key tendencies as well as discovering emerging new areas of research. Publications from the research literature database, Scopus, were used to test the approach. The topic of the publications is “the application of digital technologies in the logistics business”. The experiments were completed using the RapidMiner Studio software.",
        "link": "https://dl.acm.org/doi/10.1145/3472410.3472433"
    },
    {
        "title": "Assessment of Vulnerability Severity using Text Mining",
        "authors": [
            "Georgios Spanos",
            "Lefteris Angelis",
            "Dimitrios Toloudis"
        ],
        "date": "September 2017",
        "source": "PCI '17: Proceedings of the 21st Pan-Hellenic Conference on Informatics",
        "abstract": "Software1 vulnerabilities are closely associated with information systems security, a major and critical field in today's technology. Vulnerabilities constitute a constant and increasing threat for various aspects of everyday life, especially for safety and economy, since the social impact from the problems that they cause is complicated and often unpredictable. Although there is an entire research branch in software engineering that deals with the identification and elimination of vulnerabilities, the growing complexity of software products and the variability of software production procedures are factors contributing to the ongoing occurrence of vulnerabilities, Hence, another area that is being developed in parallel focuses on the study and management of the vulnerabilities that have already been reported and registered in databases. The information contained in such databases includes, a textual description and a number of metrics related to vulnerabilities. The purpose of this paper is to investigate to what extend the assessment of the vulnerability severity can be inferred directly from the corresponding textual description, or in other words, to examine the informative power of the description with respect to the vulnerability severity. For this purpose, text mining techniques, i.e. text analysis and three different classification methods (decision trees, neural networks and support vector machines) were employed. The application of text mining to a sample of 70,678 vulnerabilities from a public data source shows that the description itself is a reliable and highly accurate source of information for vulnerability prioritization.",
        "link": "https://dl.acm.org/doi/10.1145/3139367.3139390"
    },
    {
        "title": "Clustering Algorithms for Spatial Data Mining",
        "authors": [
            "Chetashri Bhadane",
            "Ketan Shah"
        ],
        "date": "April 2020",
        "source": "ICGDA '20: Proceedings of the 2020 3rd International Conference on Geoinformatics and Data Analysis",
        "abstract": "With the advances in mobile and wireless technologies, there has been a rise in applications that track and share the users' geospatial data. People use several social networking sites such as Twitter, Facebook and Flickr, where they share their status updates. With the integration of Global Positioning System (GPS) with mobile phones, it is now possible to share one's locations on these social networks. GPS allows us to record and track a person's movement along with the timestamp. The data set obtained from these GPS logs is vast and is widely used to analyze the users' movement patterns. Specifically, we can find out significant locations based on the number of users present at that location and the time spent by them at such places. Once significant places have been identified, it is also possible to identify the semantic importance of these locations. This paper presents an overview of the clustering techniques used to find important places of interest using large GPS based mobility datasets. Four clustering algorithms, K-Means, DBSCAN, OPTICS and Hierarchical, are implemented, and performance is tested using real-time data of 50 users collected over 2--5 years. Performance summary depicts that K-Means and DBSCAN perform well for spatial data.",
        "link": "https://dl.acm.org/doi/10.1145/3397056.3397068"
    },
    {
        "title": "Feature Selection based Arabic Text Classification using Different Machine Learning Algorithms: Comparative Study",
        "authors": [
            "Sakina Rim Bennabi",
            "Zakaria Elberrichi"
        ],
        "date": "June 2020",
        "source": "ICIST '20: Proceedings of the 10th International Conference on Information Systems and Technologies",
        "abstract": "Feature selection is a method of data pre-processing widely used when mining large data, such as textual classification. Several studies have been conducted to compare the different methods of feature selection applied to corpora in English. Unfortunately, a small number of works concern the Arabic language. This article aims to present a comparative study of different feature selection techniques including: Chi2, the ANOVA method and mutual information, applied on a corpus in Arabic language, while also diversifying the machine learning algorithms (Naive Bayes, SVM and KNN). This experimental study has shown in general that reducing dimensionality with feature selection techniques has slightly affected the performance of textual classification, reducing the size of the corpus by up to 1%.",
        "link": "https://dl.acm.org/doi/10.1145/3447568.3448531"
    },
    {
        "title": "Intelligent Data Analysis using Optimized Support Vector Machine Based Data Mining Approach for Tourism Industry",
        "authors": [
            "Ms Promila Sharma",
            "Uma Meena",
            "Girish Kumar Sharma"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Data analysis involves the deployment of sophisticated approaches from data mining methods, information theory, and artificial intelligence in various fields like tourism, hospitality, and so on for the extraction of knowledge from the gathered and preprocessed data. In tourism, pattern analysis or data analysis using classification is significant for finding the patterns that represent new and potentially useful information or knowledge about the destination and other data. Several data mining techniques are introduced for the classification of data or patterns. However, overfitting, less accuracy, local minima, sensitive to noise are the drawbacks in some existing data mining classification methods. To overcome these challenges, Support vector machine with Red deer optimization (SVM-RDO) based data mining strategy is proposed in this article. Extended Kalman filter (EKF) is utilized in the first phase, i.e., data cleaning to remove the noise and missing values from the input data. Mantaray foraging algorithm (MaFA) is used in the data selection phase, in which the significant data are selected for the further process to reduce the computational complexity. The final phase is the classification, in which SVM-RDO is proposed to access the useful pattern from the selected data. PYTHON is the implementation tool used for the experiment of the proposed model. The experimental analysis is done to show the efficacy of the proposed work. From the experimental results, the proposed SVM-RDO achieved better accuracy, precision, recall, and F1 score than the existing methods for the tourism dataset. Thus, it is showed the effectiveness of the proposed SVM-RDO for pattern analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3494566"
    },
    {
        "title": "Applying text mining to predict learners' cognitive engagement",
        "authors": [
            "Hayati Hind",
            "Mohammed Khalidi Idrissi",
            "Samir Bennani"
        ],
        "date": "October 2017",
        "source": "SCAMS '17: Proceedings of the Mediterranean Symposium on Smart City Application",
        "abstract": "In academic process, engagement represent one of the important success' factor. This paper proposes a predictive system for learners' cognitive engagement based on their online discussion forums participation. In the first level the ontology OWL and the LSA method are used to perform a semantic classification system of the threads according to a specific context chosen by the tutor. Then the Text mining, as a predictive method, is applied to the classified threads and learners' participation in the forums.",
        "link": "https://dl.acm.org/doi/10.1145/3175628.3175655"
    },
    {
        "title": "Investigate Transitions into Drug Addiction through Text Mining of Reddit Data",
        "authors": [
            "John Lu",
            "Sumati Sridhar",
            "Ritika Pandey",
            "Mohammad Al Hasan",
            "Georege Mohler"
        ],
        "date": "July 2019",
        "source": "KDD '19: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Increasing rates of opioid drug abuse and heightened prevalence of online support communities underscore the necessity of employing data mining techniques to better understand drug addiction using these rapidly developing online resources. In this work, we obtained data from Reddit, an online collection of forums, to gather insight into drug use/misuse using text snippets from users narratives. Specifically, using users' posts, we trained a binary classifier which predicts a user's transitions from casual drug discussion forums to drug recovery forums. We also proposed a Cox regression model that outputs likelihoods of such transitions. In doing so, we found that utterances of select drugs and certain linguistic features contained in one's posts can help predict these transitions. Using unfiltered drug-related posts, our research delineates drugs that are associated with higher rates of transitions from recreational drug discussion to support/recovery discussion, offers insight into modern drug culture, and provides tools with potential applications in combating the opioid crisis.",
        "link": "https://dl.acm.org/doi/10.1145/3292500.3330737"
    },
    {
        "title": "Analysing digital banking reviews using text mining",
        "authors": [
            "Li Chen Cheng",
            "Legaspi Rhea Sharmayne"
        ],
        "date": "December 2020",
        "source": "ASONAM '20: Proceedings of the 12th IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Digital banks are new entrants in the banking industry in the Philippines as they only started late 2018. Since then, a handful of players have and are still emerging. With more and more people becoming technologically savvy, it is very critical for financial institutions to develop a digital banking application that will stand out from the competition. This paper aims to use text mining methods to analyse digital banking application reviews. This study will perform topic modelling using LDA to explore customer concerns and will mine association rules between the digital banking features with the review score. The results will reveal which areas the digital banking application can further optimize for customer satisfaction and retention.",
        "link": "https://dl.acm.org/doi/10.1109/ASONAM49781.2020.9381429"
    },
    {
        "title": "Mining Moocs Videos Metadata Using Classification Techniques",
        "authors": [
            "El Harrak Othman",
            "Ghadi Abderrahim",
            "El Bouhdidi Jaber"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "In few years, the internet has become the first source of information for most people, Today MOOCs makes it possible for everyone to access to the education over the world, it's represents an emerging methodology of online teaching and an important development in open education. Due to the rapid development of Moocs and the rapid growth of digital data and video database over the Internet, it is becoming very difficult for learners to browsing and choosing the best training for them. The scientific community has increased the amount of research into new technologies, with a view to improve the digital video utilization: its archiving, indexing, accessibility, acquisition, store and even its process and usability. All these parts of the video utilization entail the necessity of the extraction of all important information of a video, especially in cases of lack of metadata information. Web video mining is retrieving the content using data mining techniques from World Wide Web. There are two approaches for web video mining using traditional image processing (signal processing) and metadata based approach. In this paper, we present the various research makes in this subject and we propose an effective methodology to extract the metadata from Moocs videos and classify them based on the extracted metadata by applying data mining techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090450"
    },
    {
        "title": "Text mining with HathiTrust",
        "authors": [
            "Eleanor Dickson Koehl",
            "Ryan Dubnicek"
        ],
        "date": "June 2019",
        "source": "JCDL '19: Proceedings of the 18th Joint Conference on Digital Libraries",
        "abstract": "This tutorial will introduce attendees to the HathiTrust Research Center and its tools and services for computational text analysis research. HTRC leverages the scope and scale of the HathiTrust Digital Library collection to create opportunities for researchers to perform text data mining on subsets of the corpus. Attendees will develop skills that will allow them to conduct basic text analysis research using HathiTrust data, learn techniques for engaging with scholars on their text data mining research, and explore strategies for computational access to large-scale digital collections.",
        "link": "https://dl.acm.org/doi/10.1109/JCDL.2019.00115"
    },
    {
        "title": "Text mining for plagiarism detection: multivariate pattern detection for recognition of text similarities",
        "authors": [
            "Konstantinos Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2018",
        "source": "ASONAM '18: Proceedings of the 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "The problem of plagiarism the recent years has been intensified by the availability of information in digital form and the accessibility of the electronic libraries through the Internet. As a result, plagiarism detection has been transformed into a big data analytics problem since the number of digital sources is extravagant and a new document needs to be compared with millions of other existing documents. In this paper, a text mining methodology is proposed that can detect all common patterns between a document and the documents in a reference database. The technique is based on a pattern detection algorithm and the corresponding data structure that enables the algorithm to detect all common patterns. The methodology has been applied in a well-defined dataset providing very promising results identifying difficult cases of plagiarism such as technical disguise.",
        "link": "https://dl.acm.org/doi/10.5555/3382225.3382424"
    },
    {
        "title": "Petroleum Engineering Data Text Classification Using Convolutional Neural Network Based Classifier",
        "authors": [
            "Tzuhan Hsu",
            "Yaoqin Zhang"
        ],
        "date": "May 2018",
        "source": "ICMLT '18: Proceedings of the 2018 International Conference on Machine Learning Technologies",
        "abstract": "In this paper, authors introduce a convolutional neural network (CNN) based classification approach for petroleum engineering event. Recently, text classification problem has raised substantial attention in the field of machine learning. The aim of text classification process is to assign correct label to a text on the basis of its content. However, due to the imprecise of feature representation, most of former text classification methods cannot export satisfying result. To solve this problem, authors propose a model based on convolutional neural network. Authors conduct experiments with real world petroleum engineering data text, artificially labelled into 6 categories. Beside purposed CNN-based model, the authors also implement other baseline machine learning approaches: k nearest neighbor, random forest, support vector machines, recurrent neural network, gated recurrent units, long short-term memory. Among all these models, our CNN-based model reach best performance in text classification task. In the last part of this paper, authors compare and analysis their purpose and baseline model.",
        "link": "https://dl.acm.org/doi/10.1145/3231884.3231898"
    },
    {
        "title": "Classifying Indonesian Online Articles as Advertisement Placement Base Using Text Mining",
        "authors": [
            "Nadhira Tasya",
            "Arian Dhini"
        ],
        "date": "July 2017",
        "source": "ICBIM 2017: Proceedings of the International Conference on Business and Information Management",
        "abstract": "Rapid development in technological aspect resulting in growing level of human needs for the latest news, so that emerged a new trend of publishing and accessing news through online media or usually called online journalism. In addition, the number of people who sell and purchase through online sites also continues to increase and this opportunity is utilized by the company and the advertiser by implementing targeted web advertising. However, the high number of articles that have been published and accessed leads to great opportunities for errors in determining where to place the ads. Therefore, it needs a system that can categorize articles accessed by users as the basis of advertisement placement by the company and this classification system can be done by applying the method of Data Mining and Text Mining. This research uses document data in the form of article content that will be categorized into twenty categories of class of advertisement by using Text Mining technique with Support Vector Machine algorithm. The results of this study may be used by companies or advertisers as a basis for placement of ads on selected online media sites.",
        "link": "https://dl.acm.org/doi/10.1145/3134271.3134288"
    },
    {
        "title": "Mining Free-Text Medical Notes for Suicide Risk Assessment",
        "authors": [
            "Marios Adamou",
            "Grigoris Antoniou",
            "Elissavet Greasidou",
            "Vincenzo Lagani",
            "Paulos Charonyktakis",
            "Ioannis Tsamardinos"
        ],
        "date": "July 2018",
        "source": "SETN '18: Proceedings of the 10th Hellenic Conference on Artificial Intelligence",
        "abstract": "Suicide has been considered as an important public health issue for a very long time, and is one of the main causes of death worldwide. Despite suicide prevention strategies being applied, the rate of suicide has not changed substantially over the past decades. Advances in machine learning make it possible to attempt to predict suicide based on the analysis of relevant data to inform clinical practice. This paper reports on findings from the analysis of data of patients who died by suicide in the period 2013-2016 and made use of both structured data and free-text medical notes. We focus on examining various text-mining approaches to support risk assessment. The results show that using advance machine learning and text-mining techniques, it is possible to predict within a specified period which people are most at risk of taking their own life at the time of referral to a mental health service.",
        "link": "https://dl.acm.org/doi/10.1145/3200947.3201020"
    },
    {
        "title": "Integration of data mining techniques in e-learning systems: Clustering Profil of Lerners and Recommender Course System",
        "authors": [
            "A. El Moustamid",
            "E. En-Naimi",
            "J. El Bouhdidi"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "Data mining aims at the extraction of knowledge from large quantities of data by automatic or semi-automatic methods. In the field of education and training e-learning, Data Mining techniques as clustering (The aim of this technique is to identify groups that are similar in some aspect) [21], prediction (is used for developing model where combination of data viewed from different aspects can effect single viewed data. Here, is important to watch how predictors variables are effecting predicted variable)[21], classification are applied in e-learning to improve the level of learners by providing learning according to the learners' requirement) are used to analyze learners profile, to predict and improve student performance. The aim of our project is to develop a system capable of analyzing learners profile and indexing web videos in order to offer learners a rich database with courses that correspond to their levels.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090453"
    },
    {
        "title": "Multi-level mining and visualization of scientific text collections: Exploring a bi-lingual scientific repository",
        "authors": [
            "Pablo Accuosto",
            "Francesco Ronzano",
            "Daniel Ferrés",
            "Horacio Saggion"
        ],
        "date": "December 2017",
        "source": "WOSP 2017: Proceedings of the 6th International Workshop on Mining Scientific Publications",
        "abstract": "We present a system to mine and visualize collections of scientific documents by semantically browsing information extracted from single publications or aggregated throughout corpora of articles. The text mining tool performs deep analysis of document collections allowing the extraction and interpretation of research paper's contents. In addition to the extraction and enrichment of documents with metadata (titles, authors, affiliations, etc), the deep analysis performed comprises semantic interpretation, rhetorical analysis of sentences, triple-based information extraction, and text summarization. The visualization components allow geographical-based exploration of collections, topic-evolution interpretation, and collaborative network analysis among others. The paper presents a case study of a bi-lingual collection in the field of Natural Language Processing (NLP).",
        "link": "https://dl.acm.org/doi/10.1145/3127526.3127529"
    },
    {
        "title": "Hierarchical Topic Mining via Joint Spherical Tree and Text Embedding",
        "authors": [
            "Yu Meng",
            "Yunyi Zhang",
            "Jiaxin Huang",
            "Yu Zhang",
            "Chao Zhang",
            "Jiawei Han"
        ],
        "date": "August 2020",
        "source": "KDD '20: Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Mining a set of meaningful topics organized into a hierarchy is intuitively appealing since topic correlations are ubiquitous in massive text corpora. To account for potential hierarchical topic structures, hierarchical topic models generalize flat topic models by incorporating latent topic hierarchies into their generative modeling process. However, due to their purely unsupervised nature, the learned topic hierarchy often deviates from users' particular needs or interests. To guide the hierarchical topic discovery process with minimal user supervision, we propose a new task, Hierarchical Topic Mining, which takes a category tree described by category names only, and aims to mine a set of representative terms for each category from a text corpus to help a user comprehend his/her interested topics. We develop a novel joint tree and text embedding method along with a principled optimization procedure that allows simultaneous modeling of the category tree structure and the corpus generative process in the spherical space for effective category-representative term discovery. Our comprehensive experiments show that our model, named JoSH, mines a high-quality set of hierarchical topics with high efficiency and benefits weakly-supervised hierarchical text classification tasks.",
        "link": "https://dl.acm.org/doi/10.1145/3394486.3403242"
    },
    {
        "title": "Unsupervised behavioural mining and clustering for malware family identification",
        "authors": [
            "Khanh Huu The Dam",
            "Thomas Given-Wilson",
            "Axel Legay"
        ],
        "date": "March 2021",
        "source": "SAC '21: Proceedings of the 36th Annual ACM Symposium on Applied Computing",
        "abstract": "More accurate and advanced detection and classification of malware requires exploiting program behaviour and not merely syntactic or static features. One approach is to use system call dependency graphs (SCDGs) that represent the program behaviour by interactions with the system, and the relations between these interactions. These SCDGs have been used with supervised learning techniques to very accurately detect and classify malware. This works considers the unsupervised learning challenge of mining for common clusters of behaviour without a priori knowledge. This allows for clustering of similar programs by behaviour, that can then be used for either classification or further analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3412841.3441919"
    },
    {
        "title": "Text Mining for Evaluating Authors' Birth and Death Years",
        "authors": [
            "Dror Moghaz",
            "Yaakov Hacohen-Kerner",
            "Dov Gabbay"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "This article presents a unique method in text and data mining for finding the era, i.e., mining temporal data, in which an anonymous author was living. Finding this era can assist in the examination of a fake document or extracting the time period in which a writer lived. The study and the experiments concern Hebrew, and in some parts, Aramaic and Yiddish rabbinic texts. The rabbinic texts are undated and contain no bibliographic sections, posing an interesting challenge. This work proposes algorithms using key phrases and key words that allow the temporal organization of citations together with linguistic patterns. Based on these key phrases, key words, and the references, we established several types of “Iron-clad,” Heuristic and Greedy rules for estimating the years of birth and death of a writer in an interesting classification task. Experiments were conducted on corpora, including documents authored by 12, 24, and 36 rabbinic writers and demonstrated promising results.",
        "link": "https://dl.acm.org/doi/10.1145/3281631"
    },
    {
        "title": "Transfer Learning to Timed Text Based Video Classification Using CNN",
        "authors": [
            "Zenun Kastrati",
            "Ali Shariq Imran",
            "Arianit Kurti"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "Open educational video resources are gaining popularity with a growing number of massive open online courses (MOOCs). This has created a niche for content providers to adopt effective solutions in automatically organizing and structuring of educational resources for maximum visibility. Recent advances in deep learning techniques are proving useful in managing and classifying resources into appropriate categories. This paper proposes one such convolutional neural network (CNN) model for classifying video lectures in a MOOC setting using a transfer learning approach. The model uses a time-aligned text transcripts corresponding to video lectures from six broader subject categories. Video lectures and their corresponding transcript dataset is gathered from the Coursera MOOC platform. Two different CNN models are proposed: i) CNN based classification using embeddings learned from our MOOC dataset, ii) CNN based classification using transfer learning. Word embeddings generated from two well known state-of-the-art pre-trained models Word2Vec and GloVe, are used in the transfer learning approach for the second case. The proposed CNN models are evaluated using precision, recall, and F1 score and the obtained performance is compared with both conventional and deep learning classifiers. The proposed CNN models have an F1 score improvement of 10-22 percentage points over DNN and conventional classifiers",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326483"
    },
    {
        "title": "Evolutionary role mining in complex networks by ensemble clustering",
        "authors": [
            "Sarvenaz Choobdar",
            "Pedro Ribeiro",
            "Fernando Silva"
        ],
        "date": "April 2017",
        "source": "SAC '17: Proceedings of the Symposium on Applied Computing",
        "abstract": "The structural patterns in the neighborhood of nodes assign unique roles to the nodes. Mining the set of existing roles in a network provides a descriptive profile of the network and draws its general picture. This paper proposes a new method to determine structural roles in a dynamic network based on the current position of nodes and their historic behavior. We develop a temporal ensemble clustering technique to dynamically find groups of nodes, holding similar tempo-structural roles. We compare two weighting functions, based on age and distribution of data, to incorporate temporal behavior of nodes in the role discovery. To evaluate the performance of the proposed method, we assess the results from two points of view: 1) goodness of fit to current structure of the network; 2) consistency with historic data. We conduct the evaluation using different ensemble clustering techniques. The results on real world networks demonstrate that our method can detect tempo-structural roles that simultaneously depict the topology of a network and reflect its dynamics with high accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3019612.3019815"
    },
    {
        "title": "Analyzing used-car web listings via text mining",
        "authors": [
            "Ayhan Demiriz",
            "Fatma Cantaş"
        ],
        "date": "October 2017",
        "source": "IML '17: Proceedings of the 1st International Conference on Internet of Things and Machine Learning",
        "abstract": "Used car trade is one of the major components of the world economies. It is not uncommon to sell a car by placing an internet advertisement irrespective of the geography in these days. A typical content of an advertisement is usually composed of two parts namely the structured and the free text data. The structured data may include some information about the asking price, make, model, year, mileage of the car and the contact info. In most cases, seller may give important clues about the car's current conditions in the free text data where the title (head) of the advertisement can be included as free text too. This paper reports preliminary results from a text mining study conducted on 75K used car internet listings collected from two major car listing web sites in Turkey. As expected, the words and the phrases related to the description of the car are observed to be frequent. The leading concepts in the free text are found to be regarding how to describe the current condition of a car, for example \"no crash history\".",
        "link": "https://dl.acm.org/doi/10.1145/3109761.3109782"
    },
    {
        "title": "Text classification of Diseases Treated by Traditional Chinese Medicine Prescription based on machine learning",
        "authors": [
            "Hanqing Zhao",
            "Bo Han",
            "Chen Li"
        ],
        "date": "September 2020",
        "source": "ISAIMS '20: Proceedings of the 1st International Symposium on Artificial Intelligence in Medical Sciences",
        "abstract": "OBJECTIVE: To explore the application of machine learning in th e identification of diseases treated by traditional Chinese medicine prescriptions. METHODS: Based on the composition of the text document of Chinese medicine prescriptions, the prescriptions were divided into cough, headache and diarrhea. THUTCT were introduced to establish and train two machine learning text classification models, LibSVM and LibLinear, and the prescriptions to be deter mined were put into the model for classification and prediction. R ESULTS: The Precision rate of LibSVM model and Liblinear mod el were 0.7283 and 0.6690. Seven prescriptions were classified an d predicted, and the results were in line with expectations. CONCLUSION: THUCTC has good universality for the content of TCM prescriptions, high classification accuracy and fast testing speed, which is suitable for the text classification and discrimination research of TCM prescriptions for diseases.",
        "link": "https://dl.acm.org/doi/10.1145/3429889.3429895"
    },
    {
        "title": "Evaluation of retweet clustering method classification method using retweets on Twitter without text data",
        "authors": [
            "K. Uchida",
            "F. Toriumi",
            "T. Sakaki"
        ],
        "date": "August 2017",
        "source": "WI '17: Proceedings of the International Conference on Web Intelligence",
        "abstract": "Burst phenomena, which frequently occur on social media, are caused by such social events as flaming on the internet, elections, and natural disasters. To understand people's thoughts and feelings, we must classify their opinions from burst phenomena. Therefore, classification methods that categorize tweets are critical. However, since most classification methods focus on text mining, they cannot group tweets by topics because each tweet has poor linguistic similarities. We used a non-text-based classification method proposed by Baba et al. that groups tweets by topics, even if they have poor linguistic similarities, and verified its validity by comparing it with a text-based classification method in two different evaluations: qualitative and quantitative. In the qualitative evaluation part, we did a questionnaire survey and validated the suitability of the topic clusters created using both the non-and text-based methods. Since evaluating the similarity of every pair of tweets in each topic is difficult, we evaluated the similarity between sampled pairs in the survey and acquired more appropriate topic clustering results using the non-text-based method than the text-based method. In the quantitative evaluation part, we focused on the robustness of each method against data reduction. Many approaches analyze social media data, especially because collecting data from social media is comparatively easy. However, since collecting the whole data of burst phenomena is very costly due to the vast amounts of available social media data, robustness against data reduction is an important index to evaluate classification methods. With the non-text-based method, over 55% of the pairs of tweets in the same cluster were also included in the same cluster even when the data were reduced to 10% in all three of our example cases. In this paper, as a source we focus on Twitter, one of the most popular microblogging services. Using clustering to conduct detailed case analyses, we scrutinized three burst cases that include natural disasters and flaming on the internet and found that a non-text-based method more effectively classified tweets in burst phenomena than a text-based method.",
        "link": "https://dl.acm.org/doi/10.1145/3106426.3106451"
    },
    {
        "title": "Using Spark for Text Mining on Large Scale Liver Cancer Literature",
        "authors": [
            "Ming-Yen Lin",
            "Yu-Ju Lin",
            "Sue-Chen Hsueh"
        ],
        "date": "January 2021",
        "source": "BDET 2021: 2021 the 3rd International Conference on Big Data Engineering and Technology (BDET)",
        "abstract": "Cancer is one of the main causes of death. The number of known cancer to-date is more than one hundred. Liver cancer, ranked after trachea and lung cancer, has long been high in the cancer leading cause in Taiwan and even ranked as second in 2016. The number of scientific articles related to cancer proliferates every year. The number reaches as high as 20 million in PubMed so that discovering useful information from the massive collection is very difficult. In addition, using a single machine to sift through these articles is very time-consuming. Therefore, we present a big data analytic framework using the distributed Apache Spark platform for text mining in PubMed literature. We establish a prediction model for liver cancer articles so that researchers may effectively validate whether an article is related to liver cancer or not. Classification models in Spark MLlib including Linear Support Vector Machines (SVM), Logistic Regression, are used in our experiments. Relevancy to liver cancer is further confirmed by using MeSH (Medical Subject Headings) terms. Logistic regression is about 3 times faster than SVMs and the accuracy of both methods is close to 95% in the experiments using hold-out validation. When max_features is 500 and min_df ≤ 0.1 (or min_df = 1), the accuracy may reach 96%. In the experiments with K-fold cross-validation, the accuracy of SVMs methods is 96%. The experimental results show that that our prediction model may effectively classify liver cancer articles.",
        "link": "https://dl.acm.org/doi/10.1145/3474944.3474958"
    },
    {
        "title": "Automatic topic classification of test cases using text mining at an Android smartphone vendor",
        "authors": [
            "Junji Shimagaki",
            "Yasutaka Kamei",
            "Naoyasu Ubayashi",
            "Abram Hindle"
        ],
        "date": "October 2018",
        "source": "ESEM '18: Proceedings of the 12th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Background: An Android smartphone is an ecosystem of applications, drivers, operating system components, and assets. The volume of the software is large and the number of test cases needed to cover the functionality of an Android system is substantial. Enormous effort has been already taken to properly quantify \"what features and apps were tested and verified?\". This insight is provided by dashboards that summarize test coverage and results per feature. One method to achieve this is to manually tag or label test cases with the topic or function they cover, much like function points. At the studied Android smartphone vendor, tests are labelled with manually defined tags, so-called \"feature labels (FLs)\", and the FLs serve to categorize 100s to 1000s test cases into 10 to 50 groups. Aim: Unfortunately for developers, manual assignment of FLs to 1000s of test cases is a time consuming task, leading to inaccurately labeled test cases, which will render the dashboard useless. We created an automated system that suggests tags/labels to the developers for their test cases rather than manual labeling. Method: We use machine learning models to predict and label the functionality tested by 10,000 test cases developed at the company. Results: Through the quantitative experiments, our models achieved acceptable F-1 performance of 0.3 to 0.88. Also through the qualitative studies with expert teams, we showed that the hierarchy and path of tests was a good predictor of a feature's label. Conclusions: We find that this method can reduce tedious manual effort that software developers spent classifying test cases, while providing more accurate classification results.",
        "link": "https://dl.acm.org/doi/10.1145/3239235.3268927"
    },
    {
        "title": "To apply Data Mining for Classification of Crowd sourced Software Requirements",
        "authors": [
            "Soonh Taj",
            "Qasim Arain",
            "Imran Memon",
            "Asma Zubedi"
        ],
        "date": "April 2019",
        "source": "ICSIE '19: Proceedings of the 8th International Conference on Software and Information Engineering",
        "abstract": "Now a day's main focus of developers is to build quality software that works according to customer needs and for this reason it is necessary to gather right requirements as requirement elicitation is the critical step that impacts on the success of software project as misinterpreted requirements leads to the failure of software project. By keeping this in mind a research is carried out on improving requirements elicitation process and automating the process of classifying requirements. In this research, a model is proposed which will help in this scenario for requirements elicitation and requirement classification. This paper presents a model in which crowd sourcing approach is used so that customers, end users, stakeholders, developers and software engineers can make active participation for requirement elicitation process and requirements gathered using crowdsourcing approach are used by model for classification process i.e. classification of requirements into functional and non-functional requirements. For the proof of proposed model a case study is conducted. Results of case study provided the usefulness and efficiency of proposed model for classification of crowd sourced software requirements.",
        "link": "https://dl.acm.org/doi/10.1145/3328833.3328837"
    },
    {
        "title": "Text classification using Fuzzy TF-IDF and Machine Learning Models",
        "authors": [
            "Mariem Bounabi",
            "Karim El Moutaouakil",
            "Khalid Satori"
        ],
        "date": "October 2019",
        "source": "BDIoT'19: Proceedings of the 4th International Conference on Big Data and Internet of Things",
        "abstract": "The representation of the information has an important impact on the text classification task. Several weighting methods were proposed in the literature, and the term frequency-inverse term frequency (TFIDF), the most know on the text treatment field. The FTF-IDF is a vector representation where the components of the TFIDF are presented as inputs to the Fuzzy Inference System (FIS). In this work, we compare several Machin Learning algorithms such as Naïve Bayes and its derivatives, SVM and Random forest classifiers, using the FTF-IDF representation. To improve the quality of the used classifiers, we call sum attribute selection methods. The recognition rate, for the tested systems, is satisfied, where the system based on naïve Bayes classifier, the FTF-IDF weighting terms, and the info gain select attributes method gives 98.7% as accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3372938.3372956"
    },
    {
        "title": "Text Mining in Cybersecurity: A Systematic Literature Review",
        "authors": [
            "Luciano Ignaczak",
            "Guilherme Goldschmidt",
            "Cristiano André Da Costa",
            "Rodrigo Da Rosa Righi"
        ],
        "date": null,
        "source": "ACM Computing Surveys",
        "abstract": "The growth of data volume has changed cybersecurity activities, demanding a higher level of automation. In this new cybersecurity landscape, text mining emerged as an alternative to improve the efficiency of the activities involving unstructured data. This article proposes a Systematic Literature Review (SLR) to present the application of text mining in the cybersecurity domain. Using a systematic protocol, we identified 2,196 studies, out of which 83 were summarized. As a contribution, we propose a taxonomy to demonstrate the different activities in the cybersecurity domain supported by text mining. We also detail the strategies evaluated in the application of text mining tasks and the use of neural networks to support activities involving unstructured data. The work also discusses text classification performance aiming its application in real-world solutions. The SLR also highlights open gaps for future research, such as the analysis of non-English content and the intensification in the usage of neural networks.",
        "link": "https://dl.acm.org/doi/10.1145/3462477"
    },
    {
        "title": "Using Text Mining to Analyze the Financial Patents",
        "authors": [
            "Yung-Feng Lu",
            "Jia-Lang Xu",
            "Mu-Yen Chen"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "With the advancement and development of science and technology, Nowadays society is the development of information technology, cloud computing, mobile Internet, Internet of Things, block chain and information technology.The combination of finance and technology has made financial technology terms gradually emerging.. This research analyzes the patents of financial technology. However, Taiwan's financial technology patents are mostly G06Q-20 (payment plan, architecture or agreement), G06Q-30 (commercial, such as marketing, shopping, payment, auction or e-commerce). G06Q-40 (financial, investment or tax treatment, insurance).The research use text mining technology to analyze and understand Taiwan's trends and developments in financial technology. The research results show that G06Q-20 patents are biased towards information security and mobile payment. G06Q-30 patents favor shopping websites and advertising. G06Q-40 patents are biased towards diversity, such as securities, insurance, finance, and stocks.",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326478"
    },
    {
        "title": "Identifying Online Profiles of Distance Learning Students Using Data Mining Techniques",
        "authors": [
            "Osama Islam",
            "Muazzam Siddiqui",
            "Naif Radi Aljohani"
        ],
        "date": "October 2019",
        "source": "ICDTE '19: Proceedings of the 3rd International Conference on Digital Technology in Education",
        "abstract": "Educational data has grown over the years with the increased use of technology within educational environments. This has led to a huge amount of data being stored in various data sources representing the student, his/her activities, and other aspects relevant to the learning process. To meet this analytical need, Educational Data Mining (EDM) has emerged to assist educational institutions in identifying key benefits such as students at risk, the level of student engagement or predicting student performance. The aim of this research was to explore the various aspects of student interaction data using data mining techniques to identify relevant patterns of behaviors and possible key attributes that have higher degrees of influence on distance learning students. The main findings identified several patterns of user online profiles based on a set of adopted learning strategies, the research proposed a framework for analyzing such interaction data based on R and Hadoop platforms to correlate online profiles with student performance.",
        "link": "https://dl.acm.org/doi/10.1145/3369199.3369249"
    },
    {
        "title": "Extremely Fast Decision Tree Mining for Evolving Data Streams",
        "authors": [
            "Albert Bifet",
            "Jiajin Zhang",
            "Wei Fan",
            "Cheng He",
            "Jianfeng Zhang",
            "Jianfeng Qian",
            "Geoff Holmes",
            "Bernhard Pfahringer"
        ],
        "date": "August 2017",
        "source": "KDD '17: Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
        "abstract": "Nowadays real-time industrial applications are generating a huge amount of data continuously every day. To process these large data streams, we need fast and efficient methodologies and systems. A useful feature desired for data scientists and analysts is to have easy to visualize and understand machine learning models. Decision trees are preferred in many real-time applications for this reason, and also, because combined in an ensemble, they are one of the most powerful methods in machine learning. In this paper, we present a new system called STREAMDM-C++, that implements decision trees for data streams in C++, and that has been used extensively at Huawei. Streaming decision trees adapt to changes on streams, a huge advantage since standard decision trees are built using a snapshot of data, and can not evolve over time. STREAMDM-C++ is easy to extend, and contains more powerful ensemble methods, and a more efficient and easy to use adaptive decision trees. We compare our new implementation with VFML, the current state of the art implementation in C, and show how our new system outperforms VFML in speed using less resources.",
        "link": "https://dl.acm.org/doi/10.1145/3097983.3098139"
    },
    {
        "title": "Data miners' little helper: data transformation activity cues for cluster analysis on document collections",
        "authors": [
            "Tania Cerquitelli",
            "Evelina Di Corso",
            "Francesco Ventura",
            "Silvia Chiusano"
        ],
        "date": "June 2017",
        "source": "WIMS '17: Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "In this paper we propose a new self-learning engine to streamline the analytics process, as it enables analysts to mine massive data repositories with minimal user intervention. In the context of cluster analysis on a collection of documents this new system, named SELF-DATA (SELF-learning DAta TrAnsformation), suggests to the analyst how to configure the whole mining process for a given dataset. SELF-DATA relies on an engine exploring different data weighting schemas (e.g., normalized term frequencies) and data transformation methods (e.g., PCA) before applying the cluster analysis, evaluating and comparing solutions through different quality indices (e.g., weighted Silhouette), and presenting the k-top solutions to the analyst. SELF-DATA will also include a knowledge base storing results of experiments on previously processed datasets, and a classification algorithm trained on the knowledge base content to forecast the best configuration for the whole mining process for an unexplored dataset. The first development of SELF-DATA running on Apache Spark has been validated on 5 collections of documents. Experimental results highlight that TF-IDF and logarithmic entropy are effective to measure item relevance with sparse datasets, and the LSI method outperforms PCA with a large dictionary.",
        "link": "https://dl.acm.org/doi/10.1145/3102254.3102288"
    },
    {
        "title": "Interactive Visual Graph Mining and Learning",
        "authors": [
            "Ryan A. Rossi",
            "Nesreen K. Ahmed",
            "Rong Zhou",
            "Hoda Eldardiry"
        ],
        "date": null,
        "source": "ACM Transactions on Intelligent Systems and Technology",
        "abstract": "This article presents a platform for interactive graph mining and relational machine learning called GraphVis. The platform combines interactive visual representations with state-of-the-art graph mining and relational machine learning techniques to aid in revealing important insights quickly as well as learning an appropriate and highly predictive model for a particular task (e.g., classification, link prediction, discovering the roles of nodes, and finding influential nodes). Visual representations and interaction techniques and tools are developed for simple, fast, and intuitive real-time interactive exploration, mining, and modeling of graph data. In particular, we propose techniques for interactive relational learning (e.g., node/link classification), interactive link prediction and weighting, role discovery and community detection, higher-order network analysis (via graphlets, network motifs), among others. GraphVis also allows for the refinement and tuning of graph mining and relational learning methods for specific application domains and constraints via an end-to-end interactive visual analytic pipeline that learns, infers, and provides rapid interactive visualization with immediate feedback at each change/prediction in real-time. Other key aspects include interactive filtering, querying, ranking, manipulating, exporting, as well as tools for dynamic network analysis and visualization, interactive graph generators (including new block model approaches), and a variety of multi-level network analysis techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3200764"
    },
    {
        "title": "Data Mining and Opinion Mining: A Tool in Educational Context",
        "authors": [
            "Myriam Peñafiel",
            "Stefanie Vásquez",
            "Diego Vásquez",
            "Juan Zaldumbide",
            "Sergio Luján-Mora"
        ],
        "date": "July 2018",
        "source": "ICoMS '18: Proceedings of the 2018 1st International Conference on Mathematics and Statistics",
        "abstract": "The use of the web as a universal communication platform generates large volumes of data (Big data), which in many cases, need to be processed so that they can become useful knowledge in face of the sceptics who have doubts about the credibility of such information. The use of web data that comes from educational contexts needs to be addressed, since that large amount of unstructured information is not being valued, losing valuable information that can be used. To solve this problem, we propose the use of data mining techniques such as sentiment analysis to validate the information that comes from the educational platforms. The objective of this research is to propose a methodology that allows the user to apply sentiment analysis in a simple way, because although some researchers have done it, very few do with data in the educational context. The results obtained prove that the proposal can be used in similar cases.",
        "link": "https://dl.acm.org/doi/10.1145/3274250.3274263"
    },
    {
        "title": "Research Hotspots Mining and Visualized Analysis Based on Linking Cluster and K-Core Decomposition",
        "authors": [
            "Ying Cai",
            "Fang Huang",
            "Shuai Wang",
            "Hao Zhang",
            "Chunxiu Du"
        ],
        "date": "May 2018",
        "source": "ICDPA 2018: Proceedings of the International Conference on Data Processing and Applications",
        "abstract": "In this paper, we focus on research hotspots mining with knowledge semantic features on discipline topic word networks, which are extracted from scientific and technical documents. For the topic networks, a novel method of combining linking community clustering with k-core decomposition is proposed to mine the hotspot community with knowledge hierarchy structure. And as a post-processing, the hierarchical community identification is implemented by k-core decomposition in order to discover the internal connection with knowledge semantic hierarchy in the research hotspot. Finally, the community density and visualization method is utilized to analyze the core and sub- problem in the research hotspot community. In experiments, the topic networks for a single discipline and interdisciplinary research hotspot are taken as examples of analysis, and the results show the proposed approach can effectively realize the objective of mining and analyzing the discipline research hotspot on the academic topic word network.",
        "link": "https://dl.acm.org/doi/10.1145/3224207.3224216"
    },
    {
        "title": "The Application of Data Mining Algorithms in the Construction of Travel Recommendation System",
        "authors": [
            "Chao Lou",
            "Zhaonan Mu",
            "Mengzhu Liu"
        ],
        "date": "January 2021",
        "source": "CONF-CDS 2021: The 2nd International Conference on Computing and Data Science",
        "abstract": "Photos shared by users of online social network often associated with tags, texts, geographic and time information. These data are ideal research resource which can be used by researchers to observe people's interests and behaviors. They can also be used to analyze people's travel behaviors and interests because it can reflect people's travel activities and experiences directly. This project aimed to consolidate people into groups based on their travel preferences of travel season, travel time span, scenery type of travel location and then analyze popular travel places for different people groups. The travel preferences were analyzed from extracted data applying PHP language and Naive Bayes classification approach. According to the analyzed data, people were divided into groups. Then, DBSCAN clustering was used to get popular locations based on the different groups. As a result, 80 thousand photo data of 5 cities have been collected from Flickr by Phython. It has been showed that people's preference of season in most of the mined cities was spring and summer and the preference of time span was short. Moreover, mined people has been divided into 16 groups according to their preference and based on the groups popular locations have been calculated by DBSCAN clustering. Theses locations have been provided in a recommendation website which was built in this project. For the future work, mining Flickr data associated with other travel social network sites could be a good way to build a more effective recommendation system.",
        "link": "https://dl.acm.org/doi/10.1145/3448734.3450477"
    },
    {
        "title": "Compare Machine Learning Models in Text Classification Using Steam User Reviews",
        "authors": [
            "Youchen Miao",
            "Zeyu Jin",
            "Yumeng Zhang",
            "Yuchen Chen",
            "Junren Lai"
        ],
        "date": "November 2021",
        "source": "ICSED '21: Proceedings of the 2021 3rd International Conference on Software Engineering and Development",
        "abstract": "Text Classification and Sentiment Analysis of game reviews are viewed as important parts in not only academic fields but also in game studies. In this paper, with more than 400 thousand game reviews on Steam platform, we preprocess the data using different libraries (sklearn, nltk, and spaCy) and use them as inputs to build three sentiment classification models based on different algorithms (Naive Bayes, SVM, and Random Forest). In contrast to previous studies that only focus on different sentiment analysis models, our paper also highlights the use of different APIs to preprocess the data and their corresponding model performance. The results show that no matter which API we choose, Random Forest models always perform the best. However, in terms of training time, Naive Bayes is the fastest. This work can be used to apply grid search for researchers to automatically find the optimum API before conducting sentiment analysis in the future.",
        "link": "https://dl.acm.org/doi/10.1145/3507473.3507480"
    },
    {
        "title": "Lost in Transduction: Transductive Transfer Learning in Text Classification",
        "authors": [
            "Alejandro Moreo",
            "Andrea Esuli",
            "Fabrizio Sebastiani"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Obtaining high-quality labelled data for training a classifier in a new application domain is often costly. Transfer Learning\n(a.k.a. “Inductive Transfer”) tries to alleviate these costs by transferring, to the “target” domain of interest, knowledge available from a different “source” domain. In transfer learning the lack of labelled information from the target domain is compensated by the availability at training time of a set of unlabelled examples from the target distribution. Transductive Transfer Learning denotes the transfer learning setting in which the only set of target documents that we are interested in classifying is known and available at training time. Although this definition is indeed in line with Vapnik’s original definition of “transduction”, current terminology in the field is confused. In this article, we discuss how the term “transduction” has been misused in the transfer learning literature, and propose a clarification consistent with the original characterization of this term given by Vapnik. We go on to observe that the above terminology misuse has brought about misleading experimental comparisons, with inductive transfer learning methods that have been incorrectly compared with transductive transfer learning methods. We then, give empirical evidence that the difference in performance between the inductive version and the transductive version of a transfer learning method can indeed be statistically significant (i.e., that knowing at training time the only data one needs to classify indeed gives an advantage). Our clarification allows a reassessment of the field, and of the relative merits of the major, state-of-the-art algorithms for transfer learning in text classification.",
        "link": "https://dl.acm.org/doi/10.1145/3453146"
    },
    {
        "title": "Machine Learning on Mining Potential Adverse Drug Reactions for Pharmacovigilance",
        "authors": [
            "Ke Jia"
        ],
        "date": "July 2021",
        "source": "DSIT 2021: 2021 4th International Conference on Data Science and Information Technology",
        "abstract": "Drugs are related to the lives and health of patients. Mining and discovering potential adverse drug reactions from a large number of unstructured texts play an important role in drug research and pharmacovigilance. It is a hot research issue in the field of biomedicine in recent years. A large number of scholars have discovered a large amount of information on adverse drug reactions from different data sources through effective machine learning methods. This paper reviews the research progress in recent years to provide a reference for future research.",
        "link": "https://dl.acm.org/doi/10.1145/3478905.3478964"
    },
    {
        "title": "TACAM: Topic And Context Aware Argument Mining",
        "authors": [
            "Michael Fromm",
            "Evgeniy Faerman",
            "Thomas Seidl"
        ],
        "date": "October 2019",
        "source": "WI '19: IEEE/WIC/ACM International Conference on Web Intelligence",
        "abstract": "In this work we address the problem of argument search. The purpose of argument search is the distillation of pro and contra arguments for requested topics from large text corpora. In previous works, the usual approach is to use a standard search engine to extract text parts which are relevant to the given topic and subsequently use an argument recognition algorithm to select arguments from them. The main challenge in the argument recognition task, which is also known as argument mining, is that often sentences containing arguments are structurally similar to purely informative sentences without any stance about the topic. In fact, they only differ semantically. Most approaches use topic or search term information only for the first search step and therefore assume that arguments can be classified independently of a topic. We argue that topic information is crucial for argument mining, since the topic defines the semantic context of an argument. Precisely, we propose different models for the classification of arguments, which take information about a topic of an argument into account. Moreover, to enrich the context of a topic and to let models understand the context of the potential argument better, we integrate information from different external sources such as Knowledge Graphs or pre-trained NLP models. Our evaluation shows that considering topic information, especially in connection with external information, provides a significant performance boost for the argument mining task.",
        "link": "https://dl.acm.org/doi/10.1145/3350546.3352506"
    },
    {
        "title": "Text mining for incoming tasks based on the urgency/importance factors and task classification using machine learning tools",
        "authors": [
            "Yasser Ali Alshehri"
        ],
        "date": "March 2020",
        "source": "ICCDA 2020: Proceedings of the 2020 the 4th International Conference on Compute and Data Analysis",
        "abstract": "In workplaces, there is a massive amount of unstructured data from different sources. In this paper, we present a case study that explains how can through communications between employees, we can help to prioritize tasks requests to increase the efficiency of their works for both technical and non-technical workers. This involves managing daily incoming tasks based on their level of urgency and importance.To allow all workers to utilize the urgency-importance matrix as a time-management tool, we need to automate this tool. The textual content of incoming tasks are analyzed, and metrics related to urgency and importance are extracted. A third factor (i.e., the response variable) is defined based on the two input variables (urgency and importance). Then, machine learning applied to the data to predict the class of incoming tasks based on data outcome desired. We used ordinal regression, neural networks, and decision tree algorithms to predict the four levels of task priority. We measure the performance of all using recalls, precisions, and F-scores. All classifiers perform higher than 89% in terms of all measures.",
        "link": "https://dl.acm.org/doi/10.1145/3388142.3388153"
    },
    {
        "title": "Text mining for malware classification using multivariate all repeated patterns detection",
        "authors": [
            "Konstantinos F. Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2019",
        "source": "ASONAM '19: Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Mobile phones have become nowadays a commodity to the majority of people. Using them, people are able to access the world of Internet and connect with their friends, their colleagues at work or even unknown people with common interests. This proliferation of the mobile devices has also been seen as an opportunity for the cyber criminals to deceive smartphone users and steel their money directly or indirectly, respectively, by accessing their bank accounts through the smartphones or by blackmailing them or selling their private data such as photos, credit card data, etc. to third parties. This is usually achieved by installing malware to smartphones masking their malevolent payload as a legitimate application and advertise it to the users with the hope that mobile users will install it in their devices. Thus, any existing application can easily be modified by integrating a malware and then presented it as a legitimate one. In response to this, scientists have proposed a number of malware detection and classification methods using a variety of techniques. Even though, several of them achieve relatively high precision in malware classification, there is still space for improvement. In this paper, we propose a text mining all repeated pattern detection method which uses the decompiled files of an application in order to classify a suspicious application into one of the known malware families. Based on the experimental results using a real malware dataset, the methodology tries to correctly classify (without any misclassification) all randomly selected malware applications of 3 categories with 3 different families each.",
        "link": "https://dl.acm.org/doi/10.1145/3341161.3350841"
    },
    {
        "title": "Comparative Study between Traditional Machine Learning and Deep Learning Approaches for Text Classification",
        "authors": [
            "Cannannore Nidhi Kamath",
            "Syed Saqib Bukhari",
            "Andreas Dengel"
        ],
        "date": "August 2018",
        "source": "DocEng '18: Proceedings of the ACM Symposium on Document Engineering 2018",
        "abstract": "In this contemporaneous world, it is an obligation for any organization working with documents to end up with the insipid task of classifying truckload of documents, which is the nascent stage of venturing into the realm of information retrieval and data mining. But classification of such humongous documents into multiple classes, calls for a lot of time and labor. Hence a system which could classify these documents with acceptable accuracy would be of an unfathomable help in document engineering. We have created multiple classifiers for document classification and compared their accuracy on raw and processed data. We have garnered data used in a corporate organization as well as publicly available data for comparison. Data is processed by removing the stop-words and stemming is implemented to produce root words. Multiple traditional machine learning techniques like Naive Bayes, Logistic Regression, Support Vector Machine, Random forest Classifier and Multi-Layer Perceptron are used for classification of documents. Classifiers are applied on raw and processed data separately and their accuracy is noted. Along with this, Deep learning technique such as Convolution Neural Network is also used to classify the data and its accuracy is compared with that of traditional machine learning techniques. We are also exploring hierarchical classifiers for classification of classes and subclasses. The system classifies the data faster and with better accuracy than if done manually. The results are discussed in the results and evaluation section.",
        "link": "https://dl.acm.org/doi/10.1145/3209280.3209526"
    },
    {
        "title": "Identification of Overpricing in the Purchase of Medication by the Federal Government of Brazil, Using Text Mining and Clustering Based on Ontology",
        "authors": [
            "Marco Aurelio O. S. Correa",
            "Adriano Galindo Leal"
        ],
        "date": "August 2018",
        "source": "ICCBDC '18: Proceedings of the 2018 2nd International Conference on Cloud and Big Data Computing",
        "abstract": "Increasing the transparency level in his actions and spending is one of the primary duties of the Brazilian Federal Government. The creation of laws that oblige full disclose of all its expenditures through transparency portals enables citizens to supervise all government entities. However, only the dissemination of these data, without a definite standard or the availability of data analysis tools, does not guarantee that the citizen is empowered to play his role. Therefore hence, the objective of this work is to identify overprice in the acquisition of products purchased by the federal government of Brazil using the unstructured data available on the Transparency Portal. The last two-years' worth of purchasing data, available in the Transparency Portal, were extracted, processed and stored. Due to his diverse nature and high volume of data, this study focused only on medicines purchased by the Ministry of Health. Ontology-based text mining and clustering techniques were applied for automatic identification and classification of products. The processing of this information was done through text mining and clustering, based on the ontology registered in another database of the Brazilian government. Because of this work, a consolidated price base per medication was created to allow the identification of distortions in prices practised, facilitating the identification of cases that merit further investigation to unravel fraud to the treasury.",
        "link": "https://dl.acm.org/doi/10.1145/3264560.3264569"
    },
    {
        "title": "Computational Estimation by Scientific Data Mining with Classical Methods to Automate Learning Strategies of Scientists",
        "authors": [
            "Aparna S. Varde"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Experimental results are often plotted as 2-dimensional graphical plots (aka graphs) in scientific domains depicting dependent versus independent variables to aid visual analysis of processes. Repeatedly performing laboratory experiments consumes significant time and resources, motivating the need for computational estimation. The goals are to estimate the graph obtained in an experiment given its input conditions, and to estimate the conditions that would lead to a desired graph. Existing estimation approaches often do not meet accuracy and efficiency needs of targeted applications. We develop a computational estimation approach called AutoDomainMine that integrates clustering and classification over complex scientific data in a framework so as to automate classical learning methods of scientists. Knowledge discovered thereby from a database of existing experiments serves as the basis for estimation. Challenges include preserving domain semantics in clustering, finding matching strategies in classification, striking a good balance between elaboration and conciseness while displaying estimation results based on needs of targeted users, and deriving objective measures to capture subjective user interests. These and other challenges are addressed in this work. The AutoDomainMine approach is used to build a computational estimation system, rigorously evaluated with real data in Materials Science. Our evaluation confirms that AutoDomainMine provides desired accuracy and efficiency in computational estimation. It is extendable to other science and engineering domains as proved by adaptation of its sub-processes within fields such as Bioinformatics and Nanotechnology.",
        "link": "https://dl.acm.org/doi/10.1145/3502736"
    },
    {
        "title": "Where is the road for issue reports classification based on text mining?",
        "authors": [
            "Qiang Fan",
            "Yue Yu",
            "Gang Yin",
            "Tao Wang",
            "Huaimin Wang"
        ],
        "date": "November 2017",
        "source": "ESEM '17: Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Currently, open source projects receive various kinds of issues daily, because of the extreme openness of Issue Tracking System (ITS) in GitHub. ITS is a labor-intensive and time-consuming task of issue categorization for project managers. However, a contributor is only required a short textual abstract to report an issue in GitHub. Thus, most traditional classification approaches based on detailed and structured data (e.g., priority, severity, software version and so on) are difficult to adopt. In this paper, issue classification approaches on a large-scale dataset, including 80 popular projects and over 252,000 issue reports collected from GitHub, were investigated. First, four traditional text-based classification methods and their performances were discussed. Semantic perplexity (i.e., an issues description confuses bug-related sentences with nonbug-related sentences) is a crucial factor that affects the classification performances based on quantitative and qualitative study. Finally, A two-stage classifier framework based on the novel metrics of semantic perplexity of issue reports was designed. Results show that our two-stage classification can significantly improve issue classification performances.",
        "link": "https://dl.acm.org/doi/10.1109/ESEM.2017.19"
    },
    {
        "title": "Feature-based Facebook reviews process model for e-management using data mining",
        "authors": [
            "Anish Kumar Varudharajulu",
            "Yongsheng Ma"
        ],
        "date": "January 2019",
        "source": "IC4E '19: Proceedings of the 10th International Conference on E-Education, E-Business, E-Management and E-Learning",
        "abstract": "The data generated from online communication acts as potential gold mines for discovering knowledge for researchers. A large amount of data is also generated in the form of web documents, emails, blogs, and feedback, etc. Text analytics is being significantly employed to mine important information. Opinion mining is the process of extracting human thoughts and perceptions from unstructured texts. The showstopper for designing an opinion mining system for analyzing reviews arise from the fact that customer reviews are often noisy. These reviews are informally written. In addition, they are subjected to spelling mistakes, grammatical errors, improper punctuation and irrational capitalization. This paper focuses on analyzing the different classification and clustering algorithms aimed at extracting and consolidating opinions of customers from social media sites like Facebook, Twitter and through surveys, at multiple levels of granularity to monitor and measure customer satisfaction. Ours is an automated approach, in which the system aids in the process of knowledge assimilation for knowledge-based building and also performs the analytics. Domain experts ratify the knowledge base and also provide training datasets for the system to intuitively gather more instances for ratification. The system identifies opinion expressions as phrases containing opinion words, opinionated features and also opinion modifiers. These expressions are categorized as positive, negative or neutral. Opinion expressions are identified and categorized using localized linguistic techniques. Opinions can be congregated at any desired level of specificity i.e. feature level or product level, user level or service level, etc. We have developed a system based on this approach, which provides the user with a platform to analyze opinion expressions crawled from a set of pre-defined datasets.",
        "link": "https://dl.acm.org/doi/10.1145/3306500.3306514"
    },
    {
        "title": "Discriminative Topic Mining via Category-Name Guided Text Embedding",
        "authors": [
            "Yu Meng",
            "Jiaxin Huang",
            "Guangyuan Wang",
            "Zihan Wang",
            "Chao Zhang",
            "Yu Zhang",
            "Jiawei Han"
        ],
        "date": "April 2020",
        "source": "WWW '20: Proceedings of The Web Conference 2020",
        "abstract": "Mining a set of meaningful and distinctive topics automatically from massive text corpora has broad applications. Existing topic models, however, typically work in a purely unsupervised way, which often generate topics that do not fit users’ particular needs and yield suboptimal performance on downstream tasks. We propose a new task, discriminative topic mining, which leverages a set of user-provided category names to mine discriminative topics from text corpora. This new task not only helps a user understand clearly and distinctively the topics he/she is most interested in, but also benefits directly keyword-driven classification tasks. We develop CatE, a novel category-name guided text embedding method for discriminative topic mining, which effectively leverages minimal user guidance to learn a discriminative embedding space and discover category representative terms in an iterative manner. We conduct a comprehensive set of experiments to show that CatE mines high-quality set of topics guided by category names only, and benefits a variety of downstream applications including weakly-supervised classification and lexical entailment direction identification.",
        "link": "https://dl.acm.org/doi/10.1145/3366423.3380278"
    },
    {
        "title": "Comparative Study of Heart Disease Diagnosis Using Top Ten Data Mining Classification Algorithms",
        "authors": [
            "I. Ketut Agung Enriko"
        ],
        "date": "June 2019",
        "source": "ICFET '19: Proceedings of the 5th International Conference on Frontiers of Educational Technologies",
        "abstract": "Data mining has been used for many purposes, especially for prediction system. In healthcare, data mining algorithms often used in disease diagnosis. Meanwhile, heart disease is known as a primary cause of death over the years. Many studies have been performed in heart disease diagnosis using data mining methods. There are some popular data mining algorithms that can be used in heart disease diagnosis, for example, k-Nearest Neighbor, CART, and AdaBoost. The algorithms are used to analyze a sample of cardiovascular patients data and predict the heart disease type that they suffer. Some parameters are taken from the patient, including EKG morphology, blood pressure, and information about the existence of chest pain, shortness of breath, palpitation, and cold sweat. In this study, medical records data are collected from Harapan Kita Hospital and utilized as a dataset sample in this research. Top ten data mining classification algorithms are used in diagnosing heart disease from Harapan Kita Hospital data and examining their performance by checking the accuracy and speed.",
        "link": "https://dl.acm.org/doi/10.1145/3338188.3338220"
    },
    {
        "title": "Text Mining Approach for Identifying Research Trends",
        "authors": [
            "Snezhana Sulova"
        ],
        "date": "June 2021",
        "source": "CompSysTech '21: Proceedings of the 22nd International Conference on Computer Systems and Technologies",
        "abstract": "With the increase of unstructured data, the issues connected with automatic text processing, the categorization of documents and the discovery of topics have become objects of growing interest. In order to improve the process of grouping and processing research publications, we would like to propose a method based upon natural language processing. It is based on text mining technologies which aim to identify key tendencies in documents. It processes the content of publications by clustering and identifies the topics of each identified group. This analysis helps by identifying key tendencies as well as discovering emerging new areas of research. Publications from the research literature database, Scopus, were used to test the approach. The topic of the publications is “the application of digital technologies in the logistics business”. The experiments were completed using the RapidMiner Studio software.",
        "link": "https://dl.acm.org/doi/10.1145/3472410.3472433"
    },
    {
        "title": "Assessment of Vulnerability Severity using Text Mining",
        "authors": [
            "Georgios Spanos",
            "Lefteris Angelis",
            "Dimitrios Toloudis"
        ],
        "date": "September 2017",
        "source": "PCI '17: Proceedings of the 21st Pan-Hellenic Conference on Informatics",
        "abstract": "Software1 vulnerabilities are closely associated with information systems security, a major and critical field in today's technology. Vulnerabilities constitute a constant and increasing threat for various aspects of everyday life, especially for safety and economy, since the social impact from the problems that they cause is complicated and often unpredictable. Although there is an entire research branch in software engineering that deals with the identification and elimination of vulnerabilities, the growing complexity of software products and the variability of software production procedures are factors contributing to the ongoing occurrence of vulnerabilities, Hence, another area that is being developed in parallel focuses on the study and management of the vulnerabilities that have already been reported and registered in databases. The information contained in such databases includes, a textual description and a number of metrics related to vulnerabilities. The purpose of this paper is to investigate to what extend the assessment of the vulnerability severity can be inferred directly from the corresponding textual description, or in other words, to examine the informative power of the description with respect to the vulnerability severity. For this purpose, text mining techniques, i.e. text analysis and three different classification methods (decision trees, neural networks and support vector machines) were employed. The application of text mining to a sample of 70,678 vulnerabilities from a public data source shows that the description itself is a reliable and highly accurate source of information for vulnerability prioritization.",
        "link": "https://dl.acm.org/doi/10.1145/3139367.3139390"
    },
    {
        "title": "Clustering Algorithms for Spatial Data Mining",
        "authors": [
            "Chetashri Bhadane",
            "Ketan Shah"
        ],
        "date": "April 2020",
        "source": "ICGDA '20: Proceedings of the 2020 3rd International Conference on Geoinformatics and Data Analysis",
        "abstract": "With the advances in mobile and wireless technologies, there has been a rise in applications that track and share the users' geospatial data. People use several social networking sites such as Twitter, Facebook and Flickr, where they share their status updates. With the integration of Global Positioning System (GPS) with mobile phones, it is now possible to share one's locations on these social networks. GPS allows us to record and track a person's movement along with the timestamp. The data set obtained from these GPS logs is vast and is widely used to analyze the users' movement patterns. Specifically, we can find out significant locations based on the number of users present at that location and the time spent by them at such places. Once significant places have been identified, it is also possible to identify the semantic importance of these locations. This paper presents an overview of the clustering techniques used to find important places of interest using large GPS based mobility datasets. Four clustering algorithms, K-Means, DBSCAN, OPTICS and Hierarchical, are implemented, and performance is tested using real-time data of 50 users collected over 2--5 years. Performance summary depicts that K-Means and DBSCAN perform well for spatial data.",
        "link": "https://dl.acm.org/doi/10.1145/3397056.3397068"
    },
    {
        "title": "Feature Selection based Arabic Text Classification using Different Machine Learning Algorithms: Comparative Study",
        "authors": [
            "Sakina Rim Bennabi",
            "Zakaria Elberrichi"
        ],
        "date": "June 2020",
        "source": "ICIST '20: Proceedings of the 10th International Conference on Information Systems and Technologies",
        "abstract": "Feature selection is a method of data pre-processing widely used when mining large data, such as textual classification. Several studies have been conducted to compare the different methods of feature selection applied to corpora in English. Unfortunately, a small number of works concern the Arabic language. This article aims to present a comparative study of different feature selection techniques including: Chi2, the ANOVA method and mutual information, applied on a corpus in Arabic language, while also diversifying the machine learning algorithms (Naive Bayes, SVM and KNN). This experimental study has shown in general that reducing dimensionality with feature selection techniques has slightly affected the performance of textual classification, reducing the size of the corpus by up to 1%.",
        "link": "https://dl.acm.org/doi/10.1145/3447568.3448531"
    },
    {
        "title": "Intelligent Data Analysis using Optimized Support Vector Machine Based Data Mining Approach for Tourism Industry",
        "authors": [
            "Ms Promila Sharma",
            "Uma Meena",
            "Girish Kumar Sharma"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Data analysis involves the deployment of sophisticated approaches from data mining methods, information theory, and artificial intelligence in various fields like tourism, hospitality, and so on for the extraction of knowledge from the gathered and preprocessed data. In tourism, pattern analysis or data analysis using classification is significant for finding the patterns that represent new and potentially useful information or knowledge about the destination and other data. Several data mining techniques are introduced for the classification of data or patterns. However, overfitting, less accuracy, local minima, sensitive to noise are the drawbacks in some existing data mining classification methods. To overcome these challenges, Support vector machine with Red deer optimization (SVM-RDO) based data mining strategy is proposed in this article. Extended Kalman filter (EKF) is utilized in the first phase, i.e., data cleaning to remove the noise and missing values from the input data. Mantaray foraging algorithm (MaFA) is used in the data selection phase, in which the significant data are selected for the further process to reduce the computational complexity. The final phase is the classification, in which SVM-RDO is proposed to access the useful pattern from the selected data. PYTHON is the implementation tool used for the experiment of the proposed model. The experimental analysis is done to show the efficacy of the proposed work. From the experimental results, the proposed SVM-RDO achieved better accuracy, precision, recall, and F1 score than the existing methods for the tourism dataset. Thus, it is showed the effectiveness of the proposed SVM-RDO for pattern analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3494566"
    },
    {
        "title": "Applying text mining to predict learners' cognitive engagement",
        "authors": [
            "Hayati Hind",
            "Mohammed Khalidi Idrissi",
            "Samir Bennani"
        ],
        "date": "October 2017",
        "source": "SCAMS '17: Proceedings of the Mediterranean Symposium on Smart City Application",
        "abstract": "In academic process, engagement represent one of the important success' factor. This paper proposes a predictive system for learners' cognitive engagement based on their online discussion forums participation. In the first level the ontology OWL and the LSA method are used to perform a semantic classification system of the threads according to a specific context chosen by the tutor. Then the Text mining, as a predictive method, is applied to the classified threads and learners' participation in the forums.",
        "link": "https://dl.acm.org/doi/10.1145/3175628.3175655"
    },
    {
        "title": "Investigate Transitions into Drug Addiction through Text Mining of Reddit Data",
        "authors": [
            "John Lu",
            "Sumati Sridhar",
            "Ritika Pandey",
            "Mohammad Al Hasan",
            "Georege Mohler"
        ],
        "date": "July 2019",
        "source": "KDD '19: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Increasing rates of opioid drug abuse and heightened prevalence of online support communities underscore the necessity of employing data mining techniques to better understand drug addiction using these rapidly developing online resources. In this work, we obtained data from Reddit, an online collection of forums, to gather insight into drug use/misuse using text snippets from users narratives. Specifically, using users' posts, we trained a binary classifier which predicts a user's transitions from casual drug discussion forums to drug recovery forums. We also proposed a Cox regression model that outputs likelihoods of such transitions. In doing so, we found that utterances of select drugs and certain linguistic features contained in one's posts can help predict these transitions. Using unfiltered drug-related posts, our research delineates drugs that are associated with higher rates of transitions from recreational drug discussion to support/recovery discussion, offers insight into modern drug culture, and provides tools with potential applications in combating the opioid crisis.",
        "link": "https://dl.acm.org/doi/10.1145/3292500.3330737"
    },
    {
        "title": "Analysing digital banking reviews using text mining",
        "authors": [
            "Li Chen Cheng",
            "Legaspi Rhea Sharmayne"
        ],
        "date": "December 2020",
        "source": "ASONAM '20: Proceedings of the 12th IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Digital banks are new entrants in the banking industry in the Philippines as they only started late 2018. Since then, a handful of players have and are still emerging. With more and more people becoming technologically savvy, it is very critical for financial institutions to develop a digital banking application that will stand out from the competition. This paper aims to use text mining methods to analyse digital banking application reviews. This study will perform topic modelling using LDA to explore customer concerns and will mine association rules between the digital banking features with the review score. The results will reveal which areas the digital banking application can further optimize for customer satisfaction and retention.",
        "link": "https://dl.acm.org/doi/10.1109/ASONAM49781.2020.9381429"
    },
    {
        "title": "Mining Moocs Videos Metadata Using Classification Techniques",
        "authors": [
            "El Harrak Othman",
            "Ghadi Abderrahim",
            "El Bouhdidi Jaber"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "In few years, the internet has become the first source of information for most people, Today MOOCs makes it possible for everyone to access to the education over the world, it's represents an emerging methodology of online teaching and an important development in open education. Due to the rapid development of Moocs and the rapid growth of digital data and video database over the Internet, it is becoming very difficult for learners to browsing and choosing the best training for them. The scientific community has increased the amount of research into new technologies, with a view to improve the digital video utilization: its archiving, indexing, accessibility, acquisition, store and even its process and usability. All these parts of the video utilization entail the necessity of the extraction of all important information of a video, especially in cases of lack of metadata information. Web video mining is retrieving the content using data mining techniques from World Wide Web. There are two approaches for web video mining using traditional image processing (signal processing) and metadata based approach. In this paper, we present the various research makes in this subject and we propose an effective methodology to extract the metadata from Moocs videos and classify them based on the extracted metadata by applying data mining techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090450"
    },
    {
        "title": "Text mining with HathiTrust",
        "authors": [
            "Eleanor Dickson Koehl",
            "Ryan Dubnicek"
        ],
        "date": "June 2019",
        "source": "JCDL '19: Proceedings of the 18th Joint Conference on Digital Libraries",
        "abstract": "This tutorial will introduce attendees to the HathiTrust Research Center and its tools and services for computational text analysis research. HTRC leverages the scope and scale of the HathiTrust Digital Library collection to create opportunities for researchers to perform text data mining on subsets of the corpus. Attendees will develop skills that will allow them to conduct basic text analysis research using HathiTrust data, learn techniques for engaging with scholars on their text data mining research, and explore strategies for computational access to large-scale digital collections.",
        "link": "https://dl.acm.org/doi/10.1109/JCDL.2019.00115"
    },
    {
        "title": "Text mining for plagiarism detection: multivariate pattern detection for recognition of text similarities",
        "authors": [
            "Konstantinos Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2018",
        "source": "ASONAM '18: Proceedings of the 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "The problem of plagiarism the recent years has been intensified by the availability of information in digital form and the accessibility of the electronic libraries through the Internet. As a result, plagiarism detection has been transformed into a big data analytics problem since the number of digital sources is extravagant and a new document needs to be compared with millions of other existing documents. In this paper, a text mining methodology is proposed that can detect all common patterns between a document and the documents in a reference database. The technique is based on a pattern detection algorithm and the corresponding data structure that enables the algorithm to detect all common patterns. The methodology has been applied in a well-defined dataset providing very promising results identifying difficult cases of plagiarism such as technical disguise.",
        "link": "https://dl.acm.org/doi/10.5555/3382225.3382424"
    },
    {
        "title": "Petroleum Engineering Data Text Classification Using Convolutional Neural Network Based Classifier",
        "authors": [
            "Tzuhan Hsu",
            "Yaoqin Zhang"
        ],
        "date": "May 2018",
        "source": "ICMLT '18: Proceedings of the 2018 International Conference on Machine Learning Technologies",
        "abstract": "In this paper, authors introduce a convolutional neural network (CNN) based classification approach for petroleum engineering event. Recently, text classification problem has raised substantial attention in the field of machine learning. The aim of text classification process is to assign correct label to a text on the basis of its content. However, due to the imprecise of feature representation, most of former text classification methods cannot export satisfying result. To solve this problem, authors propose a model based on convolutional neural network. Authors conduct experiments with real world petroleum engineering data text, artificially labelled into 6 categories. Beside purposed CNN-based model, the authors also implement other baseline machine learning approaches: k nearest neighbor, random forest, support vector machines, recurrent neural network, gated recurrent units, long short-term memory. Among all these models, our CNN-based model reach best performance in text classification task. In the last part of this paper, authors compare and analysis their purpose and baseline model.",
        "link": "https://dl.acm.org/doi/10.1145/3231884.3231898"
    },
    {
        "title": "Classifying Indonesian Online Articles as Advertisement Placement Base Using Text Mining",
        "authors": [
            "Nadhira Tasya",
            "Arian Dhini"
        ],
        "date": "July 2017",
        "source": "ICBIM 2017: Proceedings of the International Conference on Business and Information Management",
        "abstract": "Rapid development in technological aspect resulting in growing level of human needs for the latest news, so that emerged a new trend of publishing and accessing news through online media or usually called online journalism. In addition, the number of people who sell and purchase through online sites also continues to increase and this opportunity is utilized by the company and the advertiser by implementing targeted web advertising. However, the high number of articles that have been published and accessed leads to great opportunities for errors in determining where to place the ads. Therefore, it needs a system that can categorize articles accessed by users as the basis of advertisement placement by the company and this classification system can be done by applying the method of Data Mining and Text Mining. This research uses document data in the form of article content that will be categorized into twenty categories of class of advertisement by using Text Mining technique with Support Vector Machine algorithm. The results of this study may be used by companies or advertisers as a basis for placement of ads on selected online media sites.",
        "link": "https://dl.acm.org/doi/10.1145/3134271.3134288"
    },
    {
        "title": "Mining Free-Text Medical Notes for Suicide Risk Assessment",
        "authors": [
            "Marios Adamou",
            "Grigoris Antoniou",
            "Elissavet Greasidou",
            "Vincenzo Lagani",
            "Paulos Charonyktakis",
            "Ioannis Tsamardinos"
        ],
        "date": "July 2018",
        "source": "SETN '18: Proceedings of the 10th Hellenic Conference on Artificial Intelligence",
        "abstract": "Suicide has been considered as an important public health issue for a very long time, and is one of the main causes of death worldwide. Despite suicide prevention strategies being applied, the rate of suicide has not changed substantially over the past decades. Advances in machine learning make it possible to attempt to predict suicide based on the analysis of relevant data to inform clinical practice. This paper reports on findings from the analysis of data of patients who died by suicide in the period 2013-2016 and made use of both structured data and free-text medical notes. We focus on examining various text-mining approaches to support risk assessment. The results show that using advance machine learning and text-mining techniques, it is possible to predict within a specified period which people are most at risk of taking their own life at the time of referral to a mental health service.",
        "link": "https://dl.acm.org/doi/10.1145/3200947.3201020"
    },
    {
        "title": "Integration of data mining techniques in e-learning systems: Clustering Profil of Lerners and Recommender Course System",
        "authors": [
            "A. El Moustamid",
            "E. En-Naimi",
            "J. El Bouhdidi"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "Data mining aims at the extraction of knowledge from large quantities of data by automatic or semi-automatic methods. In the field of education and training e-learning, Data Mining techniques as clustering (The aim of this technique is to identify groups that are similar in some aspect) [21], prediction (is used for developing model where combination of data viewed from different aspects can effect single viewed data. Here, is important to watch how predictors variables are effecting predicted variable)[21], classification are applied in e-learning to improve the level of learners by providing learning according to the learners' requirement) are used to analyze learners profile, to predict and improve student performance. The aim of our project is to develop a system capable of analyzing learners profile and indexing web videos in order to offer learners a rich database with courses that correspond to their levels.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090453"
    },
    {
        "title": "Multi-level mining and visualization of scientific text collections: Exploring a bi-lingual scientific repository",
        "authors": [
            "Pablo Accuosto",
            "Francesco Ronzano",
            "Daniel Ferrés",
            "Horacio Saggion"
        ],
        "date": "December 2017",
        "source": "WOSP 2017: Proceedings of the 6th International Workshop on Mining Scientific Publications",
        "abstract": "We present a system to mine and visualize collections of scientific documents by semantically browsing information extracted from single publications or aggregated throughout corpora of articles. The text mining tool performs deep analysis of document collections allowing the extraction and interpretation of research paper's contents. In addition to the extraction and enrichment of documents with metadata (titles, authors, affiliations, etc), the deep analysis performed comprises semantic interpretation, rhetorical analysis of sentences, triple-based information extraction, and text summarization. The visualization components allow geographical-based exploration of collections, topic-evolution interpretation, and collaborative network analysis among others. The paper presents a case study of a bi-lingual collection in the field of Natural Language Processing (NLP).",
        "link": "https://dl.acm.org/doi/10.1145/3127526.3127529"
    },
    {
        "title": "Hierarchical Topic Mining via Joint Spherical Tree and Text Embedding",
        "authors": [
            "Yu Meng",
            "Yunyi Zhang",
            "Jiaxin Huang",
            "Yu Zhang",
            "Chao Zhang",
            "Jiawei Han"
        ],
        "date": "August 2020",
        "source": "KDD '20: Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Mining a set of meaningful topics organized into a hierarchy is intuitively appealing since topic correlations are ubiquitous in massive text corpora. To account for potential hierarchical topic structures, hierarchical topic models generalize flat topic models by incorporating latent topic hierarchies into their generative modeling process. However, due to their purely unsupervised nature, the learned topic hierarchy often deviates from users' particular needs or interests. To guide the hierarchical topic discovery process with minimal user supervision, we propose a new task, Hierarchical Topic Mining, which takes a category tree described by category names only, and aims to mine a set of representative terms for each category from a text corpus to help a user comprehend his/her interested topics. We develop a novel joint tree and text embedding method along with a principled optimization procedure that allows simultaneous modeling of the category tree structure and the corpus generative process in the spherical space for effective category-representative term discovery. Our comprehensive experiments show that our model, named JoSH, mines a high-quality set of hierarchical topics with high efficiency and benefits weakly-supervised hierarchical text classification tasks.",
        "link": "https://dl.acm.org/doi/10.1145/3394486.3403242"
    },
    {
        "title": "Unsupervised behavioural mining and clustering for malware family identification",
        "authors": [
            "Khanh Huu The Dam",
            "Thomas Given-Wilson",
            "Axel Legay"
        ],
        "date": "March 2021",
        "source": "SAC '21: Proceedings of the 36th Annual ACM Symposium on Applied Computing",
        "abstract": "More accurate and advanced detection and classification of malware requires exploiting program behaviour and not merely syntactic or static features. One approach is to use system call dependency graphs (SCDGs) that represent the program behaviour by interactions with the system, and the relations between these interactions. These SCDGs have been used with supervised learning techniques to very accurately detect and classify malware. This works considers the unsupervised learning challenge of mining for common clusters of behaviour without a priori knowledge. This allows for clustering of similar programs by behaviour, that can then be used for either classification or further analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3412841.3441919"
    },
    {
        "title": "Text Mining for Evaluating Authors' Birth and Death Years",
        "authors": [
            "Dror Moghaz",
            "Yaakov Hacohen-Kerner",
            "Dov Gabbay"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "This article presents a unique method in text and data mining for finding the era, i.e., mining temporal data, in which an anonymous author was living. Finding this era can assist in the examination of a fake document or extracting the time period in which a writer lived. The study and the experiments concern Hebrew, and in some parts, Aramaic and Yiddish rabbinic texts. The rabbinic texts are undated and contain no bibliographic sections, posing an interesting challenge. This work proposes algorithms using key phrases and key words that allow the temporal organization of citations together with linguistic patterns. Based on these key phrases, key words, and the references, we established several types of “Iron-clad,” Heuristic and Greedy rules for estimating the years of birth and death of a writer in an interesting classification task. Experiments were conducted on corpora, including documents authored by 12, 24, and 36 rabbinic writers and demonstrated promising results.",
        "link": "https://dl.acm.org/doi/10.1145/3281631"
    },
    {
        "title": "Transfer Learning to Timed Text Based Video Classification Using CNN",
        "authors": [
            "Zenun Kastrati",
            "Ali Shariq Imran",
            "Arianit Kurti"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "Open educational video resources are gaining popularity with a growing number of massive open online courses (MOOCs). This has created a niche for content providers to adopt effective solutions in automatically organizing and structuring of educational resources for maximum visibility. Recent advances in deep learning techniques are proving useful in managing and classifying resources into appropriate categories. This paper proposes one such convolutional neural network (CNN) model for classifying video lectures in a MOOC setting using a transfer learning approach. The model uses a time-aligned text transcripts corresponding to video lectures from six broader subject categories. Video lectures and their corresponding transcript dataset is gathered from the Coursera MOOC platform. Two different CNN models are proposed: i) CNN based classification using embeddings learned from our MOOC dataset, ii) CNN based classification using transfer learning. Word embeddings generated from two well known state-of-the-art pre-trained models Word2Vec and GloVe, are used in the transfer learning approach for the second case. The proposed CNN models are evaluated using precision, recall, and F1 score and the obtained performance is compared with both conventional and deep learning classifiers. The proposed CNN models have an F1 score improvement of 10-22 percentage points over DNN and conventional classifiers",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326483"
    },
    {
        "title": "Evolutionary role mining in complex networks by ensemble clustering",
        "authors": [
            "Sarvenaz Choobdar",
            "Pedro Ribeiro",
            "Fernando Silva"
        ],
        "date": "April 2017",
        "source": "SAC '17: Proceedings of the Symposium on Applied Computing",
        "abstract": "The structural patterns in the neighborhood of nodes assign unique roles to the nodes. Mining the set of existing roles in a network provides a descriptive profile of the network and draws its general picture. This paper proposes a new method to determine structural roles in a dynamic network based on the current position of nodes and their historic behavior. We develop a temporal ensemble clustering technique to dynamically find groups of nodes, holding similar tempo-structural roles. We compare two weighting functions, based on age and distribution of data, to incorporate temporal behavior of nodes in the role discovery. To evaluate the performance of the proposed method, we assess the results from two points of view: 1) goodness of fit to current structure of the network; 2) consistency with historic data. We conduct the evaluation using different ensemble clustering techniques. The results on real world networks demonstrate that our method can detect tempo-structural roles that simultaneously depict the topology of a network and reflect its dynamics with high accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3019612.3019815"
    },
    {
        "title": "Analyzing used-car web listings via text mining",
        "authors": [
            "Ayhan Demiriz",
            "Fatma Cantaş"
        ],
        "date": "October 2017",
        "source": "IML '17: Proceedings of the 1st International Conference on Internet of Things and Machine Learning",
        "abstract": "Used car trade is one of the major components of the world economies. It is not uncommon to sell a car by placing an internet advertisement irrespective of the geography in these days. A typical content of an advertisement is usually composed of two parts namely the structured and the free text data. The structured data may include some information about the asking price, make, model, year, mileage of the car and the contact info. In most cases, seller may give important clues about the car's current conditions in the free text data where the title (head) of the advertisement can be included as free text too. This paper reports preliminary results from a text mining study conducted on 75K used car internet listings collected from two major car listing web sites in Turkey. As expected, the words and the phrases related to the description of the car are observed to be frequent. The leading concepts in the free text are found to be regarding how to describe the current condition of a car, for example \"no crash history\".",
        "link": "https://dl.acm.org/doi/10.1145/3109761.3109782"
    },
    {
        "title": "Text classification of Diseases Treated by Traditional Chinese Medicine Prescription based on machine learning",
        "authors": [
            "Hanqing Zhao",
            "Bo Han",
            "Chen Li"
        ],
        "date": "September 2020",
        "source": "ISAIMS '20: Proceedings of the 1st International Symposium on Artificial Intelligence in Medical Sciences",
        "abstract": "OBJECTIVE: To explore the application of machine learning in th e identification of diseases treated by traditional Chinese medicine prescriptions. METHODS: Based on the composition of the text document of Chinese medicine prescriptions, the prescriptions were divided into cough, headache and diarrhea. THUTCT were introduced to establish and train two machine learning text classification models, LibSVM and LibLinear, and the prescriptions to be deter mined were put into the model for classification and prediction. R ESULTS: The Precision rate of LibSVM model and Liblinear mod el were 0.7283 and 0.6690. Seven prescriptions were classified an d predicted, and the results were in line with expectations. CONCLUSION: THUCTC has good universality for the content of TCM prescriptions, high classification accuracy and fast testing speed, which is suitable for the text classification and discrimination research of TCM prescriptions for diseases.",
        "link": "https://dl.acm.org/doi/10.1145/3429889.3429895"
    },
    {
        "title": "Evaluation of retweet clustering method classification method using retweets on Twitter without text data",
        "authors": [
            "K. Uchida",
            "F. Toriumi",
            "T. Sakaki"
        ],
        "date": "August 2017",
        "source": "WI '17: Proceedings of the International Conference on Web Intelligence",
        "abstract": "Burst phenomena, which frequently occur on social media, are caused by such social events as flaming on the internet, elections, and natural disasters. To understand people's thoughts and feelings, we must classify their opinions from burst phenomena. Therefore, classification methods that categorize tweets are critical. However, since most classification methods focus on text mining, they cannot group tweets by topics because each tweet has poor linguistic similarities. We used a non-text-based classification method proposed by Baba et al. that groups tweets by topics, even if they have poor linguistic similarities, and verified its validity by comparing it with a text-based classification method in two different evaluations: qualitative and quantitative. In the qualitative evaluation part, we did a questionnaire survey and validated the suitability of the topic clusters created using both the non-and text-based methods. Since evaluating the similarity of every pair of tweets in each topic is difficult, we evaluated the similarity between sampled pairs in the survey and acquired more appropriate topic clustering results using the non-text-based method than the text-based method. In the quantitative evaluation part, we focused on the robustness of each method against data reduction. Many approaches analyze social media data, especially because collecting data from social media is comparatively easy. However, since collecting the whole data of burst phenomena is very costly due to the vast amounts of available social media data, robustness against data reduction is an important index to evaluate classification methods. With the non-text-based method, over 55% of the pairs of tweets in the same cluster were also included in the same cluster even when the data were reduced to 10% in all three of our example cases. In this paper, as a source we focus on Twitter, one of the most popular microblogging services. Using clustering to conduct detailed case analyses, we scrutinized three burst cases that include natural disasters and flaming on the internet and found that a non-text-based method more effectively classified tweets in burst phenomena than a text-based method.",
        "link": "https://dl.acm.org/doi/10.1145/3106426.3106451"
    },
    {
        "title": "Using Spark for Text Mining on Large Scale Liver Cancer Literature",
        "authors": [
            "Ming-Yen Lin",
            "Yu-Ju Lin",
            "Sue-Chen Hsueh"
        ],
        "date": "January 2021",
        "source": "BDET 2021: 2021 the 3rd International Conference on Big Data Engineering and Technology (BDET)",
        "abstract": "Cancer is one of the main causes of death. The number of known cancer to-date is more than one hundred. Liver cancer, ranked after trachea and lung cancer, has long been high in the cancer leading cause in Taiwan and even ranked as second in 2016. The number of scientific articles related to cancer proliferates every year. The number reaches as high as 20 million in PubMed so that discovering useful information from the massive collection is very difficult. In addition, using a single machine to sift through these articles is very time-consuming. Therefore, we present a big data analytic framework using the distributed Apache Spark platform for text mining in PubMed literature. We establish a prediction model for liver cancer articles so that researchers may effectively validate whether an article is related to liver cancer or not. Classification models in Spark MLlib including Linear Support Vector Machines (SVM), Logistic Regression, are used in our experiments. Relevancy to liver cancer is further confirmed by using MeSH (Medical Subject Headings) terms. Logistic regression is about 3 times faster than SVMs and the accuracy of both methods is close to 95% in the experiments using hold-out validation. When max_features is 500 and min_df ≤ 0.1 (or min_df = 1), the accuracy may reach 96%. In the experiments with K-fold cross-validation, the accuracy of SVMs methods is 96%. The experimental results show that that our prediction model may effectively classify liver cancer articles.",
        "link": "https://dl.acm.org/doi/10.1145/3474944.3474958"
    },
    {
        "title": "Automatic topic classification of test cases using text mining at an Android smartphone vendor",
        "authors": [
            "Junji Shimagaki",
            "Yasutaka Kamei",
            "Naoyasu Ubayashi",
            "Abram Hindle"
        ],
        "date": "October 2018",
        "source": "ESEM '18: Proceedings of the 12th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Background: An Android smartphone is an ecosystem of applications, drivers, operating system components, and assets. The volume of the software is large and the number of test cases needed to cover the functionality of an Android system is substantial. Enormous effort has been already taken to properly quantify \"what features and apps were tested and verified?\". This insight is provided by dashboards that summarize test coverage and results per feature. One method to achieve this is to manually tag or label test cases with the topic or function they cover, much like function points. At the studied Android smartphone vendor, tests are labelled with manually defined tags, so-called \"feature labels (FLs)\", and the FLs serve to categorize 100s to 1000s test cases into 10 to 50 groups. Aim: Unfortunately for developers, manual assignment of FLs to 1000s of test cases is a time consuming task, leading to inaccurately labeled test cases, which will render the dashboard useless. We created an automated system that suggests tags/labels to the developers for their test cases rather than manual labeling. Method: We use machine learning models to predict and label the functionality tested by 10,000 test cases developed at the company. Results: Through the quantitative experiments, our models achieved acceptable F-1 performance of 0.3 to 0.88. Also through the qualitative studies with expert teams, we showed that the hierarchy and path of tests was a good predictor of a feature's label. Conclusions: We find that this method can reduce tedious manual effort that software developers spent classifying test cases, while providing more accurate classification results.",
        "link": "https://dl.acm.org/doi/10.1145/3239235.3268927"
    },
    {
        "title": "To apply Data Mining for Classification of Crowd sourced Software Requirements",
        "authors": [
            "Soonh Taj",
            "Qasim Arain",
            "Imran Memon",
            "Asma Zubedi"
        ],
        "date": "April 2019",
        "source": "ICSIE '19: Proceedings of the 8th International Conference on Software and Information Engineering",
        "abstract": "Now a day's main focus of developers is to build quality software that works according to customer needs and for this reason it is necessary to gather right requirements as requirement elicitation is the critical step that impacts on the success of software project as misinterpreted requirements leads to the failure of software project. By keeping this in mind a research is carried out on improving requirements elicitation process and automating the process of classifying requirements. In this research, a model is proposed which will help in this scenario for requirements elicitation and requirement classification. This paper presents a model in which crowd sourcing approach is used so that customers, end users, stakeholders, developers and software engineers can make active participation for requirement elicitation process and requirements gathered using crowdsourcing approach are used by model for classification process i.e. classification of requirements into functional and non-functional requirements. For the proof of proposed model a case study is conducted. Results of case study provided the usefulness and efficiency of proposed model for classification of crowd sourced software requirements.",
        "link": "https://dl.acm.org/doi/10.1145/3328833.3328837"
    },
    {
        "title": "Text classification using Fuzzy TF-IDF and Machine Learning Models",
        "authors": [
            "Mariem Bounabi",
            "Karim El Moutaouakil",
            "Khalid Satori"
        ],
        "date": "October 2019",
        "source": "BDIoT'19: Proceedings of the 4th International Conference on Big Data and Internet of Things",
        "abstract": "The representation of the information has an important impact on the text classification task. Several weighting methods were proposed in the literature, and the term frequency-inverse term frequency (TFIDF), the most know on the text treatment field. The FTF-IDF is a vector representation where the components of the TFIDF are presented as inputs to the Fuzzy Inference System (FIS). In this work, we compare several Machin Learning algorithms such as Naïve Bayes and its derivatives, SVM and Random forest classifiers, using the FTF-IDF representation. To improve the quality of the used classifiers, we call sum attribute selection methods. The recognition rate, for the tested systems, is satisfied, where the system based on naïve Bayes classifier, the FTF-IDF weighting terms, and the info gain select attributes method gives 98.7% as accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3372938.3372956"
    },
    {
        "title": "Text Mining in Cybersecurity: A Systematic Literature Review",
        "authors": [
            "Luciano Ignaczak",
            "Guilherme Goldschmidt",
            "Cristiano André Da Costa",
            "Rodrigo Da Rosa Righi"
        ],
        "date": null,
        "source": "ACM Computing Surveys",
        "abstract": "The growth of data volume has changed cybersecurity activities, demanding a higher level of automation. In this new cybersecurity landscape, text mining emerged as an alternative to improve the efficiency of the activities involving unstructured data. This article proposes a Systematic Literature Review (SLR) to present the application of text mining in the cybersecurity domain. Using a systematic protocol, we identified 2,196 studies, out of which 83 were summarized. As a contribution, we propose a taxonomy to demonstrate the different activities in the cybersecurity domain supported by text mining. We also detail the strategies evaluated in the application of text mining tasks and the use of neural networks to support activities involving unstructured data. The work also discusses text classification performance aiming its application in real-world solutions. The SLR also highlights open gaps for future research, such as the analysis of non-English content and the intensification in the usage of neural networks.",
        "link": "https://dl.acm.org/doi/10.1145/3462477"
    },
    {
        "title": "Using Text Mining to Analyze the Financial Patents",
        "authors": [
            "Yung-Feng Lu",
            "Jia-Lang Xu",
            "Mu-Yen Chen"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "With the advancement and development of science and technology, Nowadays society is the development of information technology, cloud computing, mobile Internet, Internet of Things, block chain and information technology.The combination of finance and technology has made financial technology terms gradually emerging.. This research analyzes the patents of financial technology. However, Taiwan's financial technology patents are mostly G06Q-20 (payment plan, architecture or agreement), G06Q-30 (commercial, such as marketing, shopping, payment, auction or e-commerce). G06Q-40 (financial, investment or tax treatment, insurance).The research use text mining technology to analyze and understand Taiwan's trends and developments in financial technology. The research results show that G06Q-20 patents are biased towards information security and mobile payment. G06Q-30 patents favor shopping websites and advertising. G06Q-40 patents are biased towards diversity, such as securities, insurance, finance, and stocks.",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326478"
    },
    {
        "title": "Identifying Online Profiles of Distance Learning Students Using Data Mining Techniques",
        "authors": [
            "Osama Islam",
            "Muazzam Siddiqui",
            "Naif Radi Aljohani"
        ],
        "date": "October 2019",
        "source": "ICDTE '19: Proceedings of the 3rd International Conference on Digital Technology in Education",
        "abstract": "Educational data has grown over the years with the increased use of technology within educational environments. This has led to a huge amount of data being stored in various data sources representing the student, his/her activities, and other aspects relevant to the learning process. To meet this analytical need, Educational Data Mining (EDM) has emerged to assist educational institutions in identifying key benefits such as students at risk, the level of student engagement or predicting student performance. The aim of this research was to explore the various aspects of student interaction data using data mining techniques to identify relevant patterns of behaviors and possible key attributes that have higher degrees of influence on distance learning students. The main findings identified several patterns of user online profiles based on a set of adopted learning strategies, the research proposed a framework for analyzing such interaction data based on R and Hadoop platforms to correlate online profiles with student performance.",
        "link": "https://dl.acm.org/doi/10.1145/3369199.3369249"
    },
    {
        "title": "Extremely Fast Decision Tree Mining for Evolving Data Streams",
        "authors": [
            "Albert Bifet",
            "Jiajin Zhang",
            "Wei Fan",
            "Cheng He",
            "Jianfeng Zhang",
            "Jianfeng Qian",
            "Geoff Holmes",
            "Bernhard Pfahringer"
        ],
        "date": "August 2017",
        "source": "KDD '17: Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
        "abstract": "Nowadays real-time industrial applications are generating a huge amount of data continuously every day. To process these large data streams, we need fast and efficient methodologies and systems. A useful feature desired for data scientists and analysts is to have easy to visualize and understand machine learning models. Decision trees are preferred in many real-time applications for this reason, and also, because combined in an ensemble, they are one of the most powerful methods in machine learning. In this paper, we present a new system called STREAMDM-C++, that implements decision trees for data streams in C++, and that has been used extensively at Huawei. Streaming decision trees adapt to changes on streams, a huge advantage since standard decision trees are built using a snapshot of data, and can not evolve over time. STREAMDM-C++ is easy to extend, and contains more powerful ensemble methods, and a more efficient and easy to use adaptive decision trees. We compare our new implementation with VFML, the current state of the art implementation in C, and show how our new system outperforms VFML in speed using less resources.",
        "link": "https://dl.acm.org/doi/10.1145/3097983.3098139"
    },
    {
        "title": "Data miners' little helper: data transformation activity cues for cluster analysis on document collections",
        "authors": [
            "Tania Cerquitelli",
            "Evelina Di Corso",
            "Francesco Ventura",
            "Silvia Chiusano"
        ],
        "date": "June 2017",
        "source": "WIMS '17: Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "In this paper we propose a new self-learning engine to streamline the analytics process, as it enables analysts to mine massive data repositories with minimal user intervention. In the context of cluster analysis on a collection of documents this new system, named SELF-DATA (SELF-learning DAta TrAnsformation), suggests to the analyst how to configure the whole mining process for a given dataset. SELF-DATA relies on an engine exploring different data weighting schemas (e.g., normalized term frequencies) and data transformation methods (e.g., PCA) before applying the cluster analysis, evaluating and comparing solutions through different quality indices (e.g., weighted Silhouette), and presenting the k-top solutions to the analyst. SELF-DATA will also include a knowledge base storing results of experiments on previously processed datasets, and a classification algorithm trained on the knowledge base content to forecast the best configuration for the whole mining process for an unexplored dataset. The first development of SELF-DATA running on Apache Spark has been validated on 5 collections of documents. Experimental results highlight that TF-IDF and logarithmic entropy are effective to measure item relevance with sparse datasets, and the LSI method outperforms PCA with a large dictionary.",
        "link": "https://dl.acm.org/doi/10.1145/3102254.3102288"
    },
    {
        "title": "Interactive Visual Graph Mining and Learning",
        "authors": [
            "Ryan A. Rossi",
            "Nesreen K. Ahmed",
            "Rong Zhou",
            "Hoda Eldardiry"
        ],
        "date": null,
        "source": "ACM Transactions on Intelligent Systems and Technology",
        "abstract": "This article presents a platform for interactive graph mining and relational machine learning called GraphVis. The platform combines interactive visual representations with state-of-the-art graph mining and relational machine learning techniques to aid in revealing important insights quickly as well as learning an appropriate and highly predictive model for a particular task (e.g., classification, link prediction, discovering the roles of nodes, and finding influential nodes). Visual representations and interaction techniques and tools are developed for simple, fast, and intuitive real-time interactive exploration, mining, and modeling of graph data. In particular, we propose techniques for interactive relational learning (e.g., node/link classification), interactive link prediction and weighting, role discovery and community detection, higher-order network analysis (via graphlets, network motifs), among others. GraphVis also allows for the refinement and tuning of graph mining and relational learning methods for specific application domains and constraints via an end-to-end interactive visual analytic pipeline that learns, infers, and provides rapid interactive visualization with immediate feedback at each change/prediction in real-time. Other key aspects include interactive filtering, querying, ranking, manipulating, exporting, as well as tools for dynamic network analysis and visualization, interactive graph generators (including new block model approaches), and a variety of multi-level network analysis techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3200764"
    },
    {
        "title": "Data Mining and Opinion Mining: A Tool in Educational Context",
        "authors": [
            "Myriam Peñafiel",
            "Stefanie Vásquez",
            "Diego Vásquez",
            "Juan Zaldumbide",
            "Sergio Luján-Mora"
        ],
        "date": "July 2018",
        "source": "ICoMS '18: Proceedings of the 2018 1st International Conference on Mathematics and Statistics",
        "abstract": "The use of the web as a universal communication platform generates large volumes of data (Big data), which in many cases, need to be processed so that they can become useful knowledge in face of the sceptics who have doubts about the credibility of such information. The use of web data that comes from educational contexts needs to be addressed, since that large amount of unstructured information is not being valued, losing valuable information that can be used. To solve this problem, we propose the use of data mining techniques such as sentiment analysis to validate the information that comes from the educational platforms. The objective of this research is to propose a methodology that allows the user to apply sentiment analysis in a simple way, because although some researchers have done it, very few do with data in the educational context. The results obtained prove that the proposal can be used in similar cases.",
        "link": "https://dl.acm.org/doi/10.1145/3274250.3274263"
    },
    {
        "title": "Research Hotspots Mining and Visualized Analysis Based on Linking Cluster and K-Core Decomposition",
        "authors": [
            "Ying Cai",
            "Fang Huang",
            "Shuai Wang",
            "Hao Zhang",
            "Chunxiu Du"
        ],
        "date": "May 2018",
        "source": "ICDPA 2018: Proceedings of the International Conference on Data Processing and Applications",
        "abstract": "In this paper, we focus on research hotspots mining with knowledge semantic features on discipline topic word networks, which are extracted from scientific and technical documents. For the topic networks, a novel method of combining linking community clustering with k-core decomposition is proposed to mine the hotspot community with knowledge hierarchy structure. And as a post-processing, the hierarchical community identification is implemented by k-core decomposition in order to discover the internal connection with knowledge semantic hierarchy in the research hotspot. Finally, the community density and visualization method is utilized to analyze the core and sub- problem in the research hotspot community. In experiments, the topic networks for a single discipline and interdisciplinary research hotspot are taken as examples of analysis, and the results show the proposed approach can effectively realize the objective of mining and analyzing the discipline research hotspot on the academic topic word network.",
        "link": "https://dl.acm.org/doi/10.1145/3224207.3224216"
    },
    {
        "title": "The Application of Data Mining Algorithms in the Construction of Travel Recommendation System",
        "authors": [
            "Chao Lou",
            "Zhaonan Mu",
            "Mengzhu Liu"
        ],
        "date": "January 2021",
        "source": "CONF-CDS 2021: The 2nd International Conference on Computing and Data Science",
        "abstract": "Photos shared by users of online social network often associated with tags, texts, geographic and time information. These data are ideal research resource which can be used by researchers to observe people's interests and behaviors. They can also be used to analyze people's travel behaviors and interests because it can reflect people's travel activities and experiences directly. This project aimed to consolidate people into groups based on their travel preferences of travel season, travel time span, scenery type of travel location and then analyze popular travel places for different people groups. The travel preferences were analyzed from extracted data applying PHP language and Naive Bayes classification approach. According to the analyzed data, people were divided into groups. Then, DBSCAN clustering was used to get popular locations based on the different groups. As a result, 80 thousand photo data of 5 cities have been collected from Flickr by Phython. It has been showed that people's preference of season in most of the mined cities was spring and summer and the preference of time span was short. Moreover, mined people has been divided into 16 groups according to their preference and based on the groups popular locations have been calculated by DBSCAN clustering. Theses locations have been provided in a recommendation website which was built in this project. For the future work, mining Flickr data associated with other travel social network sites could be a good way to build a more effective recommendation system.",
        "link": "https://dl.acm.org/doi/10.1145/3448734.3450477"
    },
    {
        "title": "Compare Machine Learning Models in Text Classification Using Steam User Reviews",
        "authors": [
            "Youchen Miao",
            "Zeyu Jin",
            "Yumeng Zhang",
            "Yuchen Chen",
            "Junren Lai"
        ],
        "date": "November 2021",
        "source": "ICSED '21: Proceedings of the 2021 3rd International Conference on Software Engineering and Development",
        "abstract": "Text Classification and Sentiment Analysis of game reviews are viewed as important parts in not only academic fields but also in game studies. In this paper, with more than 400 thousand game reviews on Steam platform, we preprocess the data using different libraries (sklearn, nltk, and spaCy) and use them as inputs to build three sentiment classification models based on different algorithms (Naive Bayes, SVM, and Random Forest). In contrast to previous studies that only focus on different sentiment analysis models, our paper also highlights the use of different APIs to preprocess the data and their corresponding model performance. The results show that no matter which API we choose, Random Forest models always perform the best. However, in terms of training time, Naive Bayes is the fastest. This work can be used to apply grid search for researchers to automatically find the optimum API before conducting sentiment analysis in the future.",
        "link": "https://dl.acm.org/doi/10.1145/3507473.3507480"
    },
    {
        "title": "Lost in Transduction: Transductive Transfer Learning in Text Classification",
        "authors": [
            "Alejandro Moreo",
            "Andrea Esuli",
            "Fabrizio Sebastiani"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Obtaining high-quality labelled data for training a classifier in a new application domain is often costly. Transfer Learning\n(a.k.a. “Inductive Transfer”) tries to alleviate these costs by transferring, to the “target” domain of interest, knowledge available from a different “source” domain. In transfer learning the lack of labelled information from the target domain is compensated by the availability at training time of a set of unlabelled examples from the target distribution. Transductive Transfer Learning denotes the transfer learning setting in which the only set of target documents that we are interested in classifying is known and available at training time. Although this definition is indeed in line with Vapnik’s original definition of “transduction”, current terminology in the field is confused. In this article, we discuss how the term “transduction” has been misused in the transfer learning literature, and propose a clarification consistent with the original characterization of this term given by Vapnik. We go on to observe that the above terminology misuse has brought about misleading experimental comparisons, with inductive transfer learning methods that have been incorrectly compared with transductive transfer learning methods. We then, give empirical evidence that the difference in performance between the inductive version and the transductive version of a transfer learning method can indeed be statistically significant (i.e., that knowing at training time the only data one needs to classify indeed gives an advantage). Our clarification allows a reassessment of the field, and of the relative merits of the major, state-of-the-art algorithms for transfer learning in text classification.",
        "link": "https://dl.acm.org/doi/10.1145/3453146"
    },
    {
        "title": "Machine Learning on Mining Potential Adverse Drug Reactions for Pharmacovigilance",
        "authors": [
            "Ke Jia"
        ],
        "date": "July 2021",
        "source": "DSIT 2021: 2021 4th International Conference on Data Science and Information Technology",
        "abstract": "Drugs are related to the lives and health of patients. Mining and discovering potential adverse drug reactions from a large number of unstructured texts play an important role in drug research and pharmacovigilance. It is a hot research issue in the field of biomedicine in recent years. A large number of scholars have discovered a large amount of information on adverse drug reactions from different data sources through effective machine learning methods. This paper reviews the research progress in recent years to provide a reference for future research.",
        "link": "https://dl.acm.org/doi/10.1145/3478905.3478964"
    },
    {
        "title": "TACAM: Topic And Context Aware Argument Mining",
        "authors": [
            "Michael Fromm",
            "Evgeniy Faerman",
            "Thomas Seidl"
        ],
        "date": "October 2019",
        "source": "WI '19: IEEE/WIC/ACM International Conference on Web Intelligence",
        "abstract": "In this work we address the problem of argument search. The purpose of argument search is the distillation of pro and contra arguments for requested topics from large text corpora. In previous works, the usual approach is to use a standard search engine to extract text parts which are relevant to the given topic and subsequently use an argument recognition algorithm to select arguments from them. The main challenge in the argument recognition task, which is also known as argument mining, is that often sentences containing arguments are structurally similar to purely informative sentences without any stance about the topic. In fact, they only differ semantically. Most approaches use topic or search term information only for the first search step and therefore assume that arguments can be classified independently of a topic. We argue that topic information is crucial for argument mining, since the topic defines the semantic context of an argument. Precisely, we propose different models for the classification of arguments, which take information about a topic of an argument into account. Moreover, to enrich the context of a topic and to let models understand the context of the potential argument better, we integrate information from different external sources such as Knowledge Graphs or pre-trained NLP models. Our evaluation shows that considering topic information, especially in connection with external information, provides a significant performance boost for the argument mining task.",
        "link": "https://dl.acm.org/doi/10.1145/3350546.3352506"
    },
    {
        "title": "Text mining for incoming tasks based on the urgency/importance factors and task classification using machine learning tools",
        "authors": [
            "Yasser Ali Alshehri"
        ],
        "date": "March 2020",
        "source": "ICCDA 2020: Proceedings of the 2020 the 4th International Conference on Compute and Data Analysis",
        "abstract": "In workplaces, there is a massive amount of unstructured data from different sources. In this paper, we present a case study that explains how can through communications between employees, we can help to prioritize tasks requests to increase the efficiency of their works for both technical and non-technical workers. This involves managing daily incoming tasks based on their level of urgency and importance.To allow all workers to utilize the urgency-importance matrix as a time-management tool, we need to automate this tool. The textual content of incoming tasks are analyzed, and metrics related to urgency and importance are extracted. A third factor (i.e., the response variable) is defined based on the two input variables (urgency and importance). Then, machine learning applied to the data to predict the class of incoming tasks based on data outcome desired. We used ordinal regression, neural networks, and decision tree algorithms to predict the four levels of task priority. We measure the performance of all using recalls, precisions, and F-scores. All classifiers perform higher than 89% in terms of all measures.",
        "link": "https://dl.acm.org/doi/10.1145/3388142.3388153"
    },
    {
        "title": "Text mining for malware classification using multivariate all repeated patterns detection",
        "authors": [
            "Konstantinos F. Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2019",
        "source": "ASONAM '19: Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Mobile phones have become nowadays a commodity to the majority of people. Using them, people are able to access the world of Internet and connect with their friends, their colleagues at work or even unknown people with common interests. This proliferation of the mobile devices has also been seen as an opportunity for the cyber criminals to deceive smartphone users and steel their money directly or indirectly, respectively, by accessing their bank accounts through the smartphones or by blackmailing them or selling their private data such as photos, credit card data, etc. to third parties. This is usually achieved by installing malware to smartphones masking their malevolent payload as a legitimate application and advertise it to the users with the hope that mobile users will install it in their devices. Thus, any existing application can easily be modified by integrating a malware and then presented it as a legitimate one. In response to this, scientists have proposed a number of malware detection and classification methods using a variety of techniques. Even though, several of them achieve relatively high precision in malware classification, there is still space for improvement. In this paper, we propose a text mining all repeated pattern detection method which uses the decompiled files of an application in order to classify a suspicious application into one of the known malware families. Based on the experimental results using a real malware dataset, the methodology tries to correctly classify (without any misclassification) all randomly selected malware applications of 3 categories with 3 different families each.",
        "link": "https://dl.acm.org/doi/10.1145/3341161.3350841"
    },
    {
        "title": "Comparative Study between Traditional Machine Learning and Deep Learning Approaches for Text Classification",
        "authors": [
            "Cannannore Nidhi Kamath",
            "Syed Saqib Bukhari",
            "Andreas Dengel"
        ],
        "date": "August 2018",
        "source": "DocEng '18: Proceedings of the ACM Symposium on Document Engineering 2018",
        "abstract": "In this contemporaneous world, it is an obligation for any organization working with documents to end up with the insipid task of classifying truckload of documents, which is the nascent stage of venturing into the realm of information retrieval and data mining. But classification of such humongous documents into multiple classes, calls for a lot of time and labor. Hence a system which could classify these documents with acceptable accuracy would be of an unfathomable help in document engineering. We have created multiple classifiers for document classification and compared their accuracy on raw and processed data. We have garnered data used in a corporate organization as well as publicly available data for comparison. Data is processed by removing the stop-words and stemming is implemented to produce root words. Multiple traditional machine learning techniques like Naive Bayes, Logistic Regression, Support Vector Machine, Random forest Classifier and Multi-Layer Perceptron are used for classification of documents. Classifiers are applied on raw and processed data separately and their accuracy is noted. Along with this, Deep learning technique such as Convolution Neural Network is also used to classify the data and its accuracy is compared with that of traditional machine learning techniques. We are also exploring hierarchical classifiers for classification of classes and subclasses. The system classifies the data faster and with better accuracy than if done manually. The results are discussed in the results and evaluation section.",
        "link": "https://dl.acm.org/doi/10.1145/3209280.3209526"
    },
    {
        "title": "Identification of Overpricing in the Purchase of Medication by the Federal Government of Brazil, Using Text Mining and Clustering Based on Ontology",
        "authors": [
            "Marco Aurelio O. S. Correa",
            "Adriano Galindo Leal"
        ],
        "date": "August 2018",
        "source": "ICCBDC '18: Proceedings of the 2018 2nd International Conference on Cloud and Big Data Computing",
        "abstract": "Increasing the transparency level in his actions and spending is one of the primary duties of the Brazilian Federal Government. The creation of laws that oblige full disclose of all its expenditures through transparency portals enables citizens to supervise all government entities. However, only the dissemination of these data, without a definite standard or the availability of data analysis tools, does not guarantee that the citizen is empowered to play his role. Therefore hence, the objective of this work is to identify overprice in the acquisition of products purchased by the federal government of Brazil using the unstructured data available on the Transparency Portal. The last two-years' worth of purchasing data, available in the Transparency Portal, were extracted, processed and stored. Due to his diverse nature and high volume of data, this study focused only on medicines purchased by the Ministry of Health. Ontology-based text mining and clustering techniques were applied for automatic identification and classification of products. The processing of this information was done through text mining and clustering, based on the ontology registered in another database of the Brazilian government. Because of this work, a consolidated price base per medication was created to allow the identification of distortions in prices practised, facilitating the identification of cases that merit further investigation to unravel fraud to the treasury.",
        "link": "https://dl.acm.org/doi/10.1145/3264560.3264569"
    },
    {
        "title": "Computational Estimation by Scientific Data Mining with Classical Methods to Automate Learning Strategies of Scientists",
        "authors": [
            "Aparna S. Varde"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Experimental results are often plotted as 2-dimensional graphical plots (aka graphs) in scientific domains depicting dependent versus independent variables to aid visual analysis of processes. Repeatedly performing laboratory experiments consumes significant time and resources, motivating the need for computational estimation. The goals are to estimate the graph obtained in an experiment given its input conditions, and to estimate the conditions that would lead to a desired graph. Existing estimation approaches often do not meet accuracy and efficiency needs of targeted applications. We develop a computational estimation approach called AutoDomainMine that integrates clustering and classification over complex scientific data in a framework so as to automate classical learning methods of scientists. Knowledge discovered thereby from a database of existing experiments serves as the basis for estimation. Challenges include preserving domain semantics in clustering, finding matching strategies in classification, striking a good balance between elaboration and conciseness while displaying estimation results based on needs of targeted users, and deriving objective measures to capture subjective user interests. These and other challenges are addressed in this work. The AutoDomainMine approach is used to build a computational estimation system, rigorously evaluated with real data in Materials Science. Our evaluation confirms that AutoDomainMine provides desired accuracy and efficiency in computational estimation. It is extendable to other science and engineering domains as proved by adaptation of its sub-processes within fields such as Bioinformatics and Nanotechnology.",
        "link": "https://dl.acm.org/doi/10.1145/3502736"
    },
    {
        "title": "Where is the road for issue reports classification based on text mining?",
        "authors": [
            "Qiang Fan",
            "Yue Yu",
            "Gang Yin",
            "Tao Wang",
            "Huaimin Wang"
        ],
        "date": "November 2017",
        "source": "ESEM '17: Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Currently, open source projects receive various kinds of issues daily, because of the extreme openness of Issue Tracking System (ITS) in GitHub. ITS is a labor-intensive and time-consuming task of issue categorization for project managers. However, a contributor is only required a short textual abstract to report an issue in GitHub. Thus, most traditional classification approaches based on detailed and structured data (e.g., priority, severity, software version and so on) are difficult to adopt. In this paper, issue classification approaches on a large-scale dataset, including 80 popular projects and over 252,000 issue reports collected from GitHub, were investigated. First, four traditional text-based classification methods and their performances were discussed. Semantic perplexity (i.e., an issues description confuses bug-related sentences with nonbug-related sentences) is a crucial factor that affects the classification performances based on quantitative and qualitative study. Finally, A two-stage classifier framework based on the novel metrics of semantic perplexity of issue reports was designed. Results show that our two-stage classification can significantly improve issue classification performances.",
        "link": "https://dl.acm.org/doi/10.1109/ESEM.2017.19"
    },
    {
        "title": "Feature-based Facebook reviews process model for e-management using data mining",
        "authors": [
            "Anish Kumar Varudharajulu",
            "Yongsheng Ma"
        ],
        "date": "January 2019",
        "source": "IC4E '19: Proceedings of the 10th International Conference on E-Education, E-Business, E-Management and E-Learning",
        "abstract": "The data generated from online communication acts as potential gold mines for discovering knowledge for researchers. A large amount of data is also generated in the form of web documents, emails, blogs, and feedback, etc. Text analytics is being significantly employed to mine important information. Opinion mining is the process of extracting human thoughts and perceptions from unstructured texts. The showstopper for designing an opinion mining system for analyzing reviews arise from the fact that customer reviews are often noisy. These reviews are informally written. In addition, they are subjected to spelling mistakes, grammatical errors, improper punctuation and irrational capitalization. This paper focuses on analyzing the different classification and clustering algorithms aimed at extracting and consolidating opinions of customers from social media sites like Facebook, Twitter and through surveys, at multiple levels of granularity to monitor and measure customer satisfaction. Ours is an automated approach, in which the system aids in the process of knowledge assimilation for knowledge-based building and also performs the analytics. Domain experts ratify the knowledge base and also provide training datasets for the system to intuitively gather more instances for ratification. The system identifies opinion expressions as phrases containing opinion words, opinionated features and also opinion modifiers. These expressions are categorized as positive, negative or neutral. Opinion expressions are identified and categorized using localized linguistic techniques. Opinions can be congregated at any desired level of specificity i.e. feature level or product level, user level or service level, etc. We have developed a system based on this approach, which provides the user with a platform to analyze opinion expressions crawled from a set of pre-defined datasets.",
        "link": "https://dl.acm.org/doi/10.1145/3306500.3306514"
    },
    {
        "title": "Discriminative Topic Mining via Category-Name Guided Text Embedding",
        "authors": [
            "Yu Meng",
            "Jiaxin Huang",
            "Guangyuan Wang",
            "Zihan Wang",
            "Chao Zhang",
            "Yu Zhang",
            "Jiawei Han"
        ],
        "date": "April 2020",
        "source": "WWW '20: Proceedings of The Web Conference 2020",
        "abstract": "Mining a set of meaningful and distinctive topics automatically from massive text corpora has broad applications. Existing topic models, however, typically work in a purely unsupervised way, which often generate topics that do not fit users’ particular needs and yield suboptimal performance on downstream tasks. We propose a new task, discriminative topic mining, which leverages a set of user-provided category names to mine discriminative topics from text corpora. This new task not only helps a user understand clearly and distinctively the topics he/she is most interested in, but also benefits directly keyword-driven classification tasks. We develop CatE, a novel category-name guided text embedding method for discriminative topic mining, which effectively leverages minimal user guidance to learn a discriminative embedding space and discover category representative terms in an iterative manner. We conduct a comprehensive set of experiments to show that CatE mines high-quality set of topics guided by category names only, and benefits a variety of downstream applications including weakly-supervised classification and lexical entailment direction identification.",
        "link": "https://dl.acm.org/doi/10.1145/3366423.3380278"
    },
    {
        "title": "Comparative Study of Heart Disease Diagnosis Using Top Ten Data Mining Classification Algorithms",
        "authors": [
            "I. Ketut Agung Enriko"
        ],
        "date": "June 2019",
        "source": "ICFET '19: Proceedings of the 5th International Conference on Frontiers of Educational Technologies",
        "abstract": "Data mining has been used for many purposes, especially for prediction system. In healthcare, data mining algorithms often used in disease diagnosis. Meanwhile, heart disease is known as a primary cause of death over the years. Many studies have been performed in heart disease diagnosis using data mining methods. There are some popular data mining algorithms that can be used in heart disease diagnosis, for example, k-Nearest Neighbor, CART, and AdaBoost. The algorithms are used to analyze a sample of cardiovascular patients data and predict the heart disease type that they suffer. Some parameters are taken from the patient, including EKG morphology, blood pressure, and information about the existence of chest pain, shortness of breath, palpitation, and cold sweat. In this study, medical records data are collected from Harapan Kita Hospital and utilized as a dataset sample in this research. Top ten data mining classification algorithms are used in diagnosing heart disease from Harapan Kita Hospital data and examining their performance by checking the accuracy and speed.",
        "link": "https://dl.acm.org/doi/10.1145/3338188.3338220"
    },
    {
        "title": "Text Mining Approach for Identifying Research Trends",
        "authors": [
            "Snezhana Sulova"
        ],
        "date": "June 2021",
        "source": "CompSysTech '21: Proceedings of the 22nd International Conference on Computer Systems and Technologies",
        "abstract": "With the increase of unstructured data, the issues connected with automatic text processing, the categorization of documents and the discovery of topics have become objects of growing interest. In order to improve the process of grouping and processing research publications, we would like to propose a method based upon natural language processing. It is based on text mining technologies which aim to identify key tendencies in documents. It processes the content of publications by clustering and identifies the topics of each identified group. This analysis helps by identifying key tendencies as well as discovering emerging new areas of research. Publications from the research literature database, Scopus, were used to test the approach. The topic of the publications is “the application of digital technologies in the logistics business”. The experiments were completed using the RapidMiner Studio software.",
        "link": "https://dl.acm.org/doi/10.1145/3472410.3472433"
    },
    {
        "title": "Assessment of Vulnerability Severity using Text Mining",
        "authors": [
            "Georgios Spanos",
            "Lefteris Angelis",
            "Dimitrios Toloudis"
        ],
        "date": "September 2017",
        "source": "PCI '17: Proceedings of the 21st Pan-Hellenic Conference on Informatics",
        "abstract": "Software1 vulnerabilities are closely associated with information systems security, a major and critical field in today's technology. Vulnerabilities constitute a constant and increasing threat for various aspects of everyday life, especially for safety and economy, since the social impact from the problems that they cause is complicated and often unpredictable. Although there is an entire research branch in software engineering that deals with the identification and elimination of vulnerabilities, the growing complexity of software products and the variability of software production procedures are factors contributing to the ongoing occurrence of vulnerabilities, Hence, another area that is being developed in parallel focuses on the study and management of the vulnerabilities that have already been reported and registered in databases. The information contained in such databases includes, a textual description and a number of metrics related to vulnerabilities. The purpose of this paper is to investigate to what extend the assessment of the vulnerability severity can be inferred directly from the corresponding textual description, or in other words, to examine the informative power of the description with respect to the vulnerability severity. For this purpose, text mining techniques, i.e. text analysis and three different classification methods (decision trees, neural networks and support vector machines) were employed. The application of text mining to a sample of 70,678 vulnerabilities from a public data source shows that the description itself is a reliable and highly accurate source of information for vulnerability prioritization.",
        "link": "https://dl.acm.org/doi/10.1145/3139367.3139390"
    },
    {
        "title": "Clustering Algorithms for Spatial Data Mining",
        "authors": [
            "Chetashri Bhadane",
            "Ketan Shah"
        ],
        "date": "April 2020",
        "source": "ICGDA '20: Proceedings of the 2020 3rd International Conference on Geoinformatics and Data Analysis",
        "abstract": "With the advances in mobile and wireless technologies, there has been a rise in applications that track and share the users' geospatial data. People use several social networking sites such as Twitter, Facebook and Flickr, where they share their status updates. With the integration of Global Positioning System (GPS) with mobile phones, it is now possible to share one's locations on these social networks. GPS allows us to record and track a person's movement along with the timestamp. The data set obtained from these GPS logs is vast and is widely used to analyze the users' movement patterns. Specifically, we can find out significant locations based on the number of users present at that location and the time spent by them at such places. Once significant places have been identified, it is also possible to identify the semantic importance of these locations. This paper presents an overview of the clustering techniques used to find important places of interest using large GPS based mobility datasets. Four clustering algorithms, K-Means, DBSCAN, OPTICS and Hierarchical, are implemented, and performance is tested using real-time data of 50 users collected over 2--5 years. Performance summary depicts that K-Means and DBSCAN perform well for spatial data.",
        "link": "https://dl.acm.org/doi/10.1145/3397056.3397068"
    },
    {
        "title": "Feature Selection based Arabic Text Classification using Different Machine Learning Algorithms: Comparative Study",
        "authors": [
            "Sakina Rim Bennabi",
            "Zakaria Elberrichi"
        ],
        "date": "June 2020",
        "source": "ICIST '20: Proceedings of the 10th International Conference on Information Systems and Technologies",
        "abstract": "Feature selection is a method of data pre-processing widely used when mining large data, such as textual classification. Several studies have been conducted to compare the different methods of feature selection applied to corpora in English. Unfortunately, a small number of works concern the Arabic language. This article aims to present a comparative study of different feature selection techniques including: Chi2, the ANOVA method and mutual information, applied on a corpus in Arabic language, while also diversifying the machine learning algorithms (Naive Bayes, SVM and KNN). This experimental study has shown in general that reducing dimensionality with feature selection techniques has slightly affected the performance of textual classification, reducing the size of the corpus by up to 1%.",
        "link": "https://dl.acm.org/doi/10.1145/3447568.3448531"
    },
    {
        "title": "Intelligent Data Analysis using Optimized Support Vector Machine Based Data Mining Approach for Tourism Industry",
        "authors": [
            "Ms Promila Sharma",
            "Uma Meena",
            "Girish Kumar Sharma"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Data analysis involves the deployment of sophisticated approaches from data mining methods, information theory, and artificial intelligence in various fields like tourism, hospitality, and so on for the extraction of knowledge from the gathered and preprocessed data. In tourism, pattern analysis or data analysis using classification is significant for finding the patterns that represent new and potentially useful information or knowledge about the destination and other data. Several data mining techniques are introduced for the classification of data or patterns. However, overfitting, less accuracy, local minima, sensitive to noise are the drawbacks in some existing data mining classification methods. To overcome these challenges, Support vector machine with Red deer optimization (SVM-RDO) based data mining strategy is proposed in this article. Extended Kalman filter (EKF) is utilized in the first phase, i.e., data cleaning to remove the noise and missing values from the input data. Mantaray foraging algorithm (MaFA) is used in the data selection phase, in which the significant data are selected for the further process to reduce the computational complexity. The final phase is the classification, in which SVM-RDO is proposed to access the useful pattern from the selected data. PYTHON is the implementation tool used for the experiment of the proposed model. The experimental analysis is done to show the efficacy of the proposed work. From the experimental results, the proposed SVM-RDO achieved better accuracy, precision, recall, and F1 score than the existing methods for the tourism dataset. Thus, it is showed the effectiveness of the proposed SVM-RDO for pattern analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3494566"
    },
    {
        "title": "Applying text mining to predict learners' cognitive engagement",
        "authors": [
            "Hayati Hind",
            "Mohammed Khalidi Idrissi",
            "Samir Bennani"
        ],
        "date": "October 2017",
        "source": "SCAMS '17: Proceedings of the Mediterranean Symposium on Smart City Application",
        "abstract": "In academic process, engagement represent one of the important success' factor. This paper proposes a predictive system for learners' cognitive engagement based on their online discussion forums participation. In the first level the ontology OWL and the LSA method are used to perform a semantic classification system of the threads according to a specific context chosen by the tutor. Then the Text mining, as a predictive method, is applied to the classified threads and learners' participation in the forums.",
        "link": "https://dl.acm.org/doi/10.1145/3175628.3175655"
    },
    {
        "title": "Investigate Transitions into Drug Addiction through Text Mining of Reddit Data",
        "authors": [
            "John Lu",
            "Sumati Sridhar",
            "Ritika Pandey",
            "Mohammad Al Hasan",
            "Georege Mohler"
        ],
        "date": "July 2019",
        "source": "KDD '19: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Increasing rates of opioid drug abuse and heightened prevalence of online support communities underscore the necessity of employing data mining techniques to better understand drug addiction using these rapidly developing online resources. In this work, we obtained data from Reddit, an online collection of forums, to gather insight into drug use/misuse using text snippets from users narratives. Specifically, using users' posts, we trained a binary classifier which predicts a user's transitions from casual drug discussion forums to drug recovery forums. We also proposed a Cox regression model that outputs likelihoods of such transitions. In doing so, we found that utterances of select drugs and certain linguistic features contained in one's posts can help predict these transitions. Using unfiltered drug-related posts, our research delineates drugs that are associated with higher rates of transitions from recreational drug discussion to support/recovery discussion, offers insight into modern drug culture, and provides tools with potential applications in combating the opioid crisis.",
        "link": "https://dl.acm.org/doi/10.1145/3292500.3330737"
    },
    {
        "title": "Analysing digital banking reviews using text mining",
        "authors": [
            "Li Chen Cheng",
            "Legaspi Rhea Sharmayne"
        ],
        "date": "December 2020",
        "source": "ASONAM '20: Proceedings of the 12th IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Digital banks are new entrants in the banking industry in the Philippines as they only started late 2018. Since then, a handful of players have and are still emerging. With more and more people becoming technologically savvy, it is very critical for financial institutions to develop a digital banking application that will stand out from the competition. This paper aims to use text mining methods to analyse digital banking application reviews. This study will perform topic modelling using LDA to explore customer concerns and will mine association rules between the digital banking features with the review score. The results will reveal which areas the digital banking application can further optimize for customer satisfaction and retention.",
        "link": "https://dl.acm.org/doi/10.1109/ASONAM49781.2020.9381429"
    },
    {
        "title": "Mining Moocs Videos Metadata Using Classification Techniques",
        "authors": [
            "El Harrak Othman",
            "Ghadi Abderrahim",
            "El Bouhdidi Jaber"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "In few years, the internet has become the first source of information for most people, Today MOOCs makes it possible for everyone to access to the education over the world, it's represents an emerging methodology of online teaching and an important development in open education. Due to the rapid development of Moocs and the rapid growth of digital data and video database over the Internet, it is becoming very difficult for learners to browsing and choosing the best training for them. The scientific community has increased the amount of research into new technologies, with a view to improve the digital video utilization: its archiving, indexing, accessibility, acquisition, store and even its process and usability. All these parts of the video utilization entail the necessity of the extraction of all important information of a video, especially in cases of lack of metadata information. Web video mining is retrieving the content using data mining techniques from World Wide Web. There are two approaches for web video mining using traditional image processing (signal processing) and metadata based approach. In this paper, we present the various research makes in this subject and we propose an effective methodology to extract the metadata from Moocs videos and classify them based on the extracted metadata by applying data mining techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090450"
    },
    {
        "title": "Text mining with HathiTrust",
        "authors": [
            "Eleanor Dickson Koehl",
            "Ryan Dubnicek"
        ],
        "date": "June 2019",
        "source": "JCDL '19: Proceedings of the 18th Joint Conference on Digital Libraries",
        "abstract": "This tutorial will introduce attendees to the HathiTrust Research Center and its tools and services for computational text analysis research. HTRC leverages the scope and scale of the HathiTrust Digital Library collection to create opportunities for researchers to perform text data mining on subsets of the corpus. Attendees will develop skills that will allow them to conduct basic text analysis research using HathiTrust data, learn techniques for engaging with scholars on their text data mining research, and explore strategies for computational access to large-scale digital collections.",
        "link": "https://dl.acm.org/doi/10.1109/JCDL.2019.00115"
    },
    {
        "title": "Text mining for plagiarism detection: multivariate pattern detection for recognition of text similarities",
        "authors": [
            "Konstantinos Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2018",
        "source": "ASONAM '18: Proceedings of the 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "The problem of plagiarism the recent years has been intensified by the availability of information in digital form and the accessibility of the electronic libraries through the Internet. As a result, plagiarism detection has been transformed into a big data analytics problem since the number of digital sources is extravagant and a new document needs to be compared with millions of other existing documents. In this paper, a text mining methodology is proposed that can detect all common patterns between a document and the documents in a reference database. The technique is based on a pattern detection algorithm and the corresponding data structure that enables the algorithm to detect all common patterns. The methodology has been applied in a well-defined dataset providing very promising results identifying difficult cases of plagiarism such as technical disguise.",
        "link": "https://dl.acm.org/doi/10.5555/3382225.3382424"
    },
    {
        "title": "Petroleum Engineering Data Text Classification Using Convolutional Neural Network Based Classifier",
        "authors": [
            "Tzuhan Hsu",
            "Yaoqin Zhang"
        ],
        "date": "May 2018",
        "source": "ICMLT '18: Proceedings of the 2018 International Conference on Machine Learning Technologies",
        "abstract": "In this paper, authors introduce a convolutional neural network (CNN) based classification approach for petroleum engineering event. Recently, text classification problem has raised substantial attention in the field of machine learning. The aim of text classification process is to assign correct label to a text on the basis of its content. However, due to the imprecise of feature representation, most of former text classification methods cannot export satisfying result. To solve this problem, authors propose a model based on convolutional neural network. Authors conduct experiments with real world petroleum engineering data text, artificially labelled into 6 categories. Beside purposed CNN-based model, the authors also implement other baseline machine learning approaches: k nearest neighbor, random forest, support vector machines, recurrent neural network, gated recurrent units, long short-term memory. Among all these models, our CNN-based model reach best performance in text classification task. In the last part of this paper, authors compare and analysis their purpose and baseline model.",
        "link": "https://dl.acm.org/doi/10.1145/3231884.3231898"
    },
    {
        "title": "Classifying Indonesian Online Articles as Advertisement Placement Base Using Text Mining",
        "authors": [
            "Nadhira Tasya",
            "Arian Dhini"
        ],
        "date": "July 2017",
        "source": "ICBIM 2017: Proceedings of the International Conference on Business and Information Management",
        "abstract": "Rapid development in technological aspect resulting in growing level of human needs for the latest news, so that emerged a new trend of publishing and accessing news through online media or usually called online journalism. In addition, the number of people who sell and purchase through online sites also continues to increase and this opportunity is utilized by the company and the advertiser by implementing targeted web advertising. However, the high number of articles that have been published and accessed leads to great opportunities for errors in determining where to place the ads. Therefore, it needs a system that can categorize articles accessed by users as the basis of advertisement placement by the company and this classification system can be done by applying the method of Data Mining and Text Mining. This research uses document data in the form of article content that will be categorized into twenty categories of class of advertisement by using Text Mining technique with Support Vector Machine algorithm. The results of this study may be used by companies or advertisers as a basis for placement of ads on selected online media sites.",
        "link": "https://dl.acm.org/doi/10.1145/3134271.3134288"
    },
    {
        "title": "Mining Free-Text Medical Notes for Suicide Risk Assessment",
        "authors": [
            "Marios Adamou",
            "Grigoris Antoniou",
            "Elissavet Greasidou",
            "Vincenzo Lagani",
            "Paulos Charonyktakis",
            "Ioannis Tsamardinos"
        ],
        "date": "July 2018",
        "source": "SETN '18: Proceedings of the 10th Hellenic Conference on Artificial Intelligence",
        "abstract": "Suicide has been considered as an important public health issue for a very long time, and is one of the main causes of death worldwide. Despite suicide prevention strategies being applied, the rate of suicide has not changed substantially over the past decades. Advances in machine learning make it possible to attempt to predict suicide based on the analysis of relevant data to inform clinical practice. This paper reports on findings from the analysis of data of patients who died by suicide in the period 2013-2016 and made use of both structured data and free-text medical notes. We focus on examining various text-mining approaches to support risk assessment. The results show that using advance machine learning and text-mining techniques, it is possible to predict within a specified period which people are most at risk of taking their own life at the time of referral to a mental health service.",
        "link": "https://dl.acm.org/doi/10.1145/3200947.3201020"
    },
    {
        "title": "Integration of data mining techniques in e-learning systems: Clustering Profil of Lerners and Recommender Course System",
        "authors": [
            "A. El Moustamid",
            "E. En-Naimi",
            "J. El Bouhdidi"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "Data mining aims at the extraction of knowledge from large quantities of data by automatic or semi-automatic methods. In the field of education and training e-learning, Data Mining techniques as clustering (The aim of this technique is to identify groups that are similar in some aspect) [21], prediction (is used for developing model where combination of data viewed from different aspects can effect single viewed data. Here, is important to watch how predictors variables are effecting predicted variable)[21], classification are applied in e-learning to improve the level of learners by providing learning according to the learners' requirement) are used to analyze learners profile, to predict and improve student performance. The aim of our project is to develop a system capable of analyzing learners profile and indexing web videos in order to offer learners a rich database with courses that correspond to their levels.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090453"
    },
    {
        "title": "Multi-level mining and visualization of scientific text collections: Exploring a bi-lingual scientific repository",
        "authors": [
            "Pablo Accuosto",
            "Francesco Ronzano",
            "Daniel Ferrés",
            "Horacio Saggion"
        ],
        "date": "December 2017",
        "source": "WOSP 2017: Proceedings of the 6th International Workshop on Mining Scientific Publications",
        "abstract": "We present a system to mine and visualize collections of scientific documents by semantically browsing information extracted from single publications or aggregated throughout corpora of articles. The text mining tool performs deep analysis of document collections allowing the extraction and interpretation of research paper's contents. In addition to the extraction and enrichment of documents with metadata (titles, authors, affiliations, etc), the deep analysis performed comprises semantic interpretation, rhetorical analysis of sentences, triple-based information extraction, and text summarization. The visualization components allow geographical-based exploration of collections, topic-evolution interpretation, and collaborative network analysis among others. The paper presents a case study of a bi-lingual collection in the field of Natural Language Processing (NLP).",
        "link": "https://dl.acm.org/doi/10.1145/3127526.3127529"
    },
    {
        "title": "Hierarchical Topic Mining via Joint Spherical Tree and Text Embedding",
        "authors": [
            "Yu Meng",
            "Yunyi Zhang",
            "Jiaxin Huang",
            "Yu Zhang",
            "Chao Zhang",
            "Jiawei Han"
        ],
        "date": "August 2020",
        "source": "KDD '20: Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Mining a set of meaningful topics organized into a hierarchy is intuitively appealing since topic correlations are ubiquitous in massive text corpora. To account for potential hierarchical topic structures, hierarchical topic models generalize flat topic models by incorporating latent topic hierarchies into their generative modeling process. However, due to their purely unsupervised nature, the learned topic hierarchy often deviates from users' particular needs or interests. To guide the hierarchical topic discovery process with minimal user supervision, we propose a new task, Hierarchical Topic Mining, which takes a category tree described by category names only, and aims to mine a set of representative terms for each category from a text corpus to help a user comprehend his/her interested topics. We develop a novel joint tree and text embedding method along with a principled optimization procedure that allows simultaneous modeling of the category tree structure and the corpus generative process in the spherical space for effective category-representative term discovery. Our comprehensive experiments show that our model, named JoSH, mines a high-quality set of hierarchical topics with high efficiency and benefits weakly-supervised hierarchical text classification tasks.",
        "link": "https://dl.acm.org/doi/10.1145/3394486.3403242"
    },
    {
        "title": "Unsupervised behavioural mining and clustering for malware family identification",
        "authors": [
            "Khanh Huu The Dam",
            "Thomas Given-Wilson",
            "Axel Legay"
        ],
        "date": "March 2021",
        "source": "SAC '21: Proceedings of the 36th Annual ACM Symposium on Applied Computing",
        "abstract": "More accurate and advanced detection and classification of malware requires exploiting program behaviour and not merely syntactic or static features. One approach is to use system call dependency graphs (SCDGs) that represent the program behaviour by interactions with the system, and the relations between these interactions. These SCDGs have been used with supervised learning techniques to very accurately detect and classify malware. This works considers the unsupervised learning challenge of mining for common clusters of behaviour without a priori knowledge. This allows for clustering of similar programs by behaviour, that can then be used for either classification or further analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3412841.3441919"
    },
    {
        "title": "Text Mining for Evaluating Authors' Birth and Death Years",
        "authors": [
            "Dror Moghaz",
            "Yaakov Hacohen-Kerner",
            "Dov Gabbay"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "This article presents a unique method in text and data mining for finding the era, i.e., mining temporal data, in which an anonymous author was living. Finding this era can assist in the examination of a fake document or extracting the time period in which a writer lived. The study and the experiments concern Hebrew, and in some parts, Aramaic and Yiddish rabbinic texts. The rabbinic texts are undated and contain no bibliographic sections, posing an interesting challenge. This work proposes algorithms using key phrases and key words that allow the temporal organization of citations together with linguistic patterns. Based on these key phrases, key words, and the references, we established several types of “Iron-clad,” Heuristic and Greedy rules for estimating the years of birth and death of a writer in an interesting classification task. Experiments were conducted on corpora, including documents authored by 12, 24, and 36 rabbinic writers and demonstrated promising results.",
        "link": "https://dl.acm.org/doi/10.1145/3281631"
    },
    {
        "title": "Transfer Learning to Timed Text Based Video Classification Using CNN",
        "authors": [
            "Zenun Kastrati",
            "Ali Shariq Imran",
            "Arianit Kurti"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "Open educational video resources are gaining popularity with a growing number of massive open online courses (MOOCs). This has created a niche for content providers to adopt effective solutions in automatically organizing and structuring of educational resources for maximum visibility. Recent advances in deep learning techniques are proving useful in managing and classifying resources into appropriate categories. This paper proposes one such convolutional neural network (CNN) model for classifying video lectures in a MOOC setting using a transfer learning approach. The model uses a time-aligned text transcripts corresponding to video lectures from six broader subject categories. Video lectures and their corresponding transcript dataset is gathered from the Coursera MOOC platform. Two different CNN models are proposed: i) CNN based classification using embeddings learned from our MOOC dataset, ii) CNN based classification using transfer learning. Word embeddings generated from two well known state-of-the-art pre-trained models Word2Vec and GloVe, are used in the transfer learning approach for the second case. The proposed CNN models are evaluated using precision, recall, and F1 score and the obtained performance is compared with both conventional and deep learning classifiers. The proposed CNN models have an F1 score improvement of 10-22 percentage points over DNN and conventional classifiers",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326483"
    },
    {
        "title": "Evolutionary role mining in complex networks by ensemble clustering",
        "authors": [
            "Sarvenaz Choobdar",
            "Pedro Ribeiro",
            "Fernando Silva"
        ],
        "date": "April 2017",
        "source": "SAC '17: Proceedings of the Symposium on Applied Computing",
        "abstract": "The structural patterns in the neighborhood of nodes assign unique roles to the nodes. Mining the set of existing roles in a network provides a descriptive profile of the network and draws its general picture. This paper proposes a new method to determine structural roles in a dynamic network based on the current position of nodes and their historic behavior. We develop a temporal ensemble clustering technique to dynamically find groups of nodes, holding similar tempo-structural roles. We compare two weighting functions, based on age and distribution of data, to incorporate temporal behavior of nodes in the role discovery. To evaluate the performance of the proposed method, we assess the results from two points of view: 1) goodness of fit to current structure of the network; 2) consistency with historic data. We conduct the evaluation using different ensemble clustering techniques. The results on real world networks demonstrate that our method can detect tempo-structural roles that simultaneously depict the topology of a network and reflect its dynamics with high accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3019612.3019815"
    },
    {
        "title": "Analyzing used-car web listings via text mining",
        "authors": [
            "Ayhan Demiriz",
            "Fatma Cantaş"
        ],
        "date": "October 2017",
        "source": "IML '17: Proceedings of the 1st International Conference on Internet of Things and Machine Learning",
        "abstract": "Used car trade is one of the major components of the world economies. It is not uncommon to sell a car by placing an internet advertisement irrespective of the geography in these days. A typical content of an advertisement is usually composed of two parts namely the structured and the free text data. The structured data may include some information about the asking price, make, model, year, mileage of the car and the contact info. In most cases, seller may give important clues about the car's current conditions in the free text data where the title (head) of the advertisement can be included as free text too. This paper reports preliminary results from a text mining study conducted on 75K used car internet listings collected from two major car listing web sites in Turkey. As expected, the words and the phrases related to the description of the car are observed to be frequent. The leading concepts in the free text are found to be regarding how to describe the current condition of a car, for example \"no crash history\".",
        "link": "https://dl.acm.org/doi/10.1145/3109761.3109782"
    },
    {
        "title": "Text classification of Diseases Treated by Traditional Chinese Medicine Prescription based on machine learning",
        "authors": [
            "Hanqing Zhao",
            "Bo Han",
            "Chen Li"
        ],
        "date": "September 2020",
        "source": "ISAIMS '20: Proceedings of the 1st International Symposium on Artificial Intelligence in Medical Sciences",
        "abstract": "OBJECTIVE: To explore the application of machine learning in th e identification of diseases treated by traditional Chinese medicine prescriptions. METHODS: Based on the composition of the text document of Chinese medicine prescriptions, the prescriptions were divided into cough, headache and diarrhea. THUTCT were introduced to establish and train two machine learning text classification models, LibSVM and LibLinear, and the prescriptions to be deter mined were put into the model for classification and prediction. R ESULTS: The Precision rate of LibSVM model and Liblinear mod el were 0.7283 and 0.6690. Seven prescriptions were classified an d predicted, and the results were in line with expectations. CONCLUSION: THUCTC has good universality for the content of TCM prescriptions, high classification accuracy and fast testing speed, which is suitable for the text classification and discrimination research of TCM prescriptions for diseases.",
        "link": "https://dl.acm.org/doi/10.1145/3429889.3429895"
    },
    {
        "title": "Evaluation of retweet clustering method classification method using retweets on Twitter without text data",
        "authors": [
            "K. Uchida",
            "F. Toriumi",
            "T. Sakaki"
        ],
        "date": "August 2017",
        "source": "WI '17: Proceedings of the International Conference on Web Intelligence",
        "abstract": "Burst phenomena, which frequently occur on social media, are caused by such social events as flaming on the internet, elections, and natural disasters. To understand people's thoughts and feelings, we must classify their opinions from burst phenomena. Therefore, classification methods that categorize tweets are critical. However, since most classification methods focus on text mining, they cannot group tweets by topics because each tweet has poor linguistic similarities. We used a non-text-based classification method proposed by Baba et al. that groups tweets by topics, even if they have poor linguistic similarities, and verified its validity by comparing it with a text-based classification method in two different evaluations: qualitative and quantitative. In the qualitative evaluation part, we did a questionnaire survey and validated the suitability of the topic clusters created using both the non-and text-based methods. Since evaluating the similarity of every pair of tweets in each topic is difficult, we evaluated the similarity between sampled pairs in the survey and acquired more appropriate topic clustering results using the non-text-based method than the text-based method. In the quantitative evaluation part, we focused on the robustness of each method against data reduction. Many approaches analyze social media data, especially because collecting data from social media is comparatively easy. However, since collecting the whole data of burst phenomena is very costly due to the vast amounts of available social media data, robustness against data reduction is an important index to evaluate classification methods. With the non-text-based method, over 55% of the pairs of tweets in the same cluster were also included in the same cluster even when the data were reduced to 10% in all three of our example cases. In this paper, as a source we focus on Twitter, one of the most popular microblogging services. Using clustering to conduct detailed case analyses, we scrutinized three burst cases that include natural disasters and flaming on the internet and found that a non-text-based method more effectively classified tweets in burst phenomena than a text-based method.",
        "link": "https://dl.acm.org/doi/10.1145/3106426.3106451"
    },
    {
        "title": "Using Spark for Text Mining on Large Scale Liver Cancer Literature",
        "authors": [
            "Ming-Yen Lin",
            "Yu-Ju Lin",
            "Sue-Chen Hsueh"
        ],
        "date": "January 2021",
        "source": "BDET 2021: 2021 the 3rd International Conference on Big Data Engineering and Technology (BDET)",
        "abstract": "Cancer is one of the main causes of death. The number of known cancer to-date is more than one hundred. Liver cancer, ranked after trachea and lung cancer, has long been high in the cancer leading cause in Taiwan and even ranked as second in 2016. The number of scientific articles related to cancer proliferates every year. The number reaches as high as 20 million in PubMed so that discovering useful information from the massive collection is very difficult. In addition, using a single machine to sift through these articles is very time-consuming. Therefore, we present a big data analytic framework using the distributed Apache Spark platform for text mining in PubMed literature. We establish a prediction model for liver cancer articles so that researchers may effectively validate whether an article is related to liver cancer or not. Classification models in Spark MLlib including Linear Support Vector Machines (SVM), Logistic Regression, are used in our experiments. Relevancy to liver cancer is further confirmed by using MeSH (Medical Subject Headings) terms. Logistic regression is about 3 times faster than SVMs and the accuracy of both methods is close to 95% in the experiments using hold-out validation. When max_features is 500 and min_df ≤ 0.1 (or min_df = 1), the accuracy may reach 96%. In the experiments with K-fold cross-validation, the accuracy of SVMs methods is 96%. The experimental results show that that our prediction model may effectively classify liver cancer articles.",
        "link": "https://dl.acm.org/doi/10.1145/3474944.3474958"
    },
    {
        "title": "Automatic topic classification of test cases using text mining at an Android smartphone vendor",
        "authors": [
            "Junji Shimagaki",
            "Yasutaka Kamei",
            "Naoyasu Ubayashi",
            "Abram Hindle"
        ],
        "date": "October 2018",
        "source": "ESEM '18: Proceedings of the 12th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Background: An Android smartphone is an ecosystem of applications, drivers, operating system components, and assets. The volume of the software is large and the number of test cases needed to cover the functionality of an Android system is substantial. Enormous effort has been already taken to properly quantify \"what features and apps were tested and verified?\". This insight is provided by dashboards that summarize test coverage and results per feature. One method to achieve this is to manually tag or label test cases with the topic or function they cover, much like function points. At the studied Android smartphone vendor, tests are labelled with manually defined tags, so-called \"feature labels (FLs)\", and the FLs serve to categorize 100s to 1000s test cases into 10 to 50 groups. Aim: Unfortunately for developers, manual assignment of FLs to 1000s of test cases is a time consuming task, leading to inaccurately labeled test cases, which will render the dashboard useless. We created an automated system that suggests tags/labels to the developers for their test cases rather than manual labeling. Method: We use machine learning models to predict and label the functionality tested by 10,000 test cases developed at the company. Results: Through the quantitative experiments, our models achieved acceptable F-1 performance of 0.3 to 0.88. Also through the qualitative studies with expert teams, we showed that the hierarchy and path of tests was a good predictor of a feature's label. Conclusions: We find that this method can reduce tedious manual effort that software developers spent classifying test cases, while providing more accurate classification results.",
        "link": "https://dl.acm.org/doi/10.1145/3239235.3268927"
    },
    {
        "title": "To apply Data Mining for Classification of Crowd sourced Software Requirements",
        "authors": [
            "Soonh Taj",
            "Qasim Arain",
            "Imran Memon",
            "Asma Zubedi"
        ],
        "date": "April 2019",
        "source": "ICSIE '19: Proceedings of the 8th International Conference on Software and Information Engineering",
        "abstract": "Now a day's main focus of developers is to build quality software that works according to customer needs and for this reason it is necessary to gather right requirements as requirement elicitation is the critical step that impacts on the success of software project as misinterpreted requirements leads to the failure of software project. By keeping this in mind a research is carried out on improving requirements elicitation process and automating the process of classifying requirements. In this research, a model is proposed which will help in this scenario for requirements elicitation and requirement classification. This paper presents a model in which crowd sourcing approach is used so that customers, end users, stakeholders, developers and software engineers can make active participation for requirement elicitation process and requirements gathered using crowdsourcing approach are used by model for classification process i.e. classification of requirements into functional and non-functional requirements. For the proof of proposed model a case study is conducted. Results of case study provided the usefulness and efficiency of proposed model for classification of crowd sourced software requirements.",
        "link": "https://dl.acm.org/doi/10.1145/3328833.3328837"
    },
    {
        "title": "Text classification using Fuzzy TF-IDF and Machine Learning Models",
        "authors": [
            "Mariem Bounabi",
            "Karim El Moutaouakil",
            "Khalid Satori"
        ],
        "date": "October 2019",
        "source": "BDIoT'19: Proceedings of the 4th International Conference on Big Data and Internet of Things",
        "abstract": "The representation of the information has an important impact on the text classification task. Several weighting methods were proposed in the literature, and the term frequency-inverse term frequency (TFIDF), the most know on the text treatment field. The FTF-IDF is a vector representation where the components of the TFIDF are presented as inputs to the Fuzzy Inference System (FIS). In this work, we compare several Machin Learning algorithms such as Naïve Bayes and its derivatives, SVM and Random forest classifiers, using the FTF-IDF representation. To improve the quality of the used classifiers, we call sum attribute selection methods. The recognition rate, for the tested systems, is satisfied, where the system based on naïve Bayes classifier, the FTF-IDF weighting terms, and the info gain select attributes method gives 98.7% as accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3372938.3372956"
    },
    {
        "title": "Text Mining in Cybersecurity: A Systematic Literature Review",
        "authors": [
            "Luciano Ignaczak",
            "Guilherme Goldschmidt",
            "Cristiano André Da Costa",
            "Rodrigo Da Rosa Righi"
        ],
        "date": null,
        "source": "ACM Computing Surveys",
        "abstract": "The growth of data volume has changed cybersecurity activities, demanding a higher level of automation. In this new cybersecurity landscape, text mining emerged as an alternative to improve the efficiency of the activities involving unstructured data. This article proposes a Systematic Literature Review (SLR) to present the application of text mining in the cybersecurity domain. Using a systematic protocol, we identified 2,196 studies, out of which 83 were summarized. As a contribution, we propose a taxonomy to demonstrate the different activities in the cybersecurity domain supported by text mining. We also detail the strategies evaluated in the application of text mining tasks and the use of neural networks to support activities involving unstructured data. The work also discusses text classification performance aiming its application in real-world solutions. The SLR also highlights open gaps for future research, such as the analysis of non-English content and the intensification in the usage of neural networks.",
        "link": "https://dl.acm.org/doi/10.1145/3462477"
    },
    {
        "title": "Using Text Mining to Analyze the Financial Patents",
        "authors": [
            "Yung-Feng Lu",
            "Jia-Lang Xu",
            "Mu-Yen Chen"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "With the advancement and development of science and technology, Nowadays society is the development of information technology, cloud computing, mobile Internet, Internet of Things, block chain and information technology.The combination of finance and technology has made financial technology terms gradually emerging.. This research analyzes the patents of financial technology. However, Taiwan's financial technology patents are mostly G06Q-20 (payment plan, architecture or agreement), G06Q-30 (commercial, such as marketing, shopping, payment, auction or e-commerce). G06Q-40 (financial, investment or tax treatment, insurance).The research use text mining technology to analyze and understand Taiwan's trends and developments in financial technology. The research results show that G06Q-20 patents are biased towards information security and mobile payment. G06Q-30 patents favor shopping websites and advertising. G06Q-40 patents are biased towards diversity, such as securities, insurance, finance, and stocks.",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326478"
    },
    {
        "title": "Identifying Online Profiles of Distance Learning Students Using Data Mining Techniques",
        "authors": [
            "Osama Islam",
            "Muazzam Siddiqui",
            "Naif Radi Aljohani"
        ],
        "date": "October 2019",
        "source": "ICDTE '19: Proceedings of the 3rd International Conference on Digital Technology in Education",
        "abstract": "Educational data has grown over the years with the increased use of technology within educational environments. This has led to a huge amount of data being stored in various data sources representing the student, his/her activities, and other aspects relevant to the learning process. To meet this analytical need, Educational Data Mining (EDM) has emerged to assist educational institutions in identifying key benefits such as students at risk, the level of student engagement or predicting student performance. The aim of this research was to explore the various aspects of student interaction data using data mining techniques to identify relevant patterns of behaviors and possible key attributes that have higher degrees of influence on distance learning students. The main findings identified several patterns of user online profiles based on a set of adopted learning strategies, the research proposed a framework for analyzing such interaction data based on R and Hadoop platforms to correlate online profiles with student performance.",
        "link": "https://dl.acm.org/doi/10.1145/3369199.3369249"
    },
    {
        "title": "Extremely Fast Decision Tree Mining for Evolving Data Streams",
        "authors": [
            "Albert Bifet",
            "Jiajin Zhang",
            "Wei Fan",
            "Cheng He",
            "Jianfeng Zhang",
            "Jianfeng Qian",
            "Geoff Holmes",
            "Bernhard Pfahringer"
        ],
        "date": "August 2017",
        "source": "KDD '17: Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
        "abstract": "Nowadays real-time industrial applications are generating a huge amount of data continuously every day. To process these large data streams, we need fast and efficient methodologies and systems. A useful feature desired for data scientists and analysts is to have easy to visualize and understand machine learning models. Decision trees are preferred in many real-time applications for this reason, and also, because combined in an ensemble, they are one of the most powerful methods in machine learning. In this paper, we present a new system called STREAMDM-C++, that implements decision trees for data streams in C++, and that has been used extensively at Huawei. Streaming decision trees adapt to changes on streams, a huge advantage since standard decision trees are built using a snapshot of data, and can not evolve over time. STREAMDM-C++ is easy to extend, and contains more powerful ensemble methods, and a more efficient and easy to use adaptive decision trees. We compare our new implementation with VFML, the current state of the art implementation in C, and show how our new system outperforms VFML in speed using less resources.",
        "link": "https://dl.acm.org/doi/10.1145/3097983.3098139"
    },
    {
        "title": "Data miners' little helper: data transformation activity cues for cluster analysis on document collections",
        "authors": [
            "Tania Cerquitelli",
            "Evelina Di Corso",
            "Francesco Ventura",
            "Silvia Chiusano"
        ],
        "date": "June 2017",
        "source": "WIMS '17: Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "In this paper we propose a new self-learning engine to streamline the analytics process, as it enables analysts to mine massive data repositories with minimal user intervention. In the context of cluster analysis on a collection of documents this new system, named SELF-DATA (SELF-learning DAta TrAnsformation), suggests to the analyst how to configure the whole mining process for a given dataset. SELF-DATA relies on an engine exploring different data weighting schemas (e.g., normalized term frequencies) and data transformation methods (e.g., PCA) before applying the cluster analysis, evaluating and comparing solutions through different quality indices (e.g., weighted Silhouette), and presenting the k-top solutions to the analyst. SELF-DATA will also include a knowledge base storing results of experiments on previously processed datasets, and a classification algorithm trained on the knowledge base content to forecast the best configuration for the whole mining process for an unexplored dataset. The first development of SELF-DATA running on Apache Spark has been validated on 5 collections of documents. Experimental results highlight that TF-IDF and logarithmic entropy are effective to measure item relevance with sparse datasets, and the LSI method outperforms PCA with a large dictionary.",
        "link": "https://dl.acm.org/doi/10.1145/3102254.3102288"
    },
    {
        "title": "Interactive Visual Graph Mining and Learning",
        "authors": [
            "Ryan A. Rossi",
            "Nesreen K. Ahmed",
            "Rong Zhou",
            "Hoda Eldardiry"
        ],
        "date": null,
        "source": "ACM Transactions on Intelligent Systems and Technology",
        "abstract": "This article presents a platform for interactive graph mining and relational machine learning called GraphVis. The platform combines interactive visual representations with state-of-the-art graph mining and relational machine learning techniques to aid in revealing important insights quickly as well as learning an appropriate and highly predictive model for a particular task (e.g., classification, link prediction, discovering the roles of nodes, and finding influential nodes). Visual representations and interaction techniques and tools are developed for simple, fast, and intuitive real-time interactive exploration, mining, and modeling of graph data. In particular, we propose techniques for interactive relational learning (e.g., node/link classification), interactive link prediction and weighting, role discovery and community detection, higher-order network analysis (via graphlets, network motifs), among others. GraphVis also allows for the refinement and tuning of graph mining and relational learning methods for specific application domains and constraints via an end-to-end interactive visual analytic pipeline that learns, infers, and provides rapid interactive visualization with immediate feedback at each change/prediction in real-time. Other key aspects include interactive filtering, querying, ranking, manipulating, exporting, as well as tools for dynamic network analysis and visualization, interactive graph generators (including new block model approaches), and a variety of multi-level network analysis techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3200764"
    },
    {
        "title": "Data Mining and Opinion Mining: A Tool in Educational Context",
        "authors": [
            "Myriam Peñafiel",
            "Stefanie Vásquez",
            "Diego Vásquez",
            "Juan Zaldumbide",
            "Sergio Luján-Mora"
        ],
        "date": "July 2018",
        "source": "ICoMS '18: Proceedings of the 2018 1st International Conference on Mathematics and Statistics",
        "abstract": "The use of the web as a universal communication platform generates large volumes of data (Big data), which in many cases, need to be processed so that they can become useful knowledge in face of the sceptics who have doubts about the credibility of such information. The use of web data that comes from educational contexts needs to be addressed, since that large amount of unstructured information is not being valued, losing valuable information that can be used. To solve this problem, we propose the use of data mining techniques such as sentiment analysis to validate the information that comes from the educational platforms. The objective of this research is to propose a methodology that allows the user to apply sentiment analysis in a simple way, because although some researchers have done it, very few do with data in the educational context. The results obtained prove that the proposal can be used in similar cases.",
        "link": "https://dl.acm.org/doi/10.1145/3274250.3274263"
    },
    {
        "title": "Research Hotspots Mining and Visualized Analysis Based on Linking Cluster and K-Core Decomposition",
        "authors": [
            "Ying Cai",
            "Fang Huang",
            "Shuai Wang",
            "Hao Zhang",
            "Chunxiu Du"
        ],
        "date": "May 2018",
        "source": "ICDPA 2018: Proceedings of the International Conference on Data Processing and Applications",
        "abstract": "In this paper, we focus on research hotspots mining with knowledge semantic features on discipline topic word networks, which are extracted from scientific and technical documents. For the topic networks, a novel method of combining linking community clustering with k-core decomposition is proposed to mine the hotspot community with knowledge hierarchy structure. And as a post-processing, the hierarchical community identification is implemented by k-core decomposition in order to discover the internal connection with knowledge semantic hierarchy in the research hotspot. Finally, the community density and visualization method is utilized to analyze the core and sub- problem in the research hotspot community. In experiments, the topic networks for a single discipline and interdisciplinary research hotspot are taken as examples of analysis, and the results show the proposed approach can effectively realize the objective of mining and analyzing the discipline research hotspot on the academic topic word network.",
        "link": "https://dl.acm.org/doi/10.1145/3224207.3224216"
    },
    {
        "title": "The Application of Data Mining Algorithms in the Construction of Travel Recommendation System",
        "authors": [
            "Chao Lou",
            "Zhaonan Mu",
            "Mengzhu Liu"
        ],
        "date": "January 2021",
        "source": "CONF-CDS 2021: The 2nd International Conference on Computing and Data Science",
        "abstract": "Photos shared by users of online social network often associated with tags, texts, geographic and time information. These data are ideal research resource which can be used by researchers to observe people's interests and behaviors. They can also be used to analyze people's travel behaviors and interests because it can reflect people's travel activities and experiences directly. This project aimed to consolidate people into groups based on their travel preferences of travel season, travel time span, scenery type of travel location and then analyze popular travel places for different people groups. The travel preferences were analyzed from extracted data applying PHP language and Naive Bayes classification approach. According to the analyzed data, people were divided into groups. Then, DBSCAN clustering was used to get popular locations based on the different groups. As a result, 80 thousand photo data of 5 cities have been collected from Flickr by Phython. It has been showed that people's preference of season in most of the mined cities was spring and summer and the preference of time span was short. Moreover, mined people has been divided into 16 groups according to their preference and based on the groups popular locations have been calculated by DBSCAN clustering. Theses locations have been provided in a recommendation website which was built in this project. For the future work, mining Flickr data associated with other travel social network sites could be a good way to build a more effective recommendation system.",
        "link": "https://dl.acm.org/doi/10.1145/3448734.3450477"
    },
    {
        "title": "Compare Machine Learning Models in Text Classification Using Steam User Reviews",
        "authors": [
            "Youchen Miao",
            "Zeyu Jin",
            "Yumeng Zhang",
            "Yuchen Chen",
            "Junren Lai"
        ],
        "date": "November 2021",
        "source": "ICSED '21: Proceedings of the 2021 3rd International Conference on Software Engineering and Development",
        "abstract": "Text Classification and Sentiment Analysis of game reviews are viewed as important parts in not only academic fields but also in game studies. In this paper, with more than 400 thousand game reviews on Steam platform, we preprocess the data using different libraries (sklearn, nltk, and spaCy) and use them as inputs to build three sentiment classification models based on different algorithms (Naive Bayes, SVM, and Random Forest). In contrast to previous studies that only focus on different sentiment analysis models, our paper also highlights the use of different APIs to preprocess the data and their corresponding model performance. The results show that no matter which API we choose, Random Forest models always perform the best. However, in terms of training time, Naive Bayes is the fastest. This work can be used to apply grid search for researchers to automatically find the optimum API before conducting sentiment analysis in the future.",
        "link": "https://dl.acm.org/doi/10.1145/3507473.3507480"
    },
    {
        "title": "Lost in Transduction: Transductive Transfer Learning in Text Classification",
        "authors": [
            "Alejandro Moreo",
            "Andrea Esuli",
            "Fabrizio Sebastiani"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Obtaining high-quality labelled data for training a classifier in a new application domain is often costly. Transfer Learning\n(a.k.a. “Inductive Transfer”) tries to alleviate these costs by transferring, to the “target” domain of interest, knowledge available from a different “source” domain. In transfer learning the lack of labelled information from the target domain is compensated by the availability at training time of a set of unlabelled examples from the target distribution. Transductive Transfer Learning denotes the transfer learning setting in which the only set of target documents that we are interested in classifying is known and available at training time. Although this definition is indeed in line with Vapnik’s original definition of “transduction”, current terminology in the field is confused. In this article, we discuss how the term “transduction” has been misused in the transfer learning literature, and propose a clarification consistent with the original characterization of this term given by Vapnik. We go on to observe that the above terminology misuse has brought about misleading experimental comparisons, with inductive transfer learning methods that have been incorrectly compared with transductive transfer learning methods. We then, give empirical evidence that the difference in performance between the inductive version and the transductive version of a transfer learning method can indeed be statistically significant (i.e., that knowing at training time the only data one needs to classify indeed gives an advantage). Our clarification allows a reassessment of the field, and of the relative merits of the major, state-of-the-art algorithms for transfer learning in text classification.",
        "link": "https://dl.acm.org/doi/10.1145/3453146"
    },
    {
        "title": "Machine Learning on Mining Potential Adverse Drug Reactions for Pharmacovigilance",
        "authors": [
            "Ke Jia"
        ],
        "date": "July 2021",
        "source": "DSIT 2021: 2021 4th International Conference on Data Science and Information Technology",
        "abstract": "Drugs are related to the lives and health of patients. Mining and discovering potential adverse drug reactions from a large number of unstructured texts play an important role in drug research and pharmacovigilance. It is a hot research issue in the field of biomedicine in recent years. A large number of scholars have discovered a large amount of information on adverse drug reactions from different data sources through effective machine learning methods. This paper reviews the research progress in recent years to provide a reference for future research.",
        "link": "https://dl.acm.org/doi/10.1145/3478905.3478964"
    },
    {
        "title": "TACAM: Topic And Context Aware Argument Mining",
        "authors": [
            "Michael Fromm",
            "Evgeniy Faerman",
            "Thomas Seidl"
        ],
        "date": "October 2019",
        "source": "WI '19: IEEE/WIC/ACM International Conference on Web Intelligence",
        "abstract": "In this work we address the problem of argument search. The purpose of argument search is the distillation of pro and contra arguments for requested topics from large text corpora. In previous works, the usual approach is to use a standard search engine to extract text parts which are relevant to the given topic and subsequently use an argument recognition algorithm to select arguments from them. The main challenge in the argument recognition task, which is also known as argument mining, is that often sentences containing arguments are structurally similar to purely informative sentences without any stance about the topic. In fact, they only differ semantically. Most approaches use topic or search term information only for the first search step and therefore assume that arguments can be classified independently of a topic. We argue that topic information is crucial for argument mining, since the topic defines the semantic context of an argument. Precisely, we propose different models for the classification of arguments, which take information about a topic of an argument into account. Moreover, to enrich the context of a topic and to let models understand the context of the potential argument better, we integrate information from different external sources such as Knowledge Graphs or pre-trained NLP models. Our evaluation shows that considering topic information, especially in connection with external information, provides a significant performance boost for the argument mining task.",
        "link": "https://dl.acm.org/doi/10.1145/3350546.3352506"
    },
    {
        "title": "Text mining for incoming tasks based on the urgency/importance factors and task classification using machine learning tools",
        "authors": [
            "Yasser Ali Alshehri"
        ],
        "date": "March 2020",
        "source": "ICCDA 2020: Proceedings of the 2020 the 4th International Conference on Compute and Data Analysis",
        "abstract": "In workplaces, there is a massive amount of unstructured data from different sources. In this paper, we present a case study that explains how can through communications between employees, we can help to prioritize tasks requests to increase the efficiency of their works for both technical and non-technical workers. This involves managing daily incoming tasks based on their level of urgency and importance.To allow all workers to utilize the urgency-importance matrix as a time-management tool, we need to automate this tool. The textual content of incoming tasks are analyzed, and metrics related to urgency and importance are extracted. A third factor (i.e., the response variable) is defined based on the two input variables (urgency and importance). Then, machine learning applied to the data to predict the class of incoming tasks based on data outcome desired. We used ordinal regression, neural networks, and decision tree algorithms to predict the four levels of task priority. We measure the performance of all using recalls, precisions, and F-scores. All classifiers perform higher than 89% in terms of all measures.",
        "link": "https://dl.acm.org/doi/10.1145/3388142.3388153"
    },
    {
        "title": "Text mining for malware classification using multivariate all repeated patterns detection",
        "authors": [
            "Konstantinos F. Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2019",
        "source": "ASONAM '19: Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Mobile phones have become nowadays a commodity to the majority of people. Using them, people are able to access the world of Internet and connect with their friends, their colleagues at work or even unknown people with common interests. This proliferation of the mobile devices has also been seen as an opportunity for the cyber criminals to deceive smartphone users and steel their money directly or indirectly, respectively, by accessing their bank accounts through the smartphones or by blackmailing them or selling their private data such as photos, credit card data, etc. to third parties. This is usually achieved by installing malware to smartphones masking their malevolent payload as a legitimate application and advertise it to the users with the hope that mobile users will install it in their devices. Thus, any existing application can easily be modified by integrating a malware and then presented it as a legitimate one. In response to this, scientists have proposed a number of malware detection and classification methods using a variety of techniques. Even though, several of them achieve relatively high precision in malware classification, there is still space for improvement. In this paper, we propose a text mining all repeated pattern detection method which uses the decompiled files of an application in order to classify a suspicious application into one of the known malware families. Based on the experimental results using a real malware dataset, the methodology tries to correctly classify (without any misclassification) all randomly selected malware applications of 3 categories with 3 different families each.",
        "link": "https://dl.acm.org/doi/10.1145/3341161.3350841"
    },
    {
        "title": "Comparative Study between Traditional Machine Learning and Deep Learning Approaches for Text Classification",
        "authors": [
            "Cannannore Nidhi Kamath",
            "Syed Saqib Bukhari",
            "Andreas Dengel"
        ],
        "date": "August 2018",
        "source": "DocEng '18: Proceedings of the ACM Symposium on Document Engineering 2018",
        "abstract": "In this contemporaneous world, it is an obligation for any organization working with documents to end up with the insipid task of classifying truckload of documents, which is the nascent stage of venturing into the realm of information retrieval and data mining. But classification of such humongous documents into multiple classes, calls for a lot of time and labor. Hence a system which could classify these documents with acceptable accuracy would be of an unfathomable help in document engineering. We have created multiple classifiers for document classification and compared their accuracy on raw and processed data. We have garnered data used in a corporate organization as well as publicly available data for comparison. Data is processed by removing the stop-words and stemming is implemented to produce root words. Multiple traditional machine learning techniques like Naive Bayes, Logistic Regression, Support Vector Machine, Random forest Classifier and Multi-Layer Perceptron are used for classification of documents. Classifiers are applied on raw and processed data separately and their accuracy is noted. Along with this, Deep learning technique such as Convolution Neural Network is also used to classify the data and its accuracy is compared with that of traditional machine learning techniques. We are also exploring hierarchical classifiers for classification of classes and subclasses. The system classifies the data faster and with better accuracy than if done manually. The results are discussed in the results and evaluation section.",
        "link": "https://dl.acm.org/doi/10.1145/3209280.3209526"
    },
    {
        "title": "Identification of Overpricing in the Purchase of Medication by the Federal Government of Brazil, Using Text Mining and Clustering Based on Ontology",
        "authors": [
            "Marco Aurelio O. S. Correa",
            "Adriano Galindo Leal"
        ],
        "date": "August 2018",
        "source": "ICCBDC '18: Proceedings of the 2018 2nd International Conference on Cloud and Big Data Computing",
        "abstract": "Increasing the transparency level in his actions and spending is one of the primary duties of the Brazilian Federal Government. The creation of laws that oblige full disclose of all its expenditures through transparency portals enables citizens to supervise all government entities. However, only the dissemination of these data, without a definite standard or the availability of data analysis tools, does not guarantee that the citizen is empowered to play his role. Therefore hence, the objective of this work is to identify overprice in the acquisition of products purchased by the federal government of Brazil using the unstructured data available on the Transparency Portal. The last two-years' worth of purchasing data, available in the Transparency Portal, were extracted, processed and stored. Due to his diverse nature and high volume of data, this study focused only on medicines purchased by the Ministry of Health. Ontology-based text mining and clustering techniques were applied for automatic identification and classification of products. The processing of this information was done through text mining and clustering, based on the ontology registered in another database of the Brazilian government. Because of this work, a consolidated price base per medication was created to allow the identification of distortions in prices practised, facilitating the identification of cases that merit further investigation to unravel fraud to the treasury.",
        "link": "https://dl.acm.org/doi/10.1145/3264560.3264569"
    },
    {
        "title": "Computational Estimation by Scientific Data Mining with Classical Methods to Automate Learning Strategies of Scientists",
        "authors": [
            "Aparna S. Varde"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Experimental results are often plotted as 2-dimensional graphical plots (aka graphs) in scientific domains depicting dependent versus independent variables to aid visual analysis of processes. Repeatedly performing laboratory experiments consumes significant time and resources, motivating the need for computational estimation. The goals are to estimate the graph obtained in an experiment given its input conditions, and to estimate the conditions that would lead to a desired graph. Existing estimation approaches often do not meet accuracy and efficiency needs of targeted applications. We develop a computational estimation approach called AutoDomainMine that integrates clustering and classification over complex scientific data in a framework so as to automate classical learning methods of scientists. Knowledge discovered thereby from a database of existing experiments serves as the basis for estimation. Challenges include preserving domain semantics in clustering, finding matching strategies in classification, striking a good balance between elaboration and conciseness while displaying estimation results based on needs of targeted users, and deriving objective measures to capture subjective user interests. These and other challenges are addressed in this work. The AutoDomainMine approach is used to build a computational estimation system, rigorously evaluated with real data in Materials Science. Our evaluation confirms that AutoDomainMine provides desired accuracy and efficiency in computational estimation. It is extendable to other science and engineering domains as proved by adaptation of its sub-processes within fields such as Bioinformatics and Nanotechnology.",
        "link": "https://dl.acm.org/doi/10.1145/3502736"
    },
    {
        "title": "Where is the road for issue reports classification based on text mining?",
        "authors": [
            "Qiang Fan",
            "Yue Yu",
            "Gang Yin",
            "Tao Wang",
            "Huaimin Wang"
        ],
        "date": "November 2017",
        "source": "ESEM '17: Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Currently, open source projects receive various kinds of issues daily, because of the extreme openness of Issue Tracking System (ITS) in GitHub. ITS is a labor-intensive and time-consuming task of issue categorization for project managers. However, a contributor is only required a short textual abstract to report an issue in GitHub. Thus, most traditional classification approaches based on detailed and structured data (e.g., priority, severity, software version and so on) are difficult to adopt. In this paper, issue classification approaches on a large-scale dataset, including 80 popular projects and over 252,000 issue reports collected from GitHub, were investigated. First, four traditional text-based classification methods and their performances were discussed. Semantic perplexity (i.e., an issues description confuses bug-related sentences with nonbug-related sentences) is a crucial factor that affects the classification performances based on quantitative and qualitative study. Finally, A two-stage classifier framework based on the novel metrics of semantic perplexity of issue reports was designed. Results show that our two-stage classification can significantly improve issue classification performances.",
        "link": "https://dl.acm.org/doi/10.1109/ESEM.2017.19"
    },
    {
        "title": "Feature-based Facebook reviews process model for e-management using data mining",
        "authors": [
            "Anish Kumar Varudharajulu",
            "Yongsheng Ma"
        ],
        "date": "January 2019",
        "source": "IC4E '19: Proceedings of the 10th International Conference on E-Education, E-Business, E-Management and E-Learning",
        "abstract": "The data generated from online communication acts as potential gold mines for discovering knowledge for researchers. A large amount of data is also generated in the form of web documents, emails, blogs, and feedback, etc. Text analytics is being significantly employed to mine important information. Opinion mining is the process of extracting human thoughts and perceptions from unstructured texts. The showstopper for designing an opinion mining system for analyzing reviews arise from the fact that customer reviews are often noisy. These reviews are informally written. In addition, they are subjected to spelling mistakes, grammatical errors, improper punctuation and irrational capitalization. This paper focuses on analyzing the different classification and clustering algorithms aimed at extracting and consolidating opinions of customers from social media sites like Facebook, Twitter and through surveys, at multiple levels of granularity to monitor and measure customer satisfaction. Ours is an automated approach, in which the system aids in the process of knowledge assimilation for knowledge-based building and also performs the analytics. Domain experts ratify the knowledge base and also provide training datasets for the system to intuitively gather more instances for ratification. The system identifies opinion expressions as phrases containing opinion words, opinionated features and also opinion modifiers. These expressions are categorized as positive, negative or neutral. Opinion expressions are identified and categorized using localized linguistic techniques. Opinions can be congregated at any desired level of specificity i.e. feature level or product level, user level or service level, etc. We have developed a system based on this approach, which provides the user with a platform to analyze opinion expressions crawled from a set of pre-defined datasets.",
        "link": "https://dl.acm.org/doi/10.1145/3306500.3306514"
    },
    {
        "title": "Discriminative Topic Mining via Category-Name Guided Text Embedding",
        "authors": [
            "Yu Meng",
            "Jiaxin Huang",
            "Guangyuan Wang",
            "Zihan Wang",
            "Chao Zhang",
            "Yu Zhang",
            "Jiawei Han"
        ],
        "date": "April 2020",
        "source": "WWW '20: Proceedings of The Web Conference 2020",
        "abstract": "Mining a set of meaningful and distinctive topics automatically from massive text corpora has broad applications. Existing topic models, however, typically work in a purely unsupervised way, which often generate topics that do not fit users’ particular needs and yield suboptimal performance on downstream tasks. We propose a new task, discriminative topic mining, which leverages a set of user-provided category names to mine discriminative topics from text corpora. This new task not only helps a user understand clearly and distinctively the topics he/she is most interested in, but also benefits directly keyword-driven classification tasks. We develop CatE, a novel category-name guided text embedding method for discriminative topic mining, which effectively leverages minimal user guidance to learn a discriminative embedding space and discover category representative terms in an iterative manner. We conduct a comprehensive set of experiments to show that CatE mines high-quality set of topics guided by category names only, and benefits a variety of downstream applications including weakly-supervised classification and lexical entailment direction identification.",
        "link": "https://dl.acm.org/doi/10.1145/3366423.3380278"
    },
    {
        "title": "Comparative Study of Heart Disease Diagnosis Using Top Ten Data Mining Classification Algorithms",
        "authors": [
            "I. Ketut Agung Enriko"
        ],
        "date": "June 2019",
        "source": "ICFET '19: Proceedings of the 5th International Conference on Frontiers of Educational Technologies",
        "abstract": "Data mining has been used for many purposes, especially for prediction system. In healthcare, data mining algorithms often used in disease diagnosis. Meanwhile, heart disease is known as a primary cause of death over the years. Many studies have been performed in heart disease diagnosis using data mining methods. There are some popular data mining algorithms that can be used in heart disease diagnosis, for example, k-Nearest Neighbor, CART, and AdaBoost. The algorithms are used to analyze a sample of cardiovascular patients data and predict the heart disease type that they suffer. Some parameters are taken from the patient, including EKG morphology, blood pressure, and information about the existence of chest pain, shortness of breath, palpitation, and cold sweat. In this study, medical records data are collected from Harapan Kita Hospital and utilized as a dataset sample in this research. Top ten data mining classification algorithms are used in diagnosing heart disease from Harapan Kita Hospital data and examining their performance by checking the accuracy and speed.",
        "link": "https://dl.acm.org/doi/10.1145/3338188.3338220"
    },
    {
        "title": "Text Mining Approach for Identifying Research Trends",
        "authors": [
            "Snezhana Sulova"
        ],
        "date": "June 2021",
        "source": "CompSysTech '21: Proceedings of the 22nd International Conference on Computer Systems and Technologies",
        "abstract": "With the increase of unstructured data, the issues connected with automatic text processing, the categorization of documents and the discovery of topics have become objects of growing interest. In order to improve the process of grouping and processing research publications, we would like to propose a method based upon natural language processing. It is based on text mining technologies which aim to identify key tendencies in documents. It processes the content of publications by clustering and identifies the topics of each identified group. This analysis helps by identifying key tendencies as well as discovering emerging new areas of research. Publications from the research literature database, Scopus, were used to test the approach. The topic of the publications is “the application of digital technologies in the logistics business”. The experiments were completed using the RapidMiner Studio software.",
        "link": "https://dl.acm.org/doi/10.1145/3472410.3472433"
    },
    {
        "title": "Assessment of Vulnerability Severity using Text Mining",
        "authors": [
            "Georgios Spanos",
            "Lefteris Angelis",
            "Dimitrios Toloudis"
        ],
        "date": "September 2017",
        "source": "PCI '17: Proceedings of the 21st Pan-Hellenic Conference on Informatics",
        "abstract": "Software1 vulnerabilities are closely associated with information systems security, a major and critical field in today's technology. Vulnerabilities constitute a constant and increasing threat for various aspects of everyday life, especially for safety and economy, since the social impact from the problems that they cause is complicated and often unpredictable. Although there is an entire research branch in software engineering that deals with the identification and elimination of vulnerabilities, the growing complexity of software products and the variability of software production procedures are factors contributing to the ongoing occurrence of vulnerabilities, Hence, another area that is being developed in parallel focuses on the study and management of the vulnerabilities that have already been reported and registered in databases. The information contained in such databases includes, a textual description and a number of metrics related to vulnerabilities. The purpose of this paper is to investigate to what extend the assessment of the vulnerability severity can be inferred directly from the corresponding textual description, or in other words, to examine the informative power of the description with respect to the vulnerability severity. For this purpose, text mining techniques, i.e. text analysis and three different classification methods (decision trees, neural networks and support vector machines) were employed. The application of text mining to a sample of 70,678 vulnerabilities from a public data source shows that the description itself is a reliable and highly accurate source of information for vulnerability prioritization.",
        "link": "https://dl.acm.org/doi/10.1145/3139367.3139390"
    },
    {
        "title": "Clustering Algorithms for Spatial Data Mining",
        "authors": [
            "Chetashri Bhadane",
            "Ketan Shah"
        ],
        "date": "April 2020",
        "source": "ICGDA '20: Proceedings of the 2020 3rd International Conference on Geoinformatics and Data Analysis",
        "abstract": "With the advances in mobile and wireless technologies, there has been a rise in applications that track and share the users' geospatial data. People use several social networking sites such as Twitter, Facebook and Flickr, where they share their status updates. With the integration of Global Positioning System (GPS) with mobile phones, it is now possible to share one's locations on these social networks. GPS allows us to record and track a person's movement along with the timestamp. The data set obtained from these GPS logs is vast and is widely used to analyze the users' movement patterns. Specifically, we can find out significant locations based on the number of users present at that location and the time spent by them at such places. Once significant places have been identified, it is also possible to identify the semantic importance of these locations. This paper presents an overview of the clustering techniques used to find important places of interest using large GPS based mobility datasets. Four clustering algorithms, K-Means, DBSCAN, OPTICS and Hierarchical, are implemented, and performance is tested using real-time data of 50 users collected over 2--5 years. Performance summary depicts that K-Means and DBSCAN perform well for spatial data.",
        "link": "https://dl.acm.org/doi/10.1145/3397056.3397068"
    },
    {
        "title": "Feature Selection based Arabic Text Classification using Different Machine Learning Algorithms: Comparative Study",
        "authors": [
            "Sakina Rim Bennabi",
            "Zakaria Elberrichi"
        ],
        "date": "June 2020",
        "source": "ICIST '20: Proceedings of the 10th International Conference on Information Systems and Technologies",
        "abstract": "Feature selection is a method of data pre-processing widely used when mining large data, such as textual classification. Several studies have been conducted to compare the different methods of feature selection applied to corpora in English. Unfortunately, a small number of works concern the Arabic language. This article aims to present a comparative study of different feature selection techniques including: Chi2, the ANOVA method and mutual information, applied on a corpus in Arabic language, while also diversifying the machine learning algorithms (Naive Bayes, SVM and KNN). This experimental study has shown in general that reducing dimensionality with feature selection techniques has slightly affected the performance of textual classification, reducing the size of the corpus by up to 1%.",
        "link": "https://dl.acm.org/doi/10.1145/3447568.3448531"
    },
    {
        "title": "Intelligent Data Analysis using Optimized Support Vector Machine Based Data Mining Approach for Tourism Industry",
        "authors": [
            "Ms Promila Sharma",
            "Uma Meena",
            "Girish Kumar Sharma"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Data analysis involves the deployment of sophisticated approaches from data mining methods, information theory, and artificial intelligence in various fields like tourism, hospitality, and so on for the extraction of knowledge from the gathered and preprocessed data. In tourism, pattern analysis or data analysis using classification is significant for finding the patterns that represent new and potentially useful information or knowledge about the destination and other data. Several data mining techniques are introduced for the classification of data or patterns. However, overfitting, less accuracy, local minima, sensitive to noise are the drawbacks in some existing data mining classification methods. To overcome these challenges, Support vector machine with Red deer optimization (SVM-RDO) based data mining strategy is proposed in this article. Extended Kalman filter (EKF) is utilized in the first phase, i.e., data cleaning to remove the noise and missing values from the input data. Mantaray foraging algorithm (MaFA) is used in the data selection phase, in which the significant data are selected for the further process to reduce the computational complexity. The final phase is the classification, in which SVM-RDO is proposed to access the useful pattern from the selected data. PYTHON is the implementation tool used for the experiment of the proposed model. The experimental analysis is done to show the efficacy of the proposed work. From the experimental results, the proposed SVM-RDO achieved better accuracy, precision, recall, and F1 score than the existing methods for the tourism dataset. Thus, it is showed the effectiveness of the proposed SVM-RDO for pattern analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3494566"
    },
    {
        "title": "Applying text mining to predict learners' cognitive engagement",
        "authors": [
            "Hayati Hind",
            "Mohammed Khalidi Idrissi",
            "Samir Bennani"
        ],
        "date": "October 2017",
        "source": "SCAMS '17: Proceedings of the Mediterranean Symposium on Smart City Application",
        "abstract": "In academic process, engagement represent one of the important success' factor. This paper proposes a predictive system for learners' cognitive engagement based on their online discussion forums participation. In the first level the ontology OWL and the LSA method are used to perform a semantic classification system of the threads according to a specific context chosen by the tutor. Then the Text mining, as a predictive method, is applied to the classified threads and learners' participation in the forums.",
        "link": "https://dl.acm.org/doi/10.1145/3175628.3175655"
    },
    {
        "title": "Investigate Transitions into Drug Addiction through Text Mining of Reddit Data",
        "authors": [
            "John Lu",
            "Sumati Sridhar",
            "Ritika Pandey",
            "Mohammad Al Hasan",
            "Georege Mohler"
        ],
        "date": "July 2019",
        "source": "KDD '19: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Increasing rates of opioid drug abuse and heightened prevalence of online support communities underscore the necessity of employing data mining techniques to better understand drug addiction using these rapidly developing online resources. In this work, we obtained data from Reddit, an online collection of forums, to gather insight into drug use/misuse using text snippets from users narratives. Specifically, using users' posts, we trained a binary classifier which predicts a user's transitions from casual drug discussion forums to drug recovery forums. We also proposed a Cox regression model that outputs likelihoods of such transitions. In doing so, we found that utterances of select drugs and certain linguistic features contained in one's posts can help predict these transitions. Using unfiltered drug-related posts, our research delineates drugs that are associated with higher rates of transitions from recreational drug discussion to support/recovery discussion, offers insight into modern drug culture, and provides tools with potential applications in combating the opioid crisis.",
        "link": "https://dl.acm.org/doi/10.1145/3292500.3330737"
    },
    {
        "title": "Analysing digital banking reviews using text mining",
        "authors": [
            "Li Chen Cheng",
            "Legaspi Rhea Sharmayne"
        ],
        "date": "December 2020",
        "source": "ASONAM '20: Proceedings of the 12th IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Digital banks are new entrants in the banking industry in the Philippines as they only started late 2018. Since then, a handful of players have and are still emerging. With more and more people becoming technologically savvy, it is very critical for financial institutions to develop a digital banking application that will stand out from the competition. This paper aims to use text mining methods to analyse digital banking application reviews. This study will perform topic modelling using LDA to explore customer concerns and will mine association rules between the digital banking features with the review score. The results will reveal which areas the digital banking application can further optimize for customer satisfaction and retention.",
        "link": "https://dl.acm.org/doi/10.1109/ASONAM49781.2020.9381429"
    },
    {
        "title": "Mining Moocs Videos Metadata Using Classification Techniques",
        "authors": [
            "El Harrak Othman",
            "Ghadi Abderrahim",
            "El Bouhdidi Jaber"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "In few years, the internet has become the first source of information for most people, Today MOOCs makes it possible for everyone to access to the education over the world, it's represents an emerging methodology of online teaching and an important development in open education. Due to the rapid development of Moocs and the rapid growth of digital data and video database over the Internet, it is becoming very difficult for learners to browsing and choosing the best training for them. The scientific community has increased the amount of research into new technologies, with a view to improve the digital video utilization: its archiving, indexing, accessibility, acquisition, store and even its process and usability. All these parts of the video utilization entail the necessity of the extraction of all important information of a video, especially in cases of lack of metadata information. Web video mining is retrieving the content using data mining techniques from World Wide Web. There are two approaches for web video mining using traditional image processing (signal processing) and metadata based approach. In this paper, we present the various research makes in this subject and we propose an effective methodology to extract the metadata from Moocs videos and classify them based on the extracted metadata by applying data mining techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090450"
    },
    {
        "title": "Text mining with HathiTrust",
        "authors": [
            "Eleanor Dickson Koehl",
            "Ryan Dubnicek"
        ],
        "date": "June 2019",
        "source": "JCDL '19: Proceedings of the 18th Joint Conference on Digital Libraries",
        "abstract": "This tutorial will introduce attendees to the HathiTrust Research Center and its tools and services for computational text analysis research. HTRC leverages the scope and scale of the HathiTrust Digital Library collection to create opportunities for researchers to perform text data mining on subsets of the corpus. Attendees will develop skills that will allow them to conduct basic text analysis research using HathiTrust data, learn techniques for engaging with scholars on their text data mining research, and explore strategies for computational access to large-scale digital collections.",
        "link": "https://dl.acm.org/doi/10.1109/JCDL.2019.00115"
    },
    {
        "title": "Text mining for plagiarism detection: multivariate pattern detection for recognition of text similarities",
        "authors": [
            "Konstantinos Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2018",
        "source": "ASONAM '18: Proceedings of the 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "The problem of plagiarism the recent years has been intensified by the availability of information in digital form and the accessibility of the electronic libraries through the Internet. As a result, plagiarism detection has been transformed into a big data analytics problem since the number of digital sources is extravagant and a new document needs to be compared with millions of other existing documents. In this paper, a text mining methodology is proposed that can detect all common patterns between a document and the documents in a reference database. The technique is based on a pattern detection algorithm and the corresponding data structure that enables the algorithm to detect all common patterns. The methodology has been applied in a well-defined dataset providing very promising results identifying difficult cases of plagiarism such as technical disguise.",
        "link": "https://dl.acm.org/doi/10.5555/3382225.3382424"
    },
    {
        "title": "Petroleum Engineering Data Text Classification Using Convolutional Neural Network Based Classifier",
        "authors": [
            "Tzuhan Hsu",
            "Yaoqin Zhang"
        ],
        "date": "May 2018",
        "source": "ICMLT '18: Proceedings of the 2018 International Conference on Machine Learning Technologies",
        "abstract": "In this paper, authors introduce a convolutional neural network (CNN) based classification approach for petroleum engineering event. Recently, text classification problem has raised substantial attention in the field of machine learning. The aim of text classification process is to assign correct label to a text on the basis of its content. However, due to the imprecise of feature representation, most of former text classification methods cannot export satisfying result. To solve this problem, authors propose a model based on convolutional neural network. Authors conduct experiments with real world petroleum engineering data text, artificially labelled into 6 categories. Beside purposed CNN-based model, the authors also implement other baseline machine learning approaches: k nearest neighbor, random forest, support vector machines, recurrent neural network, gated recurrent units, long short-term memory. Among all these models, our CNN-based model reach best performance in text classification task. In the last part of this paper, authors compare and analysis their purpose and baseline model.",
        "link": "https://dl.acm.org/doi/10.1145/3231884.3231898"
    },
    {
        "title": "Classifying Indonesian Online Articles as Advertisement Placement Base Using Text Mining",
        "authors": [
            "Nadhira Tasya",
            "Arian Dhini"
        ],
        "date": "July 2017",
        "source": "ICBIM 2017: Proceedings of the International Conference on Business and Information Management",
        "abstract": "Rapid development in technological aspect resulting in growing level of human needs for the latest news, so that emerged a new trend of publishing and accessing news through online media or usually called online journalism. In addition, the number of people who sell and purchase through online sites also continues to increase and this opportunity is utilized by the company and the advertiser by implementing targeted web advertising. However, the high number of articles that have been published and accessed leads to great opportunities for errors in determining where to place the ads. Therefore, it needs a system that can categorize articles accessed by users as the basis of advertisement placement by the company and this classification system can be done by applying the method of Data Mining and Text Mining. This research uses document data in the form of article content that will be categorized into twenty categories of class of advertisement by using Text Mining technique with Support Vector Machine algorithm. The results of this study may be used by companies or advertisers as a basis for placement of ads on selected online media sites.",
        "link": "https://dl.acm.org/doi/10.1145/3134271.3134288"
    },
    {
        "title": "Mining Free-Text Medical Notes for Suicide Risk Assessment",
        "authors": [
            "Marios Adamou",
            "Grigoris Antoniou",
            "Elissavet Greasidou",
            "Vincenzo Lagani",
            "Paulos Charonyktakis",
            "Ioannis Tsamardinos"
        ],
        "date": "July 2018",
        "source": "SETN '18: Proceedings of the 10th Hellenic Conference on Artificial Intelligence",
        "abstract": "Suicide has been considered as an important public health issue for a very long time, and is one of the main causes of death worldwide. Despite suicide prevention strategies being applied, the rate of suicide has not changed substantially over the past decades. Advances in machine learning make it possible to attempt to predict suicide based on the analysis of relevant data to inform clinical practice. This paper reports on findings from the analysis of data of patients who died by suicide in the period 2013-2016 and made use of both structured data and free-text medical notes. We focus on examining various text-mining approaches to support risk assessment. The results show that using advance machine learning and text-mining techniques, it is possible to predict within a specified period which people are most at risk of taking their own life at the time of referral to a mental health service.",
        "link": "https://dl.acm.org/doi/10.1145/3200947.3201020"
    },
    {
        "title": "Integration of data mining techniques in e-learning systems: Clustering Profil of Lerners and Recommender Course System",
        "authors": [
            "A. El Moustamid",
            "E. En-Naimi",
            "J. El Bouhdidi"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "Data mining aims at the extraction of knowledge from large quantities of data by automatic or semi-automatic methods. In the field of education and training e-learning, Data Mining techniques as clustering (The aim of this technique is to identify groups that are similar in some aspect) [21], prediction (is used for developing model where combination of data viewed from different aspects can effect single viewed data. Here, is important to watch how predictors variables are effecting predicted variable)[21], classification are applied in e-learning to improve the level of learners by providing learning according to the learners' requirement) are used to analyze learners profile, to predict and improve student performance. The aim of our project is to develop a system capable of analyzing learners profile and indexing web videos in order to offer learners a rich database with courses that correspond to their levels.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090453"
    },
    {
        "title": "Multi-level mining and visualization of scientific text collections: Exploring a bi-lingual scientific repository",
        "authors": [
            "Pablo Accuosto",
            "Francesco Ronzano",
            "Daniel Ferrés",
            "Horacio Saggion"
        ],
        "date": "December 2017",
        "source": "WOSP 2017: Proceedings of the 6th International Workshop on Mining Scientific Publications",
        "abstract": "We present a system to mine and visualize collections of scientific documents by semantically browsing information extracted from single publications or aggregated throughout corpora of articles. The text mining tool performs deep analysis of document collections allowing the extraction and interpretation of research paper's contents. In addition to the extraction and enrichment of documents with metadata (titles, authors, affiliations, etc), the deep analysis performed comprises semantic interpretation, rhetorical analysis of sentences, triple-based information extraction, and text summarization. The visualization components allow geographical-based exploration of collections, topic-evolution interpretation, and collaborative network analysis among others. The paper presents a case study of a bi-lingual collection in the field of Natural Language Processing (NLP).",
        "link": "https://dl.acm.org/doi/10.1145/3127526.3127529"
    },
    {
        "title": "Hierarchical Topic Mining via Joint Spherical Tree and Text Embedding",
        "authors": [
            "Yu Meng",
            "Yunyi Zhang",
            "Jiaxin Huang",
            "Yu Zhang",
            "Chao Zhang",
            "Jiawei Han"
        ],
        "date": "August 2020",
        "source": "KDD '20: Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Mining a set of meaningful topics organized into a hierarchy is intuitively appealing since topic correlations are ubiquitous in massive text corpora. To account for potential hierarchical topic structures, hierarchical topic models generalize flat topic models by incorporating latent topic hierarchies into their generative modeling process. However, due to their purely unsupervised nature, the learned topic hierarchy often deviates from users' particular needs or interests. To guide the hierarchical topic discovery process with minimal user supervision, we propose a new task, Hierarchical Topic Mining, which takes a category tree described by category names only, and aims to mine a set of representative terms for each category from a text corpus to help a user comprehend his/her interested topics. We develop a novel joint tree and text embedding method along with a principled optimization procedure that allows simultaneous modeling of the category tree structure and the corpus generative process in the spherical space for effective category-representative term discovery. Our comprehensive experiments show that our model, named JoSH, mines a high-quality set of hierarchical topics with high efficiency and benefits weakly-supervised hierarchical text classification tasks.",
        "link": "https://dl.acm.org/doi/10.1145/3394486.3403242"
    },
    {
        "title": "Unsupervised behavioural mining and clustering for malware family identification",
        "authors": [
            "Khanh Huu The Dam",
            "Thomas Given-Wilson",
            "Axel Legay"
        ],
        "date": "March 2021",
        "source": "SAC '21: Proceedings of the 36th Annual ACM Symposium on Applied Computing",
        "abstract": "More accurate and advanced detection and classification of malware requires exploiting program behaviour and not merely syntactic or static features. One approach is to use system call dependency graphs (SCDGs) that represent the program behaviour by interactions with the system, and the relations between these interactions. These SCDGs have been used with supervised learning techniques to very accurately detect and classify malware. This works considers the unsupervised learning challenge of mining for common clusters of behaviour without a priori knowledge. This allows for clustering of similar programs by behaviour, that can then be used for either classification or further analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3412841.3441919"
    },
    {
        "title": "Text Mining for Evaluating Authors' Birth and Death Years",
        "authors": [
            "Dror Moghaz",
            "Yaakov Hacohen-Kerner",
            "Dov Gabbay"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "This article presents a unique method in text and data mining for finding the era, i.e., mining temporal data, in which an anonymous author was living. Finding this era can assist in the examination of a fake document or extracting the time period in which a writer lived. The study and the experiments concern Hebrew, and in some parts, Aramaic and Yiddish rabbinic texts. The rabbinic texts are undated and contain no bibliographic sections, posing an interesting challenge. This work proposes algorithms using key phrases and key words that allow the temporal organization of citations together with linguistic patterns. Based on these key phrases, key words, and the references, we established several types of “Iron-clad,” Heuristic and Greedy rules for estimating the years of birth and death of a writer in an interesting classification task. Experiments were conducted on corpora, including documents authored by 12, 24, and 36 rabbinic writers and demonstrated promising results.",
        "link": "https://dl.acm.org/doi/10.1145/3281631"
    },
    {
        "title": "Transfer Learning to Timed Text Based Video Classification Using CNN",
        "authors": [
            "Zenun Kastrati",
            "Ali Shariq Imran",
            "Arianit Kurti"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "Open educational video resources are gaining popularity with a growing number of massive open online courses (MOOCs). This has created a niche for content providers to adopt effective solutions in automatically organizing and structuring of educational resources for maximum visibility. Recent advances in deep learning techniques are proving useful in managing and classifying resources into appropriate categories. This paper proposes one such convolutional neural network (CNN) model for classifying video lectures in a MOOC setting using a transfer learning approach. The model uses a time-aligned text transcripts corresponding to video lectures from six broader subject categories. Video lectures and their corresponding transcript dataset is gathered from the Coursera MOOC platform. Two different CNN models are proposed: i) CNN based classification using embeddings learned from our MOOC dataset, ii) CNN based classification using transfer learning. Word embeddings generated from two well known state-of-the-art pre-trained models Word2Vec and GloVe, are used in the transfer learning approach for the second case. The proposed CNN models are evaluated using precision, recall, and F1 score and the obtained performance is compared with both conventional and deep learning classifiers. The proposed CNN models have an F1 score improvement of 10-22 percentage points over DNN and conventional classifiers",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326483"
    },
    {
        "title": "Evolutionary role mining in complex networks by ensemble clustering",
        "authors": [
            "Sarvenaz Choobdar",
            "Pedro Ribeiro",
            "Fernando Silva"
        ],
        "date": "April 2017",
        "source": "SAC '17: Proceedings of the Symposium on Applied Computing",
        "abstract": "The structural patterns in the neighborhood of nodes assign unique roles to the nodes. Mining the set of existing roles in a network provides a descriptive profile of the network and draws its general picture. This paper proposes a new method to determine structural roles in a dynamic network based on the current position of nodes and their historic behavior. We develop a temporal ensemble clustering technique to dynamically find groups of nodes, holding similar tempo-structural roles. We compare two weighting functions, based on age and distribution of data, to incorporate temporal behavior of nodes in the role discovery. To evaluate the performance of the proposed method, we assess the results from two points of view: 1) goodness of fit to current structure of the network; 2) consistency with historic data. We conduct the evaluation using different ensemble clustering techniques. The results on real world networks demonstrate that our method can detect tempo-structural roles that simultaneously depict the topology of a network and reflect its dynamics with high accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3019612.3019815"
    },
    {
        "title": "Analyzing used-car web listings via text mining",
        "authors": [
            "Ayhan Demiriz",
            "Fatma Cantaş"
        ],
        "date": "October 2017",
        "source": "IML '17: Proceedings of the 1st International Conference on Internet of Things and Machine Learning",
        "abstract": "Used car trade is one of the major components of the world economies. It is not uncommon to sell a car by placing an internet advertisement irrespective of the geography in these days. A typical content of an advertisement is usually composed of two parts namely the structured and the free text data. The structured data may include some information about the asking price, make, model, year, mileage of the car and the contact info. In most cases, seller may give important clues about the car's current conditions in the free text data where the title (head) of the advertisement can be included as free text too. This paper reports preliminary results from a text mining study conducted on 75K used car internet listings collected from two major car listing web sites in Turkey. As expected, the words and the phrases related to the description of the car are observed to be frequent. The leading concepts in the free text are found to be regarding how to describe the current condition of a car, for example \"no crash history\".",
        "link": "https://dl.acm.org/doi/10.1145/3109761.3109782"
    },
    {
        "title": "Text classification of Diseases Treated by Traditional Chinese Medicine Prescription based on machine learning",
        "authors": [
            "Hanqing Zhao",
            "Bo Han",
            "Chen Li"
        ],
        "date": "September 2020",
        "source": "ISAIMS '20: Proceedings of the 1st International Symposium on Artificial Intelligence in Medical Sciences",
        "abstract": "OBJECTIVE: To explore the application of machine learning in th e identification of diseases treated by traditional Chinese medicine prescriptions. METHODS: Based on the composition of the text document of Chinese medicine prescriptions, the prescriptions were divided into cough, headache and diarrhea. THUTCT were introduced to establish and train two machine learning text classification models, LibSVM and LibLinear, and the prescriptions to be deter mined were put into the model for classification and prediction. R ESULTS: The Precision rate of LibSVM model and Liblinear mod el were 0.7283 and 0.6690. Seven prescriptions were classified an d predicted, and the results were in line with expectations. CONCLUSION: THUCTC has good universality for the content of TCM prescriptions, high classification accuracy and fast testing speed, which is suitable for the text classification and discrimination research of TCM prescriptions for diseases.",
        "link": "https://dl.acm.org/doi/10.1145/3429889.3429895"
    },
    {
        "title": "Evaluation of retweet clustering method classification method using retweets on Twitter without text data",
        "authors": [
            "K. Uchida",
            "F. Toriumi",
            "T. Sakaki"
        ],
        "date": "August 2017",
        "source": "WI '17: Proceedings of the International Conference on Web Intelligence",
        "abstract": "Burst phenomena, which frequently occur on social media, are caused by such social events as flaming on the internet, elections, and natural disasters. To understand people's thoughts and feelings, we must classify their opinions from burst phenomena. Therefore, classification methods that categorize tweets are critical. However, since most classification methods focus on text mining, they cannot group tweets by topics because each tweet has poor linguistic similarities. We used a non-text-based classification method proposed by Baba et al. that groups tweets by topics, even if they have poor linguistic similarities, and verified its validity by comparing it with a text-based classification method in two different evaluations: qualitative and quantitative. In the qualitative evaluation part, we did a questionnaire survey and validated the suitability of the topic clusters created using both the non-and text-based methods. Since evaluating the similarity of every pair of tweets in each topic is difficult, we evaluated the similarity between sampled pairs in the survey and acquired more appropriate topic clustering results using the non-text-based method than the text-based method. In the quantitative evaluation part, we focused on the robustness of each method against data reduction. Many approaches analyze social media data, especially because collecting data from social media is comparatively easy. However, since collecting the whole data of burst phenomena is very costly due to the vast amounts of available social media data, robustness against data reduction is an important index to evaluate classification methods. With the non-text-based method, over 55% of the pairs of tweets in the same cluster were also included in the same cluster even when the data were reduced to 10% in all three of our example cases. In this paper, as a source we focus on Twitter, one of the most popular microblogging services. Using clustering to conduct detailed case analyses, we scrutinized three burst cases that include natural disasters and flaming on the internet and found that a non-text-based method more effectively classified tweets in burst phenomena than a text-based method.",
        "link": "https://dl.acm.org/doi/10.1145/3106426.3106451"
    },
    {
        "title": "Using Spark for Text Mining on Large Scale Liver Cancer Literature",
        "authors": [
            "Ming-Yen Lin",
            "Yu-Ju Lin",
            "Sue-Chen Hsueh"
        ],
        "date": "January 2021",
        "source": "BDET 2021: 2021 the 3rd International Conference on Big Data Engineering and Technology (BDET)",
        "abstract": "Cancer is one of the main causes of death. The number of known cancer to-date is more than one hundred. Liver cancer, ranked after trachea and lung cancer, has long been high in the cancer leading cause in Taiwan and even ranked as second in 2016. The number of scientific articles related to cancer proliferates every year. The number reaches as high as 20 million in PubMed so that discovering useful information from the massive collection is very difficult. In addition, using a single machine to sift through these articles is very time-consuming. Therefore, we present a big data analytic framework using the distributed Apache Spark platform for text mining in PubMed literature. We establish a prediction model for liver cancer articles so that researchers may effectively validate whether an article is related to liver cancer or not. Classification models in Spark MLlib including Linear Support Vector Machines (SVM), Logistic Regression, are used in our experiments. Relevancy to liver cancer is further confirmed by using MeSH (Medical Subject Headings) terms. Logistic regression is about 3 times faster than SVMs and the accuracy of both methods is close to 95% in the experiments using hold-out validation. When max_features is 500 and min_df ≤ 0.1 (or min_df = 1), the accuracy may reach 96%. In the experiments with K-fold cross-validation, the accuracy of SVMs methods is 96%. The experimental results show that that our prediction model may effectively classify liver cancer articles.",
        "link": "https://dl.acm.org/doi/10.1145/3474944.3474958"
    },
    {
        "title": "Automatic topic classification of test cases using text mining at an Android smartphone vendor",
        "authors": [
            "Junji Shimagaki",
            "Yasutaka Kamei",
            "Naoyasu Ubayashi",
            "Abram Hindle"
        ],
        "date": "October 2018",
        "source": "ESEM '18: Proceedings of the 12th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Background: An Android smartphone is an ecosystem of applications, drivers, operating system components, and assets. The volume of the software is large and the number of test cases needed to cover the functionality of an Android system is substantial. Enormous effort has been already taken to properly quantify \"what features and apps were tested and verified?\". This insight is provided by dashboards that summarize test coverage and results per feature. One method to achieve this is to manually tag or label test cases with the topic or function they cover, much like function points. At the studied Android smartphone vendor, tests are labelled with manually defined tags, so-called \"feature labels (FLs)\", and the FLs serve to categorize 100s to 1000s test cases into 10 to 50 groups. Aim: Unfortunately for developers, manual assignment of FLs to 1000s of test cases is a time consuming task, leading to inaccurately labeled test cases, which will render the dashboard useless. We created an automated system that suggests tags/labels to the developers for their test cases rather than manual labeling. Method: We use machine learning models to predict and label the functionality tested by 10,000 test cases developed at the company. Results: Through the quantitative experiments, our models achieved acceptable F-1 performance of 0.3 to 0.88. Also through the qualitative studies with expert teams, we showed that the hierarchy and path of tests was a good predictor of a feature's label. Conclusions: We find that this method can reduce tedious manual effort that software developers spent classifying test cases, while providing more accurate classification results.",
        "link": "https://dl.acm.org/doi/10.1145/3239235.3268927"
    },
    {
        "title": "To apply Data Mining for Classification of Crowd sourced Software Requirements",
        "authors": [
            "Soonh Taj",
            "Qasim Arain",
            "Imran Memon",
            "Asma Zubedi"
        ],
        "date": "April 2019",
        "source": "ICSIE '19: Proceedings of the 8th International Conference on Software and Information Engineering",
        "abstract": "Now a day's main focus of developers is to build quality software that works according to customer needs and for this reason it is necessary to gather right requirements as requirement elicitation is the critical step that impacts on the success of software project as misinterpreted requirements leads to the failure of software project. By keeping this in mind a research is carried out on improving requirements elicitation process and automating the process of classifying requirements. In this research, a model is proposed which will help in this scenario for requirements elicitation and requirement classification. This paper presents a model in which crowd sourcing approach is used so that customers, end users, stakeholders, developers and software engineers can make active participation for requirement elicitation process and requirements gathered using crowdsourcing approach are used by model for classification process i.e. classification of requirements into functional and non-functional requirements. For the proof of proposed model a case study is conducted. Results of case study provided the usefulness and efficiency of proposed model for classification of crowd sourced software requirements.",
        "link": "https://dl.acm.org/doi/10.1145/3328833.3328837"
    },
    {
        "title": "Text classification using Fuzzy TF-IDF and Machine Learning Models",
        "authors": [
            "Mariem Bounabi",
            "Karim El Moutaouakil",
            "Khalid Satori"
        ],
        "date": "October 2019",
        "source": "BDIoT'19: Proceedings of the 4th International Conference on Big Data and Internet of Things",
        "abstract": "The representation of the information has an important impact on the text classification task. Several weighting methods were proposed in the literature, and the term frequency-inverse term frequency (TFIDF), the most know on the text treatment field. The FTF-IDF is a vector representation where the components of the TFIDF are presented as inputs to the Fuzzy Inference System (FIS). In this work, we compare several Machin Learning algorithms such as Naïve Bayes and its derivatives, SVM and Random forest classifiers, using the FTF-IDF representation. To improve the quality of the used classifiers, we call sum attribute selection methods. The recognition rate, for the tested systems, is satisfied, where the system based on naïve Bayes classifier, the FTF-IDF weighting terms, and the info gain select attributes method gives 98.7% as accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3372938.3372956"
    },
    {
        "title": "Text Mining in Cybersecurity: A Systematic Literature Review",
        "authors": [
            "Luciano Ignaczak",
            "Guilherme Goldschmidt",
            "Cristiano André Da Costa",
            "Rodrigo Da Rosa Righi"
        ],
        "date": null,
        "source": "ACM Computing Surveys",
        "abstract": "The growth of data volume has changed cybersecurity activities, demanding a higher level of automation. In this new cybersecurity landscape, text mining emerged as an alternative to improve the efficiency of the activities involving unstructured data. This article proposes a Systematic Literature Review (SLR) to present the application of text mining in the cybersecurity domain. Using a systematic protocol, we identified 2,196 studies, out of which 83 were summarized. As a contribution, we propose a taxonomy to demonstrate the different activities in the cybersecurity domain supported by text mining. We also detail the strategies evaluated in the application of text mining tasks and the use of neural networks to support activities involving unstructured data. The work also discusses text classification performance aiming its application in real-world solutions. The SLR also highlights open gaps for future research, such as the analysis of non-English content and the intensification in the usage of neural networks.",
        "link": "https://dl.acm.org/doi/10.1145/3462477"
    },
    {
        "title": "Using Text Mining to Analyze the Financial Patents",
        "authors": [
            "Yung-Feng Lu",
            "Jia-Lang Xu",
            "Mu-Yen Chen"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "With the advancement and development of science and technology, Nowadays society is the development of information technology, cloud computing, mobile Internet, Internet of Things, block chain and information technology.The combination of finance and technology has made financial technology terms gradually emerging.. This research analyzes the patents of financial technology. However, Taiwan's financial technology patents are mostly G06Q-20 (payment plan, architecture or agreement), G06Q-30 (commercial, such as marketing, shopping, payment, auction or e-commerce). G06Q-40 (financial, investment or tax treatment, insurance).The research use text mining technology to analyze and understand Taiwan's trends and developments in financial technology. The research results show that G06Q-20 patents are biased towards information security and mobile payment. G06Q-30 patents favor shopping websites and advertising. G06Q-40 patents are biased towards diversity, such as securities, insurance, finance, and stocks.",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326478"
    },
    {
        "title": "Identifying Online Profiles of Distance Learning Students Using Data Mining Techniques",
        "authors": [
            "Osama Islam",
            "Muazzam Siddiqui",
            "Naif Radi Aljohani"
        ],
        "date": "October 2019",
        "source": "ICDTE '19: Proceedings of the 3rd International Conference on Digital Technology in Education",
        "abstract": "Educational data has grown over the years with the increased use of technology within educational environments. This has led to a huge amount of data being stored in various data sources representing the student, his/her activities, and other aspects relevant to the learning process. To meet this analytical need, Educational Data Mining (EDM) has emerged to assist educational institutions in identifying key benefits such as students at risk, the level of student engagement or predicting student performance. The aim of this research was to explore the various aspects of student interaction data using data mining techniques to identify relevant patterns of behaviors and possible key attributes that have higher degrees of influence on distance learning students. The main findings identified several patterns of user online profiles based on a set of adopted learning strategies, the research proposed a framework for analyzing such interaction data based on R and Hadoop platforms to correlate online profiles with student performance.",
        "link": "https://dl.acm.org/doi/10.1145/3369199.3369249"
    },
    {
        "title": "Extremely Fast Decision Tree Mining for Evolving Data Streams",
        "authors": [
            "Albert Bifet",
            "Jiajin Zhang",
            "Wei Fan",
            "Cheng He",
            "Jianfeng Zhang",
            "Jianfeng Qian",
            "Geoff Holmes",
            "Bernhard Pfahringer"
        ],
        "date": "August 2017",
        "source": "KDD '17: Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
        "abstract": "Nowadays real-time industrial applications are generating a huge amount of data continuously every day. To process these large data streams, we need fast and efficient methodologies and systems. A useful feature desired for data scientists and analysts is to have easy to visualize and understand machine learning models. Decision trees are preferred in many real-time applications for this reason, and also, because combined in an ensemble, they are one of the most powerful methods in machine learning. In this paper, we present a new system called STREAMDM-C++, that implements decision trees for data streams in C++, and that has been used extensively at Huawei. Streaming decision trees adapt to changes on streams, a huge advantage since standard decision trees are built using a snapshot of data, and can not evolve over time. STREAMDM-C++ is easy to extend, and contains more powerful ensemble methods, and a more efficient and easy to use adaptive decision trees. We compare our new implementation with VFML, the current state of the art implementation in C, and show how our new system outperforms VFML in speed using less resources.",
        "link": "https://dl.acm.org/doi/10.1145/3097983.3098139"
    },
    {
        "title": "Data miners' little helper: data transformation activity cues for cluster analysis on document collections",
        "authors": [
            "Tania Cerquitelli",
            "Evelina Di Corso",
            "Francesco Ventura",
            "Silvia Chiusano"
        ],
        "date": "June 2017",
        "source": "WIMS '17: Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "In this paper we propose a new self-learning engine to streamline the analytics process, as it enables analysts to mine massive data repositories with minimal user intervention. In the context of cluster analysis on a collection of documents this new system, named SELF-DATA (SELF-learning DAta TrAnsformation), suggests to the analyst how to configure the whole mining process for a given dataset. SELF-DATA relies on an engine exploring different data weighting schemas (e.g., normalized term frequencies) and data transformation methods (e.g., PCA) before applying the cluster analysis, evaluating and comparing solutions through different quality indices (e.g., weighted Silhouette), and presenting the k-top solutions to the analyst. SELF-DATA will also include a knowledge base storing results of experiments on previously processed datasets, and a classification algorithm trained on the knowledge base content to forecast the best configuration for the whole mining process for an unexplored dataset. The first development of SELF-DATA running on Apache Spark has been validated on 5 collections of documents. Experimental results highlight that TF-IDF and logarithmic entropy are effective to measure item relevance with sparse datasets, and the LSI method outperforms PCA with a large dictionary.",
        "link": "https://dl.acm.org/doi/10.1145/3102254.3102288"
    },
    {
        "title": "Interactive Visual Graph Mining and Learning",
        "authors": [
            "Ryan A. Rossi",
            "Nesreen K. Ahmed",
            "Rong Zhou",
            "Hoda Eldardiry"
        ],
        "date": null,
        "source": "ACM Transactions on Intelligent Systems and Technology",
        "abstract": "This article presents a platform for interactive graph mining and relational machine learning called GraphVis. The platform combines interactive visual representations with state-of-the-art graph mining and relational machine learning techniques to aid in revealing important insights quickly as well as learning an appropriate and highly predictive model for a particular task (e.g., classification, link prediction, discovering the roles of nodes, and finding influential nodes). Visual representations and interaction techniques and tools are developed for simple, fast, and intuitive real-time interactive exploration, mining, and modeling of graph data. In particular, we propose techniques for interactive relational learning (e.g., node/link classification), interactive link prediction and weighting, role discovery and community detection, higher-order network analysis (via graphlets, network motifs), among others. GraphVis also allows for the refinement and tuning of graph mining and relational learning methods for specific application domains and constraints via an end-to-end interactive visual analytic pipeline that learns, infers, and provides rapid interactive visualization with immediate feedback at each change/prediction in real-time. Other key aspects include interactive filtering, querying, ranking, manipulating, exporting, as well as tools for dynamic network analysis and visualization, interactive graph generators (including new block model approaches), and a variety of multi-level network analysis techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3200764"
    },
    {
        "title": "Data Mining and Opinion Mining: A Tool in Educational Context",
        "authors": [
            "Myriam Peñafiel",
            "Stefanie Vásquez",
            "Diego Vásquez",
            "Juan Zaldumbide",
            "Sergio Luján-Mora"
        ],
        "date": "July 2018",
        "source": "ICoMS '18: Proceedings of the 2018 1st International Conference on Mathematics and Statistics",
        "abstract": "The use of the web as a universal communication platform generates large volumes of data (Big data), which in many cases, need to be processed so that they can become useful knowledge in face of the sceptics who have doubts about the credibility of such information. The use of web data that comes from educational contexts needs to be addressed, since that large amount of unstructured information is not being valued, losing valuable information that can be used. To solve this problem, we propose the use of data mining techniques such as sentiment analysis to validate the information that comes from the educational platforms. The objective of this research is to propose a methodology that allows the user to apply sentiment analysis in a simple way, because although some researchers have done it, very few do with data in the educational context. The results obtained prove that the proposal can be used in similar cases.",
        "link": "https://dl.acm.org/doi/10.1145/3274250.3274263"
    },
    {
        "title": "Research Hotspots Mining and Visualized Analysis Based on Linking Cluster and K-Core Decomposition",
        "authors": [
            "Ying Cai",
            "Fang Huang",
            "Shuai Wang",
            "Hao Zhang",
            "Chunxiu Du"
        ],
        "date": "May 2018",
        "source": "ICDPA 2018: Proceedings of the International Conference on Data Processing and Applications",
        "abstract": "In this paper, we focus on research hotspots mining with knowledge semantic features on discipline topic word networks, which are extracted from scientific and technical documents. For the topic networks, a novel method of combining linking community clustering with k-core decomposition is proposed to mine the hotspot community with knowledge hierarchy structure. And as a post-processing, the hierarchical community identification is implemented by k-core decomposition in order to discover the internal connection with knowledge semantic hierarchy in the research hotspot. Finally, the community density and visualization method is utilized to analyze the core and sub- problem in the research hotspot community. In experiments, the topic networks for a single discipline and interdisciplinary research hotspot are taken as examples of analysis, and the results show the proposed approach can effectively realize the objective of mining and analyzing the discipline research hotspot on the academic topic word network.",
        "link": "https://dl.acm.org/doi/10.1145/3224207.3224216"
    },
    {
        "title": "The Application of Data Mining Algorithms in the Construction of Travel Recommendation System",
        "authors": [
            "Chao Lou",
            "Zhaonan Mu",
            "Mengzhu Liu"
        ],
        "date": "January 2021",
        "source": "CONF-CDS 2021: The 2nd International Conference on Computing and Data Science",
        "abstract": "Photos shared by users of online social network often associated with tags, texts, geographic and time information. These data are ideal research resource which can be used by researchers to observe people's interests and behaviors. They can also be used to analyze people's travel behaviors and interests because it can reflect people's travel activities and experiences directly. This project aimed to consolidate people into groups based on their travel preferences of travel season, travel time span, scenery type of travel location and then analyze popular travel places for different people groups. The travel preferences were analyzed from extracted data applying PHP language and Naive Bayes classification approach. According to the analyzed data, people were divided into groups. Then, DBSCAN clustering was used to get popular locations based on the different groups. As a result, 80 thousand photo data of 5 cities have been collected from Flickr by Phython. It has been showed that people's preference of season in most of the mined cities was spring and summer and the preference of time span was short. Moreover, mined people has been divided into 16 groups according to their preference and based on the groups popular locations have been calculated by DBSCAN clustering. Theses locations have been provided in a recommendation website which was built in this project. For the future work, mining Flickr data associated with other travel social network sites could be a good way to build a more effective recommendation system.",
        "link": "https://dl.acm.org/doi/10.1145/3448734.3450477"
    },
    {
        "title": "Compare Machine Learning Models in Text Classification Using Steam User Reviews",
        "authors": [
            "Youchen Miao",
            "Zeyu Jin",
            "Yumeng Zhang",
            "Yuchen Chen",
            "Junren Lai"
        ],
        "date": "November 2021",
        "source": "ICSED '21: Proceedings of the 2021 3rd International Conference on Software Engineering and Development",
        "abstract": "Text Classification and Sentiment Analysis of game reviews are viewed as important parts in not only academic fields but also in game studies. In this paper, with more than 400 thousand game reviews on Steam platform, we preprocess the data using different libraries (sklearn, nltk, and spaCy) and use them as inputs to build three sentiment classification models based on different algorithms (Naive Bayes, SVM, and Random Forest). In contrast to previous studies that only focus on different sentiment analysis models, our paper also highlights the use of different APIs to preprocess the data and their corresponding model performance. The results show that no matter which API we choose, Random Forest models always perform the best. However, in terms of training time, Naive Bayes is the fastest. This work can be used to apply grid search for researchers to automatically find the optimum API before conducting sentiment analysis in the future.",
        "link": "https://dl.acm.org/doi/10.1145/3507473.3507480"
    },
    {
        "title": "Lost in Transduction: Transductive Transfer Learning in Text Classification",
        "authors": [
            "Alejandro Moreo",
            "Andrea Esuli",
            "Fabrizio Sebastiani"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Obtaining high-quality labelled data for training a classifier in a new application domain is often costly. Transfer Learning\n(a.k.a. “Inductive Transfer”) tries to alleviate these costs by transferring, to the “target” domain of interest, knowledge available from a different “source” domain. In transfer learning the lack of labelled information from the target domain is compensated by the availability at training time of a set of unlabelled examples from the target distribution. Transductive Transfer Learning denotes the transfer learning setting in which the only set of target documents that we are interested in classifying is known and available at training time. Although this definition is indeed in line with Vapnik’s original definition of “transduction”, current terminology in the field is confused. In this article, we discuss how the term “transduction” has been misused in the transfer learning literature, and propose a clarification consistent with the original characterization of this term given by Vapnik. We go on to observe that the above terminology misuse has brought about misleading experimental comparisons, with inductive transfer learning methods that have been incorrectly compared with transductive transfer learning methods. We then, give empirical evidence that the difference in performance between the inductive version and the transductive version of a transfer learning method can indeed be statistically significant (i.e., that knowing at training time the only data one needs to classify indeed gives an advantage). Our clarification allows a reassessment of the field, and of the relative merits of the major, state-of-the-art algorithms for transfer learning in text classification.",
        "link": "https://dl.acm.org/doi/10.1145/3453146"
    },
    {
        "title": "Machine Learning on Mining Potential Adverse Drug Reactions for Pharmacovigilance",
        "authors": [
            "Ke Jia"
        ],
        "date": "July 2021",
        "source": "DSIT 2021: 2021 4th International Conference on Data Science and Information Technology",
        "abstract": "Drugs are related to the lives and health of patients. Mining and discovering potential adverse drug reactions from a large number of unstructured texts play an important role in drug research and pharmacovigilance. It is a hot research issue in the field of biomedicine in recent years. A large number of scholars have discovered a large amount of information on adverse drug reactions from different data sources through effective machine learning methods. This paper reviews the research progress in recent years to provide a reference for future research.",
        "link": "https://dl.acm.org/doi/10.1145/3478905.3478964"
    },
    {
        "title": "TACAM: Topic And Context Aware Argument Mining",
        "authors": [
            "Michael Fromm",
            "Evgeniy Faerman",
            "Thomas Seidl"
        ],
        "date": "October 2019",
        "source": "WI '19: IEEE/WIC/ACM International Conference on Web Intelligence",
        "abstract": "In this work we address the problem of argument search. The purpose of argument search is the distillation of pro and contra arguments for requested topics from large text corpora. In previous works, the usual approach is to use a standard search engine to extract text parts which are relevant to the given topic and subsequently use an argument recognition algorithm to select arguments from them. The main challenge in the argument recognition task, which is also known as argument mining, is that often sentences containing arguments are structurally similar to purely informative sentences without any stance about the topic. In fact, they only differ semantically. Most approaches use topic or search term information only for the first search step and therefore assume that arguments can be classified independently of a topic. We argue that topic information is crucial for argument mining, since the topic defines the semantic context of an argument. Precisely, we propose different models for the classification of arguments, which take information about a topic of an argument into account. Moreover, to enrich the context of a topic and to let models understand the context of the potential argument better, we integrate information from different external sources such as Knowledge Graphs or pre-trained NLP models. Our evaluation shows that considering topic information, especially in connection with external information, provides a significant performance boost for the argument mining task.",
        "link": "https://dl.acm.org/doi/10.1145/3350546.3352506"
    },
    {
        "title": "Text mining for incoming tasks based on the urgency/importance factors and task classification using machine learning tools",
        "authors": [
            "Yasser Ali Alshehri"
        ],
        "date": "March 2020",
        "source": "ICCDA 2020: Proceedings of the 2020 the 4th International Conference on Compute and Data Analysis",
        "abstract": "In workplaces, there is a massive amount of unstructured data from different sources. In this paper, we present a case study that explains how can through communications between employees, we can help to prioritize tasks requests to increase the efficiency of their works for both technical and non-technical workers. This involves managing daily incoming tasks based on their level of urgency and importance.To allow all workers to utilize the urgency-importance matrix as a time-management tool, we need to automate this tool. The textual content of incoming tasks are analyzed, and metrics related to urgency and importance are extracted. A third factor (i.e., the response variable) is defined based on the two input variables (urgency and importance). Then, machine learning applied to the data to predict the class of incoming tasks based on data outcome desired. We used ordinal regression, neural networks, and decision tree algorithms to predict the four levels of task priority. We measure the performance of all using recalls, precisions, and F-scores. All classifiers perform higher than 89% in terms of all measures.",
        "link": "https://dl.acm.org/doi/10.1145/3388142.3388153"
    },
    {
        "title": "Text mining for malware classification using multivariate all repeated patterns detection",
        "authors": [
            "Konstantinos F. Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2019",
        "source": "ASONAM '19: Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Mobile phones have become nowadays a commodity to the majority of people. Using them, people are able to access the world of Internet and connect with their friends, their colleagues at work or even unknown people with common interests. This proliferation of the mobile devices has also been seen as an opportunity for the cyber criminals to deceive smartphone users and steel their money directly or indirectly, respectively, by accessing their bank accounts through the smartphones or by blackmailing them or selling their private data such as photos, credit card data, etc. to third parties. This is usually achieved by installing malware to smartphones masking their malevolent payload as a legitimate application and advertise it to the users with the hope that mobile users will install it in their devices. Thus, any existing application can easily be modified by integrating a malware and then presented it as a legitimate one. In response to this, scientists have proposed a number of malware detection and classification methods using a variety of techniques. Even though, several of them achieve relatively high precision in malware classification, there is still space for improvement. In this paper, we propose a text mining all repeated pattern detection method which uses the decompiled files of an application in order to classify a suspicious application into one of the known malware families. Based on the experimental results using a real malware dataset, the methodology tries to correctly classify (without any misclassification) all randomly selected malware applications of 3 categories with 3 different families each.",
        "link": "https://dl.acm.org/doi/10.1145/3341161.3350841"
    },
    {
        "title": "Comparative Study between Traditional Machine Learning and Deep Learning Approaches for Text Classification",
        "authors": [
            "Cannannore Nidhi Kamath",
            "Syed Saqib Bukhari",
            "Andreas Dengel"
        ],
        "date": "August 2018",
        "source": "DocEng '18: Proceedings of the ACM Symposium on Document Engineering 2018",
        "abstract": "In this contemporaneous world, it is an obligation for any organization working with documents to end up with the insipid task of classifying truckload of documents, which is the nascent stage of venturing into the realm of information retrieval and data mining. But classification of such humongous documents into multiple classes, calls for a lot of time and labor. Hence a system which could classify these documents with acceptable accuracy would be of an unfathomable help in document engineering. We have created multiple classifiers for document classification and compared their accuracy on raw and processed data. We have garnered data used in a corporate organization as well as publicly available data for comparison. Data is processed by removing the stop-words and stemming is implemented to produce root words. Multiple traditional machine learning techniques like Naive Bayes, Logistic Regression, Support Vector Machine, Random forest Classifier and Multi-Layer Perceptron are used for classification of documents. Classifiers are applied on raw and processed data separately and their accuracy is noted. Along with this, Deep learning technique such as Convolution Neural Network is also used to classify the data and its accuracy is compared with that of traditional machine learning techniques. We are also exploring hierarchical classifiers for classification of classes and subclasses. The system classifies the data faster and with better accuracy than if done manually. The results are discussed in the results and evaluation section.",
        "link": "https://dl.acm.org/doi/10.1145/3209280.3209526"
    },
    {
        "title": "Identification of Overpricing in the Purchase of Medication by the Federal Government of Brazil, Using Text Mining and Clustering Based on Ontology",
        "authors": [
            "Marco Aurelio O. S. Correa",
            "Adriano Galindo Leal"
        ],
        "date": "August 2018",
        "source": "ICCBDC '18: Proceedings of the 2018 2nd International Conference on Cloud and Big Data Computing",
        "abstract": "Increasing the transparency level in his actions and spending is one of the primary duties of the Brazilian Federal Government. The creation of laws that oblige full disclose of all its expenditures through transparency portals enables citizens to supervise all government entities. However, only the dissemination of these data, without a definite standard or the availability of data analysis tools, does not guarantee that the citizen is empowered to play his role. Therefore hence, the objective of this work is to identify overprice in the acquisition of products purchased by the federal government of Brazil using the unstructured data available on the Transparency Portal. The last two-years' worth of purchasing data, available in the Transparency Portal, were extracted, processed and stored. Due to his diverse nature and high volume of data, this study focused only on medicines purchased by the Ministry of Health. Ontology-based text mining and clustering techniques were applied for automatic identification and classification of products. The processing of this information was done through text mining and clustering, based on the ontology registered in another database of the Brazilian government. Because of this work, a consolidated price base per medication was created to allow the identification of distortions in prices practised, facilitating the identification of cases that merit further investigation to unravel fraud to the treasury.",
        "link": "https://dl.acm.org/doi/10.1145/3264560.3264569"
    },
    {
        "title": "Computational Estimation by Scientific Data Mining with Classical Methods to Automate Learning Strategies of Scientists",
        "authors": [
            "Aparna S. Varde"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Experimental results are often plotted as 2-dimensional graphical plots (aka graphs) in scientific domains depicting dependent versus independent variables to aid visual analysis of processes. Repeatedly performing laboratory experiments consumes significant time and resources, motivating the need for computational estimation. The goals are to estimate the graph obtained in an experiment given its input conditions, and to estimate the conditions that would lead to a desired graph. Existing estimation approaches often do not meet accuracy and efficiency needs of targeted applications. We develop a computational estimation approach called AutoDomainMine that integrates clustering and classification over complex scientific data in a framework so as to automate classical learning methods of scientists. Knowledge discovered thereby from a database of existing experiments serves as the basis for estimation. Challenges include preserving domain semantics in clustering, finding matching strategies in classification, striking a good balance between elaboration and conciseness while displaying estimation results based on needs of targeted users, and deriving objective measures to capture subjective user interests. These and other challenges are addressed in this work. The AutoDomainMine approach is used to build a computational estimation system, rigorously evaluated with real data in Materials Science. Our evaluation confirms that AutoDomainMine provides desired accuracy and efficiency in computational estimation. It is extendable to other science and engineering domains as proved by adaptation of its sub-processes within fields such as Bioinformatics and Nanotechnology.",
        "link": "https://dl.acm.org/doi/10.1145/3502736"
    },
    {
        "title": "Where is the road for issue reports classification based on text mining?",
        "authors": [
            "Qiang Fan",
            "Yue Yu",
            "Gang Yin",
            "Tao Wang",
            "Huaimin Wang"
        ],
        "date": "November 2017",
        "source": "ESEM '17: Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Currently, open source projects receive various kinds of issues daily, because of the extreme openness of Issue Tracking System (ITS) in GitHub. ITS is a labor-intensive and time-consuming task of issue categorization for project managers. However, a contributor is only required a short textual abstract to report an issue in GitHub. Thus, most traditional classification approaches based on detailed and structured data (e.g., priority, severity, software version and so on) are difficult to adopt. In this paper, issue classification approaches on a large-scale dataset, including 80 popular projects and over 252,000 issue reports collected from GitHub, were investigated. First, four traditional text-based classification methods and their performances were discussed. Semantic perplexity (i.e., an issues description confuses bug-related sentences with nonbug-related sentences) is a crucial factor that affects the classification performances based on quantitative and qualitative study. Finally, A two-stage classifier framework based on the novel metrics of semantic perplexity of issue reports was designed. Results show that our two-stage classification can significantly improve issue classification performances.",
        "link": "https://dl.acm.org/doi/10.1109/ESEM.2017.19"
    },
    {
        "title": "Feature-based Facebook reviews process model for e-management using data mining",
        "authors": [
            "Anish Kumar Varudharajulu",
            "Yongsheng Ma"
        ],
        "date": "January 2019",
        "source": "IC4E '19: Proceedings of the 10th International Conference on E-Education, E-Business, E-Management and E-Learning",
        "abstract": "The data generated from online communication acts as potential gold mines for discovering knowledge for researchers. A large amount of data is also generated in the form of web documents, emails, blogs, and feedback, etc. Text analytics is being significantly employed to mine important information. Opinion mining is the process of extracting human thoughts and perceptions from unstructured texts. The showstopper for designing an opinion mining system for analyzing reviews arise from the fact that customer reviews are often noisy. These reviews are informally written. In addition, they are subjected to spelling mistakes, grammatical errors, improper punctuation and irrational capitalization. This paper focuses on analyzing the different classification and clustering algorithms aimed at extracting and consolidating opinions of customers from social media sites like Facebook, Twitter and through surveys, at multiple levels of granularity to monitor and measure customer satisfaction. Ours is an automated approach, in which the system aids in the process of knowledge assimilation for knowledge-based building and also performs the analytics. Domain experts ratify the knowledge base and also provide training datasets for the system to intuitively gather more instances for ratification. The system identifies opinion expressions as phrases containing opinion words, opinionated features and also opinion modifiers. These expressions are categorized as positive, negative or neutral. Opinion expressions are identified and categorized using localized linguistic techniques. Opinions can be congregated at any desired level of specificity i.e. feature level or product level, user level or service level, etc. We have developed a system based on this approach, which provides the user with a platform to analyze opinion expressions crawled from a set of pre-defined datasets.",
        "link": "https://dl.acm.org/doi/10.1145/3306500.3306514"
    },
    {
        "title": "Discriminative Topic Mining via Category-Name Guided Text Embedding",
        "authors": [
            "Yu Meng",
            "Jiaxin Huang",
            "Guangyuan Wang",
            "Zihan Wang",
            "Chao Zhang",
            "Yu Zhang",
            "Jiawei Han"
        ],
        "date": "April 2020",
        "source": "WWW '20: Proceedings of The Web Conference 2020",
        "abstract": "Mining a set of meaningful and distinctive topics automatically from massive text corpora has broad applications. Existing topic models, however, typically work in a purely unsupervised way, which often generate topics that do not fit users’ particular needs and yield suboptimal performance on downstream tasks. We propose a new task, discriminative topic mining, which leverages a set of user-provided category names to mine discriminative topics from text corpora. This new task not only helps a user understand clearly and distinctively the topics he/she is most interested in, but also benefits directly keyword-driven classification tasks. We develop CatE, a novel category-name guided text embedding method for discriminative topic mining, which effectively leverages minimal user guidance to learn a discriminative embedding space and discover category representative terms in an iterative manner. We conduct a comprehensive set of experiments to show that CatE mines high-quality set of topics guided by category names only, and benefits a variety of downstream applications including weakly-supervised classification and lexical entailment direction identification.",
        "link": "https://dl.acm.org/doi/10.1145/3366423.3380278"
    },
    {
        "title": "Comparative Study of Heart Disease Diagnosis Using Top Ten Data Mining Classification Algorithms",
        "authors": [
            "I. Ketut Agung Enriko"
        ],
        "date": "June 2019",
        "source": "ICFET '19: Proceedings of the 5th International Conference on Frontiers of Educational Technologies",
        "abstract": "Data mining has been used for many purposes, especially for prediction system. In healthcare, data mining algorithms often used in disease diagnosis. Meanwhile, heart disease is known as a primary cause of death over the years. Many studies have been performed in heart disease diagnosis using data mining methods. There are some popular data mining algorithms that can be used in heart disease diagnosis, for example, k-Nearest Neighbor, CART, and AdaBoost. The algorithms are used to analyze a sample of cardiovascular patients data and predict the heart disease type that they suffer. Some parameters are taken from the patient, including EKG morphology, blood pressure, and information about the existence of chest pain, shortness of breath, palpitation, and cold sweat. In this study, medical records data are collected from Harapan Kita Hospital and utilized as a dataset sample in this research. Top ten data mining classification algorithms are used in diagnosing heart disease from Harapan Kita Hospital data and examining their performance by checking the accuracy and speed.",
        "link": "https://dl.acm.org/doi/10.1145/3338188.3338220"
    },
    {
        "title": "Text Mining Approach for Identifying Research Trends",
        "authors": [
            "Snezhana Sulova"
        ],
        "date": "June 2021",
        "source": "CompSysTech '21: Proceedings of the 22nd International Conference on Computer Systems and Technologies",
        "abstract": "With the increase of unstructured data, the issues connected with automatic text processing, the categorization of documents and the discovery of topics have become objects of growing interest. In order to improve the process of grouping and processing research publications, we would like to propose a method based upon natural language processing. It is based on text mining technologies which aim to identify key tendencies in documents. It processes the content of publications by clustering and identifies the topics of each identified group. This analysis helps by identifying key tendencies as well as discovering emerging new areas of research. Publications from the research literature database, Scopus, were used to test the approach. The topic of the publications is “the application of digital technologies in the logistics business”. The experiments were completed using the RapidMiner Studio software.",
        "link": "https://dl.acm.org/doi/10.1145/3472410.3472433"
    },
    {
        "title": "Assessment of Vulnerability Severity using Text Mining",
        "authors": [
            "Georgios Spanos",
            "Lefteris Angelis",
            "Dimitrios Toloudis"
        ],
        "date": "September 2017",
        "source": "PCI '17: Proceedings of the 21st Pan-Hellenic Conference on Informatics",
        "abstract": "Software1 vulnerabilities are closely associated with information systems security, a major and critical field in today's technology. Vulnerabilities constitute a constant and increasing threat for various aspects of everyday life, especially for safety and economy, since the social impact from the problems that they cause is complicated and often unpredictable. Although there is an entire research branch in software engineering that deals with the identification and elimination of vulnerabilities, the growing complexity of software products and the variability of software production procedures are factors contributing to the ongoing occurrence of vulnerabilities, Hence, another area that is being developed in parallel focuses on the study and management of the vulnerabilities that have already been reported and registered in databases. The information contained in such databases includes, a textual description and a number of metrics related to vulnerabilities. The purpose of this paper is to investigate to what extend the assessment of the vulnerability severity can be inferred directly from the corresponding textual description, or in other words, to examine the informative power of the description with respect to the vulnerability severity. For this purpose, text mining techniques, i.e. text analysis and three different classification methods (decision trees, neural networks and support vector machines) were employed. The application of text mining to a sample of 70,678 vulnerabilities from a public data source shows that the description itself is a reliable and highly accurate source of information for vulnerability prioritization.",
        "link": "https://dl.acm.org/doi/10.1145/3139367.3139390"
    },
    {
        "title": "Clustering Algorithms for Spatial Data Mining",
        "authors": [
            "Chetashri Bhadane",
            "Ketan Shah"
        ],
        "date": "April 2020",
        "source": "ICGDA '20: Proceedings of the 2020 3rd International Conference on Geoinformatics and Data Analysis",
        "abstract": "With the advances in mobile and wireless technologies, there has been a rise in applications that track and share the users' geospatial data. People use several social networking sites such as Twitter, Facebook and Flickr, where they share their status updates. With the integration of Global Positioning System (GPS) with mobile phones, it is now possible to share one's locations on these social networks. GPS allows us to record and track a person's movement along with the timestamp. The data set obtained from these GPS logs is vast and is widely used to analyze the users' movement patterns. Specifically, we can find out significant locations based on the number of users present at that location and the time spent by them at such places. Once significant places have been identified, it is also possible to identify the semantic importance of these locations. This paper presents an overview of the clustering techniques used to find important places of interest using large GPS based mobility datasets. Four clustering algorithms, K-Means, DBSCAN, OPTICS and Hierarchical, are implemented, and performance is tested using real-time data of 50 users collected over 2--5 years. Performance summary depicts that K-Means and DBSCAN perform well for spatial data.",
        "link": "https://dl.acm.org/doi/10.1145/3397056.3397068"
    },
    {
        "title": "Feature Selection based Arabic Text Classification using Different Machine Learning Algorithms: Comparative Study",
        "authors": [
            "Sakina Rim Bennabi",
            "Zakaria Elberrichi"
        ],
        "date": "June 2020",
        "source": "ICIST '20: Proceedings of the 10th International Conference on Information Systems and Technologies",
        "abstract": "Feature selection is a method of data pre-processing widely used when mining large data, such as textual classification. Several studies have been conducted to compare the different methods of feature selection applied to corpora in English. Unfortunately, a small number of works concern the Arabic language. This article aims to present a comparative study of different feature selection techniques including: Chi2, the ANOVA method and mutual information, applied on a corpus in Arabic language, while also diversifying the machine learning algorithms (Naive Bayes, SVM and KNN). This experimental study has shown in general that reducing dimensionality with feature selection techniques has slightly affected the performance of textual classification, reducing the size of the corpus by up to 1%.",
        "link": "https://dl.acm.org/doi/10.1145/3447568.3448531"
    },
    {
        "title": "Intelligent Data Analysis using Optimized Support Vector Machine Based Data Mining Approach for Tourism Industry",
        "authors": [
            "Ms Promila Sharma",
            "Uma Meena",
            "Girish Kumar Sharma"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Data analysis involves the deployment of sophisticated approaches from data mining methods, information theory, and artificial intelligence in various fields like tourism, hospitality, and so on for the extraction of knowledge from the gathered and preprocessed data. In tourism, pattern analysis or data analysis using classification is significant for finding the patterns that represent new and potentially useful information or knowledge about the destination and other data. Several data mining techniques are introduced for the classification of data or patterns. However, overfitting, less accuracy, local minima, sensitive to noise are the drawbacks in some existing data mining classification methods. To overcome these challenges, Support vector machine with Red deer optimization (SVM-RDO) based data mining strategy is proposed in this article. Extended Kalman filter (EKF) is utilized in the first phase, i.e., data cleaning to remove the noise and missing values from the input data. Mantaray foraging algorithm (MaFA) is used in the data selection phase, in which the significant data are selected for the further process to reduce the computational complexity. The final phase is the classification, in which SVM-RDO is proposed to access the useful pattern from the selected data. PYTHON is the implementation tool used for the experiment of the proposed model. The experimental analysis is done to show the efficacy of the proposed work. From the experimental results, the proposed SVM-RDO achieved better accuracy, precision, recall, and F1 score than the existing methods for the tourism dataset. Thus, it is showed the effectiveness of the proposed SVM-RDO for pattern analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3494566"
    },
    {
        "title": "Applying text mining to predict learners' cognitive engagement",
        "authors": [
            "Hayati Hind",
            "Mohammed Khalidi Idrissi",
            "Samir Bennani"
        ],
        "date": "October 2017",
        "source": "SCAMS '17: Proceedings of the Mediterranean Symposium on Smart City Application",
        "abstract": "In academic process, engagement represent one of the important success' factor. This paper proposes a predictive system for learners' cognitive engagement based on their online discussion forums participation. In the first level the ontology OWL and the LSA method are used to perform a semantic classification system of the threads according to a specific context chosen by the tutor. Then the Text mining, as a predictive method, is applied to the classified threads and learners' participation in the forums.",
        "link": "https://dl.acm.org/doi/10.1145/3175628.3175655"
    },
    {
        "title": "Investigate Transitions into Drug Addiction through Text Mining of Reddit Data",
        "authors": [
            "John Lu",
            "Sumati Sridhar",
            "Ritika Pandey",
            "Mohammad Al Hasan",
            "Georege Mohler"
        ],
        "date": "July 2019",
        "source": "KDD '19: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Increasing rates of opioid drug abuse and heightened prevalence of online support communities underscore the necessity of employing data mining techniques to better understand drug addiction using these rapidly developing online resources. In this work, we obtained data from Reddit, an online collection of forums, to gather insight into drug use/misuse using text snippets from users narratives. Specifically, using users' posts, we trained a binary classifier which predicts a user's transitions from casual drug discussion forums to drug recovery forums. We also proposed a Cox regression model that outputs likelihoods of such transitions. In doing so, we found that utterances of select drugs and certain linguistic features contained in one's posts can help predict these transitions. Using unfiltered drug-related posts, our research delineates drugs that are associated with higher rates of transitions from recreational drug discussion to support/recovery discussion, offers insight into modern drug culture, and provides tools with potential applications in combating the opioid crisis.",
        "link": "https://dl.acm.org/doi/10.1145/3292500.3330737"
    },
    {
        "title": "Analysing digital banking reviews using text mining",
        "authors": [
            "Li Chen Cheng",
            "Legaspi Rhea Sharmayne"
        ],
        "date": "December 2020",
        "source": "ASONAM '20: Proceedings of the 12th IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Digital banks are new entrants in the banking industry in the Philippines as they only started late 2018. Since then, a handful of players have and are still emerging. With more and more people becoming technologically savvy, it is very critical for financial institutions to develop a digital banking application that will stand out from the competition. This paper aims to use text mining methods to analyse digital banking application reviews. This study will perform topic modelling using LDA to explore customer concerns and will mine association rules between the digital banking features with the review score. The results will reveal which areas the digital banking application can further optimize for customer satisfaction and retention.",
        "link": "https://dl.acm.org/doi/10.1109/ASONAM49781.2020.9381429"
    },
    {
        "title": "Mining Moocs Videos Metadata Using Classification Techniques",
        "authors": [
            "El Harrak Othman",
            "Ghadi Abderrahim",
            "El Bouhdidi Jaber"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "In few years, the internet has become the first source of information for most people, Today MOOCs makes it possible for everyone to access to the education over the world, it's represents an emerging methodology of online teaching and an important development in open education. Due to the rapid development of Moocs and the rapid growth of digital data and video database over the Internet, it is becoming very difficult for learners to browsing and choosing the best training for them. The scientific community has increased the amount of research into new technologies, with a view to improve the digital video utilization: its archiving, indexing, accessibility, acquisition, store and even its process and usability. All these parts of the video utilization entail the necessity of the extraction of all important information of a video, especially in cases of lack of metadata information. Web video mining is retrieving the content using data mining techniques from World Wide Web. There are two approaches for web video mining using traditional image processing (signal processing) and metadata based approach. In this paper, we present the various research makes in this subject and we propose an effective methodology to extract the metadata from Moocs videos and classify them based on the extracted metadata by applying data mining techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090450"
    },
    {
        "title": "Text mining with HathiTrust",
        "authors": [
            "Eleanor Dickson Koehl",
            "Ryan Dubnicek"
        ],
        "date": "June 2019",
        "source": "JCDL '19: Proceedings of the 18th Joint Conference on Digital Libraries",
        "abstract": "This tutorial will introduce attendees to the HathiTrust Research Center and its tools and services for computational text analysis research. HTRC leverages the scope and scale of the HathiTrust Digital Library collection to create opportunities for researchers to perform text data mining on subsets of the corpus. Attendees will develop skills that will allow them to conduct basic text analysis research using HathiTrust data, learn techniques for engaging with scholars on their text data mining research, and explore strategies for computational access to large-scale digital collections.",
        "link": "https://dl.acm.org/doi/10.1109/JCDL.2019.00115"
    },
    {
        "title": "Text mining for plagiarism detection: multivariate pattern detection for recognition of text similarities",
        "authors": [
            "Konstantinos Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2018",
        "source": "ASONAM '18: Proceedings of the 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "The problem of plagiarism the recent years has been intensified by the availability of information in digital form and the accessibility of the electronic libraries through the Internet. As a result, plagiarism detection has been transformed into a big data analytics problem since the number of digital sources is extravagant and a new document needs to be compared with millions of other existing documents. In this paper, a text mining methodology is proposed that can detect all common patterns between a document and the documents in a reference database. The technique is based on a pattern detection algorithm and the corresponding data structure that enables the algorithm to detect all common patterns. The methodology has been applied in a well-defined dataset providing very promising results identifying difficult cases of plagiarism such as technical disguise.",
        "link": "https://dl.acm.org/doi/10.5555/3382225.3382424"
    },
    {
        "title": "Petroleum Engineering Data Text Classification Using Convolutional Neural Network Based Classifier",
        "authors": [
            "Tzuhan Hsu",
            "Yaoqin Zhang"
        ],
        "date": "May 2018",
        "source": "ICMLT '18: Proceedings of the 2018 International Conference on Machine Learning Technologies",
        "abstract": "In this paper, authors introduce a convolutional neural network (CNN) based classification approach for petroleum engineering event. Recently, text classification problem has raised substantial attention in the field of machine learning. The aim of text classification process is to assign correct label to a text on the basis of its content. However, due to the imprecise of feature representation, most of former text classification methods cannot export satisfying result. To solve this problem, authors propose a model based on convolutional neural network. Authors conduct experiments with real world petroleum engineering data text, artificially labelled into 6 categories. Beside purposed CNN-based model, the authors also implement other baseline machine learning approaches: k nearest neighbor, random forest, support vector machines, recurrent neural network, gated recurrent units, long short-term memory. Among all these models, our CNN-based model reach best performance in text classification task. In the last part of this paper, authors compare and analysis their purpose and baseline model.",
        "link": "https://dl.acm.org/doi/10.1145/3231884.3231898"
    },
    {
        "title": "Classifying Indonesian Online Articles as Advertisement Placement Base Using Text Mining",
        "authors": [
            "Nadhira Tasya",
            "Arian Dhini"
        ],
        "date": "July 2017",
        "source": "ICBIM 2017: Proceedings of the International Conference on Business and Information Management",
        "abstract": "Rapid development in technological aspect resulting in growing level of human needs for the latest news, so that emerged a new trend of publishing and accessing news through online media or usually called online journalism. In addition, the number of people who sell and purchase through online sites also continues to increase and this opportunity is utilized by the company and the advertiser by implementing targeted web advertising. However, the high number of articles that have been published and accessed leads to great opportunities for errors in determining where to place the ads. Therefore, it needs a system that can categorize articles accessed by users as the basis of advertisement placement by the company and this classification system can be done by applying the method of Data Mining and Text Mining. This research uses document data in the form of article content that will be categorized into twenty categories of class of advertisement by using Text Mining technique with Support Vector Machine algorithm. The results of this study may be used by companies or advertisers as a basis for placement of ads on selected online media sites.",
        "link": "https://dl.acm.org/doi/10.1145/3134271.3134288"
    },
    {
        "title": "Mining Free-Text Medical Notes for Suicide Risk Assessment",
        "authors": [
            "Marios Adamou",
            "Grigoris Antoniou",
            "Elissavet Greasidou",
            "Vincenzo Lagani",
            "Paulos Charonyktakis",
            "Ioannis Tsamardinos"
        ],
        "date": "July 2018",
        "source": "SETN '18: Proceedings of the 10th Hellenic Conference on Artificial Intelligence",
        "abstract": "Suicide has been considered as an important public health issue for a very long time, and is one of the main causes of death worldwide. Despite suicide prevention strategies being applied, the rate of suicide has not changed substantially over the past decades. Advances in machine learning make it possible to attempt to predict suicide based on the analysis of relevant data to inform clinical practice. This paper reports on findings from the analysis of data of patients who died by suicide in the period 2013-2016 and made use of both structured data and free-text medical notes. We focus on examining various text-mining approaches to support risk assessment. The results show that using advance machine learning and text-mining techniques, it is possible to predict within a specified period which people are most at risk of taking their own life at the time of referral to a mental health service.",
        "link": "https://dl.acm.org/doi/10.1145/3200947.3201020"
    },
    {
        "title": "Integration of data mining techniques in e-learning systems: Clustering Profil of Lerners and Recommender Course System",
        "authors": [
            "A. El Moustamid",
            "E. En-Naimi",
            "J. El Bouhdidi"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "Data mining aims at the extraction of knowledge from large quantities of data by automatic or semi-automatic methods. In the field of education and training e-learning, Data Mining techniques as clustering (The aim of this technique is to identify groups that are similar in some aspect) [21], prediction (is used for developing model where combination of data viewed from different aspects can effect single viewed data. Here, is important to watch how predictors variables are effecting predicted variable)[21], classification are applied in e-learning to improve the level of learners by providing learning according to the learners' requirement) are used to analyze learners profile, to predict and improve student performance. The aim of our project is to develop a system capable of analyzing learners profile and indexing web videos in order to offer learners a rich database with courses that correspond to their levels.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090453"
    },
    {
        "title": "Multi-level mining and visualization of scientific text collections: Exploring a bi-lingual scientific repository",
        "authors": [
            "Pablo Accuosto",
            "Francesco Ronzano",
            "Daniel Ferrés",
            "Horacio Saggion"
        ],
        "date": "December 2017",
        "source": "WOSP 2017: Proceedings of the 6th International Workshop on Mining Scientific Publications",
        "abstract": "We present a system to mine and visualize collections of scientific documents by semantically browsing information extracted from single publications or aggregated throughout corpora of articles. The text mining tool performs deep analysis of document collections allowing the extraction and interpretation of research paper's contents. In addition to the extraction and enrichment of documents with metadata (titles, authors, affiliations, etc), the deep analysis performed comprises semantic interpretation, rhetorical analysis of sentences, triple-based information extraction, and text summarization. The visualization components allow geographical-based exploration of collections, topic-evolution interpretation, and collaborative network analysis among others. The paper presents a case study of a bi-lingual collection in the field of Natural Language Processing (NLP).",
        "link": "https://dl.acm.org/doi/10.1145/3127526.3127529"
    },
    {
        "title": "Hierarchical Topic Mining via Joint Spherical Tree and Text Embedding",
        "authors": [
            "Yu Meng",
            "Yunyi Zhang",
            "Jiaxin Huang",
            "Yu Zhang",
            "Chao Zhang",
            "Jiawei Han"
        ],
        "date": "August 2020",
        "source": "KDD '20: Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Mining a set of meaningful topics organized into a hierarchy is intuitively appealing since topic correlations are ubiquitous in massive text corpora. To account for potential hierarchical topic structures, hierarchical topic models generalize flat topic models by incorporating latent topic hierarchies into their generative modeling process. However, due to their purely unsupervised nature, the learned topic hierarchy often deviates from users' particular needs or interests. To guide the hierarchical topic discovery process with minimal user supervision, we propose a new task, Hierarchical Topic Mining, which takes a category tree described by category names only, and aims to mine a set of representative terms for each category from a text corpus to help a user comprehend his/her interested topics. We develop a novel joint tree and text embedding method along with a principled optimization procedure that allows simultaneous modeling of the category tree structure and the corpus generative process in the spherical space for effective category-representative term discovery. Our comprehensive experiments show that our model, named JoSH, mines a high-quality set of hierarchical topics with high efficiency and benefits weakly-supervised hierarchical text classification tasks.",
        "link": "https://dl.acm.org/doi/10.1145/3394486.3403242"
    },
    {
        "title": "Unsupervised behavioural mining and clustering for malware family identification",
        "authors": [
            "Khanh Huu The Dam",
            "Thomas Given-Wilson",
            "Axel Legay"
        ],
        "date": "March 2021",
        "source": "SAC '21: Proceedings of the 36th Annual ACM Symposium on Applied Computing",
        "abstract": "More accurate and advanced detection and classification of malware requires exploiting program behaviour and not merely syntactic or static features. One approach is to use system call dependency graphs (SCDGs) that represent the program behaviour by interactions with the system, and the relations between these interactions. These SCDGs have been used with supervised learning techniques to very accurately detect and classify malware. This works considers the unsupervised learning challenge of mining for common clusters of behaviour without a priori knowledge. This allows for clustering of similar programs by behaviour, that can then be used for either classification or further analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3412841.3441919"
    },
    {
        "title": "Text Mining for Evaluating Authors' Birth and Death Years",
        "authors": [
            "Dror Moghaz",
            "Yaakov Hacohen-Kerner",
            "Dov Gabbay"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "This article presents a unique method in text and data mining for finding the era, i.e., mining temporal data, in which an anonymous author was living. Finding this era can assist in the examination of a fake document or extracting the time period in which a writer lived. The study and the experiments concern Hebrew, and in some parts, Aramaic and Yiddish rabbinic texts. The rabbinic texts are undated and contain no bibliographic sections, posing an interesting challenge. This work proposes algorithms using key phrases and key words that allow the temporal organization of citations together with linguistic patterns. Based on these key phrases, key words, and the references, we established several types of “Iron-clad,” Heuristic and Greedy rules for estimating the years of birth and death of a writer in an interesting classification task. Experiments were conducted on corpora, including documents authored by 12, 24, and 36 rabbinic writers and demonstrated promising results.",
        "link": "https://dl.acm.org/doi/10.1145/3281631"
    },
    {
        "title": "Transfer Learning to Timed Text Based Video Classification Using CNN",
        "authors": [
            "Zenun Kastrati",
            "Ali Shariq Imran",
            "Arianit Kurti"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "Open educational video resources are gaining popularity with a growing number of massive open online courses (MOOCs). This has created a niche for content providers to adopt effective solutions in automatically organizing and structuring of educational resources for maximum visibility. Recent advances in deep learning techniques are proving useful in managing and classifying resources into appropriate categories. This paper proposes one such convolutional neural network (CNN) model for classifying video lectures in a MOOC setting using a transfer learning approach. The model uses a time-aligned text transcripts corresponding to video lectures from six broader subject categories. Video lectures and their corresponding transcript dataset is gathered from the Coursera MOOC platform. Two different CNN models are proposed: i) CNN based classification using embeddings learned from our MOOC dataset, ii) CNN based classification using transfer learning. Word embeddings generated from two well known state-of-the-art pre-trained models Word2Vec and GloVe, are used in the transfer learning approach for the second case. The proposed CNN models are evaluated using precision, recall, and F1 score and the obtained performance is compared with both conventional and deep learning classifiers. The proposed CNN models have an F1 score improvement of 10-22 percentage points over DNN and conventional classifiers",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326483"
    },
    {
        "title": "Evolutionary role mining in complex networks by ensemble clustering",
        "authors": [
            "Sarvenaz Choobdar",
            "Pedro Ribeiro",
            "Fernando Silva"
        ],
        "date": "April 2017",
        "source": "SAC '17: Proceedings of the Symposium on Applied Computing",
        "abstract": "The structural patterns in the neighborhood of nodes assign unique roles to the nodes. Mining the set of existing roles in a network provides a descriptive profile of the network and draws its general picture. This paper proposes a new method to determine structural roles in a dynamic network based on the current position of nodes and their historic behavior. We develop a temporal ensemble clustering technique to dynamically find groups of nodes, holding similar tempo-structural roles. We compare two weighting functions, based on age and distribution of data, to incorporate temporal behavior of nodes in the role discovery. To evaluate the performance of the proposed method, we assess the results from two points of view: 1) goodness of fit to current structure of the network; 2) consistency with historic data. We conduct the evaluation using different ensemble clustering techniques. The results on real world networks demonstrate that our method can detect tempo-structural roles that simultaneously depict the topology of a network and reflect its dynamics with high accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3019612.3019815"
    },
    {
        "title": "Analyzing used-car web listings via text mining",
        "authors": [
            "Ayhan Demiriz",
            "Fatma Cantaş"
        ],
        "date": "October 2017",
        "source": "IML '17: Proceedings of the 1st International Conference on Internet of Things and Machine Learning",
        "abstract": "Used car trade is one of the major components of the world economies. It is not uncommon to sell a car by placing an internet advertisement irrespective of the geography in these days. A typical content of an advertisement is usually composed of two parts namely the structured and the free text data. The structured data may include some information about the asking price, make, model, year, mileage of the car and the contact info. In most cases, seller may give important clues about the car's current conditions in the free text data where the title (head) of the advertisement can be included as free text too. This paper reports preliminary results from a text mining study conducted on 75K used car internet listings collected from two major car listing web sites in Turkey. As expected, the words and the phrases related to the description of the car are observed to be frequent. The leading concepts in the free text are found to be regarding how to describe the current condition of a car, for example \"no crash history\".",
        "link": "https://dl.acm.org/doi/10.1145/3109761.3109782"
    },
    {
        "title": "Text classification of Diseases Treated by Traditional Chinese Medicine Prescription based on machine learning",
        "authors": [
            "Hanqing Zhao",
            "Bo Han",
            "Chen Li"
        ],
        "date": "September 2020",
        "source": "ISAIMS '20: Proceedings of the 1st International Symposium on Artificial Intelligence in Medical Sciences",
        "abstract": "OBJECTIVE: To explore the application of machine learning in th e identification of diseases treated by traditional Chinese medicine prescriptions. METHODS: Based on the composition of the text document of Chinese medicine prescriptions, the prescriptions were divided into cough, headache and diarrhea. THUTCT were introduced to establish and train two machine learning text classification models, LibSVM and LibLinear, and the prescriptions to be deter mined were put into the model for classification and prediction. R ESULTS: The Precision rate of LibSVM model and Liblinear mod el were 0.7283 and 0.6690. Seven prescriptions were classified an d predicted, and the results were in line with expectations. CONCLUSION: THUCTC has good universality for the content of TCM prescriptions, high classification accuracy and fast testing speed, which is suitable for the text classification and discrimination research of TCM prescriptions for diseases.",
        "link": "https://dl.acm.org/doi/10.1145/3429889.3429895"
    },
    {
        "title": "Evaluation of retweet clustering method classification method using retweets on Twitter without text data",
        "authors": [
            "K. Uchida",
            "F. Toriumi",
            "T. Sakaki"
        ],
        "date": "August 2017",
        "source": "WI '17: Proceedings of the International Conference on Web Intelligence",
        "abstract": "Burst phenomena, which frequently occur on social media, are caused by such social events as flaming on the internet, elections, and natural disasters. To understand people's thoughts and feelings, we must classify their opinions from burst phenomena. Therefore, classification methods that categorize tweets are critical. However, since most classification methods focus on text mining, they cannot group tweets by topics because each tweet has poor linguistic similarities. We used a non-text-based classification method proposed by Baba et al. that groups tweets by topics, even if they have poor linguistic similarities, and verified its validity by comparing it with a text-based classification method in two different evaluations: qualitative and quantitative. In the qualitative evaluation part, we did a questionnaire survey and validated the suitability of the topic clusters created using both the non-and text-based methods. Since evaluating the similarity of every pair of tweets in each topic is difficult, we evaluated the similarity between sampled pairs in the survey and acquired more appropriate topic clustering results using the non-text-based method than the text-based method. In the quantitative evaluation part, we focused on the robustness of each method against data reduction. Many approaches analyze social media data, especially because collecting data from social media is comparatively easy. However, since collecting the whole data of burst phenomena is very costly due to the vast amounts of available social media data, robustness against data reduction is an important index to evaluate classification methods. With the non-text-based method, over 55% of the pairs of tweets in the same cluster were also included in the same cluster even when the data were reduced to 10% in all three of our example cases. In this paper, as a source we focus on Twitter, one of the most popular microblogging services. Using clustering to conduct detailed case analyses, we scrutinized three burst cases that include natural disasters and flaming on the internet and found that a non-text-based method more effectively classified tweets in burst phenomena than a text-based method.",
        "link": "https://dl.acm.org/doi/10.1145/3106426.3106451"
    },
    {
        "title": "Using Spark for Text Mining on Large Scale Liver Cancer Literature",
        "authors": [
            "Ming-Yen Lin",
            "Yu-Ju Lin",
            "Sue-Chen Hsueh"
        ],
        "date": "January 2021",
        "source": "BDET 2021: 2021 the 3rd International Conference on Big Data Engineering and Technology (BDET)",
        "abstract": "Cancer is one of the main causes of death. The number of known cancer to-date is more than one hundred. Liver cancer, ranked after trachea and lung cancer, has long been high in the cancer leading cause in Taiwan and even ranked as second in 2016. The number of scientific articles related to cancer proliferates every year. The number reaches as high as 20 million in PubMed so that discovering useful information from the massive collection is very difficult. In addition, using a single machine to sift through these articles is very time-consuming. Therefore, we present a big data analytic framework using the distributed Apache Spark platform for text mining in PubMed literature. We establish a prediction model for liver cancer articles so that researchers may effectively validate whether an article is related to liver cancer or not. Classification models in Spark MLlib including Linear Support Vector Machines (SVM), Logistic Regression, are used in our experiments. Relevancy to liver cancer is further confirmed by using MeSH (Medical Subject Headings) terms. Logistic regression is about 3 times faster than SVMs and the accuracy of both methods is close to 95% in the experiments using hold-out validation. When max_features is 500 and min_df ≤ 0.1 (or min_df = 1), the accuracy may reach 96%. In the experiments with K-fold cross-validation, the accuracy of SVMs methods is 96%. The experimental results show that that our prediction model may effectively classify liver cancer articles.",
        "link": "https://dl.acm.org/doi/10.1145/3474944.3474958"
    },
    {
        "title": "Automatic topic classification of test cases using text mining at an Android smartphone vendor",
        "authors": [
            "Junji Shimagaki",
            "Yasutaka Kamei",
            "Naoyasu Ubayashi",
            "Abram Hindle"
        ],
        "date": "October 2018",
        "source": "ESEM '18: Proceedings of the 12th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Background: An Android smartphone is an ecosystem of applications, drivers, operating system components, and assets. The volume of the software is large and the number of test cases needed to cover the functionality of an Android system is substantial. Enormous effort has been already taken to properly quantify \"what features and apps were tested and verified?\". This insight is provided by dashboards that summarize test coverage and results per feature. One method to achieve this is to manually tag or label test cases with the topic or function they cover, much like function points. At the studied Android smartphone vendor, tests are labelled with manually defined tags, so-called \"feature labels (FLs)\", and the FLs serve to categorize 100s to 1000s test cases into 10 to 50 groups. Aim: Unfortunately for developers, manual assignment of FLs to 1000s of test cases is a time consuming task, leading to inaccurately labeled test cases, which will render the dashboard useless. We created an automated system that suggests tags/labels to the developers for their test cases rather than manual labeling. Method: We use machine learning models to predict and label the functionality tested by 10,000 test cases developed at the company. Results: Through the quantitative experiments, our models achieved acceptable F-1 performance of 0.3 to 0.88. Also through the qualitative studies with expert teams, we showed that the hierarchy and path of tests was a good predictor of a feature's label. Conclusions: We find that this method can reduce tedious manual effort that software developers spent classifying test cases, while providing more accurate classification results.",
        "link": "https://dl.acm.org/doi/10.1145/3239235.3268927"
    },
    {
        "title": "To apply Data Mining for Classification of Crowd sourced Software Requirements",
        "authors": [
            "Soonh Taj",
            "Qasim Arain",
            "Imran Memon",
            "Asma Zubedi"
        ],
        "date": "April 2019",
        "source": "ICSIE '19: Proceedings of the 8th International Conference on Software and Information Engineering",
        "abstract": "Now a day's main focus of developers is to build quality software that works according to customer needs and for this reason it is necessary to gather right requirements as requirement elicitation is the critical step that impacts on the success of software project as misinterpreted requirements leads to the failure of software project. By keeping this in mind a research is carried out on improving requirements elicitation process and automating the process of classifying requirements. In this research, a model is proposed which will help in this scenario for requirements elicitation and requirement classification. This paper presents a model in which crowd sourcing approach is used so that customers, end users, stakeholders, developers and software engineers can make active participation for requirement elicitation process and requirements gathered using crowdsourcing approach are used by model for classification process i.e. classification of requirements into functional and non-functional requirements. For the proof of proposed model a case study is conducted. Results of case study provided the usefulness and efficiency of proposed model for classification of crowd sourced software requirements.",
        "link": "https://dl.acm.org/doi/10.1145/3328833.3328837"
    },
    {
        "title": "Text classification using Fuzzy TF-IDF and Machine Learning Models",
        "authors": [
            "Mariem Bounabi",
            "Karim El Moutaouakil",
            "Khalid Satori"
        ],
        "date": "October 2019",
        "source": "BDIoT'19: Proceedings of the 4th International Conference on Big Data and Internet of Things",
        "abstract": "The representation of the information has an important impact on the text classification task. Several weighting methods were proposed in the literature, and the term frequency-inverse term frequency (TFIDF), the most know on the text treatment field. The FTF-IDF is a vector representation where the components of the TFIDF are presented as inputs to the Fuzzy Inference System (FIS). In this work, we compare several Machin Learning algorithms such as Naïve Bayes and its derivatives, SVM and Random forest classifiers, using the FTF-IDF representation. To improve the quality of the used classifiers, we call sum attribute selection methods. The recognition rate, for the tested systems, is satisfied, where the system based on naïve Bayes classifier, the FTF-IDF weighting terms, and the info gain select attributes method gives 98.7% as accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3372938.3372956"
    },
    {
        "title": "Text Mining in Cybersecurity: A Systematic Literature Review",
        "authors": [
            "Luciano Ignaczak",
            "Guilherme Goldschmidt",
            "Cristiano André Da Costa",
            "Rodrigo Da Rosa Righi"
        ],
        "date": null,
        "source": "ACM Computing Surveys",
        "abstract": "The growth of data volume has changed cybersecurity activities, demanding a higher level of automation. In this new cybersecurity landscape, text mining emerged as an alternative to improve the efficiency of the activities involving unstructured data. This article proposes a Systematic Literature Review (SLR) to present the application of text mining in the cybersecurity domain. Using a systematic protocol, we identified 2,196 studies, out of which 83 were summarized. As a contribution, we propose a taxonomy to demonstrate the different activities in the cybersecurity domain supported by text mining. We also detail the strategies evaluated in the application of text mining tasks and the use of neural networks to support activities involving unstructured data. The work also discusses text classification performance aiming its application in real-world solutions. The SLR also highlights open gaps for future research, such as the analysis of non-English content and the intensification in the usage of neural networks.",
        "link": "https://dl.acm.org/doi/10.1145/3462477"
    },
    {
        "title": "Using Text Mining to Analyze the Financial Patents",
        "authors": [
            "Yung-Feng Lu",
            "Jia-Lang Xu",
            "Mu-Yen Chen"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "With the advancement and development of science and technology, Nowadays society is the development of information technology, cloud computing, mobile Internet, Internet of Things, block chain and information technology.The combination of finance and technology has made financial technology terms gradually emerging.. This research analyzes the patents of financial technology. However, Taiwan's financial technology patents are mostly G06Q-20 (payment plan, architecture or agreement), G06Q-30 (commercial, such as marketing, shopping, payment, auction or e-commerce). G06Q-40 (financial, investment or tax treatment, insurance).The research use text mining technology to analyze and understand Taiwan's trends and developments in financial technology. The research results show that G06Q-20 patents are biased towards information security and mobile payment. G06Q-30 patents favor shopping websites and advertising. G06Q-40 patents are biased towards diversity, such as securities, insurance, finance, and stocks.",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326478"
    },
    {
        "title": "Identifying Online Profiles of Distance Learning Students Using Data Mining Techniques",
        "authors": [
            "Osama Islam",
            "Muazzam Siddiqui",
            "Naif Radi Aljohani"
        ],
        "date": "October 2019",
        "source": "ICDTE '19: Proceedings of the 3rd International Conference on Digital Technology in Education",
        "abstract": "Educational data has grown over the years with the increased use of technology within educational environments. This has led to a huge amount of data being stored in various data sources representing the student, his/her activities, and other aspects relevant to the learning process. To meet this analytical need, Educational Data Mining (EDM) has emerged to assist educational institutions in identifying key benefits such as students at risk, the level of student engagement or predicting student performance. The aim of this research was to explore the various aspects of student interaction data using data mining techniques to identify relevant patterns of behaviors and possible key attributes that have higher degrees of influence on distance learning students. The main findings identified several patterns of user online profiles based on a set of adopted learning strategies, the research proposed a framework for analyzing such interaction data based on R and Hadoop platforms to correlate online profiles with student performance.",
        "link": "https://dl.acm.org/doi/10.1145/3369199.3369249"
    },
    {
        "title": "Extremely Fast Decision Tree Mining for Evolving Data Streams",
        "authors": [
            "Albert Bifet",
            "Jiajin Zhang",
            "Wei Fan",
            "Cheng He",
            "Jianfeng Zhang",
            "Jianfeng Qian",
            "Geoff Holmes",
            "Bernhard Pfahringer"
        ],
        "date": "August 2017",
        "source": "KDD '17: Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
        "abstract": "Nowadays real-time industrial applications are generating a huge amount of data continuously every day. To process these large data streams, we need fast and efficient methodologies and systems. A useful feature desired for data scientists and analysts is to have easy to visualize and understand machine learning models. Decision trees are preferred in many real-time applications for this reason, and also, because combined in an ensemble, they are one of the most powerful methods in machine learning. In this paper, we present a new system called STREAMDM-C++, that implements decision trees for data streams in C++, and that has been used extensively at Huawei. Streaming decision trees adapt to changes on streams, a huge advantage since standard decision trees are built using a snapshot of data, and can not evolve over time. STREAMDM-C++ is easy to extend, and contains more powerful ensemble methods, and a more efficient and easy to use adaptive decision trees. We compare our new implementation with VFML, the current state of the art implementation in C, and show how our new system outperforms VFML in speed using less resources.",
        "link": "https://dl.acm.org/doi/10.1145/3097983.3098139"
    },
    {
        "title": "Data miners' little helper: data transformation activity cues for cluster analysis on document collections",
        "authors": [
            "Tania Cerquitelli",
            "Evelina Di Corso",
            "Francesco Ventura",
            "Silvia Chiusano"
        ],
        "date": "June 2017",
        "source": "WIMS '17: Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "In this paper we propose a new self-learning engine to streamline the analytics process, as it enables analysts to mine massive data repositories with minimal user intervention. In the context of cluster analysis on a collection of documents this new system, named SELF-DATA (SELF-learning DAta TrAnsformation), suggests to the analyst how to configure the whole mining process for a given dataset. SELF-DATA relies on an engine exploring different data weighting schemas (e.g., normalized term frequencies) and data transformation methods (e.g., PCA) before applying the cluster analysis, evaluating and comparing solutions through different quality indices (e.g., weighted Silhouette), and presenting the k-top solutions to the analyst. SELF-DATA will also include a knowledge base storing results of experiments on previously processed datasets, and a classification algorithm trained on the knowledge base content to forecast the best configuration for the whole mining process for an unexplored dataset. The first development of SELF-DATA running on Apache Spark has been validated on 5 collections of documents. Experimental results highlight that TF-IDF and logarithmic entropy are effective to measure item relevance with sparse datasets, and the LSI method outperforms PCA with a large dictionary.",
        "link": "https://dl.acm.org/doi/10.1145/3102254.3102288"
    },
    {
        "title": "Interactive Visual Graph Mining and Learning",
        "authors": [
            "Ryan A. Rossi",
            "Nesreen K. Ahmed",
            "Rong Zhou",
            "Hoda Eldardiry"
        ],
        "date": null,
        "source": "ACM Transactions on Intelligent Systems and Technology",
        "abstract": "This article presents a platform for interactive graph mining and relational machine learning called GraphVis. The platform combines interactive visual representations with state-of-the-art graph mining and relational machine learning techniques to aid in revealing important insights quickly as well as learning an appropriate and highly predictive model for a particular task (e.g., classification, link prediction, discovering the roles of nodes, and finding influential nodes). Visual representations and interaction techniques and tools are developed for simple, fast, and intuitive real-time interactive exploration, mining, and modeling of graph data. In particular, we propose techniques for interactive relational learning (e.g., node/link classification), interactive link prediction and weighting, role discovery and community detection, higher-order network analysis (via graphlets, network motifs), among others. GraphVis also allows for the refinement and tuning of graph mining and relational learning methods for specific application domains and constraints via an end-to-end interactive visual analytic pipeline that learns, infers, and provides rapid interactive visualization with immediate feedback at each change/prediction in real-time. Other key aspects include interactive filtering, querying, ranking, manipulating, exporting, as well as tools for dynamic network analysis and visualization, interactive graph generators (including new block model approaches), and a variety of multi-level network analysis techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3200764"
    },
    {
        "title": "Data Mining and Opinion Mining: A Tool in Educational Context",
        "authors": [
            "Myriam Peñafiel",
            "Stefanie Vásquez",
            "Diego Vásquez",
            "Juan Zaldumbide",
            "Sergio Luján-Mora"
        ],
        "date": "July 2018",
        "source": "ICoMS '18: Proceedings of the 2018 1st International Conference on Mathematics and Statistics",
        "abstract": "The use of the web as a universal communication platform generates large volumes of data (Big data), which in many cases, need to be processed so that they can become useful knowledge in face of the sceptics who have doubts about the credibility of such information. The use of web data that comes from educational contexts needs to be addressed, since that large amount of unstructured information is not being valued, losing valuable information that can be used. To solve this problem, we propose the use of data mining techniques such as sentiment analysis to validate the information that comes from the educational platforms. The objective of this research is to propose a methodology that allows the user to apply sentiment analysis in a simple way, because although some researchers have done it, very few do with data in the educational context. The results obtained prove that the proposal can be used in similar cases.",
        "link": "https://dl.acm.org/doi/10.1145/3274250.3274263"
    },
    {
        "title": "Research Hotspots Mining and Visualized Analysis Based on Linking Cluster and K-Core Decomposition",
        "authors": [
            "Ying Cai",
            "Fang Huang",
            "Shuai Wang",
            "Hao Zhang",
            "Chunxiu Du"
        ],
        "date": "May 2018",
        "source": "ICDPA 2018: Proceedings of the International Conference on Data Processing and Applications",
        "abstract": "In this paper, we focus on research hotspots mining with knowledge semantic features on discipline topic word networks, which are extracted from scientific and technical documents. For the topic networks, a novel method of combining linking community clustering with k-core decomposition is proposed to mine the hotspot community with knowledge hierarchy structure. And as a post-processing, the hierarchical community identification is implemented by k-core decomposition in order to discover the internal connection with knowledge semantic hierarchy in the research hotspot. Finally, the community density and visualization method is utilized to analyze the core and sub- problem in the research hotspot community. In experiments, the topic networks for a single discipline and interdisciplinary research hotspot are taken as examples of analysis, and the results show the proposed approach can effectively realize the objective of mining and analyzing the discipline research hotspot on the academic topic word network.",
        "link": "https://dl.acm.org/doi/10.1145/3224207.3224216"
    },
    {
        "title": "The Application of Data Mining Algorithms in the Construction of Travel Recommendation System",
        "authors": [
            "Chao Lou",
            "Zhaonan Mu",
            "Mengzhu Liu"
        ],
        "date": "January 2021",
        "source": "CONF-CDS 2021: The 2nd International Conference on Computing and Data Science",
        "abstract": "Photos shared by users of online social network often associated with tags, texts, geographic and time information. These data are ideal research resource which can be used by researchers to observe people's interests and behaviors. They can also be used to analyze people's travel behaviors and interests because it can reflect people's travel activities and experiences directly. This project aimed to consolidate people into groups based on their travel preferences of travel season, travel time span, scenery type of travel location and then analyze popular travel places for different people groups. The travel preferences were analyzed from extracted data applying PHP language and Naive Bayes classification approach. According to the analyzed data, people were divided into groups. Then, DBSCAN clustering was used to get popular locations based on the different groups. As a result, 80 thousand photo data of 5 cities have been collected from Flickr by Phython. It has been showed that people's preference of season in most of the mined cities was spring and summer and the preference of time span was short. Moreover, mined people has been divided into 16 groups according to their preference and based on the groups popular locations have been calculated by DBSCAN clustering. Theses locations have been provided in a recommendation website which was built in this project. For the future work, mining Flickr data associated with other travel social network sites could be a good way to build a more effective recommendation system.",
        "link": "https://dl.acm.org/doi/10.1145/3448734.3450477"
    },
    {
        "title": "Compare Machine Learning Models in Text Classification Using Steam User Reviews",
        "authors": [
            "Youchen Miao",
            "Zeyu Jin",
            "Yumeng Zhang",
            "Yuchen Chen",
            "Junren Lai"
        ],
        "date": "November 2021",
        "source": "ICSED '21: Proceedings of the 2021 3rd International Conference on Software Engineering and Development",
        "abstract": "Text Classification and Sentiment Analysis of game reviews are viewed as important parts in not only academic fields but also in game studies. In this paper, with more than 400 thousand game reviews on Steam platform, we preprocess the data using different libraries (sklearn, nltk, and spaCy) and use them as inputs to build three sentiment classification models based on different algorithms (Naive Bayes, SVM, and Random Forest). In contrast to previous studies that only focus on different sentiment analysis models, our paper also highlights the use of different APIs to preprocess the data and their corresponding model performance. The results show that no matter which API we choose, Random Forest models always perform the best. However, in terms of training time, Naive Bayes is the fastest. This work can be used to apply grid search for researchers to automatically find the optimum API before conducting sentiment analysis in the future.",
        "link": "https://dl.acm.org/doi/10.1145/3507473.3507480"
    },
    {
        "title": "Lost in Transduction: Transductive Transfer Learning in Text Classification",
        "authors": [
            "Alejandro Moreo",
            "Andrea Esuli",
            "Fabrizio Sebastiani"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Obtaining high-quality labelled data for training a classifier in a new application domain is often costly. Transfer Learning\n(a.k.a. “Inductive Transfer”) tries to alleviate these costs by transferring, to the “target” domain of interest, knowledge available from a different “source” domain. In transfer learning the lack of labelled information from the target domain is compensated by the availability at training time of a set of unlabelled examples from the target distribution. Transductive Transfer Learning denotes the transfer learning setting in which the only set of target documents that we are interested in classifying is known and available at training time. Although this definition is indeed in line with Vapnik’s original definition of “transduction”, current terminology in the field is confused. In this article, we discuss how the term “transduction” has been misused in the transfer learning literature, and propose a clarification consistent with the original characterization of this term given by Vapnik. We go on to observe that the above terminology misuse has brought about misleading experimental comparisons, with inductive transfer learning methods that have been incorrectly compared with transductive transfer learning methods. We then, give empirical evidence that the difference in performance between the inductive version and the transductive version of a transfer learning method can indeed be statistically significant (i.e., that knowing at training time the only data one needs to classify indeed gives an advantage). Our clarification allows a reassessment of the field, and of the relative merits of the major, state-of-the-art algorithms for transfer learning in text classification.",
        "link": "https://dl.acm.org/doi/10.1145/3453146"
    },
    {
        "title": "Machine Learning on Mining Potential Adverse Drug Reactions for Pharmacovigilance",
        "authors": [
            "Ke Jia"
        ],
        "date": "July 2021",
        "source": "DSIT 2021: 2021 4th International Conference on Data Science and Information Technology",
        "abstract": "Drugs are related to the lives and health of patients. Mining and discovering potential adverse drug reactions from a large number of unstructured texts play an important role in drug research and pharmacovigilance. It is a hot research issue in the field of biomedicine in recent years. A large number of scholars have discovered a large amount of information on adverse drug reactions from different data sources through effective machine learning methods. This paper reviews the research progress in recent years to provide a reference for future research.",
        "link": "https://dl.acm.org/doi/10.1145/3478905.3478964"
    },
    {
        "title": "TACAM: Topic And Context Aware Argument Mining",
        "authors": [
            "Michael Fromm",
            "Evgeniy Faerman",
            "Thomas Seidl"
        ],
        "date": "October 2019",
        "source": "WI '19: IEEE/WIC/ACM International Conference on Web Intelligence",
        "abstract": "In this work we address the problem of argument search. The purpose of argument search is the distillation of pro and contra arguments for requested topics from large text corpora. In previous works, the usual approach is to use a standard search engine to extract text parts which are relevant to the given topic and subsequently use an argument recognition algorithm to select arguments from them. The main challenge in the argument recognition task, which is also known as argument mining, is that often sentences containing arguments are structurally similar to purely informative sentences without any stance about the topic. In fact, they only differ semantically. Most approaches use topic or search term information only for the first search step and therefore assume that arguments can be classified independently of a topic. We argue that topic information is crucial for argument mining, since the topic defines the semantic context of an argument. Precisely, we propose different models for the classification of arguments, which take information about a topic of an argument into account. Moreover, to enrich the context of a topic and to let models understand the context of the potential argument better, we integrate information from different external sources such as Knowledge Graphs or pre-trained NLP models. Our evaluation shows that considering topic information, especially in connection with external information, provides a significant performance boost for the argument mining task.",
        "link": "https://dl.acm.org/doi/10.1145/3350546.3352506"
    },
    {
        "title": "Text mining for incoming tasks based on the urgency/importance factors and task classification using machine learning tools",
        "authors": [
            "Yasser Ali Alshehri"
        ],
        "date": "March 2020",
        "source": "ICCDA 2020: Proceedings of the 2020 the 4th International Conference on Compute and Data Analysis",
        "abstract": "In workplaces, there is a massive amount of unstructured data from different sources. In this paper, we present a case study that explains how can through communications between employees, we can help to prioritize tasks requests to increase the efficiency of their works for both technical and non-technical workers. This involves managing daily incoming tasks based on their level of urgency and importance.To allow all workers to utilize the urgency-importance matrix as a time-management tool, we need to automate this tool. The textual content of incoming tasks are analyzed, and metrics related to urgency and importance are extracted. A third factor (i.e., the response variable) is defined based on the two input variables (urgency and importance). Then, machine learning applied to the data to predict the class of incoming tasks based on data outcome desired. We used ordinal regression, neural networks, and decision tree algorithms to predict the four levels of task priority. We measure the performance of all using recalls, precisions, and F-scores. All classifiers perform higher than 89% in terms of all measures.",
        "link": "https://dl.acm.org/doi/10.1145/3388142.3388153"
    },
    {
        "title": "Text mining for malware classification using multivariate all repeated patterns detection",
        "authors": [
            "Konstantinos F. Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2019",
        "source": "ASONAM '19: Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Mobile phones have become nowadays a commodity to the majority of people. Using them, people are able to access the world of Internet and connect with their friends, their colleagues at work or even unknown people with common interests. This proliferation of the mobile devices has also been seen as an opportunity for the cyber criminals to deceive smartphone users and steel their money directly or indirectly, respectively, by accessing their bank accounts through the smartphones or by blackmailing them or selling their private data such as photos, credit card data, etc. to third parties. This is usually achieved by installing malware to smartphones masking their malevolent payload as a legitimate application and advertise it to the users with the hope that mobile users will install it in their devices. Thus, any existing application can easily be modified by integrating a malware and then presented it as a legitimate one. In response to this, scientists have proposed a number of malware detection and classification methods using a variety of techniques. Even though, several of them achieve relatively high precision in malware classification, there is still space for improvement. In this paper, we propose a text mining all repeated pattern detection method which uses the decompiled files of an application in order to classify a suspicious application into one of the known malware families. Based on the experimental results using a real malware dataset, the methodology tries to correctly classify (without any misclassification) all randomly selected malware applications of 3 categories with 3 different families each.",
        "link": "https://dl.acm.org/doi/10.1145/3341161.3350841"
    },
    {
        "title": "Comparative Study between Traditional Machine Learning and Deep Learning Approaches for Text Classification",
        "authors": [
            "Cannannore Nidhi Kamath",
            "Syed Saqib Bukhari",
            "Andreas Dengel"
        ],
        "date": "August 2018",
        "source": "DocEng '18: Proceedings of the ACM Symposium on Document Engineering 2018",
        "abstract": "In this contemporaneous world, it is an obligation for any organization working with documents to end up with the insipid task of classifying truckload of documents, which is the nascent stage of venturing into the realm of information retrieval and data mining. But classification of such humongous documents into multiple classes, calls for a lot of time and labor. Hence a system which could classify these documents with acceptable accuracy would be of an unfathomable help in document engineering. We have created multiple classifiers for document classification and compared their accuracy on raw and processed data. We have garnered data used in a corporate organization as well as publicly available data for comparison. Data is processed by removing the stop-words and stemming is implemented to produce root words. Multiple traditional machine learning techniques like Naive Bayes, Logistic Regression, Support Vector Machine, Random forest Classifier and Multi-Layer Perceptron are used for classification of documents. Classifiers are applied on raw and processed data separately and their accuracy is noted. Along with this, Deep learning technique such as Convolution Neural Network is also used to classify the data and its accuracy is compared with that of traditional machine learning techniques. We are also exploring hierarchical classifiers for classification of classes and subclasses. The system classifies the data faster and with better accuracy than if done manually. The results are discussed in the results and evaluation section.",
        "link": "https://dl.acm.org/doi/10.1145/3209280.3209526"
    },
    {
        "title": "Identification of Overpricing in the Purchase of Medication by the Federal Government of Brazil, Using Text Mining and Clustering Based on Ontology",
        "authors": [
            "Marco Aurelio O. S. Correa",
            "Adriano Galindo Leal"
        ],
        "date": "August 2018",
        "source": "ICCBDC '18: Proceedings of the 2018 2nd International Conference on Cloud and Big Data Computing",
        "abstract": "Increasing the transparency level in his actions and spending is one of the primary duties of the Brazilian Federal Government. The creation of laws that oblige full disclose of all its expenditures through transparency portals enables citizens to supervise all government entities. However, only the dissemination of these data, without a definite standard or the availability of data analysis tools, does not guarantee that the citizen is empowered to play his role. Therefore hence, the objective of this work is to identify overprice in the acquisition of products purchased by the federal government of Brazil using the unstructured data available on the Transparency Portal. The last two-years' worth of purchasing data, available in the Transparency Portal, were extracted, processed and stored. Due to his diverse nature and high volume of data, this study focused only on medicines purchased by the Ministry of Health. Ontology-based text mining and clustering techniques were applied for automatic identification and classification of products. The processing of this information was done through text mining and clustering, based on the ontology registered in another database of the Brazilian government. Because of this work, a consolidated price base per medication was created to allow the identification of distortions in prices practised, facilitating the identification of cases that merit further investigation to unravel fraud to the treasury.",
        "link": "https://dl.acm.org/doi/10.1145/3264560.3264569"
    },
    {
        "title": "Computational Estimation by Scientific Data Mining with Classical Methods to Automate Learning Strategies of Scientists",
        "authors": [
            "Aparna S. Varde"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Experimental results are often plotted as 2-dimensional graphical plots (aka graphs) in scientific domains depicting dependent versus independent variables to aid visual analysis of processes. Repeatedly performing laboratory experiments consumes significant time and resources, motivating the need for computational estimation. The goals are to estimate the graph obtained in an experiment given its input conditions, and to estimate the conditions that would lead to a desired graph. Existing estimation approaches often do not meet accuracy and efficiency needs of targeted applications. We develop a computational estimation approach called AutoDomainMine that integrates clustering and classification over complex scientific data in a framework so as to automate classical learning methods of scientists. Knowledge discovered thereby from a database of existing experiments serves as the basis for estimation. Challenges include preserving domain semantics in clustering, finding matching strategies in classification, striking a good balance between elaboration and conciseness while displaying estimation results based on needs of targeted users, and deriving objective measures to capture subjective user interests. These and other challenges are addressed in this work. The AutoDomainMine approach is used to build a computational estimation system, rigorously evaluated with real data in Materials Science. Our evaluation confirms that AutoDomainMine provides desired accuracy and efficiency in computational estimation. It is extendable to other science and engineering domains as proved by adaptation of its sub-processes within fields such as Bioinformatics and Nanotechnology.",
        "link": "https://dl.acm.org/doi/10.1145/3502736"
    },
    {
        "title": "Where is the road for issue reports classification based on text mining?",
        "authors": [
            "Qiang Fan",
            "Yue Yu",
            "Gang Yin",
            "Tao Wang",
            "Huaimin Wang"
        ],
        "date": "November 2017",
        "source": "ESEM '17: Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Currently, open source projects receive various kinds of issues daily, because of the extreme openness of Issue Tracking System (ITS) in GitHub. ITS is a labor-intensive and time-consuming task of issue categorization for project managers. However, a contributor is only required a short textual abstract to report an issue in GitHub. Thus, most traditional classification approaches based on detailed and structured data (e.g., priority, severity, software version and so on) are difficult to adopt. In this paper, issue classification approaches on a large-scale dataset, including 80 popular projects and over 252,000 issue reports collected from GitHub, were investigated. First, four traditional text-based classification methods and their performances were discussed. Semantic perplexity (i.e., an issues description confuses bug-related sentences with nonbug-related sentences) is a crucial factor that affects the classification performances based on quantitative and qualitative study. Finally, A two-stage classifier framework based on the novel metrics of semantic perplexity of issue reports was designed. Results show that our two-stage classification can significantly improve issue classification performances.",
        "link": "https://dl.acm.org/doi/10.1109/ESEM.2017.19"
    },
    {
        "title": "Feature-based Facebook reviews process model for e-management using data mining",
        "authors": [
            "Anish Kumar Varudharajulu",
            "Yongsheng Ma"
        ],
        "date": "January 2019",
        "source": "IC4E '19: Proceedings of the 10th International Conference on E-Education, E-Business, E-Management and E-Learning",
        "abstract": "The data generated from online communication acts as potential gold mines for discovering knowledge for researchers. A large amount of data is also generated in the form of web documents, emails, blogs, and feedback, etc. Text analytics is being significantly employed to mine important information. Opinion mining is the process of extracting human thoughts and perceptions from unstructured texts. The showstopper for designing an opinion mining system for analyzing reviews arise from the fact that customer reviews are often noisy. These reviews are informally written. In addition, they are subjected to spelling mistakes, grammatical errors, improper punctuation and irrational capitalization. This paper focuses on analyzing the different classification and clustering algorithms aimed at extracting and consolidating opinions of customers from social media sites like Facebook, Twitter and through surveys, at multiple levels of granularity to monitor and measure customer satisfaction. Ours is an automated approach, in which the system aids in the process of knowledge assimilation for knowledge-based building and also performs the analytics. Domain experts ratify the knowledge base and also provide training datasets for the system to intuitively gather more instances for ratification. The system identifies opinion expressions as phrases containing opinion words, opinionated features and also opinion modifiers. These expressions are categorized as positive, negative or neutral. Opinion expressions are identified and categorized using localized linguistic techniques. Opinions can be congregated at any desired level of specificity i.e. feature level or product level, user level or service level, etc. We have developed a system based on this approach, which provides the user with a platform to analyze opinion expressions crawled from a set of pre-defined datasets.",
        "link": "https://dl.acm.org/doi/10.1145/3306500.3306514"
    },
    {
        "title": "Discriminative Topic Mining via Category-Name Guided Text Embedding",
        "authors": [
            "Yu Meng",
            "Jiaxin Huang",
            "Guangyuan Wang",
            "Zihan Wang",
            "Chao Zhang",
            "Yu Zhang",
            "Jiawei Han"
        ],
        "date": "April 2020",
        "source": "WWW '20: Proceedings of The Web Conference 2020",
        "abstract": "Mining a set of meaningful and distinctive topics automatically from massive text corpora has broad applications. Existing topic models, however, typically work in a purely unsupervised way, which often generate topics that do not fit users’ particular needs and yield suboptimal performance on downstream tasks. We propose a new task, discriminative topic mining, which leverages a set of user-provided category names to mine discriminative topics from text corpora. This new task not only helps a user understand clearly and distinctively the topics he/she is most interested in, but also benefits directly keyword-driven classification tasks. We develop CatE, a novel category-name guided text embedding method for discriminative topic mining, which effectively leverages minimal user guidance to learn a discriminative embedding space and discover category representative terms in an iterative manner. We conduct a comprehensive set of experiments to show that CatE mines high-quality set of topics guided by category names only, and benefits a variety of downstream applications including weakly-supervised classification and lexical entailment direction identification.",
        "link": "https://dl.acm.org/doi/10.1145/3366423.3380278"
    },
    {
        "title": "Comparative Study of Heart Disease Diagnosis Using Top Ten Data Mining Classification Algorithms",
        "authors": [
            "I. Ketut Agung Enriko"
        ],
        "date": "June 2019",
        "source": "ICFET '19: Proceedings of the 5th International Conference on Frontiers of Educational Technologies",
        "abstract": "Data mining has been used for many purposes, especially for prediction system. In healthcare, data mining algorithms often used in disease diagnosis. Meanwhile, heart disease is known as a primary cause of death over the years. Many studies have been performed in heart disease diagnosis using data mining methods. There are some popular data mining algorithms that can be used in heart disease diagnosis, for example, k-Nearest Neighbor, CART, and AdaBoost. The algorithms are used to analyze a sample of cardiovascular patients data and predict the heart disease type that they suffer. Some parameters are taken from the patient, including EKG morphology, blood pressure, and information about the existence of chest pain, shortness of breath, palpitation, and cold sweat. In this study, medical records data are collected from Harapan Kita Hospital and utilized as a dataset sample in this research. Top ten data mining classification algorithms are used in diagnosing heart disease from Harapan Kita Hospital data and examining their performance by checking the accuracy and speed.",
        "link": "https://dl.acm.org/doi/10.1145/3338188.3338220"
    },
    {
        "title": "Text Mining Approach for Identifying Research Trends",
        "authors": [
            "Snezhana Sulova"
        ],
        "date": "June 2021",
        "source": "CompSysTech '21: Proceedings of the 22nd International Conference on Computer Systems and Technologies",
        "abstract": "With the increase of unstructured data, the issues connected with automatic text processing, the categorization of documents and the discovery of topics have become objects of growing interest. In order to improve the process of grouping and processing research publications, we would like to propose a method based upon natural language processing. It is based on text mining technologies which aim to identify key tendencies in documents. It processes the content of publications by clustering and identifies the topics of each identified group. This analysis helps by identifying key tendencies as well as discovering emerging new areas of research. Publications from the research literature database, Scopus, were used to test the approach. The topic of the publications is “the application of digital technologies in the logistics business”. The experiments were completed using the RapidMiner Studio software.",
        "link": "https://dl.acm.org/doi/10.1145/3472410.3472433"
    },
    {
        "title": "Assessment of Vulnerability Severity using Text Mining",
        "authors": [
            "Georgios Spanos",
            "Lefteris Angelis",
            "Dimitrios Toloudis"
        ],
        "date": "September 2017",
        "source": "PCI '17: Proceedings of the 21st Pan-Hellenic Conference on Informatics",
        "abstract": "Software1 vulnerabilities are closely associated with information systems security, a major and critical field in today's technology. Vulnerabilities constitute a constant and increasing threat for various aspects of everyday life, especially for safety and economy, since the social impact from the problems that they cause is complicated and often unpredictable. Although there is an entire research branch in software engineering that deals with the identification and elimination of vulnerabilities, the growing complexity of software products and the variability of software production procedures are factors contributing to the ongoing occurrence of vulnerabilities, Hence, another area that is being developed in parallel focuses on the study and management of the vulnerabilities that have already been reported and registered in databases. The information contained in such databases includes, a textual description and a number of metrics related to vulnerabilities. The purpose of this paper is to investigate to what extend the assessment of the vulnerability severity can be inferred directly from the corresponding textual description, or in other words, to examine the informative power of the description with respect to the vulnerability severity. For this purpose, text mining techniques, i.e. text analysis and three different classification methods (decision trees, neural networks and support vector machines) were employed. The application of text mining to a sample of 70,678 vulnerabilities from a public data source shows that the description itself is a reliable and highly accurate source of information for vulnerability prioritization.",
        "link": "https://dl.acm.org/doi/10.1145/3139367.3139390"
    },
    {
        "title": "Clustering Algorithms for Spatial Data Mining",
        "authors": [
            "Chetashri Bhadane",
            "Ketan Shah"
        ],
        "date": "April 2020",
        "source": "ICGDA '20: Proceedings of the 2020 3rd International Conference on Geoinformatics and Data Analysis",
        "abstract": "With the advances in mobile and wireless technologies, there has been a rise in applications that track and share the users' geospatial data. People use several social networking sites such as Twitter, Facebook and Flickr, where they share their status updates. With the integration of Global Positioning System (GPS) with mobile phones, it is now possible to share one's locations on these social networks. GPS allows us to record and track a person's movement along with the timestamp. The data set obtained from these GPS logs is vast and is widely used to analyze the users' movement patterns. Specifically, we can find out significant locations based on the number of users present at that location and the time spent by them at such places. Once significant places have been identified, it is also possible to identify the semantic importance of these locations. This paper presents an overview of the clustering techniques used to find important places of interest using large GPS based mobility datasets. Four clustering algorithms, K-Means, DBSCAN, OPTICS and Hierarchical, are implemented, and performance is tested using real-time data of 50 users collected over 2--5 years. Performance summary depicts that K-Means and DBSCAN perform well for spatial data.",
        "link": "https://dl.acm.org/doi/10.1145/3397056.3397068"
    },
    {
        "title": "Feature Selection based Arabic Text Classification using Different Machine Learning Algorithms: Comparative Study",
        "authors": [
            "Sakina Rim Bennabi",
            "Zakaria Elberrichi"
        ],
        "date": "June 2020",
        "source": "ICIST '20: Proceedings of the 10th International Conference on Information Systems and Technologies",
        "abstract": "Feature selection is a method of data pre-processing widely used when mining large data, such as textual classification. Several studies have been conducted to compare the different methods of feature selection applied to corpora in English. Unfortunately, a small number of works concern the Arabic language. This article aims to present a comparative study of different feature selection techniques including: Chi2, the ANOVA method and mutual information, applied on a corpus in Arabic language, while also diversifying the machine learning algorithms (Naive Bayes, SVM and KNN). This experimental study has shown in general that reducing dimensionality with feature selection techniques has slightly affected the performance of textual classification, reducing the size of the corpus by up to 1%.",
        "link": "https://dl.acm.org/doi/10.1145/3447568.3448531"
    },
    {
        "title": "Intelligent Data Analysis using Optimized Support Vector Machine Based Data Mining Approach for Tourism Industry",
        "authors": [
            "Ms Promila Sharma",
            "Uma Meena",
            "Girish Kumar Sharma"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Data analysis involves the deployment of sophisticated approaches from data mining methods, information theory, and artificial intelligence in various fields like tourism, hospitality, and so on for the extraction of knowledge from the gathered and preprocessed data. In tourism, pattern analysis or data analysis using classification is significant for finding the patterns that represent new and potentially useful information or knowledge about the destination and other data. Several data mining techniques are introduced for the classification of data or patterns. However, overfitting, less accuracy, local minima, sensitive to noise are the drawbacks in some existing data mining classification methods. To overcome these challenges, Support vector machine with Red deer optimization (SVM-RDO) based data mining strategy is proposed in this article. Extended Kalman filter (EKF) is utilized in the first phase, i.e., data cleaning to remove the noise and missing values from the input data. Mantaray foraging algorithm (MaFA) is used in the data selection phase, in which the significant data are selected for the further process to reduce the computational complexity. The final phase is the classification, in which SVM-RDO is proposed to access the useful pattern from the selected data. PYTHON is the implementation tool used for the experiment of the proposed model. The experimental analysis is done to show the efficacy of the proposed work. From the experimental results, the proposed SVM-RDO achieved better accuracy, precision, recall, and F1 score than the existing methods for the tourism dataset. Thus, it is showed the effectiveness of the proposed SVM-RDO for pattern analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3494566"
    },
    {
        "title": "Applying text mining to predict learners' cognitive engagement",
        "authors": [
            "Hayati Hind",
            "Mohammed Khalidi Idrissi",
            "Samir Bennani"
        ],
        "date": "October 2017",
        "source": "SCAMS '17: Proceedings of the Mediterranean Symposium on Smart City Application",
        "abstract": "In academic process, engagement represent one of the important success' factor. This paper proposes a predictive system for learners' cognitive engagement based on their online discussion forums participation. In the first level the ontology OWL and the LSA method are used to perform a semantic classification system of the threads according to a specific context chosen by the tutor. Then the Text mining, as a predictive method, is applied to the classified threads and learners' participation in the forums.",
        "link": "https://dl.acm.org/doi/10.1145/3175628.3175655"
    },
    {
        "title": "Investigate Transitions into Drug Addiction through Text Mining of Reddit Data",
        "authors": [
            "John Lu",
            "Sumati Sridhar",
            "Ritika Pandey",
            "Mohammad Al Hasan",
            "Georege Mohler"
        ],
        "date": "July 2019",
        "source": "KDD '19: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Increasing rates of opioid drug abuse and heightened prevalence of online support communities underscore the necessity of employing data mining techniques to better understand drug addiction using these rapidly developing online resources. In this work, we obtained data from Reddit, an online collection of forums, to gather insight into drug use/misuse using text snippets from users narratives. Specifically, using users' posts, we trained a binary classifier which predicts a user's transitions from casual drug discussion forums to drug recovery forums. We also proposed a Cox regression model that outputs likelihoods of such transitions. In doing so, we found that utterances of select drugs and certain linguistic features contained in one's posts can help predict these transitions. Using unfiltered drug-related posts, our research delineates drugs that are associated with higher rates of transitions from recreational drug discussion to support/recovery discussion, offers insight into modern drug culture, and provides tools with potential applications in combating the opioid crisis.",
        "link": "https://dl.acm.org/doi/10.1145/3292500.3330737"
    },
    {
        "title": "Analysing digital banking reviews using text mining",
        "authors": [
            "Li Chen Cheng",
            "Legaspi Rhea Sharmayne"
        ],
        "date": "December 2020",
        "source": "ASONAM '20: Proceedings of the 12th IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Digital banks are new entrants in the banking industry in the Philippines as they only started late 2018. Since then, a handful of players have and are still emerging. With more and more people becoming technologically savvy, it is very critical for financial institutions to develop a digital banking application that will stand out from the competition. This paper aims to use text mining methods to analyse digital banking application reviews. This study will perform topic modelling using LDA to explore customer concerns and will mine association rules between the digital banking features with the review score. The results will reveal which areas the digital banking application can further optimize for customer satisfaction and retention.",
        "link": "https://dl.acm.org/doi/10.1109/ASONAM49781.2020.9381429"
    },
    {
        "title": "Mining Moocs Videos Metadata Using Classification Techniques",
        "authors": [
            "El Harrak Othman",
            "Ghadi Abderrahim",
            "El Bouhdidi Jaber"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "In few years, the internet has become the first source of information for most people, Today MOOCs makes it possible for everyone to access to the education over the world, it's represents an emerging methodology of online teaching and an important development in open education. Due to the rapid development of Moocs and the rapid growth of digital data and video database over the Internet, it is becoming very difficult for learners to browsing and choosing the best training for them. The scientific community has increased the amount of research into new technologies, with a view to improve the digital video utilization: its archiving, indexing, accessibility, acquisition, store and even its process and usability. All these parts of the video utilization entail the necessity of the extraction of all important information of a video, especially in cases of lack of metadata information. Web video mining is retrieving the content using data mining techniques from World Wide Web. There are two approaches for web video mining using traditional image processing (signal processing) and metadata based approach. In this paper, we present the various research makes in this subject and we propose an effective methodology to extract the metadata from Moocs videos and classify them based on the extracted metadata by applying data mining techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090450"
    },
    {
        "title": "Text mining with HathiTrust",
        "authors": [
            "Eleanor Dickson Koehl",
            "Ryan Dubnicek"
        ],
        "date": "June 2019",
        "source": "JCDL '19: Proceedings of the 18th Joint Conference on Digital Libraries",
        "abstract": "This tutorial will introduce attendees to the HathiTrust Research Center and its tools and services for computational text analysis research. HTRC leverages the scope and scale of the HathiTrust Digital Library collection to create opportunities for researchers to perform text data mining on subsets of the corpus. Attendees will develop skills that will allow them to conduct basic text analysis research using HathiTrust data, learn techniques for engaging with scholars on their text data mining research, and explore strategies for computational access to large-scale digital collections.",
        "link": "https://dl.acm.org/doi/10.1109/JCDL.2019.00115"
    },
    {
        "title": "Text mining for plagiarism detection: multivariate pattern detection for recognition of text similarities",
        "authors": [
            "Konstantinos Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2018",
        "source": "ASONAM '18: Proceedings of the 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "The problem of plagiarism the recent years has been intensified by the availability of information in digital form and the accessibility of the electronic libraries through the Internet. As a result, plagiarism detection has been transformed into a big data analytics problem since the number of digital sources is extravagant and a new document needs to be compared with millions of other existing documents. In this paper, a text mining methodology is proposed that can detect all common patterns between a document and the documents in a reference database. The technique is based on a pattern detection algorithm and the corresponding data structure that enables the algorithm to detect all common patterns. The methodology has been applied in a well-defined dataset providing very promising results identifying difficult cases of plagiarism such as technical disguise.",
        "link": "https://dl.acm.org/doi/10.5555/3382225.3382424"
    },
    {
        "title": "Petroleum Engineering Data Text Classification Using Convolutional Neural Network Based Classifier",
        "authors": [
            "Tzuhan Hsu",
            "Yaoqin Zhang"
        ],
        "date": "May 2018",
        "source": "ICMLT '18: Proceedings of the 2018 International Conference on Machine Learning Technologies",
        "abstract": "In this paper, authors introduce a convolutional neural network (CNN) based classification approach for petroleum engineering event. Recently, text classification problem has raised substantial attention in the field of machine learning. The aim of text classification process is to assign correct label to a text on the basis of its content. However, due to the imprecise of feature representation, most of former text classification methods cannot export satisfying result. To solve this problem, authors propose a model based on convolutional neural network. Authors conduct experiments with real world petroleum engineering data text, artificially labelled into 6 categories. Beside purposed CNN-based model, the authors also implement other baseline machine learning approaches: k nearest neighbor, random forest, support vector machines, recurrent neural network, gated recurrent units, long short-term memory. Among all these models, our CNN-based model reach best performance in text classification task. In the last part of this paper, authors compare and analysis their purpose and baseline model.",
        "link": "https://dl.acm.org/doi/10.1145/3231884.3231898"
    },
    {
        "title": "Classifying Indonesian Online Articles as Advertisement Placement Base Using Text Mining",
        "authors": [
            "Nadhira Tasya",
            "Arian Dhini"
        ],
        "date": "July 2017",
        "source": "ICBIM 2017: Proceedings of the International Conference on Business and Information Management",
        "abstract": "Rapid development in technological aspect resulting in growing level of human needs for the latest news, so that emerged a new trend of publishing and accessing news through online media or usually called online journalism. In addition, the number of people who sell and purchase through online sites also continues to increase and this opportunity is utilized by the company and the advertiser by implementing targeted web advertising. However, the high number of articles that have been published and accessed leads to great opportunities for errors in determining where to place the ads. Therefore, it needs a system that can categorize articles accessed by users as the basis of advertisement placement by the company and this classification system can be done by applying the method of Data Mining and Text Mining. This research uses document data in the form of article content that will be categorized into twenty categories of class of advertisement by using Text Mining technique with Support Vector Machine algorithm. The results of this study may be used by companies or advertisers as a basis for placement of ads on selected online media sites.",
        "link": "https://dl.acm.org/doi/10.1145/3134271.3134288"
    },
    {
        "title": "Mining Free-Text Medical Notes for Suicide Risk Assessment",
        "authors": [
            "Marios Adamou",
            "Grigoris Antoniou",
            "Elissavet Greasidou",
            "Vincenzo Lagani",
            "Paulos Charonyktakis",
            "Ioannis Tsamardinos"
        ],
        "date": "July 2018",
        "source": "SETN '18: Proceedings of the 10th Hellenic Conference on Artificial Intelligence",
        "abstract": "Suicide has been considered as an important public health issue for a very long time, and is one of the main causes of death worldwide. Despite suicide prevention strategies being applied, the rate of suicide has not changed substantially over the past decades. Advances in machine learning make it possible to attempt to predict suicide based on the analysis of relevant data to inform clinical practice. This paper reports on findings from the analysis of data of patients who died by suicide in the period 2013-2016 and made use of both structured data and free-text medical notes. We focus on examining various text-mining approaches to support risk assessment. The results show that using advance machine learning and text-mining techniques, it is possible to predict within a specified period which people are most at risk of taking their own life at the time of referral to a mental health service.",
        "link": "https://dl.acm.org/doi/10.1145/3200947.3201020"
    },
    {
        "title": "Integration of data mining techniques in e-learning systems: Clustering Profil of Lerners and Recommender Course System",
        "authors": [
            "A. El Moustamid",
            "E. En-Naimi",
            "J. El Bouhdidi"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "Data mining aims at the extraction of knowledge from large quantities of data by automatic or semi-automatic methods. In the field of education and training e-learning, Data Mining techniques as clustering (The aim of this technique is to identify groups that are similar in some aspect) [21], prediction (is used for developing model where combination of data viewed from different aspects can effect single viewed data. Here, is important to watch how predictors variables are effecting predicted variable)[21], classification are applied in e-learning to improve the level of learners by providing learning according to the learners' requirement) are used to analyze learners profile, to predict and improve student performance. The aim of our project is to develop a system capable of analyzing learners profile and indexing web videos in order to offer learners a rich database with courses that correspond to their levels.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090453"
    },
    {
        "title": "Multi-level mining and visualization of scientific text collections: Exploring a bi-lingual scientific repository",
        "authors": [
            "Pablo Accuosto",
            "Francesco Ronzano",
            "Daniel Ferrés",
            "Horacio Saggion"
        ],
        "date": "December 2017",
        "source": "WOSP 2017: Proceedings of the 6th International Workshop on Mining Scientific Publications",
        "abstract": "We present a system to mine and visualize collections of scientific documents by semantically browsing information extracted from single publications or aggregated throughout corpora of articles. The text mining tool performs deep analysis of document collections allowing the extraction and interpretation of research paper's contents. In addition to the extraction and enrichment of documents with metadata (titles, authors, affiliations, etc), the deep analysis performed comprises semantic interpretation, rhetorical analysis of sentences, triple-based information extraction, and text summarization. The visualization components allow geographical-based exploration of collections, topic-evolution interpretation, and collaborative network analysis among others. The paper presents a case study of a bi-lingual collection in the field of Natural Language Processing (NLP).",
        "link": "https://dl.acm.org/doi/10.1145/3127526.3127529"
    },
    {
        "title": "Hierarchical Topic Mining via Joint Spherical Tree and Text Embedding",
        "authors": [
            "Yu Meng",
            "Yunyi Zhang",
            "Jiaxin Huang",
            "Yu Zhang",
            "Chao Zhang",
            "Jiawei Han"
        ],
        "date": "August 2020",
        "source": "KDD '20: Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Mining a set of meaningful topics organized into a hierarchy is intuitively appealing since topic correlations are ubiquitous in massive text corpora. To account for potential hierarchical topic structures, hierarchical topic models generalize flat topic models by incorporating latent topic hierarchies into their generative modeling process. However, due to their purely unsupervised nature, the learned topic hierarchy often deviates from users' particular needs or interests. To guide the hierarchical topic discovery process with minimal user supervision, we propose a new task, Hierarchical Topic Mining, which takes a category tree described by category names only, and aims to mine a set of representative terms for each category from a text corpus to help a user comprehend his/her interested topics. We develop a novel joint tree and text embedding method along with a principled optimization procedure that allows simultaneous modeling of the category tree structure and the corpus generative process in the spherical space for effective category-representative term discovery. Our comprehensive experiments show that our model, named JoSH, mines a high-quality set of hierarchical topics with high efficiency and benefits weakly-supervised hierarchical text classification tasks.",
        "link": "https://dl.acm.org/doi/10.1145/3394486.3403242"
    },
    {
        "title": "Unsupervised behavioural mining and clustering for malware family identification",
        "authors": [
            "Khanh Huu The Dam",
            "Thomas Given-Wilson",
            "Axel Legay"
        ],
        "date": "March 2021",
        "source": "SAC '21: Proceedings of the 36th Annual ACM Symposium on Applied Computing",
        "abstract": "More accurate and advanced detection and classification of malware requires exploiting program behaviour and not merely syntactic or static features. One approach is to use system call dependency graphs (SCDGs) that represent the program behaviour by interactions with the system, and the relations between these interactions. These SCDGs have been used with supervised learning techniques to very accurately detect and classify malware. This works considers the unsupervised learning challenge of mining for common clusters of behaviour without a priori knowledge. This allows for clustering of similar programs by behaviour, that can then be used for either classification or further analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3412841.3441919"
    },
    {
        "title": "Text Mining for Evaluating Authors' Birth and Death Years",
        "authors": [
            "Dror Moghaz",
            "Yaakov Hacohen-Kerner",
            "Dov Gabbay"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "This article presents a unique method in text and data mining for finding the era, i.e., mining temporal data, in which an anonymous author was living. Finding this era can assist in the examination of a fake document or extracting the time period in which a writer lived. The study and the experiments concern Hebrew, and in some parts, Aramaic and Yiddish rabbinic texts. The rabbinic texts are undated and contain no bibliographic sections, posing an interesting challenge. This work proposes algorithms using key phrases and key words that allow the temporal organization of citations together with linguistic patterns. Based on these key phrases, key words, and the references, we established several types of “Iron-clad,” Heuristic and Greedy rules for estimating the years of birth and death of a writer in an interesting classification task. Experiments were conducted on corpora, including documents authored by 12, 24, and 36 rabbinic writers and demonstrated promising results.",
        "link": "https://dl.acm.org/doi/10.1145/3281631"
    },
    {
        "title": "Transfer Learning to Timed Text Based Video Classification Using CNN",
        "authors": [
            "Zenun Kastrati",
            "Ali Shariq Imran",
            "Arianit Kurti"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "Open educational video resources are gaining popularity with a growing number of massive open online courses (MOOCs). This has created a niche for content providers to adopt effective solutions in automatically organizing and structuring of educational resources for maximum visibility. Recent advances in deep learning techniques are proving useful in managing and classifying resources into appropriate categories. This paper proposes one such convolutional neural network (CNN) model for classifying video lectures in a MOOC setting using a transfer learning approach. The model uses a time-aligned text transcripts corresponding to video lectures from six broader subject categories. Video lectures and their corresponding transcript dataset is gathered from the Coursera MOOC platform. Two different CNN models are proposed: i) CNN based classification using embeddings learned from our MOOC dataset, ii) CNN based classification using transfer learning. Word embeddings generated from two well known state-of-the-art pre-trained models Word2Vec and GloVe, are used in the transfer learning approach for the second case. The proposed CNN models are evaluated using precision, recall, and F1 score and the obtained performance is compared with both conventional and deep learning classifiers. The proposed CNN models have an F1 score improvement of 10-22 percentage points over DNN and conventional classifiers",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326483"
    },
    {
        "title": "Evolutionary role mining in complex networks by ensemble clustering",
        "authors": [
            "Sarvenaz Choobdar",
            "Pedro Ribeiro",
            "Fernando Silva"
        ],
        "date": "April 2017",
        "source": "SAC '17: Proceedings of the Symposium on Applied Computing",
        "abstract": "The structural patterns in the neighborhood of nodes assign unique roles to the nodes. Mining the set of existing roles in a network provides a descriptive profile of the network and draws its general picture. This paper proposes a new method to determine structural roles in a dynamic network based on the current position of nodes and their historic behavior. We develop a temporal ensemble clustering technique to dynamically find groups of nodes, holding similar tempo-structural roles. We compare two weighting functions, based on age and distribution of data, to incorporate temporal behavior of nodes in the role discovery. To evaluate the performance of the proposed method, we assess the results from two points of view: 1) goodness of fit to current structure of the network; 2) consistency with historic data. We conduct the evaluation using different ensemble clustering techniques. The results on real world networks demonstrate that our method can detect tempo-structural roles that simultaneously depict the topology of a network and reflect its dynamics with high accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3019612.3019815"
    },
    {
        "title": "Analyzing used-car web listings via text mining",
        "authors": [
            "Ayhan Demiriz",
            "Fatma Cantaş"
        ],
        "date": "October 2017",
        "source": "IML '17: Proceedings of the 1st International Conference on Internet of Things and Machine Learning",
        "abstract": "Used car trade is one of the major components of the world economies. It is not uncommon to sell a car by placing an internet advertisement irrespective of the geography in these days. A typical content of an advertisement is usually composed of two parts namely the structured and the free text data. The structured data may include some information about the asking price, make, model, year, mileage of the car and the contact info. In most cases, seller may give important clues about the car's current conditions in the free text data where the title (head) of the advertisement can be included as free text too. This paper reports preliminary results from a text mining study conducted on 75K used car internet listings collected from two major car listing web sites in Turkey. As expected, the words and the phrases related to the description of the car are observed to be frequent. The leading concepts in the free text are found to be regarding how to describe the current condition of a car, for example \"no crash history\".",
        "link": "https://dl.acm.org/doi/10.1145/3109761.3109782"
    },
    {
        "title": "Text classification of Diseases Treated by Traditional Chinese Medicine Prescription based on machine learning",
        "authors": [
            "Hanqing Zhao",
            "Bo Han",
            "Chen Li"
        ],
        "date": "September 2020",
        "source": "ISAIMS '20: Proceedings of the 1st International Symposium on Artificial Intelligence in Medical Sciences",
        "abstract": "OBJECTIVE: To explore the application of machine learning in th e identification of diseases treated by traditional Chinese medicine prescriptions. METHODS: Based on the composition of the text document of Chinese medicine prescriptions, the prescriptions were divided into cough, headache and diarrhea. THUTCT were introduced to establish and train two machine learning text classification models, LibSVM and LibLinear, and the prescriptions to be deter mined were put into the model for classification and prediction. R ESULTS: The Precision rate of LibSVM model and Liblinear mod el were 0.7283 and 0.6690. Seven prescriptions were classified an d predicted, and the results were in line with expectations. CONCLUSION: THUCTC has good universality for the content of TCM prescriptions, high classification accuracy and fast testing speed, which is suitable for the text classification and discrimination research of TCM prescriptions for diseases.",
        "link": "https://dl.acm.org/doi/10.1145/3429889.3429895"
    },
    {
        "title": "Evaluation of retweet clustering method classification method using retweets on Twitter without text data",
        "authors": [
            "K. Uchida",
            "F. Toriumi",
            "T. Sakaki"
        ],
        "date": "August 2017",
        "source": "WI '17: Proceedings of the International Conference on Web Intelligence",
        "abstract": "Burst phenomena, which frequently occur on social media, are caused by such social events as flaming on the internet, elections, and natural disasters. To understand people's thoughts and feelings, we must classify their opinions from burst phenomena. Therefore, classification methods that categorize tweets are critical. However, since most classification methods focus on text mining, they cannot group tweets by topics because each tweet has poor linguistic similarities. We used a non-text-based classification method proposed by Baba et al. that groups tweets by topics, even if they have poor linguistic similarities, and verified its validity by comparing it with a text-based classification method in two different evaluations: qualitative and quantitative. In the qualitative evaluation part, we did a questionnaire survey and validated the suitability of the topic clusters created using both the non-and text-based methods. Since evaluating the similarity of every pair of tweets in each topic is difficult, we evaluated the similarity between sampled pairs in the survey and acquired more appropriate topic clustering results using the non-text-based method than the text-based method. In the quantitative evaluation part, we focused on the robustness of each method against data reduction. Many approaches analyze social media data, especially because collecting data from social media is comparatively easy. However, since collecting the whole data of burst phenomena is very costly due to the vast amounts of available social media data, robustness against data reduction is an important index to evaluate classification methods. With the non-text-based method, over 55% of the pairs of tweets in the same cluster were also included in the same cluster even when the data were reduced to 10% in all three of our example cases. In this paper, as a source we focus on Twitter, one of the most popular microblogging services. Using clustering to conduct detailed case analyses, we scrutinized three burst cases that include natural disasters and flaming on the internet and found that a non-text-based method more effectively classified tweets in burst phenomena than a text-based method.",
        "link": "https://dl.acm.org/doi/10.1145/3106426.3106451"
    },
    {
        "title": "Using Spark for Text Mining on Large Scale Liver Cancer Literature",
        "authors": [
            "Ming-Yen Lin",
            "Yu-Ju Lin",
            "Sue-Chen Hsueh"
        ],
        "date": "January 2021",
        "source": "BDET 2021: 2021 the 3rd International Conference on Big Data Engineering and Technology (BDET)",
        "abstract": "Cancer is one of the main causes of death. The number of known cancer to-date is more than one hundred. Liver cancer, ranked after trachea and lung cancer, has long been high in the cancer leading cause in Taiwan and even ranked as second in 2016. The number of scientific articles related to cancer proliferates every year. The number reaches as high as 20 million in PubMed so that discovering useful information from the massive collection is very difficult. In addition, using a single machine to sift through these articles is very time-consuming. Therefore, we present a big data analytic framework using the distributed Apache Spark platform for text mining in PubMed literature. We establish a prediction model for liver cancer articles so that researchers may effectively validate whether an article is related to liver cancer or not. Classification models in Spark MLlib including Linear Support Vector Machines (SVM), Logistic Regression, are used in our experiments. Relevancy to liver cancer is further confirmed by using MeSH (Medical Subject Headings) terms. Logistic regression is about 3 times faster than SVMs and the accuracy of both methods is close to 95% in the experiments using hold-out validation. When max_features is 500 and min_df ≤ 0.1 (or min_df = 1), the accuracy may reach 96%. In the experiments with K-fold cross-validation, the accuracy of SVMs methods is 96%. The experimental results show that that our prediction model may effectively classify liver cancer articles.",
        "link": "https://dl.acm.org/doi/10.1145/3474944.3474958"
    },
    {
        "title": "Automatic topic classification of test cases using text mining at an Android smartphone vendor",
        "authors": [
            "Junji Shimagaki",
            "Yasutaka Kamei",
            "Naoyasu Ubayashi",
            "Abram Hindle"
        ],
        "date": "October 2018",
        "source": "ESEM '18: Proceedings of the 12th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Background: An Android smartphone is an ecosystem of applications, drivers, operating system components, and assets. The volume of the software is large and the number of test cases needed to cover the functionality of an Android system is substantial. Enormous effort has been already taken to properly quantify \"what features and apps were tested and verified?\". This insight is provided by dashboards that summarize test coverage and results per feature. One method to achieve this is to manually tag or label test cases with the topic or function they cover, much like function points. At the studied Android smartphone vendor, tests are labelled with manually defined tags, so-called \"feature labels (FLs)\", and the FLs serve to categorize 100s to 1000s test cases into 10 to 50 groups. Aim: Unfortunately for developers, manual assignment of FLs to 1000s of test cases is a time consuming task, leading to inaccurately labeled test cases, which will render the dashboard useless. We created an automated system that suggests tags/labels to the developers for their test cases rather than manual labeling. Method: We use machine learning models to predict and label the functionality tested by 10,000 test cases developed at the company. Results: Through the quantitative experiments, our models achieved acceptable F-1 performance of 0.3 to 0.88. Also through the qualitative studies with expert teams, we showed that the hierarchy and path of tests was a good predictor of a feature's label. Conclusions: We find that this method can reduce tedious manual effort that software developers spent classifying test cases, while providing more accurate classification results.",
        "link": "https://dl.acm.org/doi/10.1145/3239235.3268927"
    },
    {
        "title": "To apply Data Mining for Classification of Crowd sourced Software Requirements",
        "authors": [
            "Soonh Taj",
            "Qasim Arain",
            "Imran Memon",
            "Asma Zubedi"
        ],
        "date": "April 2019",
        "source": "ICSIE '19: Proceedings of the 8th International Conference on Software and Information Engineering",
        "abstract": "Now a day's main focus of developers is to build quality software that works according to customer needs and for this reason it is necessary to gather right requirements as requirement elicitation is the critical step that impacts on the success of software project as misinterpreted requirements leads to the failure of software project. By keeping this in mind a research is carried out on improving requirements elicitation process and automating the process of classifying requirements. In this research, a model is proposed which will help in this scenario for requirements elicitation and requirement classification. This paper presents a model in which crowd sourcing approach is used so that customers, end users, stakeholders, developers and software engineers can make active participation for requirement elicitation process and requirements gathered using crowdsourcing approach are used by model for classification process i.e. classification of requirements into functional and non-functional requirements. For the proof of proposed model a case study is conducted. Results of case study provided the usefulness and efficiency of proposed model for classification of crowd sourced software requirements.",
        "link": "https://dl.acm.org/doi/10.1145/3328833.3328837"
    },
    {
        "title": "Text classification using Fuzzy TF-IDF and Machine Learning Models",
        "authors": [
            "Mariem Bounabi",
            "Karim El Moutaouakil",
            "Khalid Satori"
        ],
        "date": "October 2019",
        "source": "BDIoT'19: Proceedings of the 4th International Conference on Big Data and Internet of Things",
        "abstract": "The representation of the information has an important impact on the text classification task. Several weighting methods were proposed in the literature, and the term frequency-inverse term frequency (TFIDF), the most know on the text treatment field. The FTF-IDF is a vector representation where the components of the TFIDF are presented as inputs to the Fuzzy Inference System (FIS). In this work, we compare several Machin Learning algorithms such as Naïve Bayes and its derivatives, SVM and Random forest classifiers, using the FTF-IDF representation. To improve the quality of the used classifiers, we call sum attribute selection methods. The recognition rate, for the tested systems, is satisfied, where the system based on naïve Bayes classifier, the FTF-IDF weighting terms, and the info gain select attributes method gives 98.7% as accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3372938.3372956"
    },
    {
        "title": "Text Mining in Cybersecurity: A Systematic Literature Review",
        "authors": [
            "Luciano Ignaczak",
            "Guilherme Goldschmidt",
            "Cristiano André Da Costa",
            "Rodrigo Da Rosa Righi"
        ],
        "date": null,
        "source": "ACM Computing Surveys",
        "abstract": "The growth of data volume has changed cybersecurity activities, demanding a higher level of automation. In this new cybersecurity landscape, text mining emerged as an alternative to improve the efficiency of the activities involving unstructured data. This article proposes a Systematic Literature Review (SLR) to present the application of text mining in the cybersecurity domain. Using a systematic protocol, we identified 2,196 studies, out of which 83 were summarized. As a contribution, we propose a taxonomy to demonstrate the different activities in the cybersecurity domain supported by text mining. We also detail the strategies evaluated in the application of text mining tasks and the use of neural networks to support activities involving unstructured data. The work also discusses text classification performance aiming its application in real-world solutions. The SLR also highlights open gaps for future research, such as the analysis of non-English content and the intensification in the usage of neural networks.",
        "link": "https://dl.acm.org/doi/10.1145/3462477"
    },
    {
        "title": "Using Text Mining to Analyze the Financial Patents",
        "authors": [
            "Yung-Feng Lu",
            "Jia-Lang Xu",
            "Mu-Yen Chen"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "With the advancement and development of science and technology, Nowadays society is the development of information technology, cloud computing, mobile Internet, Internet of Things, block chain and information technology.The combination of finance and technology has made financial technology terms gradually emerging.. This research analyzes the patents of financial technology. However, Taiwan's financial technology patents are mostly G06Q-20 (payment plan, architecture or agreement), G06Q-30 (commercial, such as marketing, shopping, payment, auction or e-commerce). G06Q-40 (financial, investment or tax treatment, insurance).The research use text mining technology to analyze and understand Taiwan's trends and developments in financial technology. The research results show that G06Q-20 patents are biased towards information security and mobile payment. G06Q-30 patents favor shopping websites and advertising. G06Q-40 patents are biased towards diversity, such as securities, insurance, finance, and stocks.",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326478"
    },
    {
        "title": "Identifying Online Profiles of Distance Learning Students Using Data Mining Techniques",
        "authors": [
            "Osama Islam",
            "Muazzam Siddiqui",
            "Naif Radi Aljohani"
        ],
        "date": "October 2019",
        "source": "ICDTE '19: Proceedings of the 3rd International Conference on Digital Technology in Education",
        "abstract": "Educational data has grown over the years with the increased use of technology within educational environments. This has led to a huge amount of data being stored in various data sources representing the student, his/her activities, and other aspects relevant to the learning process. To meet this analytical need, Educational Data Mining (EDM) has emerged to assist educational institutions in identifying key benefits such as students at risk, the level of student engagement or predicting student performance. The aim of this research was to explore the various aspects of student interaction data using data mining techniques to identify relevant patterns of behaviors and possible key attributes that have higher degrees of influence on distance learning students. The main findings identified several patterns of user online profiles based on a set of adopted learning strategies, the research proposed a framework for analyzing such interaction data based on R and Hadoop platforms to correlate online profiles with student performance.",
        "link": "https://dl.acm.org/doi/10.1145/3369199.3369249"
    },
    {
        "title": "Extremely Fast Decision Tree Mining for Evolving Data Streams",
        "authors": [
            "Albert Bifet",
            "Jiajin Zhang",
            "Wei Fan",
            "Cheng He",
            "Jianfeng Zhang",
            "Jianfeng Qian",
            "Geoff Holmes",
            "Bernhard Pfahringer"
        ],
        "date": "August 2017",
        "source": "KDD '17: Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
        "abstract": "Nowadays real-time industrial applications are generating a huge amount of data continuously every day. To process these large data streams, we need fast and efficient methodologies and systems. A useful feature desired for data scientists and analysts is to have easy to visualize and understand machine learning models. Decision trees are preferred in many real-time applications for this reason, and also, because combined in an ensemble, they are one of the most powerful methods in machine learning. In this paper, we present a new system called STREAMDM-C++, that implements decision trees for data streams in C++, and that has been used extensively at Huawei. Streaming decision trees adapt to changes on streams, a huge advantage since standard decision trees are built using a snapshot of data, and can not evolve over time. STREAMDM-C++ is easy to extend, and contains more powerful ensemble methods, and a more efficient and easy to use adaptive decision trees. We compare our new implementation with VFML, the current state of the art implementation in C, and show how our new system outperforms VFML in speed using less resources.",
        "link": "https://dl.acm.org/doi/10.1145/3097983.3098139"
    },
    {
        "title": "Data miners' little helper: data transformation activity cues for cluster analysis on document collections",
        "authors": [
            "Tania Cerquitelli",
            "Evelina Di Corso",
            "Francesco Ventura",
            "Silvia Chiusano"
        ],
        "date": "June 2017",
        "source": "WIMS '17: Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "In this paper we propose a new self-learning engine to streamline the analytics process, as it enables analysts to mine massive data repositories with minimal user intervention. In the context of cluster analysis on a collection of documents this new system, named SELF-DATA (SELF-learning DAta TrAnsformation), suggests to the analyst how to configure the whole mining process for a given dataset. SELF-DATA relies on an engine exploring different data weighting schemas (e.g., normalized term frequencies) and data transformation methods (e.g., PCA) before applying the cluster analysis, evaluating and comparing solutions through different quality indices (e.g., weighted Silhouette), and presenting the k-top solutions to the analyst. SELF-DATA will also include a knowledge base storing results of experiments on previously processed datasets, and a classification algorithm trained on the knowledge base content to forecast the best configuration for the whole mining process for an unexplored dataset. The first development of SELF-DATA running on Apache Spark has been validated on 5 collections of documents. Experimental results highlight that TF-IDF and logarithmic entropy are effective to measure item relevance with sparse datasets, and the LSI method outperforms PCA with a large dictionary.",
        "link": "https://dl.acm.org/doi/10.1145/3102254.3102288"
    },
    {
        "title": "Interactive Visual Graph Mining and Learning",
        "authors": [
            "Ryan A. Rossi",
            "Nesreen K. Ahmed",
            "Rong Zhou",
            "Hoda Eldardiry"
        ],
        "date": null,
        "source": "ACM Transactions on Intelligent Systems and Technology",
        "abstract": "This article presents a platform for interactive graph mining and relational machine learning called GraphVis. The platform combines interactive visual representations with state-of-the-art graph mining and relational machine learning techniques to aid in revealing important insights quickly as well as learning an appropriate and highly predictive model for a particular task (e.g., classification, link prediction, discovering the roles of nodes, and finding influential nodes). Visual representations and interaction techniques and tools are developed for simple, fast, and intuitive real-time interactive exploration, mining, and modeling of graph data. In particular, we propose techniques for interactive relational learning (e.g., node/link classification), interactive link prediction and weighting, role discovery and community detection, higher-order network analysis (via graphlets, network motifs), among others. GraphVis also allows for the refinement and tuning of graph mining and relational learning methods for specific application domains and constraints via an end-to-end interactive visual analytic pipeline that learns, infers, and provides rapid interactive visualization with immediate feedback at each change/prediction in real-time. Other key aspects include interactive filtering, querying, ranking, manipulating, exporting, as well as tools for dynamic network analysis and visualization, interactive graph generators (including new block model approaches), and a variety of multi-level network analysis techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3200764"
    },
    {
        "title": "Data Mining and Opinion Mining: A Tool in Educational Context",
        "authors": [
            "Myriam Peñafiel",
            "Stefanie Vásquez",
            "Diego Vásquez",
            "Juan Zaldumbide",
            "Sergio Luján-Mora"
        ],
        "date": "July 2018",
        "source": "ICoMS '18: Proceedings of the 2018 1st International Conference on Mathematics and Statistics",
        "abstract": "The use of the web as a universal communication platform generates large volumes of data (Big data), which in many cases, need to be processed so that they can become useful knowledge in face of the sceptics who have doubts about the credibility of such information. The use of web data that comes from educational contexts needs to be addressed, since that large amount of unstructured information is not being valued, losing valuable information that can be used. To solve this problem, we propose the use of data mining techniques such as sentiment analysis to validate the information that comes from the educational platforms. The objective of this research is to propose a methodology that allows the user to apply sentiment analysis in a simple way, because although some researchers have done it, very few do with data in the educational context. The results obtained prove that the proposal can be used in similar cases.",
        "link": "https://dl.acm.org/doi/10.1145/3274250.3274263"
    },
    {
        "title": "Research Hotspots Mining and Visualized Analysis Based on Linking Cluster and K-Core Decomposition",
        "authors": [
            "Ying Cai",
            "Fang Huang",
            "Shuai Wang",
            "Hao Zhang",
            "Chunxiu Du"
        ],
        "date": "May 2018",
        "source": "ICDPA 2018: Proceedings of the International Conference on Data Processing and Applications",
        "abstract": "In this paper, we focus on research hotspots mining with knowledge semantic features on discipline topic word networks, which are extracted from scientific and technical documents. For the topic networks, a novel method of combining linking community clustering with k-core decomposition is proposed to mine the hotspot community with knowledge hierarchy structure. And as a post-processing, the hierarchical community identification is implemented by k-core decomposition in order to discover the internal connection with knowledge semantic hierarchy in the research hotspot. Finally, the community density and visualization method is utilized to analyze the core and sub- problem in the research hotspot community. In experiments, the topic networks for a single discipline and interdisciplinary research hotspot are taken as examples of analysis, and the results show the proposed approach can effectively realize the objective of mining and analyzing the discipline research hotspot on the academic topic word network.",
        "link": "https://dl.acm.org/doi/10.1145/3224207.3224216"
    },
    {
        "title": "The Application of Data Mining Algorithms in the Construction of Travel Recommendation System",
        "authors": [
            "Chao Lou",
            "Zhaonan Mu",
            "Mengzhu Liu"
        ],
        "date": "January 2021",
        "source": "CONF-CDS 2021: The 2nd International Conference on Computing and Data Science",
        "abstract": "Photos shared by users of online social network often associated with tags, texts, geographic and time information. These data are ideal research resource which can be used by researchers to observe people's interests and behaviors. They can also be used to analyze people's travel behaviors and interests because it can reflect people's travel activities and experiences directly. This project aimed to consolidate people into groups based on their travel preferences of travel season, travel time span, scenery type of travel location and then analyze popular travel places for different people groups. The travel preferences were analyzed from extracted data applying PHP language and Naive Bayes classification approach. According to the analyzed data, people were divided into groups. Then, DBSCAN clustering was used to get popular locations based on the different groups. As a result, 80 thousand photo data of 5 cities have been collected from Flickr by Phython. It has been showed that people's preference of season in most of the mined cities was spring and summer and the preference of time span was short. Moreover, mined people has been divided into 16 groups according to their preference and based on the groups popular locations have been calculated by DBSCAN clustering. Theses locations have been provided in a recommendation website which was built in this project. For the future work, mining Flickr data associated with other travel social network sites could be a good way to build a more effective recommendation system.",
        "link": "https://dl.acm.org/doi/10.1145/3448734.3450477"
    },
    {
        "title": "Compare Machine Learning Models in Text Classification Using Steam User Reviews",
        "authors": [
            "Youchen Miao",
            "Zeyu Jin",
            "Yumeng Zhang",
            "Yuchen Chen",
            "Junren Lai"
        ],
        "date": "November 2021",
        "source": "ICSED '21: Proceedings of the 2021 3rd International Conference on Software Engineering and Development",
        "abstract": "Text Classification and Sentiment Analysis of game reviews are viewed as important parts in not only academic fields but also in game studies. In this paper, with more than 400 thousand game reviews on Steam platform, we preprocess the data using different libraries (sklearn, nltk, and spaCy) and use them as inputs to build three sentiment classification models based on different algorithms (Naive Bayes, SVM, and Random Forest). In contrast to previous studies that only focus on different sentiment analysis models, our paper also highlights the use of different APIs to preprocess the data and their corresponding model performance. The results show that no matter which API we choose, Random Forest models always perform the best. However, in terms of training time, Naive Bayes is the fastest. This work can be used to apply grid search for researchers to automatically find the optimum API before conducting sentiment analysis in the future.",
        "link": "https://dl.acm.org/doi/10.1145/3507473.3507480"
    },
    {
        "title": "Lost in Transduction: Transductive Transfer Learning in Text Classification",
        "authors": [
            "Alejandro Moreo",
            "Andrea Esuli",
            "Fabrizio Sebastiani"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Obtaining high-quality labelled data for training a classifier in a new application domain is often costly. Transfer Learning\n(a.k.a. “Inductive Transfer”) tries to alleviate these costs by transferring, to the “target” domain of interest, knowledge available from a different “source” domain. In transfer learning the lack of labelled information from the target domain is compensated by the availability at training time of a set of unlabelled examples from the target distribution. Transductive Transfer Learning denotes the transfer learning setting in which the only set of target documents that we are interested in classifying is known and available at training time. Although this definition is indeed in line with Vapnik’s original definition of “transduction”, current terminology in the field is confused. In this article, we discuss how the term “transduction” has been misused in the transfer learning literature, and propose a clarification consistent with the original characterization of this term given by Vapnik. We go on to observe that the above terminology misuse has brought about misleading experimental comparisons, with inductive transfer learning methods that have been incorrectly compared with transductive transfer learning methods. We then, give empirical evidence that the difference in performance between the inductive version and the transductive version of a transfer learning method can indeed be statistically significant (i.e., that knowing at training time the only data one needs to classify indeed gives an advantage). Our clarification allows a reassessment of the field, and of the relative merits of the major, state-of-the-art algorithms for transfer learning in text classification.",
        "link": "https://dl.acm.org/doi/10.1145/3453146"
    },
    {
        "title": "Machine Learning on Mining Potential Adverse Drug Reactions for Pharmacovigilance",
        "authors": [
            "Ke Jia"
        ],
        "date": "July 2021",
        "source": "DSIT 2021: 2021 4th International Conference on Data Science and Information Technology",
        "abstract": "Drugs are related to the lives and health of patients. Mining and discovering potential adverse drug reactions from a large number of unstructured texts play an important role in drug research and pharmacovigilance. It is a hot research issue in the field of biomedicine in recent years. A large number of scholars have discovered a large amount of information on adverse drug reactions from different data sources through effective machine learning methods. This paper reviews the research progress in recent years to provide a reference for future research.",
        "link": "https://dl.acm.org/doi/10.1145/3478905.3478964"
    },
    {
        "title": "TACAM: Topic And Context Aware Argument Mining",
        "authors": [
            "Michael Fromm",
            "Evgeniy Faerman",
            "Thomas Seidl"
        ],
        "date": "October 2019",
        "source": "WI '19: IEEE/WIC/ACM International Conference on Web Intelligence",
        "abstract": "In this work we address the problem of argument search. The purpose of argument search is the distillation of pro and contra arguments for requested topics from large text corpora. In previous works, the usual approach is to use a standard search engine to extract text parts which are relevant to the given topic and subsequently use an argument recognition algorithm to select arguments from them. The main challenge in the argument recognition task, which is also known as argument mining, is that often sentences containing arguments are structurally similar to purely informative sentences without any stance about the topic. In fact, they only differ semantically. Most approaches use topic or search term information only for the first search step and therefore assume that arguments can be classified independently of a topic. We argue that topic information is crucial for argument mining, since the topic defines the semantic context of an argument. Precisely, we propose different models for the classification of arguments, which take information about a topic of an argument into account. Moreover, to enrich the context of a topic and to let models understand the context of the potential argument better, we integrate information from different external sources such as Knowledge Graphs or pre-trained NLP models. Our evaluation shows that considering topic information, especially in connection with external information, provides a significant performance boost for the argument mining task.",
        "link": "https://dl.acm.org/doi/10.1145/3350546.3352506"
    },
    {
        "title": "Text mining for incoming tasks based on the urgency/importance factors and task classification using machine learning tools",
        "authors": [
            "Yasser Ali Alshehri"
        ],
        "date": "March 2020",
        "source": "ICCDA 2020: Proceedings of the 2020 the 4th International Conference on Compute and Data Analysis",
        "abstract": "In workplaces, there is a massive amount of unstructured data from different sources. In this paper, we present a case study that explains how can through communications between employees, we can help to prioritize tasks requests to increase the efficiency of their works for both technical and non-technical workers. This involves managing daily incoming tasks based on their level of urgency and importance.To allow all workers to utilize the urgency-importance matrix as a time-management tool, we need to automate this tool. The textual content of incoming tasks are analyzed, and metrics related to urgency and importance are extracted. A third factor (i.e., the response variable) is defined based on the two input variables (urgency and importance). Then, machine learning applied to the data to predict the class of incoming tasks based on data outcome desired. We used ordinal regression, neural networks, and decision tree algorithms to predict the four levels of task priority. We measure the performance of all using recalls, precisions, and F-scores. All classifiers perform higher than 89% in terms of all measures.",
        "link": "https://dl.acm.org/doi/10.1145/3388142.3388153"
    },
    {
        "title": "Text mining for malware classification using multivariate all repeated patterns detection",
        "authors": [
            "Konstantinos F. Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2019",
        "source": "ASONAM '19: Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Mobile phones have become nowadays a commodity to the majority of people. Using them, people are able to access the world of Internet and connect with their friends, their colleagues at work or even unknown people with common interests. This proliferation of the mobile devices has also been seen as an opportunity for the cyber criminals to deceive smartphone users and steel their money directly or indirectly, respectively, by accessing their bank accounts through the smartphones or by blackmailing them or selling their private data such as photos, credit card data, etc. to third parties. This is usually achieved by installing malware to smartphones masking their malevolent payload as a legitimate application and advertise it to the users with the hope that mobile users will install it in their devices. Thus, any existing application can easily be modified by integrating a malware and then presented it as a legitimate one. In response to this, scientists have proposed a number of malware detection and classification methods using a variety of techniques. Even though, several of them achieve relatively high precision in malware classification, there is still space for improvement. In this paper, we propose a text mining all repeated pattern detection method which uses the decompiled files of an application in order to classify a suspicious application into one of the known malware families. Based on the experimental results using a real malware dataset, the methodology tries to correctly classify (without any misclassification) all randomly selected malware applications of 3 categories with 3 different families each.",
        "link": "https://dl.acm.org/doi/10.1145/3341161.3350841"
    },
    {
        "title": "Comparative Study between Traditional Machine Learning and Deep Learning Approaches for Text Classification",
        "authors": [
            "Cannannore Nidhi Kamath",
            "Syed Saqib Bukhari",
            "Andreas Dengel"
        ],
        "date": "August 2018",
        "source": "DocEng '18: Proceedings of the ACM Symposium on Document Engineering 2018",
        "abstract": "In this contemporaneous world, it is an obligation for any organization working with documents to end up with the insipid task of classifying truckload of documents, which is the nascent stage of venturing into the realm of information retrieval and data mining. But classification of such humongous documents into multiple classes, calls for a lot of time and labor. Hence a system which could classify these documents with acceptable accuracy would be of an unfathomable help in document engineering. We have created multiple classifiers for document classification and compared their accuracy on raw and processed data. We have garnered data used in a corporate organization as well as publicly available data for comparison. Data is processed by removing the stop-words and stemming is implemented to produce root words. Multiple traditional machine learning techniques like Naive Bayes, Logistic Regression, Support Vector Machine, Random forest Classifier and Multi-Layer Perceptron are used for classification of documents. Classifiers are applied on raw and processed data separately and their accuracy is noted. Along with this, Deep learning technique such as Convolution Neural Network is also used to classify the data and its accuracy is compared with that of traditional machine learning techniques. We are also exploring hierarchical classifiers for classification of classes and subclasses. The system classifies the data faster and with better accuracy than if done manually. The results are discussed in the results and evaluation section.",
        "link": "https://dl.acm.org/doi/10.1145/3209280.3209526"
    },
    {
        "title": "Identification of Overpricing in the Purchase of Medication by the Federal Government of Brazil, Using Text Mining and Clustering Based on Ontology",
        "authors": [
            "Marco Aurelio O. S. Correa",
            "Adriano Galindo Leal"
        ],
        "date": "August 2018",
        "source": "ICCBDC '18: Proceedings of the 2018 2nd International Conference on Cloud and Big Data Computing",
        "abstract": "Increasing the transparency level in his actions and spending is one of the primary duties of the Brazilian Federal Government. The creation of laws that oblige full disclose of all its expenditures through transparency portals enables citizens to supervise all government entities. However, only the dissemination of these data, without a definite standard or the availability of data analysis tools, does not guarantee that the citizen is empowered to play his role. Therefore hence, the objective of this work is to identify overprice in the acquisition of products purchased by the federal government of Brazil using the unstructured data available on the Transparency Portal. The last two-years' worth of purchasing data, available in the Transparency Portal, were extracted, processed and stored. Due to his diverse nature and high volume of data, this study focused only on medicines purchased by the Ministry of Health. Ontology-based text mining and clustering techniques were applied for automatic identification and classification of products. The processing of this information was done through text mining and clustering, based on the ontology registered in another database of the Brazilian government. Because of this work, a consolidated price base per medication was created to allow the identification of distortions in prices practised, facilitating the identification of cases that merit further investigation to unravel fraud to the treasury.",
        "link": "https://dl.acm.org/doi/10.1145/3264560.3264569"
    },
    {
        "title": "Computational Estimation by Scientific Data Mining with Classical Methods to Automate Learning Strategies of Scientists",
        "authors": [
            "Aparna S. Varde"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Experimental results are often plotted as 2-dimensional graphical plots (aka graphs) in scientific domains depicting dependent versus independent variables to aid visual analysis of processes. Repeatedly performing laboratory experiments consumes significant time and resources, motivating the need for computational estimation. The goals are to estimate the graph obtained in an experiment given its input conditions, and to estimate the conditions that would lead to a desired graph. Existing estimation approaches often do not meet accuracy and efficiency needs of targeted applications. We develop a computational estimation approach called AutoDomainMine that integrates clustering and classification over complex scientific data in a framework so as to automate classical learning methods of scientists. Knowledge discovered thereby from a database of existing experiments serves as the basis for estimation. Challenges include preserving domain semantics in clustering, finding matching strategies in classification, striking a good balance between elaboration and conciseness while displaying estimation results based on needs of targeted users, and deriving objective measures to capture subjective user interests. These and other challenges are addressed in this work. The AutoDomainMine approach is used to build a computational estimation system, rigorously evaluated with real data in Materials Science. Our evaluation confirms that AutoDomainMine provides desired accuracy and efficiency in computational estimation. It is extendable to other science and engineering domains as proved by adaptation of its sub-processes within fields such as Bioinformatics and Nanotechnology.",
        "link": "https://dl.acm.org/doi/10.1145/3502736"
    },
    {
        "title": "Where is the road for issue reports classification based on text mining?",
        "authors": [
            "Qiang Fan",
            "Yue Yu",
            "Gang Yin",
            "Tao Wang",
            "Huaimin Wang"
        ],
        "date": "November 2017",
        "source": "ESEM '17: Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Currently, open source projects receive various kinds of issues daily, because of the extreme openness of Issue Tracking System (ITS) in GitHub. ITS is a labor-intensive and time-consuming task of issue categorization for project managers. However, a contributor is only required a short textual abstract to report an issue in GitHub. Thus, most traditional classification approaches based on detailed and structured data (e.g., priority, severity, software version and so on) are difficult to adopt. In this paper, issue classification approaches on a large-scale dataset, including 80 popular projects and over 252,000 issue reports collected from GitHub, were investigated. First, four traditional text-based classification methods and their performances were discussed. Semantic perplexity (i.e., an issues description confuses bug-related sentences with nonbug-related sentences) is a crucial factor that affects the classification performances based on quantitative and qualitative study. Finally, A two-stage classifier framework based on the novel metrics of semantic perplexity of issue reports was designed. Results show that our two-stage classification can significantly improve issue classification performances.",
        "link": "https://dl.acm.org/doi/10.1109/ESEM.2017.19"
    },
    {
        "title": "Feature-based Facebook reviews process model for e-management using data mining",
        "authors": [
            "Anish Kumar Varudharajulu",
            "Yongsheng Ma"
        ],
        "date": "January 2019",
        "source": "IC4E '19: Proceedings of the 10th International Conference on E-Education, E-Business, E-Management and E-Learning",
        "abstract": "The data generated from online communication acts as potential gold mines for discovering knowledge for researchers. A large amount of data is also generated in the form of web documents, emails, blogs, and feedback, etc. Text analytics is being significantly employed to mine important information. Opinion mining is the process of extracting human thoughts and perceptions from unstructured texts. The showstopper for designing an opinion mining system for analyzing reviews arise from the fact that customer reviews are often noisy. These reviews are informally written. In addition, they are subjected to spelling mistakes, grammatical errors, improper punctuation and irrational capitalization. This paper focuses on analyzing the different classification and clustering algorithms aimed at extracting and consolidating opinions of customers from social media sites like Facebook, Twitter and through surveys, at multiple levels of granularity to monitor and measure customer satisfaction. Ours is an automated approach, in which the system aids in the process of knowledge assimilation for knowledge-based building and also performs the analytics. Domain experts ratify the knowledge base and also provide training datasets for the system to intuitively gather more instances for ratification. The system identifies opinion expressions as phrases containing opinion words, opinionated features and also opinion modifiers. These expressions are categorized as positive, negative or neutral. Opinion expressions are identified and categorized using localized linguistic techniques. Opinions can be congregated at any desired level of specificity i.e. feature level or product level, user level or service level, etc. We have developed a system based on this approach, which provides the user with a platform to analyze opinion expressions crawled from a set of pre-defined datasets.",
        "link": "https://dl.acm.org/doi/10.1145/3306500.3306514"
    },
    {
        "title": "Discriminative Topic Mining via Category-Name Guided Text Embedding",
        "authors": [
            "Yu Meng",
            "Jiaxin Huang",
            "Guangyuan Wang",
            "Zihan Wang",
            "Chao Zhang",
            "Yu Zhang",
            "Jiawei Han"
        ],
        "date": "April 2020",
        "source": "WWW '20: Proceedings of The Web Conference 2020",
        "abstract": "Mining a set of meaningful and distinctive topics automatically from massive text corpora has broad applications. Existing topic models, however, typically work in a purely unsupervised way, which often generate topics that do not fit users’ particular needs and yield suboptimal performance on downstream tasks. We propose a new task, discriminative topic mining, which leverages a set of user-provided category names to mine discriminative topics from text corpora. This new task not only helps a user understand clearly and distinctively the topics he/she is most interested in, but also benefits directly keyword-driven classification tasks. We develop CatE, a novel category-name guided text embedding method for discriminative topic mining, which effectively leverages minimal user guidance to learn a discriminative embedding space and discover category representative terms in an iterative manner. We conduct a comprehensive set of experiments to show that CatE mines high-quality set of topics guided by category names only, and benefits a variety of downstream applications including weakly-supervised classification and lexical entailment direction identification.",
        "link": "https://dl.acm.org/doi/10.1145/3366423.3380278"
    },
    {
        "title": "Comparative Study of Heart Disease Diagnosis Using Top Ten Data Mining Classification Algorithms",
        "authors": [
            "I. Ketut Agung Enriko"
        ],
        "date": "June 2019",
        "source": "ICFET '19: Proceedings of the 5th International Conference on Frontiers of Educational Technologies",
        "abstract": "Data mining has been used for many purposes, especially for prediction system. In healthcare, data mining algorithms often used in disease diagnosis. Meanwhile, heart disease is known as a primary cause of death over the years. Many studies have been performed in heart disease diagnosis using data mining methods. There are some popular data mining algorithms that can be used in heart disease diagnosis, for example, k-Nearest Neighbor, CART, and AdaBoost. The algorithms are used to analyze a sample of cardiovascular patients data and predict the heart disease type that they suffer. Some parameters are taken from the patient, including EKG morphology, blood pressure, and information about the existence of chest pain, shortness of breath, palpitation, and cold sweat. In this study, medical records data are collected from Harapan Kita Hospital and utilized as a dataset sample in this research. Top ten data mining classification algorithms are used in diagnosing heart disease from Harapan Kita Hospital data and examining their performance by checking the accuracy and speed.",
        "link": "https://dl.acm.org/doi/10.1145/3338188.3338220"
    },
    {
        "title": "Text Mining Approach for Identifying Research Trends",
        "authors": [
            "Snezhana Sulova"
        ],
        "date": "June 2021",
        "source": "CompSysTech '21: Proceedings of the 22nd International Conference on Computer Systems and Technologies",
        "abstract": "With the increase of unstructured data, the issues connected with automatic text processing, the categorization of documents and the discovery of topics have become objects of growing interest. In order to improve the process of grouping and processing research publications, we would like to propose a method based upon natural language processing. It is based on text mining technologies which aim to identify key tendencies in documents. It processes the content of publications by clustering and identifies the topics of each identified group. This analysis helps by identifying key tendencies as well as discovering emerging new areas of research. Publications from the research literature database, Scopus, were used to test the approach. The topic of the publications is “the application of digital technologies in the logistics business”. The experiments were completed using the RapidMiner Studio software.",
        "link": "https://dl.acm.org/doi/10.1145/3472410.3472433"
    },
    {
        "title": "Assessment of Vulnerability Severity using Text Mining",
        "authors": [
            "Georgios Spanos",
            "Lefteris Angelis",
            "Dimitrios Toloudis"
        ],
        "date": "September 2017",
        "source": "PCI '17: Proceedings of the 21st Pan-Hellenic Conference on Informatics",
        "abstract": "Software1 vulnerabilities are closely associated with information systems security, a major and critical field in today's technology. Vulnerabilities constitute a constant and increasing threat for various aspects of everyday life, especially for safety and economy, since the social impact from the problems that they cause is complicated and often unpredictable. Although there is an entire research branch in software engineering that deals with the identification and elimination of vulnerabilities, the growing complexity of software products and the variability of software production procedures are factors contributing to the ongoing occurrence of vulnerabilities, Hence, another area that is being developed in parallel focuses on the study and management of the vulnerabilities that have already been reported and registered in databases. The information contained in such databases includes, a textual description and a number of metrics related to vulnerabilities. The purpose of this paper is to investigate to what extend the assessment of the vulnerability severity can be inferred directly from the corresponding textual description, or in other words, to examine the informative power of the description with respect to the vulnerability severity. For this purpose, text mining techniques, i.e. text analysis and three different classification methods (decision trees, neural networks and support vector machines) were employed. The application of text mining to a sample of 70,678 vulnerabilities from a public data source shows that the description itself is a reliable and highly accurate source of information for vulnerability prioritization.",
        "link": "https://dl.acm.org/doi/10.1145/3139367.3139390"
    },
    {
        "title": "Clustering Algorithms for Spatial Data Mining",
        "authors": [
            "Chetashri Bhadane",
            "Ketan Shah"
        ],
        "date": "April 2020",
        "source": "ICGDA '20: Proceedings of the 2020 3rd International Conference on Geoinformatics and Data Analysis",
        "abstract": "With the advances in mobile and wireless technologies, there has been a rise in applications that track and share the users' geospatial data. People use several social networking sites such as Twitter, Facebook and Flickr, where they share their status updates. With the integration of Global Positioning System (GPS) with mobile phones, it is now possible to share one's locations on these social networks. GPS allows us to record and track a person's movement along with the timestamp. The data set obtained from these GPS logs is vast and is widely used to analyze the users' movement patterns. Specifically, we can find out significant locations based on the number of users present at that location and the time spent by them at such places. Once significant places have been identified, it is also possible to identify the semantic importance of these locations. This paper presents an overview of the clustering techniques used to find important places of interest using large GPS based mobility datasets. Four clustering algorithms, K-Means, DBSCAN, OPTICS and Hierarchical, are implemented, and performance is tested using real-time data of 50 users collected over 2--5 years. Performance summary depicts that K-Means and DBSCAN perform well for spatial data.",
        "link": "https://dl.acm.org/doi/10.1145/3397056.3397068"
    },
    {
        "title": "Feature Selection based Arabic Text Classification using Different Machine Learning Algorithms: Comparative Study",
        "authors": [
            "Sakina Rim Bennabi",
            "Zakaria Elberrichi"
        ],
        "date": "June 2020",
        "source": "ICIST '20: Proceedings of the 10th International Conference on Information Systems and Technologies",
        "abstract": "Feature selection is a method of data pre-processing widely used when mining large data, such as textual classification. Several studies have been conducted to compare the different methods of feature selection applied to corpora in English. Unfortunately, a small number of works concern the Arabic language. This article aims to present a comparative study of different feature selection techniques including: Chi2, the ANOVA method and mutual information, applied on a corpus in Arabic language, while also diversifying the machine learning algorithms (Naive Bayes, SVM and KNN). This experimental study has shown in general that reducing dimensionality with feature selection techniques has slightly affected the performance of textual classification, reducing the size of the corpus by up to 1%.",
        "link": "https://dl.acm.org/doi/10.1145/3447568.3448531"
    },
    {
        "title": "Intelligent Data Analysis using Optimized Support Vector Machine Based Data Mining Approach for Tourism Industry",
        "authors": [
            "Ms Promila Sharma",
            "Uma Meena",
            "Girish Kumar Sharma"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Data analysis involves the deployment of sophisticated approaches from data mining methods, information theory, and artificial intelligence in various fields like tourism, hospitality, and so on for the extraction of knowledge from the gathered and preprocessed data. In tourism, pattern analysis or data analysis using classification is significant for finding the patterns that represent new and potentially useful information or knowledge about the destination and other data. Several data mining techniques are introduced for the classification of data or patterns. However, overfitting, less accuracy, local minima, sensitive to noise are the drawbacks in some existing data mining classification methods. To overcome these challenges, Support vector machine with Red deer optimization (SVM-RDO) based data mining strategy is proposed in this article. Extended Kalman filter (EKF) is utilized in the first phase, i.e., data cleaning to remove the noise and missing values from the input data. Mantaray foraging algorithm (MaFA) is used in the data selection phase, in which the significant data are selected for the further process to reduce the computational complexity. The final phase is the classification, in which SVM-RDO is proposed to access the useful pattern from the selected data. PYTHON is the implementation tool used for the experiment of the proposed model. The experimental analysis is done to show the efficacy of the proposed work. From the experimental results, the proposed SVM-RDO achieved better accuracy, precision, recall, and F1 score than the existing methods for the tourism dataset. Thus, it is showed the effectiveness of the proposed SVM-RDO for pattern analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3494566"
    },
    {
        "title": "Applying text mining to predict learners' cognitive engagement",
        "authors": [
            "Hayati Hind",
            "Mohammed Khalidi Idrissi",
            "Samir Bennani"
        ],
        "date": "October 2017",
        "source": "SCAMS '17: Proceedings of the Mediterranean Symposium on Smart City Application",
        "abstract": "In academic process, engagement represent one of the important success' factor. This paper proposes a predictive system for learners' cognitive engagement based on their online discussion forums participation. In the first level the ontology OWL and the LSA method are used to perform a semantic classification system of the threads according to a specific context chosen by the tutor. Then the Text mining, as a predictive method, is applied to the classified threads and learners' participation in the forums.",
        "link": "https://dl.acm.org/doi/10.1145/3175628.3175655"
    },
    {
        "title": "Investigate Transitions into Drug Addiction through Text Mining of Reddit Data",
        "authors": [
            "John Lu",
            "Sumati Sridhar",
            "Ritika Pandey",
            "Mohammad Al Hasan",
            "Georege Mohler"
        ],
        "date": "July 2019",
        "source": "KDD '19: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Increasing rates of opioid drug abuse and heightened prevalence of online support communities underscore the necessity of employing data mining techniques to better understand drug addiction using these rapidly developing online resources. In this work, we obtained data from Reddit, an online collection of forums, to gather insight into drug use/misuse using text snippets from users narratives. Specifically, using users' posts, we trained a binary classifier which predicts a user's transitions from casual drug discussion forums to drug recovery forums. We also proposed a Cox regression model that outputs likelihoods of such transitions. In doing so, we found that utterances of select drugs and certain linguistic features contained in one's posts can help predict these transitions. Using unfiltered drug-related posts, our research delineates drugs that are associated with higher rates of transitions from recreational drug discussion to support/recovery discussion, offers insight into modern drug culture, and provides tools with potential applications in combating the opioid crisis.",
        "link": "https://dl.acm.org/doi/10.1145/3292500.3330737"
    },
    {
        "title": "Analysing digital banking reviews using text mining",
        "authors": [
            "Li Chen Cheng",
            "Legaspi Rhea Sharmayne"
        ],
        "date": "December 2020",
        "source": "ASONAM '20: Proceedings of the 12th IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Digital banks are new entrants in the banking industry in the Philippines as they only started late 2018. Since then, a handful of players have and are still emerging. With more and more people becoming technologically savvy, it is very critical for financial institutions to develop a digital banking application that will stand out from the competition. This paper aims to use text mining methods to analyse digital banking application reviews. This study will perform topic modelling using LDA to explore customer concerns and will mine association rules between the digital banking features with the review score. The results will reveal which areas the digital banking application can further optimize for customer satisfaction and retention.",
        "link": "https://dl.acm.org/doi/10.1109/ASONAM49781.2020.9381429"
    },
    {
        "title": "Mining Moocs Videos Metadata Using Classification Techniques",
        "authors": [
            "El Harrak Othman",
            "Ghadi Abderrahim",
            "El Bouhdidi Jaber"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "In few years, the internet has become the first source of information for most people, Today MOOCs makes it possible for everyone to access to the education over the world, it's represents an emerging methodology of online teaching and an important development in open education. Due to the rapid development of Moocs and the rapid growth of digital data and video database over the Internet, it is becoming very difficult for learners to browsing and choosing the best training for them. The scientific community has increased the amount of research into new technologies, with a view to improve the digital video utilization: its archiving, indexing, accessibility, acquisition, store and even its process and usability. All these parts of the video utilization entail the necessity of the extraction of all important information of a video, especially in cases of lack of metadata information. Web video mining is retrieving the content using data mining techniques from World Wide Web. There are two approaches for web video mining using traditional image processing (signal processing) and metadata based approach. In this paper, we present the various research makes in this subject and we propose an effective methodology to extract the metadata from Moocs videos and classify them based on the extracted metadata by applying data mining techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090450"
    },
    {
        "title": "Text mining with HathiTrust",
        "authors": [
            "Eleanor Dickson Koehl",
            "Ryan Dubnicek"
        ],
        "date": "June 2019",
        "source": "JCDL '19: Proceedings of the 18th Joint Conference on Digital Libraries",
        "abstract": "This tutorial will introduce attendees to the HathiTrust Research Center and its tools and services for computational text analysis research. HTRC leverages the scope and scale of the HathiTrust Digital Library collection to create opportunities for researchers to perform text data mining on subsets of the corpus. Attendees will develop skills that will allow them to conduct basic text analysis research using HathiTrust data, learn techniques for engaging with scholars on their text data mining research, and explore strategies for computational access to large-scale digital collections.",
        "link": "https://dl.acm.org/doi/10.1109/JCDL.2019.00115"
    },
    {
        "title": "Text mining for plagiarism detection: multivariate pattern detection for recognition of text similarities",
        "authors": [
            "Konstantinos Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2018",
        "source": "ASONAM '18: Proceedings of the 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "The problem of plagiarism the recent years has been intensified by the availability of information in digital form and the accessibility of the electronic libraries through the Internet. As a result, plagiarism detection has been transformed into a big data analytics problem since the number of digital sources is extravagant and a new document needs to be compared with millions of other existing documents. In this paper, a text mining methodology is proposed that can detect all common patterns between a document and the documents in a reference database. The technique is based on a pattern detection algorithm and the corresponding data structure that enables the algorithm to detect all common patterns. The methodology has been applied in a well-defined dataset providing very promising results identifying difficult cases of plagiarism such as technical disguise.",
        "link": "https://dl.acm.org/doi/10.5555/3382225.3382424"
    },
    {
        "title": "Petroleum Engineering Data Text Classification Using Convolutional Neural Network Based Classifier",
        "authors": [
            "Tzuhan Hsu",
            "Yaoqin Zhang"
        ],
        "date": "May 2018",
        "source": "ICMLT '18: Proceedings of the 2018 International Conference on Machine Learning Technologies",
        "abstract": "In this paper, authors introduce a convolutional neural network (CNN) based classification approach for petroleum engineering event. Recently, text classification problem has raised substantial attention in the field of machine learning. The aim of text classification process is to assign correct label to a text on the basis of its content. However, due to the imprecise of feature representation, most of former text classification methods cannot export satisfying result. To solve this problem, authors propose a model based on convolutional neural network. Authors conduct experiments with real world petroleum engineering data text, artificially labelled into 6 categories. Beside purposed CNN-based model, the authors also implement other baseline machine learning approaches: k nearest neighbor, random forest, support vector machines, recurrent neural network, gated recurrent units, long short-term memory. Among all these models, our CNN-based model reach best performance in text classification task. In the last part of this paper, authors compare and analysis their purpose and baseline model.",
        "link": "https://dl.acm.org/doi/10.1145/3231884.3231898"
    },
    {
        "title": "Classifying Indonesian Online Articles as Advertisement Placement Base Using Text Mining",
        "authors": [
            "Nadhira Tasya",
            "Arian Dhini"
        ],
        "date": "July 2017",
        "source": "ICBIM 2017: Proceedings of the International Conference on Business and Information Management",
        "abstract": "Rapid development in technological aspect resulting in growing level of human needs for the latest news, so that emerged a new trend of publishing and accessing news through online media or usually called online journalism. In addition, the number of people who sell and purchase through online sites also continues to increase and this opportunity is utilized by the company and the advertiser by implementing targeted web advertising. However, the high number of articles that have been published and accessed leads to great opportunities for errors in determining where to place the ads. Therefore, it needs a system that can categorize articles accessed by users as the basis of advertisement placement by the company and this classification system can be done by applying the method of Data Mining and Text Mining. This research uses document data in the form of article content that will be categorized into twenty categories of class of advertisement by using Text Mining technique with Support Vector Machine algorithm. The results of this study may be used by companies or advertisers as a basis for placement of ads on selected online media sites.",
        "link": "https://dl.acm.org/doi/10.1145/3134271.3134288"
    },
    {
        "title": "Mining Free-Text Medical Notes for Suicide Risk Assessment",
        "authors": [
            "Marios Adamou",
            "Grigoris Antoniou",
            "Elissavet Greasidou",
            "Vincenzo Lagani",
            "Paulos Charonyktakis",
            "Ioannis Tsamardinos"
        ],
        "date": "July 2018",
        "source": "SETN '18: Proceedings of the 10th Hellenic Conference on Artificial Intelligence",
        "abstract": "Suicide has been considered as an important public health issue for a very long time, and is one of the main causes of death worldwide. Despite suicide prevention strategies being applied, the rate of suicide has not changed substantially over the past decades. Advances in machine learning make it possible to attempt to predict suicide based on the analysis of relevant data to inform clinical practice. This paper reports on findings from the analysis of data of patients who died by suicide in the period 2013-2016 and made use of both structured data and free-text medical notes. We focus on examining various text-mining approaches to support risk assessment. The results show that using advance machine learning and text-mining techniques, it is possible to predict within a specified period which people are most at risk of taking their own life at the time of referral to a mental health service.",
        "link": "https://dl.acm.org/doi/10.1145/3200947.3201020"
    },
    {
        "title": "Integration of data mining techniques in e-learning systems: Clustering Profil of Lerners and Recommender Course System",
        "authors": [
            "A. El Moustamid",
            "E. En-Naimi",
            "J. El Bouhdidi"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "Data mining aims at the extraction of knowledge from large quantities of data by automatic or semi-automatic methods. In the field of education and training e-learning, Data Mining techniques as clustering (The aim of this technique is to identify groups that are similar in some aspect) [21], prediction (is used for developing model where combination of data viewed from different aspects can effect single viewed data. Here, is important to watch how predictors variables are effecting predicted variable)[21], classification are applied in e-learning to improve the level of learners by providing learning according to the learners' requirement) are used to analyze learners profile, to predict and improve student performance. The aim of our project is to develop a system capable of analyzing learners profile and indexing web videos in order to offer learners a rich database with courses that correspond to their levels.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090453"
    },
    {
        "title": "Multi-level mining and visualization of scientific text collections: Exploring a bi-lingual scientific repository",
        "authors": [
            "Pablo Accuosto",
            "Francesco Ronzano",
            "Daniel Ferrés",
            "Horacio Saggion"
        ],
        "date": "December 2017",
        "source": "WOSP 2017: Proceedings of the 6th International Workshop on Mining Scientific Publications",
        "abstract": "We present a system to mine and visualize collections of scientific documents by semantically browsing information extracted from single publications or aggregated throughout corpora of articles. The text mining tool performs deep analysis of document collections allowing the extraction and interpretation of research paper's contents. In addition to the extraction and enrichment of documents with metadata (titles, authors, affiliations, etc), the deep analysis performed comprises semantic interpretation, rhetorical analysis of sentences, triple-based information extraction, and text summarization. The visualization components allow geographical-based exploration of collections, topic-evolution interpretation, and collaborative network analysis among others. The paper presents a case study of a bi-lingual collection in the field of Natural Language Processing (NLP).",
        "link": "https://dl.acm.org/doi/10.1145/3127526.3127529"
    },
    {
        "title": "Hierarchical Topic Mining via Joint Spherical Tree and Text Embedding",
        "authors": [
            "Yu Meng",
            "Yunyi Zhang",
            "Jiaxin Huang",
            "Yu Zhang",
            "Chao Zhang",
            "Jiawei Han"
        ],
        "date": "August 2020",
        "source": "KDD '20: Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Mining a set of meaningful topics organized into a hierarchy is intuitively appealing since topic correlations are ubiquitous in massive text corpora. To account for potential hierarchical topic structures, hierarchical topic models generalize flat topic models by incorporating latent topic hierarchies into their generative modeling process. However, due to their purely unsupervised nature, the learned topic hierarchy often deviates from users' particular needs or interests. To guide the hierarchical topic discovery process with minimal user supervision, we propose a new task, Hierarchical Topic Mining, which takes a category tree described by category names only, and aims to mine a set of representative terms for each category from a text corpus to help a user comprehend his/her interested topics. We develop a novel joint tree and text embedding method along with a principled optimization procedure that allows simultaneous modeling of the category tree structure and the corpus generative process in the spherical space for effective category-representative term discovery. Our comprehensive experiments show that our model, named JoSH, mines a high-quality set of hierarchical topics with high efficiency and benefits weakly-supervised hierarchical text classification tasks.",
        "link": "https://dl.acm.org/doi/10.1145/3394486.3403242"
    },
    {
        "title": "Unsupervised behavioural mining and clustering for malware family identification",
        "authors": [
            "Khanh Huu The Dam",
            "Thomas Given-Wilson",
            "Axel Legay"
        ],
        "date": "March 2021",
        "source": "SAC '21: Proceedings of the 36th Annual ACM Symposium on Applied Computing",
        "abstract": "More accurate and advanced detection and classification of malware requires exploiting program behaviour and not merely syntactic or static features. One approach is to use system call dependency graphs (SCDGs) that represent the program behaviour by interactions with the system, and the relations between these interactions. These SCDGs have been used with supervised learning techniques to very accurately detect and classify malware. This works considers the unsupervised learning challenge of mining for common clusters of behaviour without a priori knowledge. This allows for clustering of similar programs by behaviour, that can then be used for either classification or further analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3412841.3441919"
    },
    {
        "title": "Text Mining for Evaluating Authors' Birth and Death Years",
        "authors": [
            "Dror Moghaz",
            "Yaakov Hacohen-Kerner",
            "Dov Gabbay"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "This article presents a unique method in text and data mining for finding the era, i.e., mining temporal data, in which an anonymous author was living. Finding this era can assist in the examination of a fake document or extracting the time period in which a writer lived. The study and the experiments concern Hebrew, and in some parts, Aramaic and Yiddish rabbinic texts. The rabbinic texts are undated and contain no bibliographic sections, posing an interesting challenge. This work proposes algorithms using key phrases and key words that allow the temporal organization of citations together with linguistic patterns. Based on these key phrases, key words, and the references, we established several types of “Iron-clad,” Heuristic and Greedy rules for estimating the years of birth and death of a writer in an interesting classification task. Experiments were conducted on corpora, including documents authored by 12, 24, and 36 rabbinic writers and demonstrated promising results.",
        "link": "https://dl.acm.org/doi/10.1145/3281631"
    },
    {
        "title": "Transfer Learning to Timed Text Based Video Classification Using CNN",
        "authors": [
            "Zenun Kastrati",
            "Ali Shariq Imran",
            "Arianit Kurti"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "Open educational video resources are gaining popularity with a growing number of massive open online courses (MOOCs). This has created a niche for content providers to adopt effective solutions in automatically organizing and structuring of educational resources for maximum visibility. Recent advances in deep learning techniques are proving useful in managing and classifying resources into appropriate categories. This paper proposes one such convolutional neural network (CNN) model for classifying video lectures in a MOOC setting using a transfer learning approach. The model uses a time-aligned text transcripts corresponding to video lectures from six broader subject categories. Video lectures and their corresponding transcript dataset is gathered from the Coursera MOOC platform. Two different CNN models are proposed: i) CNN based classification using embeddings learned from our MOOC dataset, ii) CNN based classification using transfer learning. Word embeddings generated from two well known state-of-the-art pre-trained models Word2Vec and GloVe, are used in the transfer learning approach for the second case. The proposed CNN models are evaluated using precision, recall, and F1 score and the obtained performance is compared with both conventional and deep learning classifiers. The proposed CNN models have an F1 score improvement of 10-22 percentage points over DNN and conventional classifiers",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326483"
    },
    {
        "title": "Evolutionary role mining in complex networks by ensemble clustering",
        "authors": [
            "Sarvenaz Choobdar",
            "Pedro Ribeiro",
            "Fernando Silva"
        ],
        "date": "April 2017",
        "source": "SAC '17: Proceedings of the Symposium on Applied Computing",
        "abstract": "The structural patterns in the neighborhood of nodes assign unique roles to the nodes. Mining the set of existing roles in a network provides a descriptive profile of the network and draws its general picture. This paper proposes a new method to determine structural roles in a dynamic network based on the current position of nodes and their historic behavior. We develop a temporal ensemble clustering technique to dynamically find groups of nodes, holding similar tempo-structural roles. We compare two weighting functions, based on age and distribution of data, to incorporate temporal behavior of nodes in the role discovery. To evaluate the performance of the proposed method, we assess the results from two points of view: 1) goodness of fit to current structure of the network; 2) consistency with historic data. We conduct the evaluation using different ensemble clustering techniques. The results on real world networks demonstrate that our method can detect tempo-structural roles that simultaneously depict the topology of a network and reflect its dynamics with high accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3019612.3019815"
    },
    {
        "title": "Analyzing used-car web listings via text mining",
        "authors": [
            "Ayhan Demiriz",
            "Fatma Cantaş"
        ],
        "date": "October 2017",
        "source": "IML '17: Proceedings of the 1st International Conference on Internet of Things and Machine Learning",
        "abstract": "Used car trade is one of the major components of the world economies. It is not uncommon to sell a car by placing an internet advertisement irrespective of the geography in these days. A typical content of an advertisement is usually composed of two parts namely the structured and the free text data. The structured data may include some information about the asking price, make, model, year, mileage of the car and the contact info. In most cases, seller may give important clues about the car's current conditions in the free text data where the title (head) of the advertisement can be included as free text too. This paper reports preliminary results from a text mining study conducted on 75K used car internet listings collected from two major car listing web sites in Turkey. As expected, the words and the phrases related to the description of the car are observed to be frequent. The leading concepts in the free text are found to be regarding how to describe the current condition of a car, for example \"no crash history\".",
        "link": "https://dl.acm.org/doi/10.1145/3109761.3109782"
    },
    {
        "title": "Text classification of Diseases Treated by Traditional Chinese Medicine Prescription based on machine learning",
        "authors": [
            "Hanqing Zhao",
            "Bo Han",
            "Chen Li"
        ],
        "date": "September 2020",
        "source": "ISAIMS '20: Proceedings of the 1st International Symposium on Artificial Intelligence in Medical Sciences",
        "abstract": "OBJECTIVE: To explore the application of machine learning in th e identification of diseases treated by traditional Chinese medicine prescriptions. METHODS: Based on the composition of the text document of Chinese medicine prescriptions, the prescriptions were divided into cough, headache and diarrhea. THUTCT were introduced to establish and train two machine learning text classification models, LibSVM and LibLinear, and the prescriptions to be deter mined were put into the model for classification and prediction. R ESULTS: The Precision rate of LibSVM model and Liblinear mod el were 0.7283 and 0.6690. Seven prescriptions were classified an d predicted, and the results were in line with expectations. CONCLUSION: THUCTC has good universality for the content of TCM prescriptions, high classification accuracy and fast testing speed, which is suitable for the text classification and discrimination research of TCM prescriptions for diseases.",
        "link": "https://dl.acm.org/doi/10.1145/3429889.3429895"
    },
    {
        "title": "Evaluation of retweet clustering method classification method using retweets on Twitter without text data",
        "authors": [
            "K. Uchida",
            "F. Toriumi",
            "T. Sakaki"
        ],
        "date": "August 2017",
        "source": "WI '17: Proceedings of the International Conference on Web Intelligence",
        "abstract": "Burst phenomena, which frequently occur on social media, are caused by such social events as flaming on the internet, elections, and natural disasters. To understand people's thoughts and feelings, we must classify their opinions from burst phenomena. Therefore, classification methods that categorize tweets are critical. However, since most classification methods focus on text mining, they cannot group tweets by topics because each tweet has poor linguistic similarities. We used a non-text-based classification method proposed by Baba et al. that groups tweets by topics, even if they have poor linguistic similarities, and verified its validity by comparing it with a text-based classification method in two different evaluations: qualitative and quantitative. In the qualitative evaluation part, we did a questionnaire survey and validated the suitability of the topic clusters created using both the non-and text-based methods. Since evaluating the similarity of every pair of tweets in each topic is difficult, we evaluated the similarity between sampled pairs in the survey and acquired more appropriate topic clustering results using the non-text-based method than the text-based method. In the quantitative evaluation part, we focused on the robustness of each method against data reduction. Many approaches analyze social media data, especially because collecting data from social media is comparatively easy. However, since collecting the whole data of burst phenomena is very costly due to the vast amounts of available social media data, robustness against data reduction is an important index to evaluate classification methods. With the non-text-based method, over 55% of the pairs of tweets in the same cluster were also included in the same cluster even when the data were reduced to 10% in all three of our example cases. In this paper, as a source we focus on Twitter, one of the most popular microblogging services. Using clustering to conduct detailed case analyses, we scrutinized three burst cases that include natural disasters and flaming on the internet and found that a non-text-based method more effectively classified tweets in burst phenomena than a text-based method.",
        "link": "https://dl.acm.org/doi/10.1145/3106426.3106451"
    },
    {
        "title": "Using Spark for Text Mining on Large Scale Liver Cancer Literature",
        "authors": [
            "Ming-Yen Lin",
            "Yu-Ju Lin",
            "Sue-Chen Hsueh"
        ],
        "date": "January 2021",
        "source": "BDET 2021: 2021 the 3rd International Conference on Big Data Engineering and Technology (BDET)",
        "abstract": "Cancer is one of the main causes of death. The number of known cancer to-date is more than one hundred. Liver cancer, ranked after trachea and lung cancer, has long been high in the cancer leading cause in Taiwan and even ranked as second in 2016. The number of scientific articles related to cancer proliferates every year. The number reaches as high as 20 million in PubMed so that discovering useful information from the massive collection is very difficult. In addition, using a single machine to sift through these articles is very time-consuming. Therefore, we present a big data analytic framework using the distributed Apache Spark platform for text mining in PubMed literature. We establish a prediction model for liver cancer articles so that researchers may effectively validate whether an article is related to liver cancer or not. Classification models in Spark MLlib including Linear Support Vector Machines (SVM), Logistic Regression, are used in our experiments. Relevancy to liver cancer is further confirmed by using MeSH (Medical Subject Headings) terms. Logistic regression is about 3 times faster than SVMs and the accuracy of both methods is close to 95% in the experiments using hold-out validation. When max_features is 500 and min_df ≤ 0.1 (or min_df = 1), the accuracy may reach 96%. In the experiments with K-fold cross-validation, the accuracy of SVMs methods is 96%. The experimental results show that that our prediction model may effectively classify liver cancer articles.",
        "link": "https://dl.acm.org/doi/10.1145/3474944.3474958"
    },
    {
        "title": "Automatic topic classification of test cases using text mining at an Android smartphone vendor",
        "authors": [
            "Junji Shimagaki",
            "Yasutaka Kamei",
            "Naoyasu Ubayashi",
            "Abram Hindle"
        ],
        "date": "October 2018",
        "source": "ESEM '18: Proceedings of the 12th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Background: An Android smartphone is an ecosystem of applications, drivers, operating system components, and assets. The volume of the software is large and the number of test cases needed to cover the functionality of an Android system is substantial. Enormous effort has been already taken to properly quantify \"what features and apps were tested and verified?\". This insight is provided by dashboards that summarize test coverage and results per feature. One method to achieve this is to manually tag or label test cases with the topic or function they cover, much like function points. At the studied Android smartphone vendor, tests are labelled with manually defined tags, so-called \"feature labels (FLs)\", and the FLs serve to categorize 100s to 1000s test cases into 10 to 50 groups. Aim: Unfortunately for developers, manual assignment of FLs to 1000s of test cases is a time consuming task, leading to inaccurately labeled test cases, which will render the dashboard useless. We created an automated system that suggests tags/labels to the developers for their test cases rather than manual labeling. Method: We use machine learning models to predict and label the functionality tested by 10,000 test cases developed at the company. Results: Through the quantitative experiments, our models achieved acceptable F-1 performance of 0.3 to 0.88. Also through the qualitative studies with expert teams, we showed that the hierarchy and path of tests was a good predictor of a feature's label. Conclusions: We find that this method can reduce tedious manual effort that software developers spent classifying test cases, while providing more accurate classification results.",
        "link": "https://dl.acm.org/doi/10.1145/3239235.3268927"
    },
    {
        "title": "To apply Data Mining for Classification of Crowd sourced Software Requirements",
        "authors": [
            "Soonh Taj",
            "Qasim Arain",
            "Imran Memon",
            "Asma Zubedi"
        ],
        "date": "April 2019",
        "source": "ICSIE '19: Proceedings of the 8th International Conference on Software and Information Engineering",
        "abstract": "Now a day's main focus of developers is to build quality software that works according to customer needs and for this reason it is necessary to gather right requirements as requirement elicitation is the critical step that impacts on the success of software project as misinterpreted requirements leads to the failure of software project. By keeping this in mind a research is carried out on improving requirements elicitation process and automating the process of classifying requirements. In this research, a model is proposed which will help in this scenario for requirements elicitation and requirement classification. This paper presents a model in which crowd sourcing approach is used so that customers, end users, stakeholders, developers and software engineers can make active participation for requirement elicitation process and requirements gathered using crowdsourcing approach are used by model for classification process i.e. classification of requirements into functional and non-functional requirements. For the proof of proposed model a case study is conducted. Results of case study provided the usefulness and efficiency of proposed model for classification of crowd sourced software requirements.",
        "link": "https://dl.acm.org/doi/10.1145/3328833.3328837"
    },
    {
        "title": "Text classification using Fuzzy TF-IDF and Machine Learning Models",
        "authors": [
            "Mariem Bounabi",
            "Karim El Moutaouakil",
            "Khalid Satori"
        ],
        "date": "October 2019",
        "source": "BDIoT'19: Proceedings of the 4th International Conference on Big Data and Internet of Things",
        "abstract": "The representation of the information has an important impact on the text classification task. Several weighting methods were proposed in the literature, and the term frequency-inverse term frequency (TFIDF), the most know on the text treatment field. The FTF-IDF is a vector representation where the components of the TFIDF are presented as inputs to the Fuzzy Inference System (FIS). In this work, we compare several Machin Learning algorithms such as Naïve Bayes and its derivatives, SVM and Random forest classifiers, using the FTF-IDF representation. To improve the quality of the used classifiers, we call sum attribute selection methods. The recognition rate, for the tested systems, is satisfied, where the system based on naïve Bayes classifier, the FTF-IDF weighting terms, and the info gain select attributes method gives 98.7% as accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3372938.3372956"
    },
    {
        "title": "Text Mining in Cybersecurity: A Systematic Literature Review",
        "authors": [
            "Luciano Ignaczak",
            "Guilherme Goldschmidt",
            "Cristiano André Da Costa",
            "Rodrigo Da Rosa Righi"
        ],
        "date": null,
        "source": "ACM Computing Surveys",
        "abstract": "The growth of data volume has changed cybersecurity activities, demanding a higher level of automation. In this new cybersecurity landscape, text mining emerged as an alternative to improve the efficiency of the activities involving unstructured data. This article proposes a Systematic Literature Review (SLR) to present the application of text mining in the cybersecurity domain. Using a systematic protocol, we identified 2,196 studies, out of which 83 were summarized. As a contribution, we propose a taxonomy to demonstrate the different activities in the cybersecurity domain supported by text mining. We also detail the strategies evaluated in the application of text mining tasks and the use of neural networks to support activities involving unstructured data. The work also discusses text classification performance aiming its application in real-world solutions. The SLR also highlights open gaps for future research, such as the analysis of non-English content and the intensification in the usage of neural networks.",
        "link": "https://dl.acm.org/doi/10.1145/3462477"
    },
    {
        "title": "Using Text Mining to Analyze the Financial Patents",
        "authors": [
            "Yung-Feng Lu",
            "Jia-Lang Xu",
            "Mu-Yen Chen"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "With the advancement and development of science and technology, Nowadays society is the development of information technology, cloud computing, mobile Internet, Internet of Things, block chain and information technology.The combination of finance and technology has made financial technology terms gradually emerging.. This research analyzes the patents of financial technology. However, Taiwan's financial technology patents are mostly G06Q-20 (payment plan, architecture or agreement), G06Q-30 (commercial, such as marketing, shopping, payment, auction or e-commerce). G06Q-40 (financial, investment or tax treatment, insurance).The research use text mining technology to analyze and understand Taiwan's trends and developments in financial technology. The research results show that G06Q-20 patents are biased towards information security and mobile payment. G06Q-30 patents favor shopping websites and advertising. G06Q-40 patents are biased towards diversity, such as securities, insurance, finance, and stocks.",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326478"
    },
    {
        "title": "Identifying Online Profiles of Distance Learning Students Using Data Mining Techniques",
        "authors": [
            "Osama Islam",
            "Muazzam Siddiqui",
            "Naif Radi Aljohani"
        ],
        "date": "October 2019",
        "source": "ICDTE '19: Proceedings of the 3rd International Conference on Digital Technology in Education",
        "abstract": "Educational data has grown over the years with the increased use of technology within educational environments. This has led to a huge amount of data being stored in various data sources representing the student, his/her activities, and other aspects relevant to the learning process. To meet this analytical need, Educational Data Mining (EDM) has emerged to assist educational institutions in identifying key benefits such as students at risk, the level of student engagement or predicting student performance. The aim of this research was to explore the various aspects of student interaction data using data mining techniques to identify relevant patterns of behaviors and possible key attributes that have higher degrees of influence on distance learning students. The main findings identified several patterns of user online profiles based on a set of adopted learning strategies, the research proposed a framework for analyzing such interaction data based on R and Hadoop platforms to correlate online profiles with student performance.",
        "link": "https://dl.acm.org/doi/10.1145/3369199.3369249"
    },
    {
        "title": "Extremely Fast Decision Tree Mining for Evolving Data Streams",
        "authors": [
            "Albert Bifet",
            "Jiajin Zhang",
            "Wei Fan",
            "Cheng He",
            "Jianfeng Zhang",
            "Jianfeng Qian",
            "Geoff Holmes",
            "Bernhard Pfahringer"
        ],
        "date": "August 2017",
        "source": "KDD '17: Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
        "abstract": "Nowadays real-time industrial applications are generating a huge amount of data continuously every day. To process these large data streams, we need fast and efficient methodologies and systems. A useful feature desired for data scientists and analysts is to have easy to visualize and understand machine learning models. Decision trees are preferred in many real-time applications for this reason, and also, because combined in an ensemble, they are one of the most powerful methods in machine learning. In this paper, we present a new system called STREAMDM-C++, that implements decision trees for data streams in C++, and that has been used extensively at Huawei. Streaming decision trees adapt to changes on streams, a huge advantage since standard decision trees are built using a snapshot of data, and can not evolve over time. STREAMDM-C++ is easy to extend, and contains more powerful ensemble methods, and a more efficient and easy to use adaptive decision trees. We compare our new implementation with VFML, the current state of the art implementation in C, and show how our new system outperforms VFML in speed using less resources.",
        "link": "https://dl.acm.org/doi/10.1145/3097983.3098139"
    },
    {
        "title": "Data miners' little helper: data transformation activity cues for cluster analysis on document collections",
        "authors": [
            "Tania Cerquitelli",
            "Evelina Di Corso",
            "Francesco Ventura",
            "Silvia Chiusano"
        ],
        "date": "June 2017",
        "source": "WIMS '17: Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "In this paper we propose a new self-learning engine to streamline the analytics process, as it enables analysts to mine massive data repositories with minimal user intervention. In the context of cluster analysis on a collection of documents this new system, named SELF-DATA (SELF-learning DAta TrAnsformation), suggests to the analyst how to configure the whole mining process for a given dataset. SELF-DATA relies on an engine exploring different data weighting schemas (e.g., normalized term frequencies) and data transformation methods (e.g., PCA) before applying the cluster analysis, evaluating and comparing solutions through different quality indices (e.g., weighted Silhouette), and presenting the k-top solutions to the analyst. SELF-DATA will also include a knowledge base storing results of experiments on previously processed datasets, and a classification algorithm trained on the knowledge base content to forecast the best configuration for the whole mining process for an unexplored dataset. The first development of SELF-DATA running on Apache Spark has been validated on 5 collections of documents. Experimental results highlight that TF-IDF and logarithmic entropy are effective to measure item relevance with sparse datasets, and the LSI method outperforms PCA with a large dictionary.",
        "link": "https://dl.acm.org/doi/10.1145/3102254.3102288"
    },
    {
        "title": "Interactive Visual Graph Mining and Learning",
        "authors": [
            "Ryan A. Rossi",
            "Nesreen K. Ahmed",
            "Rong Zhou",
            "Hoda Eldardiry"
        ],
        "date": null,
        "source": "ACM Transactions on Intelligent Systems and Technology",
        "abstract": "This article presents a platform for interactive graph mining and relational machine learning called GraphVis. The platform combines interactive visual representations with state-of-the-art graph mining and relational machine learning techniques to aid in revealing important insights quickly as well as learning an appropriate and highly predictive model for a particular task (e.g., classification, link prediction, discovering the roles of nodes, and finding influential nodes). Visual representations and interaction techniques and tools are developed for simple, fast, and intuitive real-time interactive exploration, mining, and modeling of graph data. In particular, we propose techniques for interactive relational learning (e.g., node/link classification), interactive link prediction and weighting, role discovery and community detection, higher-order network analysis (via graphlets, network motifs), among others. GraphVis also allows for the refinement and tuning of graph mining and relational learning methods for specific application domains and constraints via an end-to-end interactive visual analytic pipeline that learns, infers, and provides rapid interactive visualization with immediate feedback at each change/prediction in real-time. Other key aspects include interactive filtering, querying, ranking, manipulating, exporting, as well as tools for dynamic network analysis and visualization, interactive graph generators (including new block model approaches), and a variety of multi-level network analysis techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3200764"
    },
    {
        "title": "Data Mining and Opinion Mining: A Tool in Educational Context",
        "authors": [
            "Myriam Peñafiel",
            "Stefanie Vásquez",
            "Diego Vásquez",
            "Juan Zaldumbide",
            "Sergio Luján-Mora"
        ],
        "date": "July 2018",
        "source": "ICoMS '18: Proceedings of the 2018 1st International Conference on Mathematics and Statistics",
        "abstract": "The use of the web as a universal communication platform generates large volumes of data (Big data), which in many cases, need to be processed so that they can become useful knowledge in face of the sceptics who have doubts about the credibility of such information. The use of web data that comes from educational contexts needs to be addressed, since that large amount of unstructured information is not being valued, losing valuable information that can be used. To solve this problem, we propose the use of data mining techniques such as sentiment analysis to validate the information that comes from the educational platforms. The objective of this research is to propose a methodology that allows the user to apply sentiment analysis in a simple way, because although some researchers have done it, very few do with data in the educational context. The results obtained prove that the proposal can be used in similar cases.",
        "link": "https://dl.acm.org/doi/10.1145/3274250.3274263"
    },
    {
        "title": "Research Hotspots Mining and Visualized Analysis Based on Linking Cluster and K-Core Decomposition",
        "authors": [
            "Ying Cai",
            "Fang Huang",
            "Shuai Wang",
            "Hao Zhang",
            "Chunxiu Du"
        ],
        "date": "May 2018",
        "source": "ICDPA 2018: Proceedings of the International Conference on Data Processing and Applications",
        "abstract": "In this paper, we focus on research hotspots mining with knowledge semantic features on discipline topic word networks, which are extracted from scientific and technical documents. For the topic networks, a novel method of combining linking community clustering with k-core decomposition is proposed to mine the hotspot community with knowledge hierarchy structure. And as a post-processing, the hierarchical community identification is implemented by k-core decomposition in order to discover the internal connection with knowledge semantic hierarchy in the research hotspot. Finally, the community density and visualization method is utilized to analyze the core and sub- problem in the research hotspot community. In experiments, the topic networks for a single discipline and interdisciplinary research hotspot are taken as examples of analysis, and the results show the proposed approach can effectively realize the objective of mining and analyzing the discipline research hotspot on the academic topic word network.",
        "link": "https://dl.acm.org/doi/10.1145/3224207.3224216"
    },
    {
        "title": "The Application of Data Mining Algorithms in the Construction of Travel Recommendation System",
        "authors": [
            "Chao Lou",
            "Zhaonan Mu",
            "Mengzhu Liu"
        ],
        "date": "January 2021",
        "source": "CONF-CDS 2021: The 2nd International Conference on Computing and Data Science",
        "abstract": "Photos shared by users of online social network often associated with tags, texts, geographic and time information. These data are ideal research resource which can be used by researchers to observe people's interests and behaviors. They can also be used to analyze people's travel behaviors and interests because it can reflect people's travel activities and experiences directly. This project aimed to consolidate people into groups based on their travel preferences of travel season, travel time span, scenery type of travel location and then analyze popular travel places for different people groups. The travel preferences were analyzed from extracted data applying PHP language and Naive Bayes classification approach. According to the analyzed data, people were divided into groups. Then, DBSCAN clustering was used to get popular locations based on the different groups. As a result, 80 thousand photo data of 5 cities have been collected from Flickr by Phython. It has been showed that people's preference of season in most of the mined cities was spring and summer and the preference of time span was short. Moreover, mined people has been divided into 16 groups according to their preference and based on the groups popular locations have been calculated by DBSCAN clustering. Theses locations have been provided in a recommendation website which was built in this project. For the future work, mining Flickr data associated with other travel social network sites could be a good way to build a more effective recommendation system.",
        "link": "https://dl.acm.org/doi/10.1145/3448734.3450477"
    },
    {
        "title": "Compare Machine Learning Models in Text Classification Using Steam User Reviews",
        "authors": [
            "Youchen Miao",
            "Zeyu Jin",
            "Yumeng Zhang",
            "Yuchen Chen",
            "Junren Lai"
        ],
        "date": "November 2021",
        "source": "ICSED '21: Proceedings of the 2021 3rd International Conference on Software Engineering and Development",
        "abstract": "Text Classification and Sentiment Analysis of game reviews are viewed as important parts in not only academic fields but also in game studies. In this paper, with more than 400 thousand game reviews on Steam platform, we preprocess the data using different libraries (sklearn, nltk, and spaCy) and use them as inputs to build three sentiment classification models based on different algorithms (Naive Bayes, SVM, and Random Forest). In contrast to previous studies that only focus on different sentiment analysis models, our paper also highlights the use of different APIs to preprocess the data and their corresponding model performance. The results show that no matter which API we choose, Random Forest models always perform the best. However, in terms of training time, Naive Bayes is the fastest. This work can be used to apply grid search for researchers to automatically find the optimum API before conducting sentiment analysis in the future.",
        "link": "https://dl.acm.org/doi/10.1145/3507473.3507480"
    },
    {
        "title": "Lost in Transduction: Transductive Transfer Learning in Text Classification",
        "authors": [
            "Alejandro Moreo",
            "Andrea Esuli",
            "Fabrizio Sebastiani"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Obtaining high-quality labelled data for training a classifier in a new application domain is often costly. Transfer Learning\n(a.k.a. “Inductive Transfer”) tries to alleviate these costs by transferring, to the “target” domain of interest, knowledge available from a different “source” domain. In transfer learning the lack of labelled information from the target domain is compensated by the availability at training time of a set of unlabelled examples from the target distribution. Transductive Transfer Learning denotes the transfer learning setting in which the only set of target documents that we are interested in classifying is known and available at training time. Although this definition is indeed in line with Vapnik’s original definition of “transduction”, current terminology in the field is confused. In this article, we discuss how the term “transduction” has been misused in the transfer learning literature, and propose a clarification consistent with the original characterization of this term given by Vapnik. We go on to observe that the above terminology misuse has brought about misleading experimental comparisons, with inductive transfer learning methods that have been incorrectly compared with transductive transfer learning methods. We then, give empirical evidence that the difference in performance between the inductive version and the transductive version of a transfer learning method can indeed be statistically significant (i.e., that knowing at training time the only data one needs to classify indeed gives an advantage). Our clarification allows a reassessment of the field, and of the relative merits of the major, state-of-the-art algorithms for transfer learning in text classification.",
        "link": "https://dl.acm.org/doi/10.1145/3453146"
    },
    {
        "title": "Machine Learning on Mining Potential Adverse Drug Reactions for Pharmacovigilance",
        "authors": [
            "Ke Jia"
        ],
        "date": "July 2021",
        "source": "DSIT 2021: 2021 4th International Conference on Data Science and Information Technology",
        "abstract": "Drugs are related to the lives and health of patients. Mining and discovering potential adverse drug reactions from a large number of unstructured texts play an important role in drug research and pharmacovigilance. It is a hot research issue in the field of biomedicine in recent years. A large number of scholars have discovered a large amount of information on adverse drug reactions from different data sources through effective machine learning methods. This paper reviews the research progress in recent years to provide a reference for future research.",
        "link": "https://dl.acm.org/doi/10.1145/3478905.3478964"
    },
    {
        "title": "TACAM: Topic And Context Aware Argument Mining",
        "authors": [
            "Michael Fromm",
            "Evgeniy Faerman",
            "Thomas Seidl"
        ],
        "date": "October 2019",
        "source": "WI '19: IEEE/WIC/ACM International Conference on Web Intelligence",
        "abstract": "In this work we address the problem of argument search. The purpose of argument search is the distillation of pro and contra arguments for requested topics from large text corpora. In previous works, the usual approach is to use a standard search engine to extract text parts which are relevant to the given topic and subsequently use an argument recognition algorithm to select arguments from them. The main challenge in the argument recognition task, which is also known as argument mining, is that often sentences containing arguments are structurally similar to purely informative sentences without any stance about the topic. In fact, they only differ semantically. Most approaches use topic or search term information only for the first search step and therefore assume that arguments can be classified independently of a topic. We argue that topic information is crucial for argument mining, since the topic defines the semantic context of an argument. Precisely, we propose different models for the classification of arguments, which take information about a topic of an argument into account. Moreover, to enrich the context of a topic and to let models understand the context of the potential argument better, we integrate information from different external sources such as Knowledge Graphs or pre-trained NLP models. Our evaluation shows that considering topic information, especially in connection with external information, provides a significant performance boost for the argument mining task.",
        "link": "https://dl.acm.org/doi/10.1145/3350546.3352506"
    },
    {
        "title": "Text mining for incoming tasks based on the urgency/importance factors and task classification using machine learning tools",
        "authors": [
            "Yasser Ali Alshehri"
        ],
        "date": "March 2020",
        "source": "ICCDA 2020: Proceedings of the 2020 the 4th International Conference on Compute and Data Analysis",
        "abstract": "In workplaces, there is a massive amount of unstructured data from different sources. In this paper, we present a case study that explains how can through communications between employees, we can help to prioritize tasks requests to increase the efficiency of their works for both technical and non-technical workers. This involves managing daily incoming tasks based on their level of urgency and importance.To allow all workers to utilize the urgency-importance matrix as a time-management tool, we need to automate this tool. The textual content of incoming tasks are analyzed, and metrics related to urgency and importance are extracted. A third factor (i.e., the response variable) is defined based on the two input variables (urgency and importance). Then, machine learning applied to the data to predict the class of incoming tasks based on data outcome desired. We used ordinal regression, neural networks, and decision tree algorithms to predict the four levels of task priority. We measure the performance of all using recalls, precisions, and F-scores. All classifiers perform higher than 89% in terms of all measures.",
        "link": "https://dl.acm.org/doi/10.1145/3388142.3388153"
    },
    {
        "title": "Text mining for malware classification using multivariate all repeated patterns detection",
        "authors": [
            "Konstantinos F. Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2019",
        "source": "ASONAM '19: Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Mobile phones have become nowadays a commodity to the majority of people. Using them, people are able to access the world of Internet and connect with their friends, their colleagues at work or even unknown people with common interests. This proliferation of the mobile devices has also been seen as an opportunity for the cyber criminals to deceive smartphone users and steel their money directly or indirectly, respectively, by accessing their bank accounts through the smartphones or by blackmailing them or selling their private data such as photos, credit card data, etc. to third parties. This is usually achieved by installing malware to smartphones masking their malevolent payload as a legitimate application and advertise it to the users with the hope that mobile users will install it in their devices. Thus, any existing application can easily be modified by integrating a malware and then presented it as a legitimate one. In response to this, scientists have proposed a number of malware detection and classification methods using a variety of techniques. Even though, several of them achieve relatively high precision in malware classification, there is still space for improvement. In this paper, we propose a text mining all repeated pattern detection method which uses the decompiled files of an application in order to classify a suspicious application into one of the known malware families. Based on the experimental results using a real malware dataset, the methodology tries to correctly classify (without any misclassification) all randomly selected malware applications of 3 categories with 3 different families each.",
        "link": "https://dl.acm.org/doi/10.1145/3341161.3350841"
    },
    {
        "title": "Comparative Study between Traditional Machine Learning and Deep Learning Approaches for Text Classification",
        "authors": [
            "Cannannore Nidhi Kamath",
            "Syed Saqib Bukhari",
            "Andreas Dengel"
        ],
        "date": "August 2018",
        "source": "DocEng '18: Proceedings of the ACM Symposium on Document Engineering 2018",
        "abstract": "In this contemporaneous world, it is an obligation for any organization working with documents to end up with the insipid task of classifying truckload of documents, which is the nascent stage of venturing into the realm of information retrieval and data mining. But classification of such humongous documents into multiple classes, calls for a lot of time and labor. Hence a system which could classify these documents with acceptable accuracy would be of an unfathomable help in document engineering. We have created multiple classifiers for document classification and compared their accuracy on raw and processed data. We have garnered data used in a corporate organization as well as publicly available data for comparison. Data is processed by removing the stop-words and stemming is implemented to produce root words. Multiple traditional machine learning techniques like Naive Bayes, Logistic Regression, Support Vector Machine, Random forest Classifier and Multi-Layer Perceptron are used for classification of documents. Classifiers are applied on raw and processed data separately and their accuracy is noted. Along with this, Deep learning technique such as Convolution Neural Network is also used to classify the data and its accuracy is compared with that of traditional machine learning techniques. We are also exploring hierarchical classifiers for classification of classes and subclasses. The system classifies the data faster and with better accuracy than if done manually. The results are discussed in the results and evaluation section.",
        "link": "https://dl.acm.org/doi/10.1145/3209280.3209526"
    },
    {
        "title": "Identification of Overpricing in the Purchase of Medication by the Federal Government of Brazil, Using Text Mining and Clustering Based on Ontology",
        "authors": [
            "Marco Aurelio O. S. Correa",
            "Adriano Galindo Leal"
        ],
        "date": "August 2018",
        "source": "ICCBDC '18: Proceedings of the 2018 2nd International Conference on Cloud and Big Data Computing",
        "abstract": "Increasing the transparency level in his actions and spending is one of the primary duties of the Brazilian Federal Government. The creation of laws that oblige full disclose of all its expenditures through transparency portals enables citizens to supervise all government entities. However, only the dissemination of these data, without a definite standard or the availability of data analysis tools, does not guarantee that the citizen is empowered to play his role. Therefore hence, the objective of this work is to identify overprice in the acquisition of products purchased by the federal government of Brazil using the unstructured data available on the Transparency Portal. The last two-years' worth of purchasing data, available in the Transparency Portal, were extracted, processed and stored. Due to his diverse nature and high volume of data, this study focused only on medicines purchased by the Ministry of Health. Ontology-based text mining and clustering techniques were applied for automatic identification and classification of products. The processing of this information was done through text mining and clustering, based on the ontology registered in another database of the Brazilian government. Because of this work, a consolidated price base per medication was created to allow the identification of distortions in prices practised, facilitating the identification of cases that merit further investigation to unravel fraud to the treasury.",
        "link": "https://dl.acm.org/doi/10.1145/3264560.3264569"
    },
    {
        "title": "Computational Estimation by Scientific Data Mining with Classical Methods to Automate Learning Strategies of Scientists",
        "authors": [
            "Aparna S. Varde"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Experimental results are often plotted as 2-dimensional graphical plots (aka graphs) in scientific domains depicting dependent versus independent variables to aid visual analysis of processes. Repeatedly performing laboratory experiments consumes significant time and resources, motivating the need for computational estimation. The goals are to estimate the graph obtained in an experiment given its input conditions, and to estimate the conditions that would lead to a desired graph. Existing estimation approaches often do not meet accuracy and efficiency needs of targeted applications. We develop a computational estimation approach called AutoDomainMine that integrates clustering and classification over complex scientific data in a framework so as to automate classical learning methods of scientists. Knowledge discovered thereby from a database of existing experiments serves as the basis for estimation. Challenges include preserving domain semantics in clustering, finding matching strategies in classification, striking a good balance between elaboration and conciseness while displaying estimation results based on needs of targeted users, and deriving objective measures to capture subjective user interests. These and other challenges are addressed in this work. The AutoDomainMine approach is used to build a computational estimation system, rigorously evaluated with real data in Materials Science. Our evaluation confirms that AutoDomainMine provides desired accuracy and efficiency in computational estimation. It is extendable to other science and engineering domains as proved by adaptation of its sub-processes within fields such as Bioinformatics and Nanotechnology.",
        "link": "https://dl.acm.org/doi/10.1145/3502736"
    },
    {
        "title": "Where is the road for issue reports classification based on text mining?",
        "authors": [
            "Qiang Fan",
            "Yue Yu",
            "Gang Yin",
            "Tao Wang",
            "Huaimin Wang"
        ],
        "date": "November 2017",
        "source": "ESEM '17: Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Currently, open source projects receive various kinds of issues daily, because of the extreme openness of Issue Tracking System (ITS) in GitHub. ITS is a labor-intensive and time-consuming task of issue categorization for project managers. However, a contributor is only required a short textual abstract to report an issue in GitHub. Thus, most traditional classification approaches based on detailed and structured data (e.g., priority, severity, software version and so on) are difficult to adopt. In this paper, issue classification approaches on a large-scale dataset, including 80 popular projects and over 252,000 issue reports collected from GitHub, were investigated. First, four traditional text-based classification methods and their performances were discussed. Semantic perplexity (i.e., an issues description confuses bug-related sentences with nonbug-related sentences) is a crucial factor that affects the classification performances based on quantitative and qualitative study. Finally, A two-stage classifier framework based on the novel metrics of semantic perplexity of issue reports was designed. Results show that our two-stage classification can significantly improve issue classification performances.",
        "link": "https://dl.acm.org/doi/10.1109/ESEM.2017.19"
    },
    {
        "title": "Feature-based Facebook reviews process model for e-management using data mining",
        "authors": [
            "Anish Kumar Varudharajulu",
            "Yongsheng Ma"
        ],
        "date": "January 2019",
        "source": "IC4E '19: Proceedings of the 10th International Conference on E-Education, E-Business, E-Management and E-Learning",
        "abstract": "The data generated from online communication acts as potential gold mines for discovering knowledge for researchers. A large amount of data is also generated in the form of web documents, emails, blogs, and feedback, etc. Text analytics is being significantly employed to mine important information. Opinion mining is the process of extracting human thoughts and perceptions from unstructured texts. The showstopper for designing an opinion mining system for analyzing reviews arise from the fact that customer reviews are often noisy. These reviews are informally written. In addition, they are subjected to spelling mistakes, grammatical errors, improper punctuation and irrational capitalization. This paper focuses on analyzing the different classification and clustering algorithms aimed at extracting and consolidating opinions of customers from social media sites like Facebook, Twitter and through surveys, at multiple levels of granularity to monitor and measure customer satisfaction. Ours is an automated approach, in which the system aids in the process of knowledge assimilation for knowledge-based building and also performs the analytics. Domain experts ratify the knowledge base and also provide training datasets for the system to intuitively gather more instances for ratification. The system identifies opinion expressions as phrases containing opinion words, opinionated features and also opinion modifiers. These expressions are categorized as positive, negative or neutral. Opinion expressions are identified and categorized using localized linguistic techniques. Opinions can be congregated at any desired level of specificity i.e. feature level or product level, user level or service level, etc. We have developed a system based on this approach, which provides the user with a platform to analyze opinion expressions crawled from a set of pre-defined datasets.",
        "link": "https://dl.acm.org/doi/10.1145/3306500.3306514"
    },
    {
        "title": "Discriminative Topic Mining via Category-Name Guided Text Embedding",
        "authors": [
            "Yu Meng",
            "Jiaxin Huang",
            "Guangyuan Wang",
            "Zihan Wang",
            "Chao Zhang",
            "Yu Zhang",
            "Jiawei Han"
        ],
        "date": "April 2020",
        "source": "WWW '20: Proceedings of The Web Conference 2020",
        "abstract": "Mining a set of meaningful and distinctive topics automatically from massive text corpora has broad applications. Existing topic models, however, typically work in a purely unsupervised way, which often generate topics that do not fit users’ particular needs and yield suboptimal performance on downstream tasks. We propose a new task, discriminative topic mining, which leverages a set of user-provided category names to mine discriminative topics from text corpora. This new task not only helps a user understand clearly and distinctively the topics he/she is most interested in, but also benefits directly keyword-driven classification tasks. We develop CatE, a novel category-name guided text embedding method for discriminative topic mining, which effectively leverages minimal user guidance to learn a discriminative embedding space and discover category representative terms in an iterative manner. We conduct a comprehensive set of experiments to show that CatE mines high-quality set of topics guided by category names only, and benefits a variety of downstream applications including weakly-supervised classification and lexical entailment direction identification.",
        "link": "https://dl.acm.org/doi/10.1145/3366423.3380278"
    },
    {
        "title": "Comparative Study of Heart Disease Diagnosis Using Top Ten Data Mining Classification Algorithms",
        "authors": [
            "I. Ketut Agung Enriko"
        ],
        "date": "June 2019",
        "source": "ICFET '19: Proceedings of the 5th International Conference on Frontiers of Educational Technologies",
        "abstract": "Data mining has been used for many purposes, especially for prediction system. In healthcare, data mining algorithms often used in disease diagnosis. Meanwhile, heart disease is known as a primary cause of death over the years. Many studies have been performed in heart disease diagnosis using data mining methods. There are some popular data mining algorithms that can be used in heart disease diagnosis, for example, k-Nearest Neighbor, CART, and AdaBoost. The algorithms are used to analyze a sample of cardiovascular patients data and predict the heart disease type that they suffer. Some parameters are taken from the patient, including EKG morphology, blood pressure, and information about the existence of chest pain, shortness of breath, palpitation, and cold sweat. In this study, medical records data are collected from Harapan Kita Hospital and utilized as a dataset sample in this research. Top ten data mining classification algorithms are used in diagnosing heart disease from Harapan Kita Hospital data and examining their performance by checking the accuracy and speed.",
        "link": "https://dl.acm.org/doi/10.1145/3338188.3338220"
    },
    {
        "title": "Text Mining Approach for Identifying Research Trends",
        "authors": [
            "Snezhana Sulova"
        ],
        "date": "June 2021",
        "source": "CompSysTech '21: Proceedings of the 22nd International Conference on Computer Systems and Technologies",
        "abstract": "With the increase of unstructured data, the issues connected with automatic text processing, the categorization of documents and the discovery of topics have become objects of growing interest. In order to improve the process of grouping and processing research publications, we would like to propose a method based upon natural language processing. It is based on text mining technologies which aim to identify key tendencies in documents. It processes the content of publications by clustering and identifies the topics of each identified group. This analysis helps by identifying key tendencies as well as discovering emerging new areas of research. Publications from the research literature database, Scopus, were used to test the approach. The topic of the publications is “the application of digital technologies in the logistics business”. The experiments were completed using the RapidMiner Studio software.",
        "link": "https://dl.acm.org/doi/10.1145/3472410.3472433"
    },
    {
        "title": "Assessment of Vulnerability Severity using Text Mining",
        "authors": [
            "Georgios Spanos",
            "Lefteris Angelis",
            "Dimitrios Toloudis"
        ],
        "date": "September 2017",
        "source": "PCI '17: Proceedings of the 21st Pan-Hellenic Conference on Informatics",
        "abstract": "Software1 vulnerabilities are closely associated with information systems security, a major and critical field in today's technology. Vulnerabilities constitute a constant and increasing threat for various aspects of everyday life, especially for safety and economy, since the social impact from the problems that they cause is complicated and often unpredictable. Although there is an entire research branch in software engineering that deals with the identification and elimination of vulnerabilities, the growing complexity of software products and the variability of software production procedures are factors contributing to the ongoing occurrence of vulnerabilities, Hence, another area that is being developed in parallel focuses on the study and management of the vulnerabilities that have already been reported and registered in databases. The information contained in such databases includes, a textual description and a number of metrics related to vulnerabilities. The purpose of this paper is to investigate to what extend the assessment of the vulnerability severity can be inferred directly from the corresponding textual description, or in other words, to examine the informative power of the description with respect to the vulnerability severity. For this purpose, text mining techniques, i.e. text analysis and three different classification methods (decision trees, neural networks and support vector machines) were employed. The application of text mining to a sample of 70,678 vulnerabilities from a public data source shows that the description itself is a reliable and highly accurate source of information for vulnerability prioritization.",
        "link": "https://dl.acm.org/doi/10.1145/3139367.3139390"
    },
    {
        "title": "Clustering Algorithms for Spatial Data Mining",
        "authors": [
            "Chetashri Bhadane",
            "Ketan Shah"
        ],
        "date": "April 2020",
        "source": "ICGDA '20: Proceedings of the 2020 3rd International Conference on Geoinformatics and Data Analysis",
        "abstract": "With the advances in mobile and wireless technologies, there has been a rise in applications that track and share the users' geospatial data. People use several social networking sites such as Twitter, Facebook and Flickr, where they share their status updates. With the integration of Global Positioning System (GPS) with mobile phones, it is now possible to share one's locations on these social networks. GPS allows us to record and track a person's movement along with the timestamp. The data set obtained from these GPS logs is vast and is widely used to analyze the users' movement patterns. Specifically, we can find out significant locations based on the number of users present at that location and the time spent by them at such places. Once significant places have been identified, it is also possible to identify the semantic importance of these locations. This paper presents an overview of the clustering techniques used to find important places of interest using large GPS based mobility datasets. Four clustering algorithms, K-Means, DBSCAN, OPTICS and Hierarchical, are implemented, and performance is tested using real-time data of 50 users collected over 2--5 years. Performance summary depicts that K-Means and DBSCAN perform well for spatial data.",
        "link": "https://dl.acm.org/doi/10.1145/3397056.3397068"
    },
    {
        "title": "Feature Selection based Arabic Text Classification using Different Machine Learning Algorithms: Comparative Study",
        "authors": [
            "Sakina Rim Bennabi",
            "Zakaria Elberrichi"
        ],
        "date": "June 2020",
        "source": "ICIST '20: Proceedings of the 10th International Conference on Information Systems and Technologies",
        "abstract": "Feature selection is a method of data pre-processing widely used when mining large data, such as textual classification. Several studies have been conducted to compare the different methods of feature selection applied to corpora in English. Unfortunately, a small number of works concern the Arabic language. This article aims to present a comparative study of different feature selection techniques including: Chi2, the ANOVA method and mutual information, applied on a corpus in Arabic language, while also diversifying the machine learning algorithms (Naive Bayes, SVM and KNN). This experimental study has shown in general that reducing dimensionality with feature selection techniques has slightly affected the performance of textual classification, reducing the size of the corpus by up to 1%.",
        "link": "https://dl.acm.org/doi/10.1145/3447568.3448531"
    },
    {
        "title": "Intelligent Data Analysis using Optimized Support Vector Machine Based Data Mining Approach for Tourism Industry",
        "authors": [
            "Ms Promila Sharma",
            "Uma Meena",
            "Girish Kumar Sharma"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Data analysis involves the deployment of sophisticated approaches from data mining methods, information theory, and artificial intelligence in various fields like tourism, hospitality, and so on for the extraction of knowledge from the gathered and preprocessed data. In tourism, pattern analysis or data analysis using classification is significant for finding the patterns that represent new and potentially useful information or knowledge about the destination and other data. Several data mining techniques are introduced for the classification of data or patterns. However, overfitting, less accuracy, local minima, sensitive to noise are the drawbacks in some existing data mining classification methods. To overcome these challenges, Support vector machine with Red deer optimization (SVM-RDO) based data mining strategy is proposed in this article. Extended Kalman filter (EKF) is utilized in the first phase, i.e., data cleaning to remove the noise and missing values from the input data. Mantaray foraging algorithm (MaFA) is used in the data selection phase, in which the significant data are selected for the further process to reduce the computational complexity. The final phase is the classification, in which SVM-RDO is proposed to access the useful pattern from the selected data. PYTHON is the implementation tool used for the experiment of the proposed model. The experimental analysis is done to show the efficacy of the proposed work. From the experimental results, the proposed SVM-RDO achieved better accuracy, precision, recall, and F1 score than the existing methods for the tourism dataset. Thus, it is showed the effectiveness of the proposed SVM-RDO for pattern analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3494566"
    },
    {
        "title": "Applying text mining to predict learners' cognitive engagement",
        "authors": [
            "Hayati Hind",
            "Mohammed Khalidi Idrissi",
            "Samir Bennani"
        ],
        "date": "October 2017",
        "source": "SCAMS '17: Proceedings of the Mediterranean Symposium on Smart City Application",
        "abstract": "In academic process, engagement represent one of the important success' factor. This paper proposes a predictive system for learners' cognitive engagement based on their online discussion forums participation. In the first level the ontology OWL and the LSA method are used to perform a semantic classification system of the threads according to a specific context chosen by the tutor. Then the Text mining, as a predictive method, is applied to the classified threads and learners' participation in the forums.",
        "link": "https://dl.acm.org/doi/10.1145/3175628.3175655"
    },
    {
        "title": "Investigate Transitions into Drug Addiction through Text Mining of Reddit Data",
        "authors": [
            "John Lu",
            "Sumati Sridhar",
            "Ritika Pandey",
            "Mohammad Al Hasan",
            "Georege Mohler"
        ],
        "date": "July 2019",
        "source": "KDD '19: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Increasing rates of opioid drug abuse and heightened prevalence of online support communities underscore the necessity of employing data mining techniques to better understand drug addiction using these rapidly developing online resources. In this work, we obtained data from Reddit, an online collection of forums, to gather insight into drug use/misuse using text snippets from users narratives. Specifically, using users' posts, we trained a binary classifier which predicts a user's transitions from casual drug discussion forums to drug recovery forums. We also proposed a Cox regression model that outputs likelihoods of such transitions. In doing so, we found that utterances of select drugs and certain linguistic features contained in one's posts can help predict these transitions. Using unfiltered drug-related posts, our research delineates drugs that are associated with higher rates of transitions from recreational drug discussion to support/recovery discussion, offers insight into modern drug culture, and provides tools with potential applications in combating the opioid crisis.",
        "link": "https://dl.acm.org/doi/10.1145/3292500.3330737"
    },
    {
        "title": "Analysing digital banking reviews using text mining",
        "authors": [
            "Li Chen Cheng",
            "Legaspi Rhea Sharmayne"
        ],
        "date": "December 2020",
        "source": "ASONAM '20: Proceedings of the 12th IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Digital banks are new entrants in the banking industry in the Philippines as they only started late 2018. Since then, a handful of players have and are still emerging. With more and more people becoming technologically savvy, it is very critical for financial institutions to develop a digital banking application that will stand out from the competition. This paper aims to use text mining methods to analyse digital banking application reviews. This study will perform topic modelling using LDA to explore customer concerns and will mine association rules between the digital banking features with the review score. The results will reveal which areas the digital banking application can further optimize for customer satisfaction and retention.",
        "link": "https://dl.acm.org/doi/10.1109/ASONAM49781.2020.9381429"
    },
    {
        "title": "Mining Moocs Videos Metadata Using Classification Techniques",
        "authors": [
            "El Harrak Othman",
            "Ghadi Abderrahim",
            "El Bouhdidi Jaber"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "In few years, the internet has become the first source of information for most people, Today MOOCs makes it possible for everyone to access to the education over the world, it's represents an emerging methodology of online teaching and an important development in open education. Due to the rapid development of Moocs and the rapid growth of digital data and video database over the Internet, it is becoming very difficult for learners to browsing and choosing the best training for them. The scientific community has increased the amount of research into new technologies, with a view to improve the digital video utilization: its archiving, indexing, accessibility, acquisition, store and even its process and usability. All these parts of the video utilization entail the necessity of the extraction of all important information of a video, especially in cases of lack of metadata information. Web video mining is retrieving the content using data mining techniques from World Wide Web. There are two approaches for web video mining using traditional image processing (signal processing) and metadata based approach. In this paper, we present the various research makes in this subject and we propose an effective methodology to extract the metadata from Moocs videos and classify them based on the extracted metadata by applying data mining techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090450"
    },
    {
        "title": "Text mining with HathiTrust",
        "authors": [
            "Eleanor Dickson Koehl",
            "Ryan Dubnicek"
        ],
        "date": "June 2019",
        "source": "JCDL '19: Proceedings of the 18th Joint Conference on Digital Libraries",
        "abstract": "This tutorial will introduce attendees to the HathiTrust Research Center and its tools and services for computational text analysis research. HTRC leverages the scope and scale of the HathiTrust Digital Library collection to create opportunities for researchers to perform text data mining on subsets of the corpus. Attendees will develop skills that will allow them to conduct basic text analysis research using HathiTrust data, learn techniques for engaging with scholars on their text data mining research, and explore strategies for computational access to large-scale digital collections.",
        "link": "https://dl.acm.org/doi/10.1109/JCDL.2019.00115"
    },
    {
        "title": "Text mining for plagiarism detection: multivariate pattern detection for recognition of text similarities",
        "authors": [
            "Konstantinos Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2018",
        "source": "ASONAM '18: Proceedings of the 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "The problem of plagiarism the recent years has been intensified by the availability of information in digital form and the accessibility of the electronic libraries through the Internet. As a result, plagiarism detection has been transformed into a big data analytics problem since the number of digital sources is extravagant and a new document needs to be compared with millions of other existing documents. In this paper, a text mining methodology is proposed that can detect all common patterns between a document and the documents in a reference database. The technique is based on a pattern detection algorithm and the corresponding data structure that enables the algorithm to detect all common patterns. The methodology has been applied in a well-defined dataset providing very promising results identifying difficult cases of plagiarism such as technical disguise.",
        "link": "https://dl.acm.org/doi/10.5555/3382225.3382424"
    },
    {
        "title": "Petroleum Engineering Data Text Classification Using Convolutional Neural Network Based Classifier",
        "authors": [
            "Tzuhan Hsu",
            "Yaoqin Zhang"
        ],
        "date": "May 2018",
        "source": "ICMLT '18: Proceedings of the 2018 International Conference on Machine Learning Technologies",
        "abstract": "In this paper, authors introduce a convolutional neural network (CNN) based classification approach for petroleum engineering event. Recently, text classification problem has raised substantial attention in the field of machine learning. The aim of text classification process is to assign correct label to a text on the basis of its content. However, due to the imprecise of feature representation, most of former text classification methods cannot export satisfying result. To solve this problem, authors propose a model based on convolutional neural network. Authors conduct experiments with real world petroleum engineering data text, artificially labelled into 6 categories. Beside purposed CNN-based model, the authors also implement other baseline machine learning approaches: k nearest neighbor, random forest, support vector machines, recurrent neural network, gated recurrent units, long short-term memory. Among all these models, our CNN-based model reach best performance in text classification task. In the last part of this paper, authors compare and analysis their purpose and baseline model.",
        "link": "https://dl.acm.org/doi/10.1145/3231884.3231898"
    },
    {
        "title": "Classifying Indonesian Online Articles as Advertisement Placement Base Using Text Mining",
        "authors": [
            "Nadhira Tasya",
            "Arian Dhini"
        ],
        "date": "July 2017",
        "source": "ICBIM 2017: Proceedings of the International Conference on Business and Information Management",
        "abstract": "Rapid development in technological aspect resulting in growing level of human needs for the latest news, so that emerged a new trend of publishing and accessing news through online media or usually called online journalism. In addition, the number of people who sell and purchase through online sites also continues to increase and this opportunity is utilized by the company and the advertiser by implementing targeted web advertising. However, the high number of articles that have been published and accessed leads to great opportunities for errors in determining where to place the ads. Therefore, it needs a system that can categorize articles accessed by users as the basis of advertisement placement by the company and this classification system can be done by applying the method of Data Mining and Text Mining. This research uses document data in the form of article content that will be categorized into twenty categories of class of advertisement by using Text Mining technique with Support Vector Machine algorithm. The results of this study may be used by companies or advertisers as a basis for placement of ads on selected online media sites.",
        "link": "https://dl.acm.org/doi/10.1145/3134271.3134288"
    },
    {
        "title": "Mining Free-Text Medical Notes for Suicide Risk Assessment",
        "authors": [
            "Marios Adamou",
            "Grigoris Antoniou",
            "Elissavet Greasidou",
            "Vincenzo Lagani",
            "Paulos Charonyktakis",
            "Ioannis Tsamardinos"
        ],
        "date": "July 2018",
        "source": "SETN '18: Proceedings of the 10th Hellenic Conference on Artificial Intelligence",
        "abstract": "Suicide has been considered as an important public health issue for a very long time, and is one of the main causes of death worldwide. Despite suicide prevention strategies being applied, the rate of suicide has not changed substantially over the past decades. Advances in machine learning make it possible to attempt to predict suicide based on the analysis of relevant data to inform clinical practice. This paper reports on findings from the analysis of data of patients who died by suicide in the period 2013-2016 and made use of both structured data and free-text medical notes. We focus on examining various text-mining approaches to support risk assessment. The results show that using advance machine learning and text-mining techniques, it is possible to predict within a specified period which people are most at risk of taking their own life at the time of referral to a mental health service.",
        "link": "https://dl.acm.org/doi/10.1145/3200947.3201020"
    },
    {
        "title": "Integration of data mining techniques in e-learning systems: Clustering Profil of Lerners and Recommender Course System",
        "authors": [
            "A. El Moustamid",
            "E. En-Naimi",
            "J. El Bouhdidi"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "Data mining aims at the extraction of knowledge from large quantities of data by automatic or semi-automatic methods. In the field of education and training e-learning, Data Mining techniques as clustering (The aim of this technique is to identify groups that are similar in some aspect) [21], prediction (is used for developing model where combination of data viewed from different aspects can effect single viewed data. Here, is important to watch how predictors variables are effecting predicted variable)[21], classification are applied in e-learning to improve the level of learners by providing learning according to the learners' requirement) are used to analyze learners profile, to predict and improve student performance. The aim of our project is to develop a system capable of analyzing learners profile and indexing web videos in order to offer learners a rich database with courses that correspond to their levels.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090453"
    },
    {
        "title": "Multi-level mining and visualization of scientific text collections: Exploring a bi-lingual scientific repository",
        "authors": [
            "Pablo Accuosto",
            "Francesco Ronzano",
            "Daniel Ferrés",
            "Horacio Saggion"
        ],
        "date": "December 2017",
        "source": "WOSP 2017: Proceedings of the 6th International Workshop on Mining Scientific Publications",
        "abstract": "We present a system to mine and visualize collections of scientific documents by semantically browsing information extracted from single publications or aggregated throughout corpora of articles. The text mining tool performs deep analysis of document collections allowing the extraction and interpretation of research paper's contents. In addition to the extraction and enrichment of documents with metadata (titles, authors, affiliations, etc), the deep analysis performed comprises semantic interpretation, rhetorical analysis of sentences, triple-based information extraction, and text summarization. The visualization components allow geographical-based exploration of collections, topic-evolution interpretation, and collaborative network analysis among others. The paper presents a case study of a bi-lingual collection in the field of Natural Language Processing (NLP).",
        "link": "https://dl.acm.org/doi/10.1145/3127526.3127529"
    },
    {
        "title": "Hierarchical Topic Mining via Joint Spherical Tree and Text Embedding",
        "authors": [
            "Yu Meng",
            "Yunyi Zhang",
            "Jiaxin Huang",
            "Yu Zhang",
            "Chao Zhang",
            "Jiawei Han"
        ],
        "date": "August 2020",
        "source": "KDD '20: Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Mining a set of meaningful topics organized into a hierarchy is intuitively appealing since topic correlations are ubiquitous in massive text corpora. To account for potential hierarchical topic structures, hierarchical topic models generalize flat topic models by incorporating latent topic hierarchies into their generative modeling process. However, due to their purely unsupervised nature, the learned topic hierarchy often deviates from users' particular needs or interests. To guide the hierarchical topic discovery process with minimal user supervision, we propose a new task, Hierarchical Topic Mining, which takes a category tree described by category names only, and aims to mine a set of representative terms for each category from a text corpus to help a user comprehend his/her interested topics. We develop a novel joint tree and text embedding method along with a principled optimization procedure that allows simultaneous modeling of the category tree structure and the corpus generative process in the spherical space for effective category-representative term discovery. Our comprehensive experiments show that our model, named JoSH, mines a high-quality set of hierarchical topics with high efficiency and benefits weakly-supervised hierarchical text classification tasks.",
        "link": "https://dl.acm.org/doi/10.1145/3394486.3403242"
    },
    {
        "title": "Unsupervised behavioural mining and clustering for malware family identification",
        "authors": [
            "Khanh Huu The Dam",
            "Thomas Given-Wilson",
            "Axel Legay"
        ],
        "date": "March 2021",
        "source": "SAC '21: Proceedings of the 36th Annual ACM Symposium on Applied Computing",
        "abstract": "More accurate and advanced detection and classification of malware requires exploiting program behaviour and not merely syntactic or static features. One approach is to use system call dependency graphs (SCDGs) that represent the program behaviour by interactions with the system, and the relations between these interactions. These SCDGs have been used with supervised learning techniques to very accurately detect and classify malware. This works considers the unsupervised learning challenge of mining for common clusters of behaviour without a priori knowledge. This allows for clustering of similar programs by behaviour, that can then be used for either classification or further analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3412841.3441919"
    },
    {
        "title": "Text Mining for Evaluating Authors' Birth and Death Years",
        "authors": [
            "Dror Moghaz",
            "Yaakov Hacohen-Kerner",
            "Dov Gabbay"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "This article presents a unique method in text and data mining for finding the era, i.e., mining temporal data, in which an anonymous author was living. Finding this era can assist in the examination of a fake document or extracting the time period in which a writer lived. The study and the experiments concern Hebrew, and in some parts, Aramaic and Yiddish rabbinic texts. The rabbinic texts are undated and contain no bibliographic sections, posing an interesting challenge. This work proposes algorithms using key phrases and key words that allow the temporal organization of citations together with linguistic patterns. Based on these key phrases, key words, and the references, we established several types of “Iron-clad,” Heuristic and Greedy rules for estimating the years of birth and death of a writer in an interesting classification task. Experiments were conducted on corpora, including documents authored by 12, 24, and 36 rabbinic writers and demonstrated promising results.",
        "link": "https://dl.acm.org/doi/10.1145/3281631"
    },
    {
        "title": "Transfer Learning to Timed Text Based Video Classification Using CNN",
        "authors": [
            "Zenun Kastrati",
            "Ali Shariq Imran",
            "Arianit Kurti"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "Open educational video resources are gaining popularity with a growing number of massive open online courses (MOOCs). This has created a niche for content providers to adopt effective solutions in automatically organizing and structuring of educational resources for maximum visibility. Recent advances in deep learning techniques are proving useful in managing and classifying resources into appropriate categories. This paper proposes one such convolutional neural network (CNN) model for classifying video lectures in a MOOC setting using a transfer learning approach. The model uses a time-aligned text transcripts corresponding to video lectures from six broader subject categories. Video lectures and their corresponding transcript dataset is gathered from the Coursera MOOC platform. Two different CNN models are proposed: i) CNN based classification using embeddings learned from our MOOC dataset, ii) CNN based classification using transfer learning. Word embeddings generated from two well known state-of-the-art pre-trained models Word2Vec and GloVe, are used in the transfer learning approach for the second case. The proposed CNN models are evaluated using precision, recall, and F1 score and the obtained performance is compared with both conventional and deep learning classifiers. The proposed CNN models have an F1 score improvement of 10-22 percentage points over DNN and conventional classifiers",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326483"
    },
    {
        "title": "Evolutionary role mining in complex networks by ensemble clustering",
        "authors": [
            "Sarvenaz Choobdar",
            "Pedro Ribeiro",
            "Fernando Silva"
        ],
        "date": "April 2017",
        "source": "SAC '17: Proceedings of the Symposium on Applied Computing",
        "abstract": "The structural patterns in the neighborhood of nodes assign unique roles to the nodes. Mining the set of existing roles in a network provides a descriptive profile of the network and draws its general picture. This paper proposes a new method to determine structural roles in a dynamic network based on the current position of nodes and their historic behavior. We develop a temporal ensemble clustering technique to dynamically find groups of nodes, holding similar tempo-structural roles. We compare two weighting functions, based on age and distribution of data, to incorporate temporal behavior of nodes in the role discovery. To evaluate the performance of the proposed method, we assess the results from two points of view: 1) goodness of fit to current structure of the network; 2) consistency with historic data. We conduct the evaluation using different ensemble clustering techniques. The results on real world networks demonstrate that our method can detect tempo-structural roles that simultaneously depict the topology of a network and reflect its dynamics with high accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3019612.3019815"
    },
    {
        "title": "Analyzing used-car web listings via text mining",
        "authors": [
            "Ayhan Demiriz",
            "Fatma Cantaş"
        ],
        "date": "October 2017",
        "source": "IML '17: Proceedings of the 1st International Conference on Internet of Things and Machine Learning",
        "abstract": "Used car trade is one of the major components of the world economies. It is not uncommon to sell a car by placing an internet advertisement irrespective of the geography in these days. A typical content of an advertisement is usually composed of two parts namely the structured and the free text data. The structured data may include some information about the asking price, make, model, year, mileage of the car and the contact info. In most cases, seller may give important clues about the car's current conditions in the free text data where the title (head) of the advertisement can be included as free text too. This paper reports preliminary results from a text mining study conducted on 75K used car internet listings collected from two major car listing web sites in Turkey. As expected, the words and the phrases related to the description of the car are observed to be frequent. The leading concepts in the free text are found to be regarding how to describe the current condition of a car, for example \"no crash history\".",
        "link": "https://dl.acm.org/doi/10.1145/3109761.3109782"
    },
    {
        "title": "Text classification of Diseases Treated by Traditional Chinese Medicine Prescription based on machine learning",
        "authors": [
            "Hanqing Zhao",
            "Bo Han",
            "Chen Li"
        ],
        "date": "September 2020",
        "source": "ISAIMS '20: Proceedings of the 1st International Symposium on Artificial Intelligence in Medical Sciences",
        "abstract": "OBJECTIVE: To explore the application of machine learning in th e identification of diseases treated by traditional Chinese medicine prescriptions. METHODS: Based on the composition of the text document of Chinese medicine prescriptions, the prescriptions were divided into cough, headache and diarrhea. THUTCT were introduced to establish and train two machine learning text classification models, LibSVM and LibLinear, and the prescriptions to be deter mined were put into the model for classification and prediction. R ESULTS: The Precision rate of LibSVM model and Liblinear mod el were 0.7283 and 0.6690. Seven prescriptions were classified an d predicted, and the results were in line with expectations. CONCLUSION: THUCTC has good universality for the content of TCM prescriptions, high classification accuracy and fast testing speed, which is suitable for the text classification and discrimination research of TCM prescriptions for diseases.",
        "link": "https://dl.acm.org/doi/10.1145/3429889.3429895"
    },
    {
        "title": "Evaluation of retweet clustering method classification method using retweets on Twitter without text data",
        "authors": [
            "K. Uchida",
            "F. Toriumi",
            "T. Sakaki"
        ],
        "date": "August 2017",
        "source": "WI '17: Proceedings of the International Conference on Web Intelligence",
        "abstract": "Burst phenomena, which frequently occur on social media, are caused by such social events as flaming on the internet, elections, and natural disasters. To understand people's thoughts and feelings, we must classify their opinions from burst phenomena. Therefore, classification methods that categorize tweets are critical. However, since most classification methods focus on text mining, they cannot group tweets by topics because each tweet has poor linguistic similarities. We used a non-text-based classification method proposed by Baba et al. that groups tweets by topics, even if they have poor linguistic similarities, and verified its validity by comparing it with a text-based classification method in two different evaluations: qualitative and quantitative. In the qualitative evaluation part, we did a questionnaire survey and validated the suitability of the topic clusters created using both the non-and text-based methods. Since evaluating the similarity of every pair of tweets in each topic is difficult, we evaluated the similarity between sampled pairs in the survey and acquired more appropriate topic clustering results using the non-text-based method than the text-based method. In the quantitative evaluation part, we focused on the robustness of each method against data reduction. Many approaches analyze social media data, especially because collecting data from social media is comparatively easy. However, since collecting the whole data of burst phenomena is very costly due to the vast amounts of available social media data, robustness against data reduction is an important index to evaluate classification methods. With the non-text-based method, over 55% of the pairs of tweets in the same cluster were also included in the same cluster even when the data were reduced to 10% in all three of our example cases. In this paper, as a source we focus on Twitter, one of the most popular microblogging services. Using clustering to conduct detailed case analyses, we scrutinized three burst cases that include natural disasters and flaming on the internet and found that a non-text-based method more effectively classified tweets in burst phenomena than a text-based method.",
        "link": "https://dl.acm.org/doi/10.1145/3106426.3106451"
    },
    {
        "title": "Using Spark for Text Mining on Large Scale Liver Cancer Literature",
        "authors": [
            "Ming-Yen Lin",
            "Yu-Ju Lin",
            "Sue-Chen Hsueh"
        ],
        "date": "January 2021",
        "source": "BDET 2021: 2021 the 3rd International Conference on Big Data Engineering and Technology (BDET)",
        "abstract": "Cancer is one of the main causes of death. The number of known cancer to-date is more than one hundred. Liver cancer, ranked after trachea and lung cancer, has long been high in the cancer leading cause in Taiwan and even ranked as second in 2016. The number of scientific articles related to cancer proliferates every year. The number reaches as high as 20 million in PubMed so that discovering useful information from the massive collection is very difficult. In addition, using a single machine to sift through these articles is very time-consuming. Therefore, we present a big data analytic framework using the distributed Apache Spark platform for text mining in PubMed literature. We establish a prediction model for liver cancer articles so that researchers may effectively validate whether an article is related to liver cancer or not. Classification models in Spark MLlib including Linear Support Vector Machines (SVM), Logistic Regression, are used in our experiments. Relevancy to liver cancer is further confirmed by using MeSH (Medical Subject Headings) terms. Logistic regression is about 3 times faster than SVMs and the accuracy of both methods is close to 95% in the experiments using hold-out validation. When max_features is 500 and min_df ≤ 0.1 (or min_df = 1), the accuracy may reach 96%. In the experiments with K-fold cross-validation, the accuracy of SVMs methods is 96%. The experimental results show that that our prediction model may effectively classify liver cancer articles.",
        "link": "https://dl.acm.org/doi/10.1145/3474944.3474958"
    },
    {
        "title": "Automatic topic classification of test cases using text mining at an Android smartphone vendor",
        "authors": [
            "Junji Shimagaki",
            "Yasutaka Kamei",
            "Naoyasu Ubayashi",
            "Abram Hindle"
        ],
        "date": "October 2018",
        "source": "ESEM '18: Proceedings of the 12th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Background: An Android smartphone is an ecosystem of applications, drivers, operating system components, and assets. The volume of the software is large and the number of test cases needed to cover the functionality of an Android system is substantial. Enormous effort has been already taken to properly quantify \"what features and apps were tested and verified?\". This insight is provided by dashboards that summarize test coverage and results per feature. One method to achieve this is to manually tag or label test cases with the topic or function they cover, much like function points. At the studied Android smartphone vendor, tests are labelled with manually defined tags, so-called \"feature labels (FLs)\", and the FLs serve to categorize 100s to 1000s test cases into 10 to 50 groups. Aim: Unfortunately for developers, manual assignment of FLs to 1000s of test cases is a time consuming task, leading to inaccurately labeled test cases, which will render the dashboard useless. We created an automated system that suggests tags/labels to the developers for their test cases rather than manual labeling. Method: We use machine learning models to predict and label the functionality tested by 10,000 test cases developed at the company. Results: Through the quantitative experiments, our models achieved acceptable F-1 performance of 0.3 to 0.88. Also through the qualitative studies with expert teams, we showed that the hierarchy and path of tests was a good predictor of a feature's label. Conclusions: We find that this method can reduce tedious manual effort that software developers spent classifying test cases, while providing more accurate classification results.",
        "link": "https://dl.acm.org/doi/10.1145/3239235.3268927"
    },
    {
        "title": "To apply Data Mining for Classification of Crowd sourced Software Requirements",
        "authors": [
            "Soonh Taj",
            "Qasim Arain",
            "Imran Memon",
            "Asma Zubedi"
        ],
        "date": "April 2019",
        "source": "ICSIE '19: Proceedings of the 8th International Conference on Software and Information Engineering",
        "abstract": "Now a day's main focus of developers is to build quality software that works according to customer needs and for this reason it is necessary to gather right requirements as requirement elicitation is the critical step that impacts on the success of software project as misinterpreted requirements leads to the failure of software project. By keeping this in mind a research is carried out on improving requirements elicitation process and automating the process of classifying requirements. In this research, a model is proposed which will help in this scenario for requirements elicitation and requirement classification. This paper presents a model in which crowd sourcing approach is used so that customers, end users, stakeholders, developers and software engineers can make active participation for requirement elicitation process and requirements gathered using crowdsourcing approach are used by model for classification process i.e. classification of requirements into functional and non-functional requirements. For the proof of proposed model a case study is conducted. Results of case study provided the usefulness and efficiency of proposed model for classification of crowd sourced software requirements.",
        "link": "https://dl.acm.org/doi/10.1145/3328833.3328837"
    },
    {
        "title": "Text classification using Fuzzy TF-IDF and Machine Learning Models",
        "authors": [
            "Mariem Bounabi",
            "Karim El Moutaouakil",
            "Khalid Satori"
        ],
        "date": "October 2019",
        "source": "BDIoT'19: Proceedings of the 4th International Conference on Big Data and Internet of Things",
        "abstract": "The representation of the information has an important impact on the text classification task. Several weighting methods were proposed in the literature, and the term frequency-inverse term frequency (TFIDF), the most know on the text treatment field. The FTF-IDF is a vector representation where the components of the TFIDF are presented as inputs to the Fuzzy Inference System (FIS). In this work, we compare several Machin Learning algorithms such as Naïve Bayes and its derivatives, SVM and Random forest classifiers, using the FTF-IDF representation. To improve the quality of the used classifiers, we call sum attribute selection methods. The recognition rate, for the tested systems, is satisfied, where the system based on naïve Bayes classifier, the FTF-IDF weighting terms, and the info gain select attributes method gives 98.7% as accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3372938.3372956"
    },
    {
        "title": "Text Mining in Cybersecurity: A Systematic Literature Review",
        "authors": [
            "Luciano Ignaczak",
            "Guilherme Goldschmidt",
            "Cristiano André Da Costa",
            "Rodrigo Da Rosa Righi"
        ],
        "date": null,
        "source": "ACM Computing Surveys",
        "abstract": "The growth of data volume has changed cybersecurity activities, demanding a higher level of automation. In this new cybersecurity landscape, text mining emerged as an alternative to improve the efficiency of the activities involving unstructured data. This article proposes a Systematic Literature Review (SLR) to present the application of text mining in the cybersecurity domain. Using a systematic protocol, we identified 2,196 studies, out of which 83 were summarized. As a contribution, we propose a taxonomy to demonstrate the different activities in the cybersecurity domain supported by text mining. We also detail the strategies evaluated in the application of text mining tasks and the use of neural networks to support activities involving unstructured data. The work also discusses text classification performance aiming its application in real-world solutions. The SLR also highlights open gaps for future research, such as the analysis of non-English content and the intensification in the usage of neural networks.",
        "link": "https://dl.acm.org/doi/10.1145/3462477"
    },
    {
        "title": "Using Text Mining to Analyze the Financial Patents",
        "authors": [
            "Yung-Feng Lu",
            "Jia-Lang Xu",
            "Mu-Yen Chen"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "With the advancement and development of science and technology, Nowadays society is the development of information technology, cloud computing, mobile Internet, Internet of Things, block chain and information technology.The combination of finance and technology has made financial technology terms gradually emerging.. This research analyzes the patents of financial technology. However, Taiwan's financial technology patents are mostly G06Q-20 (payment plan, architecture or agreement), G06Q-30 (commercial, such as marketing, shopping, payment, auction or e-commerce). G06Q-40 (financial, investment or tax treatment, insurance).The research use text mining technology to analyze and understand Taiwan's trends and developments in financial technology. The research results show that G06Q-20 patents are biased towards information security and mobile payment. G06Q-30 patents favor shopping websites and advertising. G06Q-40 patents are biased towards diversity, such as securities, insurance, finance, and stocks.",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326478"
    },
    {
        "title": "Identifying Online Profiles of Distance Learning Students Using Data Mining Techniques",
        "authors": [
            "Osama Islam",
            "Muazzam Siddiqui",
            "Naif Radi Aljohani"
        ],
        "date": "October 2019",
        "source": "ICDTE '19: Proceedings of the 3rd International Conference on Digital Technology in Education",
        "abstract": "Educational data has grown over the years with the increased use of technology within educational environments. This has led to a huge amount of data being stored in various data sources representing the student, his/her activities, and other aspects relevant to the learning process. To meet this analytical need, Educational Data Mining (EDM) has emerged to assist educational institutions in identifying key benefits such as students at risk, the level of student engagement or predicting student performance. The aim of this research was to explore the various aspects of student interaction data using data mining techniques to identify relevant patterns of behaviors and possible key attributes that have higher degrees of influence on distance learning students. The main findings identified several patterns of user online profiles based on a set of adopted learning strategies, the research proposed a framework for analyzing such interaction data based on R and Hadoop platforms to correlate online profiles with student performance.",
        "link": "https://dl.acm.org/doi/10.1145/3369199.3369249"
    },
    {
        "title": "Extremely Fast Decision Tree Mining for Evolving Data Streams",
        "authors": [
            "Albert Bifet",
            "Jiajin Zhang",
            "Wei Fan",
            "Cheng He",
            "Jianfeng Zhang",
            "Jianfeng Qian",
            "Geoff Holmes",
            "Bernhard Pfahringer"
        ],
        "date": "August 2017",
        "source": "KDD '17: Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
        "abstract": "Nowadays real-time industrial applications are generating a huge amount of data continuously every day. To process these large data streams, we need fast and efficient methodologies and systems. A useful feature desired for data scientists and analysts is to have easy to visualize and understand machine learning models. Decision trees are preferred in many real-time applications for this reason, and also, because combined in an ensemble, they are one of the most powerful methods in machine learning. In this paper, we present a new system called STREAMDM-C++, that implements decision trees for data streams in C++, and that has been used extensively at Huawei. Streaming decision trees adapt to changes on streams, a huge advantage since standard decision trees are built using a snapshot of data, and can not evolve over time. STREAMDM-C++ is easy to extend, and contains more powerful ensemble methods, and a more efficient and easy to use adaptive decision trees. We compare our new implementation with VFML, the current state of the art implementation in C, and show how our new system outperforms VFML in speed using less resources.",
        "link": "https://dl.acm.org/doi/10.1145/3097983.3098139"
    },
    {
        "title": "Data miners' little helper: data transformation activity cues for cluster analysis on document collections",
        "authors": [
            "Tania Cerquitelli",
            "Evelina Di Corso",
            "Francesco Ventura",
            "Silvia Chiusano"
        ],
        "date": "June 2017",
        "source": "WIMS '17: Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "In this paper we propose a new self-learning engine to streamline the analytics process, as it enables analysts to mine massive data repositories with minimal user intervention. In the context of cluster analysis on a collection of documents this new system, named SELF-DATA (SELF-learning DAta TrAnsformation), suggests to the analyst how to configure the whole mining process for a given dataset. SELF-DATA relies on an engine exploring different data weighting schemas (e.g., normalized term frequencies) and data transformation methods (e.g., PCA) before applying the cluster analysis, evaluating and comparing solutions through different quality indices (e.g., weighted Silhouette), and presenting the k-top solutions to the analyst. SELF-DATA will also include a knowledge base storing results of experiments on previously processed datasets, and a classification algorithm trained on the knowledge base content to forecast the best configuration for the whole mining process for an unexplored dataset. The first development of SELF-DATA running on Apache Spark has been validated on 5 collections of documents. Experimental results highlight that TF-IDF and logarithmic entropy are effective to measure item relevance with sparse datasets, and the LSI method outperforms PCA with a large dictionary.",
        "link": "https://dl.acm.org/doi/10.1145/3102254.3102288"
    },
    {
        "title": "Interactive Visual Graph Mining and Learning",
        "authors": [
            "Ryan A. Rossi",
            "Nesreen K. Ahmed",
            "Rong Zhou",
            "Hoda Eldardiry"
        ],
        "date": null,
        "source": "ACM Transactions on Intelligent Systems and Technology",
        "abstract": "This article presents a platform for interactive graph mining and relational machine learning called GraphVis. The platform combines interactive visual representations with state-of-the-art graph mining and relational machine learning techniques to aid in revealing important insights quickly as well as learning an appropriate and highly predictive model for a particular task (e.g., classification, link prediction, discovering the roles of nodes, and finding influential nodes). Visual representations and interaction techniques and tools are developed for simple, fast, and intuitive real-time interactive exploration, mining, and modeling of graph data. In particular, we propose techniques for interactive relational learning (e.g., node/link classification), interactive link prediction and weighting, role discovery and community detection, higher-order network analysis (via graphlets, network motifs), among others. GraphVis also allows for the refinement and tuning of graph mining and relational learning methods for specific application domains and constraints via an end-to-end interactive visual analytic pipeline that learns, infers, and provides rapid interactive visualization with immediate feedback at each change/prediction in real-time. Other key aspects include interactive filtering, querying, ranking, manipulating, exporting, as well as tools for dynamic network analysis and visualization, interactive graph generators (including new block model approaches), and a variety of multi-level network analysis techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3200764"
    },
    {
        "title": "Data Mining and Opinion Mining: A Tool in Educational Context",
        "authors": [
            "Myriam Peñafiel",
            "Stefanie Vásquez",
            "Diego Vásquez",
            "Juan Zaldumbide",
            "Sergio Luján-Mora"
        ],
        "date": "July 2018",
        "source": "ICoMS '18: Proceedings of the 2018 1st International Conference on Mathematics and Statistics",
        "abstract": "The use of the web as a universal communication platform generates large volumes of data (Big data), which in many cases, need to be processed so that they can become useful knowledge in face of the sceptics who have doubts about the credibility of such information. The use of web data that comes from educational contexts needs to be addressed, since that large amount of unstructured information is not being valued, losing valuable information that can be used. To solve this problem, we propose the use of data mining techniques such as sentiment analysis to validate the information that comes from the educational platforms. The objective of this research is to propose a methodology that allows the user to apply sentiment analysis in a simple way, because although some researchers have done it, very few do with data in the educational context. The results obtained prove that the proposal can be used in similar cases.",
        "link": "https://dl.acm.org/doi/10.1145/3274250.3274263"
    },
    {
        "title": "Research Hotspots Mining and Visualized Analysis Based on Linking Cluster and K-Core Decomposition",
        "authors": [
            "Ying Cai",
            "Fang Huang",
            "Shuai Wang",
            "Hao Zhang",
            "Chunxiu Du"
        ],
        "date": "May 2018",
        "source": "ICDPA 2018: Proceedings of the International Conference on Data Processing and Applications",
        "abstract": "In this paper, we focus on research hotspots mining with knowledge semantic features on discipline topic word networks, which are extracted from scientific and technical documents. For the topic networks, a novel method of combining linking community clustering with k-core decomposition is proposed to mine the hotspot community with knowledge hierarchy structure. And as a post-processing, the hierarchical community identification is implemented by k-core decomposition in order to discover the internal connection with knowledge semantic hierarchy in the research hotspot. Finally, the community density and visualization method is utilized to analyze the core and sub- problem in the research hotspot community. In experiments, the topic networks for a single discipline and interdisciplinary research hotspot are taken as examples of analysis, and the results show the proposed approach can effectively realize the objective of mining and analyzing the discipline research hotspot on the academic topic word network.",
        "link": "https://dl.acm.org/doi/10.1145/3224207.3224216"
    },
    {
        "title": "The Application of Data Mining Algorithms in the Construction of Travel Recommendation System",
        "authors": [
            "Chao Lou",
            "Zhaonan Mu",
            "Mengzhu Liu"
        ],
        "date": "January 2021",
        "source": "CONF-CDS 2021: The 2nd International Conference on Computing and Data Science",
        "abstract": "Photos shared by users of online social network often associated with tags, texts, geographic and time information. These data are ideal research resource which can be used by researchers to observe people's interests and behaviors. They can also be used to analyze people's travel behaviors and interests because it can reflect people's travel activities and experiences directly. This project aimed to consolidate people into groups based on their travel preferences of travel season, travel time span, scenery type of travel location and then analyze popular travel places for different people groups. The travel preferences were analyzed from extracted data applying PHP language and Naive Bayes classification approach. According to the analyzed data, people were divided into groups. Then, DBSCAN clustering was used to get popular locations based on the different groups. As a result, 80 thousand photo data of 5 cities have been collected from Flickr by Phython. It has been showed that people's preference of season in most of the mined cities was spring and summer and the preference of time span was short. Moreover, mined people has been divided into 16 groups according to their preference and based on the groups popular locations have been calculated by DBSCAN clustering. Theses locations have been provided in a recommendation website which was built in this project. For the future work, mining Flickr data associated with other travel social network sites could be a good way to build a more effective recommendation system.",
        "link": "https://dl.acm.org/doi/10.1145/3448734.3450477"
    },
    {
        "title": "Compare Machine Learning Models in Text Classification Using Steam User Reviews",
        "authors": [
            "Youchen Miao",
            "Zeyu Jin",
            "Yumeng Zhang",
            "Yuchen Chen",
            "Junren Lai"
        ],
        "date": "November 2021",
        "source": "ICSED '21: Proceedings of the 2021 3rd International Conference on Software Engineering and Development",
        "abstract": "Text Classification and Sentiment Analysis of game reviews are viewed as important parts in not only academic fields but also in game studies. In this paper, with more than 400 thousand game reviews on Steam platform, we preprocess the data using different libraries (sklearn, nltk, and spaCy) and use them as inputs to build three sentiment classification models based on different algorithms (Naive Bayes, SVM, and Random Forest). In contrast to previous studies that only focus on different sentiment analysis models, our paper also highlights the use of different APIs to preprocess the data and their corresponding model performance. The results show that no matter which API we choose, Random Forest models always perform the best. However, in terms of training time, Naive Bayes is the fastest. This work can be used to apply grid search for researchers to automatically find the optimum API before conducting sentiment analysis in the future.",
        "link": "https://dl.acm.org/doi/10.1145/3507473.3507480"
    },
    {
        "title": "Lost in Transduction: Transductive Transfer Learning in Text Classification",
        "authors": [
            "Alejandro Moreo",
            "Andrea Esuli",
            "Fabrizio Sebastiani"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Obtaining high-quality labelled data for training a classifier in a new application domain is often costly. Transfer Learning\n(a.k.a. “Inductive Transfer”) tries to alleviate these costs by transferring, to the “target” domain of interest, knowledge available from a different “source” domain. In transfer learning the lack of labelled information from the target domain is compensated by the availability at training time of a set of unlabelled examples from the target distribution. Transductive Transfer Learning denotes the transfer learning setting in which the only set of target documents that we are interested in classifying is known and available at training time. Although this definition is indeed in line with Vapnik’s original definition of “transduction”, current terminology in the field is confused. In this article, we discuss how the term “transduction” has been misused in the transfer learning literature, and propose a clarification consistent with the original characterization of this term given by Vapnik. We go on to observe that the above terminology misuse has brought about misleading experimental comparisons, with inductive transfer learning methods that have been incorrectly compared with transductive transfer learning methods. We then, give empirical evidence that the difference in performance between the inductive version and the transductive version of a transfer learning method can indeed be statistically significant (i.e., that knowing at training time the only data one needs to classify indeed gives an advantage). Our clarification allows a reassessment of the field, and of the relative merits of the major, state-of-the-art algorithms for transfer learning in text classification.",
        "link": "https://dl.acm.org/doi/10.1145/3453146"
    },
    {
        "title": "Machine Learning on Mining Potential Adverse Drug Reactions for Pharmacovigilance",
        "authors": [
            "Ke Jia"
        ],
        "date": "July 2021",
        "source": "DSIT 2021: 2021 4th International Conference on Data Science and Information Technology",
        "abstract": "Drugs are related to the lives and health of patients. Mining and discovering potential adverse drug reactions from a large number of unstructured texts play an important role in drug research and pharmacovigilance. It is a hot research issue in the field of biomedicine in recent years. A large number of scholars have discovered a large amount of information on adverse drug reactions from different data sources through effective machine learning methods. This paper reviews the research progress in recent years to provide a reference for future research.",
        "link": "https://dl.acm.org/doi/10.1145/3478905.3478964"
    },
    {
        "title": "TACAM: Topic And Context Aware Argument Mining",
        "authors": [
            "Michael Fromm",
            "Evgeniy Faerman",
            "Thomas Seidl"
        ],
        "date": "October 2019",
        "source": "WI '19: IEEE/WIC/ACM International Conference on Web Intelligence",
        "abstract": "In this work we address the problem of argument search. The purpose of argument search is the distillation of pro and contra arguments for requested topics from large text corpora. In previous works, the usual approach is to use a standard search engine to extract text parts which are relevant to the given topic and subsequently use an argument recognition algorithm to select arguments from them. The main challenge in the argument recognition task, which is also known as argument mining, is that often sentences containing arguments are structurally similar to purely informative sentences without any stance about the topic. In fact, they only differ semantically. Most approaches use topic or search term information only for the first search step and therefore assume that arguments can be classified independently of a topic. We argue that topic information is crucial for argument mining, since the topic defines the semantic context of an argument. Precisely, we propose different models for the classification of arguments, which take information about a topic of an argument into account. Moreover, to enrich the context of a topic and to let models understand the context of the potential argument better, we integrate information from different external sources such as Knowledge Graphs or pre-trained NLP models. Our evaluation shows that considering topic information, especially in connection with external information, provides a significant performance boost for the argument mining task.",
        "link": "https://dl.acm.org/doi/10.1145/3350546.3352506"
    },
    {
        "title": "Text mining for incoming tasks based on the urgency/importance factors and task classification using machine learning tools",
        "authors": [
            "Yasser Ali Alshehri"
        ],
        "date": "March 2020",
        "source": "ICCDA 2020: Proceedings of the 2020 the 4th International Conference on Compute and Data Analysis",
        "abstract": "In workplaces, there is a massive amount of unstructured data from different sources. In this paper, we present a case study that explains how can through communications between employees, we can help to prioritize tasks requests to increase the efficiency of their works for both technical and non-technical workers. This involves managing daily incoming tasks based on their level of urgency and importance.To allow all workers to utilize the urgency-importance matrix as a time-management tool, we need to automate this tool. The textual content of incoming tasks are analyzed, and metrics related to urgency and importance are extracted. A third factor (i.e., the response variable) is defined based on the two input variables (urgency and importance). Then, machine learning applied to the data to predict the class of incoming tasks based on data outcome desired. We used ordinal regression, neural networks, and decision tree algorithms to predict the four levels of task priority. We measure the performance of all using recalls, precisions, and F-scores. All classifiers perform higher than 89% in terms of all measures.",
        "link": "https://dl.acm.org/doi/10.1145/3388142.3388153"
    },
    {
        "title": "Text mining for malware classification using multivariate all repeated patterns detection",
        "authors": [
            "Konstantinos F. Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2019",
        "source": "ASONAM '19: Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Mobile phones have become nowadays a commodity to the majority of people. Using them, people are able to access the world of Internet and connect with their friends, their colleagues at work or even unknown people with common interests. This proliferation of the mobile devices has also been seen as an opportunity for the cyber criminals to deceive smartphone users and steel their money directly or indirectly, respectively, by accessing their bank accounts through the smartphones or by blackmailing them or selling their private data such as photos, credit card data, etc. to third parties. This is usually achieved by installing malware to smartphones masking their malevolent payload as a legitimate application and advertise it to the users with the hope that mobile users will install it in their devices. Thus, any existing application can easily be modified by integrating a malware and then presented it as a legitimate one. In response to this, scientists have proposed a number of malware detection and classification methods using a variety of techniques. Even though, several of them achieve relatively high precision in malware classification, there is still space for improvement. In this paper, we propose a text mining all repeated pattern detection method which uses the decompiled files of an application in order to classify a suspicious application into one of the known malware families. Based on the experimental results using a real malware dataset, the methodology tries to correctly classify (without any misclassification) all randomly selected malware applications of 3 categories with 3 different families each.",
        "link": "https://dl.acm.org/doi/10.1145/3341161.3350841"
    },
    {
        "title": "Comparative Study between Traditional Machine Learning and Deep Learning Approaches for Text Classification",
        "authors": [
            "Cannannore Nidhi Kamath",
            "Syed Saqib Bukhari",
            "Andreas Dengel"
        ],
        "date": "August 2018",
        "source": "DocEng '18: Proceedings of the ACM Symposium on Document Engineering 2018",
        "abstract": "In this contemporaneous world, it is an obligation for any organization working with documents to end up with the insipid task of classifying truckload of documents, which is the nascent stage of venturing into the realm of information retrieval and data mining. But classification of such humongous documents into multiple classes, calls for a lot of time and labor. Hence a system which could classify these documents with acceptable accuracy would be of an unfathomable help in document engineering. We have created multiple classifiers for document classification and compared their accuracy on raw and processed data. We have garnered data used in a corporate organization as well as publicly available data for comparison. Data is processed by removing the stop-words and stemming is implemented to produce root words. Multiple traditional machine learning techniques like Naive Bayes, Logistic Regression, Support Vector Machine, Random forest Classifier and Multi-Layer Perceptron are used for classification of documents. Classifiers are applied on raw and processed data separately and their accuracy is noted. Along with this, Deep learning technique such as Convolution Neural Network is also used to classify the data and its accuracy is compared with that of traditional machine learning techniques. We are also exploring hierarchical classifiers for classification of classes and subclasses. The system classifies the data faster and with better accuracy than if done manually. The results are discussed in the results and evaluation section.",
        "link": "https://dl.acm.org/doi/10.1145/3209280.3209526"
    },
    {
        "title": "Identification of Overpricing in the Purchase of Medication by the Federal Government of Brazil, Using Text Mining and Clustering Based on Ontology",
        "authors": [
            "Marco Aurelio O. S. Correa",
            "Adriano Galindo Leal"
        ],
        "date": "August 2018",
        "source": "ICCBDC '18: Proceedings of the 2018 2nd International Conference on Cloud and Big Data Computing",
        "abstract": "Increasing the transparency level in his actions and spending is one of the primary duties of the Brazilian Federal Government. The creation of laws that oblige full disclose of all its expenditures through transparency portals enables citizens to supervise all government entities. However, only the dissemination of these data, without a definite standard or the availability of data analysis tools, does not guarantee that the citizen is empowered to play his role. Therefore hence, the objective of this work is to identify overprice in the acquisition of products purchased by the federal government of Brazil using the unstructured data available on the Transparency Portal. The last two-years' worth of purchasing data, available in the Transparency Portal, were extracted, processed and stored. Due to his diverse nature and high volume of data, this study focused only on medicines purchased by the Ministry of Health. Ontology-based text mining and clustering techniques were applied for automatic identification and classification of products. The processing of this information was done through text mining and clustering, based on the ontology registered in another database of the Brazilian government. Because of this work, a consolidated price base per medication was created to allow the identification of distortions in prices practised, facilitating the identification of cases that merit further investigation to unravel fraud to the treasury.",
        "link": "https://dl.acm.org/doi/10.1145/3264560.3264569"
    },
    {
        "title": "Computational Estimation by Scientific Data Mining with Classical Methods to Automate Learning Strategies of Scientists",
        "authors": [
            "Aparna S. Varde"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Experimental results are often plotted as 2-dimensional graphical plots (aka graphs) in scientific domains depicting dependent versus independent variables to aid visual analysis of processes. Repeatedly performing laboratory experiments consumes significant time and resources, motivating the need for computational estimation. The goals are to estimate the graph obtained in an experiment given its input conditions, and to estimate the conditions that would lead to a desired graph. Existing estimation approaches often do not meet accuracy and efficiency needs of targeted applications. We develop a computational estimation approach called AutoDomainMine that integrates clustering and classification over complex scientific data in a framework so as to automate classical learning methods of scientists. Knowledge discovered thereby from a database of existing experiments serves as the basis for estimation. Challenges include preserving domain semantics in clustering, finding matching strategies in classification, striking a good balance between elaboration and conciseness while displaying estimation results based on needs of targeted users, and deriving objective measures to capture subjective user interests. These and other challenges are addressed in this work. The AutoDomainMine approach is used to build a computational estimation system, rigorously evaluated with real data in Materials Science. Our evaluation confirms that AutoDomainMine provides desired accuracy and efficiency in computational estimation. It is extendable to other science and engineering domains as proved by adaptation of its sub-processes within fields such as Bioinformatics and Nanotechnology.",
        "link": "https://dl.acm.org/doi/10.1145/3502736"
    },
    {
        "title": "Where is the road for issue reports classification based on text mining?",
        "authors": [
            "Qiang Fan",
            "Yue Yu",
            "Gang Yin",
            "Tao Wang",
            "Huaimin Wang"
        ],
        "date": "November 2017",
        "source": "ESEM '17: Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Currently, open source projects receive various kinds of issues daily, because of the extreme openness of Issue Tracking System (ITS) in GitHub. ITS is a labor-intensive and time-consuming task of issue categorization for project managers. However, a contributor is only required a short textual abstract to report an issue in GitHub. Thus, most traditional classification approaches based on detailed and structured data (e.g., priority, severity, software version and so on) are difficult to adopt. In this paper, issue classification approaches on a large-scale dataset, including 80 popular projects and over 252,000 issue reports collected from GitHub, were investigated. First, four traditional text-based classification methods and their performances were discussed. Semantic perplexity (i.e., an issues description confuses bug-related sentences with nonbug-related sentences) is a crucial factor that affects the classification performances based on quantitative and qualitative study. Finally, A two-stage classifier framework based on the novel metrics of semantic perplexity of issue reports was designed. Results show that our two-stage classification can significantly improve issue classification performances.",
        "link": "https://dl.acm.org/doi/10.1109/ESEM.2017.19"
    },
    {
        "title": "Feature-based Facebook reviews process model for e-management using data mining",
        "authors": [
            "Anish Kumar Varudharajulu",
            "Yongsheng Ma"
        ],
        "date": "January 2019",
        "source": "IC4E '19: Proceedings of the 10th International Conference on E-Education, E-Business, E-Management and E-Learning",
        "abstract": "The data generated from online communication acts as potential gold mines for discovering knowledge for researchers. A large amount of data is also generated in the form of web documents, emails, blogs, and feedback, etc. Text analytics is being significantly employed to mine important information. Opinion mining is the process of extracting human thoughts and perceptions from unstructured texts. The showstopper for designing an opinion mining system for analyzing reviews arise from the fact that customer reviews are often noisy. These reviews are informally written. In addition, they are subjected to spelling mistakes, grammatical errors, improper punctuation and irrational capitalization. This paper focuses on analyzing the different classification and clustering algorithms aimed at extracting and consolidating opinions of customers from social media sites like Facebook, Twitter and through surveys, at multiple levels of granularity to monitor and measure customer satisfaction. Ours is an automated approach, in which the system aids in the process of knowledge assimilation for knowledge-based building and also performs the analytics. Domain experts ratify the knowledge base and also provide training datasets for the system to intuitively gather more instances for ratification. The system identifies opinion expressions as phrases containing opinion words, opinionated features and also opinion modifiers. These expressions are categorized as positive, negative or neutral. Opinion expressions are identified and categorized using localized linguistic techniques. Opinions can be congregated at any desired level of specificity i.e. feature level or product level, user level or service level, etc. We have developed a system based on this approach, which provides the user with a platform to analyze opinion expressions crawled from a set of pre-defined datasets.",
        "link": "https://dl.acm.org/doi/10.1145/3306500.3306514"
    },
    {
        "title": "Discriminative Topic Mining via Category-Name Guided Text Embedding",
        "authors": [
            "Yu Meng",
            "Jiaxin Huang",
            "Guangyuan Wang",
            "Zihan Wang",
            "Chao Zhang",
            "Yu Zhang",
            "Jiawei Han"
        ],
        "date": "April 2020",
        "source": "WWW '20: Proceedings of The Web Conference 2020",
        "abstract": "Mining a set of meaningful and distinctive topics automatically from massive text corpora has broad applications. Existing topic models, however, typically work in a purely unsupervised way, which often generate topics that do not fit users’ particular needs and yield suboptimal performance on downstream tasks. We propose a new task, discriminative topic mining, which leverages a set of user-provided category names to mine discriminative topics from text corpora. This new task not only helps a user understand clearly and distinctively the topics he/she is most interested in, but also benefits directly keyword-driven classification tasks. We develop CatE, a novel category-name guided text embedding method for discriminative topic mining, which effectively leverages minimal user guidance to learn a discriminative embedding space and discover category representative terms in an iterative manner. We conduct a comprehensive set of experiments to show that CatE mines high-quality set of topics guided by category names only, and benefits a variety of downstream applications including weakly-supervised classification and lexical entailment direction identification.",
        "link": "https://dl.acm.org/doi/10.1145/3366423.3380278"
    },
    {
        "title": "Comparative Study of Heart Disease Diagnosis Using Top Ten Data Mining Classification Algorithms",
        "authors": [
            "I. Ketut Agung Enriko"
        ],
        "date": "June 2019",
        "source": "ICFET '19: Proceedings of the 5th International Conference on Frontiers of Educational Technologies",
        "abstract": "Data mining has been used for many purposes, especially for prediction system. In healthcare, data mining algorithms often used in disease diagnosis. Meanwhile, heart disease is known as a primary cause of death over the years. Many studies have been performed in heart disease diagnosis using data mining methods. There are some popular data mining algorithms that can be used in heart disease diagnosis, for example, k-Nearest Neighbor, CART, and AdaBoost. The algorithms are used to analyze a sample of cardiovascular patients data and predict the heart disease type that they suffer. Some parameters are taken from the patient, including EKG morphology, blood pressure, and information about the existence of chest pain, shortness of breath, palpitation, and cold sweat. In this study, medical records data are collected from Harapan Kita Hospital and utilized as a dataset sample in this research. Top ten data mining classification algorithms are used in diagnosing heart disease from Harapan Kita Hospital data and examining their performance by checking the accuracy and speed.",
        "link": "https://dl.acm.org/doi/10.1145/3338188.3338220"
    },
    {
        "title": "Text Mining Approach for Identifying Research Trends",
        "authors": [
            "Snezhana Sulova"
        ],
        "date": "June 2021",
        "source": "CompSysTech '21: Proceedings of the 22nd International Conference on Computer Systems and Technologies",
        "abstract": "With the increase of unstructured data, the issues connected with automatic text processing, the categorization of documents and the discovery of topics have become objects of growing interest. In order to improve the process of grouping and processing research publications, we would like to propose a method based upon natural language processing. It is based on text mining technologies which aim to identify key tendencies in documents. It processes the content of publications by clustering and identifies the topics of each identified group. This analysis helps by identifying key tendencies as well as discovering emerging new areas of research. Publications from the research literature database, Scopus, were used to test the approach. The topic of the publications is “the application of digital technologies in the logistics business”. The experiments were completed using the RapidMiner Studio software.",
        "link": "https://dl.acm.org/doi/10.1145/3472410.3472433"
    },
    {
        "title": "Assessment of Vulnerability Severity using Text Mining",
        "authors": [
            "Georgios Spanos",
            "Lefteris Angelis",
            "Dimitrios Toloudis"
        ],
        "date": "September 2017",
        "source": "PCI '17: Proceedings of the 21st Pan-Hellenic Conference on Informatics",
        "abstract": "Software1 vulnerabilities are closely associated with information systems security, a major and critical field in today's technology. Vulnerabilities constitute a constant and increasing threat for various aspects of everyday life, especially for safety and economy, since the social impact from the problems that they cause is complicated and often unpredictable. Although there is an entire research branch in software engineering that deals with the identification and elimination of vulnerabilities, the growing complexity of software products and the variability of software production procedures are factors contributing to the ongoing occurrence of vulnerabilities, Hence, another area that is being developed in parallel focuses on the study and management of the vulnerabilities that have already been reported and registered in databases. The information contained in such databases includes, a textual description and a number of metrics related to vulnerabilities. The purpose of this paper is to investigate to what extend the assessment of the vulnerability severity can be inferred directly from the corresponding textual description, or in other words, to examine the informative power of the description with respect to the vulnerability severity. For this purpose, text mining techniques, i.e. text analysis and three different classification methods (decision trees, neural networks and support vector machines) were employed. The application of text mining to a sample of 70,678 vulnerabilities from a public data source shows that the description itself is a reliable and highly accurate source of information for vulnerability prioritization.",
        "link": "https://dl.acm.org/doi/10.1145/3139367.3139390"
    },
    {
        "title": "Clustering Algorithms for Spatial Data Mining",
        "authors": [
            "Chetashri Bhadane",
            "Ketan Shah"
        ],
        "date": "April 2020",
        "source": "ICGDA '20: Proceedings of the 2020 3rd International Conference on Geoinformatics and Data Analysis",
        "abstract": "With the advances in mobile and wireless technologies, there has been a rise in applications that track and share the users' geospatial data. People use several social networking sites such as Twitter, Facebook and Flickr, where they share their status updates. With the integration of Global Positioning System (GPS) with mobile phones, it is now possible to share one's locations on these social networks. GPS allows us to record and track a person's movement along with the timestamp. The data set obtained from these GPS logs is vast and is widely used to analyze the users' movement patterns. Specifically, we can find out significant locations based on the number of users present at that location and the time spent by them at such places. Once significant places have been identified, it is also possible to identify the semantic importance of these locations. This paper presents an overview of the clustering techniques used to find important places of interest using large GPS based mobility datasets. Four clustering algorithms, K-Means, DBSCAN, OPTICS and Hierarchical, are implemented, and performance is tested using real-time data of 50 users collected over 2--5 years. Performance summary depicts that K-Means and DBSCAN perform well for spatial data.",
        "link": "https://dl.acm.org/doi/10.1145/3397056.3397068"
    },
    {
        "title": "Feature Selection based Arabic Text Classification using Different Machine Learning Algorithms: Comparative Study",
        "authors": [
            "Sakina Rim Bennabi",
            "Zakaria Elberrichi"
        ],
        "date": "June 2020",
        "source": "ICIST '20: Proceedings of the 10th International Conference on Information Systems and Technologies",
        "abstract": "Feature selection is a method of data pre-processing widely used when mining large data, such as textual classification. Several studies have been conducted to compare the different methods of feature selection applied to corpora in English. Unfortunately, a small number of works concern the Arabic language. This article aims to present a comparative study of different feature selection techniques including: Chi2, the ANOVA method and mutual information, applied on a corpus in Arabic language, while also diversifying the machine learning algorithms (Naive Bayes, SVM and KNN). This experimental study has shown in general that reducing dimensionality with feature selection techniques has slightly affected the performance of textual classification, reducing the size of the corpus by up to 1%.",
        "link": "https://dl.acm.org/doi/10.1145/3447568.3448531"
    },
    {
        "title": "Intelligent Data Analysis using Optimized Support Vector Machine Based Data Mining Approach for Tourism Industry",
        "authors": [
            "Ms Promila Sharma",
            "Uma Meena",
            "Girish Kumar Sharma"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Data analysis involves the deployment of sophisticated approaches from data mining methods, information theory, and artificial intelligence in various fields like tourism, hospitality, and so on for the extraction of knowledge from the gathered and preprocessed data. In tourism, pattern analysis or data analysis using classification is significant for finding the patterns that represent new and potentially useful information or knowledge about the destination and other data. Several data mining techniques are introduced for the classification of data or patterns. However, overfitting, less accuracy, local minima, sensitive to noise are the drawbacks in some existing data mining classification methods. To overcome these challenges, Support vector machine with Red deer optimization (SVM-RDO) based data mining strategy is proposed in this article. Extended Kalman filter (EKF) is utilized in the first phase, i.e., data cleaning to remove the noise and missing values from the input data. Mantaray foraging algorithm (MaFA) is used in the data selection phase, in which the significant data are selected for the further process to reduce the computational complexity. The final phase is the classification, in which SVM-RDO is proposed to access the useful pattern from the selected data. PYTHON is the implementation tool used for the experiment of the proposed model. The experimental analysis is done to show the efficacy of the proposed work. From the experimental results, the proposed SVM-RDO achieved better accuracy, precision, recall, and F1 score than the existing methods for the tourism dataset. Thus, it is showed the effectiveness of the proposed SVM-RDO for pattern analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3494566"
    },
    {
        "title": "Applying text mining to predict learners' cognitive engagement",
        "authors": [
            "Hayati Hind",
            "Mohammed Khalidi Idrissi",
            "Samir Bennani"
        ],
        "date": "October 2017",
        "source": "SCAMS '17: Proceedings of the Mediterranean Symposium on Smart City Application",
        "abstract": "In academic process, engagement represent one of the important success' factor. This paper proposes a predictive system for learners' cognitive engagement based on their online discussion forums participation. In the first level the ontology OWL and the LSA method are used to perform a semantic classification system of the threads according to a specific context chosen by the tutor. Then the Text mining, as a predictive method, is applied to the classified threads and learners' participation in the forums.",
        "link": "https://dl.acm.org/doi/10.1145/3175628.3175655"
    },
    {
        "title": "Investigate Transitions into Drug Addiction through Text Mining of Reddit Data",
        "authors": [
            "John Lu",
            "Sumati Sridhar",
            "Ritika Pandey",
            "Mohammad Al Hasan",
            "Georege Mohler"
        ],
        "date": "July 2019",
        "source": "KDD '19: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Increasing rates of opioid drug abuse and heightened prevalence of online support communities underscore the necessity of employing data mining techniques to better understand drug addiction using these rapidly developing online resources. In this work, we obtained data from Reddit, an online collection of forums, to gather insight into drug use/misuse using text snippets from users narratives. Specifically, using users' posts, we trained a binary classifier which predicts a user's transitions from casual drug discussion forums to drug recovery forums. We also proposed a Cox regression model that outputs likelihoods of such transitions. In doing so, we found that utterances of select drugs and certain linguistic features contained in one's posts can help predict these transitions. Using unfiltered drug-related posts, our research delineates drugs that are associated with higher rates of transitions from recreational drug discussion to support/recovery discussion, offers insight into modern drug culture, and provides tools with potential applications in combating the opioid crisis.",
        "link": "https://dl.acm.org/doi/10.1145/3292500.3330737"
    },
    {
        "title": "Analysing digital banking reviews using text mining",
        "authors": [
            "Li Chen Cheng",
            "Legaspi Rhea Sharmayne"
        ],
        "date": "December 2020",
        "source": "ASONAM '20: Proceedings of the 12th IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Digital banks are new entrants in the banking industry in the Philippines as they only started late 2018. Since then, a handful of players have and are still emerging. With more and more people becoming technologically savvy, it is very critical for financial institutions to develop a digital banking application that will stand out from the competition. This paper aims to use text mining methods to analyse digital banking application reviews. This study will perform topic modelling using LDA to explore customer concerns and will mine association rules between the digital banking features with the review score. The results will reveal which areas the digital banking application can further optimize for customer satisfaction and retention.",
        "link": "https://dl.acm.org/doi/10.1109/ASONAM49781.2020.9381429"
    },
    {
        "title": "Mining Moocs Videos Metadata Using Classification Techniques",
        "authors": [
            "El Harrak Othman",
            "Ghadi Abderrahim",
            "El Bouhdidi Jaber"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "In few years, the internet has become the first source of information for most people, Today MOOCs makes it possible for everyone to access to the education over the world, it's represents an emerging methodology of online teaching and an important development in open education. Due to the rapid development of Moocs and the rapid growth of digital data and video database over the Internet, it is becoming very difficult for learners to browsing and choosing the best training for them. The scientific community has increased the amount of research into new technologies, with a view to improve the digital video utilization: its archiving, indexing, accessibility, acquisition, store and even its process and usability. All these parts of the video utilization entail the necessity of the extraction of all important information of a video, especially in cases of lack of metadata information. Web video mining is retrieving the content using data mining techniques from World Wide Web. There are two approaches for web video mining using traditional image processing (signal processing) and metadata based approach. In this paper, we present the various research makes in this subject and we propose an effective methodology to extract the metadata from Moocs videos and classify them based on the extracted metadata by applying data mining techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090450"
    },
    {
        "title": "Text mining with HathiTrust",
        "authors": [
            "Eleanor Dickson Koehl",
            "Ryan Dubnicek"
        ],
        "date": "June 2019",
        "source": "JCDL '19: Proceedings of the 18th Joint Conference on Digital Libraries",
        "abstract": "This tutorial will introduce attendees to the HathiTrust Research Center and its tools and services for computational text analysis research. HTRC leverages the scope and scale of the HathiTrust Digital Library collection to create opportunities for researchers to perform text data mining on subsets of the corpus. Attendees will develop skills that will allow them to conduct basic text analysis research using HathiTrust data, learn techniques for engaging with scholars on their text data mining research, and explore strategies for computational access to large-scale digital collections.",
        "link": "https://dl.acm.org/doi/10.1109/JCDL.2019.00115"
    },
    {
        "title": "Text mining for plagiarism detection: multivariate pattern detection for recognition of text similarities",
        "authors": [
            "Konstantinos Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2018",
        "source": "ASONAM '18: Proceedings of the 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "The problem of plagiarism the recent years has been intensified by the availability of information in digital form and the accessibility of the electronic libraries through the Internet. As a result, plagiarism detection has been transformed into a big data analytics problem since the number of digital sources is extravagant and a new document needs to be compared with millions of other existing documents. In this paper, a text mining methodology is proposed that can detect all common patterns between a document and the documents in a reference database. The technique is based on a pattern detection algorithm and the corresponding data structure that enables the algorithm to detect all common patterns. The methodology has been applied in a well-defined dataset providing very promising results identifying difficult cases of plagiarism such as technical disguise.",
        "link": "https://dl.acm.org/doi/10.5555/3382225.3382424"
    },
    {
        "title": "Petroleum Engineering Data Text Classification Using Convolutional Neural Network Based Classifier",
        "authors": [
            "Tzuhan Hsu",
            "Yaoqin Zhang"
        ],
        "date": "May 2018",
        "source": "ICMLT '18: Proceedings of the 2018 International Conference on Machine Learning Technologies",
        "abstract": "In this paper, authors introduce a convolutional neural network (CNN) based classification approach for petroleum engineering event. Recently, text classification problem has raised substantial attention in the field of machine learning. The aim of text classification process is to assign correct label to a text on the basis of its content. However, due to the imprecise of feature representation, most of former text classification methods cannot export satisfying result. To solve this problem, authors propose a model based on convolutional neural network. Authors conduct experiments with real world petroleum engineering data text, artificially labelled into 6 categories. Beside purposed CNN-based model, the authors also implement other baseline machine learning approaches: k nearest neighbor, random forest, support vector machines, recurrent neural network, gated recurrent units, long short-term memory. Among all these models, our CNN-based model reach best performance in text classification task. In the last part of this paper, authors compare and analysis their purpose and baseline model.",
        "link": "https://dl.acm.org/doi/10.1145/3231884.3231898"
    },
    {
        "title": "Classifying Indonesian Online Articles as Advertisement Placement Base Using Text Mining",
        "authors": [
            "Nadhira Tasya",
            "Arian Dhini"
        ],
        "date": "July 2017",
        "source": "ICBIM 2017: Proceedings of the International Conference on Business and Information Management",
        "abstract": "Rapid development in technological aspect resulting in growing level of human needs for the latest news, so that emerged a new trend of publishing and accessing news through online media or usually called online journalism. In addition, the number of people who sell and purchase through online sites also continues to increase and this opportunity is utilized by the company and the advertiser by implementing targeted web advertising. However, the high number of articles that have been published and accessed leads to great opportunities for errors in determining where to place the ads. Therefore, it needs a system that can categorize articles accessed by users as the basis of advertisement placement by the company and this classification system can be done by applying the method of Data Mining and Text Mining. This research uses document data in the form of article content that will be categorized into twenty categories of class of advertisement by using Text Mining technique with Support Vector Machine algorithm. The results of this study may be used by companies or advertisers as a basis for placement of ads on selected online media sites.",
        "link": "https://dl.acm.org/doi/10.1145/3134271.3134288"
    },
    {
        "title": "Mining Free-Text Medical Notes for Suicide Risk Assessment",
        "authors": [
            "Marios Adamou",
            "Grigoris Antoniou",
            "Elissavet Greasidou",
            "Vincenzo Lagani",
            "Paulos Charonyktakis",
            "Ioannis Tsamardinos"
        ],
        "date": "July 2018",
        "source": "SETN '18: Proceedings of the 10th Hellenic Conference on Artificial Intelligence",
        "abstract": "Suicide has been considered as an important public health issue for a very long time, and is one of the main causes of death worldwide. Despite suicide prevention strategies being applied, the rate of suicide has not changed substantially over the past decades. Advances in machine learning make it possible to attempt to predict suicide based on the analysis of relevant data to inform clinical practice. This paper reports on findings from the analysis of data of patients who died by suicide in the period 2013-2016 and made use of both structured data and free-text medical notes. We focus on examining various text-mining approaches to support risk assessment. The results show that using advance machine learning and text-mining techniques, it is possible to predict within a specified period which people are most at risk of taking their own life at the time of referral to a mental health service.",
        "link": "https://dl.acm.org/doi/10.1145/3200947.3201020"
    },
    {
        "title": "Integration of data mining techniques in e-learning systems: Clustering Profil of Lerners and Recommender Course System",
        "authors": [
            "A. El Moustamid",
            "E. En-Naimi",
            "J. El Bouhdidi"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "Data mining aims at the extraction of knowledge from large quantities of data by automatic or semi-automatic methods. In the field of education and training e-learning, Data Mining techniques as clustering (The aim of this technique is to identify groups that are similar in some aspect) [21], prediction (is used for developing model where combination of data viewed from different aspects can effect single viewed data. Here, is important to watch how predictors variables are effecting predicted variable)[21], classification are applied in e-learning to improve the level of learners by providing learning according to the learners' requirement) are used to analyze learners profile, to predict and improve student performance. The aim of our project is to develop a system capable of analyzing learners profile and indexing web videos in order to offer learners a rich database with courses that correspond to their levels.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090453"
    },
    {
        "title": "Multi-level mining and visualization of scientific text collections: Exploring a bi-lingual scientific repository",
        "authors": [
            "Pablo Accuosto",
            "Francesco Ronzano",
            "Daniel Ferrés",
            "Horacio Saggion"
        ],
        "date": "December 2017",
        "source": "WOSP 2017: Proceedings of the 6th International Workshop on Mining Scientific Publications",
        "abstract": "We present a system to mine and visualize collections of scientific documents by semantically browsing information extracted from single publications or aggregated throughout corpora of articles. The text mining tool performs deep analysis of document collections allowing the extraction and interpretation of research paper's contents. In addition to the extraction and enrichment of documents with metadata (titles, authors, affiliations, etc), the deep analysis performed comprises semantic interpretation, rhetorical analysis of sentences, triple-based information extraction, and text summarization. The visualization components allow geographical-based exploration of collections, topic-evolution interpretation, and collaborative network analysis among others. The paper presents a case study of a bi-lingual collection in the field of Natural Language Processing (NLP).",
        "link": "https://dl.acm.org/doi/10.1145/3127526.3127529"
    },
    {
        "title": "Hierarchical Topic Mining via Joint Spherical Tree and Text Embedding",
        "authors": [
            "Yu Meng",
            "Yunyi Zhang",
            "Jiaxin Huang",
            "Yu Zhang",
            "Chao Zhang",
            "Jiawei Han"
        ],
        "date": "August 2020",
        "source": "KDD '20: Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Mining a set of meaningful topics organized into a hierarchy is intuitively appealing since topic correlations are ubiquitous in massive text corpora. To account for potential hierarchical topic structures, hierarchical topic models generalize flat topic models by incorporating latent topic hierarchies into their generative modeling process. However, due to their purely unsupervised nature, the learned topic hierarchy often deviates from users' particular needs or interests. To guide the hierarchical topic discovery process with minimal user supervision, we propose a new task, Hierarchical Topic Mining, which takes a category tree described by category names only, and aims to mine a set of representative terms for each category from a text corpus to help a user comprehend his/her interested topics. We develop a novel joint tree and text embedding method along with a principled optimization procedure that allows simultaneous modeling of the category tree structure and the corpus generative process in the spherical space for effective category-representative term discovery. Our comprehensive experiments show that our model, named JoSH, mines a high-quality set of hierarchical topics with high efficiency and benefits weakly-supervised hierarchical text classification tasks.",
        "link": "https://dl.acm.org/doi/10.1145/3394486.3403242"
    },
    {
        "title": "Unsupervised behavioural mining and clustering for malware family identification",
        "authors": [
            "Khanh Huu The Dam",
            "Thomas Given-Wilson",
            "Axel Legay"
        ],
        "date": "March 2021",
        "source": "SAC '21: Proceedings of the 36th Annual ACM Symposium on Applied Computing",
        "abstract": "More accurate and advanced detection and classification of malware requires exploiting program behaviour and not merely syntactic or static features. One approach is to use system call dependency graphs (SCDGs) that represent the program behaviour by interactions with the system, and the relations between these interactions. These SCDGs have been used with supervised learning techniques to very accurately detect and classify malware. This works considers the unsupervised learning challenge of mining for common clusters of behaviour without a priori knowledge. This allows for clustering of similar programs by behaviour, that can then be used for either classification or further analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3412841.3441919"
    },
    {
        "title": "Text Mining for Evaluating Authors' Birth and Death Years",
        "authors": [
            "Dror Moghaz",
            "Yaakov Hacohen-Kerner",
            "Dov Gabbay"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "This article presents a unique method in text and data mining for finding the era, i.e., mining temporal data, in which an anonymous author was living. Finding this era can assist in the examination of a fake document or extracting the time period in which a writer lived. The study and the experiments concern Hebrew, and in some parts, Aramaic and Yiddish rabbinic texts. The rabbinic texts are undated and contain no bibliographic sections, posing an interesting challenge. This work proposes algorithms using key phrases and key words that allow the temporal organization of citations together with linguistic patterns. Based on these key phrases, key words, and the references, we established several types of “Iron-clad,” Heuristic and Greedy rules for estimating the years of birth and death of a writer in an interesting classification task. Experiments were conducted on corpora, including documents authored by 12, 24, and 36 rabbinic writers and demonstrated promising results.",
        "link": "https://dl.acm.org/doi/10.1145/3281631"
    },
    {
        "title": "Transfer Learning to Timed Text Based Video Classification Using CNN",
        "authors": [
            "Zenun Kastrati",
            "Ali Shariq Imran",
            "Arianit Kurti"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "Open educational video resources are gaining popularity with a growing number of massive open online courses (MOOCs). This has created a niche for content providers to adopt effective solutions in automatically organizing and structuring of educational resources for maximum visibility. Recent advances in deep learning techniques are proving useful in managing and classifying resources into appropriate categories. This paper proposes one such convolutional neural network (CNN) model for classifying video lectures in a MOOC setting using a transfer learning approach. The model uses a time-aligned text transcripts corresponding to video lectures from six broader subject categories. Video lectures and their corresponding transcript dataset is gathered from the Coursera MOOC platform. Two different CNN models are proposed: i) CNN based classification using embeddings learned from our MOOC dataset, ii) CNN based classification using transfer learning. Word embeddings generated from two well known state-of-the-art pre-trained models Word2Vec and GloVe, are used in the transfer learning approach for the second case. The proposed CNN models are evaluated using precision, recall, and F1 score and the obtained performance is compared with both conventional and deep learning classifiers. The proposed CNN models have an F1 score improvement of 10-22 percentage points over DNN and conventional classifiers",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326483"
    },
    {
        "title": "Evolutionary role mining in complex networks by ensemble clustering",
        "authors": [
            "Sarvenaz Choobdar",
            "Pedro Ribeiro",
            "Fernando Silva"
        ],
        "date": "April 2017",
        "source": "SAC '17: Proceedings of the Symposium on Applied Computing",
        "abstract": "The structural patterns in the neighborhood of nodes assign unique roles to the nodes. Mining the set of existing roles in a network provides a descriptive profile of the network and draws its general picture. This paper proposes a new method to determine structural roles in a dynamic network based on the current position of nodes and their historic behavior. We develop a temporal ensemble clustering technique to dynamically find groups of nodes, holding similar tempo-structural roles. We compare two weighting functions, based on age and distribution of data, to incorporate temporal behavior of nodes in the role discovery. To evaluate the performance of the proposed method, we assess the results from two points of view: 1) goodness of fit to current structure of the network; 2) consistency with historic data. We conduct the evaluation using different ensemble clustering techniques. The results on real world networks demonstrate that our method can detect tempo-structural roles that simultaneously depict the topology of a network and reflect its dynamics with high accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3019612.3019815"
    },
    {
        "title": "Analyzing used-car web listings via text mining",
        "authors": [
            "Ayhan Demiriz",
            "Fatma Cantaş"
        ],
        "date": "October 2017",
        "source": "IML '17: Proceedings of the 1st International Conference on Internet of Things and Machine Learning",
        "abstract": "Used car trade is one of the major components of the world economies. It is not uncommon to sell a car by placing an internet advertisement irrespective of the geography in these days. A typical content of an advertisement is usually composed of two parts namely the structured and the free text data. The structured data may include some information about the asking price, make, model, year, mileage of the car and the contact info. In most cases, seller may give important clues about the car's current conditions in the free text data where the title (head) of the advertisement can be included as free text too. This paper reports preliminary results from a text mining study conducted on 75K used car internet listings collected from two major car listing web sites in Turkey. As expected, the words and the phrases related to the description of the car are observed to be frequent. The leading concepts in the free text are found to be regarding how to describe the current condition of a car, for example \"no crash history\".",
        "link": "https://dl.acm.org/doi/10.1145/3109761.3109782"
    },
    {
        "title": "Text classification of Diseases Treated by Traditional Chinese Medicine Prescription based on machine learning",
        "authors": [
            "Hanqing Zhao",
            "Bo Han",
            "Chen Li"
        ],
        "date": "September 2020",
        "source": "ISAIMS '20: Proceedings of the 1st International Symposium on Artificial Intelligence in Medical Sciences",
        "abstract": "OBJECTIVE: To explore the application of machine learning in th e identification of diseases treated by traditional Chinese medicine prescriptions. METHODS: Based on the composition of the text document of Chinese medicine prescriptions, the prescriptions were divided into cough, headache and diarrhea. THUTCT were introduced to establish and train two machine learning text classification models, LibSVM and LibLinear, and the prescriptions to be deter mined were put into the model for classification and prediction. R ESULTS: The Precision rate of LibSVM model and Liblinear mod el were 0.7283 and 0.6690. Seven prescriptions were classified an d predicted, and the results were in line with expectations. CONCLUSION: THUCTC has good universality for the content of TCM prescriptions, high classification accuracy and fast testing speed, which is suitable for the text classification and discrimination research of TCM prescriptions for diseases.",
        "link": "https://dl.acm.org/doi/10.1145/3429889.3429895"
    },
    {
        "title": "Evaluation of retweet clustering method classification method using retweets on Twitter without text data",
        "authors": [
            "K. Uchida",
            "F. Toriumi",
            "T. Sakaki"
        ],
        "date": "August 2017",
        "source": "WI '17: Proceedings of the International Conference on Web Intelligence",
        "abstract": "Burst phenomena, which frequently occur on social media, are caused by such social events as flaming on the internet, elections, and natural disasters. To understand people's thoughts and feelings, we must classify their opinions from burst phenomena. Therefore, classification methods that categorize tweets are critical. However, since most classification methods focus on text mining, they cannot group tweets by topics because each tweet has poor linguistic similarities. We used a non-text-based classification method proposed by Baba et al. that groups tweets by topics, even if they have poor linguistic similarities, and verified its validity by comparing it with a text-based classification method in two different evaluations: qualitative and quantitative. In the qualitative evaluation part, we did a questionnaire survey and validated the suitability of the topic clusters created using both the non-and text-based methods. Since evaluating the similarity of every pair of tweets in each topic is difficult, we evaluated the similarity between sampled pairs in the survey and acquired more appropriate topic clustering results using the non-text-based method than the text-based method. In the quantitative evaluation part, we focused on the robustness of each method against data reduction. Many approaches analyze social media data, especially because collecting data from social media is comparatively easy. However, since collecting the whole data of burst phenomena is very costly due to the vast amounts of available social media data, robustness against data reduction is an important index to evaluate classification methods. With the non-text-based method, over 55% of the pairs of tweets in the same cluster were also included in the same cluster even when the data were reduced to 10% in all three of our example cases. In this paper, as a source we focus on Twitter, one of the most popular microblogging services. Using clustering to conduct detailed case analyses, we scrutinized three burst cases that include natural disasters and flaming on the internet and found that a non-text-based method more effectively classified tweets in burst phenomena than a text-based method.",
        "link": "https://dl.acm.org/doi/10.1145/3106426.3106451"
    },
    {
        "title": "Using Spark for Text Mining on Large Scale Liver Cancer Literature",
        "authors": [
            "Ming-Yen Lin",
            "Yu-Ju Lin",
            "Sue-Chen Hsueh"
        ],
        "date": "January 2021",
        "source": "BDET 2021: 2021 the 3rd International Conference on Big Data Engineering and Technology (BDET)",
        "abstract": "Cancer is one of the main causes of death. The number of known cancer to-date is more than one hundred. Liver cancer, ranked after trachea and lung cancer, has long been high in the cancer leading cause in Taiwan and even ranked as second in 2016. The number of scientific articles related to cancer proliferates every year. The number reaches as high as 20 million in PubMed so that discovering useful information from the massive collection is very difficult. In addition, using a single machine to sift through these articles is very time-consuming. Therefore, we present a big data analytic framework using the distributed Apache Spark platform for text mining in PubMed literature. We establish a prediction model for liver cancer articles so that researchers may effectively validate whether an article is related to liver cancer or not. Classification models in Spark MLlib including Linear Support Vector Machines (SVM), Logistic Regression, are used in our experiments. Relevancy to liver cancer is further confirmed by using MeSH (Medical Subject Headings) terms. Logistic regression is about 3 times faster than SVMs and the accuracy of both methods is close to 95% in the experiments using hold-out validation. When max_features is 500 and min_df ≤ 0.1 (or min_df = 1), the accuracy may reach 96%. In the experiments with K-fold cross-validation, the accuracy of SVMs methods is 96%. The experimental results show that that our prediction model may effectively classify liver cancer articles.",
        "link": "https://dl.acm.org/doi/10.1145/3474944.3474958"
    },
    {
        "title": "Automatic topic classification of test cases using text mining at an Android smartphone vendor",
        "authors": [
            "Junji Shimagaki",
            "Yasutaka Kamei",
            "Naoyasu Ubayashi",
            "Abram Hindle"
        ],
        "date": "October 2018",
        "source": "ESEM '18: Proceedings of the 12th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Background: An Android smartphone is an ecosystem of applications, drivers, operating system components, and assets. The volume of the software is large and the number of test cases needed to cover the functionality of an Android system is substantial. Enormous effort has been already taken to properly quantify \"what features and apps were tested and verified?\". This insight is provided by dashboards that summarize test coverage and results per feature. One method to achieve this is to manually tag or label test cases with the topic or function they cover, much like function points. At the studied Android smartphone vendor, tests are labelled with manually defined tags, so-called \"feature labels (FLs)\", and the FLs serve to categorize 100s to 1000s test cases into 10 to 50 groups. Aim: Unfortunately for developers, manual assignment of FLs to 1000s of test cases is a time consuming task, leading to inaccurately labeled test cases, which will render the dashboard useless. We created an automated system that suggests tags/labels to the developers for their test cases rather than manual labeling. Method: We use machine learning models to predict and label the functionality tested by 10,000 test cases developed at the company. Results: Through the quantitative experiments, our models achieved acceptable F-1 performance of 0.3 to 0.88. Also through the qualitative studies with expert teams, we showed that the hierarchy and path of tests was a good predictor of a feature's label. Conclusions: We find that this method can reduce tedious manual effort that software developers spent classifying test cases, while providing more accurate classification results.",
        "link": "https://dl.acm.org/doi/10.1145/3239235.3268927"
    },
    {
        "title": "To apply Data Mining for Classification of Crowd sourced Software Requirements",
        "authors": [
            "Soonh Taj",
            "Qasim Arain",
            "Imran Memon",
            "Asma Zubedi"
        ],
        "date": "April 2019",
        "source": "ICSIE '19: Proceedings of the 8th International Conference on Software and Information Engineering",
        "abstract": "Now a day's main focus of developers is to build quality software that works according to customer needs and for this reason it is necessary to gather right requirements as requirement elicitation is the critical step that impacts on the success of software project as misinterpreted requirements leads to the failure of software project. By keeping this in mind a research is carried out on improving requirements elicitation process and automating the process of classifying requirements. In this research, a model is proposed which will help in this scenario for requirements elicitation and requirement classification. This paper presents a model in which crowd sourcing approach is used so that customers, end users, stakeholders, developers and software engineers can make active participation for requirement elicitation process and requirements gathered using crowdsourcing approach are used by model for classification process i.e. classification of requirements into functional and non-functional requirements. For the proof of proposed model a case study is conducted. Results of case study provided the usefulness and efficiency of proposed model for classification of crowd sourced software requirements.",
        "link": "https://dl.acm.org/doi/10.1145/3328833.3328837"
    },
    {
        "title": "Text classification using Fuzzy TF-IDF and Machine Learning Models",
        "authors": [
            "Mariem Bounabi",
            "Karim El Moutaouakil",
            "Khalid Satori"
        ],
        "date": "October 2019",
        "source": "BDIoT'19: Proceedings of the 4th International Conference on Big Data and Internet of Things",
        "abstract": "The representation of the information has an important impact on the text classification task. Several weighting methods were proposed in the literature, and the term frequency-inverse term frequency (TFIDF), the most know on the text treatment field. The FTF-IDF is a vector representation where the components of the TFIDF are presented as inputs to the Fuzzy Inference System (FIS). In this work, we compare several Machin Learning algorithms such as Naïve Bayes and its derivatives, SVM and Random forest classifiers, using the FTF-IDF representation. To improve the quality of the used classifiers, we call sum attribute selection methods. The recognition rate, for the tested systems, is satisfied, where the system based on naïve Bayes classifier, the FTF-IDF weighting terms, and the info gain select attributes method gives 98.7% as accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3372938.3372956"
    },
    {
        "title": "Text Mining in Cybersecurity: A Systematic Literature Review",
        "authors": [
            "Luciano Ignaczak",
            "Guilherme Goldschmidt",
            "Cristiano André Da Costa",
            "Rodrigo Da Rosa Righi"
        ],
        "date": null,
        "source": "ACM Computing Surveys",
        "abstract": "The growth of data volume has changed cybersecurity activities, demanding a higher level of automation. In this new cybersecurity landscape, text mining emerged as an alternative to improve the efficiency of the activities involving unstructured data. This article proposes a Systematic Literature Review (SLR) to present the application of text mining in the cybersecurity domain. Using a systematic protocol, we identified 2,196 studies, out of which 83 were summarized. As a contribution, we propose a taxonomy to demonstrate the different activities in the cybersecurity domain supported by text mining. We also detail the strategies evaluated in the application of text mining tasks and the use of neural networks to support activities involving unstructured data. The work also discusses text classification performance aiming its application in real-world solutions. The SLR also highlights open gaps for future research, such as the analysis of non-English content and the intensification in the usage of neural networks.",
        "link": "https://dl.acm.org/doi/10.1145/3462477"
    },
    {
        "title": "Using Text Mining to Analyze the Financial Patents",
        "authors": [
            "Yung-Feng Lu",
            "Jia-Lang Xu",
            "Mu-Yen Chen"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "With the advancement and development of science and technology, Nowadays society is the development of information technology, cloud computing, mobile Internet, Internet of Things, block chain and information technology.The combination of finance and technology has made financial technology terms gradually emerging.. This research analyzes the patents of financial technology. However, Taiwan's financial technology patents are mostly G06Q-20 (payment plan, architecture or agreement), G06Q-30 (commercial, such as marketing, shopping, payment, auction or e-commerce). G06Q-40 (financial, investment or tax treatment, insurance).The research use text mining technology to analyze and understand Taiwan's trends and developments in financial technology. The research results show that G06Q-20 patents are biased towards information security and mobile payment. G06Q-30 patents favor shopping websites and advertising. G06Q-40 patents are biased towards diversity, such as securities, insurance, finance, and stocks.",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326478"
    },
    {
        "title": "Identifying Online Profiles of Distance Learning Students Using Data Mining Techniques",
        "authors": [
            "Osama Islam",
            "Muazzam Siddiqui",
            "Naif Radi Aljohani"
        ],
        "date": "October 2019",
        "source": "ICDTE '19: Proceedings of the 3rd International Conference on Digital Technology in Education",
        "abstract": "Educational data has grown over the years with the increased use of technology within educational environments. This has led to a huge amount of data being stored in various data sources representing the student, his/her activities, and other aspects relevant to the learning process. To meet this analytical need, Educational Data Mining (EDM) has emerged to assist educational institutions in identifying key benefits such as students at risk, the level of student engagement or predicting student performance. The aim of this research was to explore the various aspects of student interaction data using data mining techniques to identify relevant patterns of behaviors and possible key attributes that have higher degrees of influence on distance learning students. The main findings identified several patterns of user online profiles based on a set of adopted learning strategies, the research proposed a framework for analyzing such interaction data based on R and Hadoop platforms to correlate online profiles with student performance.",
        "link": "https://dl.acm.org/doi/10.1145/3369199.3369249"
    },
    {
        "title": "Extremely Fast Decision Tree Mining for Evolving Data Streams",
        "authors": [
            "Albert Bifet",
            "Jiajin Zhang",
            "Wei Fan",
            "Cheng He",
            "Jianfeng Zhang",
            "Jianfeng Qian",
            "Geoff Holmes",
            "Bernhard Pfahringer"
        ],
        "date": "August 2017",
        "source": "KDD '17: Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
        "abstract": "Nowadays real-time industrial applications are generating a huge amount of data continuously every day. To process these large data streams, we need fast and efficient methodologies and systems. A useful feature desired for data scientists and analysts is to have easy to visualize and understand machine learning models. Decision trees are preferred in many real-time applications for this reason, and also, because combined in an ensemble, they are one of the most powerful methods in machine learning. In this paper, we present a new system called STREAMDM-C++, that implements decision trees for data streams in C++, and that has been used extensively at Huawei. Streaming decision trees adapt to changes on streams, a huge advantage since standard decision trees are built using a snapshot of data, and can not evolve over time. STREAMDM-C++ is easy to extend, and contains more powerful ensemble methods, and a more efficient and easy to use adaptive decision trees. We compare our new implementation with VFML, the current state of the art implementation in C, and show how our new system outperforms VFML in speed using less resources.",
        "link": "https://dl.acm.org/doi/10.1145/3097983.3098139"
    },
    {
        "title": "Data miners' little helper: data transformation activity cues for cluster analysis on document collections",
        "authors": [
            "Tania Cerquitelli",
            "Evelina Di Corso",
            "Francesco Ventura",
            "Silvia Chiusano"
        ],
        "date": "June 2017",
        "source": "WIMS '17: Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "In this paper we propose a new self-learning engine to streamline the analytics process, as it enables analysts to mine massive data repositories with minimal user intervention. In the context of cluster analysis on a collection of documents this new system, named SELF-DATA (SELF-learning DAta TrAnsformation), suggests to the analyst how to configure the whole mining process for a given dataset. SELF-DATA relies on an engine exploring different data weighting schemas (e.g., normalized term frequencies) and data transformation methods (e.g., PCA) before applying the cluster analysis, evaluating and comparing solutions through different quality indices (e.g., weighted Silhouette), and presenting the k-top solutions to the analyst. SELF-DATA will also include a knowledge base storing results of experiments on previously processed datasets, and a classification algorithm trained on the knowledge base content to forecast the best configuration for the whole mining process for an unexplored dataset. The first development of SELF-DATA running on Apache Spark has been validated on 5 collections of documents. Experimental results highlight that TF-IDF and logarithmic entropy are effective to measure item relevance with sparse datasets, and the LSI method outperforms PCA with a large dictionary.",
        "link": "https://dl.acm.org/doi/10.1145/3102254.3102288"
    },
    {
        "title": "Interactive Visual Graph Mining and Learning",
        "authors": [
            "Ryan A. Rossi",
            "Nesreen K. Ahmed",
            "Rong Zhou",
            "Hoda Eldardiry"
        ],
        "date": null,
        "source": "ACM Transactions on Intelligent Systems and Technology",
        "abstract": "This article presents a platform for interactive graph mining and relational machine learning called GraphVis. The platform combines interactive visual representations with state-of-the-art graph mining and relational machine learning techniques to aid in revealing important insights quickly as well as learning an appropriate and highly predictive model for a particular task (e.g., classification, link prediction, discovering the roles of nodes, and finding influential nodes). Visual representations and interaction techniques and tools are developed for simple, fast, and intuitive real-time interactive exploration, mining, and modeling of graph data. In particular, we propose techniques for interactive relational learning (e.g., node/link classification), interactive link prediction and weighting, role discovery and community detection, higher-order network analysis (via graphlets, network motifs), among others. GraphVis also allows for the refinement and tuning of graph mining and relational learning methods for specific application domains and constraints via an end-to-end interactive visual analytic pipeline that learns, infers, and provides rapid interactive visualization with immediate feedback at each change/prediction in real-time. Other key aspects include interactive filtering, querying, ranking, manipulating, exporting, as well as tools for dynamic network analysis and visualization, interactive graph generators (including new block model approaches), and a variety of multi-level network analysis techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3200764"
    },
    {
        "title": "Data Mining and Opinion Mining: A Tool in Educational Context",
        "authors": [
            "Myriam Peñafiel",
            "Stefanie Vásquez",
            "Diego Vásquez",
            "Juan Zaldumbide",
            "Sergio Luján-Mora"
        ],
        "date": "July 2018",
        "source": "ICoMS '18: Proceedings of the 2018 1st International Conference on Mathematics and Statistics",
        "abstract": "The use of the web as a universal communication platform generates large volumes of data (Big data), which in many cases, need to be processed so that they can become useful knowledge in face of the sceptics who have doubts about the credibility of such information. The use of web data that comes from educational contexts needs to be addressed, since that large amount of unstructured information is not being valued, losing valuable information that can be used. To solve this problem, we propose the use of data mining techniques such as sentiment analysis to validate the information that comes from the educational platforms. The objective of this research is to propose a methodology that allows the user to apply sentiment analysis in a simple way, because although some researchers have done it, very few do with data in the educational context. The results obtained prove that the proposal can be used in similar cases.",
        "link": "https://dl.acm.org/doi/10.1145/3274250.3274263"
    },
    {
        "title": "Research Hotspots Mining and Visualized Analysis Based on Linking Cluster and K-Core Decomposition",
        "authors": [
            "Ying Cai",
            "Fang Huang",
            "Shuai Wang",
            "Hao Zhang",
            "Chunxiu Du"
        ],
        "date": "May 2018",
        "source": "ICDPA 2018: Proceedings of the International Conference on Data Processing and Applications",
        "abstract": "In this paper, we focus on research hotspots mining with knowledge semantic features on discipline topic word networks, which are extracted from scientific and technical documents. For the topic networks, a novel method of combining linking community clustering with k-core decomposition is proposed to mine the hotspot community with knowledge hierarchy structure. And as a post-processing, the hierarchical community identification is implemented by k-core decomposition in order to discover the internal connection with knowledge semantic hierarchy in the research hotspot. Finally, the community density and visualization method is utilized to analyze the core and sub- problem in the research hotspot community. In experiments, the topic networks for a single discipline and interdisciplinary research hotspot are taken as examples of analysis, and the results show the proposed approach can effectively realize the objective of mining and analyzing the discipline research hotspot on the academic topic word network.",
        "link": "https://dl.acm.org/doi/10.1145/3224207.3224216"
    },
    {
        "title": "The Application of Data Mining Algorithms in the Construction of Travel Recommendation System",
        "authors": [
            "Chao Lou",
            "Zhaonan Mu",
            "Mengzhu Liu"
        ],
        "date": "January 2021",
        "source": "CONF-CDS 2021: The 2nd International Conference on Computing and Data Science",
        "abstract": "Photos shared by users of online social network often associated with tags, texts, geographic and time information. These data are ideal research resource which can be used by researchers to observe people's interests and behaviors. They can also be used to analyze people's travel behaviors and interests because it can reflect people's travel activities and experiences directly. This project aimed to consolidate people into groups based on their travel preferences of travel season, travel time span, scenery type of travel location and then analyze popular travel places for different people groups. The travel preferences were analyzed from extracted data applying PHP language and Naive Bayes classification approach. According to the analyzed data, people were divided into groups. Then, DBSCAN clustering was used to get popular locations based on the different groups. As a result, 80 thousand photo data of 5 cities have been collected from Flickr by Phython. It has been showed that people's preference of season in most of the mined cities was spring and summer and the preference of time span was short. Moreover, mined people has been divided into 16 groups according to their preference and based on the groups popular locations have been calculated by DBSCAN clustering. Theses locations have been provided in a recommendation website which was built in this project. For the future work, mining Flickr data associated with other travel social network sites could be a good way to build a more effective recommendation system.",
        "link": "https://dl.acm.org/doi/10.1145/3448734.3450477"
    },
    {
        "title": "Compare Machine Learning Models in Text Classification Using Steam User Reviews",
        "authors": [
            "Youchen Miao",
            "Zeyu Jin",
            "Yumeng Zhang",
            "Yuchen Chen",
            "Junren Lai"
        ],
        "date": "November 2021",
        "source": "ICSED '21: Proceedings of the 2021 3rd International Conference on Software Engineering and Development",
        "abstract": "Text Classification and Sentiment Analysis of game reviews are viewed as important parts in not only academic fields but also in game studies. In this paper, with more than 400 thousand game reviews on Steam platform, we preprocess the data using different libraries (sklearn, nltk, and spaCy) and use them as inputs to build three sentiment classification models based on different algorithms (Naive Bayes, SVM, and Random Forest). In contrast to previous studies that only focus on different sentiment analysis models, our paper also highlights the use of different APIs to preprocess the data and their corresponding model performance. The results show that no matter which API we choose, Random Forest models always perform the best. However, in terms of training time, Naive Bayes is the fastest. This work can be used to apply grid search for researchers to automatically find the optimum API before conducting sentiment analysis in the future.",
        "link": "https://dl.acm.org/doi/10.1145/3507473.3507480"
    },
    {
        "title": "Lost in Transduction: Transductive Transfer Learning in Text Classification",
        "authors": [
            "Alejandro Moreo",
            "Andrea Esuli",
            "Fabrizio Sebastiani"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Obtaining high-quality labelled data for training a classifier in a new application domain is often costly. Transfer Learning\n(a.k.a. “Inductive Transfer”) tries to alleviate these costs by transferring, to the “target” domain of interest, knowledge available from a different “source” domain. In transfer learning the lack of labelled information from the target domain is compensated by the availability at training time of a set of unlabelled examples from the target distribution. Transductive Transfer Learning denotes the transfer learning setting in which the only set of target documents that we are interested in classifying is known and available at training time. Although this definition is indeed in line with Vapnik’s original definition of “transduction”, current terminology in the field is confused. In this article, we discuss how the term “transduction” has been misused in the transfer learning literature, and propose a clarification consistent with the original characterization of this term given by Vapnik. We go on to observe that the above terminology misuse has brought about misleading experimental comparisons, with inductive transfer learning methods that have been incorrectly compared with transductive transfer learning methods. We then, give empirical evidence that the difference in performance between the inductive version and the transductive version of a transfer learning method can indeed be statistically significant (i.e., that knowing at training time the only data one needs to classify indeed gives an advantage). Our clarification allows a reassessment of the field, and of the relative merits of the major, state-of-the-art algorithms for transfer learning in text classification.",
        "link": "https://dl.acm.org/doi/10.1145/3453146"
    },
    {
        "title": "Machine Learning on Mining Potential Adverse Drug Reactions for Pharmacovigilance",
        "authors": [
            "Ke Jia"
        ],
        "date": "July 2021",
        "source": "DSIT 2021: 2021 4th International Conference on Data Science and Information Technology",
        "abstract": "Drugs are related to the lives and health of patients. Mining and discovering potential adverse drug reactions from a large number of unstructured texts play an important role in drug research and pharmacovigilance. It is a hot research issue in the field of biomedicine in recent years. A large number of scholars have discovered a large amount of information on adverse drug reactions from different data sources through effective machine learning methods. This paper reviews the research progress in recent years to provide a reference for future research.",
        "link": "https://dl.acm.org/doi/10.1145/3478905.3478964"
    },
    {
        "title": "TACAM: Topic And Context Aware Argument Mining",
        "authors": [
            "Michael Fromm",
            "Evgeniy Faerman",
            "Thomas Seidl"
        ],
        "date": "October 2019",
        "source": "WI '19: IEEE/WIC/ACM International Conference on Web Intelligence",
        "abstract": "In this work we address the problem of argument search. The purpose of argument search is the distillation of pro and contra arguments for requested topics from large text corpora. In previous works, the usual approach is to use a standard search engine to extract text parts which are relevant to the given topic and subsequently use an argument recognition algorithm to select arguments from them. The main challenge in the argument recognition task, which is also known as argument mining, is that often sentences containing arguments are structurally similar to purely informative sentences without any stance about the topic. In fact, they only differ semantically. Most approaches use topic or search term information only for the first search step and therefore assume that arguments can be classified independently of a topic. We argue that topic information is crucial for argument mining, since the topic defines the semantic context of an argument. Precisely, we propose different models for the classification of arguments, which take information about a topic of an argument into account. Moreover, to enrich the context of a topic and to let models understand the context of the potential argument better, we integrate information from different external sources such as Knowledge Graphs or pre-trained NLP models. Our evaluation shows that considering topic information, especially in connection with external information, provides a significant performance boost for the argument mining task.",
        "link": "https://dl.acm.org/doi/10.1145/3350546.3352506"
    },
    {
        "title": "Text mining for incoming tasks based on the urgency/importance factors and task classification using machine learning tools",
        "authors": [
            "Yasser Ali Alshehri"
        ],
        "date": "March 2020",
        "source": "ICCDA 2020: Proceedings of the 2020 the 4th International Conference on Compute and Data Analysis",
        "abstract": "In workplaces, there is a massive amount of unstructured data from different sources. In this paper, we present a case study that explains how can through communications between employees, we can help to prioritize tasks requests to increase the efficiency of their works for both technical and non-technical workers. This involves managing daily incoming tasks based on their level of urgency and importance.To allow all workers to utilize the urgency-importance matrix as a time-management tool, we need to automate this tool. The textual content of incoming tasks are analyzed, and metrics related to urgency and importance are extracted. A third factor (i.e., the response variable) is defined based on the two input variables (urgency and importance). Then, machine learning applied to the data to predict the class of incoming tasks based on data outcome desired. We used ordinal regression, neural networks, and decision tree algorithms to predict the four levels of task priority. We measure the performance of all using recalls, precisions, and F-scores. All classifiers perform higher than 89% in terms of all measures.",
        "link": "https://dl.acm.org/doi/10.1145/3388142.3388153"
    },
    {
        "title": "Text mining for malware classification using multivariate all repeated patterns detection",
        "authors": [
            "Konstantinos F. Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2019",
        "source": "ASONAM '19: Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Mobile phones have become nowadays a commodity to the majority of people. Using them, people are able to access the world of Internet and connect with their friends, their colleagues at work or even unknown people with common interests. This proliferation of the mobile devices has also been seen as an opportunity for the cyber criminals to deceive smartphone users and steel their money directly or indirectly, respectively, by accessing their bank accounts through the smartphones or by blackmailing them or selling their private data such as photos, credit card data, etc. to third parties. This is usually achieved by installing malware to smartphones masking their malevolent payload as a legitimate application and advertise it to the users with the hope that mobile users will install it in their devices. Thus, any existing application can easily be modified by integrating a malware and then presented it as a legitimate one. In response to this, scientists have proposed a number of malware detection and classification methods using a variety of techniques. Even though, several of them achieve relatively high precision in malware classification, there is still space for improvement. In this paper, we propose a text mining all repeated pattern detection method which uses the decompiled files of an application in order to classify a suspicious application into one of the known malware families. Based on the experimental results using a real malware dataset, the methodology tries to correctly classify (without any misclassification) all randomly selected malware applications of 3 categories with 3 different families each.",
        "link": "https://dl.acm.org/doi/10.1145/3341161.3350841"
    },
    {
        "title": "Comparative Study between Traditional Machine Learning and Deep Learning Approaches for Text Classification",
        "authors": [
            "Cannannore Nidhi Kamath",
            "Syed Saqib Bukhari",
            "Andreas Dengel"
        ],
        "date": "August 2018",
        "source": "DocEng '18: Proceedings of the ACM Symposium on Document Engineering 2018",
        "abstract": "In this contemporaneous world, it is an obligation for any organization working with documents to end up with the insipid task of classifying truckload of documents, which is the nascent stage of venturing into the realm of information retrieval and data mining. But classification of such humongous documents into multiple classes, calls for a lot of time and labor. Hence a system which could classify these documents with acceptable accuracy would be of an unfathomable help in document engineering. We have created multiple classifiers for document classification and compared their accuracy on raw and processed data. We have garnered data used in a corporate organization as well as publicly available data for comparison. Data is processed by removing the stop-words and stemming is implemented to produce root words. Multiple traditional machine learning techniques like Naive Bayes, Logistic Regression, Support Vector Machine, Random forest Classifier and Multi-Layer Perceptron are used for classification of documents. Classifiers are applied on raw and processed data separately and their accuracy is noted. Along with this, Deep learning technique such as Convolution Neural Network is also used to classify the data and its accuracy is compared with that of traditional machine learning techniques. We are also exploring hierarchical classifiers for classification of classes and subclasses. The system classifies the data faster and with better accuracy than if done manually. The results are discussed in the results and evaluation section.",
        "link": "https://dl.acm.org/doi/10.1145/3209280.3209526"
    },
    {
        "title": "Identification of Overpricing in the Purchase of Medication by the Federal Government of Brazil, Using Text Mining and Clustering Based on Ontology",
        "authors": [
            "Marco Aurelio O. S. Correa",
            "Adriano Galindo Leal"
        ],
        "date": "August 2018",
        "source": "ICCBDC '18: Proceedings of the 2018 2nd International Conference on Cloud and Big Data Computing",
        "abstract": "Increasing the transparency level in his actions and spending is one of the primary duties of the Brazilian Federal Government. The creation of laws that oblige full disclose of all its expenditures through transparency portals enables citizens to supervise all government entities. However, only the dissemination of these data, without a definite standard or the availability of data analysis tools, does not guarantee that the citizen is empowered to play his role. Therefore hence, the objective of this work is to identify overprice in the acquisition of products purchased by the federal government of Brazil using the unstructured data available on the Transparency Portal. The last two-years' worth of purchasing data, available in the Transparency Portal, were extracted, processed and stored. Due to his diverse nature and high volume of data, this study focused only on medicines purchased by the Ministry of Health. Ontology-based text mining and clustering techniques were applied for automatic identification and classification of products. The processing of this information was done through text mining and clustering, based on the ontology registered in another database of the Brazilian government. Because of this work, a consolidated price base per medication was created to allow the identification of distortions in prices practised, facilitating the identification of cases that merit further investigation to unravel fraud to the treasury.",
        "link": "https://dl.acm.org/doi/10.1145/3264560.3264569"
    },
    {
        "title": "Computational Estimation by Scientific Data Mining with Classical Methods to Automate Learning Strategies of Scientists",
        "authors": [
            "Aparna S. Varde"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Experimental results are often plotted as 2-dimensional graphical plots (aka graphs) in scientific domains depicting dependent versus independent variables to aid visual analysis of processes. Repeatedly performing laboratory experiments consumes significant time and resources, motivating the need for computational estimation. The goals are to estimate the graph obtained in an experiment given its input conditions, and to estimate the conditions that would lead to a desired graph. Existing estimation approaches often do not meet accuracy and efficiency needs of targeted applications. We develop a computational estimation approach called AutoDomainMine that integrates clustering and classification over complex scientific data in a framework so as to automate classical learning methods of scientists. Knowledge discovered thereby from a database of existing experiments serves as the basis for estimation. Challenges include preserving domain semantics in clustering, finding matching strategies in classification, striking a good balance between elaboration and conciseness while displaying estimation results based on needs of targeted users, and deriving objective measures to capture subjective user interests. These and other challenges are addressed in this work. The AutoDomainMine approach is used to build a computational estimation system, rigorously evaluated with real data in Materials Science. Our evaluation confirms that AutoDomainMine provides desired accuracy and efficiency in computational estimation. It is extendable to other science and engineering domains as proved by adaptation of its sub-processes within fields such as Bioinformatics and Nanotechnology.",
        "link": "https://dl.acm.org/doi/10.1145/3502736"
    },
    {
        "title": "Where is the road for issue reports classification based on text mining?",
        "authors": [
            "Qiang Fan",
            "Yue Yu",
            "Gang Yin",
            "Tao Wang",
            "Huaimin Wang"
        ],
        "date": "November 2017",
        "source": "ESEM '17: Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Currently, open source projects receive various kinds of issues daily, because of the extreme openness of Issue Tracking System (ITS) in GitHub. ITS is a labor-intensive and time-consuming task of issue categorization for project managers. However, a contributor is only required a short textual abstract to report an issue in GitHub. Thus, most traditional classification approaches based on detailed and structured data (e.g., priority, severity, software version and so on) are difficult to adopt. In this paper, issue classification approaches on a large-scale dataset, including 80 popular projects and over 252,000 issue reports collected from GitHub, were investigated. First, four traditional text-based classification methods and their performances were discussed. Semantic perplexity (i.e., an issues description confuses bug-related sentences with nonbug-related sentences) is a crucial factor that affects the classification performances based on quantitative and qualitative study. Finally, A two-stage classifier framework based on the novel metrics of semantic perplexity of issue reports was designed. Results show that our two-stage classification can significantly improve issue classification performances.",
        "link": "https://dl.acm.org/doi/10.1109/ESEM.2017.19"
    },
    {
        "title": "Feature-based Facebook reviews process model for e-management using data mining",
        "authors": [
            "Anish Kumar Varudharajulu",
            "Yongsheng Ma"
        ],
        "date": "January 2019",
        "source": "IC4E '19: Proceedings of the 10th International Conference on E-Education, E-Business, E-Management and E-Learning",
        "abstract": "The data generated from online communication acts as potential gold mines for discovering knowledge for researchers. A large amount of data is also generated in the form of web documents, emails, blogs, and feedback, etc. Text analytics is being significantly employed to mine important information. Opinion mining is the process of extracting human thoughts and perceptions from unstructured texts. The showstopper for designing an opinion mining system for analyzing reviews arise from the fact that customer reviews are often noisy. These reviews are informally written. In addition, they are subjected to spelling mistakes, grammatical errors, improper punctuation and irrational capitalization. This paper focuses on analyzing the different classification and clustering algorithms aimed at extracting and consolidating opinions of customers from social media sites like Facebook, Twitter and through surveys, at multiple levels of granularity to monitor and measure customer satisfaction. Ours is an automated approach, in which the system aids in the process of knowledge assimilation for knowledge-based building and also performs the analytics. Domain experts ratify the knowledge base and also provide training datasets for the system to intuitively gather more instances for ratification. The system identifies opinion expressions as phrases containing opinion words, opinionated features and also opinion modifiers. These expressions are categorized as positive, negative or neutral. Opinion expressions are identified and categorized using localized linguistic techniques. Opinions can be congregated at any desired level of specificity i.e. feature level or product level, user level or service level, etc. We have developed a system based on this approach, which provides the user with a platform to analyze opinion expressions crawled from a set of pre-defined datasets.",
        "link": "https://dl.acm.org/doi/10.1145/3306500.3306514"
    },
    {
        "title": "Discriminative Topic Mining via Category-Name Guided Text Embedding",
        "authors": [
            "Yu Meng",
            "Jiaxin Huang",
            "Guangyuan Wang",
            "Zihan Wang",
            "Chao Zhang",
            "Yu Zhang",
            "Jiawei Han"
        ],
        "date": "April 2020",
        "source": "WWW '20: Proceedings of The Web Conference 2020",
        "abstract": "Mining a set of meaningful and distinctive topics automatically from massive text corpora has broad applications. Existing topic models, however, typically work in a purely unsupervised way, which often generate topics that do not fit users’ particular needs and yield suboptimal performance on downstream tasks. We propose a new task, discriminative topic mining, which leverages a set of user-provided category names to mine discriminative topics from text corpora. This new task not only helps a user understand clearly and distinctively the topics he/she is most interested in, but also benefits directly keyword-driven classification tasks. We develop CatE, a novel category-name guided text embedding method for discriminative topic mining, which effectively leverages minimal user guidance to learn a discriminative embedding space and discover category representative terms in an iterative manner. We conduct a comprehensive set of experiments to show that CatE mines high-quality set of topics guided by category names only, and benefits a variety of downstream applications including weakly-supervised classification and lexical entailment direction identification.",
        "link": "https://dl.acm.org/doi/10.1145/3366423.3380278"
    },
    {
        "title": "Comparative Study of Heart Disease Diagnosis Using Top Ten Data Mining Classification Algorithms",
        "authors": [
            "I. Ketut Agung Enriko"
        ],
        "date": "June 2019",
        "source": "ICFET '19: Proceedings of the 5th International Conference on Frontiers of Educational Technologies",
        "abstract": "Data mining has been used for many purposes, especially for prediction system. In healthcare, data mining algorithms often used in disease diagnosis. Meanwhile, heart disease is known as a primary cause of death over the years. Many studies have been performed in heart disease diagnosis using data mining methods. There are some popular data mining algorithms that can be used in heart disease diagnosis, for example, k-Nearest Neighbor, CART, and AdaBoost. The algorithms are used to analyze a sample of cardiovascular patients data and predict the heart disease type that they suffer. Some parameters are taken from the patient, including EKG morphology, blood pressure, and information about the existence of chest pain, shortness of breath, palpitation, and cold sweat. In this study, medical records data are collected from Harapan Kita Hospital and utilized as a dataset sample in this research. Top ten data mining classification algorithms are used in diagnosing heart disease from Harapan Kita Hospital data and examining their performance by checking the accuracy and speed.",
        "link": "https://dl.acm.org/doi/10.1145/3338188.3338220"
    },
    {
        "title": "Text Mining Approach for Identifying Research Trends",
        "authors": [
            "Snezhana Sulova"
        ],
        "date": "June 2021",
        "source": "CompSysTech '21: Proceedings of the 22nd International Conference on Computer Systems and Technologies",
        "abstract": "With the increase of unstructured data, the issues connected with automatic text processing, the categorization of documents and the discovery of topics have become objects of growing interest. In order to improve the process of grouping and processing research publications, we would like to propose a method based upon natural language processing. It is based on text mining technologies which aim to identify key tendencies in documents. It processes the content of publications by clustering and identifies the topics of each identified group. This analysis helps by identifying key tendencies as well as discovering emerging new areas of research. Publications from the research literature database, Scopus, were used to test the approach. The topic of the publications is “the application of digital technologies in the logistics business”. The experiments were completed using the RapidMiner Studio software.",
        "link": "https://dl.acm.org/doi/10.1145/3472410.3472433"
    },
    {
        "title": "Assessment of Vulnerability Severity using Text Mining",
        "authors": [
            "Georgios Spanos",
            "Lefteris Angelis",
            "Dimitrios Toloudis"
        ],
        "date": "September 2017",
        "source": "PCI '17: Proceedings of the 21st Pan-Hellenic Conference on Informatics",
        "abstract": "Software1 vulnerabilities are closely associated with information systems security, a major and critical field in today's technology. Vulnerabilities constitute a constant and increasing threat for various aspects of everyday life, especially for safety and economy, since the social impact from the problems that they cause is complicated and often unpredictable. Although there is an entire research branch in software engineering that deals with the identification and elimination of vulnerabilities, the growing complexity of software products and the variability of software production procedures are factors contributing to the ongoing occurrence of vulnerabilities, Hence, another area that is being developed in parallel focuses on the study and management of the vulnerabilities that have already been reported and registered in databases. The information contained in such databases includes, a textual description and a number of metrics related to vulnerabilities. The purpose of this paper is to investigate to what extend the assessment of the vulnerability severity can be inferred directly from the corresponding textual description, or in other words, to examine the informative power of the description with respect to the vulnerability severity. For this purpose, text mining techniques, i.e. text analysis and three different classification methods (decision trees, neural networks and support vector machines) were employed. The application of text mining to a sample of 70,678 vulnerabilities from a public data source shows that the description itself is a reliable and highly accurate source of information for vulnerability prioritization.",
        "link": "https://dl.acm.org/doi/10.1145/3139367.3139390"
    },
    {
        "title": "Clustering Algorithms for Spatial Data Mining",
        "authors": [
            "Chetashri Bhadane",
            "Ketan Shah"
        ],
        "date": "April 2020",
        "source": "ICGDA '20: Proceedings of the 2020 3rd International Conference on Geoinformatics and Data Analysis",
        "abstract": "With the advances in mobile and wireless technologies, there has been a rise in applications that track and share the users' geospatial data. People use several social networking sites such as Twitter, Facebook and Flickr, where they share their status updates. With the integration of Global Positioning System (GPS) with mobile phones, it is now possible to share one's locations on these social networks. GPS allows us to record and track a person's movement along with the timestamp. The data set obtained from these GPS logs is vast and is widely used to analyze the users' movement patterns. Specifically, we can find out significant locations based on the number of users present at that location and the time spent by them at such places. Once significant places have been identified, it is also possible to identify the semantic importance of these locations. This paper presents an overview of the clustering techniques used to find important places of interest using large GPS based mobility datasets. Four clustering algorithms, K-Means, DBSCAN, OPTICS and Hierarchical, are implemented, and performance is tested using real-time data of 50 users collected over 2--5 years. Performance summary depicts that K-Means and DBSCAN perform well for spatial data.",
        "link": "https://dl.acm.org/doi/10.1145/3397056.3397068"
    },
    {
        "title": "Feature Selection based Arabic Text Classification using Different Machine Learning Algorithms: Comparative Study",
        "authors": [
            "Sakina Rim Bennabi",
            "Zakaria Elberrichi"
        ],
        "date": "June 2020",
        "source": "ICIST '20: Proceedings of the 10th International Conference on Information Systems and Technologies",
        "abstract": "Feature selection is a method of data pre-processing widely used when mining large data, such as textual classification. Several studies have been conducted to compare the different methods of feature selection applied to corpora in English. Unfortunately, a small number of works concern the Arabic language. This article aims to present a comparative study of different feature selection techniques including: Chi2, the ANOVA method and mutual information, applied on a corpus in Arabic language, while also diversifying the machine learning algorithms (Naive Bayes, SVM and KNN). This experimental study has shown in general that reducing dimensionality with feature selection techniques has slightly affected the performance of textual classification, reducing the size of the corpus by up to 1%.",
        "link": "https://dl.acm.org/doi/10.1145/3447568.3448531"
    },
    {
        "title": "Intelligent Data Analysis using Optimized Support Vector Machine Based Data Mining Approach for Tourism Industry",
        "authors": [
            "Ms Promila Sharma",
            "Uma Meena",
            "Girish Kumar Sharma"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Data analysis involves the deployment of sophisticated approaches from data mining methods, information theory, and artificial intelligence in various fields like tourism, hospitality, and so on for the extraction of knowledge from the gathered and preprocessed data. In tourism, pattern analysis or data analysis using classification is significant for finding the patterns that represent new and potentially useful information or knowledge about the destination and other data. Several data mining techniques are introduced for the classification of data or patterns. However, overfitting, less accuracy, local minima, sensitive to noise are the drawbacks in some existing data mining classification methods. To overcome these challenges, Support vector machine with Red deer optimization (SVM-RDO) based data mining strategy is proposed in this article. Extended Kalman filter (EKF) is utilized in the first phase, i.e., data cleaning to remove the noise and missing values from the input data. Mantaray foraging algorithm (MaFA) is used in the data selection phase, in which the significant data are selected for the further process to reduce the computational complexity. The final phase is the classification, in which SVM-RDO is proposed to access the useful pattern from the selected data. PYTHON is the implementation tool used for the experiment of the proposed model. The experimental analysis is done to show the efficacy of the proposed work. From the experimental results, the proposed SVM-RDO achieved better accuracy, precision, recall, and F1 score than the existing methods for the tourism dataset. Thus, it is showed the effectiveness of the proposed SVM-RDO for pattern analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3494566"
    },
    {
        "title": "Applying text mining to predict learners' cognitive engagement",
        "authors": [
            "Hayati Hind",
            "Mohammed Khalidi Idrissi",
            "Samir Bennani"
        ],
        "date": "October 2017",
        "source": "SCAMS '17: Proceedings of the Mediterranean Symposium on Smart City Application",
        "abstract": "In academic process, engagement represent one of the important success' factor. This paper proposes a predictive system for learners' cognitive engagement based on their online discussion forums participation. In the first level the ontology OWL and the LSA method are used to perform a semantic classification system of the threads according to a specific context chosen by the tutor. Then the Text mining, as a predictive method, is applied to the classified threads and learners' participation in the forums.",
        "link": "https://dl.acm.org/doi/10.1145/3175628.3175655"
    },
    {
        "title": "Investigate Transitions into Drug Addiction through Text Mining of Reddit Data",
        "authors": [
            "John Lu",
            "Sumati Sridhar",
            "Ritika Pandey",
            "Mohammad Al Hasan",
            "Georege Mohler"
        ],
        "date": "July 2019",
        "source": "KDD '19: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Increasing rates of opioid drug abuse and heightened prevalence of online support communities underscore the necessity of employing data mining techniques to better understand drug addiction using these rapidly developing online resources. In this work, we obtained data from Reddit, an online collection of forums, to gather insight into drug use/misuse using text snippets from users narratives. Specifically, using users' posts, we trained a binary classifier which predicts a user's transitions from casual drug discussion forums to drug recovery forums. We also proposed a Cox regression model that outputs likelihoods of such transitions. In doing so, we found that utterances of select drugs and certain linguistic features contained in one's posts can help predict these transitions. Using unfiltered drug-related posts, our research delineates drugs that are associated with higher rates of transitions from recreational drug discussion to support/recovery discussion, offers insight into modern drug culture, and provides tools with potential applications in combating the opioid crisis.",
        "link": "https://dl.acm.org/doi/10.1145/3292500.3330737"
    },
    {
        "title": "Analysing digital banking reviews using text mining",
        "authors": [
            "Li Chen Cheng",
            "Legaspi Rhea Sharmayne"
        ],
        "date": "December 2020",
        "source": "ASONAM '20: Proceedings of the 12th IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Digital banks are new entrants in the banking industry in the Philippines as they only started late 2018. Since then, a handful of players have and are still emerging. With more and more people becoming technologically savvy, it is very critical for financial institutions to develop a digital banking application that will stand out from the competition. This paper aims to use text mining methods to analyse digital banking application reviews. This study will perform topic modelling using LDA to explore customer concerns and will mine association rules between the digital banking features with the review score. The results will reveal which areas the digital banking application can further optimize for customer satisfaction and retention.",
        "link": "https://dl.acm.org/doi/10.1109/ASONAM49781.2020.9381429"
    },
    {
        "title": "Mining Moocs Videos Metadata Using Classification Techniques",
        "authors": [
            "El Harrak Othman",
            "Ghadi Abderrahim",
            "El Bouhdidi Jaber"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "In few years, the internet has become the first source of information for most people, Today MOOCs makes it possible for everyone to access to the education over the world, it's represents an emerging methodology of online teaching and an important development in open education. Due to the rapid development of Moocs and the rapid growth of digital data and video database over the Internet, it is becoming very difficult for learners to browsing and choosing the best training for them. The scientific community has increased the amount of research into new technologies, with a view to improve the digital video utilization: its archiving, indexing, accessibility, acquisition, store and even its process and usability. All these parts of the video utilization entail the necessity of the extraction of all important information of a video, especially in cases of lack of metadata information. Web video mining is retrieving the content using data mining techniques from World Wide Web. There are two approaches for web video mining using traditional image processing (signal processing) and metadata based approach. In this paper, we present the various research makes in this subject and we propose an effective methodology to extract the metadata from Moocs videos and classify them based on the extracted metadata by applying data mining techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090450"
    },
    {
        "title": "Text mining with HathiTrust",
        "authors": [
            "Eleanor Dickson Koehl",
            "Ryan Dubnicek"
        ],
        "date": "June 2019",
        "source": "JCDL '19: Proceedings of the 18th Joint Conference on Digital Libraries",
        "abstract": "This tutorial will introduce attendees to the HathiTrust Research Center and its tools and services for computational text analysis research. HTRC leverages the scope and scale of the HathiTrust Digital Library collection to create opportunities for researchers to perform text data mining on subsets of the corpus. Attendees will develop skills that will allow them to conduct basic text analysis research using HathiTrust data, learn techniques for engaging with scholars on their text data mining research, and explore strategies for computational access to large-scale digital collections.",
        "link": "https://dl.acm.org/doi/10.1109/JCDL.2019.00115"
    },
    {
        "title": "Text mining for plagiarism detection: multivariate pattern detection for recognition of text similarities",
        "authors": [
            "Konstantinos Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2018",
        "source": "ASONAM '18: Proceedings of the 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "The problem of plagiarism the recent years has been intensified by the availability of information in digital form and the accessibility of the electronic libraries through the Internet. As a result, plagiarism detection has been transformed into a big data analytics problem since the number of digital sources is extravagant and a new document needs to be compared with millions of other existing documents. In this paper, a text mining methodology is proposed that can detect all common patterns between a document and the documents in a reference database. The technique is based on a pattern detection algorithm and the corresponding data structure that enables the algorithm to detect all common patterns. The methodology has been applied in a well-defined dataset providing very promising results identifying difficult cases of plagiarism such as technical disguise.",
        "link": "https://dl.acm.org/doi/10.5555/3382225.3382424"
    },
    {
        "title": "Petroleum Engineering Data Text Classification Using Convolutional Neural Network Based Classifier",
        "authors": [
            "Tzuhan Hsu",
            "Yaoqin Zhang"
        ],
        "date": "May 2018",
        "source": "ICMLT '18: Proceedings of the 2018 International Conference on Machine Learning Technologies",
        "abstract": "In this paper, authors introduce a convolutional neural network (CNN) based classification approach for petroleum engineering event. Recently, text classification problem has raised substantial attention in the field of machine learning. The aim of text classification process is to assign correct label to a text on the basis of its content. However, due to the imprecise of feature representation, most of former text classification methods cannot export satisfying result. To solve this problem, authors propose a model based on convolutional neural network. Authors conduct experiments with real world petroleum engineering data text, artificially labelled into 6 categories. Beside purposed CNN-based model, the authors also implement other baseline machine learning approaches: k nearest neighbor, random forest, support vector machines, recurrent neural network, gated recurrent units, long short-term memory. Among all these models, our CNN-based model reach best performance in text classification task. In the last part of this paper, authors compare and analysis their purpose and baseline model.",
        "link": "https://dl.acm.org/doi/10.1145/3231884.3231898"
    },
    {
        "title": "Classifying Indonesian Online Articles as Advertisement Placement Base Using Text Mining",
        "authors": [
            "Nadhira Tasya",
            "Arian Dhini"
        ],
        "date": "July 2017",
        "source": "ICBIM 2017: Proceedings of the International Conference on Business and Information Management",
        "abstract": "Rapid development in technological aspect resulting in growing level of human needs for the latest news, so that emerged a new trend of publishing and accessing news through online media or usually called online journalism. In addition, the number of people who sell and purchase through online sites also continues to increase and this opportunity is utilized by the company and the advertiser by implementing targeted web advertising. However, the high number of articles that have been published and accessed leads to great opportunities for errors in determining where to place the ads. Therefore, it needs a system that can categorize articles accessed by users as the basis of advertisement placement by the company and this classification system can be done by applying the method of Data Mining and Text Mining. This research uses document data in the form of article content that will be categorized into twenty categories of class of advertisement by using Text Mining technique with Support Vector Machine algorithm. The results of this study may be used by companies or advertisers as a basis for placement of ads on selected online media sites.",
        "link": "https://dl.acm.org/doi/10.1145/3134271.3134288"
    },
    {
        "title": "Mining Free-Text Medical Notes for Suicide Risk Assessment",
        "authors": [
            "Marios Adamou",
            "Grigoris Antoniou",
            "Elissavet Greasidou",
            "Vincenzo Lagani",
            "Paulos Charonyktakis",
            "Ioannis Tsamardinos"
        ],
        "date": "July 2018",
        "source": "SETN '18: Proceedings of the 10th Hellenic Conference on Artificial Intelligence",
        "abstract": "Suicide has been considered as an important public health issue for a very long time, and is one of the main causes of death worldwide. Despite suicide prevention strategies being applied, the rate of suicide has not changed substantially over the past decades. Advances in machine learning make it possible to attempt to predict suicide based on the analysis of relevant data to inform clinical practice. This paper reports on findings from the analysis of data of patients who died by suicide in the period 2013-2016 and made use of both structured data and free-text medical notes. We focus on examining various text-mining approaches to support risk assessment. The results show that using advance machine learning and text-mining techniques, it is possible to predict within a specified period which people are most at risk of taking their own life at the time of referral to a mental health service.",
        "link": "https://dl.acm.org/doi/10.1145/3200947.3201020"
    },
    {
        "title": "Integration of data mining techniques in e-learning systems: Clustering Profil of Lerners and Recommender Course System",
        "authors": [
            "A. El Moustamid",
            "E. En-Naimi",
            "J. El Bouhdidi"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "Data mining aims at the extraction of knowledge from large quantities of data by automatic or semi-automatic methods. In the field of education and training e-learning, Data Mining techniques as clustering (The aim of this technique is to identify groups that are similar in some aspect) [21], prediction (is used for developing model where combination of data viewed from different aspects can effect single viewed data. Here, is important to watch how predictors variables are effecting predicted variable)[21], classification are applied in e-learning to improve the level of learners by providing learning according to the learners' requirement) are used to analyze learners profile, to predict and improve student performance. The aim of our project is to develop a system capable of analyzing learners profile and indexing web videos in order to offer learners a rich database with courses that correspond to their levels.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090453"
    },
    {
        "title": "Multi-level mining and visualization of scientific text collections: Exploring a bi-lingual scientific repository",
        "authors": [
            "Pablo Accuosto",
            "Francesco Ronzano",
            "Daniel Ferrés",
            "Horacio Saggion"
        ],
        "date": "December 2017",
        "source": "WOSP 2017: Proceedings of the 6th International Workshop on Mining Scientific Publications",
        "abstract": "We present a system to mine and visualize collections of scientific documents by semantically browsing information extracted from single publications or aggregated throughout corpora of articles. The text mining tool performs deep analysis of document collections allowing the extraction and interpretation of research paper's contents. In addition to the extraction and enrichment of documents with metadata (titles, authors, affiliations, etc), the deep analysis performed comprises semantic interpretation, rhetorical analysis of sentences, triple-based information extraction, and text summarization. The visualization components allow geographical-based exploration of collections, topic-evolution interpretation, and collaborative network analysis among others. The paper presents a case study of a bi-lingual collection in the field of Natural Language Processing (NLP).",
        "link": "https://dl.acm.org/doi/10.1145/3127526.3127529"
    },
    {
        "title": "Hierarchical Topic Mining via Joint Spherical Tree and Text Embedding",
        "authors": [
            "Yu Meng",
            "Yunyi Zhang",
            "Jiaxin Huang",
            "Yu Zhang",
            "Chao Zhang",
            "Jiawei Han"
        ],
        "date": "August 2020",
        "source": "KDD '20: Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Mining a set of meaningful topics organized into a hierarchy is intuitively appealing since topic correlations are ubiquitous in massive text corpora. To account for potential hierarchical topic structures, hierarchical topic models generalize flat topic models by incorporating latent topic hierarchies into their generative modeling process. However, due to their purely unsupervised nature, the learned topic hierarchy often deviates from users' particular needs or interests. To guide the hierarchical topic discovery process with minimal user supervision, we propose a new task, Hierarchical Topic Mining, which takes a category tree described by category names only, and aims to mine a set of representative terms for each category from a text corpus to help a user comprehend his/her interested topics. We develop a novel joint tree and text embedding method along with a principled optimization procedure that allows simultaneous modeling of the category tree structure and the corpus generative process in the spherical space for effective category-representative term discovery. Our comprehensive experiments show that our model, named JoSH, mines a high-quality set of hierarchical topics with high efficiency and benefits weakly-supervised hierarchical text classification tasks.",
        "link": "https://dl.acm.org/doi/10.1145/3394486.3403242"
    },
    {
        "title": "Unsupervised behavioural mining and clustering for malware family identification",
        "authors": [
            "Khanh Huu The Dam",
            "Thomas Given-Wilson",
            "Axel Legay"
        ],
        "date": "March 2021",
        "source": "SAC '21: Proceedings of the 36th Annual ACM Symposium on Applied Computing",
        "abstract": "More accurate and advanced detection and classification of malware requires exploiting program behaviour and not merely syntactic or static features. One approach is to use system call dependency graphs (SCDGs) that represent the program behaviour by interactions with the system, and the relations between these interactions. These SCDGs have been used with supervised learning techniques to very accurately detect and classify malware. This works considers the unsupervised learning challenge of mining for common clusters of behaviour without a priori knowledge. This allows for clustering of similar programs by behaviour, that can then be used for either classification or further analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3412841.3441919"
    },
    {
        "title": "Text Mining for Evaluating Authors' Birth and Death Years",
        "authors": [
            "Dror Moghaz",
            "Yaakov Hacohen-Kerner",
            "Dov Gabbay"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "This article presents a unique method in text and data mining for finding the era, i.e., mining temporal data, in which an anonymous author was living. Finding this era can assist in the examination of a fake document or extracting the time period in which a writer lived. The study and the experiments concern Hebrew, and in some parts, Aramaic and Yiddish rabbinic texts. The rabbinic texts are undated and contain no bibliographic sections, posing an interesting challenge. This work proposes algorithms using key phrases and key words that allow the temporal organization of citations together with linguistic patterns. Based on these key phrases, key words, and the references, we established several types of “Iron-clad,” Heuristic and Greedy rules for estimating the years of birth and death of a writer in an interesting classification task. Experiments were conducted on corpora, including documents authored by 12, 24, and 36 rabbinic writers and demonstrated promising results.",
        "link": "https://dl.acm.org/doi/10.1145/3281631"
    },
    {
        "title": "Transfer Learning to Timed Text Based Video Classification Using CNN",
        "authors": [
            "Zenun Kastrati",
            "Ali Shariq Imran",
            "Arianit Kurti"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "Open educational video resources are gaining popularity with a growing number of massive open online courses (MOOCs). This has created a niche for content providers to adopt effective solutions in automatically organizing and structuring of educational resources for maximum visibility. Recent advances in deep learning techniques are proving useful in managing and classifying resources into appropriate categories. This paper proposes one such convolutional neural network (CNN) model for classifying video lectures in a MOOC setting using a transfer learning approach. The model uses a time-aligned text transcripts corresponding to video lectures from six broader subject categories. Video lectures and their corresponding transcript dataset is gathered from the Coursera MOOC platform. Two different CNN models are proposed: i) CNN based classification using embeddings learned from our MOOC dataset, ii) CNN based classification using transfer learning. Word embeddings generated from two well known state-of-the-art pre-trained models Word2Vec and GloVe, are used in the transfer learning approach for the second case. The proposed CNN models are evaluated using precision, recall, and F1 score and the obtained performance is compared with both conventional and deep learning classifiers. The proposed CNN models have an F1 score improvement of 10-22 percentage points over DNN and conventional classifiers",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326483"
    },
    {
        "title": "Evolutionary role mining in complex networks by ensemble clustering",
        "authors": [
            "Sarvenaz Choobdar",
            "Pedro Ribeiro",
            "Fernando Silva"
        ],
        "date": "April 2017",
        "source": "SAC '17: Proceedings of the Symposium on Applied Computing",
        "abstract": "The structural patterns in the neighborhood of nodes assign unique roles to the nodes. Mining the set of existing roles in a network provides a descriptive profile of the network and draws its general picture. This paper proposes a new method to determine structural roles in a dynamic network based on the current position of nodes and their historic behavior. We develop a temporal ensemble clustering technique to dynamically find groups of nodes, holding similar tempo-structural roles. We compare two weighting functions, based on age and distribution of data, to incorporate temporal behavior of nodes in the role discovery. To evaluate the performance of the proposed method, we assess the results from two points of view: 1) goodness of fit to current structure of the network; 2) consistency with historic data. We conduct the evaluation using different ensemble clustering techniques. The results on real world networks demonstrate that our method can detect tempo-structural roles that simultaneously depict the topology of a network and reflect its dynamics with high accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3019612.3019815"
    },
    {
        "title": "Analyzing used-car web listings via text mining",
        "authors": [
            "Ayhan Demiriz",
            "Fatma Cantaş"
        ],
        "date": "October 2017",
        "source": "IML '17: Proceedings of the 1st International Conference on Internet of Things and Machine Learning",
        "abstract": "Used car trade is one of the major components of the world economies. It is not uncommon to sell a car by placing an internet advertisement irrespective of the geography in these days. A typical content of an advertisement is usually composed of two parts namely the structured and the free text data. The structured data may include some information about the asking price, make, model, year, mileage of the car and the contact info. In most cases, seller may give important clues about the car's current conditions in the free text data where the title (head) of the advertisement can be included as free text too. This paper reports preliminary results from a text mining study conducted on 75K used car internet listings collected from two major car listing web sites in Turkey. As expected, the words and the phrases related to the description of the car are observed to be frequent. The leading concepts in the free text are found to be regarding how to describe the current condition of a car, for example \"no crash history\".",
        "link": "https://dl.acm.org/doi/10.1145/3109761.3109782"
    },
    {
        "title": "Text classification of Diseases Treated by Traditional Chinese Medicine Prescription based on machine learning",
        "authors": [
            "Hanqing Zhao",
            "Bo Han",
            "Chen Li"
        ],
        "date": "September 2020",
        "source": "ISAIMS '20: Proceedings of the 1st International Symposium on Artificial Intelligence in Medical Sciences",
        "abstract": "OBJECTIVE: To explore the application of machine learning in th e identification of diseases treated by traditional Chinese medicine prescriptions. METHODS: Based on the composition of the text document of Chinese medicine prescriptions, the prescriptions were divided into cough, headache and diarrhea. THUTCT were introduced to establish and train two machine learning text classification models, LibSVM and LibLinear, and the prescriptions to be deter mined were put into the model for classification and prediction. R ESULTS: The Precision rate of LibSVM model and Liblinear mod el were 0.7283 and 0.6690. Seven prescriptions were classified an d predicted, and the results were in line with expectations. CONCLUSION: THUCTC has good universality for the content of TCM prescriptions, high classification accuracy and fast testing speed, which is suitable for the text classification and discrimination research of TCM prescriptions for diseases.",
        "link": "https://dl.acm.org/doi/10.1145/3429889.3429895"
    },
    {
        "title": "Evaluation of retweet clustering method classification method using retweets on Twitter without text data",
        "authors": [
            "K. Uchida",
            "F. Toriumi",
            "T. Sakaki"
        ],
        "date": "August 2017",
        "source": "WI '17: Proceedings of the International Conference on Web Intelligence",
        "abstract": "Burst phenomena, which frequently occur on social media, are caused by such social events as flaming on the internet, elections, and natural disasters. To understand people's thoughts and feelings, we must classify their opinions from burst phenomena. Therefore, classification methods that categorize tweets are critical. However, since most classification methods focus on text mining, they cannot group tweets by topics because each tweet has poor linguistic similarities. We used a non-text-based classification method proposed by Baba et al. that groups tweets by topics, even if they have poor linguistic similarities, and verified its validity by comparing it with a text-based classification method in two different evaluations: qualitative and quantitative. In the qualitative evaluation part, we did a questionnaire survey and validated the suitability of the topic clusters created using both the non-and text-based methods. Since evaluating the similarity of every pair of tweets in each topic is difficult, we evaluated the similarity between sampled pairs in the survey and acquired more appropriate topic clustering results using the non-text-based method than the text-based method. In the quantitative evaluation part, we focused on the robustness of each method against data reduction. Many approaches analyze social media data, especially because collecting data from social media is comparatively easy. However, since collecting the whole data of burst phenomena is very costly due to the vast amounts of available social media data, robustness against data reduction is an important index to evaluate classification methods. With the non-text-based method, over 55% of the pairs of tweets in the same cluster were also included in the same cluster even when the data were reduced to 10% in all three of our example cases. In this paper, as a source we focus on Twitter, one of the most popular microblogging services. Using clustering to conduct detailed case analyses, we scrutinized three burst cases that include natural disasters and flaming on the internet and found that a non-text-based method more effectively classified tweets in burst phenomena than a text-based method.",
        "link": "https://dl.acm.org/doi/10.1145/3106426.3106451"
    },
    {
        "title": "Using Spark for Text Mining on Large Scale Liver Cancer Literature",
        "authors": [
            "Ming-Yen Lin",
            "Yu-Ju Lin",
            "Sue-Chen Hsueh"
        ],
        "date": "January 2021",
        "source": "BDET 2021: 2021 the 3rd International Conference on Big Data Engineering and Technology (BDET)",
        "abstract": "Cancer is one of the main causes of death. The number of known cancer to-date is more than one hundred. Liver cancer, ranked after trachea and lung cancer, has long been high in the cancer leading cause in Taiwan and even ranked as second in 2016. The number of scientific articles related to cancer proliferates every year. The number reaches as high as 20 million in PubMed so that discovering useful information from the massive collection is very difficult. In addition, using a single machine to sift through these articles is very time-consuming. Therefore, we present a big data analytic framework using the distributed Apache Spark platform for text mining in PubMed literature. We establish a prediction model for liver cancer articles so that researchers may effectively validate whether an article is related to liver cancer or not. Classification models in Spark MLlib including Linear Support Vector Machines (SVM), Logistic Regression, are used in our experiments. Relevancy to liver cancer is further confirmed by using MeSH (Medical Subject Headings) terms. Logistic regression is about 3 times faster than SVMs and the accuracy of both methods is close to 95% in the experiments using hold-out validation. When max_features is 500 and min_df ≤ 0.1 (or min_df = 1), the accuracy may reach 96%. In the experiments with K-fold cross-validation, the accuracy of SVMs methods is 96%. The experimental results show that that our prediction model may effectively classify liver cancer articles.",
        "link": "https://dl.acm.org/doi/10.1145/3474944.3474958"
    },
    {
        "title": "Automatic topic classification of test cases using text mining at an Android smartphone vendor",
        "authors": [
            "Junji Shimagaki",
            "Yasutaka Kamei",
            "Naoyasu Ubayashi",
            "Abram Hindle"
        ],
        "date": "October 2018",
        "source": "ESEM '18: Proceedings of the 12th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Background: An Android smartphone is an ecosystem of applications, drivers, operating system components, and assets. The volume of the software is large and the number of test cases needed to cover the functionality of an Android system is substantial. Enormous effort has been already taken to properly quantify \"what features and apps were tested and verified?\". This insight is provided by dashboards that summarize test coverage and results per feature. One method to achieve this is to manually tag or label test cases with the topic or function they cover, much like function points. At the studied Android smartphone vendor, tests are labelled with manually defined tags, so-called \"feature labels (FLs)\", and the FLs serve to categorize 100s to 1000s test cases into 10 to 50 groups. Aim: Unfortunately for developers, manual assignment of FLs to 1000s of test cases is a time consuming task, leading to inaccurately labeled test cases, which will render the dashboard useless. We created an automated system that suggests tags/labels to the developers for their test cases rather than manual labeling. Method: We use machine learning models to predict and label the functionality tested by 10,000 test cases developed at the company. Results: Through the quantitative experiments, our models achieved acceptable F-1 performance of 0.3 to 0.88. Also through the qualitative studies with expert teams, we showed that the hierarchy and path of tests was a good predictor of a feature's label. Conclusions: We find that this method can reduce tedious manual effort that software developers spent classifying test cases, while providing more accurate classification results.",
        "link": "https://dl.acm.org/doi/10.1145/3239235.3268927"
    },
    {
        "title": "To apply Data Mining for Classification of Crowd sourced Software Requirements",
        "authors": [
            "Soonh Taj",
            "Qasim Arain",
            "Imran Memon",
            "Asma Zubedi"
        ],
        "date": "April 2019",
        "source": "ICSIE '19: Proceedings of the 8th International Conference on Software and Information Engineering",
        "abstract": "Now a day's main focus of developers is to build quality software that works according to customer needs and for this reason it is necessary to gather right requirements as requirement elicitation is the critical step that impacts on the success of software project as misinterpreted requirements leads to the failure of software project. By keeping this in mind a research is carried out on improving requirements elicitation process and automating the process of classifying requirements. In this research, a model is proposed which will help in this scenario for requirements elicitation and requirement classification. This paper presents a model in which crowd sourcing approach is used so that customers, end users, stakeholders, developers and software engineers can make active participation for requirement elicitation process and requirements gathered using crowdsourcing approach are used by model for classification process i.e. classification of requirements into functional and non-functional requirements. For the proof of proposed model a case study is conducted. Results of case study provided the usefulness and efficiency of proposed model for classification of crowd sourced software requirements.",
        "link": "https://dl.acm.org/doi/10.1145/3328833.3328837"
    },
    {
        "title": "Text classification using Fuzzy TF-IDF and Machine Learning Models",
        "authors": [
            "Mariem Bounabi",
            "Karim El Moutaouakil",
            "Khalid Satori"
        ],
        "date": "October 2019",
        "source": "BDIoT'19: Proceedings of the 4th International Conference on Big Data and Internet of Things",
        "abstract": "The representation of the information has an important impact on the text classification task. Several weighting methods were proposed in the literature, and the term frequency-inverse term frequency (TFIDF), the most know on the text treatment field. The FTF-IDF is a vector representation where the components of the TFIDF are presented as inputs to the Fuzzy Inference System (FIS). In this work, we compare several Machin Learning algorithms such as Naïve Bayes and its derivatives, SVM and Random forest classifiers, using the FTF-IDF representation. To improve the quality of the used classifiers, we call sum attribute selection methods. The recognition rate, for the tested systems, is satisfied, where the system based on naïve Bayes classifier, the FTF-IDF weighting terms, and the info gain select attributes method gives 98.7% as accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3372938.3372956"
    },
    {
        "title": "Text Mining in Cybersecurity: A Systematic Literature Review",
        "authors": [
            "Luciano Ignaczak",
            "Guilherme Goldschmidt",
            "Cristiano André Da Costa",
            "Rodrigo Da Rosa Righi"
        ],
        "date": null,
        "source": "ACM Computing Surveys",
        "abstract": "The growth of data volume has changed cybersecurity activities, demanding a higher level of automation. In this new cybersecurity landscape, text mining emerged as an alternative to improve the efficiency of the activities involving unstructured data. This article proposes a Systematic Literature Review (SLR) to present the application of text mining in the cybersecurity domain. Using a systematic protocol, we identified 2,196 studies, out of which 83 were summarized. As a contribution, we propose a taxonomy to demonstrate the different activities in the cybersecurity domain supported by text mining. We also detail the strategies evaluated in the application of text mining tasks and the use of neural networks to support activities involving unstructured data. The work also discusses text classification performance aiming its application in real-world solutions. The SLR also highlights open gaps for future research, such as the analysis of non-English content and the intensification in the usage of neural networks.",
        "link": "https://dl.acm.org/doi/10.1145/3462477"
    },
    {
        "title": "Using Text Mining to Analyze the Financial Patents",
        "authors": [
            "Yung-Feng Lu",
            "Jia-Lang Xu",
            "Mu-Yen Chen"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "With the advancement and development of science and technology, Nowadays society is the development of information technology, cloud computing, mobile Internet, Internet of Things, block chain and information technology.The combination of finance and technology has made financial technology terms gradually emerging.. This research analyzes the patents of financial technology. However, Taiwan's financial technology patents are mostly G06Q-20 (payment plan, architecture or agreement), G06Q-30 (commercial, such as marketing, shopping, payment, auction or e-commerce). G06Q-40 (financial, investment or tax treatment, insurance).The research use text mining technology to analyze and understand Taiwan's trends and developments in financial technology. The research results show that G06Q-20 patents are biased towards information security and mobile payment. G06Q-30 patents favor shopping websites and advertising. G06Q-40 patents are biased towards diversity, such as securities, insurance, finance, and stocks.",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326478"
    },
    {
        "title": "Identifying Online Profiles of Distance Learning Students Using Data Mining Techniques",
        "authors": [
            "Osama Islam",
            "Muazzam Siddiqui",
            "Naif Radi Aljohani"
        ],
        "date": "October 2019",
        "source": "ICDTE '19: Proceedings of the 3rd International Conference on Digital Technology in Education",
        "abstract": "Educational data has grown over the years with the increased use of technology within educational environments. This has led to a huge amount of data being stored in various data sources representing the student, his/her activities, and other aspects relevant to the learning process. To meet this analytical need, Educational Data Mining (EDM) has emerged to assist educational institutions in identifying key benefits such as students at risk, the level of student engagement or predicting student performance. The aim of this research was to explore the various aspects of student interaction data using data mining techniques to identify relevant patterns of behaviors and possible key attributes that have higher degrees of influence on distance learning students. The main findings identified several patterns of user online profiles based on a set of adopted learning strategies, the research proposed a framework for analyzing such interaction data based on R and Hadoop platforms to correlate online profiles with student performance.",
        "link": "https://dl.acm.org/doi/10.1145/3369199.3369249"
    },
    {
        "title": "Extremely Fast Decision Tree Mining for Evolving Data Streams",
        "authors": [
            "Albert Bifet",
            "Jiajin Zhang",
            "Wei Fan",
            "Cheng He",
            "Jianfeng Zhang",
            "Jianfeng Qian",
            "Geoff Holmes",
            "Bernhard Pfahringer"
        ],
        "date": "August 2017",
        "source": "KDD '17: Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
        "abstract": "Nowadays real-time industrial applications are generating a huge amount of data continuously every day. To process these large data streams, we need fast and efficient methodologies and systems. A useful feature desired for data scientists and analysts is to have easy to visualize and understand machine learning models. Decision trees are preferred in many real-time applications for this reason, and also, because combined in an ensemble, they are one of the most powerful methods in machine learning. In this paper, we present a new system called STREAMDM-C++, that implements decision trees for data streams in C++, and that has been used extensively at Huawei. Streaming decision trees adapt to changes on streams, a huge advantage since standard decision trees are built using a snapshot of data, and can not evolve over time. STREAMDM-C++ is easy to extend, and contains more powerful ensemble methods, and a more efficient and easy to use adaptive decision trees. We compare our new implementation with VFML, the current state of the art implementation in C, and show how our new system outperforms VFML in speed using less resources.",
        "link": "https://dl.acm.org/doi/10.1145/3097983.3098139"
    },
    {
        "title": "Data miners' little helper: data transformation activity cues for cluster analysis on document collections",
        "authors": [
            "Tania Cerquitelli",
            "Evelina Di Corso",
            "Francesco Ventura",
            "Silvia Chiusano"
        ],
        "date": "June 2017",
        "source": "WIMS '17: Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "In this paper we propose a new self-learning engine to streamline the analytics process, as it enables analysts to mine massive data repositories with minimal user intervention. In the context of cluster analysis on a collection of documents this new system, named SELF-DATA (SELF-learning DAta TrAnsformation), suggests to the analyst how to configure the whole mining process for a given dataset. SELF-DATA relies on an engine exploring different data weighting schemas (e.g., normalized term frequencies) and data transformation methods (e.g., PCA) before applying the cluster analysis, evaluating and comparing solutions through different quality indices (e.g., weighted Silhouette), and presenting the k-top solutions to the analyst. SELF-DATA will also include a knowledge base storing results of experiments on previously processed datasets, and a classification algorithm trained on the knowledge base content to forecast the best configuration for the whole mining process for an unexplored dataset. The first development of SELF-DATA running on Apache Spark has been validated on 5 collections of documents. Experimental results highlight that TF-IDF and logarithmic entropy are effective to measure item relevance with sparse datasets, and the LSI method outperforms PCA with a large dictionary.",
        "link": "https://dl.acm.org/doi/10.1145/3102254.3102288"
    },
    {
        "title": "Interactive Visual Graph Mining and Learning",
        "authors": [
            "Ryan A. Rossi",
            "Nesreen K. Ahmed",
            "Rong Zhou",
            "Hoda Eldardiry"
        ],
        "date": null,
        "source": "ACM Transactions on Intelligent Systems and Technology",
        "abstract": "This article presents a platform for interactive graph mining and relational machine learning called GraphVis. The platform combines interactive visual representations with state-of-the-art graph mining and relational machine learning techniques to aid in revealing important insights quickly as well as learning an appropriate and highly predictive model for a particular task (e.g., classification, link prediction, discovering the roles of nodes, and finding influential nodes). Visual representations and interaction techniques and tools are developed for simple, fast, and intuitive real-time interactive exploration, mining, and modeling of graph data. In particular, we propose techniques for interactive relational learning (e.g., node/link classification), interactive link prediction and weighting, role discovery and community detection, higher-order network analysis (via graphlets, network motifs), among others. GraphVis also allows for the refinement and tuning of graph mining and relational learning methods for specific application domains and constraints via an end-to-end interactive visual analytic pipeline that learns, infers, and provides rapid interactive visualization with immediate feedback at each change/prediction in real-time. Other key aspects include interactive filtering, querying, ranking, manipulating, exporting, as well as tools for dynamic network analysis and visualization, interactive graph generators (including new block model approaches), and a variety of multi-level network analysis techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3200764"
    },
    {
        "title": "Data Mining and Opinion Mining: A Tool in Educational Context",
        "authors": [
            "Myriam Peñafiel",
            "Stefanie Vásquez",
            "Diego Vásquez",
            "Juan Zaldumbide",
            "Sergio Luján-Mora"
        ],
        "date": "July 2018",
        "source": "ICoMS '18: Proceedings of the 2018 1st International Conference on Mathematics and Statistics",
        "abstract": "The use of the web as a universal communication platform generates large volumes of data (Big data), which in many cases, need to be processed so that they can become useful knowledge in face of the sceptics who have doubts about the credibility of such information. The use of web data that comes from educational contexts needs to be addressed, since that large amount of unstructured information is not being valued, losing valuable information that can be used. To solve this problem, we propose the use of data mining techniques such as sentiment analysis to validate the information that comes from the educational platforms. The objective of this research is to propose a methodology that allows the user to apply sentiment analysis in a simple way, because although some researchers have done it, very few do with data in the educational context. The results obtained prove that the proposal can be used in similar cases.",
        "link": "https://dl.acm.org/doi/10.1145/3274250.3274263"
    },
    {
        "title": "Research Hotspots Mining and Visualized Analysis Based on Linking Cluster and K-Core Decomposition",
        "authors": [
            "Ying Cai",
            "Fang Huang",
            "Shuai Wang",
            "Hao Zhang",
            "Chunxiu Du"
        ],
        "date": "May 2018",
        "source": "ICDPA 2018: Proceedings of the International Conference on Data Processing and Applications",
        "abstract": "In this paper, we focus on research hotspots mining with knowledge semantic features on discipline topic word networks, which are extracted from scientific and technical documents. For the topic networks, a novel method of combining linking community clustering with k-core decomposition is proposed to mine the hotspot community with knowledge hierarchy structure. And as a post-processing, the hierarchical community identification is implemented by k-core decomposition in order to discover the internal connection with knowledge semantic hierarchy in the research hotspot. Finally, the community density and visualization method is utilized to analyze the core and sub- problem in the research hotspot community. In experiments, the topic networks for a single discipline and interdisciplinary research hotspot are taken as examples of analysis, and the results show the proposed approach can effectively realize the objective of mining and analyzing the discipline research hotspot on the academic topic word network.",
        "link": "https://dl.acm.org/doi/10.1145/3224207.3224216"
    },
    {
        "title": "The Application of Data Mining Algorithms in the Construction of Travel Recommendation System",
        "authors": [
            "Chao Lou",
            "Zhaonan Mu",
            "Mengzhu Liu"
        ],
        "date": "January 2021",
        "source": "CONF-CDS 2021: The 2nd International Conference on Computing and Data Science",
        "abstract": "Photos shared by users of online social network often associated with tags, texts, geographic and time information. These data are ideal research resource which can be used by researchers to observe people's interests and behaviors. They can also be used to analyze people's travel behaviors and interests because it can reflect people's travel activities and experiences directly. This project aimed to consolidate people into groups based on their travel preferences of travel season, travel time span, scenery type of travel location and then analyze popular travel places for different people groups. The travel preferences were analyzed from extracted data applying PHP language and Naive Bayes classification approach. According to the analyzed data, people were divided into groups. Then, DBSCAN clustering was used to get popular locations based on the different groups. As a result, 80 thousand photo data of 5 cities have been collected from Flickr by Phython. It has been showed that people's preference of season in most of the mined cities was spring and summer and the preference of time span was short. Moreover, mined people has been divided into 16 groups according to their preference and based on the groups popular locations have been calculated by DBSCAN clustering. Theses locations have been provided in a recommendation website which was built in this project. For the future work, mining Flickr data associated with other travel social network sites could be a good way to build a more effective recommendation system.",
        "link": "https://dl.acm.org/doi/10.1145/3448734.3450477"
    },
    {
        "title": "Compare Machine Learning Models in Text Classification Using Steam User Reviews",
        "authors": [
            "Youchen Miao",
            "Zeyu Jin",
            "Yumeng Zhang",
            "Yuchen Chen",
            "Junren Lai"
        ],
        "date": "November 2021",
        "source": "ICSED '21: Proceedings of the 2021 3rd International Conference on Software Engineering and Development",
        "abstract": "Text Classification and Sentiment Analysis of game reviews are viewed as important parts in not only academic fields but also in game studies. In this paper, with more than 400 thousand game reviews on Steam platform, we preprocess the data using different libraries (sklearn, nltk, and spaCy) and use them as inputs to build three sentiment classification models based on different algorithms (Naive Bayes, SVM, and Random Forest). In contrast to previous studies that only focus on different sentiment analysis models, our paper also highlights the use of different APIs to preprocess the data and their corresponding model performance. The results show that no matter which API we choose, Random Forest models always perform the best. However, in terms of training time, Naive Bayes is the fastest. This work can be used to apply grid search for researchers to automatically find the optimum API before conducting sentiment analysis in the future.",
        "link": "https://dl.acm.org/doi/10.1145/3507473.3507480"
    },
    {
        "title": "Lost in Transduction: Transductive Transfer Learning in Text Classification",
        "authors": [
            "Alejandro Moreo",
            "Andrea Esuli",
            "Fabrizio Sebastiani"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Obtaining high-quality labelled data for training a classifier in a new application domain is often costly. Transfer Learning\n(a.k.a. “Inductive Transfer”) tries to alleviate these costs by transferring, to the “target” domain of interest, knowledge available from a different “source” domain. In transfer learning the lack of labelled information from the target domain is compensated by the availability at training time of a set of unlabelled examples from the target distribution. Transductive Transfer Learning denotes the transfer learning setting in which the only set of target documents that we are interested in classifying is known and available at training time. Although this definition is indeed in line with Vapnik’s original definition of “transduction”, current terminology in the field is confused. In this article, we discuss how the term “transduction” has been misused in the transfer learning literature, and propose a clarification consistent with the original characterization of this term given by Vapnik. We go on to observe that the above terminology misuse has brought about misleading experimental comparisons, with inductive transfer learning methods that have been incorrectly compared with transductive transfer learning methods. We then, give empirical evidence that the difference in performance between the inductive version and the transductive version of a transfer learning method can indeed be statistically significant (i.e., that knowing at training time the only data one needs to classify indeed gives an advantage). Our clarification allows a reassessment of the field, and of the relative merits of the major, state-of-the-art algorithms for transfer learning in text classification.",
        "link": "https://dl.acm.org/doi/10.1145/3453146"
    },
    {
        "title": "Machine Learning on Mining Potential Adverse Drug Reactions for Pharmacovigilance",
        "authors": [
            "Ke Jia"
        ],
        "date": "July 2021",
        "source": "DSIT 2021: 2021 4th International Conference on Data Science and Information Technology",
        "abstract": "Drugs are related to the lives and health of patients. Mining and discovering potential adverse drug reactions from a large number of unstructured texts play an important role in drug research and pharmacovigilance. It is a hot research issue in the field of biomedicine in recent years. A large number of scholars have discovered a large amount of information on adverse drug reactions from different data sources through effective machine learning methods. This paper reviews the research progress in recent years to provide a reference for future research.",
        "link": "https://dl.acm.org/doi/10.1145/3478905.3478964"
    },
    {
        "title": "TACAM: Topic And Context Aware Argument Mining",
        "authors": [
            "Michael Fromm",
            "Evgeniy Faerman",
            "Thomas Seidl"
        ],
        "date": "October 2019",
        "source": "WI '19: IEEE/WIC/ACM International Conference on Web Intelligence",
        "abstract": "In this work we address the problem of argument search. The purpose of argument search is the distillation of pro and contra arguments for requested topics from large text corpora. In previous works, the usual approach is to use a standard search engine to extract text parts which are relevant to the given topic and subsequently use an argument recognition algorithm to select arguments from them. The main challenge in the argument recognition task, which is also known as argument mining, is that often sentences containing arguments are structurally similar to purely informative sentences without any stance about the topic. In fact, they only differ semantically. Most approaches use topic or search term information only for the first search step and therefore assume that arguments can be classified independently of a topic. We argue that topic information is crucial for argument mining, since the topic defines the semantic context of an argument. Precisely, we propose different models for the classification of arguments, which take information about a topic of an argument into account. Moreover, to enrich the context of a topic and to let models understand the context of the potential argument better, we integrate information from different external sources such as Knowledge Graphs or pre-trained NLP models. Our evaluation shows that considering topic information, especially in connection with external information, provides a significant performance boost for the argument mining task.",
        "link": "https://dl.acm.org/doi/10.1145/3350546.3352506"
    },
    {
        "title": "Text mining for incoming tasks based on the urgency/importance factors and task classification using machine learning tools",
        "authors": [
            "Yasser Ali Alshehri"
        ],
        "date": "March 2020",
        "source": "ICCDA 2020: Proceedings of the 2020 the 4th International Conference on Compute and Data Analysis",
        "abstract": "In workplaces, there is a massive amount of unstructured data from different sources. In this paper, we present a case study that explains how can through communications between employees, we can help to prioritize tasks requests to increase the efficiency of their works for both technical and non-technical workers. This involves managing daily incoming tasks based on their level of urgency and importance.To allow all workers to utilize the urgency-importance matrix as a time-management tool, we need to automate this tool. The textual content of incoming tasks are analyzed, and metrics related to urgency and importance are extracted. A third factor (i.e., the response variable) is defined based on the two input variables (urgency and importance). Then, machine learning applied to the data to predict the class of incoming tasks based on data outcome desired. We used ordinal regression, neural networks, and decision tree algorithms to predict the four levels of task priority. We measure the performance of all using recalls, precisions, and F-scores. All classifiers perform higher than 89% in terms of all measures.",
        "link": "https://dl.acm.org/doi/10.1145/3388142.3388153"
    },
    {
        "title": "Text mining for malware classification using multivariate all repeated patterns detection",
        "authors": [
            "Konstantinos F. Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2019",
        "source": "ASONAM '19: Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Mobile phones have become nowadays a commodity to the majority of people. Using them, people are able to access the world of Internet and connect with their friends, their colleagues at work or even unknown people with common interests. This proliferation of the mobile devices has also been seen as an opportunity for the cyber criminals to deceive smartphone users and steel their money directly or indirectly, respectively, by accessing their bank accounts through the smartphones or by blackmailing them or selling their private data such as photos, credit card data, etc. to third parties. This is usually achieved by installing malware to smartphones masking their malevolent payload as a legitimate application and advertise it to the users with the hope that mobile users will install it in their devices. Thus, any existing application can easily be modified by integrating a malware and then presented it as a legitimate one. In response to this, scientists have proposed a number of malware detection and classification methods using a variety of techniques. Even though, several of them achieve relatively high precision in malware classification, there is still space for improvement. In this paper, we propose a text mining all repeated pattern detection method which uses the decompiled files of an application in order to classify a suspicious application into one of the known malware families. Based on the experimental results using a real malware dataset, the methodology tries to correctly classify (without any misclassification) all randomly selected malware applications of 3 categories with 3 different families each.",
        "link": "https://dl.acm.org/doi/10.1145/3341161.3350841"
    },
    {
        "title": "Comparative Study between Traditional Machine Learning and Deep Learning Approaches for Text Classification",
        "authors": [
            "Cannannore Nidhi Kamath",
            "Syed Saqib Bukhari",
            "Andreas Dengel"
        ],
        "date": "August 2018",
        "source": "DocEng '18: Proceedings of the ACM Symposium on Document Engineering 2018",
        "abstract": "In this contemporaneous world, it is an obligation for any organization working with documents to end up with the insipid task of classifying truckload of documents, which is the nascent stage of venturing into the realm of information retrieval and data mining. But classification of such humongous documents into multiple classes, calls for a lot of time and labor. Hence a system which could classify these documents with acceptable accuracy would be of an unfathomable help in document engineering. We have created multiple classifiers for document classification and compared their accuracy on raw and processed data. We have garnered data used in a corporate organization as well as publicly available data for comparison. Data is processed by removing the stop-words and stemming is implemented to produce root words. Multiple traditional machine learning techniques like Naive Bayes, Logistic Regression, Support Vector Machine, Random forest Classifier and Multi-Layer Perceptron are used for classification of documents. Classifiers are applied on raw and processed data separately and their accuracy is noted. Along with this, Deep learning technique such as Convolution Neural Network is also used to classify the data and its accuracy is compared with that of traditional machine learning techniques. We are also exploring hierarchical classifiers for classification of classes and subclasses. The system classifies the data faster and with better accuracy than if done manually. The results are discussed in the results and evaluation section.",
        "link": "https://dl.acm.org/doi/10.1145/3209280.3209526"
    },
    {
        "title": "Identification of Overpricing in the Purchase of Medication by the Federal Government of Brazil, Using Text Mining and Clustering Based on Ontology",
        "authors": [
            "Marco Aurelio O. S. Correa",
            "Adriano Galindo Leal"
        ],
        "date": "August 2018",
        "source": "ICCBDC '18: Proceedings of the 2018 2nd International Conference on Cloud and Big Data Computing",
        "abstract": "Increasing the transparency level in his actions and spending is one of the primary duties of the Brazilian Federal Government. The creation of laws that oblige full disclose of all its expenditures through transparency portals enables citizens to supervise all government entities. However, only the dissemination of these data, without a definite standard or the availability of data analysis tools, does not guarantee that the citizen is empowered to play his role. Therefore hence, the objective of this work is to identify overprice in the acquisition of products purchased by the federal government of Brazil using the unstructured data available on the Transparency Portal. The last two-years' worth of purchasing data, available in the Transparency Portal, were extracted, processed and stored. Due to his diverse nature and high volume of data, this study focused only on medicines purchased by the Ministry of Health. Ontology-based text mining and clustering techniques were applied for automatic identification and classification of products. The processing of this information was done through text mining and clustering, based on the ontology registered in another database of the Brazilian government. Because of this work, a consolidated price base per medication was created to allow the identification of distortions in prices practised, facilitating the identification of cases that merit further investigation to unravel fraud to the treasury.",
        "link": "https://dl.acm.org/doi/10.1145/3264560.3264569"
    },
    {
        "title": "Computational Estimation by Scientific Data Mining with Classical Methods to Automate Learning Strategies of Scientists",
        "authors": [
            "Aparna S. Varde"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Experimental results are often plotted as 2-dimensional graphical plots (aka graphs) in scientific domains depicting dependent versus independent variables to aid visual analysis of processes. Repeatedly performing laboratory experiments consumes significant time and resources, motivating the need for computational estimation. The goals are to estimate the graph obtained in an experiment given its input conditions, and to estimate the conditions that would lead to a desired graph. Existing estimation approaches often do not meet accuracy and efficiency needs of targeted applications. We develop a computational estimation approach called AutoDomainMine that integrates clustering and classification over complex scientific data in a framework so as to automate classical learning methods of scientists. Knowledge discovered thereby from a database of existing experiments serves as the basis for estimation. Challenges include preserving domain semantics in clustering, finding matching strategies in classification, striking a good balance between elaboration and conciseness while displaying estimation results based on needs of targeted users, and deriving objective measures to capture subjective user interests. These and other challenges are addressed in this work. The AutoDomainMine approach is used to build a computational estimation system, rigorously evaluated with real data in Materials Science. Our evaluation confirms that AutoDomainMine provides desired accuracy and efficiency in computational estimation. It is extendable to other science and engineering domains as proved by adaptation of its sub-processes within fields such as Bioinformatics and Nanotechnology.",
        "link": "https://dl.acm.org/doi/10.1145/3502736"
    },
    {
        "title": "Where is the road for issue reports classification based on text mining?",
        "authors": [
            "Qiang Fan",
            "Yue Yu",
            "Gang Yin",
            "Tao Wang",
            "Huaimin Wang"
        ],
        "date": "November 2017",
        "source": "ESEM '17: Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Currently, open source projects receive various kinds of issues daily, because of the extreme openness of Issue Tracking System (ITS) in GitHub. ITS is a labor-intensive and time-consuming task of issue categorization for project managers. However, a contributor is only required a short textual abstract to report an issue in GitHub. Thus, most traditional classification approaches based on detailed and structured data (e.g., priority, severity, software version and so on) are difficult to adopt. In this paper, issue classification approaches on a large-scale dataset, including 80 popular projects and over 252,000 issue reports collected from GitHub, were investigated. First, four traditional text-based classification methods and their performances were discussed. Semantic perplexity (i.e., an issues description confuses bug-related sentences with nonbug-related sentences) is a crucial factor that affects the classification performances based on quantitative and qualitative study. Finally, A two-stage classifier framework based on the novel metrics of semantic perplexity of issue reports was designed. Results show that our two-stage classification can significantly improve issue classification performances.",
        "link": "https://dl.acm.org/doi/10.1109/ESEM.2017.19"
    },
    {
        "title": "Feature-based Facebook reviews process model for e-management using data mining",
        "authors": [
            "Anish Kumar Varudharajulu",
            "Yongsheng Ma"
        ],
        "date": "January 2019",
        "source": "IC4E '19: Proceedings of the 10th International Conference on E-Education, E-Business, E-Management and E-Learning",
        "abstract": "The data generated from online communication acts as potential gold mines for discovering knowledge for researchers. A large amount of data is also generated in the form of web documents, emails, blogs, and feedback, etc. Text analytics is being significantly employed to mine important information. Opinion mining is the process of extracting human thoughts and perceptions from unstructured texts. The showstopper for designing an opinion mining system for analyzing reviews arise from the fact that customer reviews are often noisy. These reviews are informally written. In addition, they are subjected to spelling mistakes, grammatical errors, improper punctuation and irrational capitalization. This paper focuses on analyzing the different classification and clustering algorithms aimed at extracting and consolidating opinions of customers from social media sites like Facebook, Twitter and through surveys, at multiple levels of granularity to monitor and measure customer satisfaction. Ours is an automated approach, in which the system aids in the process of knowledge assimilation for knowledge-based building and also performs the analytics. Domain experts ratify the knowledge base and also provide training datasets for the system to intuitively gather more instances for ratification. The system identifies opinion expressions as phrases containing opinion words, opinionated features and also opinion modifiers. These expressions are categorized as positive, negative or neutral. Opinion expressions are identified and categorized using localized linguistic techniques. Opinions can be congregated at any desired level of specificity i.e. feature level or product level, user level or service level, etc. We have developed a system based on this approach, which provides the user with a platform to analyze opinion expressions crawled from a set of pre-defined datasets.",
        "link": "https://dl.acm.org/doi/10.1145/3306500.3306514"
    },
    {
        "title": "Discriminative Topic Mining via Category-Name Guided Text Embedding",
        "authors": [
            "Yu Meng",
            "Jiaxin Huang",
            "Guangyuan Wang",
            "Zihan Wang",
            "Chao Zhang",
            "Yu Zhang",
            "Jiawei Han"
        ],
        "date": "April 2020",
        "source": "WWW '20: Proceedings of The Web Conference 2020",
        "abstract": "Mining a set of meaningful and distinctive topics automatically from massive text corpora has broad applications. Existing topic models, however, typically work in a purely unsupervised way, which often generate topics that do not fit users’ particular needs and yield suboptimal performance on downstream tasks. We propose a new task, discriminative topic mining, which leverages a set of user-provided category names to mine discriminative topics from text corpora. This new task not only helps a user understand clearly and distinctively the topics he/she is most interested in, but also benefits directly keyword-driven classification tasks. We develop CatE, a novel category-name guided text embedding method for discriminative topic mining, which effectively leverages minimal user guidance to learn a discriminative embedding space and discover category representative terms in an iterative manner. We conduct a comprehensive set of experiments to show that CatE mines high-quality set of topics guided by category names only, and benefits a variety of downstream applications including weakly-supervised classification and lexical entailment direction identification.",
        "link": "https://dl.acm.org/doi/10.1145/3366423.3380278"
    },
    {
        "title": "Comparative Study of Heart Disease Diagnosis Using Top Ten Data Mining Classification Algorithms",
        "authors": [
            "I. Ketut Agung Enriko"
        ],
        "date": "June 2019",
        "source": "ICFET '19: Proceedings of the 5th International Conference on Frontiers of Educational Technologies",
        "abstract": "Data mining has been used for many purposes, especially for prediction system. In healthcare, data mining algorithms often used in disease diagnosis. Meanwhile, heart disease is known as a primary cause of death over the years. Many studies have been performed in heart disease diagnosis using data mining methods. There are some popular data mining algorithms that can be used in heart disease diagnosis, for example, k-Nearest Neighbor, CART, and AdaBoost. The algorithms are used to analyze a sample of cardiovascular patients data and predict the heart disease type that they suffer. Some parameters are taken from the patient, including EKG morphology, blood pressure, and information about the existence of chest pain, shortness of breath, palpitation, and cold sweat. In this study, medical records data are collected from Harapan Kita Hospital and utilized as a dataset sample in this research. Top ten data mining classification algorithms are used in diagnosing heart disease from Harapan Kita Hospital data and examining their performance by checking the accuracy and speed.",
        "link": "https://dl.acm.org/doi/10.1145/3338188.3338220"
    },
    {
        "title": "Text Mining Approach for Identifying Research Trends",
        "authors": [
            "Snezhana Sulova"
        ],
        "date": "June 2021",
        "source": "CompSysTech '21: Proceedings of the 22nd International Conference on Computer Systems and Technologies",
        "abstract": "With the increase of unstructured data, the issues connected with automatic text processing, the categorization of documents and the discovery of topics have become objects of growing interest. In order to improve the process of grouping and processing research publications, we would like to propose a method based upon natural language processing. It is based on text mining technologies which aim to identify key tendencies in documents. It processes the content of publications by clustering and identifies the topics of each identified group. This analysis helps by identifying key tendencies as well as discovering emerging new areas of research. Publications from the research literature database, Scopus, were used to test the approach. The topic of the publications is “the application of digital technologies in the logistics business”. The experiments were completed using the RapidMiner Studio software.",
        "link": "https://dl.acm.org/doi/10.1145/3472410.3472433"
    },
    {
        "title": "Assessment of Vulnerability Severity using Text Mining",
        "authors": [
            "Georgios Spanos",
            "Lefteris Angelis",
            "Dimitrios Toloudis"
        ],
        "date": "September 2017",
        "source": "PCI '17: Proceedings of the 21st Pan-Hellenic Conference on Informatics",
        "abstract": "Software1 vulnerabilities are closely associated with information systems security, a major and critical field in today's technology. Vulnerabilities constitute a constant and increasing threat for various aspects of everyday life, especially for safety and economy, since the social impact from the problems that they cause is complicated and often unpredictable. Although there is an entire research branch in software engineering that deals with the identification and elimination of vulnerabilities, the growing complexity of software products and the variability of software production procedures are factors contributing to the ongoing occurrence of vulnerabilities, Hence, another area that is being developed in parallel focuses on the study and management of the vulnerabilities that have already been reported and registered in databases. The information contained in such databases includes, a textual description and a number of metrics related to vulnerabilities. The purpose of this paper is to investigate to what extend the assessment of the vulnerability severity can be inferred directly from the corresponding textual description, or in other words, to examine the informative power of the description with respect to the vulnerability severity. For this purpose, text mining techniques, i.e. text analysis and three different classification methods (decision trees, neural networks and support vector machines) were employed. The application of text mining to a sample of 70,678 vulnerabilities from a public data source shows that the description itself is a reliable and highly accurate source of information for vulnerability prioritization.",
        "link": "https://dl.acm.org/doi/10.1145/3139367.3139390"
    },
    {
        "title": "Clustering Algorithms for Spatial Data Mining",
        "authors": [
            "Chetashri Bhadane",
            "Ketan Shah"
        ],
        "date": "April 2020",
        "source": "ICGDA '20: Proceedings of the 2020 3rd International Conference on Geoinformatics and Data Analysis",
        "abstract": "With the advances in mobile and wireless technologies, there has been a rise in applications that track and share the users' geospatial data. People use several social networking sites such as Twitter, Facebook and Flickr, where they share their status updates. With the integration of Global Positioning System (GPS) with mobile phones, it is now possible to share one's locations on these social networks. GPS allows us to record and track a person's movement along with the timestamp. The data set obtained from these GPS logs is vast and is widely used to analyze the users' movement patterns. Specifically, we can find out significant locations based on the number of users present at that location and the time spent by them at such places. Once significant places have been identified, it is also possible to identify the semantic importance of these locations. This paper presents an overview of the clustering techniques used to find important places of interest using large GPS based mobility datasets. Four clustering algorithms, K-Means, DBSCAN, OPTICS and Hierarchical, are implemented, and performance is tested using real-time data of 50 users collected over 2--5 years. Performance summary depicts that K-Means and DBSCAN perform well for spatial data.",
        "link": "https://dl.acm.org/doi/10.1145/3397056.3397068"
    },
    {
        "title": "Feature Selection based Arabic Text Classification using Different Machine Learning Algorithms: Comparative Study",
        "authors": [
            "Sakina Rim Bennabi",
            "Zakaria Elberrichi"
        ],
        "date": "June 2020",
        "source": "ICIST '20: Proceedings of the 10th International Conference on Information Systems and Technologies",
        "abstract": "Feature selection is a method of data pre-processing widely used when mining large data, such as textual classification. Several studies have been conducted to compare the different methods of feature selection applied to corpora in English. Unfortunately, a small number of works concern the Arabic language. This article aims to present a comparative study of different feature selection techniques including: Chi2, the ANOVA method and mutual information, applied on a corpus in Arabic language, while also diversifying the machine learning algorithms (Naive Bayes, SVM and KNN). This experimental study has shown in general that reducing dimensionality with feature selection techniques has slightly affected the performance of textual classification, reducing the size of the corpus by up to 1%.",
        "link": "https://dl.acm.org/doi/10.1145/3447568.3448531"
    },
    {
        "title": "Intelligent Data Analysis using Optimized Support Vector Machine Based Data Mining Approach for Tourism Industry",
        "authors": [
            "Ms Promila Sharma",
            "Uma Meena",
            "Girish Kumar Sharma"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Data analysis involves the deployment of sophisticated approaches from data mining methods, information theory, and artificial intelligence in various fields like tourism, hospitality, and so on for the extraction of knowledge from the gathered and preprocessed data. In tourism, pattern analysis or data analysis using classification is significant for finding the patterns that represent new and potentially useful information or knowledge about the destination and other data. Several data mining techniques are introduced for the classification of data or patterns. However, overfitting, less accuracy, local minima, sensitive to noise are the drawbacks in some existing data mining classification methods. To overcome these challenges, Support vector machine with Red deer optimization (SVM-RDO) based data mining strategy is proposed in this article. Extended Kalman filter (EKF) is utilized in the first phase, i.e., data cleaning to remove the noise and missing values from the input data. Mantaray foraging algorithm (MaFA) is used in the data selection phase, in which the significant data are selected for the further process to reduce the computational complexity. The final phase is the classification, in which SVM-RDO is proposed to access the useful pattern from the selected data. PYTHON is the implementation tool used for the experiment of the proposed model. The experimental analysis is done to show the efficacy of the proposed work. From the experimental results, the proposed SVM-RDO achieved better accuracy, precision, recall, and F1 score than the existing methods for the tourism dataset. Thus, it is showed the effectiveness of the proposed SVM-RDO for pattern analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3494566"
    },
    {
        "title": "Applying text mining to predict learners' cognitive engagement",
        "authors": [
            "Hayati Hind",
            "Mohammed Khalidi Idrissi",
            "Samir Bennani"
        ],
        "date": "October 2017",
        "source": "SCAMS '17: Proceedings of the Mediterranean Symposium on Smart City Application",
        "abstract": "In academic process, engagement represent one of the important success' factor. This paper proposes a predictive system for learners' cognitive engagement based on their online discussion forums participation. In the first level the ontology OWL and the LSA method are used to perform a semantic classification system of the threads according to a specific context chosen by the tutor. Then the Text mining, as a predictive method, is applied to the classified threads and learners' participation in the forums.",
        "link": "https://dl.acm.org/doi/10.1145/3175628.3175655"
    },
    {
        "title": "Investigate Transitions into Drug Addiction through Text Mining of Reddit Data",
        "authors": [
            "John Lu",
            "Sumati Sridhar",
            "Ritika Pandey",
            "Mohammad Al Hasan",
            "Georege Mohler"
        ],
        "date": "July 2019",
        "source": "KDD '19: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Increasing rates of opioid drug abuse and heightened prevalence of online support communities underscore the necessity of employing data mining techniques to better understand drug addiction using these rapidly developing online resources. In this work, we obtained data from Reddit, an online collection of forums, to gather insight into drug use/misuse using text snippets from users narratives. Specifically, using users' posts, we trained a binary classifier which predicts a user's transitions from casual drug discussion forums to drug recovery forums. We also proposed a Cox regression model that outputs likelihoods of such transitions. In doing so, we found that utterances of select drugs and certain linguistic features contained in one's posts can help predict these transitions. Using unfiltered drug-related posts, our research delineates drugs that are associated with higher rates of transitions from recreational drug discussion to support/recovery discussion, offers insight into modern drug culture, and provides tools with potential applications in combating the opioid crisis.",
        "link": "https://dl.acm.org/doi/10.1145/3292500.3330737"
    },
    {
        "title": "Analysing digital banking reviews using text mining",
        "authors": [
            "Li Chen Cheng",
            "Legaspi Rhea Sharmayne"
        ],
        "date": "December 2020",
        "source": "ASONAM '20: Proceedings of the 12th IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Digital banks are new entrants in the banking industry in the Philippines as they only started late 2018. Since then, a handful of players have and are still emerging. With more and more people becoming technologically savvy, it is very critical for financial institutions to develop a digital banking application that will stand out from the competition. This paper aims to use text mining methods to analyse digital banking application reviews. This study will perform topic modelling using LDA to explore customer concerns and will mine association rules between the digital banking features with the review score. The results will reveal which areas the digital banking application can further optimize for customer satisfaction and retention.",
        "link": "https://dl.acm.org/doi/10.1109/ASONAM49781.2020.9381429"
    },
    {
        "title": "Mining Moocs Videos Metadata Using Classification Techniques",
        "authors": [
            "El Harrak Othman",
            "Ghadi Abderrahim",
            "El Bouhdidi Jaber"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "In few years, the internet has become the first source of information for most people, Today MOOCs makes it possible for everyone to access to the education over the world, it's represents an emerging methodology of online teaching and an important development in open education. Due to the rapid development of Moocs and the rapid growth of digital data and video database over the Internet, it is becoming very difficult for learners to browsing and choosing the best training for them. The scientific community has increased the amount of research into new technologies, with a view to improve the digital video utilization: its archiving, indexing, accessibility, acquisition, store and even its process and usability. All these parts of the video utilization entail the necessity of the extraction of all important information of a video, especially in cases of lack of metadata information. Web video mining is retrieving the content using data mining techniques from World Wide Web. There are two approaches for web video mining using traditional image processing (signal processing) and metadata based approach. In this paper, we present the various research makes in this subject and we propose an effective methodology to extract the metadata from Moocs videos and classify them based on the extracted metadata by applying data mining techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090450"
    },
    {
        "title": "Text mining with HathiTrust",
        "authors": [
            "Eleanor Dickson Koehl",
            "Ryan Dubnicek"
        ],
        "date": "June 2019",
        "source": "JCDL '19: Proceedings of the 18th Joint Conference on Digital Libraries",
        "abstract": "This tutorial will introduce attendees to the HathiTrust Research Center and its tools and services for computational text analysis research. HTRC leverages the scope and scale of the HathiTrust Digital Library collection to create opportunities for researchers to perform text data mining on subsets of the corpus. Attendees will develop skills that will allow them to conduct basic text analysis research using HathiTrust data, learn techniques for engaging with scholars on their text data mining research, and explore strategies for computational access to large-scale digital collections.",
        "link": "https://dl.acm.org/doi/10.1109/JCDL.2019.00115"
    },
    {
        "title": "Text mining for plagiarism detection: multivariate pattern detection for recognition of text similarities",
        "authors": [
            "Konstantinos Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2018",
        "source": "ASONAM '18: Proceedings of the 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "The problem of plagiarism the recent years has been intensified by the availability of information in digital form and the accessibility of the electronic libraries through the Internet. As a result, plagiarism detection has been transformed into a big data analytics problem since the number of digital sources is extravagant and a new document needs to be compared with millions of other existing documents. In this paper, a text mining methodology is proposed that can detect all common patterns between a document and the documents in a reference database. The technique is based on a pattern detection algorithm and the corresponding data structure that enables the algorithm to detect all common patterns. The methodology has been applied in a well-defined dataset providing very promising results identifying difficult cases of plagiarism such as technical disguise.",
        "link": "https://dl.acm.org/doi/10.5555/3382225.3382424"
    },
    {
        "title": "Petroleum Engineering Data Text Classification Using Convolutional Neural Network Based Classifier",
        "authors": [
            "Tzuhan Hsu",
            "Yaoqin Zhang"
        ],
        "date": "May 2018",
        "source": "ICMLT '18: Proceedings of the 2018 International Conference on Machine Learning Technologies",
        "abstract": "In this paper, authors introduce a convolutional neural network (CNN) based classification approach for petroleum engineering event. Recently, text classification problem has raised substantial attention in the field of machine learning. The aim of text classification process is to assign correct label to a text on the basis of its content. However, due to the imprecise of feature representation, most of former text classification methods cannot export satisfying result. To solve this problem, authors propose a model based on convolutional neural network. Authors conduct experiments with real world petroleum engineering data text, artificially labelled into 6 categories. Beside purposed CNN-based model, the authors also implement other baseline machine learning approaches: k nearest neighbor, random forest, support vector machines, recurrent neural network, gated recurrent units, long short-term memory. Among all these models, our CNN-based model reach best performance in text classification task. In the last part of this paper, authors compare and analysis their purpose and baseline model.",
        "link": "https://dl.acm.org/doi/10.1145/3231884.3231898"
    },
    {
        "title": "Classifying Indonesian Online Articles as Advertisement Placement Base Using Text Mining",
        "authors": [
            "Nadhira Tasya",
            "Arian Dhini"
        ],
        "date": "July 2017",
        "source": "ICBIM 2017: Proceedings of the International Conference on Business and Information Management",
        "abstract": "Rapid development in technological aspect resulting in growing level of human needs for the latest news, so that emerged a new trend of publishing and accessing news through online media or usually called online journalism. In addition, the number of people who sell and purchase through online sites also continues to increase and this opportunity is utilized by the company and the advertiser by implementing targeted web advertising. However, the high number of articles that have been published and accessed leads to great opportunities for errors in determining where to place the ads. Therefore, it needs a system that can categorize articles accessed by users as the basis of advertisement placement by the company and this classification system can be done by applying the method of Data Mining and Text Mining. This research uses document data in the form of article content that will be categorized into twenty categories of class of advertisement by using Text Mining technique with Support Vector Machine algorithm. The results of this study may be used by companies or advertisers as a basis for placement of ads on selected online media sites.",
        "link": "https://dl.acm.org/doi/10.1145/3134271.3134288"
    },
    {
        "title": "Mining Free-Text Medical Notes for Suicide Risk Assessment",
        "authors": [
            "Marios Adamou",
            "Grigoris Antoniou",
            "Elissavet Greasidou",
            "Vincenzo Lagani",
            "Paulos Charonyktakis",
            "Ioannis Tsamardinos"
        ],
        "date": "July 2018",
        "source": "SETN '18: Proceedings of the 10th Hellenic Conference on Artificial Intelligence",
        "abstract": "Suicide has been considered as an important public health issue for a very long time, and is one of the main causes of death worldwide. Despite suicide prevention strategies being applied, the rate of suicide has not changed substantially over the past decades. Advances in machine learning make it possible to attempt to predict suicide based on the analysis of relevant data to inform clinical practice. This paper reports on findings from the analysis of data of patients who died by suicide in the period 2013-2016 and made use of both structured data and free-text medical notes. We focus on examining various text-mining approaches to support risk assessment. The results show that using advance machine learning and text-mining techniques, it is possible to predict within a specified period which people are most at risk of taking their own life at the time of referral to a mental health service.",
        "link": "https://dl.acm.org/doi/10.1145/3200947.3201020"
    },
    {
        "title": "Integration of data mining techniques in e-learning systems: Clustering Profil of Lerners and Recommender Course System",
        "authors": [
            "A. El Moustamid",
            "E. En-Naimi",
            "J. El Bouhdidi"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "Data mining aims at the extraction of knowledge from large quantities of data by automatic or semi-automatic methods. In the field of education and training e-learning, Data Mining techniques as clustering (The aim of this technique is to identify groups that are similar in some aspect) [21], prediction (is used for developing model where combination of data viewed from different aspects can effect single viewed data. Here, is important to watch how predictors variables are effecting predicted variable)[21], classification are applied in e-learning to improve the level of learners by providing learning according to the learners' requirement) are used to analyze learners profile, to predict and improve student performance. The aim of our project is to develop a system capable of analyzing learners profile and indexing web videos in order to offer learners a rich database with courses that correspond to their levels.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090453"
    },
    {
        "title": "Multi-level mining and visualization of scientific text collections: Exploring a bi-lingual scientific repository",
        "authors": [
            "Pablo Accuosto",
            "Francesco Ronzano",
            "Daniel Ferrés",
            "Horacio Saggion"
        ],
        "date": "December 2017",
        "source": "WOSP 2017: Proceedings of the 6th International Workshop on Mining Scientific Publications",
        "abstract": "We present a system to mine and visualize collections of scientific documents by semantically browsing information extracted from single publications or aggregated throughout corpora of articles. The text mining tool performs deep analysis of document collections allowing the extraction and interpretation of research paper's contents. In addition to the extraction and enrichment of documents with metadata (titles, authors, affiliations, etc), the deep analysis performed comprises semantic interpretation, rhetorical analysis of sentences, triple-based information extraction, and text summarization. The visualization components allow geographical-based exploration of collections, topic-evolution interpretation, and collaborative network analysis among others. The paper presents a case study of a bi-lingual collection in the field of Natural Language Processing (NLP).",
        "link": "https://dl.acm.org/doi/10.1145/3127526.3127529"
    },
    {
        "title": "Hierarchical Topic Mining via Joint Spherical Tree and Text Embedding",
        "authors": [
            "Yu Meng",
            "Yunyi Zhang",
            "Jiaxin Huang",
            "Yu Zhang",
            "Chao Zhang",
            "Jiawei Han"
        ],
        "date": "August 2020",
        "source": "KDD '20: Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Mining a set of meaningful topics organized into a hierarchy is intuitively appealing since topic correlations are ubiquitous in massive text corpora. To account for potential hierarchical topic structures, hierarchical topic models generalize flat topic models by incorporating latent topic hierarchies into their generative modeling process. However, due to their purely unsupervised nature, the learned topic hierarchy often deviates from users' particular needs or interests. To guide the hierarchical topic discovery process with minimal user supervision, we propose a new task, Hierarchical Topic Mining, which takes a category tree described by category names only, and aims to mine a set of representative terms for each category from a text corpus to help a user comprehend his/her interested topics. We develop a novel joint tree and text embedding method along with a principled optimization procedure that allows simultaneous modeling of the category tree structure and the corpus generative process in the spherical space for effective category-representative term discovery. Our comprehensive experiments show that our model, named JoSH, mines a high-quality set of hierarchical topics with high efficiency and benefits weakly-supervised hierarchical text classification tasks.",
        "link": "https://dl.acm.org/doi/10.1145/3394486.3403242"
    },
    {
        "title": "Unsupervised behavioural mining and clustering for malware family identification",
        "authors": [
            "Khanh Huu The Dam",
            "Thomas Given-Wilson",
            "Axel Legay"
        ],
        "date": "March 2021",
        "source": "SAC '21: Proceedings of the 36th Annual ACM Symposium on Applied Computing",
        "abstract": "More accurate and advanced detection and classification of malware requires exploiting program behaviour and not merely syntactic or static features. One approach is to use system call dependency graphs (SCDGs) that represent the program behaviour by interactions with the system, and the relations between these interactions. These SCDGs have been used with supervised learning techniques to very accurately detect and classify malware. This works considers the unsupervised learning challenge of mining for common clusters of behaviour without a priori knowledge. This allows for clustering of similar programs by behaviour, that can then be used for either classification or further analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3412841.3441919"
    },
    {
        "title": "Text Mining for Evaluating Authors' Birth and Death Years",
        "authors": [
            "Dror Moghaz",
            "Yaakov Hacohen-Kerner",
            "Dov Gabbay"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "This article presents a unique method in text and data mining for finding the era, i.e., mining temporal data, in which an anonymous author was living. Finding this era can assist in the examination of a fake document or extracting the time period in which a writer lived. The study and the experiments concern Hebrew, and in some parts, Aramaic and Yiddish rabbinic texts. The rabbinic texts are undated and contain no bibliographic sections, posing an interesting challenge. This work proposes algorithms using key phrases and key words that allow the temporal organization of citations together with linguistic patterns. Based on these key phrases, key words, and the references, we established several types of “Iron-clad,” Heuristic and Greedy rules for estimating the years of birth and death of a writer in an interesting classification task. Experiments were conducted on corpora, including documents authored by 12, 24, and 36 rabbinic writers and demonstrated promising results.",
        "link": "https://dl.acm.org/doi/10.1145/3281631"
    },
    {
        "title": "Transfer Learning to Timed Text Based Video Classification Using CNN",
        "authors": [
            "Zenun Kastrati",
            "Ali Shariq Imran",
            "Arianit Kurti"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "Open educational video resources are gaining popularity with a growing number of massive open online courses (MOOCs). This has created a niche for content providers to adopt effective solutions in automatically organizing and structuring of educational resources for maximum visibility. Recent advances in deep learning techniques are proving useful in managing and classifying resources into appropriate categories. This paper proposes one such convolutional neural network (CNN) model for classifying video lectures in a MOOC setting using a transfer learning approach. The model uses a time-aligned text transcripts corresponding to video lectures from six broader subject categories. Video lectures and their corresponding transcript dataset is gathered from the Coursera MOOC platform. Two different CNN models are proposed: i) CNN based classification using embeddings learned from our MOOC dataset, ii) CNN based classification using transfer learning. Word embeddings generated from two well known state-of-the-art pre-trained models Word2Vec and GloVe, are used in the transfer learning approach for the second case. The proposed CNN models are evaluated using precision, recall, and F1 score and the obtained performance is compared with both conventional and deep learning classifiers. The proposed CNN models have an F1 score improvement of 10-22 percentage points over DNN and conventional classifiers",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326483"
    },
    {
        "title": "Evolutionary role mining in complex networks by ensemble clustering",
        "authors": [
            "Sarvenaz Choobdar",
            "Pedro Ribeiro",
            "Fernando Silva"
        ],
        "date": "April 2017",
        "source": "SAC '17: Proceedings of the Symposium on Applied Computing",
        "abstract": "The structural patterns in the neighborhood of nodes assign unique roles to the nodes. Mining the set of existing roles in a network provides a descriptive profile of the network and draws its general picture. This paper proposes a new method to determine structural roles in a dynamic network based on the current position of nodes and their historic behavior. We develop a temporal ensemble clustering technique to dynamically find groups of nodes, holding similar tempo-structural roles. We compare two weighting functions, based on age and distribution of data, to incorporate temporal behavior of nodes in the role discovery. To evaluate the performance of the proposed method, we assess the results from two points of view: 1) goodness of fit to current structure of the network; 2) consistency with historic data. We conduct the evaluation using different ensemble clustering techniques. The results on real world networks demonstrate that our method can detect tempo-structural roles that simultaneously depict the topology of a network and reflect its dynamics with high accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3019612.3019815"
    },
    {
        "title": "Analyzing used-car web listings via text mining",
        "authors": [
            "Ayhan Demiriz",
            "Fatma Cantaş"
        ],
        "date": "October 2017",
        "source": "IML '17: Proceedings of the 1st International Conference on Internet of Things and Machine Learning",
        "abstract": "Used car trade is one of the major components of the world economies. It is not uncommon to sell a car by placing an internet advertisement irrespective of the geography in these days. A typical content of an advertisement is usually composed of two parts namely the structured and the free text data. The structured data may include some information about the asking price, make, model, year, mileage of the car and the contact info. In most cases, seller may give important clues about the car's current conditions in the free text data where the title (head) of the advertisement can be included as free text too. This paper reports preliminary results from a text mining study conducted on 75K used car internet listings collected from two major car listing web sites in Turkey. As expected, the words and the phrases related to the description of the car are observed to be frequent. The leading concepts in the free text are found to be regarding how to describe the current condition of a car, for example \"no crash history\".",
        "link": "https://dl.acm.org/doi/10.1145/3109761.3109782"
    },
    {
        "title": "Text classification of Diseases Treated by Traditional Chinese Medicine Prescription based on machine learning",
        "authors": [
            "Hanqing Zhao",
            "Bo Han",
            "Chen Li"
        ],
        "date": "September 2020",
        "source": "ISAIMS '20: Proceedings of the 1st International Symposium on Artificial Intelligence in Medical Sciences",
        "abstract": "OBJECTIVE: To explore the application of machine learning in th e identification of diseases treated by traditional Chinese medicine prescriptions. METHODS: Based on the composition of the text document of Chinese medicine prescriptions, the prescriptions were divided into cough, headache and diarrhea. THUTCT were introduced to establish and train two machine learning text classification models, LibSVM and LibLinear, and the prescriptions to be deter mined were put into the model for classification and prediction. R ESULTS: The Precision rate of LibSVM model and Liblinear mod el were 0.7283 and 0.6690. Seven prescriptions were classified an d predicted, and the results were in line with expectations. CONCLUSION: THUCTC has good universality for the content of TCM prescriptions, high classification accuracy and fast testing speed, which is suitable for the text classification and discrimination research of TCM prescriptions for diseases.",
        "link": "https://dl.acm.org/doi/10.1145/3429889.3429895"
    },
    {
        "title": "Evaluation of retweet clustering method classification method using retweets on Twitter without text data",
        "authors": [
            "K. Uchida",
            "F. Toriumi",
            "T. Sakaki"
        ],
        "date": "August 2017",
        "source": "WI '17: Proceedings of the International Conference on Web Intelligence",
        "abstract": "Burst phenomena, which frequently occur on social media, are caused by such social events as flaming on the internet, elections, and natural disasters. To understand people's thoughts and feelings, we must classify their opinions from burst phenomena. Therefore, classification methods that categorize tweets are critical. However, since most classification methods focus on text mining, they cannot group tweets by topics because each tweet has poor linguistic similarities. We used a non-text-based classification method proposed by Baba et al. that groups tweets by topics, even if they have poor linguistic similarities, and verified its validity by comparing it with a text-based classification method in two different evaluations: qualitative and quantitative. In the qualitative evaluation part, we did a questionnaire survey and validated the suitability of the topic clusters created using both the non-and text-based methods. Since evaluating the similarity of every pair of tweets in each topic is difficult, we evaluated the similarity between sampled pairs in the survey and acquired more appropriate topic clustering results using the non-text-based method than the text-based method. In the quantitative evaluation part, we focused on the robustness of each method against data reduction. Many approaches analyze social media data, especially because collecting data from social media is comparatively easy. However, since collecting the whole data of burst phenomena is very costly due to the vast amounts of available social media data, robustness against data reduction is an important index to evaluate classification methods. With the non-text-based method, over 55% of the pairs of tweets in the same cluster were also included in the same cluster even when the data were reduced to 10% in all three of our example cases. In this paper, as a source we focus on Twitter, one of the most popular microblogging services. Using clustering to conduct detailed case analyses, we scrutinized three burst cases that include natural disasters and flaming on the internet and found that a non-text-based method more effectively classified tweets in burst phenomena than a text-based method.",
        "link": "https://dl.acm.org/doi/10.1145/3106426.3106451"
    },
    {
        "title": "Using Spark for Text Mining on Large Scale Liver Cancer Literature",
        "authors": [
            "Ming-Yen Lin",
            "Yu-Ju Lin",
            "Sue-Chen Hsueh"
        ],
        "date": "January 2021",
        "source": "BDET 2021: 2021 the 3rd International Conference on Big Data Engineering and Technology (BDET)",
        "abstract": "Cancer is one of the main causes of death. The number of known cancer to-date is more than one hundred. Liver cancer, ranked after trachea and lung cancer, has long been high in the cancer leading cause in Taiwan and even ranked as second in 2016. The number of scientific articles related to cancer proliferates every year. The number reaches as high as 20 million in PubMed so that discovering useful information from the massive collection is very difficult. In addition, using a single machine to sift through these articles is very time-consuming. Therefore, we present a big data analytic framework using the distributed Apache Spark platform for text mining in PubMed literature. We establish a prediction model for liver cancer articles so that researchers may effectively validate whether an article is related to liver cancer or not. Classification models in Spark MLlib including Linear Support Vector Machines (SVM), Logistic Regression, are used in our experiments. Relevancy to liver cancer is further confirmed by using MeSH (Medical Subject Headings) terms. Logistic regression is about 3 times faster than SVMs and the accuracy of both methods is close to 95% in the experiments using hold-out validation. When max_features is 500 and min_df ≤ 0.1 (or min_df = 1), the accuracy may reach 96%. In the experiments with K-fold cross-validation, the accuracy of SVMs methods is 96%. The experimental results show that that our prediction model may effectively classify liver cancer articles.",
        "link": "https://dl.acm.org/doi/10.1145/3474944.3474958"
    },
    {
        "title": "Automatic topic classification of test cases using text mining at an Android smartphone vendor",
        "authors": [
            "Junji Shimagaki",
            "Yasutaka Kamei",
            "Naoyasu Ubayashi",
            "Abram Hindle"
        ],
        "date": "October 2018",
        "source": "ESEM '18: Proceedings of the 12th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Background: An Android smartphone is an ecosystem of applications, drivers, operating system components, and assets. The volume of the software is large and the number of test cases needed to cover the functionality of an Android system is substantial. Enormous effort has been already taken to properly quantify \"what features and apps were tested and verified?\". This insight is provided by dashboards that summarize test coverage and results per feature. One method to achieve this is to manually tag or label test cases with the topic or function they cover, much like function points. At the studied Android smartphone vendor, tests are labelled with manually defined tags, so-called \"feature labels (FLs)\", and the FLs serve to categorize 100s to 1000s test cases into 10 to 50 groups. Aim: Unfortunately for developers, manual assignment of FLs to 1000s of test cases is a time consuming task, leading to inaccurately labeled test cases, which will render the dashboard useless. We created an automated system that suggests tags/labels to the developers for their test cases rather than manual labeling. Method: We use machine learning models to predict and label the functionality tested by 10,000 test cases developed at the company. Results: Through the quantitative experiments, our models achieved acceptable F-1 performance of 0.3 to 0.88. Also through the qualitative studies with expert teams, we showed that the hierarchy and path of tests was a good predictor of a feature's label. Conclusions: We find that this method can reduce tedious manual effort that software developers spent classifying test cases, while providing more accurate classification results.",
        "link": "https://dl.acm.org/doi/10.1145/3239235.3268927"
    },
    {
        "title": "To apply Data Mining for Classification of Crowd sourced Software Requirements",
        "authors": [
            "Soonh Taj",
            "Qasim Arain",
            "Imran Memon",
            "Asma Zubedi"
        ],
        "date": "April 2019",
        "source": "ICSIE '19: Proceedings of the 8th International Conference on Software and Information Engineering",
        "abstract": "Now a day's main focus of developers is to build quality software that works according to customer needs and for this reason it is necessary to gather right requirements as requirement elicitation is the critical step that impacts on the success of software project as misinterpreted requirements leads to the failure of software project. By keeping this in mind a research is carried out on improving requirements elicitation process and automating the process of classifying requirements. In this research, a model is proposed which will help in this scenario for requirements elicitation and requirement classification. This paper presents a model in which crowd sourcing approach is used so that customers, end users, stakeholders, developers and software engineers can make active participation for requirement elicitation process and requirements gathered using crowdsourcing approach are used by model for classification process i.e. classification of requirements into functional and non-functional requirements. For the proof of proposed model a case study is conducted. Results of case study provided the usefulness and efficiency of proposed model for classification of crowd sourced software requirements.",
        "link": "https://dl.acm.org/doi/10.1145/3328833.3328837"
    },
    {
        "title": "Text classification using Fuzzy TF-IDF and Machine Learning Models",
        "authors": [
            "Mariem Bounabi",
            "Karim El Moutaouakil",
            "Khalid Satori"
        ],
        "date": "October 2019",
        "source": "BDIoT'19: Proceedings of the 4th International Conference on Big Data and Internet of Things",
        "abstract": "The representation of the information has an important impact on the text classification task. Several weighting methods were proposed in the literature, and the term frequency-inverse term frequency (TFIDF), the most know on the text treatment field. The FTF-IDF is a vector representation where the components of the TFIDF are presented as inputs to the Fuzzy Inference System (FIS). In this work, we compare several Machin Learning algorithms such as Naïve Bayes and its derivatives, SVM and Random forest classifiers, using the FTF-IDF representation. To improve the quality of the used classifiers, we call sum attribute selection methods. The recognition rate, for the tested systems, is satisfied, where the system based on naïve Bayes classifier, the FTF-IDF weighting terms, and the info gain select attributes method gives 98.7% as accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3372938.3372956"
    },
    {
        "title": "Text Mining in Cybersecurity: A Systematic Literature Review",
        "authors": [
            "Luciano Ignaczak",
            "Guilherme Goldschmidt",
            "Cristiano André Da Costa",
            "Rodrigo Da Rosa Righi"
        ],
        "date": null,
        "source": "ACM Computing Surveys",
        "abstract": "The growth of data volume has changed cybersecurity activities, demanding a higher level of automation. In this new cybersecurity landscape, text mining emerged as an alternative to improve the efficiency of the activities involving unstructured data. This article proposes a Systematic Literature Review (SLR) to present the application of text mining in the cybersecurity domain. Using a systematic protocol, we identified 2,196 studies, out of which 83 were summarized. As a contribution, we propose a taxonomy to demonstrate the different activities in the cybersecurity domain supported by text mining. We also detail the strategies evaluated in the application of text mining tasks and the use of neural networks to support activities involving unstructured data. The work also discusses text classification performance aiming its application in real-world solutions. The SLR also highlights open gaps for future research, such as the analysis of non-English content and the intensification in the usage of neural networks.",
        "link": "https://dl.acm.org/doi/10.1145/3462477"
    },
    {
        "title": "Using Text Mining to Analyze the Financial Patents",
        "authors": [
            "Yung-Feng Lu",
            "Jia-Lang Xu",
            "Mu-Yen Chen"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "With the advancement and development of science and technology, Nowadays society is the development of information technology, cloud computing, mobile Internet, Internet of Things, block chain and information technology.The combination of finance and technology has made financial technology terms gradually emerging.. This research analyzes the patents of financial technology. However, Taiwan's financial technology patents are mostly G06Q-20 (payment plan, architecture or agreement), G06Q-30 (commercial, such as marketing, shopping, payment, auction or e-commerce). G06Q-40 (financial, investment or tax treatment, insurance).The research use text mining technology to analyze and understand Taiwan's trends and developments in financial technology. The research results show that G06Q-20 patents are biased towards information security and mobile payment. G06Q-30 patents favor shopping websites and advertising. G06Q-40 patents are biased towards diversity, such as securities, insurance, finance, and stocks.",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326478"
    },
    {
        "title": "Identifying Online Profiles of Distance Learning Students Using Data Mining Techniques",
        "authors": [
            "Osama Islam",
            "Muazzam Siddiqui",
            "Naif Radi Aljohani"
        ],
        "date": "October 2019",
        "source": "ICDTE '19: Proceedings of the 3rd International Conference on Digital Technology in Education",
        "abstract": "Educational data has grown over the years with the increased use of technology within educational environments. This has led to a huge amount of data being stored in various data sources representing the student, his/her activities, and other aspects relevant to the learning process. To meet this analytical need, Educational Data Mining (EDM) has emerged to assist educational institutions in identifying key benefits such as students at risk, the level of student engagement or predicting student performance. The aim of this research was to explore the various aspects of student interaction data using data mining techniques to identify relevant patterns of behaviors and possible key attributes that have higher degrees of influence on distance learning students. The main findings identified several patterns of user online profiles based on a set of adopted learning strategies, the research proposed a framework for analyzing such interaction data based on R and Hadoop platforms to correlate online profiles with student performance.",
        "link": "https://dl.acm.org/doi/10.1145/3369199.3369249"
    },
    {
        "title": "Extremely Fast Decision Tree Mining for Evolving Data Streams",
        "authors": [
            "Albert Bifet",
            "Jiajin Zhang",
            "Wei Fan",
            "Cheng He",
            "Jianfeng Zhang",
            "Jianfeng Qian",
            "Geoff Holmes",
            "Bernhard Pfahringer"
        ],
        "date": "August 2017",
        "source": "KDD '17: Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
        "abstract": "Nowadays real-time industrial applications are generating a huge amount of data continuously every day. To process these large data streams, we need fast and efficient methodologies and systems. A useful feature desired for data scientists and analysts is to have easy to visualize and understand machine learning models. Decision trees are preferred in many real-time applications for this reason, and also, because combined in an ensemble, they are one of the most powerful methods in machine learning. In this paper, we present a new system called STREAMDM-C++, that implements decision trees for data streams in C++, and that has been used extensively at Huawei. Streaming decision trees adapt to changes on streams, a huge advantage since standard decision trees are built using a snapshot of data, and can not evolve over time. STREAMDM-C++ is easy to extend, and contains more powerful ensemble methods, and a more efficient and easy to use adaptive decision trees. We compare our new implementation with VFML, the current state of the art implementation in C, and show how our new system outperforms VFML in speed using less resources.",
        "link": "https://dl.acm.org/doi/10.1145/3097983.3098139"
    },
    {
        "title": "Data miners' little helper: data transformation activity cues for cluster analysis on document collections",
        "authors": [
            "Tania Cerquitelli",
            "Evelina Di Corso",
            "Francesco Ventura",
            "Silvia Chiusano"
        ],
        "date": "June 2017",
        "source": "WIMS '17: Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "In this paper we propose a new self-learning engine to streamline the analytics process, as it enables analysts to mine massive data repositories with minimal user intervention. In the context of cluster analysis on a collection of documents this new system, named SELF-DATA (SELF-learning DAta TrAnsformation), suggests to the analyst how to configure the whole mining process for a given dataset. SELF-DATA relies on an engine exploring different data weighting schemas (e.g., normalized term frequencies) and data transformation methods (e.g., PCA) before applying the cluster analysis, evaluating and comparing solutions through different quality indices (e.g., weighted Silhouette), and presenting the k-top solutions to the analyst. SELF-DATA will also include a knowledge base storing results of experiments on previously processed datasets, and a classification algorithm trained on the knowledge base content to forecast the best configuration for the whole mining process for an unexplored dataset. The first development of SELF-DATA running on Apache Spark has been validated on 5 collections of documents. Experimental results highlight that TF-IDF and logarithmic entropy are effective to measure item relevance with sparse datasets, and the LSI method outperforms PCA with a large dictionary.",
        "link": "https://dl.acm.org/doi/10.1145/3102254.3102288"
    },
    {
        "title": "Interactive Visual Graph Mining and Learning",
        "authors": [
            "Ryan A. Rossi",
            "Nesreen K. Ahmed",
            "Rong Zhou",
            "Hoda Eldardiry"
        ],
        "date": null,
        "source": "ACM Transactions on Intelligent Systems and Technology",
        "abstract": "This article presents a platform for interactive graph mining and relational machine learning called GraphVis. The platform combines interactive visual representations with state-of-the-art graph mining and relational machine learning techniques to aid in revealing important insights quickly as well as learning an appropriate and highly predictive model for a particular task (e.g., classification, link prediction, discovering the roles of nodes, and finding influential nodes). Visual representations and interaction techniques and tools are developed for simple, fast, and intuitive real-time interactive exploration, mining, and modeling of graph data. In particular, we propose techniques for interactive relational learning (e.g., node/link classification), interactive link prediction and weighting, role discovery and community detection, higher-order network analysis (via graphlets, network motifs), among others. GraphVis also allows for the refinement and tuning of graph mining and relational learning methods for specific application domains and constraints via an end-to-end interactive visual analytic pipeline that learns, infers, and provides rapid interactive visualization with immediate feedback at each change/prediction in real-time. Other key aspects include interactive filtering, querying, ranking, manipulating, exporting, as well as tools for dynamic network analysis and visualization, interactive graph generators (including new block model approaches), and a variety of multi-level network analysis techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3200764"
    },
    {
        "title": "Data Mining and Opinion Mining: A Tool in Educational Context",
        "authors": [
            "Myriam Peñafiel",
            "Stefanie Vásquez",
            "Diego Vásquez",
            "Juan Zaldumbide",
            "Sergio Luján-Mora"
        ],
        "date": "July 2018",
        "source": "ICoMS '18: Proceedings of the 2018 1st International Conference on Mathematics and Statistics",
        "abstract": "The use of the web as a universal communication platform generates large volumes of data (Big data), which in many cases, need to be processed so that they can become useful knowledge in face of the sceptics who have doubts about the credibility of such information. The use of web data that comes from educational contexts needs to be addressed, since that large amount of unstructured information is not being valued, losing valuable information that can be used. To solve this problem, we propose the use of data mining techniques such as sentiment analysis to validate the information that comes from the educational platforms. The objective of this research is to propose a methodology that allows the user to apply sentiment analysis in a simple way, because although some researchers have done it, very few do with data in the educational context. The results obtained prove that the proposal can be used in similar cases.",
        "link": "https://dl.acm.org/doi/10.1145/3274250.3274263"
    },
    {
        "title": "Research Hotspots Mining and Visualized Analysis Based on Linking Cluster and K-Core Decomposition",
        "authors": [
            "Ying Cai",
            "Fang Huang",
            "Shuai Wang",
            "Hao Zhang",
            "Chunxiu Du"
        ],
        "date": "May 2018",
        "source": "ICDPA 2018: Proceedings of the International Conference on Data Processing and Applications",
        "abstract": "In this paper, we focus on research hotspots mining with knowledge semantic features on discipline topic word networks, which are extracted from scientific and technical documents. For the topic networks, a novel method of combining linking community clustering with k-core decomposition is proposed to mine the hotspot community with knowledge hierarchy structure. And as a post-processing, the hierarchical community identification is implemented by k-core decomposition in order to discover the internal connection with knowledge semantic hierarchy in the research hotspot. Finally, the community density and visualization method is utilized to analyze the core and sub- problem in the research hotspot community. In experiments, the topic networks for a single discipline and interdisciplinary research hotspot are taken as examples of analysis, and the results show the proposed approach can effectively realize the objective of mining and analyzing the discipline research hotspot on the academic topic word network.",
        "link": "https://dl.acm.org/doi/10.1145/3224207.3224216"
    },
    {
        "title": "The Application of Data Mining Algorithms in the Construction of Travel Recommendation System",
        "authors": [
            "Chao Lou",
            "Zhaonan Mu",
            "Mengzhu Liu"
        ],
        "date": "January 2021",
        "source": "CONF-CDS 2021: The 2nd International Conference on Computing and Data Science",
        "abstract": "Photos shared by users of online social network often associated with tags, texts, geographic and time information. These data are ideal research resource which can be used by researchers to observe people's interests and behaviors. They can also be used to analyze people's travel behaviors and interests because it can reflect people's travel activities and experiences directly. This project aimed to consolidate people into groups based on their travel preferences of travel season, travel time span, scenery type of travel location and then analyze popular travel places for different people groups. The travel preferences were analyzed from extracted data applying PHP language and Naive Bayes classification approach. According to the analyzed data, people were divided into groups. Then, DBSCAN clustering was used to get popular locations based on the different groups. As a result, 80 thousand photo data of 5 cities have been collected from Flickr by Phython. It has been showed that people's preference of season in most of the mined cities was spring and summer and the preference of time span was short. Moreover, mined people has been divided into 16 groups according to their preference and based on the groups popular locations have been calculated by DBSCAN clustering. Theses locations have been provided in a recommendation website which was built in this project. For the future work, mining Flickr data associated with other travel social network sites could be a good way to build a more effective recommendation system.",
        "link": "https://dl.acm.org/doi/10.1145/3448734.3450477"
    },
    {
        "title": "Compare Machine Learning Models in Text Classification Using Steam User Reviews",
        "authors": [
            "Youchen Miao",
            "Zeyu Jin",
            "Yumeng Zhang",
            "Yuchen Chen",
            "Junren Lai"
        ],
        "date": "November 2021",
        "source": "ICSED '21: Proceedings of the 2021 3rd International Conference on Software Engineering and Development",
        "abstract": "Text Classification and Sentiment Analysis of game reviews are viewed as important parts in not only academic fields but also in game studies. In this paper, with more than 400 thousand game reviews on Steam platform, we preprocess the data using different libraries (sklearn, nltk, and spaCy) and use them as inputs to build three sentiment classification models based on different algorithms (Naive Bayes, SVM, and Random Forest). In contrast to previous studies that only focus on different sentiment analysis models, our paper also highlights the use of different APIs to preprocess the data and their corresponding model performance. The results show that no matter which API we choose, Random Forest models always perform the best. However, in terms of training time, Naive Bayes is the fastest. This work can be used to apply grid search for researchers to automatically find the optimum API before conducting sentiment analysis in the future.",
        "link": "https://dl.acm.org/doi/10.1145/3507473.3507480"
    },
    {
        "title": "Lost in Transduction: Transductive Transfer Learning in Text Classification",
        "authors": [
            "Alejandro Moreo",
            "Andrea Esuli",
            "Fabrizio Sebastiani"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Obtaining high-quality labelled data for training a classifier in a new application domain is often costly. Transfer Learning\n(a.k.a. “Inductive Transfer”) tries to alleviate these costs by transferring, to the “target” domain of interest, knowledge available from a different “source” domain. In transfer learning the lack of labelled information from the target domain is compensated by the availability at training time of a set of unlabelled examples from the target distribution. Transductive Transfer Learning denotes the transfer learning setting in which the only set of target documents that we are interested in classifying is known and available at training time. Although this definition is indeed in line with Vapnik’s original definition of “transduction”, current terminology in the field is confused. In this article, we discuss how the term “transduction” has been misused in the transfer learning literature, and propose a clarification consistent with the original characterization of this term given by Vapnik. We go on to observe that the above terminology misuse has brought about misleading experimental comparisons, with inductive transfer learning methods that have been incorrectly compared with transductive transfer learning methods. We then, give empirical evidence that the difference in performance between the inductive version and the transductive version of a transfer learning method can indeed be statistically significant (i.e., that knowing at training time the only data one needs to classify indeed gives an advantage). Our clarification allows a reassessment of the field, and of the relative merits of the major, state-of-the-art algorithms for transfer learning in text classification.",
        "link": "https://dl.acm.org/doi/10.1145/3453146"
    },
    {
        "title": "Machine Learning on Mining Potential Adverse Drug Reactions for Pharmacovigilance",
        "authors": [
            "Ke Jia"
        ],
        "date": "July 2021",
        "source": "DSIT 2021: 2021 4th International Conference on Data Science and Information Technology",
        "abstract": "Drugs are related to the lives and health of patients. Mining and discovering potential adverse drug reactions from a large number of unstructured texts play an important role in drug research and pharmacovigilance. It is a hot research issue in the field of biomedicine in recent years. A large number of scholars have discovered a large amount of information on adverse drug reactions from different data sources through effective machine learning methods. This paper reviews the research progress in recent years to provide a reference for future research.",
        "link": "https://dl.acm.org/doi/10.1145/3478905.3478964"
    },
    {
        "title": "TACAM: Topic And Context Aware Argument Mining",
        "authors": [
            "Michael Fromm",
            "Evgeniy Faerman",
            "Thomas Seidl"
        ],
        "date": "October 2019",
        "source": "WI '19: IEEE/WIC/ACM International Conference on Web Intelligence",
        "abstract": "In this work we address the problem of argument search. The purpose of argument search is the distillation of pro and contra arguments for requested topics from large text corpora. In previous works, the usual approach is to use a standard search engine to extract text parts which are relevant to the given topic and subsequently use an argument recognition algorithm to select arguments from them. The main challenge in the argument recognition task, which is also known as argument mining, is that often sentences containing arguments are structurally similar to purely informative sentences without any stance about the topic. In fact, they only differ semantically. Most approaches use topic or search term information only for the first search step and therefore assume that arguments can be classified independently of a topic. We argue that topic information is crucial for argument mining, since the topic defines the semantic context of an argument. Precisely, we propose different models for the classification of arguments, which take information about a topic of an argument into account. Moreover, to enrich the context of a topic and to let models understand the context of the potential argument better, we integrate information from different external sources such as Knowledge Graphs or pre-trained NLP models. Our evaluation shows that considering topic information, especially in connection with external information, provides a significant performance boost for the argument mining task.",
        "link": "https://dl.acm.org/doi/10.1145/3350546.3352506"
    },
    {
        "title": "Text mining for incoming tasks based on the urgency/importance factors and task classification using machine learning tools",
        "authors": [
            "Yasser Ali Alshehri"
        ],
        "date": "March 2020",
        "source": "ICCDA 2020: Proceedings of the 2020 the 4th International Conference on Compute and Data Analysis",
        "abstract": "In workplaces, there is a massive amount of unstructured data from different sources. In this paper, we present a case study that explains how can through communications between employees, we can help to prioritize tasks requests to increase the efficiency of their works for both technical and non-technical workers. This involves managing daily incoming tasks based on their level of urgency and importance.To allow all workers to utilize the urgency-importance matrix as a time-management tool, we need to automate this tool. The textual content of incoming tasks are analyzed, and metrics related to urgency and importance are extracted. A third factor (i.e., the response variable) is defined based on the two input variables (urgency and importance). Then, machine learning applied to the data to predict the class of incoming tasks based on data outcome desired. We used ordinal regression, neural networks, and decision tree algorithms to predict the four levels of task priority. We measure the performance of all using recalls, precisions, and F-scores. All classifiers perform higher than 89% in terms of all measures.",
        "link": "https://dl.acm.org/doi/10.1145/3388142.3388153"
    },
    {
        "title": "Text mining for malware classification using multivariate all repeated patterns detection",
        "authors": [
            "Konstantinos F. Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2019",
        "source": "ASONAM '19: Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Mobile phones have become nowadays a commodity to the majority of people. Using them, people are able to access the world of Internet and connect with their friends, their colleagues at work or even unknown people with common interests. This proliferation of the mobile devices has also been seen as an opportunity for the cyber criminals to deceive smartphone users and steel their money directly or indirectly, respectively, by accessing their bank accounts through the smartphones or by blackmailing them or selling their private data such as photos, credit card data, etc. to third parties. This is usually achieved by installing malware to smartphones masking their malevolent payload as a legitimate application and advertise it to the users with the hope that mobile users will install it in their devices. Thus, any existing application can easily be modified by integrating a malware and then presented it as a legitimate one. In response to this, scientists have proposed a number of malware detection and classification methods using a variety of techniques. Even though, several of them achieve relatively high precision in malware classification, there is still space for improvement. In this paper, we propose a text mining all repeated pattern detection method which uses the decompiled files of an application in order to classify a suspicious application into one of the known malware families. Based on the experimental results using a real malware dataset, the methodology tries to correctly classify (without any misclassification) all randomly selected malware applications of 3 categories with 3 different families each.",
        "link": "https://dl.acm.org/doi/10.1145/3341161.3350841"
    },
    {
        "title": "Comparative Study between Traditional Machine Learning and Deep Learning Approaches for Text Classification",
        "authors": [
            "Cannannore Nidhi Kamath",
            "Syed Saqib Bukhari",
            "Andreas Dengel"
        ],
        "date": "August 2018",
        "source": "DocEng '18: Proceedings of the ACM Symposium on Document Engineering 2018",
        "abstract": "In this contemporaneous world, it is an obligation for any organization working with documents to end up with the insipid task of classifying truckload of documents, which is the nascent stage of venturing into the realm of information retrieval and data mining. But classification of such humongous documents into multiple classes, calls for a lot of time and labor. Hence a system which could classify these documents with acceptable accuracy would be of an unfathomable help in document engineering. We have created multiple classifiers for document classification and compared their accuracy on raw and processed data. We have garnered data used in a corporate organization as well as publicly available data for comparison. Data is processed by removing the stop-words and stemming is implemented to produce root words. Multiple traditional machine learning techniques like Naive Bayes, Logistic Regression, Support Vector Machine, Random forest Classifier and Multi-Layer Perceptron are used for classification of documents. Classifiers are applied on raw and processed data separately and their accuracy is noted. Along with this, Deep learning technique such as Convolution Neural Network is also used to classify the data and its accuracy is compared with that of traditional machine learning techniques. We are also exploring hierarchical classifiers for classification of classes and subclasses. The system classifies the data faster and with better accuracy than if done manually. The results are discussed in the results and evaluation section.",
        "link": "https://dl.acm.org/doi/10.1145/3209280.3209526"
    },
    {
        "title": "Identification of Overpricing in the Purchase of Medication by the Federal Government of Brazil, Using Text Mining and Clustering Based on Ontology",
        "authors": [
            "Marco Aurelio O. S. Correa",
            "Adriano Galindo Leal"
        ],
        "date": "August 2018",
        "source": "ICCBDC '18: Proceedings of the 2018 2nd International Conference on Cloud and Big Data Computing",
        "abstract": "Increasing the transparency level in his actions and spending is one of the primary duties of the Brazilian Federal Government. The creation of laws that oblige full disclose of all its expenditures through transparency portals enables citizens to supervise all government entities. However, only the dissemination of these data, without a definite standard or the availability of data analysis tools, does not guarantee that the citizen is empowered to play his role. Therefore hence, the objective of this work is to identify overprice in the acquisition of products purchased by the federal government of Brazil using the unstructured data available on the Transparency Portal. The last two-years' worth of purchasing data, available in the Transparency Portal, were extracted, processed and stored. Due to his diverse nature and high volume of data, this study focused only on medicines purchased by the Ministry of Health. Ontology-based text mining and clustering techniques were applied for automatic identification and classification of products. The processing of this information was done through text mining and clustering, based on the ontology registered in another database of the Brazilian government. Because of this work, a consolidated price base per medication was created to allow the identification of distortions in prices practised, facilitating the identification of cases that merit further investigation to unravel fraud to the treasury.",
        "link": "https://dl.acm.org/doi/10.1145/3264560.3264569"
    },
    {
        "title": "Computational Estimation by Scientific Data Mining with Classical Methods to Automate Learning Strategies of Scientists",
        "authors": [
            "Aparna S. Varde"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Experimental results are often plotted as 2-dimensional graphical plots (aka graphs) in scientific domains depicting dependent versus independent variables to aid visual analysis of processes. Repeatedly performing laboratory experiments consumes significant time and resources, motivating the need for computational estimation. The goals are to estimate the graph obtained in an experiment given its input conditions, and to estimate the conditions that would lead to a desired graph. Existing estimation approaches often do not meet accuracy and efficiency needs of targeted applications. We develop a computational estimation approach called AutoDomainMine that integrates clustering and classification over complex scientific data in a framework so as to automate classical learning methods of scientists. Knowledge discovered thereby from a database of existing experiments serves as the basis for estimation. Challenges include preserving domain semantics in clustering, finding matching strategies in classification, striking a good balance between elaboration and conciseness while displaying estimation results based on needs of targeted users, and deriving objective measures to capture subjective user interests. These and other challenges are addressed in this work. The AutoDomainMine approach is used to build a computational estimation system, rigorously evaluated with real data in Materials Science. Our evaluation confirms that AutoDomainMine provides desired accuracy and efficiency in computational estimation. It is extendable to other science and engineering domains as proved by adaptation of its sub-processes within fields such as Bioinformatics and Nanotechnology.",
        "link": "https://dl.acm.org/doi/10.1145/3502736"
    },
    {
        "title": "Where is the road for issue reports classification based on text mining?",
        "authors": [
            "Qiang Fan",
            "Yue Yu",
            "Gang Yin",
            "Tao Wang",
            "Huaimin Wang"
        ],
        "date": "November 2017",
        "source": "ESEM '17: Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Currently, open source projects receive various kinds of issues daily, because of the extreme openness of Issue Tracking System (ITS) in GitHub. ITS is a labor-intensive and time-consuming task of issue categorization for project managers. However, a contributor is only required a short textual abstract to report an issue in GitHub. Thus, most traditional classification approaches based on detailed and structured data (e.g., priority, severity, software version and so on) are difficult to adopt. In this paper, issue classification approaches on a large-scale dataset, including 80 popular projects and over 252,000 issue reports collected from GitHub, were investigated. First, four traditional text-based classification methods and their performances were discussed. Semantic perplexity (i.e., an issues description confuses bug-related sentences with nonbug-related sentences) is a crucial factor that affects the classification performances based on quantitative and qualitative study. Finally, A two-stage classifier framework based on the novel metrics of semantic perplexity of issue reports was designed. Results show that our two-stage classification can significantly improve issue classification performances.",
        "link": "https://dl.acm.org/doi/10.1109/ESEM.2017.19"
    },
    {
        "title": "Feature-based Facebook reviews process model for e-management using data mining",
        "authors": [
            "Anish Kumar Varudharajulu",
            "Yongsheng Ma"
        ],
        "date": "January 2019",
        "source": "IC4E '19: Proceedings of the 10th International Conference on E-Education, E-Business, E-Management and E-Learning",
        "abstract": "The data generated from online communication acts as potential gold mines for discovering knowledge for researchers. A large amount of data is also generated in the form of web documents, emails, blogs, and feedback, etc. Text analytics is being significantly employed to mine important information. Opinion mining is the process of extracting human thoughts and perceptions from unstructured texts. The showstopper for designing an opinion mining system for analyzing reviews arise from the fact that customer reviews are often noisy. These reviews are informally written. In addition, they are subjected to spelling mistakes, grammatical errors, improper punctuation and irrational capitalization. This paper focuses on analyzing the different classification and clustering algorithms aimed at extracting and consolidating opinions of customers from social media sites like Facebook, Twitter and through surveys, at multiple levels of granularity to monitor and measure customer satisfaction. Ours is an automated approach, in which the system aids in the process of knowledge assimilation for knowledge-based building and also performs the analytics. Domain experts ratify the knowledge base and also provide training datasets for the system to intuitively gather more instances for ratification. The system identifies opinion expressions as phrases containing opinion words, opinionated features and also opinion modifiers. These expressions are categorized as positive, negative or neutral. Opinion expressions are identified and categorized using localized linguistic techniques. Opinions can be congregated at any desired level of specificity i.e. feature level or product level, user level or service level, etc. We have developed a system based on this approach, which provides the user with a platform to analyze opinion expressions crawled from a set of pre-defined datasets.",
        "link": "https://dl.acm.org/doi/10.1145/3306500.3306514"
    },
    {
        "title": "Discriminative Topic Mining via Category-Name Guided Text Embedding",
        "authors": [
            "Yu Meng",
            "Jiaxin Huang",
            "Guangyuan Wang",
            "Zihan Wang",
            "Chao Zhang",
            "Yu Zhang",
            "Jiawei Han"
        ],
        "date": "April 2020",
        "source": "WWW '20: Proceedings of The Web Conference 2020",
        "abstract": "Mining a set of meaningful and distinctive topics automatically from massive text corpora has broad applications. Existing topic models, however, typically work in a purely unsupervised way, which often generate topics that do not fit users’ particular needs and yield suboptimal performance on downstream tasks. We propose a new task, discriminative topic mining, which leverages a set of user-provided category names to mine discriminative topics from text corpora. This new task not only helps a user understand clearly and distinctively the topics he/she is most interested in, but also benefits directly keyword-driven classification tasks. We develop CatE, a novel category-name guided text embedding method for discriminative topic mining, which effectively leverages minimal user guidance to learn a discriminative embedding space and discover category representative terms in an iterative manner. We conduct a comprehensive set of experiments to show that CatE mines high-quality set of topics guided by category names only, and benefits a variety of downstream applications including weakly-supervised classification and lexical entailment direction identification.",
        "link": "https://dl.acm.org/doi/10.1145/3366423.3380278"
    },
    {
        "title": "Comparative Study of Heart Disease Diagnosis Using Top Ten Data Mining Classification Algorithms",
        "authors": [
            "I. Ketut Agung Enriko"
        ],
        "date": "June 2019",
        "source": "ICFET '19: Proceedings of the 5th International Conference on Frontiers of Educational Technologies",
        "abstract": "Data mining has been used for many purposes, especially for prediction system. In healthcare, data mining algorithms often used in disease diagnosis. Meanwhile, heart disease is known as a primary cause of death over the years. Many studies have been performed in heart disease diagnosis using data mining methods. There are some popular data mining algorithms that can be used in heart disease diagnosis, for example, k-Nearest Neighbor, CART, and AdaBoost. The algorithms are used to analyze a sample of cardiovascular patients data and predict the heart disease type that they suffer. Some parameters are taken from the patient, including EKG morphology, blood pressure, and information about the existence of chest pain, shortness of breath, palpitation, and cold sweat. In this study, medical records data are collected from Harapan Kita Hospital and utilized as a dataset sample in this research. Top ten data mining classification algorithms are used in diagnosing heart disease from Harapan Kita Hospital data and examining their performance by checking the accuracy and speed.",
        "link": "https://dl.acm.org/doi/10.1145/3338188.3338220"
    },
    {
        "title": "Text Mining Approach for Identifying Research Trends",
        "authors": [
            "Snezhana Sulova"
        ],
        "date": "June 2021",
        "source": "CompSysTech '21: Proceedings of the 22nd International Conference on Computer Systems and Technologies",
        "abstract": "With the increase of unstructured data, the issues connected with automatic text processing, the categorization of documents and the discovery of topics have become objects of growing interest. In order to improve the process of grouping and processing research publications, we would like to propose a method based upon natural language processing. It is based on text mining technologies which aim to identify key tendencies in documents. It processes the content of publications by clustering and identifies the topics of each identified group. This analysis helps by identifying key tendencies as well as discovering emerging new areas of research. Publications from the research literature database, Scopus, were used to test the approach. The topic of the publications is “the application of digital technologies in the logistics business”. The experiments were completed using the RapidMiner Studio software.",
        "link": "https://dl.acm.org/doi/10.1145/3472410.3472433"
    },
    {
        "title": "Assessment of Vulnerability Severity using Text Mining",
        "authors": [
            "Georgios Spanos",
            "Lefteris Angelis",
            "Dimitrios Toloudis"
        ],
        "date": "September 2017",
        "source": "PCI '17: Proceedings of the 21st Pan-Hellenic Conference on Informatics",
        "abstract": "Software1 vulnerabilities are closely associated with information systems security, a major and critical field in today's technology. Vulnerabilities constitute a constant and increasing threat for various aspects of everyday life, especially for safety and economy, since the social impact from the problems that they cause is complicated and often unpredictable. Although there is an entire research branch in software engineering that deals with the identification and elimination of vulnerabilities, the growing complexity of software products and the variability of software production procedures are factors contributing to the ongoing occurrence of vulnerabilities, Hence, another area that is being developed in parallel focuses on the study and management of the vulnerabilities that have already been reported and registered in databases. The information contained in such databases includes, a textual description and a number of metrics related to vulnerabilities. The purpose of this paper is to investigate to what extend the assessment of the vulnerability severity can be inferred directly from the corresponding textual description, or in other words, to examine the informative power of the description with respect to the vulnerability severity. For this purpose, text mining techniques, i.e. text analysis and three different classification methods (decision trees, neural networks and support vector machines) were employed. The application of text mining to a sample of 70,678 vulnerabilities from a public data source shows that the description itself is a reliable and highly accurate source of information for vulnerability prioritization.",
        "link": "https://dl.acm.org/doi/10.1145/3139367.3139390"
    },
    {
        "title": "Clustering Algorithms for Spatial Data Mining",
        "authors": [
            "Chetashri Bhadane",
            "Ketan Shah"
        ],
        "date": "April 2020",
        "source": "ICGDA '20: Proceedings of the 2020 3rd International Conference on Geoinformatics and Data Analysis",
        "abstract": "With the advances in mobile and wireless technologies, there has been a rise in applications that track and share the users' geospatial data. People use several social networking sites such as Twitter, Facebook and Flickr, where they share their status updates. With the integration of Global Positioning System (GPS) with mobile phones, it is now possible to share one's locations on these social networks. GPS allows us to record and track a person's movement along with the timestamp. The data set obtained from these GPS logs is vast and is widely used to analyze the users' movement patterns. Specifically, we can find out significant locations based on the number of users present at that location and the time spent by them at such places. Once significant places have been identified, it is also possible to identify the semantic importance of these locations. This paper presents an overview of the clustering techniques used to find important places of interest using large GPS based mobility datasets. Four clustering algorithms, K-Means, DBSCAN, OPTICS and Hierarchical, are implemented, and performance is tested using real-time data of 50 users collected over 2--5 years. Performance summary depicts that K-Means and DBSCAN perform well for spatial data.",
        "link": "https://dl.acm.org/doi/10.1145/3397056.3397068"
    },
    {
        "title": "Feature Selection based Arabic Text Classification using Different Machine Learning Algorithms: Comparative Study",
        "authors": [
            "Sakina Rim Bennabi",
            "Zakaria Elberrichi"
        ],
        "date": "June 2020",
        "source": "ICIST '20: Proceedings of the 10th International Conference on Information Systems and Technologies",
        "abstract": "Feature selection is a method of data pre-processing widely used when mining large data, such as textual classification. Several studies have been conducted to compare the different methods of feature selection applied to corpora in English. Unfortunately, a small number of works concern the Arabic language. This article aims to present a comparative study of different feature selection techniques including: Chi2, the ANOVA method and mutual information, applied on a corpus in Arabic language, while also diversifying the machine learning algorithms (Naive Bayes, SVM and KNN). This experimental study has shown in general that reducing dimensionality with feature selection techniques has slightly affected the performance of textual classification, reducing the size of the corpus by up to 1%.",
        "link": "https://dl.acm.org/doi/10.1145/3447568.3448531"
    },
    {
        "title": "Intelligent Data Analysis using Optimized Support Vector Machine Based Data Mining Approach for Tourism Industry",
        "authors": [
            "Ms Promila Sharma",
            "Uma Meena",
            "Girish Kumar Sharma"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Data analysis involves the deployment of sophisticated approaches from data mining methods, information theory, and artificial intelligence in various fields like tourism, hospitality, and so on for the extraction of knowledge from the gathered and preprocessed data. In tourism, pattern analysis or data analysis using classification is significant for finding the patterns that represent new and potentially useful information or knowledge about the destination and other data. Several data mining techniques are introduced for the classification of data or patterns. However, overfitting, less accuracy, local minima, sensitive to noise are the drawbacks in some existing data mining classification methods. To overcome these challenges, Support vector machine with Red deer optimization (SVM-RDO) based data mining strategy is proposed in this article. Extended Kalman filter (EKF) is utilized in the first phase, i.e., data cleaning to remove the noise and missing values from the input data. Mantaray foraging algorithm (MaFA) is used in the data selection phase, in which the significant data are selected for the further process to reduce the computational complexity. The final phase is the classification, in which SVM-RDO is proposed to access the useful pattern from the selected data. PYTHON is the implementation tool used for the experiment of the proposed model. The experimental analysis is done to show the efficacy of the proposed work. From the experimental results, the proposed SVM-RDO achieved better accuracy, precision, recall, and F1 score than the existing methods for the tourism dataset. Thus, it is showed the effectiveness of the proposed SVM-RDO for pattern analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3494566"
    },
    {
        "title": "Applying text mining to predict learners' cognitive engagement",
        "authors": [
            "Hayati Hind",
            "Mohammed Khalidi Idrissi",
            "Samir Bennani"
        ],
        "date": "October 2017",
        "source": "SCAMS '17: Proceedings of the Mediterranean Symposium on Smart City Application",
        "abstract": "In academic process, engagement represent one of the important success' factor. This paper proposes a predictive system for learners' cognitive engagement based on their online discussion forums participation. In the first level the ontology OWL and the LSA method are used to perform a semantic classification system of the threads according to a specific context chosen by the tutor. Then the Text mining, as a predictive method, is applied to the classified threads and learners' participation in the forums.",
        "link": "https://dl.acm.org/doi/10.1145/3175628.3175655"
    },
    {
        "title": "Investigate Transitions into Drug Addiction through Text Mining of Reddit Data",
        "authors": [
            "John Lu",
            "Sumati Sridhar",
            "Ritika Pandey",
            "Mohammad Al Hasan",
            "Georege Mohler"
        ],
        "date": "July 2019",
        "source": "KDD '19: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Increasing rates of opioid drug abuse and heightened prevalence of online support communities underscore the necessity of employing data mining techniques to better understand drug addiction using these rapidly developing online resources. In this work, we obtained data from Reddit, an online collection of forums, to gather insight into drug use/misuse using text snippets from users narratives. Specifically, using users' posts, we trained a binary classifier which predicts a user's transitions from casual drug discussion forums to drug recovery forums. We also proposed a Cox regression model that outputs likelihoods of such transitions. In doing so, we found that utterances of select drugs and certain linguistic features contained in one's posts can help predict these transitions. Using unfiltered drug-related posts, our research delineates drugs that are associated with higher rates of transitions from recreational drug discussion to support/recovery discussion, offers insight into modern drug culture, and provides tools with potential applications in combating the opioid crisis.",
        "link": "https://dl.acm.org/doi/10.1145/3292500.3330737"
    },
    {
        "title": "Analysing digital banking reviews using text mining",
        "authors": [
            "Li Chen Cheng",
            "Legaspi Rhea Sharmayne"
        ],
        "date": "December 2020",
        "source": "ASONAM '20: Proceedings of the 12th IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Digital banks are new entrants in the banking industry in the Philippines as they only started late 2018. Since then, a handful of players have and are still emerging. With more and more people becoming technologically savvy, it is very critical for financial institutions to develop a digital banking application that will stand out from the competition. This paper aims to use text mining methods to analyse digital banking application reviews. This study will perform topic modelling using LDA to explore customer concerns and will mine association rules between the digital banking features with the review score. The results will reveal which areas the digital banking application can further optimize for customer satisfaction and retention.",
        "link": "https://dl.acm.org/doi/10.1109/ASONAM49781.2020.9381429"
    },
    {
        "title": "Mining Moocs Videos Metadata Using Classification Techniques",
        "authors": [
            "El Harrak Othman",
            "Ghadi Abderrahim",
            "El Bouhdidi Jaber"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "In few years, the internet has become the first source of information for most people, Today MOOCs makes it possible for everyone to access to the education over the world, it's represents an emerging methodology of online teaching and an important development in open education. Due to the rapid development of Moocs and the rapid growth of digital data and video database over the Internet, it is becoming very difficult for learners to browsing and choosing the best training for them. The scientific community has increased the amount of research into new technologies, with a view to improve the digital video utilization: its archiving, indexing, accessibility, acquisition, store and even its process and usability. All these parts of the video utilization entail the necessity of the extraction of all important information of a video, especially in cases of lack of metadata information. Web video mining is retrieving the content using data mining techniques from World Wide Web. There are two approaches for web video mining using traditional image processing (signal processing) and metadata based approach. In this paper, we present the various research makes in this subject and we propose an effective methodology to extract the metadata from Moocs videos and classify them based on the extracted metadata by applying data mining techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090450"
    },
    {
        "title": "Text mining with HathiTrust",
        "authors": [
            "Eleanor Dickson Koehl",
            "Ryan Dubnicek"
        ],
        "date": "June 2019",
        "source": "JCDL '19: Proceedings of the 18th Joint Conference on Digital Libraries",
        "abstract": "This tutorial will introduce attendees to the HathiTrust Research Center and its tools and services for computational text analysis research. HTRC leverages the scope and scale of the HathiTrust Digital Library collection to create opportunities for researchers to perform text data mining on subsets of the corpus. Attendees will develop skills that will allow them to conduct basic text analysis research using HathiTrust data, learn techniques for engaging with scholars on their text data mining research, and explore strategies for computational access to large-scale digital collections.",
        "link": "https://dl.acm.org/doi/10.1109/JCDL.2019.00115"
    },
    {
        "title": "Text mining for plagiarism detection: multivariate pattern detection for recognition of text similarities",
        "authors": [
            "Konstantinos Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2018",
        "source": "ASONAM '18: Proceedings of the 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "The problem of plagiarism the recent years has been intensified by the availability of information in digital form and the accessibility of the electronic libraries through the Internet. As a result, plagiarism detection has been transformed into a big data analytics problem since the number of digital sources is extravagant and a new document needs to be compared with millions of other existing documents. In this paper, a text mining methodology is proposed that can detect all common patterns between a document and the documents in a reference database. The technique is based on a pattern detection algorithm and the corresponding data structure that enables the algorithm to detect all common patterns. The methodology has been applied in a well-defined dataset providing very promising results identifying difficult cases of plagiarism such as technical disguise.",
        "link": "https://dl.acm.org/doi/10.5555/3382225.3382424"
    },
    {
        "title": "Petroleum Engineering Data Text Classification Using Convolutional Neural Network Based Classifier",
        "authors": [
            "Tzuhan Hsu",
            "Yaoqin Zhang"
        ],
        "date": "May 2018",
        "source": "ICMLT '18: Proceedings of the 2018 International Conference on Machine Learning Technologies",
        "abstract": "In this paper, authors introduce a convolutional neural network (CNN) based classification approach for petroleum engineering event. Recently, text classification problem has raised substantial attention in the field of machine learning. The aim of text classification process is to assign correct label to a text on the basis of its content. However, due to the imprecise of feature representation, most of former text classification methods cannot export satisfying result. To solve this problem, authors propose a model based on convolutional neural network. Authors conduct experiments with real world petroleum engineering data text, artificially labelled into 6 categories. Beside purposed CNN-based model, the authors also implement other baseline machine learning approaches: k nearest neighbor, random forest, support vector machines, recurrent neural network, gated recurrent units, long short-term memory. Among all these models, our CNN-based model reach best performance in text classification task. In the last part of this paper, authors compare and analysis their purpose and baseline model.",
        "link": "https://dl.acm.org/doi/10.1145/3231884.3231898"
    },
    {
        "title": "Classifying Indonesian Online Articles as Advertisement Placement Base Using Text Mining",
        "authors": [
            "Nadhira Tasya",
            "Arian Dhini"
        ],
        "date": "July 2017",
        "source": "ICBIM 2017: Proceedings of the International Conference on Business and Information Management",
        "abstract": "Rapid development in technological aspect resulting in growing level of human needs for the latest news, so that emerged a new trend of publishing and accessing news through online media or usually called online journalism. In addition, the number of people who sell and purchase through online sites also continues to increase and this opportunity is utilized by the company and the advertiser by implementing targeted web advertising. However, the high number of articles that have been published and accessed leads to great opportunities for errors in determining where to place the ads. Therefore, it needs a system that can categorize articles accessed by users as the basis of advertisement placement by the company and this classification system can be done by applying the method of Data Mining and Text Mining. This research uses document data in the form of article content that will be categorized into twenty categories of class of advertisement by using Text Mining technique with Support Vector Machine algorithm. The results of this study may be used by companies or advertisers as a basis for placement of ads on selected online media sites.",
        "link": "https://dl.acm.org/doi/10.1145/3134271.3134288"
    },
    {
        "title": "Mining Free-Text Medical Notes for Suicide Risk Assessment",
        "authors": [
            "Marios Adamou",
            "Grigoris Antoniou",
            "Elissavet Greasidou",
            "Vincenzo Lagani",
            "Paulos Charonyktakis",
            "Ioannis Tsamardinos"
        ],
        "date": "July 2018",
        "source": "SETN '18: Proceedings of the 10th Hellenic Conference on Artificial Intelligence",
        "abstract": "Suicide has been considered as an important public health issue for a very long time, and is one of the main causes of death worldwide. Despite suicide prevention strategies being applied, the rate of suicide has not changed substantially over the past decades. Advances in machine learning make it possible to attempt to predict suicide based on the analysis of relevant data to inform clinical practice. This paper reports on findings from the analysis of data of patients who died by suicide in the period 2013-2016 and made use of both structured data and free-text medical notes. We focus on examining various text-mining approaches to support risk assessment. The results show that using advance machine learning and text-mining techniques, it is possible to predict within a specified period which people are most at risk of taking their own life at the time of referral to a mental health service.",
        "link": "https://dl.acm.org/doi/10.1145/3200947.3201020"
    },
    {
        "title": "Integration of data mining techniques in e-learning systems: Clustering Profil of Lerners and Recommender Course System",
        "authors": [
            "A. El Moustamid",
            "E. En-Naimi",
            "J. El Bouhdidi"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "Data mining aims at the extraction of knowledge from large quantities of data by automatic or semi-automatic methods. In the field of education and training e-learning, Data Mining techniques as clustering (The aim of this technique is to identify groups that are similar in some aspect) [21], prediction (is used for developing model where combination of data viewed from different aspects can effect single viewed data. Here, is important to watch how predictors variables are effecting predicted variable)[21], classification are applied in e-learning to improve the level of learners by providing learning according to the learners' requirement) are used to analyze learners profile, to predict and improve student performance. The aim of our project is to develop a system capable of analyzing learners profile and indexing web videos in order to offer learners a rich database with courses that correspond to their levels.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090453"
    },
    {
        "title": "Multi-level mining and visualization of scientific text collections: Exploring a bi-lingual scientific repository",
        "authors": [
            "Pablo Accuosto",
            "Francesco Ronzano",
            "Daniel Ferrés",
            "Horacio Saggion"
        ],
        "date": "December 2017",
        "source": "WOSP 2017: Proceedings of the 6th International Workshop on Mining Scientific Publications",
        "abstract": "We present a system to mine and visualize collections of scientific documents by semantically browsing information extracted from single publications or aggregated throughout corpora of articles. The text mining tool performs deep analysis of document collections allowing the extraction and interpretation of research paper's contents. In addition to the extraction and enrichment of documents with metadata (titles, authors, affiliations, etc), the deep analysis performed comprises semantic interpretation, rhetorical analysis of sentences, triple-based information extraction, and text summarization. The visualization components allow geographical-based exploration of collections, topic-evolution interpretation, and collaborative network analysis among others. The paper presents a case study of a bi-lingual collection in the field of Natural Language Processing (NLP).",
        "link": "https://dl.acm.org/doi/10.1145/3127526.3127529"
    },
    {
        "title": "Hierarchical Topic Mining via Joint Spherical Tree and Text Embedding",
        "authors": [
            "Yu Meng",
            "Yunyi Zhang",
            "Jiaxin Huang",
            "Yu Zhang",
            "Chao Zhang",
            "Jiawei Han"
        ],
        "date": "August 2020",
        "source": "KDD '20: Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Mining a set of meaningful topics organized into a hierarchy is intuitively appealing since topic correlations are ubiquitous in massive text corpora. To account for potential hierarchical topic structures, hierarchical topic models generalize flat topic models by incorporating latent topic hierarchies into their generative modeling process. However, due to their purely unsupervised nature, the learned topic hierarchy often deviates from users' particular needs or interests. To guide the hierarchical topic discovery process with minimal user supervision, we propose a new task, Hierarchical Topic Mining, which takes a category tree described by category names only, and aims to mine a set of representative terms for each category from a text corpus to help a user comprehend his/her interested topics. We develop a novel joint tree and text embedding method along with a principled optimization procedure that allows simultaneous modeling of the category tree structure and the corpus generative process in the spherical space for effective category-representative term discovery. Our comprehensive experiments show that our model, named JoSH, mines a high-quality set of hierarchical topics with high efficiency and benefits weakly-supervised hierarchical text classification tasks.",
        "link": "https://dl.acm.org/doi/10.1145/3394486.3403242"
    },
    {
        "title": "Unsupervised behavioural mining and clustering for malware family identification",
        "authors": [
            "Khanh Huu The Dam",
            "Thomas Given-Wilson",
            "Axel Legay"
        ],
        "date": "March 2021",
        "source": "SAC '21: Proceedings of the 36th Annual ACM Symposium on Applied Computing",
        "abstract": "More accurate and advanced detection and classification of malware requires exploiting program behaviour and not merely syntactic or static features. One approach is to use system call dependency graphs (SCDGs) that represent the program behaviour by interactions with the system, and the relations between these interactions. These SCDGs have been used with supervised learning techniques to very accurately detect and classify malware. This works considers the unsupervised learning challenge of mining for common clusters of behaviour without a priori knowledge. This allows for clustering of similar programs by behaviour, that can then be used for either classification or further analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3412841.3441919"
    },
    {
        "title": "Text Mining for Evaluating Authors' Birth and Death Years",
        "authors": [
            "Dror Moghaz",
            "Yaakov Hacohen-Kerner",
            "Dov Gabbay"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "This article presents a unique method in text and data mining for finding the era, i.e., mining temporal data, in which an anonymous author was living. Finding this era can assist in the examination of a fake document or extracting the time period in which a writer lived. The study and the experiments concern Hebrew, and in some parts, Aramaic and Yiddish rabbinic texts. The rabbinic texts are undated and contain no bibliographic sections, posing an interesting challenge. This work proposes algorithms using key phrases and key words that allow the temporal organization of citations together with linguistic patterns. Based on these key phrases, key words, and the references, we established several types of “Iron-clad,” Heuristic and Greedy rules for estimating the years of birth and death of a writer in an interesting classification task. Experiments were conducted on corpora, including documents authored by 12, 24, and 36 rabbinic writers and demonstrated promising results.",
        "link": "https://dl.acm.org/doi/10.1145/3281631"
    },
    {
        "title": "Transfer Learning to Timed Text Based Video Classification Using CNN",
        "authors": [
            "Zenun Kastrati",
            "Ali Shariq Imran",
            "Arianit Kurti"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "Open educational video resources are gaining popularity with a growing number of massive open online courses (MOOCs). This has created a niche for content providers to adopt effective solutions in automatically organizing and structuring of educational resources for maximum visibility. Recent advances in deep learning techniques are proving useful in managing and classifying resources into appropriate categories. This paper proposes one such convolutional neural network (CNN) model for classifying video lectures in a MOOC setting using a transfer learning approach. The model uses a time-aligned text transcripts corresponding to video lectures from six broader subject categories. Video lectures and their corresponding transcript dataset is gathered from the Coursera MOOC platform. Two different CNN models are proposed: i) CNN based classification using embeddings learned from our MOOC dataset, ii) CNN based classification using transfer learning. Word embeddings generated from two well known state-of-the-art pre-trained models Word2Vec and GloVe, are used in the transfer learning approach for the second case. The proposed CNN models are evaluated using precision, recall, and F1 score and the obtained performance is compared with both conventional and deep learning classifiers. The proposed CNN models have an F1 score improvement of 10-22 percentage points over DNN and conventional classifiers",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326483"
    },
    {
        "title": "Evolutionary role mining in complex networks by ensemble clustering",
        "authors": [
            "Sarvenaz Choobdar",
            "Pedro Ribeiro",
            "Fernando Silva"
        ],
        "date": "April 2017",
        "source": "SAC '17: Proceedings of the Symposium on Applied Computing",
        "abstract": "The structural patterns in the neighborhood of nodes assign unique roles to the nodes. Mining the set of existing roles in a network provides a descriptive profile of the network and draws its general picture. This paper proposes a new method to determine structural roles in a dynamic network based on the current position of nodes and their historic behavior. We develop a temporal ensemble clustering technique to dynamically find groups of nodes, holding similar tempo-structural roles. We compare two weighting functions, based on age and distribution of data, to incorporate temporal behavior of nodes in the role discovery. To evaluate the performance of the proposed method, we assess the results from two points of view: 1) goodness of fit to current structure of the network; 2) consistency with historic data. We conduct the evaluation using different ensemble clustering techniques. The results on real world networks demonstrate that our method can detect tempo-structural roles that simultaneously depict the topology of a network and reflect its dynamics with high accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3019612.3019815"
    },
    {
        "title": "Analyzing used-car web listings via text mining",
        "authors": [
            "Ayhan Demiriz",
            "Fatma Cantaş"
        ],
        "date": "October 2017",
        "source": "IML '17: Proceedings of the 1st International Conference on Internet of Things and Machine Learning",
        "abstract": "Used car trade is one of the major components of the world economies. It is not uncommon to sell a car by placing an internet advertisement irrespective of the geography in these days. A typical content of an advertisement is usually composed of two parts namely the structured and the free text data. The structured data may include some information about the asking price, make, model, year, mileage of the car and the contact info. In most cases, seller may give important clues about the car's current conditions in the free text data where the title (head) of the advertisement can be included as free text too. This paper reports preliminary results from a text mining study conducted on 75K used car internet listings collected from two major car listing web sites in Turkey. As expected, the words and the phrases related to the description of the car are observed to be frequent. The leading concepts in the free text are found to be regarding how to describe the current condition of a car, for example \"no crash history\".",
        "link": "https://dl.acm.org/doi/10.1145/3109761.3109782"
    },
    {
        "title": "Text classification of Diseases Treated by Traditional Chinese Medicine Prescription based on machine learning",
        "authors": [
            "Hanqing Zhao",
            "Bo Han",
            "Chen Li"
        ],
        "date": "September 2020",
        "source": "ISAIMS '20: Proceedings of the 1st International Symposium on Artificial Intelligence in Medical Sciences",
        "abstract": "OBJECTIVE: To explore the application of machine learning in th e identification of diseases treated by traditional Chinese medicine prescriptions. METHODS: Based on the composition of the text document of Chinese medicine prescriptions, the prescriptions were divided into cough, headache and diarrhea. THUTCT were introduced to establish and train two machine learning text classification models, LibSVM and LibLinear, and the prescriptions to be deter mined were put into the model for classification and prediction. R ESULTS: The Precision rate of LibSVM model and Liblinear mod el were 0.7283 and 0.6690. Seven prescriptions were classified an d predicted, and the results were in line with expectations. CONCLUSION: THUCTC has good universality for the content of TCM prescriptions, high classification accuracy and fast testing speed, which is suitable for the text classification and discrimination research of TCM prescriptions for diseases.",
        "link": "https://dl.acm.org/doi/10.1145/3429889.3429895"
    },
    {
        "title": "Evaluation of retweet clustering method classification method using retweets on Twitter without text data",
        "authors": [
            "K. Uchida",
            "F. Toriumi",
            "T. Sakaki"
        ],
        "date": "August 2017",
        "source": "WI '17: Proceedings of the International Conference on Web Intelligence",
        "abstract": "Burst phenomena, which frequently occur on social media, are caused by such social events as flaming on the internet, elections, and natural disasters. To understand people's thoughts and feelings, we must classify their opinions from burst phenomena. Therefore, classification methods that categorize tweets are critical. However, since most classification methods focus on text mining, they cannot group tweets by topics because each tweet has poor linguistic similarities. We used a non-text-based classification method proposed by Baba et al. that groups tweets by topics, even if they have poor linguistic similarities, and verified its validity by comparing it with a text-based classification method in two different evaluations: qualitative and quantitative. In the qualitative evaluation part, we did a questionnaire survey and validated the suitability of the topic clusters created using both the non-and text-based methods. Since evaluating the similarity of every pair of tweets in each topic is difficult, we evaluated the similarity between sampled pairs in the survey and acquired more appropriate topic clustering results using the non-text-based method than the text-based method. In the quantitative evaluation part, we focused on the robustness of each method against data reduction. Many approaches analyze social media data, especially because collecting data from social media is comparatively easy. However, since collecting the whole data of burst phenomena is very costly due to the vast amounts of available social media data, robustness against data reduction is an important index to evaluate classification methods. With the non-text-based method, over 55% of the pairs of tweets in the same cluster were also included in the same cluster even when the data were reduced to 10% in all three of our example cases. In this paper, as a source we focus on Twitter, one of the most popular microblogging services. Using clustering to conduct detailed case analyses, we scrutinized three burst cases that include natural disasters and flaming on the internet and found that a non-text-based method more effectively classified tweets in burst phenomena than a text-based method.",
        "link": "https://dl.acm.org/doi/10.1145/3106426.3106451"
    },
    {
        "title": "Using Spark for Text Mining on Large Scale Liver Cancer Literature",
        "authors": [
            "Ming-Yen Lin",
            "Yu-Ju Lin",
            "Sue-Chen Hsueh"
        ],
        "date": "January 2021",
        "source": "BDET 2021: 2021 the 3rd International Conference on Big Data Engineering and Technology (BDET)",
        "abstract": "Cancer is one of the main causes of death. The number of known cancer to-date is more than one hundred. Liver cancer, ranked after trachea and lung cancer, has long been high in the cancer leading cause in Taiwan and even ranked as second in 2016. The number of scientific articles related to cancer proliferates every year. The number reaches as high as 20 million in PubMed so that discovering useful information from the massive collection is very difficult. In addition, using a single machine to sift through these articles is very time-consuming. Therefore, we present a big data analytic framework using the distributed Apache Spark platform for text mining in PubMed literature. We establish a prediction model for liver cancer articles so that researchers may effectively validate whether an article is related to liver cancer or not. Classification models in Spark MLlib including Linear Support Vector Machines (SVM), Logistic Regression, are used in our experiments. Relevancy to liver cancer is further confirmed by using MeSH (Medical Subject Headings) terms. Logistic regression is about 3 times faster than SVMs and the accuracy of both methods is close to 95% in the experiments using hold-out validation. When max_features is 500 and min_df ≤ 0.1 (or min_df = 1), the accuracy may reach 96%. In the experiments with K-fold cross-validation, the accuracy of SVMs methods is 96%. The experimental results show that that our prediction model may effectively classify liver cancer articles.",
        "link": "https://dl.acm.org/doi/10.1145/3474944.3474958"
    },
    {
        "title": "Automatic topic classification of test cases using text mining at an Android smartphone vendor",
        "authors": [
            "Junji Shimagaki",
            "Yasutaka Kamei",
            "Naoyasu Ubayashi",
            "Abram Hindle"
        ],
        "date": "October 2018",
        "source": "ESEM '18: Proceedings of the 12th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Background: An Android smartphone is an ecosystem of applications, drivers, operating system components, and assets. The volume of the software is large and the number of test cases needed to cover the functionality of an Android system is substantial. Enormous effort has been already taken to properly quantify \"what features and apps were tested and verified?\". This insight is provided by dashboards that summarize test coverage and results per feature. One method to achieve this is to manually tag or label test cases with the topic or function they cover, much like function points. At the studied Android smartphone vendor, tests are labelled with manually defined tags, so-called \"feature labels (FLs)\", and the FLs serve to categorize 100s to 1000s test cases into 10 to 50 groups. Aim: Unfortunately for developers, manual assignment of FLs to 1000s of test cases is a time consuming task, leading to inaccurately labeled test cases, which will render the dashboard useless. We created an automated system that suggests tags/labels to the developers for their test cases rather than manual labeling. Method: We use machine learning models to predict and label the functionality tested by 10,000 test cases developed at the company. Results: Through the quantitative experiments, our models achieved acceptable F-1 performance of 0.3 to 0.88. Also through the qualitative studies with expert teams, we showed that the hierarchy and path of tests was a good predictor of a feature's label. Conclusions: We find that this method can reduce tedious manual effort that software developers spent classifying test cases, while providing more accurate classification results.",
        "link": "https://dl.acm.org/doi/10.1145/3239235.3268927"
    },
    {
        "title": "To apply Data Mining for Classification of Crowd sourced Software Requirements",
        "authors": [
            "Soonh Taj",
            "Qasim Arain",
            "Imran Memon",
            "Asma Zubedi"
        ],
        "date": "April 2019",
        "source": "ICSIE '19: Proceedings of the 8th International Conference on Software and Information Engineering",
        "abstract": "Now a day's main focus of developers is to build quality software that works according to customer needs and for this reason it is necessary to gather right requirements as requirement elicitation is the critical step that impacts on the success of software project as misinterpreted requirements leads to the failure of software project. By keeping this in mind a research is carried out on improving requirements elicitation process and automating the process of classifying requirements. In this research, a model is proposed which will help in this scenario for requirements elicitation and requirement classification. This paper presents a model in which crowd sourcing approach is used so that customers, end users, stakeholders, developers and software engineers can make active participation for requirement elicitation process and requirements gathered using crowdsourcing approach are used by model for classification process i.e. classification of requirements into functional and non-functional requirements. For the proof of proposed model a case study is conducted. Results of case study provided the usefulness and efficiency of proposed model for classification of crowd sourced software requirements.",
        "link": "https://dl.acm.org/doi/10.1145/3328833.3328837"
    },
    {
        "title": "Text classification using Fuzzy TF-IDF and Machine Learning Models",
        "authors": [
            "Mariem Bounabi",
            "Karim El Moutaouakil",
            "Khalid Satori"
        ],
        "date": "October 2019",
        "source": "BDIoT'19: Proceedings of the 4th International Conference on Big Data and Internet of Things",
        "abstract": "The representation of the information has an important impact on the text classification task. Several weighting methods were proposed in the literature, and the term frequency-inverse term frequency (TFIDF), the most know on the text treatment field. The FTF-IDF is a vector representation where the components of the TFIDF are presented as inputs to the Fuzzy Inference System (FIS). In this work, we compare several Machin Learning algorithms such as Naïve Bayes and its derivatives, SVM and Random forest classifiers, using the FTF-IDF representation. To improve the quality of the used classifiers, we call sum attribute selection methods. The recognition rate, for the tested systems, is satisfied, where the system based on naïve Bayes classifier, the FTF-IDF weighting terms, and the info gain select attributes method gives 98.7% as accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3372938.3372956"
    },
    {
        "title": "Text Mining in Cybersecurity: A Systematic Literature Review",
        "authors": [
            "Luciano Ignaczak",
            "Guilherme Goldschmidt",
            "Cristiano André Da Costa",
            "Rodrigo Da Rosa Righi"
        ],
        "date": null,
        "source": "ACM Computing Surveys",
        "abstract": "The growth of data volume has changed cybersecurity activities, demanding a higher level of automation. In this new cybersecurity landscape, text mining emerged as an alternative to improve the efficiency of the activities involving unstructured data. This article proposes a Systematic Literature Review (SLR) to present the application of text mining in the cybersecurity domain. Using a systematic protocol, we identified 2,196 studies, out of which 83 were summarized. As a contribution, we propose a taxonomy to demonstrate the different activities in the cybersecurity domain supported by text mining. We also detail the strategies evaluated in the application of text mining tasks and the use of neural networks to support activities involving unstructured data. The work also discusses text classification performance aiming its application in real-world solutions. The SLR also highlights open gaps for future research, such as the analysis of non-English content and the intensification in the usage of neural networks.",
        "link": "https://dl.acm.org/doi/10.1145/3462477"
    },
    {
        "title": "Using Text Mining to Analyze the Financial Patents",
        "authors": [
            "Yung-Feng Lu",
            "Jia-Lang Xu",
            "Mu-Yen Chen"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "With the advancement and development of science and technology, Nowadays society is the development of information technology, cloud computing, mobile Internet, Internet of Things, block chain and information technology.The combination of finance and technology has made financial technology terms gradually emerging.. This research analyzes the patents of financial technology. However, Taiwan's financial technology patents are mostly G06Q-20 (payment plan, architecture or agreement), G06Q-30 (commercial, such as marketing, shopping, payment, auction or e-commerce). G06Q-40 (financial, investment or tax treatment, insurance).The research use text mining technology to analyze and understand Taiwan's trends and developments in financial technology. The research results show that G06Q-20 patents are biased towards information security and mobile payment. G06Q-30 patents favor shopping websites and advertising. G06Q-40 patents are biased towards diversity, such as securities, insurance, finance, and stocks.",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326478"
    },
    {
        "title": "Identifying Online Profiles of Distance Learning Students Using Data Mining Techniques",
        "authors": [
            "Osama Islam",
            "Muazzam Siddiqui",
            "Naif Radi Aljohani"
        ],
        "date": "October 2019",
        "source": "ICDTE '19: Proceedings of the 3rd International Conference on Digital Technology in Education",
        "abstract": "Educational data has grown over the years with the increased use of technology within educational environments. This has led to a huge amount of data being stored in various data sources representing the student, his/her activities, and other aspects relevant to the learning process. To meet this analytical need, Educational Data Mining (EDM) has emerged to assist educational institutions in identifying key benefits such as students at risk, the level of student engagement or predicting student performance. The aim of this research was to explore the various aspects of student interaction data using data mining techniques to identify relevant patterns of behaviors and possible key attributes that have higher degrees of influence on distance learning students. The main findings identified several patterns of user online profiles based on a set of adopted learning strategies, the research proposed a framework for analyzing such interaction data based on R and Hadoop platforms to correlate online profiles with student performance.",
        "link": "https://dl.acm.org/doi/10.1145/3369199.3369249"
    },
    {
        "title": "Extremely Fast Decision Tree Mining for Evolving Data Streams",
        "authors": [
            "Albert Bifet",
            "Jiajin Zhang",
            "Wei Fan",
            "Cheng He",
            "Jianfeng Zhang",
            "Jianfeng Qian",
            "Geoff Holmes",
            "Bernhard Pfahringer"
        ],
        "date": "August 2017",
        "source": "KDD '17: Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
        "abstract": "Nowadays real-time industrial applications are generating a huge amount of data continuously every day. To process these large data streams, we need fast and efficient methodologies and systems. A useful feature desired for data scientists and analysts is to have easy to visualize and understand machine learning models. Decision trees are preferred in many real-time applications for this reason, and also, because combined in an ensemble, they are one of the most powerful methods in machine learning. In this paper, we present a new system called STREAMDM-C++, that implements decision trees for data streams in C++, and that has been used extensively at Huawei. Streaming decision trees adapt to changes on streams, a huge advantage since standard decision trees are built using a snapshot of data, and can not evolve over time. STREAMDM-C++ is easy to extend, and contains more powerful ensemble methods, and a more efficient and easy to use adaptive decision trees. We compare our new implementation with VFML, the current state of the art implementation in C, and show how our new system outperforms VFML in speed using less resources.",
        "link": "https://dl.acm.org/doi/10.1145/3097983.3098139"
    },
    {
        "title": "Data miners' little helper: data transformation activity cues for cluster analysis on document collections",
        "authors": [
            "Tania Cerquitelli",
            "Evelina Di Corso",
            "Francesco Ventura",
            "Silvia Chiusano"
        ],
        "date": "June 2017",
        "source": "WIMS '17: Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "In this paper we propose a new self-learning engine to streamline the analytics process, as it enables analysts to mine massive data repositories with minimal user intervention. In the context of cluster analysis on a collection of documents this new system, named SELF-DATA (SELF-learning DAta TrAnsformation), suggests to the analyst how to configure the whole mining process for a given dataset. SELF-DATA relies on an engine exploring different data weighting schemas (e.g., normalized term frequencies) and data transformation methods (e.g., PCA) before applying the cluster analysis, evaluating and comparing solutions through different quality indices (e.g., weighted Silhouette), and presenting the k-top solutions to the analyst. SELF-DATA will also include a knowledge base storing results of experiments on previously processed datasets, and a classification algorithm trained on the knowledge base content to forecast the best configuration for the whole mining process for an unexplored dataset. The first development of SELF-DATA running on Apache Spark has been validated on 5 collections of documents. Experimental results highlight that TF-IDF and logarithmic entropy are effective to measure item relevance with sparse datasets, and the LSI method outperforms PCA with a large dictionary.",
        "link": "https://dl.acm.org/doi/10.1145/3102254.3102288"
    },
    {
        "title": "Interactive Visual Graph Mining and Learning",
        "authors": [
            "Ryan A. Rossi",
            "Nesreen K. Ahmed",
            "Rong Zhou",
            "Hoda Eldardiry"
        ],
        "date": null,
        "source": "ACM Transactions on Intelligent Systems and Technology",
        "abstract": "This article presents a platform for interactive graph mining and relational machine learning called GraphVis. The platform combines interactive visual representations with state-of-the-art graph mining and relational machine learning techniques to aid in revealing important insights quickly as well as learning an appropriate and highly predictive model for a particular task (e.g., classification, link prediction, discovering the roles of nodes, and finding influential nodes). Visual representations and interaction techniques and tools are developed for simple, fast, and intuitive real-time interactive exploration, mining, and modeling of graph data. In particular, we propose techniques for interactive relational learning (e.g., node/link classification), interactive link prediction and weighting, role discovery and community detection, higher-order network analysis (via graphlets, network motifs), among others. GraphVis also allows for the refinement and tuning of graph mining and relational learning methods for specific application domains and constraints via an end-to-end interactive visual analytic pipeline that learns, infers, and provides rapid interactive visualization with immediate feedback at each change/prediction in real-time. Other key aspects include interactive filtering, querying, ranking, manipulating, exporting, as well as tools for dynamic network analysis and visualization, interactive graph generators (including new block model approaches), and a variety of multi-level network analysis techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3200764"
    },
    {
        "title": "Data Mining and Opinion Mining: A Tool in Educational Context",
        "authors": [
            "Myriam Peñafiel",
            "Stefanie Vásquez",
            "Diego Vásquez",
            "Juan Zaldumbide",
            "Sergio Luján-Mora"
        ],
        "date": "July 2018",
        "source": "ICoMS '18: Proceedings of the 2018 1st International Conference on Mathematics and Statistics",
        "abstract": "The use of the web as a universal communication platform generates large volumes of data (Big data), which in many cases, need to be processed so that they can become useful knowledge in face of the sceptics who have doubts about the credibility of such information. The use of web data that comes from educational contexts needs to be addressed, since that large amount of unstructured information is not being valued, losing valuable information that can be used. To solve this problem, we propose the use of data mining techniques such as sentiment analysis to validate the information that comes from the educational platforms. The objective of this research is to propose a methodology that allows the user to apply sentiment analysis in a simple way, because although some researchers have done it, very few do with data in the educational context. The results obtained prove that the proposal can be used in similar cases.",
        "link": "https://dl.acm.org/doi/10.1145/3274250.3274263"
    },
    {
        "title": "Research Hotspots Mining and Visualized Analysis Based on Linking Cluster and K-Core Decomposition",
        "authors": [
            "Ying Cai",
            "Fang Huang",
            "Shuai Wang",
            "Hao Zhang",
            "Chunxiu Du"
        ],
        "date": "May 2018",
        "source": "ICDPA 2018: Proceedings of the International Conference on Data Processing and Applications",
        "abstract": "In this paper, we focus on research hotspots mining with knowledge semantic features on discipline topic word networks, which are extracted from scientific and technical documents. For the topic networks, a novel method of combining linking community clustering with k-core decomposition is proposed to mine the hotspot community with knowledge hierarchy structure. And as a post-processing, the hierarchical community identification is implemented by k-core decomposition in order to discover the internal connection with knowledge semantic hierarchy in the research hotspot. Finally, the community density and visualization method is utilized to analyze the core and sub- problem in the research hotspot community. In experiments, the topic networks for a single discipline and interdisciplinary research hotspot are taken as examples of analysis, and the results show the proposed approach can effectively realize the objective of mining and analyzing the discipline research hotspot on the academic topic word network.",
        "link": "https://dl.acm.org/doi/10.1145/3224207.3224216"
    },
    {
        "title": "The Application of Data Mining Algorithms in the Construction of Travel Recommendation System",
        "authors": [
            "Chao Lou",
            "Zhaonan Mu",
            "Mengzhu Liu"
        ],
        "date": "January 2021",
        "source": "CONF-CDS 2021: The 2nd International Conference on Computing and Data Science",
        "abstract": "Photos shared by users of online social network often associated with tags, texts, geographic and time information. These data are ideal research resource which can be used by researchers to observe people's interests and behaviors. They can also be used to analyze people's travel behaviors and interests because it can reflect people's travel activities and experiences directly. This project aimed to consolidate people into groups based on their travel preferences of travel season, travel time span, scenery type of travel location and then analyze popular travel places for different people groups. The travel preferences were analyzed from extracted data applying PHP language and Naive Bayes classification approach. According to the analyzed data, people were divided into groups. Then, DBSCAN clustering was used to get popular locations based on the different groups. As a result, 80 thousand photo data of 5 cities have been collected from Flickr by Phython. It has been showed that people's preference of season in most of the mined cities was spring and summer and the preference of time span was short. Moreover, mined people has been divided into 16 groups according to their preference and based on the groups popular locations have been calculated by DBSCAN clustering. Theses locations have been provided in a recommendation website which was built in this project. For the future work, mining Flickr data associated with other travel social network sites could be a good way to build a more effective recommendation system.",
        "link": "https://dl.acm.org/doi/10.1145/3448734.3450477"
    },
    {
        "title": "Compare Machine Learning Models in Text Classification Using Steam User Reviews",
        "authors": [
            "Youchen Miao",
            "Zeyu Jin",
            "Yumeng Zhang",
            "Yuchen Chen",
            "Junren Lai"
        ],
        "date": "November 2021",
        "source": "ICSED '21: Proceedings of the 2021 3rd International Conference on Software Engineering and Development",
        "abstract": "Text Classification and Sentiment Analysis of game reviews are viewed as important parts in not only academic fields but also in game studies. In this paper, with more than 400 thousand game reviews on Steam platform, we preprocess the data using different libraries (sklearn, nltk, and spaCy) and use them as inputs to build three sentiment classification models based on different algorithms (Naive Bayes, SVM, and Random Forest). In contrast to previous studies that only focus on different sentiment analysis models, our paper also highlights the use of different APIs to preprocess the data and their corresponding model performance. The results show that no matter which API we choose, Random Forest models always perform the best. However, in terms of training time, Naive Bayes is the fastest. This work can be used to apply grid search for researchers to automatically find the optimum API before conducting sentiment analysis in the future.",
        "link": "https://dl.acm.org/doi/10.1145/3507473.3507480"
    },
    {
        "title": "Lost in Transduction: Transductive Transfer Learning in Text Classification",
        "authors": [
            "Alejandro Moreo",
            "Andrea Esuli",
            "Fabrizio Sebastiani"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Obtaining high-quality labelled data for training a classifier in a new application domain is often costly. Transfer Learning\n(a.k.a. “Inductive Transfer”) tries to alleviate these costs by transferring, to the “target” domain of interest, knowledge available from a different “source” domain. In transfer learning the lack of labelled information from the target domain is compensated by the availability at training time of a set of unlabelled examples from the target distribution. Transductive Transfer Learning denotes the transfer learning setting in which the only set of target documents that we are interested in classifying is known and available at training time. Although this definition is indeed in line with Vapnik’s original definition of “transduction”, current terminology in the field is confused. In this article, we discuss how the term “transduction” has been misused in the transfer learning literature, and propose a clarification consistent with the original characterization of this term given by Vapnik. We go on to observe that the above terminology misuse has brought about misleading experimental comparisons, with inductive transfer learning methods that have been incorrectly compared with transductive transfer learning methods. We then, give empirical evidence that the difference in performance between the inductive version and the transductive version of a transfer learning method can indeed be statistically significant (i.e., that knowing at training time the only data one needs to classify indeed gives an advantage). Our clarification allows a reassessment of the field, and of the relative merits of the major, state-of-the-art algorithms for transfer learning in text classification.",
        "link": "https://dl.acm.org/doi/10.1145/3453146"
    },
    {
        "title": "Machine Learning on Mining Potential Adverse Drug Reactions for Pharmacovigilance",
        "authors": [
            "Ke Jia"
        ],
        "date": "July 2021",
        "source": "DSIT 2021: 2021 4th International Conference on Data Science and Information Technology",
        "abstract": "Drugs are related to the lives and health of patients. Mining and discovering potential adverse drug reactions from a large number of unstructured texts play an important role in drug research and pharmacovigilance. It is a hot research issue in the field of biomedicine in recent years. A large number of scholars have discovered a large amount of information on adverse drug reactions from different data sources through effective machine learning methods. This paper reviews the research progress in recent years to provide a reference for future research.",
        "link": "https://dl.acm.org/doi/10.1145/3478905.3478964"
    },
    {
        "title": "TACAM: Topic And Context Aware Argument Mining",
        "authors": [
            "Michael Fromm",
            "Evgeniy Faerman",
            "Thomas Seidl"
        ],
        "date": "October 2019",
        "source": "WI '19: IEEE/WIC/ACM International Conference on Web Intelligence",
        "abstract": "In this work we address the problem of argument search. The purpose of argument search is the distillation of pro and contra arguments for requested topics from large text corpora. In previous works, the usual approach is to use a standard search engine to extract text parts which are relevant to the given topic and subsequently use an argument recognition algorithm to select arguments from them. The main challenge in the argument recognition task, which is also known as argument mining, is that often sentences containing arguments are structurally similar to purely informative sentences without any stance about the topic. In fact, they only differ semantically. Most approaches use topic or search term information only for the first search step and therefore assume that arguments can be classified independently of a topic. We argue that topic information is crucial for argument mining, since the topic defines the semantic context of an argument. Precisely, we propose different models for the classification of arguments, which take information about a topic of an argument into account. Moreover, to enrich the context of a topic and to let models understand the context of the potential argument better, we integrate information from different external sources such as Knowledge Graphs or pre-trained NLP models. Our evaluation shows that considering topic information, especially in connection with external information, provides a significant performance boost for the argument mining task.",
        "link": "https://dl.acm.org/doi/10.1145/3350546.3352506"
    },
    {
        "title": "Text mining for incoming tasks based on the urgency/importance factors and task classification using machine learning tools",
        "authors": [
            "Yasser Ali Alshehri"
        ],
        "date": "March 2020",
        "source": "ICCDA 2020: Proceedings of the 2020 the 4th International Conference on Compute and Data Analysis",
        "abstract": "In workplaces, there is a massive amount of unstructured data from different sources. In this paper, we present a case study that explains how can through communications between employees, we can help to prioritize tasks requests to increase the efficiency of their works for both technical and non-technical workers. This involves managing daily incoming tasks based on their level of urgency and importance.To allow all workers to utilize the urgency-importance matrix as a time-management tool, we need to automate this tool. The textual content of incoming tasks are analyzed, and metrics related to urgency and importance are extracted. A third factor (i.e., the response variable) is defined based on the two input variables (urgency and importance). Then, machine learning applied to the data to predict the class of incoming tasks based on data outcome desired. We used ordinal regression, neural networks, and decision tree algorithms to predict the four levels of task priority. We measure the performance of all using recalls, precisions, and F-scores. All classifiers perform higher than 89% in terms of all measures.",
        "link": "https://dl.acm.org/doi/10.1145/3388142.3388153"
    },
    {
        "title": "Text mining for malware classification using multivariate all repeated patterns detection",
        "authors": [
            "Konstantinos F. Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2019",
        "source": "ASONAM '19: Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Mobile phones have become nowadays a commodity to the majority of people. Using them, people are able to access the world of Internet and connect with their friends, their colleagues at work or even unknown people with common interests. This proliferation of the mobile devices has also been seen as an opportunity for the cyber criminals to deceive smartphone users and steel their money directly or indirectly, respectively, by accessing their bank accounts through the smartphones or by blackmailing them or selling their private data such as photos, credit card data, etc. to third parties. This is usually achieved by installing malware to smartphones masking their malevolent payload as a legitimate application and advertise it to the users with the hope that mobile users will install it in their devices. Thus, any existing application can easily be modified by integrating a malware and then presented it as a legitimate one. In response to this, scientists have proposed a number of malware detection and classification methods using a variety of techniques. Even though, several of them achieve relatively high precision in malware classification, there is still space for improvement. In this paper, we propose a text mining all repeated pattern detection method which uses the decompiled files of an application in order to classify a suspicious application into one of the known malware families. Based on the experimental results using a real malware dataset, the methodology tries to correctly classify (without any misclassification) all randomly selected malware applications of 3 categories with 3 different families each.",
        "link": "https://dl.acm.org/doi/10.1145/3341161.3350841"
    },
    {
        "title": "Comparative Study between Traditional Machine Learning and Deep Learning Approaches for Text Classification",
        "authors": [
            "Cannannore Nidhi Kamath",
            "Syed Saqib Bukhari",
            "Andreas Dengel"
        ],
        "date": "August 2018",
        "source": "DocEng '18: Proceedings of the ACM Symposium on Document Engineering 2018",
        "abstract": "In this contemporaneous world, it is an obligation for any organization working with documents to end up with the insipid task of classifying truckload of documents, which is the nascent stage of venturing into the realm of information retrieval and data mining. But classification of such humongous documents into multiple classes, calls for a lot of time and labor. Hence a system which could classify these documents with acceptable accuracy would be of an unfathomable help in document engineering. We have created multiple classifiers for document classification and compared their accuracy on raw and processed data. We have garnered data used in a corporate organization as well as publicly available data for comparison. Data is processed by removing the stop-words and stemming is implemented to produce root words. Multiple traditional machine learning techniques like Naive Bayes, Logistic Regression, Support Vector Machine, Random forest Classifier and Multi-Layer Perceptron are used for classification of documents. Classifiers are applied on raw and processed data separately and their accuracy is noted. Along with this, Deep learning technique such as Convolution Neural Network is also used to classify the data and its accuracy is compared with that of traditional machine learning techniques. We are also exploring hierarchical classifiers for classification of classes and subclasses. The system classifies the data faster and with better accuracy than if done manually. The results are discussed in the results and evaluation section.",
        "link": "https://dl.acm.org/doi/10.1145/3209280.3209526"
    },
    {
        "title": "Identification of Overpricing in the Purchase of Medication by the Federal Government of Brazil, Using Text Mining and Clustering Based on Ontology",
        "authors": [
            "Marco Aurelio O. S. Correa",
            "Adriano Galindo Leal"
        ],
        "date": "August 2018",
        "source": "ICCBDC '18: Proceedings of the 2018 2nd International Conference on Cloud and Big Data Computing",
        "abstract": "Increasing the transparency level in his actions and spending is one of the primary duties of the Brazilian Federal Government. The creation of laws that oblige full disclose of all its expenditures through transparency portals enables citizens to supervise all government entities. However, only the dissemination of these data, without a definite standard or the availability of data analysis tools, does not guarantee that the citizen is empowered to play his role. Therefore hence, the objective of this work is to identify overprice in the acquisition of products purchased by the federal government of Brazil using the unstructured data available on the Transparency Portal. The last two-years' worth of purchasing data, available in the Transparency Portal, were extracted, processed and stored. Due to his diverse nature and high volume of data, this study focused only on medicines purchased by the Ministry of Health. Ontology-based text mining and clustering techniques were applied for automatic identification and classification of products. The processing of this information was done through text mining and clustering, based on the ontology registered in another database of the Brazilian government. Because of this work, a consolidated price base per medication was created to allow the identification of distortions in prices practised, facilitating the identification of cases that merit further investigation to unravel fraud to the treasury.",
        "link": "https://dl.acm.org/doi/10.1145/3264560.3264569"
    },
    {
        "title": "Computational Estimation by Scientific Data Mining with Classical Methods to Automate Learning Strategies of Scientists",
        "authors": [
            "Aparna S. Varde"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Experimental results are often plotted as 2-dimensional graphical plots (aka graphs) in scientific domains depicting dependent versus independent variables to aid visual analysis of processes. Repeatedly performing laboratory experiments consumes significant time and resources, motivating the need for computational estimation. The goals are to estimate the graph obtained in an experiment given its input conditions, and to estimate the conditions that would lead to a desired graph. Existing estimation approaches often do not meet accuracy and efficiency needs of targeted applications. We develop a computational estimation approach called AutoDomainMine that integrates clustering and classification over complex scientific data in a framework so as to automate classical learning methods of scientists. Knowledge discovered thereby from a database of existing experiments serves as the basis for estimation. Challenges include preserving domain semantics in clustering, finding matching strategies in classification, striking a good balance between elaboration and conciseness while displaying estimation results based on needs of targeted users, and deriving objective measures to capture subjective user interests. These and other challenges are addressed in this work. The AutoDomainMine approach is used to build a computational estimation system, rigorously evaluated with real data in Materials Science. Our evaluation confirms that AutoDomainMine provides desired accuracy and efficiency in computational estimation. It is extendable to other science and engineering domains as proved by adaptation of its sub-processes within fields such as Bioinformatics and Nanotechnology.",
        "link": "https://dl.acm.org/doi/10.1145/3502736"
    },
    {
        "title": "Where is the road for issue reports classification based on text mining?",
        "authors": [
            "Qiang Fan",
            "Yue Yu",
            "Gang Yin",
            "Tao Wang",
            "Huaimin Wang"
        ],
        "date": "November 2017",
        "source": "ESEM '17: Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Currently, open source projects receive various kinds of issues daily, because of the extreme openness of Issue Tracking System (ITS) in GitHub. ITS is a labor-intensive and time-consuming task of issue categorization for project managers. However, a contributor is only required a short textual abstract to report an issue in GitHub. Thus, most traditional classification approaches based on detailed and structured data (e.g., priority, severity, software version and so on) are difficult to adopt. In this paper, issue classification approaches on a large-scale dataset, including 80 popular projects and over 252,000 issue reports collected from GitHub, were investigated. First, four traditional text-based classification methods and their performances were discussed. Semantic perplexity (i.e., an issues description confuses bug-related sentences with nonbug-related sentences) is a crucial factor that affects the classification performances based on quantitative and qualitative study. Finally, A two-stage classifier framework based on the novel metrics of semantic perplexity of issue reports was designed. Results show that our two-stage classification can significantly improve issue classification performances.",
        "link": "https://dl.acm.org/doi/10.1109/ESEM.2017.19"
    },
    {
        "title": "Feature-based Facebook reviews process model for e-management using data mining",
        "authors": [
            "Anish Kumar Varudharajulu",
            "Yongsheng Ma"
        ],
        "date": "January 2019",
        "source": "IC4E '19: Proceedings of the 10th International Conference on E-Education, E-Business, E-Management and E-Learning",
        "abstract": "The data generated from online communication acts as potential gold mines for discovering knowledge for researchers. A large amount of data is also generated in the form of web documents, emails, blogs, and feedback, etc. Text analytics is being significantly employed to mine important information. Opinion mining is the process of extracting human thoughts and perceptions from unstructured texts. The showstopper for designing an opinion mining system for analyzing reviews arise from the fact that customer reviews are often noisy. These reviews are informally written. In addition, they are subjected to spelling mistakes, grammatical errors, improper punctuation and irrational capitalization. This paper focuses on analyzing the different classification and clustering algorithms aimed at extracting and consolidating opinions of customers from social media sites like Facebook, Twitter and through surveys, at multiple levels of granularity to monitor and measure customer satisfaction. Ours is an automated approach, in which the system aids in the process of knowledge assimilation for knowledge-based building and also performs the analytics. Domain experts ratify the knowledge base and also provide training datasets for the system to intuitively gather more instances for ratification. The system identifies opinion expressions as phrases containing opinion words, opinionated features and also opinion modifiers. These expressions are categorized as positive, negative or neutral. Opinion expressions are identified and categorized using localized linguistic techniques. Opinions can be congregated at any desired level of specificity i.e. feature level or product level, user level or service level, etc. We have developed a system based on this approach, which provides the user with a platform to analyze opinion expressions crawled from a set of pre-defined datasets.",
        "link": "https://dl.acm.org/doi/10.1145/3306500.3306514"
    },
    {
        "title": "Discriminative Topic Mining via Category-Name Guided Text Embedding",
        "authors": [
            "Yu Meng",
            "Jiaxin Huang",
            "Guangyuan Wang",
            "Zihan Wang",
            "Chao Zhang",
            "Yu Zhang",
            "Jiawei Han"
        ],
        "date": "April 2020",
        "source": "WWW '20: Proceedings of The Web Conference 2020",
        "abstract": "Mining a set of meaningful and distinctive topics automatically from massive text corpora has broad applications. Existing topic models, however, typically work in a purely unsupervised way, which often generate topics that do not fit users’ particular needs and yield suboptimal performance on downstream tasks. We propose a new task, discriminative topic mining, which leverages a set of user-provided category names to mine discriminative topics from text corpora. This new task not only helps a user understand clearly and distinctively the topics he/she is most interested in, but also benefits directly keyword-driven classification tasks. We develop CatE, a novel category-name guided text embedding method for discriminative topic mining, which effectively leverages minimal user guidance to learn a discriminative embedding space and discover category representative terms in an iterative manner. We conduct a comprehensive set of experiments to show that CatE mines high-quality set of topics guided by category names only, and benefits a variety of downstream applications including weakly-supervised classification and lexical entailment direction identification.",
        "link": "https://dl.acm.org/doi/10.1145/3366423.3380278"
    },
    {
        "title": "Comparative Study of Heart Disease Diagnosis Using Top Ten Data Mining Classification Algorithms",
        "authors": [
            "I. Ketut Agung Enriko"
        ],
        "date": "June 2019",
        "source": "ICFET '19: Proceedings of the 5th International Conference on Frontiers of Educational Technologies",
        "abstract": "Data mining has been used for many purposes, especially for prediction system. In healthcare, data mining algorithms often used in disease diagnosis. Meanwhile, heart disease is known as a primary cause of death over the years. Many studies have been performed in heart disease diagnosis using data mining methods. There are some popular data mining algorithms that can be used in heart disease diagnosis, for example, k-Nearest Neighbor, CART, and AdaBoost. The algorithms are used to analyze a sample of cardiovascular patients data and predict the heart disease type that they suffer. Some parameters are taken from the patient, including EKG morphology, blood pressure, and information about the existence of chest pain, shortness of breath, palpitation, and cold sweat. In this study, medical records data are collected from Harapan Kita Hospital and utilized as a dataset sample in this research. Top ten data mining classification algorithms are used in diagnosing heart disease from Harapan Kita Hospital data and examining their performance by checking the accuracy and speed.",
        "link": "https://dl.acm.org/doi/10.1145/3338188.3338220"
    },
    {
        "title": "Text Mining Approach for Identifying Research Trends",
        "authors": [
            "Snezhana Sulova"
        ],
        "date": "June 2021",
        "source": "CompSysTech '21: Proceedings of the 22nd International Conference on Computer Systems and Technologies",
        "abstract": "With the increase of unstructured data, the issues connected with automatic text processing, the categorization of documents and the discovery of topics have become objects of growing interest. In order to improve the process of grouping and processing research publications, we would like to propose a method based upon natural language processing. It is based on text mining technologies which aim to identify key tendencies in documents. It processes the content of publications by clustering and identifies the topics of each identified group. This analysis helps by identifying key tendencies as well as discovering emerging new areas of research. Publications from the research literature database, Scopus, were used to test the approach. The topic of the publications is “the application of digital technologies in the logistics business”. The experiments were completed using the RapidMiner Studio software.",
        "link": "https://dl.acm.org/doi/10.1145/3472410.3472433"
    },
    {
        "title": "Assessment of Vulnerability Severity using Text Mining",
        "authors": [
            "Georgios Spanos",
            "Lefteris Angelis",
            "Dimitrios Toloudis"
        ],
        "date": "September 2017",
        "source": "PCI '17: Proceedings of the 21st Pan-Hellenic Conference on Informatics",
        "abstract": "Software1 vulnerabilities are closely associated with information systems security, a major and critical field in today's technology. Vulnerabilities constitute a constant and increasing threat for various aspects of everyday life, especially for safety and economy, since the social impact from the problems that they cause is complicated and often unpredictable. Although there is an entire research branch in software engineering that deals with the identification and elimination of vulnerabilities, the growing complexity of software products and the variability of software production procedures are factors contributing to the ongoing occurrence of vulnerabilities, Hence, another area that is being developed in parallel focuses on the study and management of the vulnerabilities that have already been reported and registered in databases. The information contained in such databases includes, a textual description and a number of metrics related to vulnerabilities. The purpose of this paper is to investigate to what extend the assessment of the vulnerability severity can be inferred directly from the corresponding textual description, or in other words, to examine the informative power of the description with respect to the vulnerability severity. For this purpose, text mining techniques, i.e. text analysis and three different classification methods (decision trees, neural networks and support vector machines) were employed. The application of text mining to a sample of 70,678 vulnerabilities from a public data source shows that the description itself is a reliable and highly accurate source of information for vulnerability prioritization.",
        "link": "https://dl.acm.org/doi/10.1145/3139367.3139390"
    },
    {
        "title": "Clustering Algorithms for Spatial Data Mining",
        "authors": [
            "Chetashri Bhadane",
            "Ketan Shah"
        ],
        "date": "April 2020",
        "source": "ICGDA '20: Proceedings of the 2020 3rd International Conference on Geoinformatics and Data Analysis",
        "abstract": "With the advances in mobile and wireless technologies, there has been a rise in applications that track and share the users' geospatial data. People use several social networking sites such as Twitter, Facebook and Flickr, where they share their status updates. With the integration of Global Positioning System (GPS) with mobile phones, it is now possible to share one's locations on these social networks. GPS allows us to record and track a person's movement along with the timestamp. The data set obtained from these GPS logs is vast and is widely used to analyze the users' movement patterns. Specifically, we can find out significant locations based on the number of users present at that location and the time spent by them at such places. Once significant places have been identified, it is also possible to identify the semantic importance of these locations. This paper presents an overview of the clustering techniques used to find important places of interest using large GPS based mobility datasets. Four clustering algorithms, K-Means, DBSCAN, OPTICS and Hierarchical, are implemented, and performance is tested using real-time data of 50 users collected over 2--5 years. Performance summary depicts that K-Means and DBSCAN perform well for spatial data.",
        "link": "https://dl.acm.org/doi/10.1145/3397056.3397068"
    },
    {
        "title": "Feature Selection based Arabic Text Classification using Different Machine Learning Algorithms: Comparative Study",
        "authors": [
            "Sakina Rim Bennabi",
            "Zakaria Elberrichi"
        ],
        "date": "June 2020",
        "source": "ICIST '20: Proceedings of the 10th International Conference on Information Systems and Technologies",
        "abstract": "Feature selection is a method of data pre-processing widely used when mining large data, such as textual classification. Several studies have been conducted to compare the different methods of feature selection applied to corpora in English. Unfortunately, a small number of works concern the Arabic language. This article aims to present a comparative study of different feature selection techniques including: Chi2, the ANOVA method and mutual information, applied on a corpus in Arabic language, while also diversifying the machine learning algorithms (Naive Bayes, SVM and KNN). This experimental study has shown in general that reducing dimensionality with feature selection techniques has slightly affected the performance of textual classification, reducing the size of the corpus by up to 1%.",
        "link": "https://dl.acm.org/doi/10.1145/3447568.3448531"
    },
    {
        "title": "Intelligent Data Analysis using Optimized Support Vector Machine Based Data Mining Approach for Tourism Industry",
        "authors": [
            "Ms Promila Sharma",
            "Uma Meena",
            "Girish Kumar Sharma"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Data analysis involves the deployment of sophisticated approaches from data mining methods, information theory, and artificial intelligence in various fields like tourism, hospitality, and so on for the extraction of knowledge from the gathered and preprocessed data. In tourism, pattern analysis or data analysis using classification is significant for finding the patterns that represent new and potentially useful information or knowledge about the destination and other data. Several data mining techniques are introduced for the classification of data or patterns. However, overfitting, less accuracy, local minima, sensitive to noise are the drawbacks in some existing data mining classification methods. To overcome these challenges, Support vector machine with Red deer optimization (SVM-RDO) based data mining strategy is proposed in this article. Extended Kalman filter (EKF) is utilized in the first phase, i.e., data cleaning to remove the noise and missing values from the input data. Mantaray foraging algorithm (MaFA) is used in the data selection phase, in which the significant data are selected for the further process to reduce the computational complexity. The final phase is the classification, in which SVM-RDO is proposed to access the useful pattern from the selected data. PYTHON is the implementation tool used for the experiment of the proposed model. The experimental analysis is done to show the efficacy of the proposed work. From the experimental results, the proposed SVM-RDO achieved better accuracy, precision, recall, and F1 score than the existing methods for the tourism dataset. Thus, it is showed the effectiveness of the proposed SVM-RDO for pattern analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3494566"
    },
    {
        "title": "Applying text mining to predict learners' cognitive engagement",
        "authors": [
            "Hayati Hind",
            "Mohammed Khalidi Idrissi",
            "Samir Bennani"
        ],
        "date": "October 2017",
        "source": "SCAMS '17: Proceedings of the Mediterranean Symposium on Smart City Application",
        "abstract": "In academic process, engagement represent one of the important success' factor. This paper proposes a predictive system for learners' cognitive engagement based on their online discussion forums participation. In the first level the ontology OWL and the LSA method are used to perform a semantic classification system of the threads according to a specific context chosen by the tutor. Then the Text mining, as a predictive method, is applied to the classified threads and learners' participation in the forums.",
        "link": "https://dl.acm.org/doi/10.1145/3175628.3175655"
    },
    {
        "title": "Investigate Transitions into Drug Addiction through Text Mining of Reddit Data",
        "authors": [
            "John Lu",
            "Sumati Sridhar",
            "Ritika Pandey",
            "Mohammad Al Hasan",
            "Georege Mohler"
        ],
        "date": "July 2019",
        "source": "KDD '19: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Increasing rates of opioid drug abuse and heightened prevalence of online support communities underscore the necessity of employing data mining techniques to better understand drug addiction using these rapidly developing online resources. In this work, we obtained data from Reddit, an online collection of forums, to gather insight into drug use/misuse using text snippets from users narratives. Specifically, using users' posts, we trained a binary classifier which predicts a user's transitions from casual drug discussion forums to drug recovery forums. We also proposed a Cox regression model that outputs likelihoods of such transitions. In doing so, we found that utterances of select drugs and certain linguistic features contained in one's posts can help predict these transitions. Using unfiltered drug-related posts, our research delineates drugs that are associated with higher rates of transitions from recreational drug discussion to support/recovery discussion, offers insight into modern drug culture, and provides tools with potential applications in combating the opioid crisis.",
        "link": "https://dl.acm.org/doi/10.1145/3292500.3330737"
    },
    {
        "title": "Analysing digital banking reviews using text mining",
        "authors": [
            "Li Chen Cheng",
            "Legaspi Rhea Sharmayne"
        ],
        "date": "December 2020",
        "source": "ASONAM '20: Proceedings of the 12th IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Digital banks are new entrants in the banking industry in the Philippines as they only started late 2018. Since then, a handful of players have and are still emerging. With more and more people becoming technologically savvy, it is very critical for financial institutions to develop a digital banking application that will stand out from the competition. This paper aims to use text mining methods to analyse digital banking application reviews. This study will perform topic modelling using LDA to explore customer concerns and will mine association rules between the digital banking features with the review score. The results will reveal which areas the digital banking application can further optimize for customer satisfaction and retention.",
        "link": "https://dl.acm.org/doi/10.1109/ASONAM49781.2020.9381429"
    },
    {
        "title": "Mining Moocs Videos Metadata Using Classification Techniques",
        "authors": [
            "El Harrak Othman",
            "Ghadi Abderrahim",
            "El Bouhdidi Jaber"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "In few years, the internet has become the first source of information for most people, Today MOOCs makes it possible for everyone to access to the education over the world, it's represents an emerging methodology of online teaching and an important development in open education. Due to the rapid development of Moocs and the rapid growth of digital data and video database over the Internet, it is becoming very difficult for learners to browsing and choosing the best training for them. The scientific community has increased the amount of research into new technologies, with a view to improve the digital video utilization: its archiving, indexing, accessibility, acquisition, store and even its process and usability. All these parts of the video utilization entail the necessity of the extraction of all important information of a video, especially in cases of lack of metadata information. Web video mining is retrieving the content using data mining techniques from World Wide Web. There are two approaches for web video mining using traditional image processing (signal processing) and metadata based approach. In this paper, we present the various research makes in this subject and we propose an effective methodology to extract the metadata from Moocs videos and classify them based on the extracted metadata by applying data mining techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090450"
    },
    {
        "title": "Text mining with HathiTrust",
        "authors": [
            "Eleanor Dickson Koehl",
            "Ryan Dubnicek"
        ],
        "date": "June 2019",
        "source": "JCDL '19: Proceedings of the 18th Joint Conference on Digital Libraries",
        "abstract": "This tutorial will introduce attendees to the HathiTrust Research Center and its tools and services for computational text analysis research. HTRC leverages the scope and scale of the HathiTrust Digital Library collection to create opportunities for researchers to perform text data mining on subsets of the corpus. Attendees will develop skills that will allow them to conduct basic text analysis research using HathiTrust data, learn techniques for engaging with scholars on their text data mining research, and explore strategies for computational access to large-scale digital collections.",
        "link": "https://dl.acm.org/doi/10.1109/JCDL.2019.00115"
    },
    {
        "title": "Text mining for plagiarism detection: multivariate pattern detection for recognition of text similarities",
        "authors": [
            "Konstantinos Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2018",
        "source": "ASONAM '18: Proceedings of the 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "The problem of plagiarism the recent years has been intensified by the availability of information in digital form and the accessibility of the electronic libraries through the Internet. As a result, plagiarism detection has been transformed into a big data analytics problem since the number of digital sources is extravagant and a new document needs to be compared with millions of other existing documents. In this paper, a text mining methodology is proposed that can detect all common patterns between a document and the documents in a reference database. The technique is based on a pattern detection algorithm and the corresponding data structure that enables the algorithm to detect all common patterns. The methodology has been applied in a well-defined dataset providing very promising results identifying difficult cases of plagiarism such as technical disguise.",
        "link": "https://dl.acm.org/doi/10.5555/3382225.3382424"
    },
    {
        "title": "Petroleum Engineering Data Text Classification Using Convolutional Neural Network Based Classifier",
        "authors": [
            "Tzuhan Hsu",
            "Yaoqin Zhang"
        ],
        "date": "May 2018",
        "source": "ICMLT '18: Proceedings of the 2018 International Conference on Machine Learning Technologies",
        "abstract": "In this paper, authors introduce a convolutional neural network (CNN) based classification approach for petroleum engineering event. Recently, text classification problem has raised substantial attention in the field of machine learning. The aim of text classification process is to assign correct label to a text on the basis of its content. However, due to the imprecise of feature representation, most of former text classification methods cannot export satisfying result. To solve this problem, authors propose a model based on convolutional neural network. Authors conduct experiments with real world petroleum engineering data text, artificially labelled into 6 categories. Beside purposed CNN-based model, the authors also implement other baseline machine learning approaches: k nearest neighbor, random forest, support vector machines, recurrent neural network, gated recurrent units, long short-term memory. Among all these models, our CNN-based model reach best performance in text classification task. In the last part of this paper, authors compare and analysis their purpose and baseline model.",
        "link": "https://dl.acm.org/doi/10.1145/3231884.3231898"
    },
    {
        "title": "Classifying Indonesian Online Articles as Advertisement Placement Base Using Text Mining",
        "authors": [
            "Nadhira Tasya",
            "Arian Dhini"
        ],
        "date": "July 2017",
        "source": "ICBIM 2017: Proceedings of the International Conference on Business and Information Management",
        "abstract": "Rapid development in technological aspect resulting in growing level of human needs for the latest news, so that emerged a new trend of publishing and accessing news through online media or usually called online journalism. In addition, the number of people who sell and purchase through online sites also continues to increase and this opportunity is utilized by the company and the advertiser by implementing targeted web advertising. However, the high number of articles that have been published and accessed leads to great opportunities for errors in determining where to place the ads. Therefore, it needs a system that can categorize articles accessed by users as the basis of advertisement placement by the company and this classification system can be done by applying the method of Data Mining and Text Mining. This research uses document data in the form of article content that will be categorized into twenty categories of class of advertisement by using Text Mining technique with Support Vector Machine algorithm. The results of this study may be used by companies or advertisers as a basis for placement of ads on selected online media sites.",
        "link": "https://dl.acm.org/doi/10.1145/3134271.3134288"
    },
    {
        "title": "Mining Free-Text Medical Notes for Suicide Risk Assessment",
        "authors": [
            "Marios Adamou",
            "Grigoris Antoniou",
            "Elissavet Greasidou",
            "Vincenzo Lagani",
            "Paulos Charonyktakis",
            "Ioannis Tsamardinos"
        ],
        "date": "July 2018",
        "source": "SETN '18: Proceedings of the 10th Hellenic Conference on Artificial Intelligence",
        "abstract": "Suicide has been considered as an important public health issue for a very long time, and is one of the main causes of death worldwide. Despite suicide prevention strategies being applied, the rate of suicide has not changed substantially over the past decades. Advances in machine learning make it possible to attempt to predict suicide based on the analysis of relevant data to inform clinical practice. This paper reports on findings from the analysis of data of patients who died by suicide in the period 2013-2016 and made use of both structured data and free-text medical notes. We focus on examining various text-mining approaches to support risk assessment. The results show that using advance machine learning and text-mining techniques, it is possible to predict within a specified period which people are most at risk of taking their own life at the time of referral to a mental health service.",
        "link": "https://dl.acm.org/doi/10.1145/3200947.3201020"
    },
    {
        "title": "Integration of data mining techniques in e-learning systems: Clustering Profil of Lerners and Recommender Course System",
        "authors": [
            "A. El Moustamid",
            "E. En-Naimi",
            "J. El Bouhdidi"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "Data mining aims at the extraction of knowledge from large quantities of data by automatic or semi-automatic methods. In the field of education and training e-learning, Data Mining techniques as clustering (The aim of this technique is to identify groups that are similar in some aspect) [21], prediction (is used for developing model where combination of data viewed from different aspects can effect single viewed data. Here, is important to watch how predictors variables are effecting predicted variable)[21], classification are applied in e-learning to improve the level of learners by providing learning according to the learners' requirement) are used to analyze learners profile, to predict and improve student performance. The aim of our project is to develop a system capable of analyzing learners profile and indexing web videos in order to offer learners a rich database with courses that correspond to their levels.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090453"
    },
    {
        "title": "Multi-level mining and visualization of scientific text collections: Exploring a bi-lingual scientific repository",
        "authors": [
            "Pablo Accuosto",
            "Francesco Ronzano",
            "Daniel Ferrés",
            "Horacio Saggion"
        ],
        "date": "December 2017",
        "source": "WOSP 2017: Proceedings of the 6th International Workshop on Mining Scientific Publications",
        "abstract": "We present a system to mine and visualize collections of scientific documents by semantically browsing information extracted from single publications or aggregated throughout corpora of articles. The text mining tool performs deep analysis of document collections allowing the extraction and interpretation of research paper's contents. In addition to the extraction and enrichment of documents with metadata (titles, authors, affiliations, etc), the deep analysis performed comprises semantic interpretation, rhetorical analysis of sentences, triple-based information extraction, and text summarization. The visualization components allow geographical-based exploration of collections, topic-evolution interpretation, and collaborative network analysis among others. The paper presents a case study of a bi-lingual collection in the field of Natural Language Processing (NLP).",
        "link": "https://dl.acm.org/doi/10.1145/3127526.3127529"
    },
    {
        "title": "Hierarchical Topic Mining via Joint Spherical Tree and Text Embedding",
        "authors": [
            "Yu Meng",
            "Yunyi Zhang",
            "Jiaxin Huang",
            "Yu Zhang",
            "Chao Zhang",
            "Jiawei Han"
        ],
        "date": "August 2020",
        "source": "KDD '20: Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Mining a set of meaningful topics organized into a hierarchy is intuitively appealing since topic correlations are ubiquitous in massive text corpora. To account for potential hierarchical topic structures, hierarchical topic models generalize flat topic models by incorporating latent topic hierarchies into their generative modeling process. However, due to their purely unsupervised nature, the learned topic hierarchy often deviates from users' particular needs or interests. To guide the hierarchical topic discovery process with minimal user supervision, we propose a new task, Hierarchical Topic Mining, which takes a category tree described by category names only, and aims to mine a set of representative terms for each category from a text corpus to help a user comprehend his/her interested topics. We develop a novel joint tree and text embedding method along with a principled optimization procedure that allows simultaneous modeling of the category tree structure and the corpus generative process in the spherical space for effective category-representative term discovery. Our comprehensive experiments show that our model, named JoSH, mines a high-quality set of hierarchical topics with high efficiency and benefits weakly-supervised hierarchical text classification tasks.",
        "link": "https://dl.acm.org/doi/10.1145/3394486.3403242"
    },
    {
        "title": "Unsupervised behavioural mining and clustering for malware family identification",
        "authors": [
            "Khanh Huu The Dam",
            "Thomas Given-Wilson",
            "Axel Legay"
        ],
        "date": "March 2021",
        "source": "SAC '21: Proceedings of the 36th Annual ACM Symposium on Applied Computing",
        "abstract": "More accurate and advanced detection and classification of malware requires exploiting program behaviour and not merely syntactic or static features. One approach is to use system call dependency graphs (SCDGs) that represent the program behaviour by interactions with the system, and the relations between these interactions. These SCDGs have been used with supervised learning techniques to very accurately detect and classify malware. This works considers the unsupervised learning challenge of mining for common clusters of behaviour without a priori knowledge. This allows for clustering of similar programs by behaviour, that can then be used for either classification or further analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3412841.3441919"
    },
    {
        "title": "Text Mining for Evaluating Authors' Birth and Death Years",
        "authors": [
            "Dror Moghaz",
            "Yaakov Hacohen-Kerner",
            "Dov Gabbay"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "This article presents a unique method in text and data mining for finding the era, i.e., mining temporal data, in which an anonymous author was living. Finding this era can assist in the examination of a fake document or extracting the time period in which a writer lived. The study and the experiments concern Hebrew, and in some parts, Aramaic and Yiddish rabbinic texts. The rabbinic texts are undated and contain no bibliographic sections, posing an interesting challenge. This work proposes algorithms using key phrases and key words that allow the temporal organization of citations together with linguistic patterns. Based on these key phrases, key words, and the references, we established several types of “Iron-clad,” Heuristic and Greedy rules for estimating the years of birth and death of a writer in an interesting classification task. Experiments were conducted on corpora, including documents authored by 12, 24, and 36 rabbinic writers and demonstrated promising results.",
        "link": "https://dl.acm.org/doi/10.1145/3281631"
    },
    {
        "title": "Transfer Learning to Timed Text Based Video Classification Using CNN",
        "authors": [
            "Zenun Kastrati",
            "Ali Shariq Imran",
            "Arianit Kurti"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "Open educational video resources are gaining popularity with a growing number of massive open online courses (MOOCs). This has created a niche for content providers to adopt effective solutions in automatically organizing and structuring of educational resources for maximum visibility. Recent advances in deep learning techniques are proving useful in managing and classifying resources into appropriate categories. This paper proposes one such convolutional neural network (CNN) model for classifying video lectures in a MOOC setting using a transfer learning approach. The model uses a time-aligned text transcripts corresponding to video lectures from six broader subject categories. Video lectures and their corresponding transcript dataset is gathered from the Coursera MOOC platform. Two different CNN models are proposed: i) CNN based classification using embeddings learned from our MOOC dataset, ii) CNN based classification using transfer learning. Word embeddings generated from two well known state-of-the-art pre-trained models Word2Vec and GloVe, are used in the transfer learning approach for the second case. The proposed CNN models are evaluated using precision, recall, and F1 score and the obtained performance is compared with both conventional and deep learning classifiers. The proposed CNN models have an F1 score improvement of 10-22 percentage points over DNN and conventional classifiers",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326483"
    },
    {
        "title": "Evolutionary role mining in complex networks by ensemble clustering",
        "authors": [
            "Sarvenaz Choobdar",
            "Pedro Ribeiro",
            "Fernando Silva"
        ],
        "date": "April 2017",
        "source": "SAC '17: Proceedings of the Symposium on Applied Computing",
        "abstract": "The structural patterns in the neighborhood of nodes assign unique roles to the nodes. Mining the set of existing roles in a network provides a descriptive profile of the network and draws its general picture. This paper proposes a new method to determine structural roles in a dynamic network based on the current position of nodes and their historic behavior. We develop a temporal ensemble clustering technique to dynamically find groups of nodes, holding similar tempo-structural roles. We compare two weighting functions, based on age and distribution of data, to incorporate temporal behavior of nodes in the role discovery. To evaluate the performance of the proposed method, we assess the results from two points of view: 1) goodness of fit to current structure of the network; 2) consistency with historic data. We conduct the evaluation using different ensemble clustering techniques. The results on real world networks demonstrate that our method can detect tempo-structural roles that simultaneously depict the topology of a network and reflect its dynamics with high accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3019612.3019815"
    },
    {
        "title": "Analyzing used-car web listings via text mining",
        "authors": [
            "Ayhan Demiriz",
            "Fatma Cantaş"
        ],
        "date": "October 2017",
        "source": "IML '17: Proceedings of the 1st International Conference on Internet of Things and Machine Learning",
        "abstract": "Used car trade is one of the major components of the world economies. It is not uncommon to sell a car by placing an internet advertisement irrespective of the geography in these days. A typical content of an advertisement is usually composed of two parts namely the structured and the free text data. The structured data may include some information about the asking price, make, model, year, mileage of the car and the contact info. In most cases, seller may give important clues about the car's current conditions in the free text data where the title (head) of the advertisement can be included as free text too. This paper reports preliminary results from a text mining study conducted on 75K used car internet listings collected from two major car listing web sites in Turkey. As expected, the words and the phrases related to the description of the car are observed to be frequent. The leading concepts in the free text are found to be regarding how to describe the current condition of a car, for example \"no crash history\".",
        "link": "https://dl.acm.org/doi/10.1145/3109761.3109782"
    },
    {
        "title": "Text classification of Diseases Treated by Traditional Chinese Medicine Prescription based on machine learning",
        "authors": [
            "Hanqing Zhao",
            "Bo Han",
            "Chen Li"
        ],
        "date": "September 2020",
        "source": "ISAIMS '20: Proceedings of the 1st International Symposium on Artificial Intelligence in Medical Sciences",
        "abstract": "OBJECTIVE: To explore the application of machine learning in th e identification of diseases treated by traditional Chinese medicine prescriptions. METHODS: Based on the composition of the text document of Chinese medicine prescriptions, the prescriptions were divided into cough, headache and diarrhea. THUTCT were introduced to establish and train two machine learning text classification models, LibSVM and LibLinear, and the prescriptions to be deter mined were put into the model for classification and prediction. R ESULTS: The Precision rate of LibSVM model and Liblinear mod el were 0.7283 and 0.6690. Seven prescriptions were classified an d predicted, and the results were in line with expectations. CONCLUSION: THUCTC has good universality for the content of TCM prescriptions, high classification accuracy and fast testing speed, which is suitable for the text classification and discrimination research of TCM prescriptions for diseases.",
        "link": "https://dl.acm.org/doi/10.1145/3429889.3429895"
    },
    {
        "title": "Evaluation of retweet clustering method classification method using retweets on Twitter without text data",
        "authors": [
            "K. Uchida",
            "F. Toriumi",
            "T. Sakaki"
        ],
        "date": "August 2017",
        "source": "WI '17: Proceedings of the International Conference on Web Intelligence",
        "abstract": "Burst phenomena, which frequently occur on social media, are caused by such social events as flaming on the internet, elections, and natural disasters. To understand people's thoughts and feelings, we must classify their opinions from burst phenomena. Therefore, classification methods that categorize tweets are critical. However, since most classification methods focus on text mining, they cannot group tweets by topics because each tweet has poor linguistic similarities. We used a non-text-based classification method proposed by Baba et al. that groups tweets by topics, even if they have poor linguistic similarities, and verified its validity by comparing it with a text-based classification method in two different evaluations: qualitative and quantitative. In the qualitative evaluation part, we did a questionnaire survey and validated the suitability of the topic clusters created using both the non-and text-based methods. Since evaluating the similarity of every pair of tweets in each topic is difficult, we evaluated the similarity between sampled pairs in the survey and acquired more appropriate topic clustering results using the non-text-based method than the text-based method. In the quantitative evaluation part, we focused on the robustness of each method against data reduction. Many approaches analyze social media data, especially because collecting data from social media is comparatively easy. However, since collecting the whole data of burst phenomena is very costly due to the vast amounts of available social media data, robustness against data reduction is an important index to evaluate classification methods. With the non-text-based method, over 55% of the pairs of tweets in the same cluster were also included in the same cluster even when the data were reduced to 10% in all three of our example cases. In this paper, as a source we focus on Twitter, one of the most popular microblogging services. Using clustering to conduct detailed case analyses, we scrutinized three burst cases that include natural disasters and flaming on the internet and found that a non-text-based method more effectively classified tweets in burst phenomena than a text-based method.",
        "link": "https://dl.acm.org/doi/10.1145/3106426.3106451"
    },
    {
        "title": "Using Spark for Text Mining on Large Scale Liver Cancer Literature",
        "authors": [
            "Ming-Yen Lin",
            "Yu-Ju Lin",
            "Sue-Chen Hsueh"
        ],
        "date": "January 2021",
        "source": "BDET 2021: 2021 the 3rd International Conference on Big Data Engineering and Technology (BDET)",
        "abstract": "Cancer is one of the main causes of death. The number of known cancer to-date is more than one hundred. Liver cancer, ranked after trachea and lung cancer, has long been high in the cancer leading cause in Taiwan and even ranked as second in 2016. The number of scientific articles related to cancer proliferates every year. The number reaches as high as 20 million in PubMed so that discovering useful information from the massive collection is very difficult. In addition, using a single machine to sift through these articles is very time-consuming. Therefore, we present a big data analytic framework using the distributed Apache Spark platform for text mining in PubMed literature. We establish a prediction model for liver cancer articles so that researchers may effectively validate whether an article is related to liver cancer or not. Classification models in Spark MLlib including Linear Support Vector Machines (SVM), Logistic Regression, are used in our experiments. Relevancy to liver cancer is further confirmed by using MeSH (Medical Subject Headings) terms. Logistic regression is about 3 times faster than SVMs and the accuracy of both methods is close to 95% in the experiments using hold-out validation. When max_features is 500 and min_df ≤ 0.1 (or min_df = 1), the accuracy may reach 96%. In the experiments with K-fold cross-validation, the accuracy of SVMs methods is 96%. The experimental results show that that our prediction model may effectively classify liver cancer articles.",
        "link": "https://dl.acm.org/doi/10.1145/3474944.3474958"
    },
    {
        "title": "Automatic topic classification of test cases using text mining at an Android smartphone vendor",
        "authors": [
            "Junji Shimagaki",
            "Yasutaka Kamei",
            "Naoyasu Ubayashi",
            "Abram Hindle"
        ],
        "date": "October 2018",
        "source": "ESEM '18: Proceedings of the 12th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Background: An Android smartphone is an ecosystem of applications, drivers, operating system components, and assets. The volume of the software is large and the number of test cases needed to cover the functionality of an Android system is substantial. Enormous effort has been already taken to properly quantify \"what features and apps were tested and verified?\". This insight is provided by dashboards that summarize test coverage and results per feature. One method to achieve this is to manually tag or label test cases with the topic or function they cover, much like function points. At the studied Android smartphone vendor, tests are labelled with manually defined tags, so-called \"feature labels (FLs)\", and the FLs serve to categorize 100s to 1000s test cases into 10 to 50 groups. Aim: Unfortunately for developers, manual assignment of FLs to 1000s of test cases is a time consuming task, leading to inaccurately labeled test cases, which will render the dashboard useless. We created an automated system that suggests tags/labels to the developers for their test cases rather than manual labeling. Method: We use machine learning models to predict and label the functionality tested by 10,000 test cases developed at the company. Results: Through the quantitative experiments, our models achieved acceptable F-1 performance of 0.3 to 0.88. Also through the qualitative studies with expert teams, we showed that the hierarchy and path of tests was a good predictor of a feature's label. Conclusions: We find that this method can reduce tedious manual effort that software developers spent classifying test cases, while providing more accurate classification results.",
        "link": "https://dl.acm.org/doi/10.1145/3239235.3268927"
    },
    {
        "title": "To apply Data Mining for Classification of Crowd sourced Software Requirements",
        "authors": [
            "Soonh Taj",
            "Qasim Arain",
            "Imran Memon",
            "Asma Zubedi"
        ],
        "date": "April 2019",
        "source": "ICSIE '19: Proceedings of the 8th International Conference on Software and Information Engineering",
        "abstract": "Now a day's main focus of developers is to build quality software that works according to customer needs and for this reason it is necessary to gather right requirements as requirement elicitation is the critical step that impacts on the success of software project as misinterpreted requirements leads to the failure of software project. By keeping this in mind a research is carried out on improving requirements elicitation process and automating the process of classifying requirements. In this research, a model is proposed which will help in this scenario for requirements elicitation and requirement classification. This paper presents a model in which crowd sourcing approach is used so that customers, end users, stakeholders, developers and software engineers can make active participation for requirement elicitation process and requirements gathered using crowdsourcing approach are used by model for classification process i.e. classification of requirements into functional and non-functional requirements. For the proof of proposed model a case study is conducted. Results of case study provided the usefulness and efficiency of proposed model for classification of crowd sourced software requirements.",
        "link": "https://dl.acm.org/doi/10.1145/3328833.3328837"
    },
    {
        "title": "Text classification using Fuzzy TF-IDF and Machine Learning Models",
        "authors": [
            "Mariem Bounabi",
            "Karim El Moutaouakil",
            "Khalid Satori"
        ],
        "date": "October 2019",
        "source": "BDIoT'19: Proceedings of the 4th International Conference on Big Data and Internet of Things",
        "abstract": "The representation of the information has an important impact on the text classification task. Several weighting methods were proposed in the literature, and the term frequency-inverse term frequency (TFIDF), the most know on the text treatment field. The FTF-IDF is a vector representation where the components of the TFIDF are presented as inputs to the Fuzzy Inference System (FIS). In this work, we compare several Machin Learning algorithms such as Naïve Bayes and its derivatives, SVM and Random forest classifiers, using the FTF-IDF representation. To improve the quality of the used classifiers, we call sum attribute selection methods. The recognition rate, for the tested systems, is satisfied, where the system based on naïve Bayes classifier, the FTF-IDF weighting terms, and the info gain select attributes method gives 98.7% as accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3372938.3372956"
    },
    {
        "title": "Text Mining in Cybersecurity: A Systematic Literature Review",
        "authors": [
            "Luciano Ignaczak",
            "Guilherme Goldschmidt",
            "Cristiano André Da Costa",
            "Rodrigo Da Rosa Righi"
        ],
        "date": null,
        "source": "ACM Computing Surveys",
        "abstract": "The growth of data volume has changed cybersecurity activities, demanding a higher level of automation. In this new cybersecurity landscape, text mining emerged as an alternative to improve the efficiency of the activities involving unstructured data. This article proposes a Systematic Literature Review (SLR) to present the application of text mining in the cybersecurity domain. Using a systematic protocol, we identified 2,196 studies, out of which 83 were summarized. As a contribution, we propose a taxonomy to demonstrate the different activities in the cybersecurity domain supported by text mining. We also detail the strategies evaluated in the application of text mining tasks and the use of neural networks to support activities involving unstructured data. The work also discusses text classification performance aiming its application in real-world solutions. The SLR also highlights open gaps for future research, such as the analysis of non-English content and the intensification in the usage of neural networks.",
        "link": "https://dl.acm.org/doi/10.1145/3462477"
    },
    {
        "title": "Using Text Mining to Analyze the Financial Patents",
        "authors": [
            "Yung-Feng Lu",
            "Jia-Lang Xu",
            "Mu-Yen Chen"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "With the advancement and development of science and technology, Nowadays society is the development of information technology, cloud computing, mobile Internet, Internet of Things, block chain and information technology.The combination of finance and technology has made financial technology terms gradually emerging.. This research analyzes the patents of financial technology. However, Taiwan's financial technology patents are mostly G06Q-20 (payment plan, architecture or agreement), G06Q-30 (commercial, such as marketing, shopping, payment, auction or e-commerce). G06Q-40 (financial, investment or tax treatment, insurance).The research use text mining technology to analyze and understand Taiwan's trends and developments in financial technology. The research results show that G06Q-20 patents are biased towards information security and mobile payment. G06Q-30 patents favor shopping websites and advertising. G06Q-40 patents are biased towards diversity, such as securities, insurance, finance, and stocks.",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326478"
    },
    {
        "title": "Identifying Online Profiles of Distance Learning Students Using Data Mining Techniques",
        "authors": [
            "Osama Islam",
            "Muazzam Siddiqui",
            "Naif Radi Aljohani"
        ],
        "date": "October 2019",
        "source": "ICDTE '19: Proceedings of the 3rd International Conference on Digital Technology in Education",
        "abstract": "Educational data has grown over the years with the increased use of technology within educational environments. This has led to a huge amount of data being stored in various data sources representing the student, his/her activities, and other aspects relevant to the learning process. To meet this analytical need, Educational Data Mining (EDM) has emerged to assist educational institutions in identifying key benefits such as students at risk, the level of student engagement or predicting student performance. The aim of this research was to explore the various aspects of student interaction data using data mining techniques to identify relevant patterns of behaviors and possible key attributes that have higher degrees of influence on distance learning students. The main findings identified several patterns of user online profiles based on a set of adopted learning strategies, the research proposed a framework for analyzing such interaction data based on R and Hadoop platforms to correlate online profiles with student performance.",
        "link": "https://dl.acm.org/doi/10.1145/3369199.3369249"
    },
    {
        "title": "Extremely Fast Decision Tree Mining for Evolving Data Streams",
        "authors": [
            "Albert Bifet",
            "Jiajin Zhang",
            "Wei Fan",
            "Cheng He",
            "Jianfeng Zhang",
            "Jianfeng Qian",
            "Geoff Holmes",
            "Bernhard Pfahringer"
        ],
        "date": "August 2017",
        "source": "KDD '17: Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
        "abstract": "Nowadays real-time industrial applications are generating a huge amount of data continuously every day. To process these large data streams, we need fast and efficient methodologies and systems. A useful feature desired for data scientists and analysts is to have easy to visualize and understand machine learning models. Decision trees are preferred in many real-time applications for this reason, and also, because combined in an ensemble, they are one of the most powerful methods in machine learning. In this paper, we present a new system called STREAMDM-C++, that implements decision trees for data streams in C++, and that has been used extensively at Huawei. Streaming decision trees adapt to changes on streams, a huge advantage since standard decision trees are built using a snapshot of data, and can not evolve over time. STREAMDM-C++ is easy to extend, and contains more powerful ensemble methods, and a more efficient and easy to use adaptive decision trees. We compare our new implementation with VFML, the current state of the art implementation in C, and show how our new system outperforms VFML in speed using less resources.",
        "link": "https://dl.acm.org/doi/10.1145/3097983.3098139"
    },
    {
        "title": "Data miners' little helper: data transformation activity cues for cluster analysis on document collections",
        "authors": [
            "Tania Cerquitelli",
            "Evelina Di Corso",
            "Francesco Ventura",
            "Silvia Chiusano"
        ],
        "date": "June 2017",
        "source": "WIMS '17: Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "In this paper we propose a new self-learning engine to streamline the analytics process, as it enables analysts to mine massive data repositories with minimal user intervention. In the context of cluster analysis on a collection of documents this new system, named SELF-DATA (SELF-learning DAta TrAnsformation), suggests to the analyst how to configure the whole mining process for a given dataset. SELF-DATA relies on an engine exploring different data weighting schemas (e.g., normalized term frequencies) and data transformation methods (e.g., PCA) before applying the cluster analysis, evaluating and comparing solutions through different quality indices (e.g., weighted Silhouette), and presenting the k-top solutions to the analyst. SELF-DATA will also include a knowledge base storing results of experiments on previously processed datasets, and a classification algorithm trained on the knowledge base content to forecast the best configuration for the whole mining process for an unexplored dataset. The first development of SELF-DATA running on Apache Spark has been validated on 5 collections of documents. Experimental results highlight that TF-IDF and logarithmic entropy are effective to measure item relevance with sparse datasets, and the LSI method outperforms PCA with a large dictionary.",
        "link": "https://dl.acm.org/doi/10.1145/3102254.3102288"
    },
    {
        "title": "Interactive Visual Graph Mining and Learning",
        "authors": [
            "Ryan A. Rossi",
            "Nesreen K. Ahmed",
            "Rong Zhou",
            "Hoda Eldardiry"
        ],
        "date": null,
        "source": "ACM Transactions on Intelligent Systems and Technology",
        "abstract": "This article presents a platform for interactive graph mining and relational machine learning called GraphVis. The platform combines interactive visual representations with state-of-the-art graph mining and relational machine learning techniques to aid in revealing important insights quickly as well as learning an appropriate and highly predictive model for a particular task (e.g., classification, link prediction, discovering the roles of nodes, and finding influential nodes). Visual representations and interaction techniques and tools are developed for simple, fast, and intuitive real-time interactive exploration, mining, and modeling of graph data. In particular, we propose techniques for interactive relational learning (e.g., node/link classification), interactive link prediction and weighting, role discovery and community detection, higher-order network analysis (via graphlets, network motifs), among others. GraphVis also allows for the refinement and tuning of graph mining and relational learning methods for specific application domains and constraints via an end-to-end interactive visual analytic pipeline that learns, infers, and provides rapid interactive visualization with immediate feedback at each change/prediction in real-time. Other key aspects include interactive filtering, querying, ranking, manipulating, exporting, as well as tools for dynamic network analysis and visualization, interactive graph generators (including new block model approaches), and a variety of multi-level network analysis techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3200764"
    },
    {
        "title": "Data Mining and Opinion Mining: A Tool in Educational Context",
        "authors": [
            "Myriam Peñafiel",
            "Stefanie Vásquez",
            "Diego Vásquez",
            "Juan Zaldumbide",
            "Sergio Luján-Mora"
        ],
        "date": "July 2018",
        "source": "ICoMS '18: Proceedings of the 2018 1st International Conference on Mathematics and Statistics",
        "abstract": "The use of the web as a universal communication platform generates large volumes of data (Big data), which in many cases, need to be processed so that they can become useful knowledge in face of the sceptics who have doubts about the credibility of such information. The use of web data that comes from educational contexts needs to be addressed, since that large amount of unstructured information is not being valued, losing valuable information that can be used. To solve this problem, we propose the use of data mining techniques such as sentiment analysis to validate the information that comes from the educational platforms. The objective of this research is to propose a methodology that allows the user to apply sentiment analysis in a simple way, because although some researchers have done it, very few do with data in the educational context. The results obtained prove that the proposal can be used in similar cases.",
        "link": "https://dl.acm.org/doi/10.1145/3274250.3274263"
    },
    {
        "title": "Research Hotspots Mining and Visualized Analysis Based on Linking Cluster and K-Core Decomposition",
        "authors": [
            "Ying Cai",
            "Fang Huang",
            "Shuai Wang",
            "Hao Zhang",
            "Chunxiu Du"
        ],
        "date": "May 2018",
        "source": "ICDPA 2018: Proceedings of the International Conference on Data Processing and Applications",
        "abstract": "In this paper, we focus on research hotspots mining with knowledge semantic features on discipline topic word networks, which are extracted from scientific and technical documents. For the topic networks, a novel method of combining linking community clustering with k-core decomposition is proposed to mine the hotspot community with knowledge hierarchy structure. And as a post-processing, the hierarchical community identification is implemented by k-core decomposition in order to discover the internal connection with knowledge semantic hierarchy in the research hotspot. Finally, the community density and visualization method is utilized to analyze the core and sub- problem in the research hotspot community. In experiments, the topic networks for a single discipline and interdisciplinary research hotspot are taken as examples of analysis, and the results show the proposed approach can effectively realize the objective of mining and analyzing the discipline research hotspot on the academic topic word network.",
        "link": "https://dl.acm.org/doi/10.1145/3224207.3224216"
    },
    {
        "title": "The Application of Data Mining Algorithms in the Construction of Travel Recommendation System",
        "authors": [
            "Chao Lou",
            "Zhaonan Mu",
            "Mengzhu Liu"
        ],
        "date": "January 2021",
        "source": "CONF-CDS 2021: The 2nd International Conference on Computing and Data Science",
        "abstract": "Photos shared by users of online social network often associated with tags, texts, geographic and time information. These data are ideal research resource which can be used by researchers to observe people's interests and behaviors. They can also be used to analyze people's travel behaviors and interests because it can reflect people's travel activities and experiences directly. This project aimed to consolidate people into groups based on their travel preferences of travel season, travel time span, scenery type of travel location and then analyze popular travel places for different people groups. The travel preferences were analyzed from extracted data applying PHP language and Naive Bayes classification approach. According to the analyzed data, people were divided into groups. Then, DBSCAN clustering was used to get popular locations based on the different groups. As a result, 80 thousand photo data of 5 cities have been collected from Flickr by Phython. It has been showed that people's preference of season in most of the mined cities was spring and summer and the preference of time span was short. Moreover, mined people has been divided into 16 groups according to their preference and based on the groups popular locations have been calculated by DBSCAN clustering. Theses locations have been provided in a recommendation website which was built in this project. For the future work, mining Flickr data associated with other travel social network sites could be a good way to build a more effective recommendation system.",
        "link": "https://dl.acm.org/doi/10.1145/3448734.3450477"
    },
    {
        "title": "Compare Machine Learning Models in Text Classification Using Steam User Reviews",
        "authors": [
            "Youchen Miao",
            "Zeyu Jin",
            "Yumeng Zhang",
            "Yuchen Chen",
            "Junren Lai"
        ],
        "date": "November 2021",
        "source": "ICSED '21: Proceedings of the 2021 3rd International Conference on Software Engineering and Development",
        "abstract": "Text Classification and Sentiment Analysis of game reviews are viewed as important parts in not only academic fields but also in game studies. In this paper, with more than 400 thousand game reviews on Steam platform, we preprocess the data using different libraries (sklearn, nltk, and spaCy) and use them as inputs to build three sentiment classification models based on different algorithms (Naive Bayes, SVM, and Random Forest). In contrast to previous studies that only focus on different sentiment analysis models, our paper also highlights the use of different APIs to preprocess the data and their corresponding model performance. The results show that no matter which API we choose, Random Forest models always perform the best. However, in terms of training time, Naive Bayes is the fastest. This work can be used to apply grid search for researchers to automatically find the optimum API before conducting sentiment analysis in the future.",
        "link": "https://dl.acm.org/doi/10.1145/3507473.3507480"
    },
    {
        "title": "Lost in Transduction: Transductive Transfer Learning in Text Classification",
        "authors": [
            "Alejandro Moreo",
            "Andrea Esuli",
            "Fabrizio Sebastiani"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Obtaining high-quality labelled data for training a classifier in a new application domain is often costly. Transfer Learning\n(a.k.a. “Inductive Transfer”) tries to alleviate these costs by transferring, to the “target” domain of interest, knowledge available from a different “source” domain. In transfer learning the lack of labelled information from the target domain is compensated by the availability at training time of a set of unlabelled examples from the target distribution. Transductive Transfer Learning denotes the transfer learning setting in which the only set of target documents that we are interested in classifying is known and available at training time. Although this definition is indeed in line with Vapnik’s original definition of “transduction”, current terminology in the field is confused. In this article, we discuss how the term “transduction” has been misused in the transfer learning literature, and propose a clarification consistent with the original characterization of this term given by Vapnik. We go on to observe that the above terminology misuse has brought about misleading experimental comparisons, with inductive transfer learning methods that have been incorrectly compared with transductive transfer learning methods. We then, give empirical evidence that the difference in performance between the inductive version and the transductive version of a transfer learning method can indeed be statistically significant (i.e., that knowing at training time the only data one needs to classify indeed gives an advantage). Our clarification allows a reassessment of the field, and of the relative merits of the major, state-of-the-art algorithms for transfer learning in text classification.",
        "link": "https://dl.acm.org/doi/10.1145/3453146"
    },
    {
        "title": "Machine Learning on Mining Potential Adverse Drug Reactions for Pharmacovigilance",
        "authors": [
            "Ke Jia"
        ],
        "date": "July 2021",
        "source": "DSIT 2021: 2021 4th International Conference on Data Science and Information Technology",
        "abstract": "Drugs are related to the lives and health of patients. Mining and discovering potential adverse drug reactions from a large number of unstructured texts play an important role in drug research and pharmacovigilance. It is a hot research issue in the field of biomedicine in recent years. A large number of scholars have discovered a large amount of information on adverse drug reactions from different data sources through effective machine learning methods. This paper reviews the research progress in recent years to provide a reference for future research.",
        "link": "https://dl.acm.org/doi/10.1145/3478905.3478964"
    },
    {
        "title": "TACAM: Topic And Context Aware Argument Mining",
        "authors": [
            "Michael Fromm",
            "Evgeniy Faerman",
            "Thomas Seidl"
        ],
        "date": "October 2019",
        "source": "WI '19: IEEE/WIC/ACM International Conference on Web Intelligence",
        "abstract": "In this work we address the problem of argument search. The purpose of argument search is the distillation of pro and contra arguments for requested topics from large text corpora. In previous works, the usual approach is to use a standard search engine to extract text parts which are relevant to the given topic and subsequently use an argument recognition algorithm to select arguments from them. The main challenge in the argument recognition task, which is also known as argument mining, is that often sentences containing arguments are structurally similar to purely informative sentences without any stance about the topic. In fact, they only differ semantically. Most approaches use topic or search term information only for the first search step and therefore assume that arguments can be classified independently of a topic. We argue that topic information is crucial for argument mining, since the topic defines the semantic context of an argument. Precisely, we propose different models for the classification of arguments, which take information about a topic of an argument into account. Moreover, to enrich the context of a topic and to let models understand the context of the potential argument better, we integrate information from different external sources such as Knowledge Graphs or pre-trained NLP models. Our evaluation shows that considering topic information, especially in connection with external information, provides a significant performance boost for the argument mining task.",
        "link": "https://dl.acm.org/doi/10.1145/3350546.3352506"
    },
    {
        "title": "Text mining for incoming tasks based on the urgency/importance factors and task classification using machine learning tools",
        "authors": [
            "Yasser Ali Alshehri"
        ],
        "date": "March 2020",
        "source": "ICCDA 2020: Proceedings of the 2020 the 4th International Conference on Compute and Data Analysis",
        "abstract": "In workplaces, there is a massive amount of unstructured data from different sources. In this paper, we present a case study that explains how can through communications between employees, we can help to prioritize tasks requests to increase the efficiency of their works for both technical and non-technical workers. This involves managing daily incoming tasks based on their level of urgency and importance.To allow all workers to utilize the urgency-importance matrix as a time-management tool, we need to automate this tool. The textual content of incoming tasks are analyzed, and metrics related to urgency and importance are extracted. A third factor (i.e., the response variable) is defined based on the two input variables (urgency and importance). Then, machine learning applied to the data to predict the class of incoming tasks based on data outcome desired. We used ordinal regression, neural networks, and decision tree algorithms to predict the four levels of task priority. We measure the performance of all using recalls, precisions, and F-scores. All classifiers perform higher than 89% in terms of all measures.",
        "link": "https://dl.acm.org/doi/10.1145/3388142.3388153"
    },
    {
        "title": "Text mining for malware classification using multivariate all repeated patterns detection",
        "authors": [
            "Konstantinos F. Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2019",
        "source": "ASONAM '19: Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Mobile phones have become nowadays a commodity to the majority of people. Using them, people are able to access the world of Internet and connect with their friends, their colleagues at work or even unknown people with common interests. This proliferation of the mobile devices has also been seen as an opportunity for the cyber criminals to deceive smartphone users and steel their money directly or indirectly, respectively, by accessing their bank accounts through the smartphones or by blackmailing them or selling their private data such as photos, credit card data, etc. to third parties. This is usually achieved by installing malware to smartphones masking their malevolent payload as a legitimate application and advertise it to the users with the hope that mobile users will install it in their devices. Thus, any existing application can easily be modified by integrating a malware and then presented it as a legitimate one. In response to this, scientists have proposed a number of malware detection and classification methods using a variety of techniques. Even though, several of them achieve relatively high precision in malware classification, there is still space for improvement. In this paper, we propose a text mining all repeated pattern detection method which uses the decompiled files of an application in order to classify a suspicious application into one of the known malware families. Based on the experimental results using a real malware dataset, the methodology tries to correctly classify (without any misclassification) all randomly selected malware applications of 3 categories with 3 different families each.",
        "link": "https://dl.acm.org/doi/10.1145/3341161.3350841"
    },
    {
        "title": "Comparative Study between Traditional Machine Learning and Deep Learning Approaches for Text Classification",
        "authors": [
            "Cannannore Nidhi Kamath",
            "Syed Saqib Bukhari",
            "Andreas Dengel"
        ],
        "date": "August 2018",
        "source": "DocEng '18: Proceedings of the ACM Symposium on Document Engineering 2018",
        "abstract": "In this contemporaneous world, it is an obligation for any organization working with documents to end up with the insipid task of classifying truckload of documents, which is the nascent stage of venturing into the realm of information retrieval and data mining. But classification of such humongous documents into multiple classes, calls for a lot of time and labor. Hence a system which could classify these documents with acceptable accuracy would be of an unfathomable help in document engineering. We have created multiple classifiers for document classification and compared their accuracy on raw and processed data. We have garnered data used in a corporate organization as well as publicly available data for comparison. Data is processed by removing the stop-words and stemming is implemented to produce root words. Multiple traditional machine learning techniques like Naive Bayes, Logistic Regression, Support Vector Machine, Random forest Classifier and Multi-Layer Perceptron are used for classification of documents. Classifiers are applied on raw and processed data separately and their accuracy is noted. Along with this, Deep learning technique such as Convolution Neural Network is also used to classify the data and its accuracy is compared with that of traditional machine learning techniques. We are also exploring hierarchical classifiers for classification of classes and subclasses. The system classifies the data faster and with better accuracy than if done manually. The results are discussed in the results and evaluation section.",
        "link": "https://dl.acm.org/doi/10.1145/3209280.3209526"
    },
    {
        "title": "Identification of Overpricing in the Purchase of Medication by the Federal Government of Brazil, Using Text Mining and Clustering Based on Ontology",
        "authors": [
            "Marco Aurelio O. S. Correa",
            "Adriano Galindo Leal"
        ],
        "date": "August 2018",
        "source": "ICCBDC '18: Proceedings of the 2018 2nd International Conference on Cloud and Big Data Computing",
        "abstract": "Increasing the transparency level in his actions and spending is one of the primary duties of the Brazilian Federal Government. The creation of laws that oblige full disclose of all its expenditures through transparency portals enables citizens to supervise all government entities. However, only the dissemination of these data, without a definite standard or the availability of data analysis tools, does not guarantee that the citizen is empowered to play his role. Therefore hence, the objective of this work is to identify overprice in the acquisition of products purchased by the federal government of Brazil using the unstructured data available on the Transparency Portal. The last two-years' worth of purchasing data, available in the Transparency Portal, were extracted, processed and stored. Due to his diverse nature and high volume of data, this study focused only on medicines purchased by the Ministry of Health. Ontology-based text mining and clustering techniques were applied for automatic identification and classification of products. The processing of this information was done through text mining and clustering, based on the ontology registered in another database of the Brazilian government. Because of this work, a consolidated price base per medication was created to allow the identification of distortions in prices practised, facilitating the identification of cases that merit further investigation to unravel fraud to the treasury.",
        "link": "https://dl.acm.org/doi/10.1145/3264560.3264569"
    },
    {
        "title": "Computational Estimation by Scientific Data Mining with Classical Methods to Automate Learning Strategies of Scientists",
        "authors": [
            "Aparna S. Varde"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Experimental results are often plotted as 2-dimensional graphical plots (aka graphs) in scientific domains depicting dependent versus independent variables to aid visual analysis of processes. Repeatedly performing laboratory experiments consumes significant time and resources, motivating the need for computational estimation. The goals are to estimate the graph obtained in an experiment given its input conditions, and to estimate the conditions that would lead to a desired graph. Existing estimation approaches often do not meet accuracy and efficiency needs of targeted applications. We develop a computational estimation approach called AutoDomainMine that integrates clustering and classification over complex scientific data in a framework so as to automate classical learning methods of scientists. Knowledge discovered thereby from a database of existing experiments serves as the basis for estimation. Challenges include preserving domain semantics in clustering, finding matching strategies in classification, striking a good balance between elaboration and conciseness while displaying estimation results based on needs of targeted users, and deriving objective measures to capture subjective user interests. These and other challenges are addressed in this work. The AutoDomainMine approach is used to build a computational estimation system, rigorously evaluated with real data in Materials Science. Our evaluation confirms that AutoDomainMine provides desired accuracy and efficiency in computational estimation. It is extendable to other science and engineering domains as proved by adaptation of its sub-processes within fields such as Bioinformatics and Nanotechnology.",
        "link": "https://dl.acm.org/doi/10.1145/3502736"
    },
    {
        "title": "Where is the road for issue reports classification based on text mining?",
        "authors": [
            "Qiang Fan",
            "Yue Yu",
            "Gang Yin",
            "Tao Wang",
            "Huaimin Wang"
        ],
        "date": "November 2017",
        "source": "ESEM '17: Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Currently, open source projects receive various kinds of issues daily, because of the extreme openness of Issue Tracking System (ITS) in GitHub. ITS is a labor-intensive and time-consuming task of issue categorization for project managers. However, a contributor is only required a short textual abstract to report an issue in GitHub. Thus, most traditional classification approaches based on detailed and structured data (e.g., priority, severity, software version and so on) are difficult to adopt. In this paper, issue classification approaches on a large-scale dataset, including 80 popular projects and over 252,000 issue reports collected from GitHub, were investigated. First, four traditional text-based classification methods and their performances were discussed. Semantic perplexity (i.e., an issues description confuses bug-related sentences with nonbug-related sentences) is a crucial factor that affects the classification performances based on quantitative and qualitative study. Finally, A two-stage classifier framework based on the novel metrics of semantic perplexity of issue reports was designed. Results show that our two-stage classification can significantly improve issue classification performances.",
        "link": "https://dl.acm.org/doi/10.1109/ESEM.2017.19"
    },
    {
        "title": "Feature-based Facebook reviews process model for e-management using data mining",
        "authors": [
            "Anish Kumar Varudharajulu",
            "Yongsheng Ma"
        ],
        "date": "January 2019",
        "source": "IC4E '19: Proceedings of the 10th International Conference on E-Education, E-Business, E-Management and E-Learning",
        "abstract": "The data generated from online communication acts as potential gold mines for discovering knowledge for researchers. A large amount of data is also generated in the form of web documents, emails, blogs, and feedback, etc. Text analytics is being significantly employed to mine important information. Opinion mining is the process of extracting human thoughts and perceptions from unstructured texts. The showstopper for designing an opinion mining system for analyzing reviews arise from the fact that customer reviews are often noisy. These reviews are informally written. In addition, they are subjected to spelling mistakes, grammatical errors, improper punctuation and irrational capitalization. This paper focuses on analyzing the different classification and clustering algorithms aimed at extracting and consolidating opinions of customers from social media sites like Facebook, Twitter and through surveys, at multiple levels of granularity to monitor and measure customer satisfaction. Ours is an automated approach, in which the system aids in the process of knowledge assimilation for knowledge-based building and also performs the analytics. Domain experts ratify the knowledge base and also provide training datasets for the system to intuitively gather more instances for ratification. The system identifies opinion expressions as phrases containing opinion words, opinionated features and also opinion modifiers. These expressions are categorized as positive, negative or neutral. Opinion expressions are identified and categorized using localized linguistic techniques. Opinions can be congregated at any desired level of specificity i.e. feature level or product level, user level or service level, etc. We have developed a system based on this approach, which provides the user with a platform to analyze opinion expressions crawled from a set of pre-defined datasets.",
        "link": "https://dl.acm.org/doi/10.1145/3306500.3306514"
    },
    {
        "title": "Discriminative Topic Mining via Category-Name Guided Text Embedding",
        "authors": [
            "Yu Meng",
            "Jiaxin Huang",
            "Guangyuan Wang",
            "Zihan Wang",
            "Chao Zhang",
            "Yu Zhang",
            "Jiawei Han"
        ],
        "date": "April 2020",
        "source": "WWW '20: Proceedings of The Web Conference 2020",
        "abstract": "Mining a set of meaningful and distinctive topics automatically from massive text corpora has broad applications. Existing topic models, however, typically work in a purely unsupervised way, which often generate topics that do not fit users’ particular needs and yield suboptimal performance on downstream tasks. We propose a new task, discriminative topic mining, which leverages a set of user-provided category names to mine discriminative topics from text corpora. This new task not only helps a user understand clearly and distinctively the topics he/she is most interested in, but also benefits directly keyword-driven classification tasks. We develop CatE, a novel category-name guided text embedding method for discriminative topic mining, which effectively leverages minimal user guidance to learn a discriminative embedding space and discover category representative terms in an iterative manner. We conduct a comprehensive set of experiments to show that CatE mines high-quality set of topics guided by category names only, and benefits a variety of downstream applications including weakly-supervised classification and lexical entailment direction identification.",
        "link": "https://dl.acm.org/doi/10.1145/3366423.3380278"
    },
    {
        "title": "Comparative Study of Heart Disease Diagnosis Using Top Ten Data Mining Classification Algorithms",
        "authors": [
            "I. Ketut Agung Enriko"
        ],
        "date": "June 2019",
        "source": "ICFET '19: Proceedings of the 5th International Conference on Frontiers of Educational Technologies",
        "abstract": "Data mining has been used for many purposes, especially for prediction system. In healthcare, data mining algorithms often used in disease diagnosis. Meanwhile, heart disease is known as a primary cause of death over the years. Many studies have been performed in heart disease diagnosis using data mining methods. There are some popular data mining algorithms that can be used in heart disease diagnosis, for example, k-Nearest Neighbor, CART, and AdaBoost. The algorithms are used to analyze a sample of cardiovascular patients data and predict the heart disease type that they suffer. Some parameters are taken from the patient, including EKG morphology, blood pressure, and information about the existence of chest pain, shortness of breath, palpitation, and cold sweat. In this study, medical records data are collected from Harapan Kita Hospital and utilized as a dataset sample in this research. Top ten data mining classification algorithms are used in diagnosing heart disease from Harapan Kita Hospital data and examining their performance by checking the accuracy and speed.",
        "link": "https://dl.acm.org/doi/10.1145/3338188.3338220"
    },
    {
        "title": "Text Mining Approach for Identifying Research Trends",
        "authors": [
            "Snezhana Sulova"
        ],
        "date": "June 2021",
        "source": "CompSysTech '21: Proceedings of the 22nd International Conference on Computer Systems and Technologies",
        "abstract": "With the increase of unstructured data, the issues connected with automatic text processing, the categorization of documents and the discovery of topics have become objects of growing interest. In order to improve the process of grouping and processing research publications, we would like to propose a method based upon natural language processing. It is based on text mining technologies which aim to identify key tendencies in documents. It processes the content of publications by clustering and identifies the topics of each identified group. This analysis helps by identifying key tendencies as well as discovering emerging new areas of research. Publications from the research literature database, Scopus, were used to test the approach. The topic of the publications is “the application of digital technologies in the logistics business”. The experiments were completed using the RapidMiner Studio software.",
        "link": "https://dl.acm.org/doi/10.1145/3472410.3472433"
    },
    {
        "title": "Assessment of Vulnerability Severity using Text Mining",
        "authors": [
            "Georgios Spanos",
            "Lefteris Angelis",
            "Dimitrios Toloudis"
        ],
        "date": "September 2017",
        "source": "PCI '17: Proceedings of the 21st Pan-Hellenic Conference on Informatics",
        "abstract": "Software1 vulnerabilities are closely associated with information systems security, a major and critical field in today's technology. Vulnerabilities constitute a constant and increasing threat for various aspects of everyday life, especially for safety and economy, since the social impact from the problems that they cause is complicated and often unpredictable. Although there is an entire research branch in software engineering that deals with the identification and elimination of vulnerabilities, the growing complexity of software products and the variability of software production procedures are factors contributing to the ongoing occurrence of vulnerabilities, Hence, another area that is being developed in parallel focuses on the study and management of the vulnerabilities that have already been reported and registered in databases. The information contained in such databases includes, a textual description and a number of metrics related to vulnerabilities. The purpose of this paper is to investigate to what extend the assessment of the vulnerability severity can be inferred directly from the corresponding textual description, or in other words, to examine the informative power of the description with respect to the vulnerability severity. For this purpose, text mining techniques, i.e. text analysis and three different classification methods (decision trees, neural networks and support vector machines) were employed. The application of text mining to a sample of 70,678 vulnerabilities from a public data source shows that the description itself is a reliable and highly accurate source of information for vulnerability prioritization.",
        "link": "https://dl.acm.org/doi/10.1145/3139367.3139390"
    },
    {
        "title": "Clustering Algorithms for Spatial Data Mining",
        "authors": [
            "Chetashri Bhadane",
            "Ketan Shah"
        ],
        "date": "April 2020",
        "source": "ICGDA '20: Proceedings of the 2020 3rd International Conference on Geoinformatics and Data Analysis",
        "abstract": "With the advances in mobile and wireless technologies, there has been a rise in applications that track and share the users' geospatial data. People use several social networking sites such as Twitter, Facebook and Flickr, where they share their status updates. With the integration of Global Positioning System (GPS) with mobile phones, it is now possible to share one's locations on these social networks. GPS allows us to record and track a person's movement along with the timestamp. The data set obtained from these GPS logs is vast and is widely used to analyze the users' movement patterns. Specifically, we can find out significant locations based on the number of users present at that location and the time spent by them at such places. Once significant places have been identified, it is also possible to identify the semantic importance of these locations. This paper presents an overview of the clustering techniques used to find important places of interest using large GPS based mobility datasets. Four clustering algorithms, K-Means, DBSCAN, OPTICS and Hierarchical, are implemented, and performance is tested using real-time data of 50 users collected over 2--5 years. Performance summary depicts that K-Means and DBSCAN perform well for spatial data.",
        "link": "https://dl.acm.org/doi/10.1145/3397056.3397068"
    },
    {
        "title": "Feature Selection based Arabic Text Classification using Different Machine Learning Algorithms: Comparative Study",
        "authors": [
            "Sakina Rim Bennabi",
            "Zakaria Elberrichi"
        ],
        "date": "June 2020",
        "source": "ICIST '20: Proceedings of the 10th International Conference on Information Systems and Technologies",
        "abstract": "Feature selection is a method of data pre-processing widely used when mining large data, such as textual classification. Several studies have been conducted to compare the different methods of feature selection applied to corpora in English. Unfortunately, a small number of works concern the Arabic language. This article aims to present a comparative study of different feature selection techniques including: Chi2, the ANOVA method and mutual information, applied on a corpus in Arabic language, while also diversifying the machine learning algorithms (Naive Bayes, SVM and KNN). This experimental study has shown in general that reducing dimensionality with feature selection techniques has slightly affected the performance of textual classification, reducing the size of the corpus by up to 1%.",
        "link": "https://dl.acm.org/doi/10.1145/3447568.3448531"
    },
    {
        "title": "Intelligent Data Analysis using Optimized Support Vector Machine Based Data Mining Approach for Tourism Industry",
        "authors": [
            "Ms Promila Sharma",
            "Uma Meena",
            "Girish Kumar Sharma"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Data analysis involves the deployment of sophisticated approaches from data mining methods, information theory, and artificial intelligence in various fields like tourism, hospitality, and so on for the extraction of knowledge from the gathered and preprocessed data. In tourism, pattern analysis or data analysis using classification is significant for finding the patterns that represent new and potentially useful information or knowledge about the destination and other data. Several data mining techniques are introduced for the classification of data or patterns. However, overfitting, less accuracy, local minima, sensitive to noise are the drawbacks in some existing data mining classification methods. To overcome these challenges, Support vector machine with Red deer optimization (SVM-RDO) based data mining strategy is proposed in this article. Extended Kalman filter (EKF) is utilized in the first phase, i.e., data cleaning to remove the noise and missing values from the input data. Mantaray foraging algorithm (MaFA) is used in the data selection phase, in which the significant data are selected for the further process to reduce the computational complexity. The final phase is the classification, in which SVM-RDO is proposed to access the useful pattern from the selected data. PYTHON is the implementation tool used for the experiment of the proposed model. The experimental analysis is done to show the efficacy of the proposed work. From the experimental results, the proposed SVM-RDO achieved better accuracy, precision, recall, and F1 score than the existing methods for the tourism dataset. Thus, it is showed the effectiveness of the proposed SVM-RDO for pattern analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3494566"
    },
    {
        "title": "Applying text mining to predict learners' cognitive engagement",
        "authors": [
            "Hayati Hind",
            "Mohammed Khalidi Idrissi",
            "Samir Bennani"
        ],
        "date": "October 2017",
        "source": "SCAMS '17: Proceedings of the Mediterranean Symposium on Smart City Application",
        "abstract": "In academic process, engagement represent one of the important success' factor. This paper proposes a predictive system for learners' cognitive engagement based on their online discussion forums participation. In the first level the ontology OWL and the LSA method are used to perform a semantic classification system of the threads according to a specific context chosen by the tutor. Then the Text mining, as a predictive method, is applied to the classified threads and learners' participation in the forums.",
        "link": "https://dl.acm.org/doi/10.1145/3175628.3175655"
    },
    {
        "title": "Investigate Transitions into Drug Addiction through Text Mining of Reddit Data",
        "authors": [
            "John Lu",
            "Sumati Sridhar",
            "Ritika Pandey",
            "Mohammad Al Hasan",
            "Georege Mohler"
        ],
        "date": "July 2019",
        "source": "KDD '19: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Increasing rates of opioid drug abuse and heightened prevalence of online support communities underscore the necessity of employing data mining techniques to better understand drug addiction using these rapidly developing online resources. In this work, we obtained data from Reddit, an online collection of forums, to gather insight into drug use/misuse using text snippets from users narratives. Specifically, using users' posts, we trained a binary classifier which predicts a user's transitions from casual drug discussion forums to drug recovery forums. We also proposed a Cox regression model that outputs likelihoods of such transitions. In doing so, we found that utterances of select drugs and certain linguistic features contained in one's posts can help predict these transitions. Using unfiltered drug-related posts, our research delineates drugs that are associated with higher rates of transitions from recreational drug discussion to support/recovery discussion, offers insight into modern drug culture, and provides tools with potential applications in combating the opioid crisis.",
        "link": "https://dl.acm.org/doi/10.1145/3292500.3330737"
    },
    {
        "title": "Analysing digital banking reviews using text mining",
        "authors": [
            "Li Chen Cheng",
            "Legaspi Rhea Sharmayne"
        ],
        "date": "December 2020",
        "source": "ASONAM '20: Proceedings of the 12th IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Digital banks are new entrants in the banking industry in the Philippines as they only started late 2018. Since then, a handful of players have and are still emerging. With more and more people becoming technologically savvy, it is very critical for financial institutions to develop a digital banking application that will stand out from the competition. This paper aims to use text mining methods to analyse digital banking application reviews. This study will perform topic modelling using LDA to explore customer concerns and will mine association rules between the digital banking features with the review score. The results will reveal which areas the digital banking application can further optimize for customer satisfaction and retention.",
        "link": "https://dl.acm.org/doi/10.1109/ASONAM49781.2020.9381429"
    },
    {
        "title": "Mining Moocs Videos Metadata Using Classification Techniques",
        "authors": [
            "El Harrak Othman",
            "Ghadi Abderrahim",
            "El Bouhdidi Jaber"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "In few years, the internet has become the first source of information for most people, Today MOOCs makes it possible for everyone to access to the education over the world, it's represents an emerging methodology of online teaching and an important development in open education. Due to the rapid development of Moocs and the rapid growth of digital data and video database over the Internet, it is becoming very difficult for learners to browsing and choosing the best training for them. The scientific community has increased the amount of research into new technologies, with a view to improve the digital video utilization: its archiving, indexing, accessibility, acquisition, store and even its process and usability. All these parts of the video utilization entail the necessity of the extraction of all important information of a video, especially in cases of lack of metadata information. Web video mining is retrieving the content using data mining techniques from World Wide Web. There are two approaches for web video mining using traditional image processing (signal processing) and metadata based approach. In this paper, we present the various research makes in this subject and we propose an effective methodology to extract the metadata from Moocs videos and classify them based on the extracted metadata by applying data mining techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090450"
    },
    {
        "title": "Text mining with HathiTrust",
        "authors": [
            "Eleanor Dickson Koehl",
            "Ryan Dubnicek"
        ],
        "date": "June 2019",
        "source": "JCDL '19: Proceedings of the 18th Joint Conference on Digital Libraries",
        "abstract": "This tutorial will introduce attendees to the HathiTrust Research Center and its tools and services for computational text analysis research. HTRC leverages the scope and scale of the HathiTrust Digital Library collection to create opportunities for researchers to perform text data mining on subsets of the corpus. Attendees will develop skills that will allow them to conduct basic text analysis research using HathiTrust data, learn techniques for engaging with scholars on their text data mining research, and explore strategies for computational access to large-scale digital collections.",
        "link": "https://dl.acm.org/doi/10.1109/JCDL.2019.00115"
    },
    {
        "title": "Text mining for plagiarism detection: multivariate pattern detection for recognition of text similarities",
        "authors": [
            "Konstantinos Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2018",
        "source": "ASONAM '18: Proceedings of the 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "The problem of plagiarism the recent years has been intensified by the availability of information in digital form and the accessibility of the electronic libraries through the Internet. As a result, plagiarism detection has been transformed into a big data analytics problem since the number of digital sources is extravagant and a new document needs to be compared with millions of other existing documents. In this paper, a text mining methodology is proposed that can detect all common patterns between a document and the documents in a reference database. The technique is based on a pattern detection algorithm and the corresponding data structure that enables the algorithm to detect all common patterns. The methodology has been applied in a well-defined dataset providing very promising results identifying difficult cases of plagiarism such as technical disguise.",
        "link": "https://dl.acm.org/doi/10.5555/3382225.3382424"
    },
    {
        "title": "Petroleum Engineering Data Text Classification Using Convolutional Neural Network Based Classifier",
        "authors": [
            "Tzuhan Hsu",
            "Yaoqin Zhang"
        ],
        "date": "May 2018",
        "source": "ICMLT '18: Proceedings of the 2018 International Conference on Machine Learning Technologies",
        "abstract": "In this paper, authors introduce a convolutional neural network (CNN) based classification approach for petroleum engineering event. Recently, text classification problem has raised substantial attention in the field of machine learning. The aim of text classification process is to assign correct label to a text on the basis of its content. However, due to the imprecise of feature representation, most of former text classification methods cannot export satisfying result. To solve this problem, authors propose a model based on convolutional neural network. Authors conduct experiments with real world petroleum engineering data text, artificially labelled into 6 categories. Beside purposed CNN-based model, the authors also implement other baseline machine learning approaches: k nearest neighbor, random forest, support vector machines, recurrent neural network, gated recurrent units, long short-term memory. Among all these models, our CNN-based model reach best performance in text classification task. In the last part of this paper, authors compare and analysis their purpose and baseline model.",
        "link": "https://dl.acm.org/doi/10.1145/3231884.3231898"
    },
    {
        "title": "Classifying Indonesian Online Articles as Advertisement Placement Base Using Text Mining",
        "authors": [
            "Nadhira Tasya",
            "Arian Dhini"
        ],
        "date": "July 2017",
        "source": "ICBIM 2017: Proceedings of the International Conference on Business and Information Management",
        "abstract": "Rapid development in technological aspect resulting in growing level of human needs for the latest news, so that emerged a new trend of publishing and accessing news through online media or usually called online journalism. In addition, the number of people who sell and purchase through online sites also continues to increase and this opportunity is utilized by the company and the advertiser by implementing targeted web advertising. However, the high number of articles that have been published and accessed leads to great opportunities for errors in determining where to place the ads. Therefore, it needs a system that can categorize articles accessed by users as the basis of advertisement placement by the company and this classification system can be done by applying the method of Data Mining and Text Mining. This research uses document data in the form of article content that will be categorized into twenty categories of class of advertisement by using Text Mining technique with Support Vector Machine algorithm. The results of this study may be used by companies or advertisers as a basis for placement of ads on selected online media sites.",
        "link": "https://dl.acm.org/doi/10.1145/3134271.3134288"
    },
    {
        "title": "Mining Free-Text Medical Notes for Suicide Risk Assessment",
        "authors": [
            "Marios Adamou",
            "Grigoris Antoniou",
            "Elissavet Greasidou",
            "Vincenzo Lagani",
            "Paulos Charonyktakis",
            "Ioannis Tsamardinos"
        ],
        "date": "July 2018",
        "source": "SETN '18: Proceedings of the 10th Hellenic Conference on Artificial Intelligence",
        "abstract": "Suicide has been considered as an important public health issue for a very long time, and is one of the main causes of death worldwide. Despite suicide prevention strategies being applied, the rate of suicide has not changed substantially over the past decades. Advances in machine learning make it possible to attempt to predict suicide based on the analysis of relevant data to inform clinical practice. This paper reports on findings from the analysis of data of patients who died by suicide in the period 2013-2016 and made use of both structured data and free-text medical notes. We focus on examining various text-mining approaches to support risk assessment. The results show that using advance machine learning and text-mining techniques, it is possible to predict within a specified period which people are most at risk of taking their own life at the time of referral to a mental health service.",
        "link": "https://dl.acm.org/doi/10.1145/3200947.3201020"
    },
    {
        "title": "Integration of data mining techniques in e-learning systems: Clustering Profil of Lerners and Recommender Course System",
        "authors": [
            "A. El Moustamid",
            "E. En-Naimi",
            "J. El Bouhdidi"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "Data mining aims at the extraction of knowledge from large quantities of data by automatic or semi-automatic methods. In the field of education and training e-learning, Data Mining techniques as clustering (The aim of this technique is to identify groups that are similar in some aspect) [21], prediction (is used for developing model where combination of data viewed from different aspects can effect single viewed data. Here, is important to watch how predictors variables are effecting predicted variable)[21], classification are applied in e-learning to improve the level of learners by providing learning according to the learners' requirement) are used to analyze learners profile, to predict and improve student performance. The aim of our project is to develop a system capable of analyzing learners profile and indexing web videos in order to offer learners a rich database with courses that correspond to their levels.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090453"
    },
    {
        "title": "Multi-level mining and visualization of scientific text collections: Exploring a bi-lingual scientific repository",
        "authors": [
            "Pablo Accuosto",
            "Francesco Ronzano",
            "Daniel Ferrés",
            "Horacio Saggion"
        ],
        "date": "December 2017",
        "source": "WOSP 2017: Proceedings of the 6th International Workshop on Mining Scientific Publications",
        "abstract": "We present a system to mine and visualize collections of scientific documents by semantically browsing information extracted from single publications or aggregated throughout corpora of articles. The text mining tool performs deep analysis of document collections allowing the extraction and interpretation of research paper's contents. In addition to the extraction and enrichment of documents with metadata (titles, authors, affiliations, etc), the deep analysis performed comprises semantic interpretation, rhetorical analysis of sentences, triple-based information extraction, and text summarization. The visualization components allow geographical-based exploration of collections, topic-evolution interpretation, and collaborative network analysis among others. The paper presents a case study of a bi-lingual collection in the field of Natural Language Processing (NLP).",
        "link": "https://dl.acm.org/doi/10.1145/3127526.3127529"
    },
    {
        "title": "Hierarchical Topic Mining via Joint Spherical Tree and Text Embedding",
        "authors": [
            "Yu Meng",
            "Yunyi Zhang",
            "Jiaxin Huang",
            "Yu Zhang",
            "Chao Zhang",
            "Jiawei Han"
        ],
        "date": "August 2020",
        "source": "KDD '20: Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Mining a set of meaningful topics organized into a hierarchy is intuitively appealing since topic correlations are ubiquitous in massive text corpora. To account for potential hierarchical topic structures, hierarchical topic models generalize flat topic models by incorporating latent topic hierarchies into their generative modeling process. However, due to their purely unsupervised nature, the learned topic hierarchy often deviates from users' particular needs or interests. To guide the hierarchical topic discovery process with minimal user supervision, we propose a new task, Hierarchical Topic Mining, which takes a category tree described by category names only, and aims to mine a set of representative terms for each category from a text corpus to help a user comprehend his/her interested topics. We develop a novel joint tree and text embedding method along with a principled optimization procedure that allows simultaneous modeling of the category tree structure and the corpus generative process in the spherical space for effective category-representative term discovery. Our comprehensive experiments show that our model, named JoSH, mines a high-quality set of hierarchical topics with high efficiency and benefits weakly-supervised hierarchical text classification tasks.",
        "link": "https://dl.acm.org/doi/10.1145/3394486.3403242"
    },
    {
        "title": "Unsupervised behavioural mining and clustering for malware family identification",
        "authors": [
            "Khanh Huu The Dam",
            "Thomas Given-Wilson",
            "Axel Legay"
        ],
        "date": "March 2021",
        "source": "SAC '21: Proceedings of the 36th Annual ACM Symposium on Applied Computing",
        "abstract": "More accurate and advanced detection and classification of malware requires exploiting program behaviour and not merely syntactic or static features. One approach is to use system call dependency graphs (SCDGs) that represent the program behaviour by interactions with the system, and the relations between these interactions. These SCDGs have been used with supervised learning techniques to very accurately detect and classify malware. This works considers the unsupervised learning challenge of mining for common clusters of behaviour without a priori knowledge. This allows for clustering of similar programs by behaviour, that can then be used for either classification or further analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3412841.3441919"
    },
    {
        "title": "Text Mining for Evaluating Authors' Birth and Death Years",
        "authors": [
            "Dror Moghaz",
            "Yaakov Hacohen-Kerner",
            "Dov Gabbay"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "This article presents a unique method in text and data mining for finding the era, i.e., mining temporal data, in which an anonymous author was living. Finding this era can assist in the examination of a fake document or extracting the time period in which a writer lived. The study and the experiments concern Hebrew, and in some parts, Aramaic and Yiddish rabbinic texts. The rabbinic texts are undated and contain no bibliographic sections, posing an interesting challenge. This work proposes algorithms using key phrases and key words that allow the temporal organization of citations together with linguistic patterns. Based on these key phrases, key words, and the references, we established several types of “Iron-clad,” Heuristic and Greedy rules for estimating the years of birth and death of a writer in an interesting classification task. Experiments were conducted on corpora, including documents authored by 12, 24, and 36 rabbinic writers and demonstrated promising results.",
        "link": "https://dl.acm.org/doi/10.1145/3281631"
    },
    {
        "title": "Transfer Learning to Timed Text Based Video Classification Using CNN",
        "authors": [
            "Zenun Kastrati",
            "Ali Shariq Imran",
            "Arianit Kurti"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "Open educational video resources are gaining popularity with a growing number of massive open online courses (MOOCs). This has created a niche for content providers to adopt effective solutions in automatically organizing and structuring of educational resources for maximum visibility. Recent advances in deep learning techniques are proving useful in managing and classifying resources into appropriate categories. This paper proposes one such convolutional neural network (CNN) model for classifying video lectures in a MOOC setting using a transfer learning approach. The model uses a time-aligned text transcripts corresponding to video lectures from six broader subject categories. Video lectures and their corresponding transcript dataset is gathered from the Coursera MOOC platform. Two different CNN models are proposed: i) CNN based classification using embeddings learned from our MOOC dataset, ii) CNN based classification using transfer learning. Word embeddings generated from two well known state-of-the-art pre-trained models Word2Vec and GloVe, are used in the transfer learning approach for the second case. The proposed CNN models are evaluated using precision, recall, and F1 score and the obtained performance is compared with both conventional and deep learning classifiers. The proposed CNN models have an F1 score improvement of 10-22 percentage points over DNN and conventional classifiers",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326483"
    },
    {
        "title": "Evolutionary role mining in complex networks by ensemble clustering",
        "authors": [
            "Sarvenaz Choobdar",
            "Pedro Ribeiro",
            "Fernando Silva"
        ],
        "date": "April 2017",
        "source": "SAC '17: Proceedings of the Symposium on Applied Computing",
        "abstract": "The structural patterns in the neighborhood of nodes assign unique roles to the nodes. Mining the set of existing roles in a network provides a descriptive profile of the network and draws its general picture. This paper proposes a new method to determine structural roles in a dynamic network based on the current position of nodes and their historic behavior. We develop a temporal ensemble clustering technique to dynamically find groups of nodes, holding similar tempo-structural roles. We compare two weighting functions, based on age and distribution of data, to incorporate temporal behavior of nodes in the role discovery. To evaluate the performance of the proposed method, we assess the results from two points of view: 1) goodness of fit to current structure of the network; 2) consistency with historic data. We conduct the evaluation using different ensemble clustering techniques. The results on real world networks demonstrate that our method can detect tempo-structural roles that simultaneously depict the topology of a network and reflect its dynamics with high accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3019612.3019815"
    },
    {
        "title": "Analyzing used-car web listings via text mining",
        "authors": [
            "Ayhan Demiriz",
            "Fatma Cantaş"
        ],
        "date": "October 2017",
        "source": "IML '17: Proceedings of the 1st International Conference on Internet of Things and Machine Learning",
        "abstract": "Used car trade is one of the major components of the world economies. It is not uncommon to sell a car by placing an internet advertisement irrespective of the geography in these days. A typical content of an advertisement is usually composed of two parts namely the structured and the free text data. The structured data may include some information about the asking price, make, model, year, mileage of the car and the contact info. In most cases, seller may give important clues about the car's current conditions in the free text data where the title (head) of the advertisement can be included as free text too. This paper reports preliminary results from a text mining study conducted on 75K used car internet listings collected from two major car listing web sites in Turkey. As expected, the words and the phrases related to the description of the car are observed to be frequent. The leading concepts in the free text are found to be regarding how to describe the current condition of a car, for example \"no crash history\".",
        "link": "https://dl.acm.org/doi/10.1145/3109761.3109782"
    },
    {
        "title": "Text classification of Diseases Treated by Traditional Chinese Medicine Prescription based on machine learning",
        "authors": [
            "Hanqing Zhao",
            "Bo Han",
            "Chen Li"
        ],
        "date": "September 2020",
        "source": "ISAIMS '20: Proceedings of the 1st International Symposium on Artificial Intelligence in Medical Sciences",
        "abstract": "OBJECTIVE: To explore the application of machine learning in th e identification of diseases treated by traditional Chinese medicine prescriptions. METHODS: Based on the composition of the text document of Chinese medicine prescriptions, the prescriptions were divided into cough, headache and diarrhea. THUTCT were introduced to establish and train two machine learning text classification models, LibSVM and LibLinear, and the prescriptions to be deter mined were put into the model for classification and prediction. R ESULTS: The Precision rate of LibSVM model and Liblinear mod el were 0.7283 and 0.6690. Seven prescriptions were classified an d predicted, and the results were in line with expectations. CONCLUSION: THUCTC has good universality for the content of TCM prescriptions, high classification accuracy and fast testing speed, which is suitable for the text classification and discrimination research of TCM prescriptions for diseases.",
        "link": "https://dl.acm.org/doi/10.1145/3429889.3429895"
    },
    {
        "title": "Evaluation of retweet clustering method classification method using retweets on Twitter without text data",
        "authors": [
            "K. Uchida",
            "F. Toriumi",
            "T. Sakaki"
        ],
        "date": "August 2017",
        "source": "WI '17: Proceedings of the International Conference on Web Intelligence",
        "abstract": "Burst phenomena, which frequently occur on social media, are caused by such social events as flaming on the internet, elections, and natural disasters. To understand people's thoughts and feelings, we must classify their opinions from burst phenomena. Therefore, classification methods that categorize tweets are critical. However, since most classification methods focus on text mining, they cannot group tweets by topics because each tweet has poor linguistic similarities. We used a non-text-based classification method proposed by Baba et al. that groups tweets by topics, even if they have poor linguistic similarities, and verified its validity by comparing it with a text-based classification method in two different evaluations: qualitative and quantitative. In the qualitative evaluation part, we did a questionnaire survey and validated the suitability of the topic clusters created using both the non-and text-based methods. Since evaluating the similarity of every pair of tweets in each topic is difficult, we evaluated the similarity between sampled pairs in the survey and acquired more appropriate topic clustering results using the non-text-based method than the text-based method. In the quantitative evaluation part, we focused on the robustness of each method against data reduction. Many approaches analyze social media data, especially because collecting data from social media is comparatively easy. However, since collecting the whole data of burst phenomena is very costly due to the vast amounts of available social media data, robustness against data reduction is an important index to evaluate classification methods. With the non-text-based method, over 55% of the pairs of tweets in the same cluster were also included in the same cluster even when the data were reduced to 10% in all three of our example cases. In this paper, as a source we focus on Twitter, one of the most popular microblogging services. Using clustering to conduct detailed case analyses, we scrutinized three burst cases that include natural disasters and flaming on the internet and found that a non-text-based method more effectively classified tweets in burst phenomena than a text-based method.",
        "link": "https://dl.acm.org/doi/10.1145/3106426.3106451"
    },
    {
        "title": "Using Spark for Text Mining on Large Scale Liver Cancer Literature",
        "authors": [
            "Ming-Yen Lin",
            "Yu-Ju Lin",
            "Sue-Chen Hsueh"
        ],
        "date": "January 2021",
        "source": "BDET 2021: 2021 the 3rd International Conference on Big Data Engineering and Technology (BDET)",
        "abstract": "Cancer is one of the main causes of death. The number of known cancer to-date is more than one hundred. Liver cancer, ranked after trachea and lung cancer, has long been high in the cancer leading cause in Taiwan and even ranked as second in 2016. The number of scientific articles related to cancer proliferates every year. The number reaches as high as 20 million in PubMed so that discovering useful information from the massive collection is very difficult. In addition, using a single machine to sift through these articles is very time-consuming. Therefore, we present a big data analytic framework using the distributed Apache Spark platform for text mining in PubMed literature. We establish a prediction model for liver cancer articles so that researchers may effectively validate whether an article is related to liver cancer or not. Classification models in Spark MLlib including Linear Support Vector Machines (SVM), Logistic Regression, are used in our experiments. Relevancy to liver cancer is further confirmed by using MeSH (Medical Subject Headings) terms. Logistic regression is about 3 times faster than SVMs and the accuracy of both methods is close to 95% in the experiments using hold-out validation. When max_features is 500 and min_df ≤ 0.1 (or min_df = 1), the accuracy may reach 96%. In the experiments with K-fold cross-validation, the accuracy of SVMs methods is 96%. The experimental results show that that our prediction model may effectively classify liver cancer articles.",
        "link": "https://dl.acm.org/doi/10.1145/3474944.3474958"
    },
    {
        "title": "Automatic topic classification of test cases using text mining at an Android smartphone vendor",
        "authors": [
            "Junji Shimagaki",
            "Yasutaka Kamei",
            "Naoyasu Ubayashi",
            "Abram Hindle"
        ],
        "date": "October 2018",
        "source": "ESEM '18: Proceedings of the 12th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Background: An Android smartphone is an ecosystem of applications, drivers, operating system components, and assets. The volume of the software is large and the number of test cases needed to cover the functionality of an Android system is substantial. Enormous effort has been already taken to properly quantify \"what features and apps were tested and verified?\". This insight is provided by dashboards that summarize test coverage and results per feature. One method to achieve this is to manually tag or label test cases with the topic or function they cover, much like function points. At the studied Android smartphone vendor, tests are labelled with manually defined tags, so-called \"feature labels (FLs)\", and the FLs serve to categorize 100s to 1000s test cases into 10 to 50 groups. Aim: Unfortunately for developers, manual assignment of FLs to 1000s of test cases is a time consuming task, leading to inaccurately labeled test cases, which will render the dashboard useless. We created an automated system that suggests tags/labels to the developers for their test cases rather than manual labeling. Method: We use machine learning models to predict and label the functionality tested by 10,000 test cases developed at the company. Results: Through the quantitative experiments, our models achieved acceptable F-1 performance of 0.3 to 0.88. Also through the qualitative studies with expert teams, we showed that the hierarchy and path of tests was a good predictor of a feature's label. Conclusions: We find that this method can reduce tedious manual effort that software developers spent classifying test cases, while providing more accurate classification results.",
        "link": "https://dl.acm.org/doi/10.1145/3239235.3268927"
    },
    {
        "title": "To apply Data Mining for Classification of Crowd sourced Software Requirements",
        "authors": [
            "Soonh Taj",
            "Qasim Arain",
            "Imran Memon",
            "Asma Zubedi"
        ],
        "date": "April 2019",
        "source": "ICSIE '19: Proceedings of the 8th International Conference on Software and Information Engineering",
        "abstract": "Now a day's main focus of developers is to build quality software that works according to customer needs and for this reason it is necessary to gather right requirements as requirement elicitation is the critical step that impacts on the success of software project as misinterpreted requirements leads to the failure of software project. By keeping this in mind a research is carried out on improving requirements elicitation process and automating the process of classifying requirements. In this research, a model is proposed which will help in this scenario for requirements elicitation and requirement classification. This paper presents a model in which crowd sourcing approach is used so that customers, end users, stakeholders, developers and software engineers can make active participation for requirement elicitation process and requirements gathered using crowdsourcing approach are used by model for classification process i.e. classification of requirements into functional and non-functional requirements. For the proof of proposed model a case study is conducted. Results of case study provided the usefulness and efficiency of proposed model for classification of crowd sourced software requirements.",
        "link": "https://dl.acm.org/doi/10.1145/3328833.3328837"
    },
    {
        "title": "Text classification using Fuzzy TF-IDF and Machine Learning Models",
        "authors": [
            "Mariem Bounabi",
            "Karim El Moutaouakil",
            "Khalid Satori"
        ],
        "date": "October 2019",
        "source": "BDIoT'19: Proceedings of the 4th International Conference on Big Data and Internet of Things",
        "abstract": "The representation of the information has an important impact on the text classification task. Several weighting methods were proposed in the literature, and the term frequency-inverse term frequency (TFIDF), the most know on the text treatment field. The FTF-IDF is a vector representation where the components of the TFIDF are presented as inputs to the Fuzzy Inference System (FIS). In this work, we compare several Machin Learning algorithms such as Naïve Bayes and its derivatives, SVM and Random forest classifiers, using the FTF-IDF representation. To improve the quality of the used classifiers, we call sum attribute selection methods. The recognition rate, for the tested systems, is satisfied, where the system based on naïve Bayes classifier, the FTF-IDF weighting terms, and the info gain select attributes method gives 98.7% as accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3372938.3372956"
    },
    {
        "title": "Text Mining in Cybersecurity: A Systematic Literature Review",
        "authors": [
            "Luciano Ignaczak",
            "Guilherme Goldschmidt",
            "Cristiano André Da Costa",
            "Rodrigo Da Rosa Righi"
        ],
        "date": null,
        "source": "ACM Computing Surveys",
        "abstract": "The growth of data volume has changed cybersecurity activities, demanding a higher level of automation. In this new cybersecurity landscape, text mining emerged as an alternative to improve the efficiency of the activities involving unstructured data. This article proposes a Systematic Literature Review (SLR) to present the application of text mining in the cybersecurity domain. Using a systematic protocol, we identified 2,196 studies, out of which 83 were summarized. As a contribution, we propose a taxonomy to demonstrate the different activities in the cybersecurity domain supported by text mining. We also detail the strategies evaluated in the application of text mining tasks and the use of neural networks to support activities involving unstructured data. The work also discusses text classification performance aiming its application in real-world solutions. The SLR also highlights open gaps for future research, such as the analysis of non-English content and the intensification in the usage of neural networks.",
        "link": "https://dl.acm.org/doi/10.1145/3462477"
    },
    {
        "title": "Using Text Mining to Analyze the Financial Patents",
        "authors": [
            "Yung-Feng Lu",
            "Jia-Lang Xu",
            "Mu-Yen Chen"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "With the advancement and development of science and technology, Nowadays society is the development of information technology, cloud computing, mobile Internet, Internet of Things, block chain and information technology.The combination of finance and technology has made financial technology terms gradually emerging.. This research analyzes the patents of financial technology. However, Taiwan's financial technology patents are mostly G06Q-20 (payment plan, architecture or agreement), G06Q-30 (commercial, such as marketing, shopping, payment, auction or e-commerce). G06Q-40 (financial, investment or tax treatment, insurance).The research use text mining technology to analyze and understand Taiwan's trends and developments in financial technology. The research results show that G06Q-20 patents are biased towards information security and mobile payment. G06Q-30 patents favor shopping websites and advertising. G06Q-40 patents are biased towards diversity, such as securities, insurance, finance, and stocks.",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326478"
    },
    {
        "title": "Identifying Online Profiles of Distance Learning Students Using Data Mining Techniques",
        "authors": [
            "Osama Islam",
            "Muazzam Siddiqui",
            "Naif Radi Aljohani"
        ],
        "date": "October 2019",
        "source": "ICDTE '19: Proceedings of the 3rd International Conference on Digital Technology in Education",
        "abstract": "Educational data has grown over the years with the increased use of technology within educational environments. This has led to a huge amount of data being stored in various data sources representing the student, his/her activities, and other aspects relevant to the learning process. To meet this analytical need, Educational Data Mining (EDM) has emerged to assist educational institutions in identifying key benefits such as students at risk, the level of student engagement or predicting student performance. The aim of this research was to explore the various aspects of student interaction data using data mining techniques to identify relevant patterns of behaviors and possible key attributes that have higher degrees of influence on distance learning students. The main findings identified several patterns of user online profiles based on a set of adopted learning strategies, the research proposed a framework for analyzing such interaction data based on R and Hadoop platforms to correlate online profiles with student performance.",
        "link": "https://dl.acm.org/doi/10.1145/3369199.3369249"
    },
    {
        "title": "Extremely Fast Decision Tree Mining for Evolving Data Streams",
        "authors": [
            "Albert Bifet",
            "Jiajin Zhang",
            "Wei Fan",
            "Cheng He",
            "Jianfeng Zhang",
            "Jianfeng Qian",
            "Geoff Holmes",
            "Bernhard Pfahringer"
        ],
        "date": "August 2017",
        "source": "KDD '17: Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
        "abstract": "Nowadays real-time industrial applications are generating a huge amount of data continuously every day. To process these large data streams, we need fast and efficient methodologies and systems. A useful feature desired for data scientists and analysts is to have easy to visualize and understand machine learning models. Decision trees are preferred in many real-time applications for this reason, and also, because combined in an ensemble, they are one of the most powerful methods in machine learning. In this paper, we present a new system called STREAMDM-C++, that implements decision trees for data streams in C++, and that has been used extensively at Huawei. Streaming decision trees adapt to changes on streams, a huge advantage since standard decision trees are built using a snapshot of data, and can not evolve over time. STREAMDM-C++ is easy to extend, and contains more powerful ensemble methods, and a more efficient and easy to use adaptive decision trees. We compare our new implementation with VFML, the current state of the art implementation in C, and show how our new system outperforms VFML in speed using less resources.",
        "link": "https://dl.acm.org/doi/10.1145/3097983.3098139"
    },
    {
        "title": "Data miners' little helper: data transformation activity cues for cluster analysis on document collections",
        "authors": [
            "Tania Cerquitelli",
            "Evelina Di Corso",
            "Francesco Ventura",
            "Silvia Chiusano"
        ],
        "date": "June 2017",
        "source": "WIMS '17: Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "In this paper we propose a new self-learning engine to streamline the analytics process, as it enables analysts to mine massive data repositories with minimal user intervention. In the context of cluster analysis on a collection of documents this new system, named SELF-DATA (SELF-learning DAta TrAnsformation), suggests to the analyst how to configure the whole mining process for a given dataset. SELF-DATA relies on an engine exploring different data weighting schemas (e.g., normalized term frequencies) and data transformation methods (e.g., PCA) before applying the cluster analysis, evaluating and comparing solutions through different quality indices (e.g., weighted Silhouette), and presenting the k-top solutions to the analyst. SELF-DATA will also include a knowledge base storing results of experiments on previously processed datasets, and a classification algorithm trained on the knowledge base content to forecast the best configuration for the whole mining process for an unexplored dataset. The first development of SELF-DATA running on Apache Spark has been validated on 5 collections of documents. Experimental results highlight that TF-IDF and logarithmic entropy are effective to measure item relevance with sparse datasets, and the LSI method outperforms PCA with a large dictionary.",
        "link": "https://dl.acm.org/doi/10.1145/3102254.3102288"
    },
    {
        "title": "Interactive Visual Graph Mining and Learning",
        "authors": [
            "Ryan A. Rossi",
            "Nesreen K. Ahmed",
            "Rong Zhou",
            "Hoda Eldardiry"
        ],
        "date": null,
        "source": "ACM Transactions on Intelligent Systems and Technology",
        "abstract": "This article presents a platform for interactive graph mining and relational machine learning called GraphVis. The platform combines interactive visual representations with state-of-the-art graph mining and relational machine learning techniques to aid in revealing important insights quickly as well as learning an appropriate and highly predictive model for a particular task (e.g., classification, link prediction, discovering the roles of nodes, and finding influential nodes). Visual representations and interaction techniques and tools are developed for simple, fast, and intuitive real-time interactive exploration, mining, and modeling of graph data. In particular, we propose techniques for interactive relational learning (e.g., node/link classification), interactive link prediction and weighting, role discovery and community detection, higher-order network analysis (via graphlets, network motifs), among others. GraphVis also allows for the refinement and tuning of graph mining and relational learning methods for specific application domains and constraints via an end-to-end interactive visual analytic pipeline that learns, infers, and provides rapid interactive visualization with immediate feedback at each change/prediction in real-time. Other key aspects include interactive filtering, querying, ranking, manipulating, exporting, as well as tools for dynamic network analysis and visualization, interactive graph generators (including new block model approaches), and a variety of multi-level network analysis techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3200764"
    },
    {
        "title": "Data Mining and Opinion Mining: A Tool in Educational Context",
        "authors": [
            "Myriam Peñafiel",
            "Stefanie Vásquez",
            "Diego Vásquez",
            "Juan Zaldumbide",
            "Sergio Luján-Mora"
        ],
        "date": "July 2018",
        "source": "ICoMS '18: Proceedings of the 2018 1st International Conference on Mathematics and Statistics",
        "abstract": "The use of the web as a universal communication platform generates large volumes of data (Big data), which in many cases, need to be processed so that they can become useful knowledge in face of the sceptics who have doubts about the credibility of such information. The use of web data that comes from educational contexts needs to be addressed, since that large amount of unstructured information is not being valued, losing valuable information that can be used. To solve this problem, we propose the use of data mining techniques such as sentiment analysis to validate the information that comes from the educational platforms. The objective of this research is to propose a methodology that allows the user to apply sentiment analysis in a simple way, because although some researchers have done it, very few do with data in the educational context. The results obtained prove that the proposal can be used in similar cases.",
        "link": "https://dl.acm.org/doi/10.1145/3274250.3274263"
    },
    {
        "title": "Research Hotspots Mining and Visualized Analysis Based on Linking Cluster and K-Core Decomposition",
        "authors": [
            "Ying Cai",
            "Fang Huang",
            "Shuai Wang",
            "Hao Zhang",
            "Chunxiu Du"
        ],
        "date": "May 2018",
        "source": "ICDPA 2018: Proceedings of the International Conference on Data Processing and Applications",
        "abstract": "In this paper, we focus on research hotspots mining with knowledge semantic features on discipline topic word networks, which are extracted from scientific and technical documents. For the topic networks, a novel method of combining linking community clustering with k-core decomposition is proposed to mine the hotspot community with knowledge hierarchy structure. And as a post-processing, the hierarchical community identification is implemented by k-core decomposition in order to discover the internal connection with knowledge semantic hierarchy in the research hotspot. Finally, the community density and visualization method is utilized to analyze the core and sub- problem in the research hotspot community. In experiments, the topic networks for a single discipline and interdisciplinary research hotspot are taken as examples of analysis, and the results show the proposed approach can effectively realize the objective of mining and analyzing the discipline research hotspot on the academic topic word network.",
        "link": "https://dl.acm.org/doi/10.1145/3224207.3224216"
    },
    {
        "title": "The Application of Data Mining Algorithms in the Construction of Travel Recommendation System",
        "authors": [
            "Chao Lou",
            "Zhaonan Mu",
            "Mengzhu Liu"
        ],
        "date": "January 2021",
        "source": "CONF-CDS 2021: The 2nd International Conference on Computing and Data Science",
        "abstract": "Photos shared by users of online social network often associated with tags, texts, geographic and time information. These data are ideal research resource which can be used by researchers to observe people's interests and behaviors. They can also be used to analyze people's travel behaviors and interests because it can reflect people's travel activities and experiences directly. This project aimed to consolidate people into groups based on their travel preferences of travel season, travel time span, scenery type of travel location and then analyze popular travel places for different people groups. The travel preferences were analyzed from extracted data applying PHP language and Naive Bayes classification approach. According to the analyzed data, people were divided into groups. Then, DBSCAN clustering was used to get popular locations based on the different groups. As a result, 80 thousand photo data of 5 cities have been collected from Flickr by Phython. It has been showed that people's preference of season in most of the mined cities was spring and summer and the preference of time span was short. Moreover, mined people has been divided into 16 groups according to their preference and based on the groups popular locations have been calculated by DBSCAN clustering. Theses locations have been provided in a recommendation website which was built in this project. For the future work, mining Flickr data associated with other travel social network sites could be a good way to build a more effective recommendation system.",
        "link": "https://dl.acm.org/doi/10.1145/3448734.3450477"
    },
    {
        "title": "Compare Machine Learning Models in Text Classification Using Steam User Reviews",
        "authors": [
            "Youchen Miao",
            "Zeyu Jin",
            "Yumeng Zhang",
            "Yuchen Chen",
            "Junren Lai"
        ],
        "date": "November 2021",
        "source": "ICSED '21: Proceedings of the 2021 3rd International Conference on Software Engineering and Development",
        "abstract": "Text Classification and Sentiment Analysis of game reviews are viewed as important parts in not only academic fields but also in game studies. In this paper, with more than 400 thousand game reviews on Steam platform, we preprocess the data using different libraries (sklearn, nltk, and spaCy) and use them as inputs to build three sentiment classification models based on different algorithms (Naive Bayes, SVM, and Random Forest). In contrast to previous studies that only focus on different sentiment analysis models, our paper also highlights the use of different APIs to preprocess the data and their corresponding model performance. The results show that no matter which API we choose, Random Forest models always perform the best. However, in terms of training time, Naive Bayes is the fastest. This work can be used to apply grid search for researchers to automatically find the optimum API before conducting sentiment analysis in the future.",
        "link": "https://dl.acm.org/doi/10.1145/3507473.3507480"
    },
    {
        "title": "Lost in Transduction: Transductive Transfer Learning in Text Classification",
        "authors": [
            "Alejandro Moreo",
            "Andrea Esuli",
            "Fabrizio Sebastiani"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Obtaining high-quality labelled data for training a classifier in a new application domain is often costly. Transfer Learning\n(a.k.a. “Inductive Transfer”) tries to alleviate these costs by transferring, to the “target” domain of interest, knowledge available from a different “source” domain. In transfer learning the lack of labelled information from the target domain is compensated by the availability at training time of a set of unlabelled examples from the target distribution. Transductive Transfer Learning denotes the transfer learning setting in which the only set of target documents that we are interested in classifying is known and available at training time. Although this definition is indeed in line with Vapnik’s original definition of “transduction”, current terminology in the field is confused. In this article, we discuss how the term “transduction” has been misused in the transfer learning literature, and propose a clarification consistent with the original characterization of this term given by Vapnik. We go on to observe that the above terminology misuse has brought about misleading experimental comparisons, with inductive transfer learning methods that have been incorrectly compared with transductive transfer learning methods. We then, give empirical evidence that the difference in performance between the inductive version and the transductive version of a transfer learning method can indeed be statistically significant (i.e., that knowing at training time the only data one needs to classify indeed gives an advantage). Our clarification allows a reassessment of the field, and of the relative merits of the major, state-of-the-art algorithms for transfer learning in text classification.",
        "link": "https://dl.acm.org/doi/10.1145/3453146"
    },
    {
        "title": "Machine Learning on Mining Potential Adverse Drug Reactions for Pharmacovigilance",
        "authors": [
            "Ke Jia"
        ],
        "date": "July 2021",
        "source": "DSIT 2021: 2021 4th International Conference on Data Science and Information Technology",
        "abstract": "Drugs are related to the lives and health of patients. Mining and discovering potential adverse drug reactions from a large number of unstructured texts play an important role in drug research and pharmacovigilance. It is a hot research issue in the field of biomedicine in recent years. A large number of scholars have discovered a large amount of information on adverse drug reactions from different data sources through effective machine learning methods. This paper reviews the research progress in recent years to provide a reference for future research.",
        "link": "https://dl.acm.org/doi/10.1145/3478905.3478964"
    },
    {
        "title": "TACAM: Topic And Context Aware Argument Mining",
        "authors": [
            "Michael Fromm",
            "Evgeniy Faerman",
            "Thomas Seidl"
        ],
        "date": "October 2019",
        "source": "WI '19: IEEE/WIC/ACM International Conference on Web Intelligence",
        "abstract": "In this work we address the problem of argument search. The purpose of argument search is the distillation of pro and contra arguments for requested topics from large text corpora. In previous works, the usual approach is to use a standard search engine to extract text parts which are relevant to the given topic and subsequently use an argument recognition algorithm to select arguments from them. The main challenge in the argument recognition task, which is also known as argument mining, is that often sentences containing arguments are structurally similar to purely informative sentences without any stance about the topic. In fact, they only differ semantically. Most approaches use topic or search term information only for the first search step and therefore assume that arguments can be classified independently of a topic. We argue that topic information is crucial for argument mining, since the topic defines the semantic context of an argument. Precisely, we propose different models for the classification of arguments, which take information about a topic of an argument into account. Moreover, to enrich the context of a topic and to let models understand the context of the potential argument better, we integrate information from different external sources such as Knowledge Graphs or pre-trained NLP models. Our evaluation shows that considering topic information, especially in connection with external information, provides a significant performance boost for the argument mining task.",
        "link": "https://dl.acm.org/doi/10.1145/3350546.3352506"
    },
    {
        "title": "Text mining for incoming tasks based on the urgency/importance factors and task classification using machine learning tools",
        "authors": [
            "Yasser Ali Alshehri"
        ],
        "date": "March 2020",
        "source": "ICCDA 2020: Proceedings of the 2020 the 4th International Conference on Compute and Data Analysis",
        "abstract": "In workplaces, there is a massive amount of unstructured data from different sources. In this paper, we present a case study that explains how can through communications between employees, we can help to prioritize tasks requests to increase the efficiency of their works for both technical and non-technical workers. This involves managing daily incoming tasks based on their level of urgency and importance.To allow all workers to utilize the urgency-importance matrix as a time-management tool, we need to automate this tool. The textual content of incoming tasks are analyzed, and metrics related to urgency and importance are extracted. A third factor (i.e., the response variable) is defined based on the two input variables (urgency and importance). Then, machine learning applied to the data to predict the class of incoming tasks based on data outcome desired. We used ordinal regression, neural networks, and decision tree algorithms to predict the four levels of task priority. We measure the performance of all using recalls, precisions, and F-scores. All classifiers perform higher than 89% in terms of all measures.",
        "link": "https://dl.acm.org/doi/10.1145/3388142.3388153"
    },
    {
        "title": "Text mining for malware classification using multivariate all repeated patterns detection",
        "authors": [
            "Konstantinos F. Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2019",
        "source": "ASONAM '19: Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Mobile phones have become nowadays a commodity to the majority of people. Using them, people are able to access the world of Internet and connect with their friends, their colleagues at work or even unknown people with common interests. This proliferation of the mobile devices has also been seen as an opportunity for the cyber criminals to deceive smartphone users and steel their money directly or indirectly, respectively, by accessing their bank accounts through the smartphones or by blackmailing them or selling their private data such as photos, credit card data, etc. to third parties. This is usually achieved by installing malware to smartphones masking their malevolent payload as a legitimate application and advertise it to the users with the hope that mobile users will install it in their devices. Thus, any existing application can easily be modified by integrating a malware and then presented it as a legitimate one. In response to this, scientists have proposed a number of malware detection and classification methods using a variety of techniques. Even though, several of them achieve relatively high precision in malware classification, there is still space for improvement. In this paper, we propose a text mining all repeated pattern detection method which uses the decompiled files of an application in order to classify a suspicious application into one of the known malware families. Based on the experimental results using a real malware dataset, the methodology tries to correctly classify (without any misclassification) all randomly selected malware applications of 3 categories with 3 different families each.",
        "link": "https://dl.acm.org/doi/10.1145/3341161.3350841"
    },
    {
        "title": "Comparative Study between Traditional Machine Learning and Deep Learning Approaches for Text Classification",
        "authors": [
            "Cannannore Nidhi Kamath",
            "Syed Saqib Bukhari",
            "Andreas Dengel"
        ],
        "date": "August 2018",
        "source": "DocEng '18: Proceedings of the ACM Symposium on Document Engineering 2018",
        "abstract": "In this contemporaneous world, it is an obligation for any organization working with documents to end up with the insipid task of classifying truckload of documents, which is the nascent stage of venturing into the realm of information retrieval and data mining. But classification of such humongous documents into multiple classes, calls for a lot of time and labor. Hence a system which could classify these documents with acceptable accuracy would be of an unfathomable help in document engineering. We have created multiple classifiers for document classification and compared their accuracy on raw and processed data. We have garnered data used in a corporate organization as well as publicly available data for comparison. Data is processed by removing the stop-words and stemming is implemented to produce root words. Multiple traditional machine learning techniques like Naive Bayes, Logistic Regression, Support Vector Machine, Random forest Classifier and Multi-Layer Perceptron are used for classification of documents. Classifiers are applied on raw and processed data separately and their accuracy is noted. Along with this, Deep learning technique such as Convolution Neural Network is also used to classify the data and its accuracy is compared with that of traditional machine learning techniques. We are also exploring hierarchical classifiers for classification of classes and subclasses. The system classifies the data faster and with better accuracy than if done manually. The results are discussed in the results and evaluation section.",
        "link": "https://dl.acm.org/doi/10.1145/3209280.3209526"
    },
    {
        "title": "Identification of Overpricing in the Purchase of Medication by the Federal Government of Brazil, Using Text Mining and Clustering Based on Ontology",
        "authors": [
            "Marco Aurelio O. S. Correa",
            "Adriano Galindo Leal"
        ],
        "date": "August 2018",
        "source": "ICCBDC '18: Proceedings of the 2018 2nd International Conference on Cloud and Big Data Computing",
        "abstract": "Increasing the transparency level in his actions and spending is one of the primary duties of the Brazilian Federal Government. The creation of laws that oblige full disclose of all its expenditures through transparency portals enables citizens to supervise all government entities. However, only the dissemination of these data, without a definite standard or the availability of data analysis tools, does not guarantee that the citizen is empowered to play his role. Therefore hence, the objective of this work is to identify overprice in the acquisition of products purchased by the federal government of Brazil using the unstructured data available on the Transparency Portal. The last two-years' worth of purchasing data, available in the Transparency Portal, were extracted, processed and stored. Due to his diverse nature and high volume of data, this study focused only on medicines purchased by the Ministry of Health. Ontology-based text mining and clustering techniques were applied for automatic identification and classification of products. The processing of this information was done through text mining and clustering, based on the ontology registered in another database of the Brazilian government. Because of this work, a consolidated price base per medication was created to allow the identification of distortions in prices practised, facilitating the identification of cases that merit further investigation to unravel fraud to the treasury.",
        "link": "https://dl.acm.org/doi/10.1145/3264560.3264569"
    },
    {
        "title": "Computational Estimation by Scientific Data Mining with Classical Methods to Automate Learning Strategies of Scientists",
        "authors": [
            "Aparna S. Varde"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Experimental results are often plotted as 2-dimensional graphical plots (aka graphs) in scientific domains depicting dependent versus independent variables to aid visual analysis of processes. Repeatedly performing laboratory experiments consumes significant time and resources, motivating the need for computational estimation. The goals are to estimate the graph obtained in an experiment given its input conditions, and to estimate the conditions that would lead to a desired graph. Existing estimation approaches often do not meet accuracy and efficiency needs of targeted applications. We develop a computational estimation approach called AutoDomainMine that integrates clustering and classification over complex scientific data in a framework so as to automate classical learning methods of scientists. Knowledge discovered thereby from a database of existing experiments serves as the basis for estimation. Challenges include preserving domain semantics in clustering, finding matching strategies in classification, striking a good balance between elaboration and conciseness while displaying estimation results based on needs of targeted users, and deriving objective measures to capture subjective user interests. These and other challenges are addressed in this work. The AutoDomainMine approach is used to build a computational estimation system, rigorously evaluated with real data in Materials Science. Our evaluation confirms that AutoDomainMine provides desired accuracy and efficiency in computational estimation. It is extendable to other science and engineering domains as proved by adaptation of its sub-processes within fields such as Bioinformatics and Nanotechnology.",
        "link": "https://dl.acm.org/doi/10.1145/3502736"
    },
    {
        "title": "Where is the road for issue reports classification based on text mining?",
        "authors": [
            "Qiang Fan",
            "Yue Yu",
            "Gang Yin",
            "Tao Wang",
            "Huaimin Wang"
        ],
        "date": "November 2017",
        "source": "ESEM '17: Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Currently, open source projects receive various kinds of issues daily, because of the extreme openness of Issue Tracking System (ITS) in GitHub. ITS is a labor-intensive and time-consuming task of issue categorization for project managers. However, a contributor is only required a short textual abstract to report an issue in GitHub. Thus, most traditional classification approaches based on detailed and structured data (e.g., priority, severity, software version and so on) are difficult to adopt. In this paper, issue classification approaches on a large-scale dataset, including 80 popular projects and over 252,000 issue reports collected from GitHub, were investigated. First, four traditional text-based classification methods and their performances were discussed. Semantic perplexity (i.e., an issues description confuses bug-related sentences with nonbug-related sentences) is a crucial factor that affects the classification performances based on quantitative and qualitative study. Finally, A two-stage classifier framework based on the novel metrics of semantic perplexity of issue reports was designed. Results show that our two-stage classification can significantly improve issue classification performances.",
        "link": "https://dl.acm.org/doi/10.1109/ESEM.2017.19"
    },
    {
        "title": "Feature-based Facebook reviews process model for e-management using data mining",
        "authors": [
            "Anish Kumar Varudharajulu",
            "Yongsheng Ma"
        ],
        "date": "January 2019",
        "source": "IC4E '19: Proceedings of the 10th International Conference on E-Education, E-Business, E-Management and E-Learning",
        "abstract": "The data generated from online communication acts as potential gold mines for discovering knowledge for researchers. A large amount of data is also generated in the form of web documents, emails, blogs, and feedback, etc. Text analytics is being significantly employed to mine important information. Opinion mining is the process of extracting human thoughts and perceptions from unstructured texts. The showstopper for designing an opinion mining system for analyzing reviews arise from the fact that customer reviews are often noisy. These reviews are informally written. In addition, they are subjected to spelling mistakes, grammatical errors, improper punctuation and irrational capitalization. This paper focuses on analyzing the different classification and clustering algorithms aimed at extracting and consolidating opinions of customers from social media sites like Facebook, Twitter and through surveys, at multiple levels of granularity to monitor and measure customer satisfaction. Ours is an automated approach, in which the system aids in the process of knowledge assimilation for knowledge-based building and also performs the analytics. Domain experts ratify the knowledge base and also provide training datasets for the system to intuitively gather more instances for ratification. The system identifies opinion expressions as phrases containing opinion words, opinionated features and also opinion modifiers. These expressions are categorized as positive, negative or neutral. Opinion expressions are identified and categorized using localized linguistic techniques. Opinions can be congregated at any desired level of specificity i.e. feature level or product level, user level or service level, etc. We have developed a system based on this approach, which provides the user with a platform to analyze opinion expressions crawled from a set of pre-defined datasets.",
        "link": "https://dl.acm.org/doi/10.1145/3306500.3306514"
    },
    {
        "title": "Discriminative Topic Mining via Category-Name Guided Text Embedding",
        "authors": [
            "Yu Meng",
            "Jiaxin Huang",
            "Guangyuan Wang",
            "Zihan Wang",
            "Chao Zhang",
            "Yu Zhang",
            "Jiawei Han"
        ],
        "date": "April 2020",
        "source": "WWW '20: Proceedings of The Web Conference 2020",
        "abstract": "Mining a set of meaningful and distinctive topics automatically from massive text corpora has broad applications. Existing topic models, however, typically work in a purely unsupervised way, which often generate topics that do not fit users’ particular needs and yield suboptimal performance on downstream tasks. We propose a new task, discriminative topic mining, which leverages a set of user-provided category names to mine discriminative topics from text corpora. This new task not only helps a user understand clearly and distinctively the topics he/she is most interested in, but also benefits directly keyword-driven classification tasks. We develop CatE, a novel category-name guided text embedding method for discriminative topic mining, which effectively leverages minimal user guidance to learn a discriminative embedding space and discover category representative terms in an iterative manner. We conduct a comprehensive set of experiments to show that CatE mines high-quality set of topics guided by category names only, and benefits a variety of downstream applications including weakly-supervised classification and lexical entailment direction identification.",
        "link": "https://dl.acm.org/doi/10.1145/3366423.3380278"
    },
    {
        "title": "Comparative Study of Heart Disease Diagnosis Using Top Ten Data Mining Classification Algorithms",
        "authors": [
            "I. Ketut Agung Enriko"
        ],
        "date": "June 2019",
        "source": "ICFET '19: Proceedings of the 5th International Conference on Frontiers of Educational Technologies",
        "abstract": "Data mining has been used for many purposes, especially for prediction system. In healthcare, data mining algorithms often used in disease diagnosis. Meanwhile, heart disease is known as a primary cause of death over the years. Many studies have been performed in heart disease diagnosis using data mining methods. There are some popular data mining algorithms that can be used in heart disease diagnosis, for example, k-Nearest Neighbor, CART, and AdaBoost. The algorithms are used to analyze a sample of cardiovascular patients data and predict the heart disease type that they suffer. Some parameters are taken from the patient, including EKG morphology, blood pressure, and information about the existence of chest pain, shortness of breath, palpitation, and cold sweat. In this study, medical records data are collected from Harapan Kita Hospital and utilized as a dataset sample in this research. Top ten data mining classification algorithms are used in diagnosing heart disease from Harapan Kita Hospital data and examining their performance by checking the accuracy and speed.",
        "link": "https://dl.acm.org/doi/10.1145/3338188.3338220"
    },
    {
        "title": "Text Mining Approach for Identifying Research Trends",
        "authors": [
            "Snezhana Sulova"
        ],
        "date": "June 2021",
        "source": "CompSysTech '21: Proceedings of the 22nd International Conference on Computer Systems and Technologies",
        "abstract": "With the increase of unstructured data, the issues connected with automatic text processing, the categorization of documents and the discovery of topics have become objects of growing interest. In order to improve the process of grouping and processing research publications, we would like to propose a method based upon natural language processing. It is based on text mining technologies which aim to identify key tendencies in documents. It processes the content of publications by clustering and identifies the topics of each identified group. This analysis helps by identifying key tendencies as well as discovering emerging new areas of research. Publications from the research literature database, Scopus, were used to test the approach. The topic of the publications is “the application of digital technologies in the logistics business”. The experiments were completed using the RapidMiner Studio software.",
        "link": "https://dl.acm.org/doi/10.1145/3472410.3472433"
    },
    {
        "title": "Assessment of Vulnerability Severity using Text Mining",
        "authors": [
            "Georgios Spanos",
            "Lefteris Angelis",
            "Dimitrios Toloudis"
        ],
        "date": "September 2017",
        "source": "PCI '17: Proceedings of the 21st Pan-Hellenic Conference on Informatics",
        "abstract": "Software1 vulnerabilities are closely associated with information systems security, a major and critical field in today's technology. Vulnerabilities constitute a constant and increasing threat for various aspects of everyday life, especially for safety and economy, since the social impact from the problems that they cause is complicated and often unpredictable. Although there is an entire research branch in software engineering that deals with the identification and elimination of vulnerabilities, the growing complexity of software products and the variability of software production procedures are factors contributing to the ongoing occurrence of vulnerabilities, Hence, another area that is being developed in parallel focuses on the study and management of the vulnerabilities that have already been reported and registered in databases. The information contained in such databases includes, a textual description and a number of metrics related to vulnerabilities. The purpose of this paper is to investigate to what extend the assessment of the vulnerability severity can be inferred directly from the corresponding textual description, or in other words, to examine the informative power of the description with respect to the vulnerability severity. For this purpose, text mining techniques, i.e. text analysis and three different classification methods (decision trees, neural networks and support vector machines) were employed. The application of text mining to a sample of 70,678 vulnerabilities from a public data source shows that the description itself is a reliable and highly accurate source of information for vulnerability prioritization.",
        "link": "https://dl.acm.org/doi/10.1145/3139367.3139390"
    },
    {
        "title": "Clustering Algorithms for Spatial Data Mining",
        "authors": [
            "Chetashri Bhadane",
            "Ketan Shah"
        ],
        "date": "April 2020",
        "source": "ICGDA '20: Proceedings of the 2020 3rd International Conference on Geoinformatics and Data Analysis",
        "abstract": "With the advances in mobile and wireless technologies, there has been a rise in applications that track and share the users' geospatial data. People use several social networking sites such as Twitter, Facebook and Flickr, where they share their status updates. With the integration of Global Positioning System (GPS) with mobile phones, it is now possible to share one's locations on these social networks. GPS allows us to record and track a person's movement along with the timestamp. The data set obtained from these GPS logs is vast and is widely used to analyze the users' movement patterns. Specifically, we can find out significant locations based on the number of users present at that location and the time spent by them at such places. Once significant places have been identified, it is also possible to identify the semantic importance of these locations. This paper presents an overview of the clustering techniques used to find important places of interest using large GPS based mobility datasets. Four clustering algorithms, K-Means, DBSCAN, OPTICS and Hierarchical, are implemented, and performance is tested using real-time data of 50 users collected over 2--5 years. Performance summary depicts that K-Means and DBSCAN perform well for spatial data.",
        "link": "https://dl.acm.org/doi/10.1145/3397056.3397068"
    },
    {
        "title": "Feature Selection based Arabic Text Classification using Different Machine Learning Algorithms: Comparative Study",
        "authors": [
            "Sakina Rim Bennabi",
            "Zakaria Elberrichi"
        ],
        "date": "June 2020",
        "source": "ICIST '20: Proceedings of the 10th International Conference on Information Systems and Technologies",
        "abstract": "Feature selection is a method of data pre-processing widely used when mining large data, such as textual classification. Several studies have been conducted to compare the different methods of feature selection applied to corpora in English. Unfortunately, a small number of works concern the Arabic language. This article aims to present a comparative study of different feature selection techniques including: Chi2, the ANOVA method and mutual information, applied on a corpus in Arabic language, while also diversifying the machine learning algorithms (Naive Bayes, SVM and KNN). This experimental study has shown in general that reducing dimensionality with feature selection techniques has slightly affected the performance of textual classification, reducing the size of the corpus by up to 1%.",
        "link": "https://dl.acm.org/doi/10.1145/3447568.3448531"
    },
    {
        "title": "Intelligent Data Analysis using Optimized Support Vector Machine Based Data Mining Approach for Tourism Industry",
        "authors": [
            "Ms Promila Sharma",
            "Uma Meena",
            "Girish Kumar Sharma"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Data analysis involves the deployment of sophisticated approaches from data mining methods, information theory, and artificial intelligence in various fields like tourism, hospitality, and so on for the extraction of knowledge from the gathered and preprocessed data. In tourism, pattern analysis or data analysis using classification is significant for finding the patterns that represent new and potentially useful information or knowledge about the destination and other data. Several data mining techniques are introduced for the classification of data or patterns. However, overfitting, less accuracy, local minima, sensitive to noise are the drawbacks in some existing data mining classification methods. To overcome these challenges, Support vector machine with Red deer optimization (SVM-RDO) based data mining strategy is proposed in this article. Extended Kalman filter (EKF) is utilized in the first phase, i.e., data cleaning to remove the noise and missing values from the input data. Mantaray foraging algorithm (MaFA) is used in the data selection phase, in which the significant data are selected for the further process to reduce the computational complexity. The final phase is the classification, in which SVM-RDO is proposed to access the useful pattern from the selected data. PYTHON is the implementation tool used for the experiment of the proposed model. The experimental analysis is done to show the efficacy of the proposed work. From the experimental results, the proposed SVM-RDO achieved better accuracy, precision, recall, and F1 score than the existing methods for the tourism dataset. Thus, it is showed the effectiveness of the proposed SVM-RDO for pattern analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3494566"
    },
    {
        "title": "Applying text mining to predict learners' cognitive engagement",
        "authors": [
            "Hayati Hind",
            "Mohammed Khalidi Idrissi",
            "Samir Bennani"
        ],
        "date": "October 2017",
        "source": "SCAMS '17: Proceedings of the Mediterranean Symposium on Smart City Application",
        "abstract": "In academic process, engagement represent one of the important success' factor. This paper proposes a predictive system for learners' cognitive engagement based on their online discussion forums participation. In the first level the ontology OWL and the LSA method are used to perform a semantic classification system of the threads according to a specific context chosen by the tutor. Then the Text mining, as a predictive method, is applied to the classified threads and learners' participation in the forums.",
        "link": "https://dl.acm.org/doi/10.1145/3175628.3175655"
    },
    {
        "title": "Investigate Transitions into Drug Addiction through Text Mining of Reddit Data",
        "authors": [
            "John Lu",
            "Sumati Sridhar",
            "Ritika Pandey",
            "Mohammad Al Hasan",
            "Georege Mohler"
        ],
        "date": "July 2019",
        "source": "KDD '19: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Increasing rates of opioid drug abuse and heightened prevalence of online support communities underscore the necessity of employing data mining techniques to better understand drug addiction using these rapidly developing online resources. In this work, we obtained data from Reddit, an online collection of forums, to gather insight into drug use/misuse using text snippets from users narratives. Specifically, using users' posts, we trained a binary classifier which predicts a user's transitions from casual drug discussion forums to drug recovery forums. We also proposed a Cox regression model that outputs likelihoods of such transitions. In doing so, we found that utterances of select drugs and certain linguistic features contained in one's posts can help predict these transitions. Using unfiltered drug-related posts, our research delineates drugs that are associated with higher rates of transitions from recreational drug discussion to support/recovery discussion, offers insight into modern drug culture, and provides tools with potential applications in combating the opioid crisis.",
        "link": "https://dl.acm.org/doi/10.1145/3292500.3330737"
    },
    {
        "title": "Analysing digital banking reviews using text mining",
        "authors": [
            "Li Chen Cheng",
            "Legaspi Rhea Sharmayne"
        ],
        "date": "December 2020",
        "source": "ASONAM '20: Proceedings of the 12th IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Digital banks are new entrants in the banking industry in the Philippines as they only started late 2018. Since then, a handful of players have and are still emerging. With more and more people becoming technologically savvy, it is very critical for financial institutions to develop a digital banking application that will stand out from the competition. This paper aims to use text mining methods to analyse digital banking application reviews. This study will perform topic modelling using LDA to explore customer concerns and will mine association rules between the digital banking features with the review score. The results will reveal which areas the digital banking application can further optimize for customer satisfaction and retention.",
        "link": "https://dl.acm.org/doi/10.1109/ASONAM49781.2020.9381429"
    },
    {
        "title": "Mining Moocs Videos Metadata Using Classification Techniques",
        "authors": [
            "El Harrak Othman",
            "Ghadi Abderrahim",
            "El Bouhdidi Jaber"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "In few years, the internet has become the first source of information for most people, Today MOOCs makes it possible for everyone to access to the education over the world, it's represents an emerging methodology of online teaching and an important development in open education. Due to the rapid development of Moocs and the rapid growth of digital data and video database over the Internet, it is becoming very difficult for learners to browsing and choosing the best training for them. The scientific community has increased the amount of research into new technologies, with a view to improve the digital video utilization: its archiving, indexing, accessibility, acquisition, store and even its process and usability. All these parts of the video utilization entail the necessity of the extraction of all important information of a video, especially in cases of lack of metadata information. Web video mining is retrieving the content using data mining techniques from World Wide Web. There are two approaches for web video mining using traditional image processing (signal processing) and metadata based approach. In this paper, we present the various research makes in this subject and we propose an effective methodology to extract the metadata from Moocs videos and classify them based on the extracted metadata by applying data mining techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090450"
    },
    {
        "title": "Text mining with HathiTrust",
        "authors": [
            "Eleanor Dickson Koehl",
            "Ryan Dubnicek"
        ],
        "date": "June 2019",
        "source": "JCDL '19: Proceedings of the 18th Joint Conference on Digital Libraries",
        "abstract": "This tutorial will introduce attendees to the HathiTrust Research Center and its tools and services for computational text analysis research. HTRC leverages the scope and scale of the HathiTrust Digital Library collection to create opportunities for researchers to perform text data mining on subsets of the corpus. Attendees will develop skills that will allow them to conduct basic text analysis research using HathiTrust data, learn techniques for engaging with scholars on their text data mining research, and explore strategies for computational access to large-scale digital collections.",
        "link": "https://dl.acm.org/doi/10.1109/JCDL.2019.00115"
    },
    {
        "title": "Text mining for plagiarism detection: multivariate pattern detection for recognition of text similarities",
        "authors": [
            "Konstantinos Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2018",
        "source": "ASONAM '18: Proceedings of the 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "The problem of plagiarism the recent years has been intensified by the availability of information in digital form and the accessibility of the electronic libraries through the Internet. As a result, plagiarism detection has been transformed into a big data analytics problem since the number of digital sources is extravagant and a new document needs to be compared with millions of other existing documents. In this paper, a text mining methodology is proposed that can detect all common patterns between a document and the documents in a reference database. The technique is based on a pattern detection algorithm and the corresponding data structure that enables the algorithm to detect all common patterns. The methodology has been applied in a well-defined dataset providing very promising results identifying difficult cases of plagiarism such as technical disguise.",
        "link": "https://dl.acm.org/doi/10.5555/3382225.3382424"
    },
    {
        "title": "Petroleum Engineering Data Text Classification Using Convolutional Neural Network Based Classifier",
        "authors": [
            "Tzuhan Hsu",
            "Yaoqin Zhang"
        ],
        "date": "May 2018",
        "source": "ICMLT '18: Proceedings of the 2018 International Conference on Machine Learning Technologies",
        "abstract": "In this paper, authors introduce a convolutional neural network (CNN) based classification approach for petroleum engineering event. Recently, text classification problem has raised substantial attention in the field of machine learning. The aim of text classification process is to assign correct label to a text on the basis of its content. However, due to the imprecise of feature representation, most of former text classification methods cannot export satisfying result. To solve this problem, authors propose a model based on convolutional neural network. Authors conduct experiments with real world petroleum engineering data text, artificially labelled into 6 categories. Beside purposed CNN-based model, the authors also implement other baseline machine learning approaches: k nearest neighbor, random forest, support vector machines, recurrent neural network, gated recurrent units, long short-term memory. Among all these models, our CNN-based model reach best performance in text classification task. In the last part of this paper, authors compare and analysis their purpose and baseline model.",
        "link": "https://dl.acm.org/doi/10.1145/3231884.3231898"
    },
    {
        "title": "Classifying Indonesian Online Articles as Advertisement Placement Base Using Text Mining",
        "authors": [
            "Nadhira Tasya",
            "Arian Dhini"
        ],
        "date": "July 2017",
        "source": "ICBIM 2017: Proceedings of the International Conference on Business and Information Management",
        "abstract": "Rapid development in technological aspect resulting in growing level of human needs for the latest news, so that emerged a new trend of publishing and accessing news through online media or usually called online journalism. In addition, the number of people who sell and purchase through online sites also continues to increase and this opportunity is utilized by the company and the advertiser by implementing targeted web advertising. However, the high number of articles that have been published and accessed leads to great opportunities for errors in determining where to place the ads. Therefore, it needs a system that can categorize articles accessed by users as the basis of advertisement placement by the company and this classification system can be done by applying the method of Data Mining and Text Mining. This research uses document data in the form of article content that will be categorized into twenty categories of class of advertisement by using Text Mining technique with Support Vector Machine algorithm. The results of this study may be used by companies or advertisers as a basis for placement of ads on selected online media sites.",
        "link": "https://dl.acm.org/doi/10.1145/3134271.3134288"
    },
    {
        "title": "Mining Free-Text Medical Notes for Suicide Risk Assessment",
        "authors": [
            "Marios Adamou",
            "Grigoris Antoniou",
            "Elissavet Greasidou",
            "Vincenzo Lagani",
            "Paulos Charonyktakis",
            "Ioannis Tsamardinos"
        ],
        "date": "July 2018",
        "source": "SETN '18: Proceedings of the 10th Hellenic Conference on Artificial Intelligence",
        "abstract": "Suicide has been considered as an important public health issue for a very long time, and is one of the main causes of death worldwide. Despite suicide prevention strategies being applied, the rate of suicide has not changed substantially over the past decades. Advances in machine learning make it possible to attempt to predict suicide based on the analysis of relevant data to inform clinical practice. This paper reports on findings from the analysis of data of patients who died by suicide in the period 2013-2016 and made use of both structured data and free-text medical notes. We focus on examining various text-mining approaches to support risk assessment. The results show that using advance machine learning and text-mining techniques, it is possible to predict within a specified period which people are most at risk of taking their own life at the time of referral to a mental health service.",
        "link": "https://dl.acm.org/doi/10.1145/3200947.3201020"
    },
    {
        "title": "Integration of data mining techniques in e-learning systems: Clustering Profil of Lerners and Recommender Course System",
        "authors": [
            "A. El Moustamid",
            "E. En-Naimi",
            "J. El Bouhdidi"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "Data mining aims at the extraction of knowledge from large quantities of data by automatic or semi-automatic methods. In the field of education and training e-learning, Data Mining techniques as clustering (The aim of this technique is to identify groups that are similar in some aspect) [21], prediction (is used for developing model where combination of data viewed from different aspects can effect single viewed data. Here, is important to watch how predictors variables are effecting predicted variable)[21], classification are applied in e-learning to improve the level of learners by providing learning according to the learners' requirement) are used to analyze learners profile, to predict and improve student performance. The aim of our project is to develop a system capable of analyzing learners profile and indexing web videos in order to offer learners a rich database with courses that correspond to their levels.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090453"
    },
    {
        "title": "Multi-level mining and visualization of scientific text collections: Exploring a bi-lingual scientific repository",
        "authors": [
            "Pablo Accuosto",
            "Francesco Ronzano",
            "Daniel Ferrés",
            "Horacio Saggion"
        ],
        "date": "December 2017",
        "source": "WOSP 2017: Proceedings of the 6th International Workshop on Mining Scientific Publications",
        "abstract": "We present a system to mine and visualize collections of scientific documents by semantically browsing information extracted from single publications or aggregated throughout corpora of articles. The text mining tool performs deep analysis of document collections allowing the extraction and interpretation of research paper's contents. In addition to the extraction and enrichment of documents with metadata (titles, authors, affiliations, etc), the deep analysis performed comprises semantic interpretation, rhetorical analysis of sentences, triple-based information extraction, and text summarization. The visualization components allow geographical-based exploration of collections, topic-evolution interpretation, and collaborative network analysis among others. The paper presents a case study of a bi-lingual collection in the field of Natural Language Processing (NLP).",
        "link": "https://dl.acm.org/doi/10.1145/3127526.3127529"
    },
    {
        "title": "Hierarchical Topic Mining via Joint Spherical Tree and Text Embedding",
        "authors": [
            "Yu Meng",
            "Yunyi Zhang",
            "Jiaxin Huang",
            "Yu Zhang",
            "Chao Zhang",
            "Jiawei Han"
        ],
        "date": "August 2020",
        "source": "KDD '20: Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Mining a set of meaningful topics organized into a hierarchy is intuitively appealing since topic correlations are ubiquitous in massive text corpora. To account for potential hierarchical topic structures, hierarchical topic models generalize flat topic models by incorporating latent topic hierarchies into their generative modeling process. However, due to their purely unsupervised nature, the learned topic hierarchy often deviates from users' particular needs or interests. To guide the hierarchical topic discovery process with minimal user supervision, we propose a new task, Hierarchical Topic Mining, which takes a category tree described by category names only, and aims to mine a set of representative terms for each category from a text corpus to help a user comprehend his/her interested topics. We develop a novel joint tree and text embedding method along with a principled optimization procedure that allows simultaneous modeling of the category tree structure and the corpus generative process in the spherical space for effective category-representative term discovery. Our comprehensive experiments show that our model, named JoSH, mines a high-quality set of hierarchical topics with high efficiency and benefits weakly-supervised hierarchical text classification tasks.",
        "link": "https://dl.acm.org/doi/10.1145/3394486.3403242"
    },
    {
        "title": "Unsupervised behavioural mining and clustering for malware family identification",
        "authors": [
            "Khanh Huu The Dam",
            "Thomas Given-Wilson",
            "Axel Legay"
        ],
        "date": "March 2021",
        "source": "SAC '21: Proceedings of the 36th Annual ACM Symposium on Applied Computing",
        "abstract": "More accurate and advanced detection and classification of malware requires exploiting program behaviour and not merely syntactic or static features. One approach is to use system call dependency graphs (SCDGs) that represent the program behaviour by interactions with the system, and the relations between these interactions. These SCDGs have been used with supervised learning techniques to very accurately detect and classify malware. This works considers the unsupervised learning challenge of mining for common clusters of behaviour without a priori knowledge. This allows for clustering of similar programs by behaviour, that can then be used for either classification or further analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3412841.3441919"
    },
    {
        "title": "Text Mining for Evaluating Authors' Birth and Death Years",
        "authors": [
            "Dror Moghaz",
            "Yaakov Hacohen-Kerner",
            "Dov Gabbay"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "This article presents a unique method in text and data mining for finding the era, i.e., mining temporal data, in which an anonymous author was living. Finding this era can assist in the examination of a fake document or extracting the time period in which a writer lived. The study and the experiments concern Hebrew, and in some parts, Aramaic and Yiddish rabbinic texts. The rabbinic texts are undated and contain no bibliographic sections, posing an interesting challenge. This work proposes algorithms using key phrases and key words that allow the temporal organization of citations together with linguistic patterns. Based on these key phrases, key words, and the references, we established several types of “Iron-clad,” Heuristic and Greedy rules for estimating the years of birth and death of a writer in an interesting classification task. Experiments were conducted on corpora, including documents authored by 12, 24, and 36 rabbinic writers and demonstrated promising results.",
        "link": "https://dl.acm.org/doi/10.1145/3281631"
    },
    {
        "title": "Transfer Learning to Timed Text Based Video Classification Using CNN",
        "authors": [
            "Zenun Kastrati",
            "Ali Shariq Imran",
            "Arianit Kurti"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "Open educational video resources are gaining popularity with a growing number of massive open online courses (MOOCs). This has created a niche for content providers to adopt effective solutions in automatically organizing and structuring of educational resources for maximum visibility. Recent advances in deep learning techniques are proving useful in managing and classifying resources into appropriate categories. This paper proposes one such convolutional neural network (CNN) model for classifying video lectures in a MOOC setting using a transfer learning approach. The model uses a time-aligned text transcripts corresponding to video lectures from six broader subject categories. Video lectures and their corresponding transcript dataset is gathered from the Coursera MOOC platform. Two different CNN models are proposed: i) CNN based classification using embeddings learned from our MOOC dataset, ii) CNN based classification using transfer learning. Word embeddings generated from two well known state-of-the-art pre-trained models Word2Vec and GloVe, are used in the transfer learning approach for the second case. The proposed CNN models are evaluated using precision, recall, and F1 score and the obtained performance is compared with both conventional and deep learning classifiers. The proposed CNN models have an F1 score improvement of 10-22 percentage points over DNN and conventional classifiers",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326483"
    },
    {
        "title": "Evolutionary role mining in complex networks by ensemble clustering",
        "authors": [
            "Sarvenaz Choobdar",
            "Pedro Ribeiro",
            "Fernando Silva"
        ],
        "date": "April 2017",
        "source": "SAC '17: Proceedings of the Symposium on Applied Computing",
        "abstract": "The structural patterns in the neighborhood of nodes assign unique roles to the nodes. Mining the set of existing roles in a network provides a descriptive profile of the network and draws its general picture. This paper proposes a new method to determine structural roles in a dynamic network based on the current position of nodes and their historic behavior. We develop a temporal ensemble clustering technique to dynamically find groups of nodes, holding similar tempo-structural roles. We compare two weighting functions, based on age and distribution of data, to incorporate temporal behavior of nodes in the role discovery. To evaluate the performance of the proposed method, we assess the results from two points of view: 1) goodness of fit to current structure of the network; 2) consistency with historic data. We conduct the evaluation using different ensemble clustering techniques. The results on real world networks demonstrate that our method can detect tempo-structural roles that simultaneously depict the topology of a network and reflect its dynamics with high accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3019612.3019815"
    },
    {
        "title": "Analyzing used-car web listings via text mining",
        "authors": [
            "Ayhan Demiriz",
            "Fatma Cantaş"
        ],
        "date": "October 2017",
        "source": "IML '17: Proceedings of the 1st International Conference on Internet of Things and Machine Learning",
        "abstract": "Used car trade is one of the major components of the world economies. It is not uncommon to sell a car by placing an internet advertisement irrespective of the geography in these days. A typical content of an advertisement is usually composed of two parts namely the structured and the free text data. The structured data may include some information about the asking price, make, model, year, mileage of the car and the contact info. In most cases, seller may give important clues about the car's current conditions in the free text data where the title (head) of the advertisement can be included as free text too. This paper reports preliminary results from a text mining study conducted on 75K used car internet listings collected from two major car listing web sites in Turkey. As expected, the words and the phrases related to the description of the car are observed to be frequent. The leading concepts in the free text are found to be regarding how to describe the current condition of a car, for example \"no crash history\".",
        "link": "https://dl.acm.org/doi/10.1145/3109761.3109782"
    },
    {
        "title": "Text classification of Diseases Treated by Traditional Chinese Medicine Prescription based on machine learning",
        "authors": [
            "Hanqing Zhao",
            "Bo Han",
            "Chen Li"
        ],
        "date": "September 2020",
        "source": "ISAIMS '20: Proceedings of the 1st International Symposium on Artificial Intelligence in Medical Sciences",
        "abstract": "OBJECTIVE: To explore the application of machine learning in th e identification of diseases treated by traditional Chinese medicine prescriptions. METHODS: Based on the composition of the text document of Chinese medicine prescriptions, the prescriptions were divided into cough, headache and diarrhea. THUTCT were introduced to establish and train two machine learning text classification models, LibSVM and LibLinear, and the prescriptions to be deter mined were put into the model for classification and prediction. R ESULTS: The Precision rate of LibSVM model and Liblinear mod el were 0.7283 and 0.6690. Seven prescriptions were classified an d predicted, and the results were in line with expectations. CONCLUSION: THUCTC has good universality for the content of TCM prescriptions, high classification accuracy and fast testing speed, which is suitable for the text classification and discrimination research of TCM prescriptions for diseases.",
        "link": "https://dl.acm.org/doi/10.1145/3429889.3429895"
    },
    {
        "title": "Evaluation of retweet clustering method classification method using retweets on Twitter without text data",
        "authors": [
            "K. Uchida",
            "F. Toriumi",
            "T. Sakaki"
        ],
        "date": "August 2017",
        "source": "WI '17: Proceedings of the International Conference on Web Intelligence",
        "abstract": "Burst phenomena, which frequently occur on social media, are caused by such social events as flaming on the internet, elections, and natural disasters. To understand people's thoughts and feelings, we must classify their opinions from burst phenomena. Therefore, classification methods that categorize tweets are critical. However, since most classification methods focus on text mining, they cannot group tweets by topics because each tweet has poor linguistic similarities. We used a non-text-based classification method proposed by Baba et al. that groups tweets by topics, even if they have poor linguistic similarities, and verified its validity by comparing it with a text-based classification method in two different evaluations: qualitative and quantitative. In the qualitative evaluation part, we did a questionnaire survey and validated the suitability of the topic clusters created using both the non-and text-based methods. Since evaluating the similarity of every pair of tweets in each topic is difficult, we evaluated the similarity between sampled pairs in the survey and acquired more appropriate topic clustering results using the non-text-based method than the text-based method. In the quantitative evaluation part, we focused on the robustness of each method against data reduction. Many approaches analyze social media data, especially because collecting data from social media is comparatively easy. However, since collecting the whole data of burst phenomena is very costly due to the vast amounts of available social media data, robustness against data reduction is an important index to evaluate classification methods. With the non-text-based method, over 55% of the pairs of tweets in the same cluster were also included in the same cluster even when the data were reduced to 10% in all three of our example cases. In this paper, as a source we focus on Twitter, one of the most popular microblogging services. Using clustering to conduct detailed case analyses, we scrutinized three burst cases that include natural disasters and flaming on the internet and found that a non-text-based method more effectively classified tweets in burst phenomena than a text-based method.",
        "link": "https://dl.acm.org/doi/10.1145/3106426.3106451"
    },
    {
        "title": "Using Spark for Text Mining on Large Scale Liver Cancer Literature",
        "authors": [
            "Ming-Yen Lin",
            "Yu-Ju Lin",
            "Sue-Chen Hsueh"
        ],
        "date": "January 2021",
        "source": "BDET 2021: 2021 the 3rd International Conference on Big Data Engineering and Technology (BDET)",
        "abstract": "Cancer is one of the main causes of death. The number of known cancer to-date is more than one hundred. Liver cancer, ranked after trachea and lung cancer, has long been high in the cancer leading cause in Taiwan and even ranked as second in 2016. The number of scientific articles related to cancer proliferates every year. The number reaches as high as 20 million in PubMed so that discovering useful information from the massive collection is very difficult. In addition, using a single machine to sift through these articles is very time-consuming. Therefore, we present a big data analytic framework using the distributed Apache Spark platform for text mining in PubMed literature. We establish a prediction model for liver cancer articles so that researchers may effectively validate whether an article is related to liver cancer or not. Classification models in Spark MLlib including Linear Support Vector Machines (SVM), Logistic Regression, are used in our experiments. Relevancy to liver cancer is further confirmed by using MeSH (Medical Subject Headings) terms. Logistic regression is about 3 times faster than SVMs and the accuracy of both methods is close to 95% in the experiments using hold-out validation. When max_features is 500 and min_df ≤ 0.1 (or min_df = 1), the accuracy may reach 96%. In the experiments with K-fold cross-validation, the accuracy of SVMs methods is 96%. The experimental results show that that our prediction model may effectively classify liver cancer articles.",
        "link": "https://dl.acm.org/doi/10.1145/3474944.3474958"
    },
    {
        "title": "Automatic topic classification of test cases using text mining at an Android smartphone vendor",
        "authors": [
            "Junji Shimagaki",
            "Yasutaka Kamei",
            "Naoyasu Ubayashi",
            "Abram Hindle"
        ],
        "date": "October 2018",
        "source": "ESEM '18: Proceedings of the 12th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Background: An Android smartphone is an ecosystem of applications, drivers, operating system components, and assets. The volume of the software is large and the number of test cases needed to cover the functionality of an Android system is substantial. Enormous effort has been already taken to properly quantify \"what features and apps were tested and verified?\". This insight is provided by dashboards that summarize test coverage and results per feature. One method to achieve this is to manually tag or label test cases with the topic or function they cover, much like function points. At the studied Android smartphone vendor, tests are labelled with manually defined tags, so-called \"feature labels (FLs)\", and the FLs serve to categorize 100s to 1000s test cases into 10 to 50 groups. Aim: Unfortunately for developers, manual assignment of FLs to 1000s of test cases is a time consuming task, leading to inaccurately labeled test cases, which will render the dashboard useless. We created an automated system that suggests tags/labels to the developers for their test cases rather than manual labeling. Method: We use machine learning models to predict and label the functionality tested by 10,000 test cases developed at the company. Results: Through the quantitative experiments, our models achieved acceptable F-1 performance of 0.3 to 0.88. Also through the qualitative studies with expert teams, we showed that the hierarchy and path of tests was a good predictor of a feature's label. Conclusions: We find that this method can reduce tedious manual effort that software developers spent classifying test cases, while providing more accurate classification results.",
        "link": "https://dl.acm.org/doi/10.1145/3239235.3268927"
    },
    {
        "title": "To apply Data Mining for Classification of Crowd sourced Software Requirements",
        "authors": [
            "Soonh Taj",
            "Qasim Arain",
            "Imran Memon",
            "Asma Zubedi"
        ],
        "date": "April 2019",
        "source": "ICSIE '19: Proceedings of the 8th International Conference on Software and Information Engineering",
        "abstract": "Now a day's main focus of developers is to build quality software that works according to customer needs and for this reason it is necessary to gather right requirements as requirement elicitation is the critical step that impacts on the success of software project as misinterpreted requirements leads to the failure of software project. By keeping this in mind a research is carried out on improving requirements elicitation process and automating the process of classifying requirements. In this research, a model is proposed which will help in this scenario for requirements elicitation and requirement classification. This paper presents a model in which crowd sourcing approach is used so that customers, end users, stakeholders, developers and software engineers can make active participation for requirement elicitation process and requirements gathered using crowdsourcing approach are used by model for classification process i.e. classification of requirements into functional and non-functional requirements. For the proof of proposed model a case study is conducted. Results of case study provided the usefulness and efficiency of proposed model for classification of crowd sourced software requirements.",
        "link": "https://dl.acm.org/doi/10.1145/3328833.3328837"
    },
    {
        "title": "Text classification using Fuzzy TF-IDF and Machine Learning Models",
        "authors": [
            "Mariem Bounabi",
            "Karim El Moutaouakil",
            "Khalid Satori"
        ],
        "date": "October 2019",
        "source": "BDIoT'19: Proceedings of the 4th International Conference on Big Data and Internet of Things",
        "abstract": "The representation of the information has an important impact on the text classification task. Several weighting methods were proposed in the literature, and the term frequency-inverse term frequency (TFIDF), the most know on the text treatment field. The FTF-IDF is a vector representation where the components of the TFIDF are presented as inputs to the Fuzzy Inference System (FIS). In this work, we compare several Machin Learning algorithms such as Naïve Bayes and its derivatives, SVM and Random forest classifiers, using the FTF-IDF representation. To improve the quality of the used classifiers, we call sum attribute selection methods. The recognition rate, for the tested systems, is satisfied, where the system based on naïve Bayes classifier, the FTF-IDF weighting terms, and the info gain select attributes method gives 98.7% as accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3372938.3372956"
    },
    {
        "title": "Text Mining in Cybersecurity: A Systematic Literature Review",
        "authors": [
            "Luciano Ignaczak",
            "Guilherme Goldschmidt",
            "Cristiano André Da Costa",
            "Rodrigo Da Rosa Righi"
        ],
        "date": null,
        "source": "ACM Computing Surveys",
        "abstract": "The growth of data volume has changed cybersecurity activities, demanding a higher level of automation. In this new cybersecurity landscape, text mining emerged as an alternative to improve the efficiency of the activities involving unstructured data. This article proposes a Systematic Literature Review (SLR) to present the application of text mining in the cybersecurity domain. Using a systematic protocol, we identified 2,196 studies, out of which 83 were summarized. As a contribution, we propose a taxonomy to demonstrate the different activities in the cybersecurity domain supported by text mining. We also detail the strategies evaluated in the application of text mining tasks and the use of neural networks to support activities involving unstructured data. The work also discusses text classification performance aiming its application in real-world solutions. The SLR also highlights open gaps for future research, such as the analysis of non-English content and the intensification in the usage of neural networks.",
        "link": "https://dl.acm.org/doi/10.1145/3462477"
    },
    {
        "title": "Using Text Mining to Analyze the Financial Patents",
        "authors": [
            "Yung-Feng Lu",
            "Jia-Lang Xu",
            "Mu-Yen Chen"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "With the advancement and development of science and technology, Nowadays society is the development of information technology, cloud computing, mobile Internet, Internet of Things, block chain and information technology.The combination of finance and technology has made financial technology terms gradually emerging.. This research analyzes the patents of financial technology. However, Taiwan's financial technology patents are mostly G06Q-20 (payment plan, architecture or agreement), G06Q-30 (commercial, such as marketing, shopping, payment, auction or e-commerce). G06Q-40 (financial, investment or tax treatment, insurance).The research use text mining technology to analyze and understand Taiwan's trends and developments in financial technology. The research results show that G06Q-20 patents are biased towards information security and mobile payment. G06Q-30 patents favor shopping websites and advertising. G06Q-40 patents are biased towards diversity, such as securities, insurance, finance, and stocks.",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326478"
    },
    {
        "title": "Identifying Online Profiles of Distance Learning Students Using Data Mining Techniques",
        "authors": [
            "Osama Islam",
            "Muazzam Siddiqui",
            "Naif Radi Aljohani"
        ],
        "date": "October 2019",
        "source": "ICDTE '19: Proceedings of the 3rd International Conference on Digital Technology in Education",
        "abstract": "Educational data has grown over the years with the increased use of technology within educational environments. This has led to a huge amount of data being stored in various data sources representing the student, his/her activities, and other aspects relevant to the learning process. To meet this analytical need, Educational Data Mining (EDM) has emerged to assist educational institutions in identifying key benefits such as students at risk, the level of student engagement or predicting student performance. The aim of this research was to explore the various aspects of student interaction data using data mining techniques to identify relevant patterns of behaviors and possible key attributes that have higher degrees of influence on distance learning students. The main findings identified several patterns of user online profiles based on a set of adopted learning strategies, the research proposed a framework for analyzing such interaction data based on R and Hadoop platforms to correlate online profiles with student performance.",
        "link": "https://dl.acm.org/doi/10.1145/3369199.3369249"
    },
    {
        "title": "Extremely Fast Decision Tree Mining for Evolving Data Streams",
        "authors": [
            "Albert Bifet",
            "Jiajin Zhang",
            "Wei Fan",
            "Cheng He",
            "Jianfeng Zhang",
            "Jianfeng Qian",
            "Geoff Holmes",
            "Bernhard Pfahringer"
        ],
        "date": "August 2017",
        "source": "KDD '17: Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
        "abstract": "Nowadays real-time industrial applications are generating a huge amount of data continuously every day. To process these large data streams, we need fast and efficient methodologies and systems. A useful feature desired for data scientists and analysts is to have easy to visualize and understand machine learning models. Decision trees are preferred in many real-time applications for this reason, and also, because combined in an ensemble, they are one of the most powerful methods in machine learning. In this paper, we present a new system called STREAMDM-C++, that implements decision trees for data streams in C++, and that has been used extensively at Huawei. Streaming decision trees adapt to changes on streams, a huge advantage since standard decision trees are built using a snapshot of data, and can not evolve over time. STREAMDM-C++ is easy to extend, and contains more powerful ensemble methods, and a more efficient and easy to use adaptive decision trees. We compare our new implementation with VFML, the current state of the art implementation in C, and show how our new system outperforms VFML in speed using less resources.",
        "link": "https://dl.acm.org/doi/10.1145/3097983.3098139"
    },
    {
        "title": "Data miners' little helper: data transformation activity cues for cluster analysis on document collections",
        "authors": [
            "Tania Cerquitelli",
            "Evelina Di Corso",
            "Francesco Ventura",
            "Silvia Chiusano"
        ],
        "date": "June 2017",
        "source": "WIMS '17: Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "In this paper we propose a new self-learning engine to streamline the analytics process, as it enables analysts to mine massive data repositories with minimal user intervention. In the context of cluster analysis on a collection of documents this new system, named SELF-DATA (SELF-learning DAta TrAnsformation), suggests to the analyst how to configure the whole mining process for a given dataset. SELF-DATA relies on an engine exploring different data weighting schemas (e.g., normalized term frequencies) and data transformation methods (e.g., PCA) before applying the cluster analysis, evaluating and comparing solutions through different quality indices (e.g., weighted Silhouette), and presenting the k-top solutions to the analyst. SELF-DATA will also include a knowledge base storing results of experiments on previously processed datasets, and a classification algorithm trained on the knowledge base content to forecast the best configuration for the whole mining process for an unexplored dataset. The first development of SELF-DATA running on Apache Spark has been validated on 5 collections of documents. Experimental results highlight that TF-IDF and logarithmic entropy are effective to measure item relevance with sparse datasets, and the LSI method outperforms PCA with a large dictionary.",
        "link": "https://dl.acm.org/doi/10.1145/3102254.3102288"
    },
    {
        "title": "Interactive Visual Graph Mining and Learning",
        "authors": [
            "Ryan A. Rossi",
            "Nesreen K. Ahmed",
            "Rong Zhou",
            "Hoda Eldardiry"
        ],
        "date": null,
        "source": "ACM Transactions on Intelligent Systems and Technology",
        "abstract": "This article presents a platform for interactive graph mining and relational machine learning called GraphVis. The platform combines interactive visual representations with state-of-the-art graph mining and relational machine learning techniques to aid in revealing important insights quickly as well as learning an appropriate and highly predictive model for a particular task (e.g., classification, link prediction, discovering the roles of nodes, and finding influential nodes). Visual representations and interaction techniques and tools are developed for simple, fast, and intuitive real-time interactive exploration, mining, and modeling of graph data. In particular, we propose techniques for interactive relational learning (e.g., node/link classification), interactive link prediction and weighting, role discovery and community detection, higher-order network analysis (via graphlets, network motifs), among others. GraphVis also allows for the refinement and tuning of graph mining and relational learning methods for specific application domains and constraints via an end-to-end interactive visual analytic pipeline that learns, infers, and provides rapid interactive visualization with immediate feedback at each change/prediction in real-time. Other key aspects include interactive filtering, querying, ranking, manipulating, exporting, as well as tools for dynamic network analysis and visualization, interactive graph generators (including new block model approaches), and a variety of multi-level network analysis techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3200764"
    },
    {
        "title": "Data Mining and Opinion Mining: A Tool in Educational Context",
        "authors": [
            "Myriam Peñafiel",
            "Stefanie Vásquez",
            "Diego Vásquez",
            "Juan Zaldumbide",
            "Sergio Luján-Mora"
        ],
        "date": "July 2018",
        "source": "ICoMS '18: Proceedings of the 2018 1st International Conference on Mathematics and Statistics",
        "abstract": "The use of the web as a universal communication platform generates large volumes of data (Big data), which in many cases, need to be processed so that they can become useful knowledge in face of the sceptics who have doubts about the credibility of such information. The use of web data that comes from educational contexts needs to be addressed, since that large amount of unstructured information is not being valued, losing valuable information that can be used. To solve this problem, we propose the use of data mining techniques such as sentiment analysis to validate the information that comes from the educational platforms. The objective of this research is to propose a methodology that allows the user to apply sentiment analysis in a simple way, because although some researchers have done it, very few do with data in the educational context. The results obtained prove that the proposal can be used in similar cases.",
        "link": "https://dl.acm.org/doi/10.1145/3274250.3274263"
    },
    {
        "title": "Research Hotspots Mining and Visualized Analysis Based on Linking Cluster and K-Core Decomposition",
        "authors": [
            "Ying Cai",
            "Fang Huang",
            "Shuai Wang",
            "Hao Zhang",
            "Chunxiu Du"
        ],
        "date": "May 2018",
        "source": "ICDPA 2018: Proceedings of the International Conference on Data Processing and Applications",
        "abstract": "In this paper, we focus on research hotspots mining with knowledge semantic features on discipline topic word networks, which are extracted from scientific and technical documents. For the topic networks, a novel method of combining linking community clustering with k-core decomposition is proposed to mine the hotspot community with knowledge hierarchy structure. And as a post-processing, the hierarchical community identification is implemented by k-core decomposition in order to discover the internal connection with knowledge semantic hierarchy in the research hotspot. Finally, the community density and visualization method is utilized to analyze the core and sub- problem in the research hotspot community. In experiments, the topic networks for a single discipline and interdisciplinary research hotspot are taken as examples of analysis, and the results show the proposed approach can effectively realize the objective of mining and analyzing the discipline research hotspot on the academic topic word network.",
        "link": "https://dl.acm.org/doi/10.1145/3224207.3224216"
    },
    {
        "title": "The Application of Data Mining Algorithms in the Construction of Travel Recommendation System",
        "authors": [
            "Chao Lou",
            "Zhaonan Mu",
            "Mengzhu Liu"
        ],
        "date": "January 2021",
        "source": "CONF-CDS 2021: The 2nd International Conference on Computing and Data Science",
        "abstract": "Photos shared by users of online social network often associated with tags, texts, geographic and time information. These data are ideal research resource which can be used by researchers to observe people's interests and behaviors. They can also be used to analyze people's travel behaviors and interests because it can reflect people's travel activities and experiences directly. This project aimed to consolidate people into groups based on their travel preferences of travel season, travel time span, scenery type of travel location and then analyze popular travel places for different people groups. The travel preferences were analyzed from extracted data applying PHP language and Naive Bayes classification approach. According to the analyzed data, people were divided into groups. Then, DBSCAN clustering was used to get popular locations based on the different groups. As a result, 80 thousand photo data of 5 cities have been collected from Flickr by Phython. It has been showed that people's preference of season in most of the mined cities was spring and summer and the preference of time span was short. Moreover, mined people has been divided into 16 groups according to their preference and based on the groups popular locations have been calculated by DBSCAN clustering. Theses locations have been provided in a recommendation website which was built in this project. For the future work, mining Flickr data associated with other travel social network sites could be a good way to build a more effective recommendation system.",
        "link": "https://dl.acm.org/doi/10.1145/3448734.3450477"
    },
    {
        "title": "Compare Machine Learning Models in Text Classification Using Steam User Reviews",
        "authors": [
            "Youchen Miao",
            "Zeyu Jin",
            "Yumeng Zhang",
            "Yuchen Chen",
            "Junren Lai"
        ],
        "date": "November 2021",
        "source": "ICSED '21: Proceedings of the 2021 3rd International Conference on Software Engineering and Development",
        "abstract": "Text Classification and Sentiment Analysis of game reviews are viewed as important parts in not only academic fields but also in game studies. In this paper, with more than 400 thousand game reviews on Steam platform, we preprocess the data using different libraries (sklearn, nltk, and spaCy) and use them as inputs to build three sentiment classification models based on different algorithms (Naive Bayes, SVM, and Random Forest). In contrast to previous studies that only focus on different sentiment analysis models, our paper also highlights the use of different APIs to preprocess the data and their corresponding model performance. The results show that no matter which API we choose, Random Forest models always perform the best. However, in terms of training time, Naive Bayes is the fastest. This work can be used to apply grid search for researchers to automatically find the optimum API before conducting sentiment analysis in the future.",
        "link": "https://dl.acm.org/doi/10.1145/3507473.3507480"
    },
    {
        "title": "Lost in Transduction: Transductive Transfer Learning in Text Classification",
        "authors": [
            "Alejandro Moreo",
            "Andrea Esuli",
            "Fabrizio Sebastiani"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Obtaining high-quality labelled data for training a classifier in a new application domain is often costly. Transfer Learning\n(a.k.a. “Inductive Transfer”) tries to alleviate these costs by transferring, to the “target” domain of interest, knowledge available from a different “source” domain. In transfer learning the lack of labelled information from the target domain is compensated by the availability at training time of a set of unlabelled examples from the target distribution. Transductive Transfer Learning denotes the transfer learning setting in which the only set of target documents that we are interested in classifying is known and available at training time. Although this definition is indeed in line with Vapnik’s original definition of “transduction”, current terminology in the field is confused. In this article, we discuss how the term “transduction” has been misused in the transfer learning literature, and propose a clarification consistent with the original characterization of this term given by Vapnik. We go on to observe that the above terminology misuse has brought about misleading experimental comparisons, with inductive transfer learning methods that have been incorrectly compared with transductive transfer learning methods. We then, give empirical evidence that the difference in performance between the inductive version and the transductive version of a transfer learning method can indeed be statistically significant (i.e., that knowing at training time the only data one needs to classify indeed gives an advantage). Our clarification allows a reassessment of the field, and of the relative merits of the major, state-of-the-art algorithms for transfer learning in text classification.",
        "link": "https://dl.acm.org/doi/10.1145/3453146"
    },
    {
        "title": "Machine Learning on Mining Potential Adverse Drug Reactions for Pharmacovigilance",
        "authors": [
            "Ke Jia"
        ],
        "date": "July 2021",
        "source": "DSIT 2021: 2021 4th International Conference on Data Science and Information Technology",
        "abstract": "Drugs are related to the lives and health of patients. Mining and discovering potential adverse drug reactions from a large number of unstructured texts play an important role in drug research and pharmacovigilance. It is a hot research issue in the field of biomedicine in recent years. A large number of scholars have discovered a large amount of information on adverse drug reactions from different data sources through effective machine learning methods. This paper reviews the research progress in recent years to provide a reference for future research.",
        "link": "https://dl.acm.org/doi/10.1145/3478905.3478964"
    },
    {
        "title": "TACAM: Topic And Context Aware Argument Mining",
        "authors": [
            "Michael Fromm",
            "Evgeniy Faerman",
            "Thomas Seidl"
        ],
        "date": "October 2019",
        "source": "WI '19: IEEE/WIC/ACM International Conference on Web Intelligence",
        "abstract": "In this work we address the problem of argument search. The purpose of argument search is the distillation of pro and contra arguments for requested topics from large text corpora. In previous works, the usual approach is to use a standard search engine to extract text parts which are relevant to the given topic and subsequently use an argument recognition algorithm to select arguments from them. The main challenge in the argument recognition task, which is also known as argument mining, is that often sentences containing arguments are structurally similar to purely informative sentences without any stance about the topic. In fact, they only differ semantically. Most approaches use topic or search term information only for the first search step and therefore assume that arguments can be classified independently of a topic. We argue that topic information is crucial for argument mining, since the topic defines the semantic context of an argument. Precisely, we propose different models for the classification of arguments, which take information about a topic of an argument into account. Moreover, to enrich the context of a topic and to let models understand the context of the potential argument better, we integrate information from different external sources such as Knowledge Graphs or pre-trained NLP models. Our evaluation shows that considering topic information, especially in connection with external information, provides a significant performance boost for the argument mining task.",
        "link": "https://dl.acm.org/doi/10.1145/3350546.3352506"
    },
    {
        "title": "Text mining for incoming tasks based on the urgency/importance factors and task classification using machine learning tools",
        "authors": [
            "Yasser Ali Alshehri"
        ],
        "date": "March 2020",
        "source": "ICCDA 2020: Proceedings of the 2020 the 4th International Conference on Compute and Data Analysis",
        "abstract": "In workplaces, there is a massive amount of unstructured data from different sources. In this paper, we present a case study that explains how can through communications between employees, we can help to prioritize tasks requests to increase the efficiency of their works for both technical and non-technical workers. This involves managing daily incoming tasks based on their level of urgency and importance.To allow all workers to utilize the urgency-importance matrix as a time-management tool, we need to automate this tool. The textual content of incoming tasks are analyzed, and metrics related to urgency and importance are extracted. A third factor (i.e., the response variable) is defined based on the two input variables (urgency and importance). Then, machine learning applied to the data to predict the class of incoming tasks based on data outcome desired. We used ordinal regression, neural networks, and decision tree algorithms to predict the four levels of task priority. We measure the performance of all using recalls, precisions, and F-scores. All classifiers perform higher than 89% in terms of all measures.",
        "link": "https://dl.acm.org/doi/10.1145/3388142.3388153"
    },
    {
        "title": "Text mining for malware classification using multivariate all repeated patterns detection",
        "authors": [
            "Konstantinos F. Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2019",
        "source": "ASONAM '19: Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Mobile phones have become nowadays a commodity to the majority of people. Using them, people are able to access the world of Internet and connect with their friends, their colleagues at work or even unknown people with common interests. This proliferation of the mobile devices has also been seen as an opportunity for the cyber criminals to deceive smartphone users and steel their money directly or indirectly, respectively, by accessing their bank accounts through the smartphones or by blackmailing them or selling their private data such as photos, credit card data, etc. to third parties. This is usually achieved by installing malware to smartphones masking their malevolent payload as a legitimate application and advertise it to the users with the hope that mobile users will install it in their devices. Thus, any existing application can easily be modified by integrating a malware and then presented it as a legitimate one. In response to this, scientists have proposed a number of malware detection and classification methods using a variety of techniques. Even though, several of them achieve relatively high precision in malware classification, there is still space for improvement. In this paper, we propose a text mining all repeated pattern detection method which uses the decompiled files of an application in order to classify a suspicious application into one of the known malware families. Based on the experimental results using a real malware dataset, the methodology tries to correctly classify (without any misclassification) all randomly selected malware applications of 3 categories with 3 different families each.",
        "link": "https://dl.acm.org/doi/10.1145/3341161.3350841"
    },
    {
        "title": "Comparative Study between Traditional Machine Learning and Deep Learning Approaches for Text Classification",
        "authors": [
            "Cannannore Nidhi Kamath",
            "Syed Saqib Bukhari",
            "Andreas Dengel"
        ],
        "date": "August 2018",
        "source": "DocEng '18: Proceedings of the ACM Symposium on Document Engineering 2018",
        "abstract": "In this contemporaneous world, it is an obligation for any organization working with documents to end up with the insipid task of classifying truckload of documents, which is the nascent stage of venturing into the realm of information retrieval and data mining. But classification of such humongous documents into multiple classes, calls for a lot of time and labor. Hence a system which could classify these documents with acceptable accuracy would be of an unfathomable help in document engineering. We have created multiple classifiers for document classification and compared their accuracy on raw and processed data. We have garnered data used in a corporate organization as well as publicly available data for comparison. Data is processed by removing the stop-words and stemming is implemented to produce root words. Multiple traditional machine learning techniques like Naive Bayes, Logistic Regression, Support Vector Machine, Random forest Classifier and Multi-Layer Perceptron are used for classification of documents. Classifiers are applied on raw and processed data separately and their accuracy is noted. Along with this, Deep learning technique such as Convolution Neural Network is also used to classify the data and its accuracy is compared with that of traditional machine learning techniques. We are also exploring hierarchical classifiers for classification of classes and subclasses. The system classifies the data faster and with better accuracy than if done manually. The results are discussed in the results and evaluation section.",
        "link": "https://dl.acm.org/doi/10.1145/3209280.3209526"
    },
    {
        "title": "Identification of Overpricing in the Purchase of Medication by the Federal Government of Brazil, Using Text Mining and Clustering Based on Ontology",
        "authors": [
            "Marco Aurelio O. S. Correa",
            "Adriano Galindo Leal"
        ],
        "date": "August 2018",
        "source": "ICCBDC '18: Proceedings of the 2018 2nd International Conference on Cloud and Big Data Computing",
        "abstract": "Increasing the transparency level in his actions and spending is one of the primary duties of the Brazilian Federal Government. The creation of laws that oblige full disclose of all its expenditures through transparency portals enables citizens to supervise all government entities. However, only the dissemination of these data, without a definite standard or the availability of data analysis tools, does not guarantee that the citizen is empowered to play his role. Therefore hence, the objective of this work is to identify overprice in the acquisition of products purchased by the federal government of Brazil using the unstructured data available on the Transparency Portal. The last two-years' worth of purchasing data, available in the Transparency Portal, were extracted, processed and stored. Due to his diverse nature and high volume of data, this study focused only on medicines purchased by the Ministry of Health. Ontology-based text mining and clustering techniques were applied for automatic identification and classification of products. The processing of this information was done through text mining and clustering, based on the ontology registered in another database of the Brazilian government. Because of this work, a consolidated price base per medication was created to allow the identification of distortions in prices practised, facilitating the identification of cases that merit further investigation to unravel fraud to the treasury.",
        "link": "https://dl.acm.org/doi/10.1145/3264560.3264569"
    },
    {
        "title": "Computational Estimation by Scientific Data Mining with Classical Methods to Automate Learning Strategies of Scientists",
        "authors": [
            "Aparna S. Varde"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Experimental results are often plotted as 2-dimensional graphical plots (aka graphs) in scientific domains depicting dependent versus independent variables to aid visual analysis of processes. Repeatedly performing laboratory experiments consumes significant time and resources, motivating the need for computational estimation. The goals are to estimate the graph obtained in an experiment given its input conditions, and to estimate the conditions that would lead to a desired graph. Existing estimation approaches often do not meet accuracy and efficiency needs of targeted applications. We develop a computational estimation approach called AutoDomainMine that integrates clustering and classification over complex scientific data in a framework so as to automate classical learning methods of scientists. Knowledge discovered thereby from a database of existing experiments serves as the basis for estimation. Challenges include preserving domain semantics in clustering, finding matching strategies in classification, striking a good balance between elaboration and conciseness while displaying estimation results based on needs of targeted users, and deriving objective measures to capture subjective user interests. These and other challenges are addressed in this work. The AutoDomainMine approach is used to build a computational estimation system, rigorously evaluated with real data in Materials Science. Our evaluation confirms that AutoDomainMine provides desired accuracy and efficiency in computational estimation. It is extendable to other science and engineering domains as proved by adaptation of its sub-processes within fields such as Bioinformatics and Nanotechnology.",
        "link": "https://dl.acm.org/doi/10.1145/3502736"
    },
    {
        "title": "Where is the road for issue reports classification based on text mining?",
        "authors": [
            "Qiang Fan",
            "Yue Yu",
            "Gang Yin",
            "Tao Wang",
            "Huaimin Wang"
        ],
        "date": "November 2017",
        "source": "ESEM '17: Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Currently, open source projects receive various kinds of issues daily, because of the extreme openness of Issue Tracking System (ITS) in GitHub. ITS is a labor-intensive and time-consuming task of issue categorization for project managers. However, a contributor is only required a short textual abstract to report an issue in GitHub. Thus, most traditional classification approaches based on detailed and structured data (e.g., priority, severity, software version and so on) are difficult to adopt. In this paper, issue classification approaches on a large-scale dataset, including 80 popular projects and over 252,000 issue reports collected from GitHub, were investigated. First, four traditional text-based classification methods and their performances were discussed. Semantic perplexity (i.e., an issues description confuses bug-related sentences with nonbug-related sentences) is a crucial factor that affects the classification performances based on quantitative and qualitative study. Finally, A two-stage classifier framework based on the novel metrics of semantic perplexity of issue reports was designed. Results show that our two-stage classification can significantly improve issue classification performances.",
        "link": "https://dl.acm.org/doi/10.1109/ESEM.2017.19"
    },
    {
        "title": "Feature-based Facebook reviews process model for e-management using data mining",
        "authors": [
            "Anish Kumar Varudharajulu",
            "Yongsheng Ma"
        ],
        "date": "January 2019",
        "source": "IC4E '19: Proceedings of the 10th International Conference on E-Education, E-Business, E-Management and E-Learning",
        "abstract": "The data generated from online communication acts as potential gold mines for discovering knowledge for researchers. A large amount of data is also generated in the form of web documents, emails, blogs, and feedback, etc. Text analytics is being significantly employed to mine important information. Opinion mining is the process of extracting human thoughts and perceptions from unstructured texts. The showstopper for designing an opinion mining system for analyzing reviews arise from the fact that customer reviews are often noisy. These reviews are informally written. In addition, they are subjected to spelling mistakes, grammatical errors, improper punctuation and irrational capitalization. This paper focuses on analyzing the different classification and clustering algorithms aimed at extracting and consolidating opinions of customers from social media sites like Facebook, Twitter and through surveys, at multiple levels of granularity to monitor and measure customer satisfaction. Ours is an automated approach, in which the system aids in the process of knowledge assimilation for knowledge-based building and also performs the analytics. Domain experts ratify the knowledge base and also provide training datasets for the system to intuitively gather more instances for ratification. The system identifies opinion expressions as phrases containing opinion words, opinionated features and also opinion modifiers. These expressions are categorized as positive, negative or neutral. Opinion expressions are identified and categorized using localized linguistic techniques. Opinions can be congregated at any desired level of specificity i.e. feature level or product level, user level or service level, etc. We have developed a system based on this approach, which provides the user with a platform to analyze opinion expressions crawled from a set of pre-defined datasets.",
        "link": "https://dl.acm.org/doi/10.1145/3306500.3306514"
    },
    {
        "title": "Discriminative Topic Mining via Category-Name Guided Text Embedding",
        "authors": [
            "Yu Meng",
            "Jiaxin Huang",
            "Guangyuan Wang",
            "Zihan Wang",
            "Chao Zhang",
            "Yu Zhang",
            "Jiawei Han"
        ],
        "date": "April 2020",
        "source": "WWW '20: Proceedings of The Web Conference 2020",
        "abstract": "Mining a set of meaningful and distinctive topics automatically from massive text corpora has broad applications. Existing topic models, however, typically work in a purely unsupervised way, which often generate topics that do not fit users’ particular needs and yield suboptimal performance on downstream tasks. We propose a new task, discriminative topic mining, which leverages a set of user-provided category names to mine discriminative topics from text corpora. This new task not only helps a user understand clearly and distinctively the topics he/she is most interested in, but also benefits directly keyword-driven classification tasks. We develop CatE, a novel category-name guided text embedding method for discriminative topic mining, which effectively leverages minimal user guidance to learn a discriminative embedding space and discover category representative terms in an iterative manner. We conduct a comprehensive set of experiments to show that CatE mines high-quality set of topics guided by category names only, and benefits a variety of downstream applications including weakly-supervised classification and lexical entailment direction identification.",
        "link": "https://dl.acm.org/doi/10.1145/3366423.3380278"
    },
    {
        "title": "Comparative Study of Heart Disease Diagnosis Using Top Ten Data Mining Classification Algorithms",
        "authors": [
            "I. Ketut Agung Enriko"
        ],
        "date": "June 2019",
        "source": "ICFET '19: Proceedings of the 5th International Conference on Frontiers of Educational Technologies",
        "abstract": "Data mining has been used for many purposes, especially for prediction system. In healthcare, data mining algorithms often used in disease diagnosis. Meanwhile, heart disease is known as a primary cause of death over the years. Many studies have been performed in heart disease diagnosis using data mining methods. There are some popular data mining algorithms that can be used in heart disease diagnosis, for example, k-Nearest Neighbor, CART, and AdaBoost. The algorithms are used to analyze a sample of cardiovascular patients data and predict the heart disease type that they suffer. Some parameters are taken from the patient, including EKG morphology, blood pressure, and information about the existence of chest pain, shortness of breath, palpitation, and cold sweat. In this study, medical records data are collected from Harapan Kita Hospital and utilized as a dataset sample in this research. Top ten data mining classification algorithms are used in diagnosing heart disease from Harapan Kita Hospital data and examining their performance by checking the accuracy and speed.",
        "link": "https://dl.acm.org/doi/10.1145/3338188.3338220"
    },
    {
        "title": "Text Mining Approach for Identifying Research Trends",
        "authors": [
            "Snezhana Sulova"
        ],
        "date": "June 2021",
        "source": "CompSysTech '21: Proceedings of the 22nd International Conference on Computer Systems and Technologies",
        "abstract": "With the increase of unstructured data, the issues connected with automatic text processing, the categorization of documents and the discovery of topics have become objects of growing interest. In order to improve the process of grouping and processing research publications, we would like to propose a method based upon natural language processing. It is based on text mining technologies which aim to identify key tendencies in documents. It processes the content of publications by clustering and identifies the topics of each identified group. This analysis helps by identifying key tendencies as well as discovering emerging new areas of research. Publications from the research literature database, Scopus, were used to test the approach. The topic of the publications is “the application of digital technologies in the logistics business”. The experiments were completed using the RapidMiner Studio software.",
        "link": "https://dl.acm.org/doi/10.1145/3472410.3472433"
    },
    {
        "title": "Assessment of Vulnerability Severity using Text Mining",
        "authors": [
            "Georgios Spanos",
            "Lefteris Angelis",
            "Dimitrios Toloudis"
        ],
        "date": "September 2017",
        "source": "PCI '17: Proceedings of the 21st Pan-Hellenic Conference on Informatics",
        "abstract": "Software1 vulnerabilities are closely associated with information systems security, a major and critical field in today's technology. Vulnerabilities constitute a constant and increasing threat for various aspects of everyday life, especially for safety and economy, since the social impact from the problems that they cause is complicated and often unpredictable. Although there is an entire research branch in software engineering that deals with the identification and elimination of vulnerabilities, the growing complexity of software products and the variability of software production procedures are factors contributing to the ongoing occurrence of vulnerabilities, Hence, another area that is being developed in parallel focuses on the study and management of the vulnerabilities that have already been reported and registered in databases. The information contained in such databases includes, a textual description and a number of metrics related to vulnerabilities. The purpose of this paper is to investigate to what extend the assessment of the vulnerability severity can be inferred directly from the corresponding textual description, or in other words, to examine the informative power of the description with respect to the vulnerability severity. For this purpose, text mining techniques, i.e. text analysis and three different classification methods (decision trees, neural networks and support vector machines) were employed. The application of text mining to a sample of 70,678 vulnerabilities from a public data source shows that the description itself is a reliable and highly accurate source of information for vulnerability prioritization.",
        "link": "https://dl.acm.org/doi/10.1145/3139367.3139390"
    },
    {
        "title": "Clustering Algorithms for Spatial Data Mining",
        "authors": [
            "Chetashri Bhadane",
            "Ketan Shah"
        ],
        "date": "April 2020",
        "source": "ICGDA '20: Proceedings of the 2020 3rd International Conference on Geoinformatics and Data Analysis",
        "abstract": "With the advances in mobile and wireless technologies, there has been a rise in applications that track and share the users' geospatial data. People use several social networking sites such as Twitter, Facebook and Flickr, where they share their status updates. With the integration of Global Positioning System (GPS) with mobile phones, it is now possible to share one's locations on these social networks. GPS allows us to record and track a person's movement along with the timestamp. The data set obtained from these GPS logs is vast and is widely used to analyze the users' movement patterns. Specifically, we can find out significant locations based on the number of users present at that location and the time spent by them at such places. Once significant places have been identified, it is also possible to identify the semantic importance of these locations. This paper presents an overview of the clustering techniques used to find important places of interest using large GPS based mobility datasets. Four clustering algorithms, K-Means, DBSCAN, OPTICS and Hierarchical, are implemented, and performance is tested using real-time data of 50 users collected over 2--5 years. Performance summary depicts that K-Means and DBSCAN perform well for spatial data.",
        "link": "https://dl.acm.org/doi/10.1145/3397056.3397068"
    },
    {
        "title": "Feature Selection based Arabic Text Classification using Different Machine Learning Algorithms: Comparative Study",
        "authors": [
            "Sakina Rim Bennabi",
            "Zakaria Elberrichi"
        ],
        "date": "June 2020",
        "source": "ICIST '20: Proceedings of the 10th International Conference on Information Systems and Technologies",
        "abstract": "Feature selection is a method of data pre-processing widely used when mining large data, such as textual classification. Several studies have been conducted to compare the different methods of feature selection applied to corpora in English. Unfortunately, a small number of works concern the Arabic language. This article aims to present a comparative study of different feature selection techniques including: Chi2, the ANOVA method and mutual information, applied on a corpus in Arabic language, while also diversifying the machine learning algorithms (Naive Bayes, SVM and KNN). This experimental study has shown in general that reducing dimensionality with feature selection techniques has slightly affected the performance of textual classification, reducing the size of the corpus by up to 1%.",
        "link": "https://dl.acm.org/doi/10.1145/3447568.3448531"
    },
    {
        "title": "Intelligent Data Analysis using Optimized Support Vector Machine Based Data Mining Approach for Tourism Industry",
        "authors": [
            "Ms Promila Sharma",
            "Uma Meena",
            "Girish Kumar Sharma"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Data analysis involves the deployment of sophisticated approaches from data mining methods, information theory, and artificial intelligence in various fields like tourism, hospitality, and so on for the extraction of knowledge from the gathered and preprocessed data. In tourism, pattern analysis or data analysis using classification is significant for finding the patterns that represent new and potentially useful information or knowledge about the destination and other data. Several data mining techniques are introduced for the classification of data or patterns. However, overfitting, less accuracy, local minima, sensitive to noise are the drawbacks in some existing data mining classification methods. To overcome these challenges, Support vector machine with Red deer optimization (SVM-RDO) based data mining strategy is proposed in this article. Extended Kalman filter (EKF) is utilized in the first phase, i.e., data cleaning to remove the noise and missing values from the input data. Mantaray foraging algorithm (MaFA) is used in the data selection phase, in which the significant data are selected for the further process to reduce the computational complexity. The final phase is the classification, in which SVM-RDO is proposed to access the useful pattern from the selected data. PYTHON is the implementation tool used for the experiment of the proposed model. The experimental analysis is done to show the efficacy of the proposed work. From the experimental results, the proposed SVM-RDO achieved better accuracy, precision, recall, and F1 score than the existing methods for the tourism dataset. Thus, it is showed the effectiveness of the proposed SVM-RDO for pattern analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3494566"
    },
    {
        "title": "Applying text mining to predict learners' cognitive engagement",
        "authors": [
            "Hayati Hind",
            "Mohammed Khalidi Idrissi",
            "Samir Bennani"
        ],
        "date": "October 2017",
        "source": "SCAMS '17: Proceedings of the Mediterranean Symposium on Smart City Application",
        "abstract": "In academic process, engagement represent one of the important success' factor. This paper proposes a predictive system for learners' cognitive engagement based on their online discussion forums participation. In the first level the ontology OWL and the LSA method are used to perform a semantic classification system of the threads according to a specific context chosen by the tutor. Then the Text mining, as a predictive method, is applied to the classified threads and learners' participation in the forums.",
        "link": "https://dl.acm.org/doi/10.1145/3175628.3175655"
    },
    {
        "title": "Investigate Transitions into Drug Addiction through Text Mining of Reddit Data",
        "authors": [
            "John Lu",
            "Sumati Sridhar",
            "Ritika Pandey",
            "Mohammad Al Hasan",
            "Georege Mohler"
        ],
        "date": "July 2019",
        "source": "KDD '19: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Increasing rates of opioid drug abuse and heightened prevalence of online support communities underscore the necessity of employing data mining techniques to better understand drug addiction using these rapidly developing online resources. In this work, we obtained data from Reddit, an online collection of forums, to gather insight into drug use/misuse using text snippets from users narratives. Specifically, using users' posts, we trained a binary classifier which predicts a user's transitions from casual drug discussion forums to drug recovery forums. We also proposed a Cox regression model that outputs likelihoods of such transitions. In doing so, we found that utterances of select drugs and certain linguistic features contained in one's posts can help predict these transitions. Using unfiltered drug-related posts, our research delineates drugs that are associated with higher rates of transitions from recreational drug discussion to support/recovery discussion, offers insight into modern drug culture, and provides tools with potential applications in combating the opioid crisis.",
        "link": "https://dl.acm.org/doi/10.1145/3292500.3330737"
    },
    {
        "title": "Analysing digital banking reviews using text mining",
        "authors": [
            "Li Chen Cheng",
            "Legaspi Rhea Sharmayne"
        ],
        "date": "December 2020",
        "source": "ASONAM '20: Proceedings of the 12th IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Digital banks are new entrants in the banking industry in the Philippines as they only started late 2018. Since then, a handful of players have and are still emerging. With more and more people becoming technologically savvy, it is very critical for financial institutions to develop a digital banking application that will stand out from the competition. This paper aims to use text mining methods to analyse digital banking application reviews. This study will perform topic modelling using LDA to explore customer concerns and will mine association rules between the digital banking features with the review score. The results will reveal which areas the digital banking application can further optimize for customer satisfaction and retention.",
        "link": "https://dl.acm.org/doi/10.1109/ASONAM49781.2020.9381429"
    },
    {
        "title": "Mining Moocs Videos Metadata Using Classification Techniques",
        "authors": [
            "El Harrak Othman",
            "Ghadi Abderrahim",
            "El Bouhdidi Jaber"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "In few years, the internet has become the first source of information for most people, Today MOOCs makes it possible for everyone to access to the education over the world, it's represents an emerging methodology of online teaching and an important development in open education. Due to the rapid development of Moocs and the rapid growth of digital data and video database over the Internet, it is becoming very difficult for learners to browsing and choosing the best training for them. The scientific community has increased the amount of research into new technologies, with a view to improve the digital video utilization: its archiving, indexing, accessibility, acquisition, store and even its process and usability. All these parts of the video utilization entail the necessity of the extraction of all important information of a video, especially in cases of lack of metadata information. Web video mining is retrieving the content using data mining techniques from World Wide Web. There are two approaches for web video mining using traditional image processing (signal processing) and metadata based approach. In this paper, we present the various research makes in this subject and we propose an effective methodology to extract the metadata from Moocs videos and classify them based on the extracted metadata by applying data mining techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090450"
    },
    {
        "title": "Text mining with HathiTrust",
        "authors": [
            "Eleanor Dickson Koehl",
            "Ryan Dubnicek"
        ],
        "date": "June 2019",
        "source": "JCDL '19: Proceedings of the 18th Joint Conference on Digital Libraries",
        "abstract": "This tutorial will introduce attendees to the HathiTrust Research Center and its tools and services for computational text analysis research. HTRC leverages the scope and scale of the HathiTrust Digital Library collection to create opportunities for researchers to perform text data mining on subsets of the corpus. Attendees will develop skills that will allow them to conduct basic text analysis research using HathiTrust data, learn techniques for engaging with scholars on their text data mining research, and explore strategies for computational access to large-scale digital collections.",
        "link": "https://dl.acm.org/doi/10.1109/JCDL.2019.00115"
    },
    {
        "title": "Text mining for plagiarism detection: multivariate pattern detection for recognition of text similarities",
        "authors": [
            "Konstantinos Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2018",
        "source": "ASONAM '18: Proceedings of the 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "The problem of plagiarism the recent years has been intensified by the availability of information in digital form and the accessibility of the electronic libraries through the Internet. As a result, plagiarism detection has been transformed into a big data analytics problem since the number of digital sources is extravagant and a new document needs to be compared with millions of other existing documents. In this paper, a text mining methodology is proposed that can detect all common patterns between a document and the documents in a reference database. The technique is based on a pattern detection algorithm and the corresponding data structure that enables the algorithm to detect all common patterns. The methodology has been applied in a well-defined dataset providing very promising results identifying difficult cases of plagiarism such as technical disguise.",
        "link": "https://dl.acm.org/doi/10.5555/3382225.3382424"
    },
    {
        "title": "Petroleum Engineering Data Text Classification Using Convolutional Neural Network Based Classifier",
        "authors": [
            "Tzuhan Hsu",
            "Yaoqin Zhang"
        ],
        "date": "May 2018",
        "source": "ICMLT '18: Proceedings of the 2018 International Conference on Machine Learning Technologies",
        "abstract": "In this paper, authors introduce a convolutional neural network (CNN) based classification approach for petroleum engineering event. Recently, text classification problem has raised substantial attention in the field of machine learning. The aim of text classification process is to assign correct label to a text on the basis of its content. However, due to the imprecise of feature representation, most of former text classification methods cannot export satisfying result. To solve this problem, authors propose a model based on convolutional neural network. Authors conduct experiments with real world petroleum engineering data text, artificially labelled into 6 categories. Beside purposed CNN-based model, the authors also implement other baseline machine learning approaches: k nearest neighbor, random forest, support vector machines, recurrent neural network, gated recurrent units, long short-term memory. Among all these models, our CNN-based model reach best performance in text classification task. In the last part of this paper, authors compare and analysis their purpose and baseline model.",
        "link": "https://dl.acm.org/doi/10.1145/3231884.3231898"
    },
    {
        "title": "Classifying Indonesian Online Articles as Advertisement Placement Base Using Text Mining",
        "authors": [
            "Nadhira Tasya",
            "Arian Dhini"
        ],
        "date": "July 2017",
        "source": "ICBIM 2017: Proceedings of the International Conference on Business and Information Management",
        "abstract": "Rapid development in technological aspect resulting in growing level of human needs for the latest news, so that emerged a new trend of publishing and accessing news through online media or usually called online journalism. In addition, the number of people who sell and purchase through online sites also continues to increase and this opportunity is utilized by the company and the advertiser by implementing targeted web advertising. However, the high number of articles that have been published and accessed leads to great opportunities for errors in determining where to place the ads. Therefore, it needs a system that can categorize articles accessed by users as the basis of advertisement placement by the company and this classification system can be done by applying the method of Data Mining and Text Mining. This research uses document data in the form of article content that will be categorized into twenty categories of class of advertisement by using Text Mining technique with Support Vector Machine algorithm. The results of this study may be used by companies or advertisers as a basis for placement of ads on selected online media sites.",
        "link": "https://dl.acm.org/doi/10.1145/3134271.3134288"
    },
    {
        "title": "Mining Free-Text Medical Notes for Suicide Risk Assessment",
        "authors": [
            "Marios Adamou",
            "Grigoris Antoniou",
            "Elissavet Greasidou",
            "Vincenzo Lagani",
            "Paulos Charonyktakis",
            "Ioannis Tsamardinos"
        ],
        "date": "July 2018",
        "source": "SETN '18: Proceedings of the 10th Hellenic Conference on Artificial Intelligence",
        "abstract": "Suicide has been considered as an important public health issue for a very long time, and is one of the main causes of death worldwide. Despite suicide prevention strategies being applied, the rate of suicide has not changed substantially over the past decades. Advances in machine learning make it possible to attempt to predict suicide based on the analysis of relevant data to inform clinical practice. This paper reports on findings from the analysis of data of patients who died by suicide in the period 2013-2016 and made use of both structured data and free-text medical notes. We focus on examining various text-mining approaches to support risk assessment. The results show that using advance machine learning and text-mining techniques, it is possible to predict within a specified period which people are most at risk of taking their own life at the time of referral to a mental health service.",
        "link": "https://dl.acm.org/doi/10.1145/3200947.3201020"
    },
    {
        "title": "Integration of data mining techniques in e-learning systems: Clustering Profil of Lerners and Recommender Course System",
        "authors": [
            "A. El Moustamid",
            "E. En-Naimi",
            "J. El Bouhdidi"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "Data mining aims at the extraction of knowledge from large quantities of data by automatic or semi-automatic methods. In the field of education and training e-learning, Data Mining techniques as clustering (The aim of this technique is to identify groups that are similar in some aspect) [21], prediction (is used for developing model where combination of data viewed from different aspects can effect single viewed data. Here, is important to watch how predictors variables are effecting predicted variable)[21], classification are applied in e-learning to improve the level of learners by providing learning according to the learners' requirement) are used to analyze learners profile, to predict and improve student performance. The aim of our project is to develop a system capable of analyzing learners profile and indexing web videos in order to offer learners a rich database with courses that correspond to their levels.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090453"
    },
    {
        "title": "Multi-level mining and visualization of scientific text collections: Exploring a bi-lingual scientific repository",
        "authors": [
            "Pablo Accuosto",
            "Francesco Ronzano",
            "Daniel Ferrés",
            "Horacio Saggion"
        ],
        "date": "December 2017",
        "source": "WOSP 2017: Proceedings of the 6th International Workshop on Mining Scientific Publications",
        "abstract": "We present a system to mine and visualize collections of scientific documents by semantically browsing information extracted from single publications or aggregated throughout corpora of articles. The text mining tool performs deep analysis of document collections allowing the extraction and interpretation of research paper's contents. In addition to the extraction and enrichment of documents with metadata (titles, authors, affiliations, etc), the deep analysis performed comprises semantic interpretation, rhetorical analysis of sentences, triple-based information extraction, and text summarization. The visualization components allow geographical-based exploration of collections, topic-evolution interpretation, and collaborative network analysis among others. The paper presents a case study of a bi-lingual collection in the field of Natural Language Processing (NLP).",
        "link": "https://dl.acm.org/doi/10.1145/3127526.3127529"
    },
    {
        "title": "Hierarchical Topic Mining via Joint Spherical Tree and Text Embedding",
        "authors": [
            "Yu Meng",
            "Yunyi Zhang",
            "Jiaxin Huang",
            "Yu Zhang",
            "Chao Zhang",
            "Jiawei Han"
        ],
        "date": "August 2020",
        "source": "KDD '20: Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Mining a set of meaningful topics organized into a hierarchy is intuitively appealing since topic correlations are ubiquitous in massive text corpora. To account for potential hierarchical topic structures, hierarchical topic models generalize flat topic models by incorporating latent topic hierarchies into their generative modeling process. However, due to their purely unsupervised nature, the learned topic hierarchy often deviates from users' particular needs or interests. To guide the hierarchical topic discovery process with minimal user supervision, we propose a new task, Hierarchical Topic Mining, which takes a category tree described by category names only, and aims to mine a set of representative terms for each category from a text corpus to help a user comprehend his/her interested topics. We develop a novel joint tree and text embedding method along with a principled optimization procedure that allows simultaneous modeling of the category tree structure and the corpus generative process in the spherical space for effective category-representative term discovery. Our comprehensive experiments show that our model, named JoSH, mines a high-quality set of hierarchical topics with high efficiency and benefits weakly-supervised hierarchical text classification tasks.",
        "link": "https://dl.acm.org/doi/10.1145/3394486.3403242"
    },
    {
        "title": "Unsupervised behavioural mining and clustering for malware family identification",
        "authors": [
            "Khanh Huu The Dam",
            "Thomas Given-Wilson",
            "Axel Legay"
        ],
        "date": "March 2021",
        "source": "SAC '21: Proceedings of the 36th Annual ACM Symposium on Applied Computing",
        "abstract": "More accurate and advanced detection and classification of malware requires exploiting program behaviour and not merely syntactic or static features. One approach is to use system call dependency graphs (SCDGs) that represent the program behaviour by interactions with the system, and the relations between these interactions. These SCDGs have been used with supervised learning techniques to very accurately detect and classify malware. This works considers the unsupervised learning challenge of mining for common clusters of behaviour without a priori knowledge. This allows for clustering of similar programs by behaviour, that can then be used for either classification or further analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3412841.3441919"
    },
    {
        "title": "Text Mining for Evaluating Authors' Birth and Death Years",
        "authors": [
            "Dror Moghaz",
            "Yaakov Hacohen-Kerner",
            "Dov Gabbay"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "This article presents a unique method in text and data mining for finding the era, i.e., mining temporal data, in which an anonymous author was living. Finding this era can assist in the examination of a fake document or extracting the time period in which a writer lived. The study and the experiments concern Hebrew, and in some parts, Aramaic and Yiddish rabbinic texts. The rabbinic texts are undated and contain no bibliographic sections, posing an interesting challenge. This work proposes algorithms using key phrases and key words that allow the temporal organization of citations together with linguistic patterns. Based on these key phrases, key words, and the references, we established several types of “Iron-clad,” Heuristic and Greedy rules for estimating the years of birth and death of a writer in an interesting classification task. Experiments were conducted on corpora, including documents authored by 12, 24, and 36 rabbinic writers and demonstrated promising results.",
        "link": "https://dl.acm.org/doi/10.1145/3281631"
    },
    {
        "title": "Transfer Learning to Timed Text Based Video Classification Using CNN",
        "authors": [
            "Zenun Kastrati",
            "Ali Shariq Imran",
            "Arianit Kurti"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "Open educational video resources are gaining popularity with a growing number of massive open online courses (MOOCs). This has created a niche for content providers to adopt effective solutions in automatically organizing and structuring of educational resources for maximum visibility. Recent advances in deep learning techniques are proving useful in managing and classifying resources into appropriate categories. This paper proposes one such convolutional neural network (CNN) model for classifying video lectures in a MOOC setting using a transfer learning approach. The model uses a time-aligned text transcripts corresponding to video lectures from six broader subject categories. Video lectures and their corresponding transcript dataset is gathered from the Coursera MOOC platform. Two different CNN models are proposed: i) CNN based classification using embeddings learned from our MOOC dataset, ii) CNN based classification using transfer learning. Word embeddings generated from two well known state-of-the-art pre-trained models Word2Vec and GloVe, are used in the transfer learning approach for the second case. The proposed CNN models are evaluated using precision, recall, and F1 score and the obtained performance is compared with both conventional and deep learning classifiers. The proposed CNN models have an F1 score improvement of 10-22 percentage points over DNN and conventional classifiers",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326483"
    },
    {
        "title": "Evolutionary role mining in complex networks by ensemble clustering",
        "authors": [
            "Sarvenaz Choobdar",
            "Pedro Ribeiro",
            "Fernando Silva"
        ],
        "date": "April 2017",
        "source": "SAC '17: Proceedings of the Symposium on Applied Computing",
        "abstract": "The structural patterns in the neighborhood of nodes assign unique roles to the nodes. Mining the set of existing roles in a network provides a descriptive profile of the network and draws its general picture. This paper proposes a new method to determine structural roles in a dynamic network based on the current position of nodes and their historic behavior. We develop a temporal ensemble clustering technique to dynamically find groups of nodes, holding similar tempo-structural roles. We compare two weighting functions, based on age and distribution of data, to incorporate temporal behavior of nodes in the role discovery. To evaluate the performance of the proposed method, we assess the results from two points of view: 1) goodness of fit to current structure of the network; 2) consistency with historic data. We conduct the evaluation using different ensemble clustering techniques. The results on real world networks demonstrate that our method can detect tempo-structural roles that simultaneously depict the topology of a network and reflect its dynamics with high accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3019612.3019815"
    },
    {
        "title": "Analyzing used-car web listings via text mining",
        "authors": [
            "Ayhan Demiriz",
            "Fatma Cantaş"
        ],
        "date": "October 2017",
        "source": "IML '17: Proceedings of the 1st International Conference on Internet of Things and Machine Learning",
        "abstract": "Used car trade is one of the major components of the world economies. It is not uncommon to sell a car by placing an internet advertisement irrespective of the geography in these days. A typical content of an advertisement is usually composed of two parts namely the structured and the free text data. The structured data may include some information about the asking price, make, model, year, mileage of the car and the contact info. In most cases, seller may give important clues about the car's current conditions in the free text data where the title (head) of the advertisement can be included as free text too. This paper reports preliminary results from a text mining study conducted on 75K used car internet listings collected from two major car listing web sites in Turkey. As expected, the words and the phrases related to the description of the car are observed to be frequent. The leading concepts in the free text are found to be regarding how to describe the current condition of a car, for example \"no crash history\".",
        "link": "https://dl.acm.org/doi/10.1145/3109761.3109782"
    },
    {
        "title": "Text classification of Diseases Treated by Traditional Chinese Medicine Prescription based on machine learning",
        "authors": [
            "Hanqing Zhao",
            "Bo Han",
            "Chen Li"
        ],
        "date": "September 2020",
        "source": "ISAIMS '20: Proceedings of the 1st International Symposium on Artificial Intelligence in Medical Sciences",
        "abstract": "OBJECTIVE: To explore the application of machine learning in th e identification of diseases treated by traditional Chinese medicine prescriptions. METHODS: Based on the composition of the text document of Chinese medicine prescriptions, the prescriptions were divided into cough, headache and diarrhea. THUTCT were introduced to establish and train two machine learning text classification models, LibSVM and LibLinear, and the prescriptions to be deter mined were put into the model for classification and prediction. R ESULTS: The Precision rate of LibSVM model and Liblinear mod el were 0.7283 and 0.6690. Seven prescriptions were classified an d predicted, and the results were in line with expectations. CONCLUSION: THUCTC has good universality for the content of TCM prescriptions, high classification accuracy and fast testing speed, which is suitable for the text classification and discrimination research of TCM prescriptions for diseases.",
        "link": "https://dl.acm.org/doi/10.1145/3429889.3429895"
    },
    {
        "title": "Evaluation of retweet clustering method classification method using retweets on Twitter without text data",
        "authors": [
            "K. Uchida",
            "F. Toriumi",
            "T. Sakaki"
        ],
        "date": "August 2017",
        "source": "WI '17: Proceedings of the International Conference on Web Intelligence",
        "abstract": "Burst phenomena, which frequently occur on social media, are caused by such social events as flaming on the internet, elections, and natural disasters. To understand people's thoughts and feelings, we must classify their opinions from burst phenomena. Therefore, classification methods that categorize tweets are critical. However, since most classification methods focus on text mining, they cannot group tweets by topics because each tweet has poor linguistic similarities. We used a non-text-based classification method proposed by Baba et al. that groups tweets by topics, even if they have poor linguistic similarities, and verified its validity by comparing it with a text-based classification method in two different evaluations: qualitative and quantitative. In the qualitative evaluation part, we did a questionnaire survey and validated the suitability of the topic clusters created using both the non-and text-based methods. Since evaluating the similarity of every pair of tweets in each topic is difficult, we evaluated the similarity between sampled pairs in the survey and acquired more appropriate topic clustering results using the non-text-based method than the text-based method. In the quantitative evaluation part, we focused on the robustness of each method against data reduction. Many approaches analyze social media data, especially because collecting data from social media is comparatively easy. However, since collecting the whole data of burst phenomena is very costly due to the vast amounts of available social media data, robustness against data reduction is an important index to evaluate classification methods. With the non-text-based method, over 55% of the pairs of tweets in the same cluster were also included in the same cluster even when the data were reduced to 10% in all three of our example cases. In this paper, as a source we focus on Twitter, one of the most popular microblogging services. Using clustering to conduct detailed case analyses, we scrutinized three burst cases that include natural disasters and flaming on the internet and found that a non-text-based method more effectively classified tweets in burst phenomena than a text-based method.",
        "link": "https://dl.acm.org/doi/10.1145/3106426.3106451"
    },
    {
        "title": "Using Spark for Text Mining on Large Scale Liver Cancer Literature",
        "authors": [
            "Ming-Yen Lin",
            "Yu-Ju Lin",
            "Sue-Chen Hsueh"
        ],
        "date": "January 2021",
        "source": "BDET 2021: 2021 the 3rd International Conference on Big Data Engineering and Technology (BDET)",
        "abstract": "Cancer is one of the main causes of death. The number of known cancer to-date is more than one hundred. Liver cancer, ranked after trachea and lung cancer, has long been high in the cancer leading cause in Taiwan and even ranked as second in 2016. The number of scientific articles related to cancer proliferates every year. The number reaches as high as 20 million in PubMed so that discovering useful information from the massive collection is very difficult. In addition, using a single machine to sift through these articles is very time-consuming. Therefore, we present a big data analytic framework using the distributed Apache Spark platform for text mining in PubMed literature. We establish a prediction model for liver cancer articles so that researchers may effectively validate whether an article is related to liver cancer or not. Classification models in Spark MLlib including Linear Support Vector Machines (SVM), Logistic Regression, are used in our experiments. Relevancy to liver cancer is further confirmed by using MeSH (Medical Subject Headings) terms. Logistic regression is about 3 times faster than SVMs and the accuracy of both methods is close to 95% in the experiments using hold-out validation. When max_features is 500 and min_df ≤ 0.1 (or min_df = 1), the accuracy may reach 96%. In the experiments with K-fold cross-validation, the accuracy of SVMs methods is 96%. The experimental results show that that our prediction model may effectively classify liver cancer articles.",
        "link": "https://dl.acm.org/doi/10.1145/3474944.3474958"
    },
    {
        "title": "Automatic topic classification of test cases using text mining at an Android smartphone vendor",
        "authors": [
            "Junji Shimagaki",
            "Yasutaka Kamei",
            "Naoyasu Ubayashi",
            "Abram Hindle"
        ],
        "date": "October 2018",
        "source": "ESEM '18: Proceedings of the 12th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Background: An Android smartphone is an ecosystem of applications, drivers, operating system components, and assets. The volume of the software is large and the number of test cases needed to cover the functionality of an Android system is substantial. Enormous effort has been already taken to properly quantify \"what features and apps were tested and verified?\". This insight is provided by dashboards that summarize test coverage and results per feature. One method to achieve this is to manually tag or label test cases with the topic or function they cover, much like function points. At the studied Android smartphone vendor, tests are labelled with manually defined tags, so-called \"feature labels (FLs)\", and the FLs serve to categorize 100s to 1000s test cases into 10 to 50 groups. Aim: Unfortunately for developers, manual assignment of FLs to 1000s of test cases is a time consuming task, leading to inaccurately labeled test cases, which will render the dashboard useless. We created an automated system that suggests tags/labels to the developers for their test cases rather than manual labeling. Method: We use machine learning models to predict and label the functionality tested by 10,000 test cases developed at the company. Results: Through the quantitative experiments, our models achieved acceptable F-1 performance of 0.3 to 0.88. Also through the qualitative studies with expert teams, we showed that the hierarchy and path of tests was a good predictor of a feature's label. Conclusions: We find that this method can reduce tedious manual effort that software developers spent classifying test cases, while providing more accurate classification results.",
        "link": "https://dl.acm.org/doi/10.1145/3239235.3268927"
    },
    {
        "title": "To apply Data Mining for Classification of Crowd sourced Software Requirements",
        "authors": [
            "Soonh Taj",
            "Qasim Arain",
            "Imran Memon",
            "Asma Zubedi"
        ],
        "date": "April 2019",
        "source": "ICSIE '19: Proceedings of the 8th International Conference on Software and Information Engineering",
        "abstract": "Now a day's main focus of developers is to build quality software that works according to customer needs and for this reason it is necessary to gather right requirements as requirement elicitation is the critical step that impacts on the success of software project as misinterpreted requirements leads to the failure of software project. By keeping this in mind a research is carried out on improving requirements elicitation process and automating the process of classifying requirements. In this research, a model is proposed which will help in this scenario for requirements elicitation and requirement classification. This paper presents a model in which crowd sourcing approach is used so that customers, end users, stakeholders, developers and software engineers can make active participation for requirement elicitation process and requirements gathered using crowdsourcing approach are used by model for classification process i.e. classification of requirements into functional and non-functional requirements. For the proof of proposed model a case study is conducted. Results of case study provided the usefulness and efficiency of proposed model for classification of crowd sourced software requirements.",
        "link": "https://dl.acm.org/doi/10.1145/3328833.3328837"
    },
    {
        "title": "Text classification using Fuzzy TF-IDF and Machine Learning Models",
        "authors": [
            "Mariem Bounabi",
            "Karim El Moutaouakil",
            "Khalid Satori"
        ],
        "date": "October 2019",
        "source": "BDIoT'19: Proceedings of the 4th International Conference on Big Data and Internet of Things",
        "abstract": "The representation of the information has an important impact on the text classification task. Several weighting methods were proposed in the literature, and the term frequency-inverse term frequency (TFIDF), the most know on the text treatment field. The FTF-IDF is a vector representation where the components of the TFIDF are presented as inputs to the Fuzzy Inference System (FIS). In this work, we compare several Machin Learning algorithms such as Naïve Bayes and its derivatives, SVM and Random forest classifiers, using the FTF-IDF representation. To improve the quality of the used classifiers, we call sum attribute selection methods. The recognition rate, for the tested systems, is satisfied, where the system based on naïve Bayes classifier, the FTF-IDF weighting terms, and the info gain select attributes method gives 98.7% as accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3372938.3372956"
    },
    {
        "title": "Text Mining in Cybersecurity: A Systematic Literature Review",
        "authors": [
            "Luciano Ignaczak",
            "Guilherme Goldschmidt",
            "Cristiano André Da Costa",
            "Rodrigo Da Rosa Righi"
        ],
        "date": null,
        "source": "ACM Computing Surveys",
        "abstract": "The growth of data volume has changed cybersecurity activities, demanding a higher level of automation. In this new cybersecurity landscape, text mining emerged as an alternative to improve the efficiency of the activities involving unstructured data. This article proposes a Systematic Literature Review (SLR) to present the application of text mining in the cybersecurity domain. Using a systematic protocol, we identified 2,196 studies, out of which 83 were summarized. As a contribution, we propose a taxonomy to demonstrate the different activities in the cybersecurity domain supported by text mining. We also detail the strategies evaluated in the application of text mining tasks and the use of neural networks to support activities involving unstructured data. The work also discusses text classification performance aiming its application in real-world solutions. The SLR also highlights open gaps for future research, such as the analysis of non-English content and the intensification in the usage of neural networks.",
        "link": "https://dl.acm.org/doi/10.1145/3462477"
    },
    {
        "title": "Using Text Mining to Analyze the Financial Patents",
        "authors": [
            "Yung-Feng Lu",
            "Jia-Lang Xu",
            "Mu-Yen Chen"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "With the advancement and development of science and technology, Nowadays society is the development of information technology, cloud computing, mobile Internet, Internet of Things, block chain and information technology.The combination of finance and technology has made financial technology terms gradually emerging.. This research analyzes the patents of financial technology. However, Taiwan's financial technology patents are mostly G06Q-20 (payment plan, architecture or agreement), G06Q-30 (commercial, such as marketing, shopping, payment, auction or e-commerce). G06Q-40 (financial, investment or tax treatment, insurance).The research use text mining technology to analyze and understand Taiwan's trends and developments in financial technology. The research results show that G06Q-20 patents are biased towards information security and mobile payment. G06Q-30 patents favor shopping websites and advertising. G06Q-40 patents are biased towards diversity, such as securities, insurance, finance, and stocks.",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326478"
    },
    {
        "title": "Identifying Online Profiles of Distance Learning Students Using Data Mining Techniques",
        "authors": [
            "Osama Islam",
            "Muazzam Siddiqui",
            "Naif Radi Aljohani"
        ],
        "date": "October 2019",
        "source": "ICDTE '19: Proceedings of the 3rd International Conference on Digital Technology in Education",
        "abstract": "Educational data has grown over the years with the increased use of technology within educational environments. This has led to a huge amount of data being stored in various data sources representing the student, his/her activities, and other aspects relevant to the learning process. To meet this analytical need, Educational Data Mining (EDM) has emerged to assist educational institutions in identifying key benefits such as students at risk, the level of student engagement or predicting student performance. The aim of this research was to explore the various aspects of student interaction data using data mining techniques to identify relevant patterns of behaviors and possible key attributes that have higher degrees of influence on distance learning students. The main findings identified several patterns of user online profiles based on a set of adopted learning strategies, the research proposed a framework for analyzing such interaction data based on R and Hadoop platforms to correlate online profiles with student performance.",
        "link": "https://dl.acm.org/doi/10.1145/3369199.3369249"
    },
    {
        "title": "Extremely Fast Decision Tree Mining for Evolving Data Streams",
        "authors": [
            "Albert Bifet",
            "Jiajin Zhang",
            "Wei Fan",
            "Cheng He",
            "Jianfeng Zhang",
            "Jianfeng Qian",
            "Geoff Holmes",
            "Bernhard Pfahringer"
        ],
        "date": "August 2017",
        "source": "KDD '17: Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
        "abstract": "Nowadays real-time industrial applications are generating a huge amount of data continuously every day. To process these large data streams, we need fast and efficient methodologies and systems. A useful feature desired for data scientists and analysts is to have easy to visualize and understand machine learning models. Decision trees are preferred in many real-time applications for this reason, and also, because combined in an ensemble, they are one of the most powerful methods in machine learning. In this paper, we present a new system called STREAMDM-C++, that implements decision trees for data streams in C++, and that has been used extensively at Huawei. Streaming decision trees adapt to changes on streams, a huge advantage since standard decision trees are built using a snapshot of data, and can not evolve over time. STREAMDM-C++ is easy to extend, and contains more powerful ensemble methods, and a more efficient and easy to use adaptive decision trees. We compare our new implementation with VFML, the current state of the art implementation in C, and show how our new system outperforms VFML in speed using less resources.",
        "link": "https://dl.acm.org/doi/10.1145/3097983.3098139"
    },
    {
        "title": "Data miners' little helper: data transformation activity cues for cluster analysis on document collections",
        "authors": [
            "Tania Cerquitelli",
            "Evelina Di Corso",
            "Francesco Ventura",
            "Silvia Chiusano"
        ],
        "date": "June 2017",
        "source": "WIMS '17: Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "In this paper we propose a new self-learning engine to streamline the analytics process, as it enables analysts to mine massive data repositories with minimal user intervention. In the context of cluster analysis on a collection of documents this new system, named SELF-DATA (SELF-learning DAta TrAnsformation), suggests to the analyst how to configure the whole mining process for a given dataset. SELF-DATA relies on an engine exploring different data weighting schemas (e.g., normalized term frequencies) and data transformation methods (e.g., PCA) before applying the cluster analysis, evaluating and comparing solutions through different quality indices (e.g., weighted Silhouette), and presenting the k-top solutions to the analyst. SELF-DATA will also include a knowledge base storing results of experiments on previously processed datasets, and a classification algorithm trained on the knowledge base content to forecast the best configuration for the whole mining process for an unexplored dataset. The first development of SELF-DATA running on Apache Spark has been validated on 5 collections of documents. Experimental results highlight that TF-IDF and logarithmic entropy are effective to measure item relevance with sparse datasets, and the LSI method outperforms PCA with a large dictionary.",
        "link": "https://dl.acm.org/doi/10.1145/3102254.3102288"
    },
    {
        "title": "Interactive Visual Graph Mining and Learning",
        "authors": [
            "Ryan A. Rossi",
            "Nesreen K. Ahmed",
            "Rong Zhou",
            "Hoda Eldardiry"
        ],
        "date": null,
        "source": "ACM Transactions on Intelligent Systems and Technology",
        "abstract": "This article presents a platform for interactive graph mining and relational machine learning called GraphVis. The platform combines interactive visual representations with state-of-the-art graph mining and relational machine learning techniques to aid in revealing important insights quickly as well as learning an appropriate and highly predictive model for a particular task (e.g., classification, link prediction, discovering the roles of nodes, and finding influential nodes). Visual representations and interaction techniques and tools are developed for simple, fast, and intuitive real-time interactive exploration, mining, and modeling of graph data. In particular, we propose techniques for interactive relational learning (e.g., node/link classification), interactive link prediction and weighting, role discovery and community detection, higher-order network analysis (via graphlets, network motifs), among others. GraphVis also allows for the refinement and tuning of graph mining and relational learning methods for specific application domains and constraints via an end-to-end interactive visual analytic pipeline that learns, infers, and provides rapid interactive visualization with immediate feedback at each change/prediction in real-time. Other key aspects include interactive filtering, querying, ranking, manipulating, exporting, as well as tools for dynamic network analysis and visualization, interactive graph generators (including new block model approaches), and a variety of multi-level network analysis techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3200764"
    },
    {
        "title": "Data Mining and Opinion Mining: A Tool in Educational Context",
        "authors": [
            "Myriam Peñafiel",
            "Stefanie Vásquez",
            "Diego Vásquez",
            "Juan Zaldumbide",
            "Sergio Luján-Mora"
        ],
        "date": "July 2018",
        "source": "ICoMS '18: Proceedings of the 2018 1st International Conference on Mathematics and Statistics",
        "abstract": "The use of the web as a universal communication platform generates large volumes of data (Big data), which in many cases, need to be processed so that they can become useful knowledge in face of the sceptics who have doubts about the credibility of such information. The use of web data that comes from educational contexts needs to be addressed, since that large amount of unstructured information is not being valued, losing valuable information that can be used. To solve this problem, we propose the use of data mining techniques such as sentiment analysis to validate the information that comes from the educational platforms. The objective of this research is to propose a methodology that allows the user to apply sentiment analysis in a simple way, because although some researchers have done it, very few do with data in the educational context. The results obtained prove that the proposal can be used in similar cases.",
        "link": "https://dl.acm.org/doi/10.1145/3274250.3274263"
    },
    {
        "title": "Research Hotspots Mining and Visualized Analysis Based on Linking Cluster and K-Core Decomposition",
        "authors": [
            "Ying Cai",
            "Fang Huang",
            "Shuai Wang",
            "Hao Zhang",
            "Chunxiu Du"
        ],
        "date": "May 2018",
        "source": "ICDPA 2018: Proceedings of the International Conference on Data Processing and Applications",
        "abstract": "In this paper, we focus on research hotspots mining with knowledge semantic features on discipline topic word networks, which are extracted from scientific and technical documents. For the topic networks, a novel method of combining linking community clustering with k-core decomposition is proposed to mine the hotspot community with knowledge hierarchy structure. And as a post-processing, the hierarchical community identification is implemented by k-core decomposition in order to discover the internal connection with knowledge semantic hierarchy in the research hotspot. Finally, the community density and visualization method is utilized to analyze the core and sub- problem in the research hotspot community. In experiments, the topic networks for a single discipline and interdisciplinary research hotspot are taken as examples of analysis, and the results show the proposed approach can effectively realize the objective of mining and analyzing the discipline research hotspot on the academic topic word network.",
        "link": "https://dl.acm.org/doi/10.1145/3224207.3224216"
    },
    {
        "title": "The Application of Data Mining Algorithms in the Construction of Travel Recommendation System",
        "authors": [
            "Chao Lou",
            "Zhaonan Mu",
            "Mengzhu Liu"
        ],
        "date": "January 2021",
        "source": "CONF-CDS 2021: The 2nd International Conference on Computing and Data Science",
        "abstract": "Photos shared by users of online social network often associated with tags, texts, geographic and time information. These data are ideal research resource which can be used by researchers to observe people's interests and behaviors. They can also be used to analyze people's travel behaviors and interests because it can reflect people's travel activities and experiences directly. This project aimed to consolidate people into groups based on their travel preferences of travel season, travel time span, scenery type of travel location and then analyze popular travel places for different people groups. The travel preferences were analyzed from extracted data applying PHP language and Naive Bayes classification approach. According to the analyzed data, people were divided into groups. Then, DBSCAN clustering was used to get popular locations based on the different groups. As a result, 80 thousand photo data of 5 cities have been collected from Flickr by Phython. It has been showed that people's preference of season in most of the mined cities was spring and summer and the preference of time span was short. Moreover, mined people has been divided into 16 groups according to their preference and based on the groups popular locations have been calculated by DBSCAN clustering. Theses locations have been provided in a recommendation website which was built in this project. For the future work, mining Flickr data associated with other travel social network sites could be a good way to build a more effective recommendation system.",
        "link": "https://dl.acm.org/doi/10.1145/3448734.3450477"
    },
    {
        "title": "Compare Machine Learning Models in Text Classification Using Steam User Reviews",
        "authors": [
            "Youchen Miao",
            "Zeyu Jin",
            "Yumeng Zhang",
            "Yuchen Chen",
            "Junren Lai"
        ],
        "date": "November 2021",
        "source": "ICSED '21: Proceedings of the 2021 3rd International Conference on Software Engineering and Development",
        "abstract": "Text Classification and Sentiment Analysis of game reviews are viewed as important parts in not only academic fields but also in game studies. In this paper, with more than 400 thousand game reviews on Steam platform, we preprocess the data using different libraries (sklearn, nltk, and spaCy) and use them as inputs to build three sentiment classification models based on different algorithms (Naive Bayes, SVM, and Random Forest). In contrast to previous studies that only focus on different sentiment analysis models, our paper also highlights the use of different APIs to preprocess the data and their corresponding model performance. The results show that no matter which API we choose, Random Forest models always perform the best. However, in terms of training time, Naive Bayes is the fastest. This work can be used to apply grid search for researchers to automatically find the optimum API before conducting sentiment analysis in the future.",
        "link": "https://dl.acm.org/doi/10.1145/3507473.3507480"
    },
    {
        "title": "Lost in Transduction: Transductive Transfer Learning in Text Classification",
        "authors": [
            "Alejandro Moreo",
            "Andrea Esuli",
            "Fabrizio Sebastiani"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Obtaining high-quality labelled data for training a classifier in a new application domain is often costly. Transfer Learning\n(a.k.a. “Inductive Transfer”) tries to alleviate these costs by transferring, to the “target” domain of interest, knowledge available from a different “source” domain. In transfer learning the lack of labelled information from the target domain is compensated by the availability at training time of a set of unlabelled examples from the target distribution. Transductive Transfer Learning denotes the transfer learning setting in which the only set of target documents that we are interested in classifying is known and available at training time. Although this definition is indeed in line with Vapnik’s original definition of “transduction”, current terminology in the field is confused. In this article, we discuss how the term “transduction” has been misused in the transfer learning literature, and propose a clarification consistent with the original characterization of this term given by Vapnik. We go on to observe that the above terminology misuse has brought about misleading experimental comparisons, with inductive transfer learning methods that have been incorrectly compared with transductive transfer learning methods. We then, give empirical evidence that the difference in performance between the inductive version and the transductive version of a transfer learning method can indeed be statistically significant (i.e., that knowing at training time the only data one needs to classify indeed gives an advantage). Our clarification allows a reassessment of the field, and of the relative merits of the major, state-of-the-art algorithms for transfer learning in text classification.",
        "link": "https://dl.acm.org/doi/10.1145/3453146"
    },
    {
        "title": "Machine Learning on Mining Potential Adverse Drug Reactions for Pharmacovigilance",
        "authors": [
            "Ke Jia"
        ],
        "date": "July 2021",
        "source": "DSIT 2021: 2021 4th International Conference on Data Science and Information Technology",
        "abstract": "Drugs are related to the lives and health of patients. Mining and discovering potential adverse drug reactions from a large number of unstructured texts play an important role in drug research and pharmacovigilance. It is a hot research issue in the field of biomedicine in recent years. A large number of scholars have discovered a large amount of information on adverse drug reactions from different data sources through effective machine learning methods. This paper reviews the research progress in recent years to provide a reference for future research.",
        "link": "https://dl.acm.org/doi/10.1145/3478905.3478964"
    },
    {
        "title": "TACAM: Topic And Context Aware Argument Mining",
        "authors": [
            "Michael Fromm",
            "Evgeniy Faerman",
            "Thomas Seidl"
        ],
        "date": "October 2019",
        "source": "WI '19: IEEE/WIC/ACM International Conference on Web Intelligence",
        "abstract": "In this work we address the problem of argument search. The purpose of argument search is the distillation of pro and contra arguments for requested topics from large text corpora. In previous works, the usual approach is to use a standard search engine to extract text parts which are relevant to the given topic and subsequently use an argument recognition algorithm to select arguments from them. The main challenge in the argument recognition task, which is also known as argument mining, is that often sentences containing arguments are structurally similar to purely informative sentences without any stance about the topic. In fact, they only differ semantically. Most approaches use topic or search term information only for the first search step and therefore assume that arguments can be classified independently of a topic. We argue that topic information is crucial for argument mining, since the topic defines the semantic context of an argument. Precisely, we propose different models for the classification of arguments, which take information about a topic of an argument into account. Moreover, to enrich the context of a topic and to let models understand the context of the potential argument better, we integrate information from different external sources such as Knowledge Graphs or pre-trained NLP models. Our evaluation shows that considering topic information, especially in connection with external information, provides a significant performance boost for the argument mining task.",
        "link": "https://dl.acm.org/doi/10.1145/3350546.3352506"
    },
    {
        "title": "Text mining for incoming tasks based on the urgency/importance factors and task classification using machine learning tools",
        "authors": [
            "Yasser Ali Alshehri"
        ],
        "date": "March 2020",
        "source": "ICCDA 2020: Proceedings of the 2020 the 4th International Conference on Compute and Data Analysis",
        "abstract": "In workplaces, there is a massive amount of unstructured data from different sources. In this paper, we present a case study that explains how can through communications between employees, we can help to prioritize tasks requests to increase the efficiency of their works for both technical and non-technical workers. This involves managing daily incoming tasks based on their level of urgency and importance.To allow all workers to utilize the urgency-importance matrix as a time-management tool, we need to automate this tool. The textual content of incoming tasks are analyzed, and metrics related to urgency and importance are extracted. A third factor (i.e., the response variable) is defined based on the two input variables (urgency and importance). Then, machine learning applied to the data to predict the class of incoming tasks based on data outcome desired. We used ordinal regression, neural networks, and decision tree algorithms to predict the four levels of task priority. We measure the performance of all using recalls, precisions, and F-scores. All classifiers perform higher than 89% in terms of all measures.",
        "link": "https://dl.acm.org/doi/10.1145/3388142.3388153"
    },
    {
        "title": "Text mining for malware classification using multivariate all repeated patterns detection",
        "authors": [
            "Konstantinos F. Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2019",
        "source": "ASONAM '19: Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Mobile phones have become nowadays a commodity to the majority of people. Using them, people are able to access the world of Internet and connect with their friends, their colleagues at work or even unknown people with common interests. This proliferation of the mobile devices has also been seen as an opportunity for the cyber criminals to deceive smartphone users and steel their money directly or indirectly, respectively, by accessing their bank accounts through the smartphones or by blackmailing them or selling their private data such as photos, credit card data, etc. to third parties. This is usually achieved by installing malware to smartphones masking their malevolent payload as a legitimate application and advertise it to the users with the hope that mobile users will install it in their devices. Thus, any existing application can easily be modified by integrating a malware and then presented it as a legitimate one. In response to this, scientists have proposed a number of malware detection and classification methods using a variety of techniques. Even though, several of them achieve relatively high precision in malware classification, there is still space for improvement. In this paper, we propose a text mining all repeated pattern detection method which uses the decompiled files of an application in order to classify a suspicious application into one of the known malware families. Based on the experimental results using a real malware dataset, the methodology tries to correctly classify (without any misclassification) all randomly selected malware applications of 3 categories with 3 different families each.",
        "link": "https://dl.acm.org/doi/10.1145/3341161.3350841"
    },
    {
        "title": "Comparative Study between Traditional Machine Learning and Deep Learning Approaches for Text Classification",
        "authors": [
            "Cannannore Nidhi Kamath",
            "Syed Saqib Bukhari",
            "Andreas Dengel"
        ],
        "date": "August 2018",
        "source": "DocEng '18: Proceedings of the ACM Symposium on Document Engineering 2018",
        "abstract": "In this contemporaneous world, it is an obligation for any organization working with documents to end up with the insipid task of classifying truckload of documents, which is the nascent stage of venturing into the realm of information retrieval and data mining. But classification of such humongous documents into multiple classes, calls for a lot of time and labor. Hence a system which could classify these documents with acceptable accuracy would be of an unfathomable help in document engineering. We have created multiple classifiers for document classification and compared their accuracy on raw and processed data. We have garnered data used in a corporate organization as well as publicly available data for comparison. Data is processed by removing the stop-words and stemming is implemented to produce root words. Multiple traditional machine learning techniques like Naive Bayes, Logistic Regression, Support Vector Machine, Random forest Classifier and Multi-Layer Perceptron are used for classification of documents. Classifiers are applied on raw and processed data separately and their accuracy is noted. Along with this, Deep learning technique such as Convolution Neural Network is also used to classify the data and its accuracy is compared with that of traditional machine learning techniques. We are also exploring hierarchical classifiers for classification of classes and subclasses. The system classifies the data faster and with better accuracy than if done manually. The results are discussed in the results and evaluation section.",
        "link": "https://dl.acm.org/doi/10.1145/3209280.3209526"
    },
    {
        "title": "Identification of Overpricing in the Purchase of Medication by the Federal Government of Brazil, Using Text Mining and Clustering Based on Ontology",
        "authors": [
            "Marco Aurelio O. S. Correa",
            "Adriano Galindo Leal"
        ],
        "date": "August 2018",
        "source": "ICCBDC '18: Proceedings of the 2018 2nd International Conference on Cloud and Big Data Computing",
        "abstract": "Increasing the transparency level in his actions and spending is one of the primary duties of the Brazilian Federal Government. The creation of laws that oblige full disclose of all its expenditures through transparency portals enables citizens to supervise all government entities. However, only the dissemination of these data, without a definite standard or the availability of data analysis tools, does not guarantee that the citizen is empowered to play his role. Therefore hence, the objective of this work is to identify overprice in the acquisition of products purchased by the federal government of Brazil using the unstructured data available on the Transparency Portal. The last two-years' worth of purchasing data, available in the Transparency Portal, were extracted, processed and stored. Due to his diverse nature and high volume of data, this study focused only on medicines purchased by the Ministry of Health. Ontology-based text mining and clustering techniques were applied for automatic identification and classification of products. The processing of this information was done through text mining and clustering, based on the ontology registered in another database of the Brazilian government. Because of this work, a consolidated price base per medication was created to allow the identification of distortions in prices practised, facilitating the identification of cases that merit further investigation to unravel fraud to the treasury.",
        "link": "https://dl.acm.org/doi/10.1145/3264560.3264569"
    },
    {
        "title": "Computational Estimation by Scientific Data Mining with Classical Methods to Automate Learning Strategies of Scientists",
        "authors": [
            "Aparna S. Varde"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Experimental results are often plotted as 2-dimensional graphical plots (aka graphs) in scientific domains depicting dependent versus independent variables to aid visual analysis of processes. Repeatedly performing laboratory experiments consumes significant time and resources, motivating the need for computational estimation. The goals are to estimate the graph obtained in an experiment given its input conditions, and to estimate the conditions that would lead to a desired graph. Existing estimation approaches often do not meet accuracy and efficiency needs of targeted applications. We develop a computational estimation approach called AutoDomainMine that integrates clustering and classification over complex scientific data in a framework so as to automate classical learning methods of scientists. Knowledge discovered thereby from a database of existing experiments serves as the basis for estimation. Challenges include preserving domain semantics in clustering, finding matching strategies in classification, striking a good balance between elaboration and conciseness while displaying estimation results based on needs of targeted users, and deriving objective measures to capture subjective user interests. These and other challenges are addressed in this work. The AutoDomainMine approach is used to build a computational estimation system, rigorously evaluated with real data in Materials Science. Our evaluation confirms that AutoDomainMine provides desired accuracy and efficiency in computational estimation. It is extendable to other science and engineering domains as proved by adaptation of its sub-processes within fields such as Bioinformatics and Nanotechnology.",
        "link": "https://dl.acm.org/doi/10.1145/3502736"
    },
    {
        "title": "Where is the road for issue reports classification based on text mining?",
        "authors": [
            "Qiang Fan",
            "Yue Yu",
            "Gang Yin",
            "Tao Wang",
            "Huaimin Wang"
        ],
        "date": "November 2017",
        "source": "ESEM '17: Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Currently, open source projects receive various kinds of issues daily, because of the extreme openness of Issue Tracking System (ITS) in GitHub. ITS is a labor-intensive and time-consuming task of issue categorization for project managers. However, a contributor is only required a short textual abstract to report an issue in GitHub. Thus, most traditional classification approaches based on detailed and structured data (e.g., priority, severity, software version and so on) are difficult to adopt. In this paper, issue classification approaches on a large-scale dataset, including 80 popular projects and over 252,000 issue reports collected from GitHub, were investigated. First, four traditional text-based classification methods and their performances were discussed. Semantic perplexity (i.e., an issues description confuses bug-related sentences with nonbug-related sentences) is a crucial factor that affects the classification performances based on quantitative and qualitative study. Finally, A two-stage classifier framework based on the novel metrics of semantic perplexity of issue reports was designed. Results show that our two-stage classification can significantly improve issue classification performances.",
        "link": "https://dl.acm.org/doi/10.1109/ESEM.2017.19"
    },
    {
        "title": "Feature-based Facebook reviews process model for e-management using data mining",
        "authors": [
            "Anish Kumar Varudharajulu",
            "Yongsheng Ma"
        ],
        "date": "January 2019",
        "source": "IC4E '19: Proceedings of the 10th International Conference on E-Education, E-Business, E-Management and E-Learning",
        "abstract": "The data generated from online communication acts as potential gold mines for discovering knowledge for researchers. A large amount of data is also generated in the form of web documents, emails, blogs, and feedback, etc. Text analytics is being significantly employed to mine important information. Opinion mining is the process of extracting human thoughts and perceptions from unstructured texts. The showstopper for designing an opinion mining system for analyzing reviews arise from the fact that customer reviews are often noisy. These reviews are informally written. In addition, they are subjected to spelling mistakes, grammatical errors, improper punctuation and irrational capitalization. This paper focuses on analyzing the different classification and clustering algorithms aimed at extracting and consolidating opinions of customers from social media sites like Facebook, Twitter and through surveys, at multiple levels of granularity to monitor and measure customer satisfaction. Ours is an automated approach, in which the system aids in the process of knowledge assimilation for knowledge-based building and also performs the analytics. Domain experts ratify the knowledge base and also provide training datasets for the system to intuitively gather more instances for ratification. The system identifies opinion expressions as phrases containing opinion words, opinionated features and also opinion modifiers. These expressions are categorized as positive, negative or neutral. Opinion expressions are identified and categorized using localized linguistic techniques. Opinions can be congregated at any desired level of specificity i.e. feature level or product level, user level or service level, etc. We have developed a system based on this approach, which provides the user with a platform to analyze opinion expressions crawled from a set of pre-defined datasets.",
        "link": "https://dl.acm.org/doi/10.1145/3306500.3306514"
    },
    {
        "title": "Discriminative Topic Mining via Category-Name Guided Text Embedding",
        "authors": [
            "Yu Meng",
            "Jiaxin Huang",
            "Guangyuan Wang",
            "Zihan Wang",
            "Chao Zhang",
            "Yu Zhang",
            "Jiawei Han"
        ],
        "date": "April 2020",
        "source": "WWW '20: Proceedings of The Web Conference 2020",
        "abstract": "Mining a set of meaningful and distinctive topics automatically from massive text corpora has broad applications. Existing topic models, however, typically work in a purely unsupervised way, which often generate topics that do not fit users’ particular needs and yield suboptimal performance on downstream tasks. We propose a new task, discriminative topic mining, which leverages a set of user-provided category names to mine discriminative topics from text corpora. This new task not only helps a user understand clearly and distinctively the topics he/she is most interested in, but also benefits directly keyword-driven classification tasks. We develop CatE, a novel category-name guided text embedding method for discriminative topic mining, which effectively leverages minimal user guidance to learn a discriminative embedding space and discover category representative terms in an iterative manner. We conduct a comprehensive set of experiments to show that CatE mines high-quality set of topics guided by category names only, and benefits a variety of downstream applications including weakly-supervised classification and lexical entailment direction identification.",
        "link": "https://dl.acm.org/doi/10.1145/3366423.3380278"
    },
    {
        "title": "Comparative Study of Heart Disease Diagnosis Using Top Ten Data Mining Classification Algorithms",
        "authors": [
            "I. Ketut Agung Enriko"
        ],
        "date": "June 2019",
        "source": "ICFET '19: Proceedings of the 5th International Conference on Frontiers of Educational Technologies",
        "abstract": "Data mining has been used for many purposes, especially for prediction system. In healthcare, data mining algorithms often used in disease diagnosis. Meanwhile, heart disease is known as a primary cause of death over the years. Many studies have been performed in heart disease diagnosis using data mining methods. There are some popular data mining algorithms that can be used in heart disease diagnosis, for example, k-Nearest Neighbor, CART, and AdaBoost. The algorithms are used to analyze a sample of cardiovascular patients data and predict the heart disease type that they suffer. Some parameters are taken from the patient, including EKG morphology, blood pressure, and information about the existence of chest pain, shortness of breath, palpitation, and cold sweat. In this study, medical records data are collected from Harapan Kita Hospital and utilized as a dataset sample in this research. Top ten data mining classification algorithms are used in diagnosing heart disease from Harapan Kita Hospital data and examining their performance by checking the accuracy and speed.",
        "link": "https://dl.acm.org/doi/10.1145/3338188.3338220"
    },
    {
        "title": "Text Mining Approach for Identifying Research Trends",
        "authors": [
            "Snezhana Sulova"
        ],
        "date": "June 2021",
        "source": "CompSysTech '21: Proceedings of the 22nd International Conference on Computer Systems and Technologies",
        "abstract": "With the increase of unstructured data, the issues connected with automatic text processing, the categorization of documents and the discovery of topics have become objects of growing interest. In order to improve the process of grouping and processing research publications, we would like to propose a method based upon natural language processing. It is based on text mining technologies which aim to identify key tendencies in documents. It processes the content of publications by clustering and identifies the topics of each identified group. This analysis helps by identifying key tendencies as well as discovering emerging new areas of research. Publications from the research literature database, Scopus, were used to test the approach. The topic of the publications is “the application of digital technologies in the logistics business”. The experiments were completed using the RapidMiner Studio software.",
        "link": "https://dl.acm.org/doi/10.1145/3472410.3472433"
    },
    {
        "title": "Assessment of Vulnerability Severity using Text Mining",
        "authors": [
            "Georgios Spanos",
            "Lefteris Angelis",
            "Dimitrios Toloudis"
        ],
        "date": "September 2017",
        "source": "PCI '17: Proceedings of the 21st Pan-Hellenic Conference on Informatics",
        "abstract": "Software1 vulnerabilities are closely associated with information systems security, a major and critical field in today's technology. Vulnerabilities constitute a constant and increasing threat for various aspects of everyday life, especially for safety and economy, since the social impact from the problems that they cause is complicated and often unpredictable. Although there is an entire research branch in software engineering that deals with the identification and elimination of vulnerabilities, the growing complexity of software products and the variability of software production procedures are factors contributing to the ongoing occurrence of vulnerabilities, Hence, another area that is being developed in parallel focuses on the study and management of the vulnerabilities that have already been reported and registered in databases. The information contained in such databases includes, a textual description and a number of metrics related to vulnerabilities. The purpose of this paper is to investigate to what extend the assessment of the vulnerability severity can be inferred directly from the corresponding textual description, or in other words, to examine the informative power of the description with respect to the vulnerability severity. For this purpose, text mining techniques, i.e. text analysis and three different classification methods (decision trees, neural networks and support vector machines) were employed. The application of text mining to a sample of 70,678 vulnerabilities from a public data source shows that the description itself is a reliable and highly accurate source of information for vulnerability prioritization.",
        "link": "https://dl.acm.org/doi/10.1145/3139367.3139390"
    },
    {
        "title": "Clustering Algorithms for Spatial Data Mining",
        "authors": [
            "Chetashri Bhadane",
            "Ketan Shah"
        ],
        "date": "April 2020",
        "source": "ICGDA '20: Proceedings of the 2020 3rd International Conference on Geoinformatics and Data Analysis",
        "abstract": "With the advances in mobile and wireless technologies, there has been a rise in applications that track and share the users' geospatial data. People use several social networking sites such as Twitter, Facebook and Flickr, where they share their status updates. With the integration of Global Positioning System (GPS) with mobile phones, it is now possible to share one's locations on these social networks. GPS allows us to record and track a person's movement along with the timestamp. The data set obtained from these GPS logs is vast and is widely used to analyze the users' movement patterns. Specifically, we can find out significant locations based on the number of users present at that location and the time spent by them at such places. Once significant places have been identified, it is also possible to identify the semantic importance of these locations. This paper presents an overview of the clustering techniques used to find important places of interest using large GPS based mobility datasets. Four clustering algorithms, K-Means, DBSCAN, OPTICS and Hierarchical, are implemented, and performance is tested using real-time data of 50 users collected over 2--5 years. Performance summary depicts that K-Means and DBSCAN perform well for spatial data.",
        "link": "https://dl.acm.org/doi/10.1145/3397056.3397068"
    },
    {
        "title": "Feature Selection based Arabic Text Classification using Different Machine Learning Algorithms: Comparative Study",
        "authors": [
            "Sakina Rim Bennabi",
            "Zakaria Elberrichi"
        ],
        "date": "June 2020",
        "source": "ICIST '20: Proceedings of the 10th International Conference on Information Systems and Technologies",
        "abstract": "Feature selection is a method of data pre-processing widely used when mining large data, such as textual classification. Several studies have been conducted to compare the different methods of feature selection applied to corpora in English. Unfortunately, a small number of works concern the Arabic language. This article aims to present a comparative study of different feature selection techniques including: Chi2, the ANOVA method and mutual information, applied on a corpus in Arabic language, while also diversifying the machine learning algorithms (Naive Bayes, SVM and KNN). This experimental study has shown in general that reducing dimensionality with feature selection techniques has slightly affected the performance of textual classification, reducing the size of the corpus by up to 1%.",
        "link": "https://dl.acm.org/doi/10.1145/3447568.3448531"
    },
    {
        "title": "Intelligent Data Analysis using Optimized Support Vector Machine Based Data Mining Approach for Tourism Industry",
        "authors": [
            "Ms Promila Sharma",
            "Uma Meena",
            "Girish Kumar Sharma"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Data analysis involves the deployment of sophisticated approaches from data mining methods, information theory, and artificial intelligence in various fields like tourism, hospitality, and so on for the extraction of knowledge from the gathered and preprocessed data. In tourism, pattern analysis or data analysis using classification is significant for finding the patterns that represent new and potentially useful information or knowledge about the destination and other data. Several data mining techniques are introduced for the classification of data or patterns. However, overfitting, less accuracy, local minima, sensitive to noise are the drawbacks in some existing data mining classification methods. To overcome these challenges, Support vector machine with Red deer optimization (SVM-RDO) based data mining strategy is proposed in this article. Extended Kalman filter (EKF) is utilized in the first phase, i.e., data cleaning to remove the noise and missing values from the input data. Mantaray foraging algorithm (MaFA) is used in the data selection phase, in which the significant data are selected for the further process to reduce the computational complexity. The final phase is the classification, in which SVM-RDO is proposed to access the useful pattern from the selected data. PYTHON is the implementation tool used for the experiment of the proposed model. The experimental analysis is done to show the efficacy of the proposed work. From the experimental results, the proposed SVM-RDO achieved better accuracy, precision, recall, and F1 score than the existing methods for the tourism dataset. Thus, it is showed the effectiveness of the proposed SVM-RDO for pattern analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3494566"
    },
    {
        "title": "Applying text mining to predict learners' cognitive engagement",
        "authors": [
            "Hayati Hind",
            "Mohammed Khalidi Idrissi",
            "Samir Bennani"
        ],
        "date": "October 2017",
        "source": "SCAMS '17: Proceedings of the Mediterranean Symposium on Smart City Application",
        "abstract": "In academic process, engagement represent one of the important success' factor. This paper proposes a predictive system for learners' cognitive engagement based on their online discussion forums participation. In the first level the ontology OWL and the LSA method are used to perform a semantic classification system of the threads according to a specific context chosen by the tutor. Then the Text mining, as a predictive method, is applied to the classified threads and learners' participation in the forums.",
        "link": "https://dl.acm.org/doi/10.1145/3175628.3175655"
    },
    {
        "title": "Investigate Transitions into Drug Addiction through Text Mining of Reddit Data",
        "authors": [
            "John Lu",
            "Sumati Sridhar",
            "Ritika Pandey",
            "Mohammad Al Hasan",
            "Georege Mohler"
        ],
        "date": "July 2019",
        "source": "KDD '19: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Increasing rates of opioid drug abuse and heightened prevalence of online support communities underscore the necessity of employing data mining techniques to better understand drug addiction using these rapidly developing online resources. In this work, we obtained data from Reddit, an online collection of forums, to gather insight into drug use/misuse using text snippets from users narratives. Specifically, using users' posts, we trained a binary classifier which predicts a user's transitions from casual drug discussion forums to drug recovery forums. We also proposed a Cox regression model that outputs likelihoods of such transitions. In doing so, we found that utterances of select drugs and certain linguistic features contained in one's posts can help predict these transitions. Using unfiltered drug-related posts, our research delineates drugs that are associated with higher rates of transitions from recreational drug discussion to support/recovery discussion, offers insight into modern drug culture, and provides tools with potential applications in combating the opioid crisis.",
        "link": "https://dl.acm.org/doi/10.1145/3292500.3330737"
    },
    {
        "title": "Analysing digital banking reviews using text mining",
        "authors": [
            "Li Chen Cheng",
            "Legaspi Rhea Sharmayne"
        ],
        "date": "December 2020",
        "source": "ASONAM '20: Proceedings of the 12th IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Digital banks are new entrants in the banking industry in the Philippines as they only started late 2018. Since then, a handful of players have and are still emerging. With more and more people becoming technologically savvy, it is very critical for financial institutions to develop a digital banking application that will stand out from the competition. This paper aims to use text mining methods to analyse digital banking application reviews. This study will perform topic modelling using LDA to explore customer concerns and will mine association rules between the digital banking features with the review score. The results will reveal which areas the digital banking application can further optimize for customer satisfaction and retention.",
        "link": "https://dl.acm.org/doi/10.1109/ASONAM49781.2020.9381429"
    },
    {
        "title": "Mining Moocs Videos Metadata Using Classification Techniques",
        "authors": [
            "El Harrak Othman",
            "Ghadi Abderrahim",
            "El Bouhdidi Jaber"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "In few years, the internet has become the first source of information for most people, Today MOOCs makes it possible for everyone to access to the education over the world, it's represents an emerging methodology of online teaching and an important development in open education. Due to the rapid development of Moocs and the rapid growth of digital data and video database over the Internet, it is becoming very difficult for learners to browsing and choosing the best training for them. The scientific community has increased the amount of research into new technologies, with a view to improve the digital video utilization: its archiving, indexing, accessibility, acquisition, store and even its process and usability. All these parts of the video utilization entail the necessity of the extraction of all important information of a video, especially in cases of lack of metadata information. Web video mining is retrieving the content using data mining techniques from World Wide Web. There are two approaches for web video mining using traditional image processing (signal processing) and metadata based approach. In this paper, we present the various research makes in this subject and we propose an effective methodology to extract the metadata from Moocs videos and classify them based on the extracted metadata by applying data mining techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090450"
    },
    {
        "title": "Text mining with HathiTrust",
        "authors": [
            "Eleanor Dickson Koehl",
            "Ryan Dubnicek"
        ],
        "date": "June 2019",
        "source": "JCDL '19: Proceedings of the 18th Joint Conference on Digital Libraries",
        "abstract": "This tutorial will introduce attendees to the HathiTrust Research Center and its tools and services for computational text analysis research. HTRC leverages the scope and scale of the HathiTrust Digital Library collection to create opportunities for researchers to perform text data mining on subsets of the corpus. Attendees will develop skills that will allow them to conduct basic text analysis research using HathiTrust data, learn techniques for engaging with scholars on their text data mining research, and explore strategies for computational access to large-scale digital collections.",
        "link": "https://dl.acm.org/doi/10.1109/JCDL.2019.00115"
    },
    {
        "title": "Text mining for plagiarism detection: multivariate pattern detection for recognition of text similarities",
        "authors": [
            "Konstantinos Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2018",
        "source": "ASONAM '18: Proceedings of the 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "The problem of plagiarism the recent years has been intensified by the availability of information in digital form and the accessibility of the electronic libraries through the Internet. As a result, plagiarism detection has been transformed into a big data analytics problem since the number of digital sources is extravagant and a new document needs to be compared with millions of other existing documents. In this paper, a text mining methodology is proposed that can detect all common patterns between a document and the documents in a reference database. The technique is based on a pattern detection algorithm and the corresponding data structure that enables the algorithm to detect all common patterns. The methodology has been applied in a well-defined dataset providing very promising results identifying difficult cases of plagiarism such as technical disguise.",
        "link": "https://dl.acm.org/doi/10.5555/3382225.3382424"
    },
    {
        "title": "Petroleum Engineering Data Text Classification Using Convolutional Neural Network Based Classifier",
        "authors": [
            "Tzuhan Hsu",
            "Yaoqin Zhang"
        ],
        "date": "May 2018",
        "source": "ICMLT '18: Proceedings of the 2018 International Conference on Machine Learning Technologies",
        "abstract": "In this paper, authors introduce a convolutional neural network (CNN) based classification approach for petroleum engineering event. Recently, text classification problem has raised substantial attention in the field of machine learning. The aim of text classification process is to assign correct label to a text on the basis of its content. However, due to the imprecise of feature representation, most of former text classification methods cannot export satisfying result. To solve this problem, authors propose a model based on convolutional neural network. Authors conduct experiments with real world petroleum engineering data text, artificially labelled into 6 categories. Beside purposed CNN-based model, the authors also implement other baseline machine learning approaches: k nearest neighbor, random forest, support vector machines, recurrent neural network, gated recurrent units, long short-term memory. Among all these models, our CNN-based model reach best performance in text classification task. In the last part of this paper, authors compare and analysis their purpose and baseline model.",
        "link": "https://dl.acm.org/doi/10.1145/3231884.3231898"
    },
    {
        "title": "Classifying Indonesian Online Articles as Advertisement Placement Base Using Text Mining",
        "authors": [
            "Nadhira Tasya",
            "Arian Dhini"
        ],
        "date": "July 2017",
        "source": "ICBIM 2017: Proceedings of the International Conference on Business and Information Management",
        "abstract": "Rapid development in technological aspect resulting in growing level of human needs for the latest news, so that emerged a new trend of publishing and accessing news through online media or usually called online journalism. In addition, the number of people who sell and purchase through online sites also continues to increase and this opportunity is utilized by the company and the advertiser by implementing targeted web advertising. However, the high number of articles that have been published and accessed leads to great opportunities for errors in determining where to place the ads. Therefore, it needs a system that can categorize articles accessed by users as the basis of advertisement placement by the company and this classification system can be done by applying the method of Data Mining and Text Mining. This research uses document data in the form of article content that will be categorized into twenty categories of class of advertisement by using Text Mining technique with Support Vector Machine algorithm. The results of this study may be used by companies or advertisers as a basis for placement of ads on selected online media sites.",
        "link": "https://dl.acm.org/doi/10.1145/3134271.3134288"
    },
    {
        "title": "Mining Free-Text Medical Notes for Suicide Risk Assessment",
        "authors": [
            "Marios Adamou",
            "Grigoris Antoniou",
            "Elissavet Greasidou",
            "Vincenzo Lagani",
            "Paulos Charonyktakis",
            "Ioannis Tsamardinos"
        ],
        "date": "July 2018",
        "source": "SETN '18: Proceedings of the 10th Hellenic Conference on Artificial Intelligence",
        "abstract": "Suicide has been considered as an important public health issue for a very long time, and is one of the main causes of death worldwide. Despite suicide prevention strategies being applied, the rate of suicide has not changed substantially over the past decades. Advances in machine learning make it possible to attempt to predict suicide based on the analysis of relevant data to inform clinical practice. This paper reports on findings from the analysis of data of patients who died by suicide in the period 2013-2016 and made use of both structured data and free-text medical notes. We focus on examining various text-mining approaches to support risk assessment. The results show that using advance machine learning and text-mining techniques, it is possible to predict within a specified period which people are most at risk of taking their own life at the time of referral to a mental health service.",
        "link": "https://dl.acm.org/doi/10.1145/3200947.3201020"
    },
    {
        "title": "Integration of data mining techniques in e-learning systems: Clustering Profil of Lerners and Recommender Course System",
        "authors": [
            "A. El Moustamid",
            "E. En-Naimi",
            "J. El Bouhdidi"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "Data mining aims at the extraction of knowledge from large quantities of data by automatic or semi-automatic methods. In the field of education and training e-learning, Data Mining techniques as clustering (The aim of this technique is to identify groups that are similar in some aspect) [21], prediction (is used for developing model where combination of data viewed from different aspects can effect single viewed data. Here, is important to watch how predictors variables are effecting predicted variable)[21], classification are applied in e-learning to improve the level of learners by providing learning according to the learners' requirement) are used to analyze learners profile, to predict and improve student performance. The aim of our project is to develop a system capable of analyzing learners profile and indexing web videos in order to offer learners a rich database with courses that correspond to their levels.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090453"
    },
    {
        "title": "Multi-level mining and visualization of scientific text collections: Exploring a bi-lingual scientific repository",
        "authors": [
            "Pablo Accuosto",
            "Francesco Ronzano",
            "Daniel Ferrés",
            "Horacio Saggion"
        ],
        "date": "December 2017",
        "source": "WOSP 2017: Proceedings of the 6th International Workshop on Mining Scientific Publications",
        "abstract": "We present a system to mine and visualize collections of scientific documents by semantically browsing information extracted from single publications or aggregated throughout corpora of articles. The text mining tool performs deep analysis of document collections allowing the extraction and interpretation of research paper's contents. In addition to the extraction and enrichment of documents with metadata (titles, authors, affiliations, etc), the deep analysis performed comprises semantic interpretation, rhetorical analysis of sentences, triple-based information extraction, and text summarization. The visualization components allow geographical-based exploration of collections, topic-evolution interpretation, and collaborative network analysis among others. The paper presents a case study of a bi-lingual collection in the field of Natural Language Processing (NLP).",
        "link": "https://dl.acm.org/doi/10.1145/3127526.3127529"
    },
    {
        "title": "Hierarchical Topic Mining via Joint Spherical Tree and Text Embedding",
        "authors": [
            "Yu Meng",
            "Yunyi Zhang",
            "Jiaxin Huang",
            "Yu Zhang",
            "Chao Zhang",
            "Jiawei Han"
        ],
        "date": "August 2020",
        "source": "KDD '20: Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Mining a set of meaningful topics organized into a hierarchy is intuitively appealing since topic correlations are ubiquitous in massive text corpora. To account for potential hierarchical topic structures, hierarchical topic models generalize flat topic models by incorporating latent topic hierarchies into their generative modeling process. However, due to their purely unsupervised nature, the learned topic hierarchy often deviates from users' particular needs or interests. To guide the hierarchical topic discovery process with minimal user supervision, we propose a new task, Hierarchical Topic Mining, which takes a category tree described by category names only, and aims to mine a set of representative terms for each category from a text corpus to help a user comprehend his/her interested topics. We develop a novel joint tree and text embedding method along with a principled optimization procedure that allows simultaneous modeling of the category tree structure and the corpus generative process in the spherical space for effective category-representative term discovery. Our comprehensive experiments show that our model, named JoSH, mines a high-quality set of hierarchical topics with high efficiency and benefits weakly-supervised hierarchical text classification tasks.",
        "link": "https://dl.acm.org/doi/10.1145/3394486.3403242"
    },
    {
        "title": "Unsupervised behavioural mining and clustering for malware family identification",
        "authors": [
            "Khanh Huu The Dam",
            "Thomas Given-Wilson",
            "Axel Legay"
        ],
        "date": "March 2021",
        "source": "SAC '21: Proceedings of the 36th Annual ACM Symposium on Applied Computing",
        "abstract": "More accurate and advanced detection and classification of malware requires exploiting program behaviour and not merely syntactic or static features. One approach is to use system call dependency graphs (SCDGs) that represent the program behaviour by interactions with the system, and the relations between these interactions. These SCDGs have been used with supervised learning techniques to very accurately detect and classify malware. This works considers the unsupervised learning challenge of mining for common clusters of behaviour without a priori knowledge. This allows for clustering of similar programs by behaviour, that can then be used for either classification or further analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3412841.3441919"
    },
    {
        "title": "Text Mining for Evaluating Authors' Birth and Death Years",
        "authors": [
            "Dror Moghaz",
            "Yaakov Hacohen-Kerner",
            "Dov Gabbay"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "This article presents a unique method in text and data mining for finding the era, i.e., mining temporal data, in which an anonymous author was living. Finding this era can assist in the examination of a fake document or extracting the time period in which a writer lived. The study and the experiments concern Hebrew, and in some parts, Aramaic and Yiddish rabbinic texts. The rabbinic texts are undated and contain no bibliographic sections, posing an interesting challenge. This work proposes algorithms using key phrases and key words that allow the temporal organization of citations together with linguistic patterns. Based on these key phrases, key words, and the references, we established several types of “Iron-clad,” Heuristic and Greedy rules for estimating the years of birth and death of a writer in an interesting classification task. Experiments were conducted on corpora, including documents authored by 12, 24, and 36 rabbinic writers and demonstrated promising results.",
        "link": "https://dl.acm.org/doi/10.1145/3281631"
    },
    {
        "title": "Transfer Learning to Timed Text Based Video Classification Using CNN",
        "authors": [
            "Zenun Kastrati",
            "Ali Shariq Imran",
            "Arianit Kurti"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "Open educational video resources are gaining popularity with a growing number of massive open online courses (MOOCs). This has created a niche for content providers to adopt effective solutions in automatically organizing and structuring of educational resources for maximum visibility. Recent advances in deep learning techniques are proving useful in managing and classifying resources into appropriate categories. This paper proposes one such convolutional neural network (CNN) model for classifying video lectures in a MOOC setting using a transfer learning approach. The model uses a time-aligned text transcripts corresponding to video lectures from six broader subject categories. Video lectures and their corresponding transcript dataset is gathered from the Coursera MOOC platform. Two different CNN models are proposed: i) CNN based classification using embeddings learned from our MOOC dataset, ii) CNN based classification using transfer learning. Word embeddings generated from two well known state-of-the-art pre-trained models Word2Vec and GloVe, are used in the transfer learning approach for the second case. The proposed CNN models are evaluated using precision, recall, and F1 score and the obtained performance is compared with both conventional and deep learning classifiers. The proposed CNN models have an F1 score improvement of 10-22 percentage points over DNN and conventional classifiers",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326483"
    },
    {
        "title": "Evolutionary role mining in complex networks by ensemble clustering",
        "authors": [
            "Sarvenaz Choobdar",
            "Pedro Ribeiro",
            "Fernando Silva"
        ],
        "date": "April 2017",
        "source": "SAC '17: Proceedings of the Symposium on Applied Computing",
        "abstract": "The structural patterns in the neighborhood of nodes assign unique roles to the nodes. Mining the set of existing roles in a network provides a descriptive profile of the network and draws its general picture. This paper proposes a new method to determine structural roles in a dynamic network based on the current position of nodes and their historic behavior. We develop a temporal ensemble clustering technique to dynamically find groups of nodes, holding similar tempo-structural roles. We compare two weighting functions, based on age and distribution of data, to incorporate temporal behavior of nodes in the role discovery. To evaluate the performance of the proposed method, we assess the results from two points of view: 1) goodness of fit to current structure of the network; 2) consistency with historic data. We conduct the evaluation using different ensemble clustering techniques. The results on real world networks demonstrate that our method can detect tempo-structural roles that simultaneously depict the topology of a network and reflect its dynamics with high accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3019612.3019815"
    },
    {
        "title": "Analyzing used-car web listings via text mining",
        "authors": [
            "Ayhan Demiriz",
            "Fatma Cantaş"
        ],
        "date": "October 2017",
        "source": "IML '17: Proceedings of the 1st International Conference on Internet of Things and Machine Learning",
        "abstract": "Used car trade is one of the major components of the world economies. It is not uncommon to sell a car by placing an internet advertisement irrespective of the geography in these days. A typical content of an advertisement is usually composed of two parts namely the structured and the free text data. The structured data may include some information about the asking price, make, model, year, mileage of the car and the contact info. In most cases, seller may give important clues about the car's current conditions in the free text data where the title (head) of the advertisement can be included as free text too. This paper reports preliminary results from a text mining study conducted on 75K used car internet listings collected from two major car listing web sites in Turkey. As expected, the words and the phrases related to the description of the car are observed to be frequent. The leading concepts in the free text are found to be regarding how to describe the current condition of a car, for example \"no crash history\".",
        "link": "https://dl.acm.org/doi/10.1145/3109761.3109782"
    },
    {
        "title": "Text classification of Diseases Treated by Traditional Chinese Medicine Prescription based on machine learning",
        "authors": [
            "Hanqing Zhao",
            "Bo Han",
            "Chen Li"
        ],
        "date": "September 2020",
        "source": "ISAIMS '20: Proceedings of the 1st International Symposium on Artificial Intelligence in Medical Sciences",
        "abstract": "OBJECTIVE: To explore the application of machine learning in th e identification of diseases treated by traditional Chinese medicine prescriptions. METHODS: Based on the composition of the text document of Chinese medicine prescriptions, the prescriptions were divided into cough, headache and diarrhea. THUTCT were introduced to establish and train two machine learning text classification models, LibSVM and LibLinear, and the prescriptions to be deter mined were put into the model for classification and prediction. R ESULTS: The Precision rate of LibSVM model and Liblinear mod el were 0.7283 and 0.6690. Seven prescriptions were classified an d predicted, and the results were in line with expectations. CONCLUSION: THUCTC has good universality for the content of TCM prescriptions, high classification accuracy and fast testing speed, which is suitable for the text classification and discrimination research of TCM prescriptions for diseases.",
        "link": "https://dl.acm.org/doi/10.1145/3429889.3429895"
    },
    {
        "title": "Evaluation of retweet clustering method classification method using retweets on Twitter without text data",
        "authors": [
            "K. Uchida",
            "F. Toriumi",
            "T. Sakaki"
        ],
        "date": "August 2017",
        "source": "WI '17: Proceedings of the International Conference on Web Intelligence",
        "abstract": "Burst phenomena, which frequently occur on social media, are caused by such social events as flaming on the internet, elections, and natural disasters. To understand people's thoughts and feelings, we must classify their opinions from burst phenomena. Therefore, classification methods that categorize tweets are critical. However, since most classification methods focus on text mining, they cannot group tweets by topics because each tweet has poor linguistic similarities. We used a non-text-based classification method proposed by Baba et al. that groups tweets by topics, even if they have poor linguistic similarities, and verified its validity by comparing it with a text-based classification method in two different evaluations: qualitative and quantitative. In the qualitative evaluation part, we did a questionnaire survey and validated the suitability of the topic clusters created using both the non-and text-based methods. Since evaluating the similarity of every pair of tweets in each topic is difficult, we evaluated the similarity between sampled pairs in the survey and acquired more appropriate topic clustering results using the non-text-based method than the text-based method. In the quantitative evaluation part, we focused on the robustness of each method against data reduction. Many approaches analyze social media data, especially because collecting data from social media is comparatively easy. However, since collecting the whole data of burst phenomena is very costly due to the vast amounts of available social media data, robustness against data reduction is an important index to evaluate classification methods. With the non-text-based method, over 55% of the pairs of tweets in the same cluster were also included in the same cluster even when the data were reduced to 10% in all three of our example cases. In this paper, as a source we focus on Twitter, one of the most popular microblogging services. Using clustering to conduct detailed case analyses, we scrutinized three burst cases that include natural disasters and flaming on the internet and found that a non-text-based method more effectively classified tweets in burst phenomena than a text-based method.",
        "link": "https://dl.acm.org/doi/10.1145/3106426.3106451"
    },
    {
        "title": "Using Spark for Text Mining on Large Scale Liver Cancer Literature",
        "authors": [
            "Ming-Yen Lin",
            "Yu-Ju Lin",
            "Sue-Chen Hsueh"
        ],
        "date": "January 2021",
        "source": "BDET 2021: 2021 the 3rd International Conference on Big Data Engineering and Technology (BDET)",
        "abstract": "Cancer is one of the main causes of death. The number of known cancer to-date is more than one hundred. Liver cancer, ranked after trachea and lung cancer, has long been high in the cancer leading cause in Taiwan and even ranked as second in 2016. The number of scientific articles related to cancer proliferates every year. The number reaches as high as 20 million in PubMed so that discovering useful information from the massive collection is very difficult. In addition, using a single machine to sift through these articles is very time-consuming. Therefore, we present a big data analytic framework using the distributed Apache Spark platform for text mining in PubMed literature. We establish a prediction model for liver cancer articles so that researchers may effectively validate whether an article is related to liver cancer or not. Classification models in Spark MLlib including Linear Support Vector Machines (SVM), Logistic Regression, are used in our experiments. Relevancy to liver cancer is further confirmed by using MeSH (Medical Subject Headings) terms. Logistic regression is about 3 times faster than SVMs and the accuracy of both methods is close to 95% in the experiments using hold-out validation. When max_features is 500 and min_df ≤ 0.1 (or min_df = 1), the accuracy may reach 96%. In the experiments with K-fold cross-validation, the accuracy of SVMs methods is 96%. The experimental results show that that our prediction model may effectively classify liver cancer articles.",
        "link": "https://dl.acm.org/doi/10.1145/3474944.3474958"
    },
    {
        "title": "Automatic topic classification of test cases using text mining at an Android smartphone vendor",
        "authors": [
            "Junji Shimagaki",
            "Yasutaka Kamei",
            "Naoyasu Ubayashi",
            "Abram Hindle"
        ],
        "date": "October 2018",
        "source": "ESEM '18: Proceedings of the 12th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Background: An Android smartphone is an ecosystem of applications, drivers, operating system components, and assets. The volume of the software is large and the number of test cases needed to cover the functionality of an Android system is substantial. Enormous effort has been already taken to properly quantify \"what features and apps were tested and verified?\". This insight is provided by dashboards that summarize test coverage and results per feature. One method to achieve this is to manually tag or label test cases with the topic or function they cover, much like function points. At the studied Android smartphone vendor, tests are labelled with manually defined tags, so-called \"feature labels (FLs)\", and the FLs serve to categorize 100s to 1000s test cases into 10 to 50 groups. Aim: Unfortunately for developers, manual assignment of FLs to 1000s of test cases is a time consuming task, leading to inaccurately labeled test cases, which will render the dashboard useless. We created an automated system that suggests tags/labels to the developers for their test cases rather than manual labeling. Method: We use machine learning models to predict and label the functionality tested by 10,000 test cases developed at the company. Results: Through the quantitative experiments, our models achieved acceptable F-1 performance of 0.3 to 0.88. Also through the qualitative studies with expert teams, we showed that the hierarchy and path of tests was a good predictor of a feature's label. Conclusions: We find that this method can reduce tedious manual effort that software developers spent classifying test cases, while providing more accurate classification results.",
        "link": "https://dl.acm.org/doi/10.1145/3239235.3268927"
    },
    {
        "title": "To apply Data Mining for Classification of Crowd sourced Software Requirements",
        "authors": [
            "Soonh Taj",
            "Qasim Arain",
            "Imran Memon",
            "Asma Zubedi"
        ],
        "date": "April 2019",
        "source": "ICSIE '19: Proceedings of the 8th International Conference on Software and Information Engineering",
        "abstract": "Now a day's main focus of developers is to build quality software that works according to customer needs and for this reason it is necessary to gather right requirements as requirement elicitation is the critical step that impacts on the success of software project as misinterpreted requirements leads to the failure of software project. By keeping this in mind a research is carried out on improving requirements elicitation process and automating the process of classifying requirements. In this research, a model is proposed which will help in this scenario for requirements elicitation and requirement classification. This paper presents a model in which crowd sourcing approach is used so that customers, end users, stakeholders, developers and software engineers can make active participation for requirement elicitation process and requirements gathered using crowdsourcing approach are used by model for classification process i.e. classification of requirements into functional and non-functional requirements. For the proof of proposed model a case study is conducted. Results of case study provided the usefulness and efficiency of proposed model for classification of crowd sourced software requirements.",
        "link": "https://dl.acm.org/doi/10.1145/3328833.3328837"
    },
    {
        "title": "Text classification using Fuzzy TF-IDF and Machine Learning Models",
        "authors": [
            "Mariem Bounabi",
            "Karim El Moutaouakil",
            "Khalid Satori"
        ],
        "date": "October 2019",
        "source": "BDIoT'19: Proceedings of the 4th International Conference on Big Data and Internet of Things",
        "abstract": "The representation of the information has an important impact on the text classification task. Several weighting methods were proposed in the literature, and the term frequency-inverse term frequency (TFIDF), the most know on the text treatment field. The FTF-IDF is a vector representation where the components of the TFIDF are presented as inputs to the Fuzzy Inference System (FIS). In this work, we compare several Machin Learning algorithms such as Naïve Bayes and its derivatives, SVM and Random forest classifiers, using the FTF-IDF representation. To improve the quality of the used classifiers, we call sum attribute selection methods. The recognition rate, for the tested systems, is satisfied, where the system based on naïve Bayes classifier, the FTF-IDF weighting terms, and the info gain select attributes method gives 98.7% as accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3372938.3372956"
    },
    {
        "title": "Text Mining in Cybersecurity: A Systematic Literature Review",
        "authors": [
            "Luciano Ignaczak",
            "Guilherme Goldschmidt",
            "Cristiano André Da Costa",
            "Rodrigo Da Rosa Righi"
        ],
        "date": null,
        "source": "ACM Computing Surveys",
        "abstract": "The growth of data volume has changed cybersecurity activities, demanding a higher level of automation. In this new cybersecurity landscape, text mining emerged as an alternative to improve the efficiency of the activities involving unstructured data. This article proposes a Systematic Literature Review (SLR) to present the application of text mining in the cybersecurity domain. Using a systematic protocol, we identified 2,196 studies, out of which 83 were summarized. As a contribution, we propose a taxonomy to demonstrate the different activities in the cybersecurity domain supported by text mining. We also detail the strategies evaluated in the application of text mining tasks and the use of neural networks to support activities involving unstructured data. The work also discusses text classification performance aiming its application in real-world solutions. The SLR also highlights open gaps for future research, such as the analysis of non-English content and the intensification in the usage of neural networks.",
        "link": "https://dl.acm.org/doi/10.1145/3462477"
    },
    {
        "title": "Using Text Mining to Analyze the Financial Patents",
        "authors": [
            "Yung-Feng Lu",
            "Jia-Lang Xu",
            "Mu-Yen Chen"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "With the advancement and development of science and technology, Nowadays society is the development of information technology, cloud computing, mobile Internet, Internet of Things, block chain and information technology.The combination of finance and technology has made financial technology terms gradually emerging.. This research analyzes the patents of financial technology. However, Taiwan's financial technology patents are mostly G06Q-20 (payment plan, architecture or agreement), G06Q-30 (commercial, such as marketing, shopping, payment, auction or e-commerce). G06Q-40 (financial, investment or tax treatment, insurance).The research use text mining technology to analyze and understand Taiwan's trends and developments in financial technology. The research results show that G06Q-20 patents are biased towards information security and mobile payment. G06Q-30 patents favor shopping websites and advertising. G06Q-40 patents are biased towards diversity, such as securities, insurance, finance, and stocks.",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326478"
    },
    {
        "title": "Identifying Online Profiles of Distance Learning Students Using Data Mining Techniques",
        "authors": [
            "Osama Islam",
            "Muazzam Siddiqui",
            "Naif Radi Aljohani"
        ],
        "date": "October 2019",
        "source": "ICDTE '19: Proceedings of the 3rd International Conference on Digital Technology in Education",
        "abstract": "Educational data has grown over the years with the increased use of technology within educational environments. This has led to a huge amount of data being stored in various data sources representing the student, his/her activities, and other aspects relevant to the learning process. To meet this analytical need, Educational Data Mining (EDM) has emerged to assist educational institutions in identifying key benefits such as students at risk, the level of student engagement or predicting student performance. The aim of this research was to explore the various aspects of student interaction data using data mining techniques to identify relevant patterns of behaviors and possible key attributes that have higher degrees of influence on distance learning students. The main findings identified several patterns of user online profiles based on a set of adopted learning strategies, the research proposed a framework for analyzing such interaction data based on R and Hadoop platforms to correlate online profiles with student performance.",
        "link": "https://dl.acm.org/doi/10.1145/3369199.3369249"
    },
    {
        "title": "Extremely Fast Decision Tree Mining for Evolving Data Streams",
        "authors": [
            "Albert Bifet",
            "Jiajin Zhang",
            "Wei Fan",
            "Cheng He",
            "Jianfeng Zhang",
            "Jianfeng Qian",
            "Geoff Holmes",
            "Bernhard Pfahringer"
        ],
        "date": "August 2017",
        "source": "KDD '17: Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
        "abstract": "Nowadays real-time industrial applications are generating a huge amount of data continuously every day. To process these large data streams, we need fast and efficient methodologies and systems. A useful feature desired for data scientists and analysts is to have easy to visualize and understand machine learning models. Decision trees are preferred in many real-time applications for this reason, and also, because combined in an ensemble, they are one of the most powerful methods in machine learning. In this paper, we present a new system called STREAMDM-C++, that implements decision trees for data streams in C++, and that has been used extensively at Huawei. Streaming decision trees adapt to changes on streams, a huge advantage since standard decision trees are built using a snapshot of data, and can not evolve over time. STREAMDM-C++ is easy to extend, and contains more powerful ensemble methods, and a more efficient and easy to use adaptive decision trees. We compare our new implementation with VFML, the current state of the art implementation in C, and show how our new system outperforms VFML in speed using less resources.",
        "link": "https://dl.acm.org/doi/10.1145/3097983.3098139"
    },
    {
        "title": "Data miners' little helper: data transformation activity cues for cluster analysis on document collections",
        "authors": [
            "Tania Cerquitelli",
            "Evelina Di Corso",
            "Francesco Ventura",
            "Silvia Chiusano"
        ],
        "date": "June 2017",
        "source": "WIMS '17: Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "In this paper we propose a new self-learning engine to streamline the analytics process, as it enables analysts to mine massive data repositories with minimal user intervention. In the context of cluster analysis on a collection of documents this new system, named SELF-DATA (SELF-learning DAta TrAnsformation), suggests to the analyst how to configure the whole mining process for a given dataset. SELF-DATA relies on an engine exploring different data weighting schemas (e.g., normalized term frequencies) and data transformation methods (e.g., PCA) before applying the cluster analysis, evaluating and comparing solutions through different quality indices (e.g., weighted Silhouette), and presenting the k-top solutions to the analyst. SELF-DATA will also include a knowledge base storing results of experiments on previously processed datasets, and a classification algorithm trained on the knowledge base content to forecast the best configuration for the whole mining process for an unexplored dataset. The first development of SELF-DATA running on Apache Spark has been validated on 5 collections of documents. Experimental results highlight that TF-IDF and logarithmic entropy are effective to measure item relevance with sparse datasets, and the LSI method outperforms PCA with a large dictionary.",
        "link": "https://dl.acm.org/doi/10.1145/3102254.3102288"
    },
    {
        "title": "Interactive Visual Graph Mining and Learning",
        "authors": [
            "Ryan A. Rossi",
            "Nesreen K. Ahmed",
            "Rong Zhou",
            "Hoda Eldardiry"
        ],
        "date": null,
        "source": "ACM Transactions on Intelligent Systems and Technology",
        "abstract": "This article presents a platform for interactive graph mining and relational machine learning called GraphVis. The platform combines interactive visual representations with state-of-the-art graph mining and relational machine learning techniques to aid in revealing important insights quickly as well as learning an appropriate and highly predictive model for a particular task (e.g., classification, link prediction, discovering the roles of nodes, and finding influential nodes). Visual representations and interaction techniques and tools are developed for simple, fast, and intuitive real-time interactive exploration, mining, and modeling of graph data. In particular, we propose techniques for interactive relational learning (e.g., node/link classification), interactive link prediction and weighting, role discovery and community detection, higher-order network analysis (via graphlets, network motifs), among others. GraphVis also allows for the refinement and tuning of graph mining and relational learning methods for specific application domains and constraints via an end-to-end interactive visual analytic pipeline that learns, infers, and provides rapid interactive visualization with immediate feedback at each change/prediction in real-time. Other key aspects include interactive filtering, querying, ranking, manipulating, exporting, as well as tools for dynamic network analysis and visualization, interactive graph generators (including new block model approaches), and a variety of multi-level network analysis techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3200764"
    },
    {
        "title": "Data Mining and Opinion Mining: A Tool in Educational Context",
        "authors": [
            "Myriam Peñafiel",
            "Stefanie Vásquez",
            "Diego Vásquez",
            "Juan Zaldumbide",
            "Sergio Luján-Mora"
        ],
        "date": "July 2018",
        "source": "ICoMS '18: Proceedings of the 2018 1st International Conference on Mathematics and Statistics",
        "abstract": "The use of the web as a universal communication platform generates large volumes of data (Big data), which in many cases, need to be processed so that they can become useful knowledge in face of the sceptics who have doubts about the credibility of such information. The use of web data that comes from educational contexts needs to be addressed, since that large amount of unstructured information is not being valued, losing valuable information that can be used. To solve this problem, we propose the use of data mining techniques such as sentiment analysis to validate the information that comes from the educational platforms. The objective of this research is to propose a methodology that allows the user to apply sentiment analysis in a simple way, because although some researchers have done it, very few do with data in the educational context. The results obtained prove that the proposal can be used in similar cases.",
        "link": "https://dl.acm.org/doi/10.1145/3274250.3274263"
    },
    {
        "title": "Research Hotspots Mining and Visualized Analysis Based on Linking Cluster and K-Core Decomposition",
        "authors": [
            "Ying Cai",
            "Fang Huang",
            "Shuai Wang",
            "Hao Zhang",
            "Chunxiu Du"
        ],
        "date": "May 2018",
        "source": "ICDPA 2018: Proceedings of the International Conference on Data Processing and Applications",
        "abstract": "In this paper, we focus on research hotspots mining with knowledge semantic features on discipline topic word networks, which are extracted from scientific and technical documents. For the topic networks, a novel method of combining linking community clustering with k-core decomposition is proposed to mine the hotspot community with knowledge hierarchy structure. And as a post-processing, the hierarchical community identification is implemented by k-core decomposition in order to discover the internal connection with knowledge semantic hierarchy in the research hotspot. Finally, the community density and visualization method is utilized to analyze the core and sub- problem in the research hotspot community. In experiments, the topic networks for a single discipline and interdisciplinary research hotspot are taken as examples of analysis, and the results show the proposed approach can effectively realize the objective of mining and analyzing the discipline research hotspot on the academic topic word network.",
        "link": "https://dl.acm.org/doi/10.1145/3224207.3224216"
    },
    {
        "title": "The Application of Data Mining Algorithms in the Construction of Travel Recommendation System",
        "authors": [
            "Chao Lou",
            "Zhaonan Mu",
            "Mengzhu Liu"
        ],
        "date": "January 2021",
        "source": "CONF-CDS 2021: The 2nd International Conference on Computing and Data Science",
        "abstract": "Photos shared by users of online social network often associated with tags, texts, geographic and time information. These data are ideal research resource which can be used by researchers to observe people's interests and behaviors. They can also be used to analyze people's travel behaviors and interests because it can reflect people's travel activities and experiences directly. This project aimed to consolidate people into groups based on their travel preferences of travel season, travel time span, scenery type of travel location and then analyze popular travel places for different people groups. The travel preferences were analyzed from extracted data applying PHP language and Naive Bayes classification approach. According to the analyzed data, people were divided into groups. Then, DBSCAN clustering was used to get popular locations based on the different groups. As a result, 80 thousand photo data of 5 cities have been collected from Flickr by Phython. It has been showed that people's preference of season in most of the mined cities was spring and summer and the preference of time span was short. Moreover, mined people has been divided into 16 groups according to their preference and based on the groups popular locations have been calculated by DBSCAN clustering. Theses locations have been provided in a recommendation website which was built in this project. For the future work, mining Flickr data associated with other travel social network sites could be a good way to build a more effective recommendation system.",
        "link": "https://dl.acm.org/doi/10.1145/3448734.3450477"
    },
    {
        "title": "Compare Machine Learning Models in Text Classification Using Steam User Reviews",
        "authors": [
            "Youchen Miao",
            "Zeyu Jin",
            "Yumeng Zhang",
            "Yuchen Chen",
            "Junren Lai"
        ],
        "date": "November 2021",
        "source": "ICSED '21: Proceedings of the 2021 3rd International Conference on Software Engineering and Development",
        "abstract": "Text Classification and Sentiment Analysis of game reviews are viewed as important parts in not only academic fields but also in game studies. In this paper, with more than 400 thousand game reviews on Steam platform, we preprocess the data using different libraries (sklearn, nltk, and spaCy) and use them as inputs to build three sentiment classification models based on different algorithms (Naive Bayes, SVM, and Random Forest). In contrast to previous studies that only focus on different sentiment analysis models, our paper also highlights the use of different APIs to preprocess the data and their corresponding model performance. The results show that no matter which API we choose, Random Forest models always perform the best. However, in terms of training time, Naive Bayes is the fastest. This work can be used to apply grid search for researchers to automatically find the optimum API before conducting sentiment analysis in the future.",
        "link": "https://dl.acm.org/doi/10.1145/3507473.3507480"
    },
    {
        "title": "Lost in Transduction: Transductive Transfer Learning in Text Classification",
        "authors": [
            "Alejandro Moreo",
            "Andrea Esuli",
            "Fabrizio Sebastiani"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Obtaining high-quality labelled data for training a classifier in a new application domain is often costly. Transfer Learning\n(a.k.a. “Inductive Transfer”) tries to alleviate these costs by transferring, to the “target” domain of interest, knowledge available from a different “source” domain. In transfer learning the lack of labelled information from the target domain is compensated by the availability at training time of a set of unlabelled examples from the target distribution. Transductive Transfer Learning denotes the transfer learning setting in which the only set of target documents that we are interested in classifying is known and available at training time. Although this definition is indeed in line with Vapnik’s original definition of “transduction”, current terminology in the field is confused. In this article, we discuss how the term “transduction” has been misused in the transfer learning literature, and propose a clarification consistent with the original characterization of this term given by Vapnik. We go on to observe that the above terminology misuse has brought about misleading experimental comparisons, with inductive transfer learning methods that have been incorrectly compared with transductive transfer learning methods. We then, give empirical evidence that the difference in performance between the inductive version and the transductive version of a transfer learning method can indeed be statistically significant (i.e., that knowing at training time the only data one needs to classify indeed gives an advantage). Our clarification allows a reassessment of the field, and of the relative merits of the major, state-of-the-art algorithms for transfer learning in text classification.",
        "link": "https://dl.acm.org/doi/10.1145/3453146"
    },
    {
        "title": "Machine Learning on Mining Potential Adverse Drug Reactions for Pharmacovigilance",
        "authors": [
            "Ke Jia"
        ],
        "date": "July 2021",
        "source": "DSIT 2021: 2021 4th International Conference on Data Science and Information Technology",
        "abstract": "Drugs are related to the lives and health of patients. Mining and discovering potential adverse drug reactions from a large number of unstructured texts play an important role in drug research and pharmacovigilance. It is a hot research issue in the field of biomedicine in recent years. A large number of scholars have discovered a large amount of information on adverse drug reactions from different data sources through effective machine learning methods. This paper reviews the research progress in recent years to provide a reference for future research.",
        "link": "https://dl.acm.org/doi/10.1145/3478905.3478964"
    },
    {
        "title": "TACAM: Topic And Context Aware Argument Mining",
        "authors": [
            "Michael Fromm",
            "Evgeniy Faerman",
            "Thomas Seidl"
        ],
        "date": "October 2019",
        "source": "WI '19: IEEE/WIC/ACM International Conference on Web Intelligence",
        "abstract": "In this work we address the problem of argument search. The purpose of argument search is the distillation of pro and contra arguments for requested topics from large text corpora. In previous works, the usual approach is to use a standard search engine to extract text parts which are relevant to the given topic and subsequently use an argument recognition algorithm to select arguments from them. The main challenge in the argument recognition task, which is also known as argument mining, is that often sentences containing arguments are structurally similar to purely informative sentences without any stance about the topic. In fact, they only differ semantically. Most approaches use topic or search term information only for the first search step and therefore assume that arguments can be classified independently of a topic. We argue that topic information is crucial for argument mining, since the topic defines the semantic context of an argument. Precisely, we propose different models for the classification of arguments, which take information about a topic of an argument into account. Moreover, to enrich the context of a topic and to let models understand the context of the potential argument better, we integrate information from different external sources such as Knowledge Graphs or pre-trained NLP models. Our evaluation shows that considering topic information, especially in connection with external information, provides a significant performance boost for the argument mining task.",
        "link": "https://dl.acm.org/doi/10.1145/3350546.3352506"
    },
    {
        "title": "Text mining for incoming tasks based on the urgency/importance factors and task classification using machine learning tools",
        "authors": [
            "Yasser Ali Alshehri"
        ],
        "date": "March 2020",
        "source": "ICCDA 2020: Proceedings of the 2020 the 4th International Conference on Compute and Data Analysis",
        "abstract": "In workplaces, there is a massive amount of unstructured data from different sources. In this paper, we present a case study that explains how can through communications between employees, we can help to prioritize tasks requests to increase the efficiency of their works for both technical and non-technical workers. This involves managing daily incoming tasks based on their level of urgency and importance.To allow all workers to utilize the urgency-importance matrix as a time-management tool, we need to automate this tool. The textual content of incoming tasks are analyzed, and metrics related to urgency and importance are extracted. A third factor (i.e., the response variable) is defined based on the two input variables (urgency and importance). Then, machine learning applied to the data to predict the class of incoming tasks based on data outcome desired. We used ordinal regression, neural networks, and decision tree algorithms to predict the four levels of task priority. We measure the performance of all using recalls, precisions, and F-scores. All classifiers perform higher than 89% in terms of all measures.",
        "link": "https://dl.acm.org/doi/10.1145/3388142.3388153"
    },
    {
        "title": "Text mining for malware classification using multivariate all repeated patterns detection",
        "authors": [
            "Konstantinos F. Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2019",
        "source": "ASONAM '19: Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Mobile phones have become nowadays a commodity to the majority of people. Using them, people are able to access the world of Internet and connect with their friends, their colleagues at work or even unknown people with common interests. This proliferation of the mobile devices has also been seen as an opportunity for the cyber criminals to deceive smartphone users and steel their money directly or indirectly, respectively, by accessing their bank accounts through the smartphones or by blackmailing them or selling their private data such as photos, credit card data, etc. to third parties. This is usually achieved by installing malware to smartphones masking their malevolent payload as a legitimate application and advertise it to the users with the hope that mobile users will install it in their devices. Thus, any existing application can easily be modified by integrating a malware and then presented it as a legitimate one. In response to this, scientists have proposed a number of malware detection and classification methods using a variety of techniques. Even though, several of them achieve relatively high precision in malware classification, there is still space for improvement. In this paper, we propose a text mining all repeated pattern detection method which uses the decompiled files of an application in order to classify a suspicious application into one of the known malware families. Based on the experimental results using a real malware dataset, the methodology tries to correctly classify (without any misclassification) all randomly selected malware applications of 3 categories with 3 different families each.",
        "link": "https://dl.acm.org/doi/10.1145/3341161.3350841"
    },
    {
        "title": "Comparative Study between Traditional Machine Learning and Deep Learning Approaches for Text Classification",
        "authors": [
            "Cannannore Nidhi Kamath",
            "Syed Saqib Bukhari",
            "Andreas Dengel"
        ],
        "date": "August 2018",
        "source": "DocEng '18: Proceedings of the ACM Symposium on Document Engineering 2018",
        "abstract": "In this contemporaneous world, it is an obligation for any organization working with documents to end up with the insipid task of classifying truckload of documents, which is the nascent stage of venturing into the realm of information retrieval and data mining. But classification of such humongous documents into multiple classes, calls for a lot of time and labor. Hence a system which could classify these documents with acceptable accuracy would be of an unfathomable help in document engineering. We have created multiple classifiers for document classification and compared their accuracy on raw and processed data. We have garnered data used in a corporate organization as well as publicly available data for comparison. Data is processed by removing the stop-words and stemming is implemented to produce root words. Multiple traditional machine learning techniques like Naive Bayes, Logistic Regression, Support Vector Machine, Random forest Classifier and Multi-Layer Perceptron are used for classification of documents. Classifiers are applied on raw and processed data separately and their accuracy is noted. Along with this, Deep learning technique such as Convolution Neural Network is also used to classify the data and its accuracy is compared with that of traditional machine learning techniques. We are also exploring hierarchical classifiers for classification of classes and subclasses. The system classifies the data faster and with better accuracy than if done manually. The results are discussed in the results and evaluation section.",
        "link": "https://dl.acm.org/doi/10.1145/3209280.3209526"
    },
    {
        "title": "Identification of Overpricing in the Purchase of Medication by the Federal Government of Brazil, Using Text Mining and Clustering Based on Ontology",
        "authors": [
            "Marco Aurelio O. S. Correa",
            "Adriano Galindo Leal"
        ],
        "date": "August 2018",
        "source": "ICCBDC '18: Proceedings of the 2018 2nd International Conference on Cloud and Big Data Computing",
        "abstract": "Increasing the transparency level in his actions and spending is one of the primary duties of the Brazilian Federal Government. The creation of laws that oblige full disclose of all its expenditures through transparency portals enables citizens to supervise all government entities. However, only the dissemination of these data, without a definite standard or the availability of data analysis tools, does not guarantee that the citizen is empowered to play his role. Therefore hence, the objective of this work is to identify overprice in the acquisition of products purchased by the federal government of Brazil using the unstructured data available on the Transparency Portal. The last two-years' worth of purchasing data, available in the Transparency Portal, were extracted, processed and stored. Due to his diverse nature and high volume of data, this study focused only on medicines purchased by the Ministry of Health. Ontology-based text mining and clustering techniques were applied for automatic identification and classification of products. The processing of this information was done through text mining and clustering, based on the ontology registered in another database of the Brazilian government. Because of this work, a consolidated price base per medication was created to allow the identification of distortions in prices practised, facilitating the identification of cases that merit further investigation to unravel fraud to the treasury.",
        "link": "https://dl.acm.org/doi/10.1145/3264560.3264569"
    },
    {
        "title": "Computational Estimation by Scientific Data Mining with Classical Methods to Automate Learning Strategies of Scientists",
        "authors": [
            "Aparna S. Varde"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Experimental results are often plotted as 2-dimensional graphical plots (aka graphs) in scientific domains depicting dependent versus independent variables to aid visual analysis of processes. Repeatedly performing laboratory experiments consumes significant time and resources, motivating the need for computational estimation. The goals are to estimate the graph obtained in an experiment given its input conditions, and to estimate the conditions that would lead to a desired graph. Existing estimation approaches often do not meet accuracy and efficiency needs of targeted applications. We develop a computational estimation approach called AutoDomainMine that integrates clustering and classification over complex scientific data in a framework so as to automate classical learning methods of scientists. Knowledge discovered thereby from a database of existing experiments serves as the basis for estimation. Challenges include preserving domain semantics in clustering, finding matching strategies in classification, striking a good balance between elaboration and conciseness while displaying estimation results based on needs of targeted users, and deriving objective measures to capture subjective user interests. These and other challenges are addressed in this work. The AutoDomainMine approach is used to build a computational estimation system, rigorously evaluated with real data in Materials Science. Our evaluation confirms that AutoDomainMine provides desired accuracy and efficiency in computational estimation. It is extendable to other science and engineering domains as proved by adaptation of its sub-processes within fields such as Bioinformatics and Nanotechnology.",
        "link": "https://dl.acm.org/doi/10.1145/3502736"
    },
    {
        "title": "Where is the road for issue reports classification based on text mining?",
        "authors": [
            "Qiang Fan",
            "Yue Yu",
            "Gang Yin",
            "Tao Wang",
            "Huaimin Wang"
        ],
        "date": "November 2017",
        "source": "ESEM '17: Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Currently, open source projects receive various kinds of issues daily, because of the extreme openness of Issue Tracking System (ITS) in GitHub. ITS is a labor-intensive and time-consuming task of issue categorization for project managers. However, a contributor is only required a short textual abstract to report an issue in GitHub. Thus, most traditional classification approaches based on detailed and structured data (e.g., priority, severity, software version and so on) are difficult to adopt. In this paper, issue classification approaches on a large-scale dataset, including 80 popular projects and over 252,000 issue reports collected from GitHub, were investigated. First, four traditional text-based classification methods and their performances were discussed. Semantic perplexity (i.e., an issues description confuses bug-related sentences with nonbug-related sentences) is a crucial factor that affects the classification performances based on quantitative and qualitative study. Finally, A two-stage classifier framework based on the novel metrics of semantic perplexity of issue reports was designed. Results show that our two-stage classification can significantly improve issue classification performances.",
        "link": "https://dl.acm.org/doi/10.1109/ESEM.2017.19"
    },
    {
        "title": "Feature-based Facebook reviews process model for e-management using data mining",
        "authors": [
            "Anish Kumar Varudharajulu",
            "Yongsheng Ma"
        ],
        "date": "January 2019",
        "source": "IC4E '19: Proceedings of the 10th International Conference on E-Education, E-Business, E-Management and E-Learning",
        "abstract": "The data generated from online communication acts as potential gold mines for discovering knowledge for researchers. A large amount of data is also generated in the form of web documents, emails, blogs, and feedback, etc. Text analytics is being significantly employed to mine important information. Opinion mining is the process of extracting human thoughts and perceptions from unstructured texts. The showstopper for designing an opinion mining system for analyzing reviews arise from the fact that customer reviews are often noisy. These reviews are informally written. In addition, they are subjected to spelling mistakes, grammatical errors, improper punctuation and irrational capitalization. This paper focuses on analyzing the different classification and clustering algorithms aimed at extracting and consolidating opinions of customers from social media sites like Facebook, Twitter and through surveys, at multiple levels of granularity to monitor and measure customer satisfaction. Ours is an automated approach, in which the system aids in the process of knowledge assimilation for knowledge-based building and also performs the analytics. Domain experts ratify the knowledge base and also provide training datasets for the system to intuitively gather more instances for ratification. The system identifies opinion expressions as phrases containing opinion words, opinionated features and also opinion modifiers. These expressions are categorized as positive, negative or neutral. Opinion expressions are identified and categorized using localized linguistic techniques. Opinions can be congregated at any desired level of specificity i.e. feature level or product level, user level or service level, etc. We have developed a system based on this approach, which provides the user with a platform to analyze opinion expressions crawled from a set of pre-defined datasets.",
        "link": "https://dl.acm.org/doi/10.1145/3306500.3306514"
    },
    {
        "title": "Discriminative Topic Mining via Category-Name Guided Text Embedding",
        "authors": [
            "Yu Meng",
            "Jiaxin Huang",
            "Guangyuan Wang",
            "Zihan Wang",
            "Chao Zhang",
            "Yu Zhang",
            "Jiawei Han"
        ],
        "date": "April 2020",
        "source": "WWW '20: Proceedings of The Web Conference 2020",
        "abstract": "Mining a set of meaningful and distinctive topics automatically from massive text corpora has broad applications. Existing topic models, however, typically work in a purely unsupervised way, which often generate topics that do not fit users’ particular needs and yield suboptimal performance on downstream tasks. We propose a new task, discriminative topic mining, which leverages a set of user-provided category names to mine discriminative topics from text corpora. This new task not only helps a user understand clearly and distinctively the topics he/she is most interested in, but also benefits directly keyword-driven classification tasks. We develop CatE, a novel category-name guided text embedding method for discriminative topic mining, which effectively leverages minimal user guidance to learn a discriminative embedding space and discover category representative terms in an iterative manner. We conduct a comprehensive set of experiments to show that CatE mines high-quality set of topics guided by category names only, and benefits a variety of downstream applications including weakly-supervised classification and lexical entailment direction identification.",
        "link": "https://dl.acm.org/doi/10.1145/3366423.3380278"
    },
    {
        "title": "Comparative Study of Heart Disease Diagnosis Using Top Ten Data Mining Classification Algorithms",
        "authors": [
            "I. Ketut Agung Enriko"
        ],
        "date": "June 2019",
        "source": "ICFET '19: Proceedings of the 5th International Conference on Frontiers of Educational Technologies",
        "abstract": "Data mining has been used for many purposes, especially for prediction system. In healthcare, data mining algorithms often used in disease diagnosis. Meanwhile, heart disease is known as a primary cause of death over the years. Many studies have been performed in heart disease diagnosis using data mining methods. There are some popular data mining algorithms that can be used in heart disease diagnosis, for example, k-Nearest Neighbor, CART, and AdaBoost. The algorithms are used to analyze a sample of cardiovascular patients data and predict the heart disease type that they suffer. Some parameters are taken from the patient, including EKG morphology, blood pressure, and information about the existence of chest pain, shortness of breath, palpitation, and cold sweat. In this study, medical records data are collected from Harapan Kita Hospital and utilized as a dataset sample in this research. Top ten data mining classification algorithms are used in diagnosing heart disease from Harapan Kita Hospital data and examining their performance by checking the accuracy and speed.",
        "link": "https://dl.acm.org/doi/10.1145/3338188.3338220"
    },
    {
        "title": "Text Mining Approach for Identifying Research Trends",
        "authors": [
            "Snezhana Sulova"
        ],
        "date": "June 2021",
        "source": "CompSysTech '21: Proceedings of the 22nd International Conference on Computer Systems and Technologies",
        "abstract": "With the increase of unstructured data, the issues connected with automatic text processing, the categorization of documents and the discovery of topics have become objects of growing interest. In order to improve the process of grouping and processing research publications, we would like to propose a method based upon natural language processing. It is based on text mining technologies which aim to identify key tendencies in documents. It processes the content of publications by clustering and identifies the topics of each identified group. This analysis helps by identifying key tendencies as well as discovering emerging new areas of research. Publications from the research literature database, Scopus, were used to test the approach. The topic of the publications is “the application of digital technologies in the logistics business”. The experiments were completed using the RapidMiner Studio software.",
        "link": "https://dl.acm.org/doi/10.1145/3472410.3472433"
    },
    {
        "title": "Assessment of Vulnerability Severity using Text Mining",
        "authors": [
            "Georgios Spanos",
            "Lefteris Angelis",
            "Dimitrios Toloudis"
        ],
        "date": "September 2017",
        "source": "PCI '17: Proceedings of the 21st Pan-Hellenic Conference on Informatics",
        "abstract": "Software1 vulnerabilities are closely associated with information systems security, a major and critical field in today's technology. Vulnerabilities constitute a constant and increasing threat for various aspects of everyday life, especially for safety and economy, since the social impact from the problems that they cause is complicated and often unpredictable. Although there is an entire research branch in software engineering that deals with the identification and elimination of vulnerabilities, the growing complexity of software products and the variability of software production procedures are factors contributing to the ongoing occurrence of vulnerabilities, Hence, another area that is being developed in parallel focuses on the study and management of the vulnerabilities that have already been reported and registered in databases. The information contained in such databases includes, a textual description and a number of metrics related to vulnerabilities. The purpose of this paper is to investigate to what extend the assessment of the vulnerability severity can be inferred directly from the corresponding textual description, or in other words, to examine the informative power of the description with respect to the vulnerability severity. For this purpose, text mining techniques, i.e. text analysis and three different classification methods (decision trees, neural networks and support vector machines) were employed. The application of text mining to a sample of 70,678 vulnerabilities from a public data source shows that the description itself is a reliable and highly accurate source of information for vulnerability prioritization.",
        "link": "https://dl.acm.org/doi/10.1145/3139367.3139390"
    },
    {
        "title": "Clustering Algorithms for Spatial Data Mining",
        "authors": [
            "Chetashri Bhadane",
            "Ketan Shah"
        ],
        "date": "April 2020",
        "source": "ICGDA '20: Proceedings of the 2020 3rd International Conference on Geoinformatics and Data Analysis",
        "abstract": "With the advances in mobile and wireless technologies, there has been a rise in applications that track and share the users' geospatial data. People use several social networking sites such as Twitter, Facebook and Flickr, where they share their status updates. With the integration of Global Positioning System (GPS) with mobile phones, it is now possible to share one's locations on these social networks. GPS allows us to record and track a person's movement along with the timestamp. The data set obtained from these GPS logs is vast and is widely used to analyze the users' movement patterns. Specifically, we can find out significant locations based on the number of users present at that location and the time spent by them at such places. Once significant places have been identified, it is also possible to identify the semantic importance of these locations. This paper presents an overview of the clustering techniques used to find important places of interest using large GPS based mobility datasets. Four clustering algorithms, K-Means, DBSCAN, OPTICS and Hierarchical, are implemented, and performance is tested using real-time data of 50 users collected over 2--5 years. Performance summary depicts that K-Means and DBSCAN perform well for spatial data.",
        "link": "https://dl.acm.org/doi/10.1145/3397056.3397068"
    },
    {
        "title": "Feature Selection based Arabic Text Classification using Different Machine Learning Algorithms: Comparative Study",
        "authors": [
            "Sakina Rim Bennabi",
            "Zakaria Elberrichi"
        ],
        "date": "June 2020",
        "source": "ICIST '20: Proceedings of the 10th International Conference on Information Systems and Technologies",
        "abstract": "Feature selection is a method of data pre-processing widely used when mining large data, such as textual classification. Several studies have been conducted to compare the different methods of feature selection applied to corpora in English. Unfortunately, a small number of works concern the Arabic language. This article aims to present a comparative study of different feature selection techniques including: Chi2, the ANOVA method and mutual information, applied on a corpus in Arabic language, while also diversifying the machine learning algorithms (Naive Bayes, SVM and KNN). This experimental study has shown in general that reducing dimensionality with feature selection techniques has slightly affected the performance of textual classification, reducing the size of the corpus by up to 1%.",
        "link": "https://dl.acm.org/doi/10.1145/3447568.3448531"
    },
    {
        "title": "Intelligent Data Analysis using Optimized Support Vector Machine Based Data Mining Approach for Tourism Industry",
        "authors": [
            "Ms Promila Sharma",
            "Uma Meena",
            "Girish Kumar Sharma"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Data analysis involves the deployment of sophisticated approaches from data mining methods, information theory, and artificial intelligence in various fields like tourism, hospitality, and so on for the extraction of knowledge from the gathered and preprocessed data. In tourism, pattern analysis or data analysis using classification is significant for finding the patterns that represent new and potentially useful information or knowledge about the destination and other data. Several data mining techniques are introduced for the classification of data or patterns. However, overfitting, less accuracy, local minima, sensitive to noise are the drawbacks in some existing data mining classification methods. To overcome these challenges, Support vector machine with Red deer optimization (SVM-RDO) based data mining strategy is proposed in this article. Extended Kalman filter (EKF) is utilized in the first phase, i.e., data cleaning to remove the noise and missing values from the input data. Mantaray foraging algorithm (MaFA) is used in the data selection phase, in which the significant data are selected for the further process to reduce the computational complexity. The final phase is the classification, in which SVM-RDO is proposed to access the useful pattern from the selected data. PYTHON is the implementation tool used for the experiment of the proposed model. The experimental analysis is done to show the efficacy of the proposed work. From the experimental results, the proposed SVM-RDO achieved better accuracy, precision, recall, and F1 score than the existing methods for the tourism dataset. Thus, it is showed the effectiveness of the proposed SVM-RDO for pattern analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3494566"
    },
    {
        "title": "Applying text mining to predict learners' cognitive engagement",
        "authors": [
            "Hayati Hind",
            "Mohammed Khalidi Idrissi",
            "Samir Bennani"
        ],
        "date": "October 2017",
        "source": "SCAMS '17: Proceedings of the Mediterranean Symposium on Smart City Application",
        "abstract": "In academic process, engagement represent one of the important success' factor. This paper proposes a predictive system for learners' cognitive engagement based on their online discussion forums participation. In the first level the ontology OWL and the LSA method are used to perform a semantic classification system of the threads according to a specific context chosen by the tutor. Then the Text mining, as a predictive method, is applied to the classified threads and learners' participation in the forums.",
        "link": "https://dl.acm.org/doi/10.1145/3175628.3175655"
    },
    {
        "title": "Investigate Transitions into Drug Addiction through Text Mining of Reddit Data",
        "authors": [
            "John Lu",
            "Sumati Sridhar",
            "Ritika Pandey",
            "Mohammad Al Hasan",
            "Georege Mohler"
        ],
        "date": "July 2019",
        "source": "KDD '19: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Increasing rates of opioid drug abuse and heightened prevalence of online support communities underscore the necessity of employing data mining techniques to better understand drug addiction using these rapidly developing online resources. In this work, we obtained data from Reddit, an online collection of forums, to gather insight into drug use/misuse using text snippets from users narratives. Specifically, using users' posts, we trained a binary classifier which predicts a user's transitions from casual drug discussion forums to drug recovery forums. We also proposed a Cox regression model that outputs likelihoods of such transitions. In doing so, we found that utterances of select drugs and certain linguistic features contained in one's posts can help predict these transitions. Using unfiltered drug-related posts, our research delineates drugs that are associated with higher rates of transitions from recreational drug discussion to support/recovery discussion, offers insight into modern drug culture, and provides tools with potential applications in combating the opioid crisis.",
        "link": "https://dl.acm.org/doi/10.1145/3292500.3330737"
    },
    {
        "title": "Analysing digital banking reviews using text mining",
        "authors": [
            "Li Chen Cheng",
            "Legaspi Rhea Sharmayne"
        ],
        "date": "December 2020",
        "source": "ASONAM '20: Proceedings of the 12th IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Digital banks are new entrants in the banking industry in the Philippines as they only started late 2018. Since then, a handful of players have and are still emerging. With more and more people becoming technologically savvy, it is very critical for financial institutions to develop a digital banking application that will stand out from the competition. This paper aims to use text mining methods to analyse digital banking application reviews. This study will perform topic modelling using LDA to explore customer concerns and will mine association rules between the digital banking features with the review score. The results will reveal which areas the digital banking application can further optimize for customer satisfaction and retention.",
        "link": "https://dl.acm.org/doi/10.1109/ASONAM49781.2020.9381429"
    },
    {
        "title": "Mining Moocs Videos Metadata Using Classification Techniques",
        "authors": [
            "El Harrak Othman",
            "Ghadi Abderrahim",
            "El Bouhdidi Jaber"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "In few years, the internet has become the first source of information for most people, Today MOOCs makes it possible for everyone to access to the education over the world, it's represents an emerging methodology of online teaching and an important development in open education. Due to the rapid development of Moocs and the rapid growth of digital data and video database over the Internet, it is becoming very difficult for learners to browsing and choosing the best training for them. The scientific community has increased the amount of research into new technologies, with a view to improve the digital video utilization: its archiving, indexing, accessibility, acquisition, store and even its process and usability. All these parts of the video utilization entail the necessity of the extraction of all important information of a video, especially in cases of lack of metadata information. Web video mining is retrieving the content using data mining techniques from World Wide Web. There are two approaches for web video mining using traditional image processing (signal processing) and metadata based approach. In this paper, we present the various research makes in this subject and we propose an effective methodology to extract the metadata from Moocs videos and classify them based on the extracted metadata by applying data mining techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090450"
    },
    {
        "title": "Text mining with HathiTrust",
        "authors": [
            "Eleanor Dickson Koehl",
            "Ryan Dubnicek"
        ],
        "date": "June 2019",
        "source": "JCDL '19: Proceedings of the 18th Joint Conference on Digital Libraries",
        "abstract": "This tutorial will introduce attendees to the HathiTrust Research Center and its tools and services for computational text analysis research. HTRC leverages the scope and scale of the HathiTrust Digital Library collection to create opportunities for researchers to perform text data mining on subsets of the corpus. Attendees will develop skills that will allow them to conduct basic text analysis research using HathiTrust data, learn techniques for engaging with scholars on their text data mining research, and explore strategies for computational access to large-scale digital collections.",
        "link": "https://dl.acm.org/doi/10.1109/JCDL.2019.00115"
    },
    {
        "title": "Text mining for plagiarism detection: multivariate pattern detection for recognition of text similarities",
        "authors": [
            "Konstantinos Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2018",
        "source": "ASONAM '18: Proceedings of the 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "The problem of plagiarism the recent years has been intensified by the availability of information in digital form and the accessibility of the electronic libraries through the Internet. As a result, plagiarism detection has been transformed into a big data analytics problem since the number of digital sources is extravagant and a new document needs to be compared with millions of other existing documents. In this paper, a text mining methodology is proposed that can detect all common patterns between a document and the documents in a reference database. The technique is based on a pattern detection algorithm and the corresponding data structure that enables the algorithm to detect all common patterns. The methodology has been applied in a well-defined dataset providing very promising results identifying difficult cases of plagiarism such as technical disguise.",
        "link": "https://dl.acm.org/doi/10.5555/3382225.3382424"
    },
    {
        "title": "Petroleum Engineering Data Text Classification Using Convolutional Neural Network Based Classifier",
        "authors": [
            "Tzuhan Hsu",
            "Yaoqin Zhang"
        ],
        "date": "May 2018",
        "source": "ICMLT '18: Proceedings of the 2018 International Conference on Machine Learning Technologies",
        "abstract": "In this paper, authors introduce a convolutional neural network (CNN) based classification approach for petroleum engineering event. Recently, text classification problem has raised substantial attention in the field of machine learning. The aim of text classification process is to assign correct label to a text on the basis of its content. However, due to the imprecise of feature representation, most of former text classification methods cannot export satisfying result. To solve this problem, authors propose a model based on convolutional neural network. Authors conduct experiments with real world petroleum engineering data text, artificially labelled into 6 categories. Beside purposed CNN-based model, the authors also implement other baseline machine learning approaches: k nearest neighbor, random forest, support vector machines, recurrent neural network, gated recurrent units, long short-term memory. Among all these models, our CNN-based model reach best performance in text classification task. In the last part of this paper, authors compare and analysis their purpose and baseline model.",
        "link": "https://dl.acm.org/doi/10.1145/3231884.3231898"
    },
    {
        "title": "Classifying Indonesian Online Articles as Advertisement Placement Base Using Text Mining",
        "authors": [
            "Nadhira Tasya",
            "Arian Dhini"
        ],
        "date": "July 2017",
        "source": "ICBIM 2017: Proceedings of the International Conference on Business and Information Management",
        "abstract": "Rapid development in technological aspect resulting in growing level of human needs for the latest news, so that emerged a new trend of publishing and accessing news through online media or usually called online journalism. In addition, the number of people who sell and purchase through online sites also continues to increase and this opportunity is utilized by the company and the advertiser by implementing targeted web advertising. However, the high number of articles that have been published and accessed leads to great opportunities for errors in determining where to place the ads. Therefore, it needs a system that can categorize articles accessed by users as the basis of advertisement placement by the company and this classification system can be done by applying the method of Data Mining and Text Mining. This research uses document data in the form of article content that will be categorized into twenty categories of class of advertisement by using Text Mining technique with Support Vector Machine algorithm. The results of this study may be used by companies or advertisers as a basis for placement of ads on selected online media sites.",
        "link": "https://dl.acm.org/doi/10.1145/3134271.3134288"
    },
    {
        "title": "Mining Free-Text Medical Notes for Suicide Risk Assessment",
        "authors": [
            "Marios Adamou",
            "Grigoris Antoniou",
            "Elissavet Greasidou",
            "Vincenzo Lagani",
            "Paulos Charonyktakis",
            "Ioannis Tsamardinos"
        ],
        "date": "July 2018",
        "source": "SETN '18: Proceedings of the 10th Hellenic Conference on Artificial Intelligence",
        "abstract": "Suicide has been considered as an important public health issue for a very long time, and is one of the main causes of death worldwide. Despite suicide prevention strategies being applied, the rate of suicide has not changed substantially over the past decades. Advances in machine learning make it possible to attempt to predict suicide based on the analysis of relevant data to inform clinical practice. This paper reports on findings from the analysis of data of patients who died by suicide in the period 2013-2016 and made use of both structured data and free-text medical notes. We focus on examining various text-mining approaches to support risk assessment. The results show that using advance machine learning and text-mining techniques, it is possible to predict within a specified period which people are most at risk of taking their own life at the time of referral to a mental health service.",
        "link": "https://dl.acm.org/doi/10.1145/3200947.3201020"
    },
    {
        "title": "Integration of data mining techniques in e-learning systems: Clustering Profil of Lerners and Recommender Course System",
        "authors": [
            "A. El Moustamid",
            "E. En-Naimi",
            "J. El Bouhdidi"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "Data mining aims at the extraction of knowledge from large quantities of data by automatic or semi-automatic methods. In the field of education and training e-learning, Data Mining techniques as clustering (The aim of this technique is to identify groups that are similar in some aspect) [21], prediction (is used for developing model where combination of data viewed from different aspects can effect single viewed data. Here, is important to watch how predictors variables are effecting predicted variable)[21], classification are applied in e-learning to improve the level of learners by providing learning according to the learners' requirement) are used to analyze learners profile, to predict and improve student performance. The aim of our project is to develop a system capable of analyzing learners profile and indexing web videos in order to offer learners a rich database with courses that correspond to their levels.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090453"
    },
    {
        "title": "Multi-level mining and visualization of scientific text collections: Exploring a bi-lingual scientific repository",
        "authors": [
            "Pablo Accuosto",
            "Francesco Ronzano",
            "Daniel Ferrés",
            "Horacio Saggion"
        ],
        "date": "December 2017",
        "source": "WOSP 2017: Proceedings of the 6th International Workshop on Mining Scientific Publications",
        "abstract": "We present a system to mine and visualize collections of scientific documents by semantically browsing information extracted from single publications or aggregated throughout corpora of articles. The text mining tool performs deep analysis of document collections allowing the extraction and interpretation of research paper's contents. In addition to the extraction and enrichment of documents with metadata (titles, authors, affiliations, etc), the deep analysis performed comprises semantic interpretation, rhetorical analysis of sentences, triple-based information extraction, and text summarization. The visualization components allow geographical-based exploration of collections, topic-evolution interpretation, and collaborative network analysis among others. The paper presents a case study of a bi-lingual collection in the field of Natural Language Processing (NLP).",
        "link": "https://dl.acm.org/doi/10.1145/3127526.3127529"
    },
    {
        "title": "Hierarchical Topic Mining via Joint Spherical Tree and Text Embedding",
        "authors": [
            "Yu Meng",
            "Yunyi Zhang",
            "Jiaxin Huang",
            "Yu Zhang",
            "Chao Zhang",
            "Jiawei Han"
        ],
        "date": "August 2020",
        "source": "KDD '20: Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Mining a set of meaningful topics organized into a hierarchy is intuitively appealing since topic correlations are ubiquitous in massive text corpora. To account for potential hierarchical topic structures, hierarchical topic models generalize flat topic models by incorporating latent topic hierarchies into their generative modeling process. However, due to their purely unsupervised nature, the learned topic hierarchy often deviates from users' particular needs or interests. To guide the hierarchical topic discovery process with minimal user supervision, we propose a new task, Hierarchical Topic Mining, which takes a category tree described by category names only, and aims to mine a set of representative terms for each category from a text corpus to help a user comprehend his/her interested topics. We develop a novel joint tree and text embedding method along with a principled optimization procedure that allows simultaneous modeling of the category tree structure and the corpus generative process in the spherical space for effective category-representative term discovery. Our comprehensive experiments show that our model, named JoSH, mines a high-quality set of hierarchical topics with high efficiency and benefits weakly-supervised hierarchical text classification tasks.",
        "link": "https://dl.acm.org/doi/10.1145/3394486.3403242"
    },
    {
        "title": "Unsupervised behavioural mining and clustering for malware family identification",
        "authors": [
            "Khanh Huu The Dam",
            "Thomas Given-Wilson",
            "Axel Legay"
        ],
        "date": "March 2021",
        "source": "SAC '21: Proceedings of the 36th Annual ACM Symposium on Applied Computing",
        "abstract": "More accurate and advanced detection and classification of malware requires exploiting program behaviour and not merely syntactic or static features. One approach is to use system call dependency graphs (SCDGs) that represent the program behaviour by interactions with the system, and the relations between these interactions. These SCDGs have been used with supervised learning techniques to very accurately detect and classify malware. This works considers the unsupervised learning challenge of mining for common clusters of behaviour without a priori knowledge. This allows for clustering of similar programs by behaviour, that can then be used for either classification or further analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3412841.3441919"
    },
    {
        "title": "Text Mining for Evaluating Authors' Birth and Death Years",
        "authors": [
            "Dror Moghaz",
            "Yaakov Hacohen-Kerner",
            "Dov Gabbay"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "This article presents a unique method in text and data mining for finding the era, i.e., mining temporal data, in which an anonymous author was living. Finding this era can assist in the examination of a fake document or extracting the time period in which a writer lived. The study and the experiments concern Hebrew, and in some parts, Aramaic and Yiddish rabbinic texts. The rabbinic texts are undated and contain no bibliographic sections, posing an interesting challenge. This work proposes algorithms using key phrases and key words that allow the temporal organization of citations together with linguistic patterns. Based on these key phrases, key words, and the references, we established several types of “Iron-clad,” Heuristic and Greedy rules for estimating the years of birth and death of a writer in an interesting classification task. Experiments were conducted on corpora, including documents authored by 12, 24, and 36 rabbinic writers and demonstrated promising results.",
        "link": "https://dl.acm.org/doi/10.1145/3281631"
    },
    {
        "title": "Transfer Learning to Timed Text Based Video Classification Using CNN",
        "authors": [
            "Zenun Kastrati",
            "Ali Shariq Imran",
            "Arianit Kurti"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "Open educational video resources are gaining popularity with a growing number of massive open online courses (MOOCs). This has created a niche for content providers to adopt effective solutions in automatically organizing and structuring of educational resources for maximum visibility. Recent advances in deep learning techniques are proving useful in managing and classifying resources into appropriate categories. This paper proposes one such convolutional neural network (CNN) model for classifying video lectures in a MOOC setting using a transfer learning approach. The model uses a time-aligned text transcripts corresponding to video lectures from six broader subject categories. Video lectures and their corresponding transcript dataset is gathered from the Coursera MOOC platform. Two different CNN models are proposed: i) CNN based classification using embeddings learned from our MOOC dataset, ii) CNN based classification using transfer learning. Word embeddings generated from two well known state-of-the-art pre-trained models Word2Vec and GloVe, are used in the transfer learning approach for the second case. The proposed CNN models are evaluated using precision, recall, and F1 score and the obtained performance is compared with both conventional and deep learning classifiers. The proposed CNN models have an F1 score improvement of 10-22 percentage points over DNN and conventional classifiers",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326483"
    },
    {
        "title": "Evolutionary role mining in complex networks by ensemble clustering",
        "authors": [
            "Sarvenaz Choobdar",
            "Pedro Ribeiro",
            "Fernando Silva"
        ],
        "date": "April 2017",
        "source": "SAC '17: Proceedings of the Symposium on Applied Computing",
        "abstract": "The structural patterns in the neighborhood of nodes assign unique roles to the nodes. Mining the set of existing roles in a network provides a descriptive profile of the network and draws its general picture. This paper proposes a new method to determine structural roles in a dynamic network based on the current position of nodes and their historic behavior. We develop a temporal ensemble clustering technique to dynamically find groups of nodes, holding similar tempo-structural roles. We compare two weighting functions, based on age and distribution of data, to incorporate temporal behavior of nodes in the role discovery. To evaluate the performance of the proposed method, we assess the results from two points of view: 1) goodness of fit to current structure of the network; 2) consistency with historic data. We conduct the evaluation using different ensemble clustering techniques. The results on real world networks demonstrate that our method can detect tempo-structural roles that simultaneously depict the topology of a network and reflect its dynamics with high accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3019612.3019815"
    },
    {
        "title": "Analyzing used-car web listings via text mining",
        "authors": [
            "Ayhan Demiriz",
            "Fatma Cantaş"
        ],
        "date": "October 2017",
        "source": "IML '17: Proceedings of the 1st International Conference on Internet of Things and Machine Learning",
        "abstract": "Used car trade is one of the major components of the world economies. It is not uncommon to sell a car by placing an internet advertisement irrespective of the geography in these days. A typical content of an advertisement is usually composed of two parts namely the structured and the free text data. The structured data may include some information about the asking price, make, model, year, mileage of the car and the contact info. In most cases, seller may give important clues about the car's current conditions in the free text data where the title (head) of the advertisement can be included as free text too. This paper reports preliminary results from a text mining study conducted on 75K used car internet listings collected from two major car listing web sites in Turkey. As expected, the words and the phrases related to the description of the car are observed to be frequent. The leading concepts in the free text are found to be regarding how to describe the current condition of a car, for example \"no crash history\".",
        "link": "https://dl.acm.org/doi/10.1145/3109761.3109782"
    },
    {
        "title": "Text classification of Diseases Treated by Traditional Chinese Medicine Prescription based on machine learning",
        "authors": [
            "Hanqing Zhao",
            "Bo Han",
            "Chen Li"
        ],
        "date": "September 2020",
        "source": "ISAIMS '20: Proceedings of the 1st International Symposium on Artificial Intelligence in Medical Sciences",
        "abstract": "OBJECTIVE: To explore the application of machine learning in th e identification of diseases treated by traditional Chinese medicine prescriptions. METHODS: Based on the composition of the text document of Chinese medicine prescriptions, the prescriptions were divided into cough, headache and diarrhea. THUTCT were introduced to establish and train two machine learning text classification models, LibSVM and LibLinear, and the prescriptions to be deter mined were put into the model for classification and prediction. R ESULTS: The Precision rate of LibSVM model and Liblinear mod el were 0.7283 and 0.6690. Seven prescriptions were classified an d predicted, and the results were in line with expectations. CONCLUSION: THUCTC has good universality for the content of TCM prescriptions, high classification accuracy and fast testing speed, which is suitable for the text classification and discrimination research of TCM prescriptions for diseases.",
        "link": "https://dl.acm.org/doi/10.1145/3429889.3429895"
    },
    {
        "title": "Evaluation of retweet clustering method classification method using retweets on Twitter without text data",
        "authors": [
            "K. Uchida",
            "F. Toriumi",
            "T. Sakaki"
        ],
        "date": "August 2017",
        "source": "WI '17: Proceedings of the International Conference on Web Intelligence",
        "abstract": "Burst phenomena, which frequently occur on social media, are caused by such social events as flaming on the internet, elections, and natural disasters. To understand people's thoughts and feelings, we must classify their opinions from burst phenomena. Therefore, classification methods that categorize tweets are critical. However, since most classification methods focus on text mining, they cannot group tweets by topics because each tweet has poor linguistic similarities. We used a non-text-based classification method proposed by Baba et al. that groups tweets by topics, even if they have poor linguistic similarities, and verified its validity by comparing it with a text-based classification method in two different evaluations: qualitative and quantitative. In the qualitative evaluation part, we did a questionnaire survey and validated the suitability of the topic clusters created using both the non-and text-based methods. Since evaluating the similarity of every pair of tweets in each topic is difficult, we evaluated the similarity between sampled pairs in the survey and acquired more appropriate topic clustering results using the non-text-based method than the text-based method. In the quantitative evaluation part, we focused on the robustness of each method against data reduction. Many approaches analyze social media data, especially because collecting data from social media is comparatively easy. However, since collecting the whole data of burst phenomena is very costly due to the vast amounts of available social media data, robustness against data reduction is an important index to evaluate classification methods. With the non-text-based method, over 55% of the pairs of tweets in the same cluster were also included in the same cluster even when the data were reduced to 10% in all three of our example cases. In this paper, as a source we focus on Twitter, one of the most popular microblogging services. Using clustering to conduct detailed case analyses, we scrutinized three burst cases that include natural disasters and flaming on the internet and found that a non-text-based method more effectively classified tweets in burst phenomena than a text-based method.",
        "link": "https://dl.acm.org/doi/10.1145/3106426.3106451"
    },
    {
        "title": "Using Spark for Text Mining on Large Scale Liver Cancer Literature",
        "authors": [
            "Ming-Yen Lin",
            "Yu-Ju Lin",
            "Sue-Chen Hsueh"
        ],
        "date": "January 2021",
        "source": "BDET 2021: 2021 the 3rd International Conference on Big Data Engineering and Technology (BDET)",
        "abstract": "Cancer is one of the main causes of death. The number of known cancer to-date is more than one hundred. Liver cancer, ranked after trachea and lung cancer, has long been high in the cancer leading cause in Taiwan and even ranked as second in 2016. The number of scientific articles related to cancer proliferates every year. The number reaches as high as 20 million in PubMed so that discovering useful information from the massive collection is very difficult. In addition, using a single machine to sift through these articles is very time-consuming. Therefore, we present a big data analytic framework using the distributed Apache Spark platform for text mining in PubMed literature. We establish a prediction model for liver cancer articles so that researchers may effectively validate whether an article is related to liver cancer or not. Classification models in Spark MLlib including Linear Support Vector Machines (SVM), Logistic Regression, are used in our experiments. Relevancy to liver cancer is further confirmed by using MeSH (Medical Subject Headings) terms. Logistic regression is about 3 times faster than SVMs and the accuracy of both methods is close to 95% in the experiments using hold-out validation. When max_features is 500 and min_df ≤ 0.1 (or min_df = 1), the accuracy may reach 96%. In the experiments with K-fold cross-validation, the accuracy of SVMs methods is 96%. The experimental results show that that our prediction model may effectively classify liver cancer articles.",
        "link": "https://dl.acm.org/doi/10.1145/3474944.3474958"
    },
    {
        "title": "Automatic topic classification of test cases using text mining at an Android smartphone vendor",
        "authors": [
            "Junji Shimagaki",
            "Yasutaka Kamei",
            "Naoyasu Ubayashi",
            "Abram Hindle"
        ],
        "date": "October 2018",
        "source": "ESEM '18: Proceedings of the 12th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Background: An Android smartphone is an ecosystem of applications, drivers, operating system components, and assets. The volume of the software is large and the number of test cases needed to cover the functionality of an Android system is substantial. Enormous effort has been already taken to properly quantify \"what features and apps were tested and verified?\". This insight is provided by dashboards that summarize test coverage and results per feature. One method to achieve this is to manually tag or label test cases with the topic or function they cover, much like function points. At the studied Android smartphone vendor, tests are labelled with manually defined tags, so-called \"feature labels (FLs)\", and the FLs serve to categorize 100s to 1000s test cases into 10 to 50 groups. Aim: Unfortunately for developers, manual assignment of FLs to 1000s of test cases is a time consuming task, leading to inaccurately labeled test cases, which will render the dashboard useless. We created an automated system that suggests tags/labels to the developers for their test cases rather than manual labeling. Method: We use machine learning models to predict and label the functionality tested by 10,000 test cases developed at the company. Results: Through the quantitative experiments, our models achieved acceptable F-1 performance of 0.3 to 0.88. Also through the qualitative studies with expert teams, we showed that the hierarchy and path of tests was a good predictor of a feature's label. Conclusions: We find that this method can reduce tedious manual effort that software developers spent classifying test cases, while providing more accurate classification results.",
        "link": "https://dl.acm.org/doi/10.1145/3239235.3268927"
    },
    {
        "title": "To apply Data Mining for Classification of Crowd sourced Software Requirements",
        "authors": [
            "Soonh Taj",
            "Qasim Arain",
            "Imran Memon",
            "Asma Zubedi"
        ],
        "date": "April 2019",
        "source": "ICSIE '19: Proceedings of the 8th International Conference on Software and Information Engineering",
        "abstract": "Now a day's main focus of developers is to build quality software that works according to customer needs and for this reason it is necessary to gather right requirements as requirement elicitation is the critical step that impacts on the success of software project as misinterpreted requirements leads to the failure of software project. By keeping this in mind a research is carried out on improving requirements elicitation process and automating the process of classifying requirements. In this research, a model is proposed which will help in this scenario for requirements elicitation and requirement classification. This paper presents a model in which crowd sourcing approach is used so that customers, end users, stakeholders, developers and software engineers can make active participation for requirement elicitation process and requirements gathered using crowdsourcing approach are used by model for classification process i.e. classification of requirements into functional and non-functional requirements. For the proof of proposed model a case study is conducted. Results of case study provided the usefulness and efficiency of proposed model for classification of crowd sourced software requirements.",
        "link": "https://dl.acm.org/doi/10.1145/3328833.3328837"
    },
    {
        "title": "Text classification using Fuzzy TF-IDF and Machine Learning Models",
        "authors": [
            "Mariem Bounabi",
            "Karim El Moutaouakil",
            "Khalid Satori"
        ],
        "date": "October 2019",
        "source": "BDIoT'19: Proceedings of the 4th International Conference on Big Data and Internet of Things",
        "abstract": "The representation of the information has an important impact on the text classification task. Several weighting methods were proposed in the literature, and the term frequency-inverse term frequency (TFIDF), the most know on the text treatment field. The FTF-IDF is a vector representation where the components of the TFIDF are presented as inputs to the Fuzzy Inference System (FIS). In this work, we compare several Machin Learning algorithms such as Naïve Bayes and its derivatives, SVM and Random forest classifiers, using the FTF-IDF representation. To improve the quality of the used classifiers, we call sum attribute selection methods. The recognition rate, for the tested systems, is satisfied, where the system based on naïve Bayes classifier, the FTF-IDF weighting terms, and the info gain select attributes method gives 98.7% as accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3372938.3372956"
    },
    {
        "title": "Text Mining in Cybersecurity: A Systematic Literature Review",
        "authors": [
            "Luciano Ignaczak",
            "Guilherme Goldschmidt",
            "Cristiano André Da Costa",
            "Rodrigo Da Rosa Righi"
        ],
        "date": null,
        "source": "ACM Computing Surveys",
        "abstract": "The growth of data volume has changed cybersecurity activities, demanding a higher level of automation. In this new cybersecurity landscape, text mining emerged as an alternative to improve the efficiency of the activities involving unstructured data. This article proposes a Systematic Literature Review (SLR) to present the application of text mining in the cybersecurity domain. Using a systematic protocol, we identified 2,196 studies, out of which 83 were summarized. As a contribution, we propose a taxonomy to demonstrate the different activities in the cybersecurity domain supported by text mining. We also detail the strategies evaluated in the application of text mining tasks and the use of neural networks to support activities involving unstructured data. The work also discusses text classification performance aiming its application in real-world solutions. The SLR also highlights open gaps for future research, such as the analysis of non-English content and the intensification in the usage of neural networks.",
        "link": "https://dl.acm.org/doi/10.1145/3462477"
    },
    {
        "title": "Using Text Mining to Analyze the Financial Patents",
        "authors": [
            "Yung-Feng Lu",
            "Jia-Lang Xu",
            "Mu-Yen Chen"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "With the advancement and development of science and technology, Nowadays society is the development of information technology, cloud computing, mobile Internet, Internet of Things, block chain and information technology.The combination of finance and technology has made financial technology terms gradually emerging.. This research analyzes the patents of financial technology. However, Taiwan's financial technology patents are mostly G06Q-20 (payment plan, architecture or agreement), G06Q-30 (commercial, such as marketing, shopping, payment, auction or e-commerce). G06Q-40 (financial, investment or tax treatment, insurance).The research use text mining technology to analyze and understand Taiwan's trends and developments in financial technology. The research results show that G06Q-20 patents are biased towards information security and mobile payment. G06Q-30 patents favor shopping websites and advertising. G06Q-40 patents are biased towards diversity, such as securities, insurance, finance, and stocks.",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326478"
    },
    {
        "title": "Identifying Online Profiles of Distance Learning Students Using Data Mining Techniques",
        "authors": [
            "Osama Islam",
            "Muazzam Siddiqui",
            "Naif Radi Aljohani"
        ],
        "date": "October 2019",
        "source": "ICDTE '19: Proceedings of the 3rd International Conference on Digital Technology in Education",
        "abstract": "Educational data has grown over the years with the increased use of technology within educational environments. This has led to a huge amount of data being stored in various data sources representing the student, his/her activities, and other aspects relevant to the learning process. To meet this analytical need, Educational Data Mining (EDM) has emerged to assist educational institutions in identifying key benefits such as students at risk, the level of student engagement or predicting student performance. The aim of this research was to explore the various aspects of student interaction data using data mining techniques to identify relevant patterns of behaviors and possible key attributes that have higher degrees of influence on distance learning students. The main findings identified several patterns of user online profiles based on a set of adopted learning strategies, the research proposed a framework for analyzing such interaction data based on R and Hadoop platforms to correlate online profiles with student performance.",
        "link": "https://dl.acm.org/doi/10.1145/3369199.3369249"
    },
    {
        "title": "Extremely Fast Decision Tree Mining for Evolving Data Streams",
        "authors": [
            "Albert Bifet",
            "Jiajin Zhang",
            "Wei Fan",
            "Cheng He",
            "Jianfeng Zhang",
            "Jianfeng Qian",
            "Geoff Holmes",
            "Bernhard Pfahringer"
        ],
        "date": "August 2017",
        "source": "KDD '17: Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
        "abstract": "Nowadays real-time industrial applications are generating a huge amount of data continuously every day. To process these large data streams, we need fast and efficient methodologies and systems. A useful feature desired for data scientists and analysts is to have easy to visualize and understand machine learning models. Decision trees are preferred in many real-time applications for this reason, and also, because combined in an ensemble, they are one of the most powerful methods in machine learning. In this paper, we present a new system called STREAMDM-C++, that implements decision trees for data streams in C++, and that has been used extensively at Huawei. Streaming decision trees adapt to changes on streams, a huge advantage since standard decision trees are built using a snapshot of data, and can not evolve over time. STREAMDM-C++ is easy to extend, and contains more powerful ensemble methods, and a more efficient and easy to use adaptive decision trees. We compare our new implementation with VFML, the current state of the art implementation in C, and show how our new system outperforms VFML in speed using less resources.",
        "link": "https://dl.acm.org/doi/10.1145/3097983.3098139"
    },
    {
        "title": "Data miners' little helper: data transformation activity cues for cluster analysis on document collections",
        "authors": [
            "Tania Cerquitelli",
            "Evelina Di Corso",
            "Francesco Ventura",
            "Silvia Chiusano"
        ],
        "date": "June 2017",
        "source": "WIMS '17: Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "In this paper we propose a new self-learning engine to streamline the analytics process, as it enables analysts to mine massive data repositories with minimal user intervention. In the context of cluster analysis on a collection of documents this new system, named SELF-DATA (SELF-learning DAta TrAnsformation), suggests to the analyst how to configure the whole mining process for a given dataset. SELF-DATA relies on an engine exploring different data weighting schemas (e.g., normalized term frequencies) and data transformation methods (e.g., PCA) before applying the cluster analysis, evaluating and comparing solutions through different quality indices (e.g., weighted Silhouette), and presenting the k-top solutions to the analyst. SELF-DATA will also include a knowledge base storing results of experiments on previously processed datasets, and a classification algorithm trained on the knowledge base content to forecast the best configuration for the whole mining process for an unexplored dataset. The first development of SELF-DATA running on Apache Spark has been validated on 5 collections of documents. Experimental results highlight that TF-IDF and logarithmic entropy are effective to measure item relevance with sparse datasets, and the LSI method outperforms PCA with a large dictionary.",
        "link": "https://dl.acm.org/doi/10.1145/3102254.3102288"
    },
    {
        "title": "Interactive Visual Graph Mining and Learning",
        "authors": [
            "Ryan A. Rossi",
            "Nesreen K. Ahmed",
            "Rong Zhou",
            "Hoda Eldardiry"
        ],
        "date": null,
        "source": "ACM Transactions on Intelligent Systems and Technology",
        "abstract": "This article presents a platform for interactive graph mining and relational machine learning called GraphVis. The platform combines interactive visual representations with state-of-the-art graph mining and relational machine learning techniques to aid in revealing important insights quickly as well as learning an appropriate and highly predictive model for a particular task (e.g., classification, link prediction, discovering the roles of nodes, and finding influential nodes). Visual representations and interaction techniques and tools are developed for simple, fast, and intuitive real-time interactive exploration, mining, and modeling of graph data. In particular, we propose techniques for interactive relational learning (e.g., node/link classification), interactive link prediction and weighting, role discovery and community detection, higher-order network analysis (via graphlets, network motifs), among others. GraphVis also allows for the refinement and tuning of graph mining and relational learning methods for specific application domains and constraints via an end-to-end interactive visual analytic pipeline that learns, infers, and provides rapid interactive visualization with immediate feedback at each change/prediction in real-time. Other key aspects include interactive filtering, querying, ranking, manipulating, exporting, as well as tools for dynamic network analysis and visualization, interactive graph generators (including new block model approaches), and a variety of multi-level network analysis techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3200764"
    },
    {
        "title": "Data Mining and Opinion Mining: A Tool in Educational Context",
        "authors": [
            "Myriam Peñafiel",
            "Stefanie Vásquez",
            "Diego Vásquez",
            "Juan Zaldumbide",
            "Sergio Luján-Mora"
        ],
        "date": "July 2018",
        "source": "ICoMS '18: Proceedings of the 2018 1st International Conference on Mathematics and Statistics",
        "abstract": "The use of the web as a universal communication platform generates large volumes of data (Big data), which in many cases, need to be processed so that they can become useful knowledge in face of the sceptics who have doubts about the credibility of such information. The use of web data that comes from educational contexts needs to be addressed, since that large amount of unstructured information is not being valued, losing valuable information that can be used. To solve this problem, we propose the use of data mining techniques such as sentiment analysis to validate the information that comes from the educational platforms. The objective of this research is to propose a methodology that allows the user to apply sentiment analysis in a simple way, because although some researchers have done it, very few do with data in the educational context. The results obtained prove that the proposal can be used in similar cases.",
        "link": "https://dl.acm.org/doi/10.1145/3274250.3274263"
    },
    {
        "title": "Research Hotspots Mining and Visualized Analysis Based on Linking Cluster and K-Core Decomposition",
        "authors": [
            "Ying Cai",
            "Fang Huang",
            "Shuai Wang",
            "Hao Zhang",
            "Chunxiu Du"
        ],
        "date": "May 2018",
        "source": "ICDPA 2018: Proceedings of the International Conference on Data Processing and Applications",
        "abstract": "In this paper, we focus on research hotspots mining with knowledge semantic features on discipline topic word networks, which are extracted from scientific and technical documents. For the topic networks, a novel method of combining linking community clustering with k-core decomposition is proposed to mine the hotspot community with knowledge hierarchy structure. And as a post-processing, the hierarchical community identification is implemented by k-core decomposition in order to discover the internal connection with knowledge semantic hierarchy in the research hotspot. Finally, the community density and visualization method is utilized to analyze the core and sub- problem in the research hotspot community. In experiments, the topic networks for a single discipline and interdisciplinary research hotspot are taken as examples of analysis, and the results show the proposed approach can effectively realize the objective of mining and analyzing the discipline research hotspot on the academic topic word network.",
        "link": "https://dl.acm.org/doi/10.1145/3224207.3224216"
    },
    {
        "title": "The Application of Data Mining Algorithms in the Construction of Travel Recommendation System",
        "authors": [
            "Chao Lou",
            "Zhaonan Mu",
            "Mengzhu Liu"
        ],
        "date": "January 2021",
        "source": "CONF-CDS 2021: The 2nd International Conference on Computing and Data Science",
        "abstract": "Photos shared by users of online social network often associated with tags, texts, geographic and time information. These data are ideal research resource which can be used by researchers to observe people's interests and behaviors. They can also be used to analyze people's travel behaviors and interests because it can reflect people's travel activities and experiences directly. This project aimed to consolidate people into groups based on their travel preferences of travel season, travel time span, scenery type of travel location and then analyze popular travel places for different people groups. The travel preferences were analyzed from extracted data applying PHP language and Naive Bayes classification approach. According to the analyzed data, people were divided into groups. Then, DBSCAN clustering was used to get popular locations based on the different groups. As a result, 80 thousand photo data of 5 cities have been collected from Flickr by Phython. It has been showed that people's preference of season in most of the mined cities was spring and summer and the preference of time span was short. Moreover, mined people has been divided into 16 groups according to their preference and based on the groups popular locations have been calculated by DBSCAN clustering. Theses locations have been provided in a recommendation website which was built in this project. For the future work, mining Flickr data associated with other travel social network sites could be a good way to build a more effective recommendation system.",
        "link": "https://dl.acm.org/doi/10.1145/3448734.3450477"
    },
    {
        "title": "Compare Machine Learning Models in Text Classification Using Steam User Reviews",
        "authors": [
            "Youchen Miao",
            "Zeyu Jin",
            "Yumeng Zhang",
            "Yuchen Chen",
            "Junren Lai"
        ],
        "date": "November 2021",
        "source": "ICSED '21: Proceedings of the 2021 3rd International Conference on Software Engineering and Development",
        "abstract": "Text Classification and Sentiment Analysis of game reviews are viewed as important parts in not only academic fields but also in game studies. In this paper, with more than 400 thousand game reviews on Steam platform, we preprocess the data using different libraries (sklearn, nltk, and spaCy) and use them as inputs to build three sentiment classification models based on different algorithms (Naive Bayes, SVM, and Random Forest). In contrast to previous studies that only focus on different sentiment analysis models, our paper also highlights the use of different APIs to preprocess the data and their corresponding model performance. The results show that no matter which API we choose, Random Forest models always perform the best. However, in terms of training time, Naive Bayes is the fastest. This work can be used to apply grid search for researchers to automatically find the optimum API before conducting sentiment analysis in the future.",
        "link": "https://dl.acm.org/doi/10.1145/3507473.3507480"
    },
    {
        "title": "Lost in Transduction: Transductive Transfer Learning in Text Classification",
        "authors": [
            "Alejandro Moreo",
            "Andrea Esuli",
            "Fabrizio Sebastiani"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Obtaining high-quality labelled data for training a classifier in a new application domain is often costly. Transfer Learning\n(a.k.a. “Inductive Transfer”) tries to alleviate these costs by transferring, to the “target” domain of interest, knowledge available from a different “source” domain. In transfer learning the lack of labelled information from the target domain is compensated by the availability at training time of a set of unlabelled examples from the target distribution. Transductive Transfer Learning denotes the transfer learning setting in which the only set of target documents that we are interested in classifying is known and available at training time. Although this definition is indeed in line with Vapnik’s original definition of “transduction”, current terminology in the field is confused. In this article, we discuss how the term “transduction” has been misused in the transfer learning literature, and propose a clarification consistent with the original characterization of this term given by Vapnik. We go on to observe that the above terminology misuse has brought about misleading experimental comparisons, with inductive transfer learning methods that have been incorrectly compared with transductive transfer learning methods. We then, give empirical evidence that the difference in performance between the inductive version and the transductive version of a transfer learning method can indeed be statistically significant (i.e., that knowing at training time the only data one needs to classify indeed gives an advantage). Our clarification allows a reassessment of the field, and of the relative merits of the major, state-of-the-art algorithms for transfer learning in text classification.",
        "link": "https://dl.acm.org/doi/10.1145/3453146"
    },
    {
        "title": "Machine Learning on Mining Potential Adverse Drug Reactions for Pharmacovigilance",
        "authors": [
            "Ke Jia"
        ],
        "date": "July 2021",
        "source": "DSIT 2021: 2021 4th International Conference on Data Science and Information Technology",
        "abstract": "Drugs are related to the lives and health of patients. Mining and discovering potential adverse drug reactions from a large number of unstructured texts play an important role in drug research and pharmacovigilance. It is a hot research issue in the field of biomedicine in recent years. A large number of scholars have discovered a large amount of information on adverse drug reactions from different data sources through effective machine learning methods. This paper reviews the research progress in recent years to provide a reference for future research.",
        "link": "https://dl.acm.org/doi/10.1145/3478905.3478964"
    },
    {
        "title": "TACAM: Topic And Context Aware Argument Mining",
        "authors": [
            "Michael Fromm",
            "Evgeniy Faerman",
            "Thomas Seidl"
        ],
        "date": "October 2019",
        "source": "WI '19: IEEE/WIC/ACM International Conference on Web Intelligence",
        "abstract": "In this work we address the problem of argument search. The purpose of argument search is the distillation of pro and contra arguments for requested topics from large text corpora. In previous works, the usual approach is to use a standard search engine to extract text parts which are relevant to the given topic and subsequently use an argument recognition algorithm to select arguments from them. The main challenge in the argument recognition task, which is also known as argument mining, is that often sentences containing arguments are structurally similar to purely informative sentences without any stance about the topic. In fact, they only differ semantically. Most approaches use topic or search term information only for the first search step and therefore assume that arguments can be classified independently of a topic. We argue that topic information is crucial for argument mining, since the topic defines the semantic context of an argument. Precisely, we propose different models for the classification of arguments, which take information about a topic of an argument into account. Moreover, to enrich the context of a topic and to let models understand the context of the potential argument better, we integrate information from different external sources such as Knowledge Graphs or pre-trained NLP models. Our evaluation shows that considering topic information, especially in connection with external information, provides a significant performance boost for the argument mining task.",
        "link": "https://dl.acm.org/doi/10.1145/3350546.3352506"
    },
    {
        "title": "Text mining for incoming tasks based on the urgency/importance factors and task classification using machine learning tools",
        "authors": [
            "Yasser Ali Alshehri"
        ],
        "date": "March 2020",
        "source": "ICCDA 2020: Proceedings of the 2020 the 4th International Conference on Compute and Data Analysis",
        "abstract": "In workplaces, there is a massive amount of unstructured data from different sources. In this paper, we present a case study that explains how can through communications between employees, we can help to prioritize tasks requests to increase the efficiency of their works for both technical and non-technical workers. This involves managing daily incoming tasks based on their level of urgency and importance.To allow all workers to utilize the urgency-importance matrix as a time-management tool, we need to automate this tool. The textual content of incoming tasks are analyzed, and metrics related to urgency and importance are extracted. A third factor (i.e., the response variable) is defined based on the two input variables (urgency and importance). Then, machine learning applied to the data to predict the class of incoming tasks based on data outcome desired. We used ordinal regression, neural networks, and decision tree algorithms to predict the four levels of task priority. We measure the performance of all using recalls, precisions, and F-scores. All classifiers perform higher than 89% in terms of all measures.",
        "link": "https://dl.acm.org/doi/10.1145/3388142.3388153"
    },
    {
        "title": "Text mining for malware classification using multivariate all repeated patterns detection",
        "authors": [
            "Konstantinos F. Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2019",
        "source": "ASONAM '19: Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Mobile phones have become nowadays a commodity to the majority of people. Using them, people are able to access the world of Internet and connect with their friends, their colleagues at work or even unknown people with common interests. This proliferation of the mobile devices has also been seen as an opportunity for the cyber criminals to deceive smartphone users and steel their money directly or indirectly, respectively, by accessing their bank accounts through the smartphones or by blackmailing them or selling their private data such as photos, credit card data, etc. to third parties. This is usually achieved by installing malware to smartphones masking their malevolent payload as a legitimate application and advertise it to the users with the hope that mobile users will install it in their devices. Thus, any existing application can easily be modified by integrating a malware and then presented it as a legitimate one. In response to this, scientists have proposed a number of malware detection and classification methods using a variety of techniques. Even though, several of them achieve relatively high precision in malware classification, there is still space for improvement. In this paper, we propose a text mining all repeated pattern detection method which uses the decompiled files of an application in order to classify a suspicious application into one of the known malware families. Based on the experimental results using a real malware dataset, the methodology tries to correctly classify (without any misclassification) all randomly selected malware applications of 3 categories with 3 different families each.",
        "link": "https://dl.acm.org/doi/10.1145/3341161.3350841"
    },
    {
        "title": "Comparative Study between Traditional Machine Learning and Deep Learning Approaches for Text Classification",
        "authors": [
            "Cannannore Nidhi Kamath",
            "Syed Saqib Bukhari",
            "Andreas Dengel"
        ],
        "date": "August 2018",
        "source": "DocEng '18: Proceedings of the ACM Symposium on Document Engineering 2018",
        "abstract": "In this contemporaneous world, it is an obligation for any organization working with documents to end up with the insipid task of classifying truckload of documents, which is the nascent stage of venturing into the realm of information retrieval and data mining. But classification of such humongous documents into multiple classes, calls for a lot of time and labor. Hence a system which could classify these documents with acceptable accuracy would be of an unfathomable help in document engineering. We have created multiple classifiers for document classification and compared their accuracy on raw and processed data. We have garnered data used in a corporate organization as well as publicly available data for comparison. Data is processed by removing the stop-words and stemming is implemented to produce root words. Multiple traditional machine learning techniques like Naive Bayes, Logistic Regression, Support Vector Machine, Random forest Classifier and Multi-Layer Perceptron are used for classification of documents. Classifiers are applied on raw and processed data separately and their accuracy is noted. Along with this, Deep learning technique such as Convolution Neural Network is also used to classify the data and its accuracy is compared with that of traditional machine learning techniques. We are also exploring hierarchical classifiers for classification of classes and subclasses. The system classifies the data faster and with better accuracy than if done manually. The results are discussed in the results and evaluation section.",
        "link": "https://dl.acm.org/doi/10.1145/3209280.3209526"
    },
    {
        "title": "Identification of Overpricing in the Purchase of Medication by the Federal Government of Brazil, Using Text Mining and Clustering Based on Ontology",
        "authors": [
            "Marco Aurelio O. S. Correa",
            "Adriano Galindo Leal"
        ],
        "date": "August 2018",
        "source": "ICCBDC '18: Proceedings of the 2018 2nd International Conference on Cloud and Big Data Computing",
        "abstract": "Increasing the transparency level in his actions and spending is one of the primary duties of the Brazilian Federal Government. The creation of laws that oblige full disclose of all its expenditures through transparency portals enables citizens to supervise all government entities. However, only the dissemination of these data, without a definite standard or the availability of data analysis tools, does not guarantee that the citizen is empowered to play his role. Therefore hence, the objective of this work is to identify overprice in the acquisition of products purchased by the federal government of Brazil using the unstructured data available on the Transparency Portal. The last two-years' worth of purchasing data, available in the Transparency Portal, were extracted, processed and stored. Due to his diverse nature and high volume of data, this study focused only on medicines purchased by the Ministry of Health. Ontology-based text mining and clustering techniques were applied for automatic identification and classification of products. The processing of this information was done through text mining and clustering, based on the ontology registered in another database of the Brazilian government. Because of this work, a consolidated price base per medication was created to allow the identification of distortions in prices practised, facilitating the identification of cases that merit further investigation to unravel fraud to the treasury.",
        "link": "https://dl.acm.org/doi/10.1145/3264560.3264569"
    },
    {
        "title": "Computational Estimation by Scientific Data Mining with Classical Methods to Automate Learning Strategies of Scientists",
        "authors": [
            "Aparna S. Varde"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Experimental results are often plotted as 2-dimensional graphical plots (aka graphs) in scientific domains depicting dependent versus independent variables to aid visual analysis of processes. Repeatedly performing laboratory experiments consumes significant time and resources, motivating the need for computational estimation. The goals are to estimate the graph obtained in an experiment given its input conditions, and to estimate the conditions that would lead to a desired graph. Existing estimation approaches often do not meet accuracy and efficiency needs of targeted applications. We develop a computational estimation approach called AutoDomainMine that integrates clustering and classification over complex scientific data in a framework so as to automate classical learning methods of scientists. Knowledge discovered thereby from a database of existing experiments serves as the basis for estimation. Challenges include preserving domain semantics in clustering, finding matching strategies in classification, striking a good balance between elaboration and conciseness while displaying estimation results based on needs of targeted users, and deriving objective measures to capture subjective user interests. These and other challenges are addressed in this work. The AutoDomainMine approach is used to build a computational estimation system, rigorously evaluated with real data in Materials Science. Our evaluation confirms that AutoDomainMine provides desired accuracy and efficiency in computational estimation. It is extendable to other science and engineering domains as proved by adaptation of its sub-processes within fields such as Bioinformatics and Nanotechnology.",
        "link": "https://dl.acm.org/doi/10.1145/3502736"
    },
    {
        "title": "Where is the road for issue reports classification based on text mining?",
        "authors": [
            "Qiang Fan",
            "Yue Yu",
            "Gang Yin",
            "Tao Wang",
            "Huaimin Wang"
        ],
        "date": "November 2017",
        "source": "ESEM '17: Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Currently, open source projects receive various kinds of issues daily, because of the extreme openness of Issue Tracking System (ITS) in GitHub. ITS is a labor-intensive and time-consuming task of issue categorization for project managers. However, a contributor is only required a short textual abstract to report an issue in GitHub. Thus, most traditional classification approaches based on detailed and structured data (e.g., priority, severity, software version and so on) are difficult to adopt. In this paper, issue classification approaches on a large-scale dataset, including 80 popular projects and over 252,000 issue reports collected from GitHub, were investigated. First, four traditional text-based classification methods and their performances were discussed. Semantic perplexity (i.e., an issues description confuses bug-related sentences with nonbug-related sentences) is a crucial factor that affects the classification performances based on quantitative and qualitative study. Finally, A two-stage classifier framework based on the novel metrics of semantic perplexity of issue reports was designed. Results show that our two-stage classification can significantly improve issue classification performances.",
        "link": "https://dl.acm.org/doi/10.1109/ESEM.2017.19"
    },
    {
        "title": "Feature-based Facebook reviews process model for e-management using data mining",
        "authors": [
            "Anish Kumar Varudharajulu",
            "Yongsheng Ma"
        ],
        "date": "January 2019",
        "source": "IC4E '19: Proceedings of the 10th International Conference on E-Education, E-Business, E-Management and E-Learning",
        "abstract": "The data generated from online communication acts as potential gold mines for discovering knowledge for researchers. A large amount of data is also generated in the form of web documents, emails, blogs, and feedback, etc. Text analytics is being significantly employed to mine important information. Opinion mining is the process of extracting human thoughts and perceptions from unstructured texts. The showstopper for designing an opinion mining system for analyzing reviews arise from the fact that customer reviews are often noisy. These reviews are informally written. In addition, they are subjected to spelling mistakes, grammatical errors, improper punctuation and irrational capitalization. This paper focuses on analyzing the different classification and clustering algorithms aimed at extracting and consolidating opinions of customers from social media sites like Facebook, Twitter and through surveys, at multiple levels of granularity to monitor and measure customer satisfaction. Ours is an automated approach, in which the system aids in the process of knowledge assimilation for knowledge-based building and also performs the analytics. Domain experts ratify the knowledge base and also provide training datasets for the system to intuitively gather more instances for ratification. The system identifies opinion expressions as phrases containing opinion words, opinionated features and also opinion modifiers. These expressions are categorized as positive, negative or neutral. Opinion expressions are identified and categorized using localized linguistic techniques. Opinions can be congregated at any desired level of specificity i.e. feature level or product level, user level or service level, etc. We have developed a system based on this approach, which provides the user with a platform to analyze opinion expressions crawled from a set of pre-defined datasets.",
        "link": "https://dl.acm.org/doi/10.1145/3306500.3306514"
    },
    {
        "title": "Discriminative Topic Mining via Category-Name Guided Text Embedding",
        "authors": [
            "Yu Meng",
            "Jiaxin Huang",
            "Guangyuan Wang",
            "Zihan Wang",
            "Chao Zhang",
            "Yu Zhang",
            "Jiawei Han"
        ],
        "date": "April 2020",
        "source": "WWW '20: Proceedings of The Web Conference 2020",
        "abstract": "Mining a set of meaningful and distinctive topics automatically from massive text corpora has broad applications. Existing topic models, however, typically work in a purely unsupervised way, which often generate topics that do not fit users’ particular needs and yield suboptimal performance on downstream tasks. We propose a new task, discriminative topic mining, which leverages a set of user-provided category names to mine discriminative topics from text corpora. This new task not only helps a user understand clearly and distinctively the topics he/she is most interested in, but also benefits directly keyword-driven classification tasks. We develop CatE, a novel category-name guided text embedding method for discriminative topic mining, which effectively leverages minimal user guidance to learn a discriminative embedding space and discover category representative terms in an iterative manner. We conduct a comprehensive set of experiments to show that CatE mines high-quality set of topics guided by category names only, and benefits a variety of downstream applications including weakly-supervised classification and lexical entailment direction identification.",
        "link": "https://dl.acm.org/doi/10.1145/3366423.3380278"
    },
    {
        "title": "Comparative Study of Heart Disease Diagnosis Using Top Ten Data Mining Classification Algorithms",
        "authors": [
            "I. Ketut Agung Enriko"
        ],
        "date": "June 2019",
        "source": "ICFET '19: Proceedings of the 5th International Conference on Frontiers of Educational Technologies",
        "abstract": "Data mining has been used for many purposes, especially for prediction system. In healthcare, data mining algorithms often used in disease diagnosis. Meanwhile, heart disease is known as a primary cause of death over the years. Many studies have been performed in heart disease diagnosis using data mining methods. There are some popular data mining algorithms that can be used in heart disease diagnosis, for example, k-Nearest Neighbor, CART, and AdaBoost. The algorithms are used to analyze a sample of cardiovascular patients data and predict the heart disease type that they suffer. Some parameters are taken from the patient, including EKG morphology, blood pressure, and information about the existence of chest pain, shortness of breath, palpitation, and cold sweat. In this study, medical records data are collected from Harapan Kita Hospital and utilized as a dataset sample in this research. Top ten data mining classification algorithms are used in diagnosing heart disease from Harapan Kita Hospital data and examining their performance by checking the accuracy and speed.",
        "link": "https://dl.acm.org/doi/10.1145/3338188.3338220"
    },
    {
        "title": "Text Mining Approach for Identifying Research Trends",
        "authors": [
            "Snezhana Sulova"
        ],
        "date": "June 2021",
        "source": "CompSysTech '21: Proceedings of the 22nd International Conference on Computer Systems and Technologies",
        "abstract": "With the increase of unstructured data, the issues connected with automatic text processing, the categorization of documents and the discovery of topics have become objects of growing interest. In order to improve the process of grouping and processing research publications, we would like to propose a method based upon natural language processing. It is based on text mining technologies which aim to identify key tendencies in documents. It processes the content of publications by clustering and identifies the topics of each identified group. This analysis helps by identifying key tendencies as well as discovering emerging new areas of research. Publications from the research literature database, Scopus, were used to test the approach. The topic of the publications is “the application of digital technologies in the logistics business”. The experiments were completed using the RapidMiner Studio software.",
        "link": "https://dl.acm.org/doi/10.1145/3472410.3472433"
    },
    {
        "title": "Assessment of Vulnerability Severity using Text Mining",
        "authors": [
            "Georgios Spanos",
            "Lefteris Angelis",
            "Dimitrios Toloudis"
        ],
        "date": "September 2017",
        "source": "PCI '17: Proceedings of the 21st Pan-Hellenic Conference on Informatics",
        "abstract": "Software1 vulnerabilities are closely associated with information systems security, a major and critical field in today's technology. Vulnerabilities constitute a constant and increasing threat for various aspects of everyday life, especially for safety and economy, since the social impact from the problems that they cause is complicated and often unpredictable. Although there is an entire research branch in software engineering that deals with the identification and elimination of vulnerabilities, the growing complexity of software products and the variability of software production procedures are factors contributing to the ongoing occurrence of vulnerabilities, Hence, another area that is being developed in parallel focuses on the study and management of the vulnerabilities that have already been reported and registered in databases. The information contained in such databases includes, a textual description and a number of metrics related to vulnerabilities. The purpose of this paper is to investigate to what extend the assessment of the vulnerability severity can be inferred directly from the corresponding textual description, or in other words, to examine the informative power of the description with respect to the vulnerability severity. For this purpose, text mining techniques, i.e. text analysis and three different classification methods (decision trees, neural networks and support vector machines) were employed. The application of text mining to a sample of 70,678 vulnerabilities from a public data source shows that the description itself is a reliable and highly accurate source of information for vulnerability prioritization.",
        "link": "https://dl.acm.org/doi/10.1145/3139367.3139390"
    },
    {
        "title": "Clustering Algorithms for Spatial Data Mining",
        "authors": [
            "Chetashri Bhadane",
            "Ketan Shah"
        ],
        "date": "April 2020",
        "source": "ICGDA '20: Proceedings of the 2020 3rd International Conference on Geoinformatics and Data Analysis",
        "abstract": "With the advances in mobile and wireless technologies, there has been a rise in applications that track and share the users' geospatial data. People use several social networking sites such as Twitter, Facebook and Flickr, where they share their status updates. With the integration of Global Positioning System (GPS) with mobile phones, it is now possible to share one's locations on these social networks. GPS allows us to record and track a person's movement along with the timestamp. The data set obtained from these GPS logs is vast and is widely used to analyze the users' movement patterns. Specifically, we can find out significant locations based on the number of users present at that location and the time spent by them at such places. Once significant places have been identified, it is also possible to identify the semantic importance of these locations. This paper presents an overview of the clustering techniques used to find important places of interest using large GPS based mobility datasets. Four clustering algorithms, K-Means, DBSCAN, OPTICS and Hierarchical, are implemented, and performance is tested using real-time data of 50 users collected over 2--5 years. Performance summary depicts that K-Means and DBSCAN perform well for spatial data.",
        "link": "https://dl.acm.org/doi/10.1145/3397056.3397068"
    },
    {
        "title": "Feature Selection based Arabic Text Classification using Different Machine Learning Algorithms: Comparative Study",
        "authors": [
            "Sakina Rim Bennabi",
            "Zakaria Elberrichi"
        ],
        "date": "June 2020",
        "source": "ICIST '20: Proceedings of the 10th International Conference on Information Systems and Technologies",
        "abstract": "Feature selection is a method of data pre-processing widely used when mining large data, such as textual classification. Several studies have been conducted to compare the different methods of feature selection applied to corpora in English. Unfortunately, a small number of works concern the Arabic language. This article aims to present a comparative study of different feature selection techniques including: Chi2, the ANOVA method and mutual information, applied on a corpus in Arabic language, while also diversifying the machine learning algorithms (Naive Bayes, SVM and KNN). This experimental study has shown in general that reducing dimensionality with feature selection techniques has slightly affected the performance of textual classification, reducing the size of the corpus by up to 1%.",
        "link": "https://dl.acm.org/doi/10.1145/3447568.3448531"
    },
    {
        "title": "Intelligent Data Analysis using Optimized Support Vector Machine Based Data Mining Approach for Tourism Industry",
        "authors": [
            "Ms Promila Sharma",
            "Uma Meena",
            "Girish Kumar Sharma"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Data analysis involves the deployment of sophisticated approaches from data mining methods, information theory, and artificial intelligence in various fields like tourism, hospitality, and so on for the extraction of knowledge from the gathered and preprocessed data. In tourism, pattern analysis or data analysis using classification is significant for finding the patterns that represent new and potentially useful information or knowledge about the destination and other data. Several data mining techniques are introduced for the classification of data or patterns. However, overfitting, less accuracy, local minima, sensitive to noise are the drawbacks in some existing data mining classification methods. To overcome these challenges, Support vector machine with Red deer optimization (SVM-RDO) based data mining strategy is proposed in this article. Extended Kalman filter (EKF) is utilized in the first phase, i.e., data cleaning to remove the noise and missing values from the input data. Mantaray foraging algorithm (MaFA) is used in the data selection phase, in which the significant data are selected for the further process to reduce the computational complexity. The final phase is the classification, in which SVM-RDO is proposed to access the useful pattern from the selected data. PYTHON is the implementation tool used for the experiment of the proposed model. The experimental analysis is done to show the efficacy of the proposed work. From the experimental results, the proposed SVM-RDO achieved better accuracy, precision, recall, and F1 score than the existing methods for the tourism dataset. Thus, it is showed the effectiveness of the proposed SVM-RDO for pattern analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3494566"
    },
    {
        "title": "Applying text mining to predict learners' cognitive engagement",
        "authors": [
            "Hayati Hind",
            "Mohammed Khalidi Idrissi",
            "Samir Bennani"
        ],
        "date": "October 2017",
        "source": "SCAMS '17: Proceedings of the Mediterranean Symposium on Smart City Application",
        "abstract": "In academic process, engagement represent one of the important success' factor. This paper proposes a predictive system for learners' cognitive engagement based on their online discussion forums participation. In the first level the ontology OWL and the LSA method are used to perform a semantic classification system of the threads according to a specific context chosen by the tutor. Then the Text mining, as a predictive method, is applied to the classified threads and learners' participation in the forums.",
        "link": "https://dl.acm.org/doi/10.1145/3175628.3175655"
    },
    {
        "title": "Investigate Transitions into Drug Addiction through Text Mining of Reddit Data",
        "authors": [
            "John Lu",
            "Sumati Sridhar",
            "Ritika Pandey",
            "Mohammad Al Hasan",
            "Georege Mohler"
        ],
        "date": "July 2019",
        "source": "KDD '19: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Increasing rates of opioid drug abuse and heightened prevalence of online support communities underscore the necessity of employing data mining techniques to better understand drug addiction using these rapidly developing online resources. In this work, we obtained data from Reddit, an online collection of forums, to gather insight into drug use/misuse using text snippets from users narratives. Specifically, using users' posts, we trained a binary classifier which predicts a user's transitions from casual drug discussion forums to drug recovery forums. We also proposed a Cox regression model that outputs likelihoods of such transitions. In doing so, we found that utterances of select drugs and certain linguistic features contained in one's posts can help predict these transitions. Using unfiltered drug-related posts, our research delineates drugs that are associated with higher rates of transitions from recreational drug discussion to support/recovery discussion, offers insight into modern drug culture, and provides tools with potential applications in combating the opioid crisis.",
        "link": "https://dl.acm.org/doi/10.1145/3292500.3330737"
    },
    {
        "title": "Analysing digital banking reviews using text mining",
        "authors": [
            "Li Chen Cheng",
            "Legaspi Rhea Sharmayne"
        ],
        "date": "December 2020",
        "source": "ASONAM '20: Proceedings of the 12th IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Digital banks are new entrants in the banking industry in the Philippines as they only started late 2018. Since then, a handful of players have and are still emerging. With more and more people becoming technologically savvy, it is very critical for financial institutions to develop a digital banking application that will stand out from the competition. This paper aims to use text mining methods to analyse digital banking application reviews. This study will perform topic modelling using LDA to explore customer concerns and will mine association rules between the digital banking features with the review score. The results will reveal which areas the digital banking application can further optimize for customer satisfaction and retention.",
        "link": "https://dl.acm.org/doi/10.1109/ASONAM49781.2020.9381429"
    },
    {
        "title": "Mining Moocs Videos Metadata Using Classification Techniques",
        "authors": [
            "El Harrak Othman",
            "Ghadi Abderrahim",
            "El Bouhdidi Jaber"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "In few years, the internet has become the first source of information for most people, Today MOOCs makes it possible for everyone to access to the education over the world, it's represents an emerging methodology of online teaching and an important development in open education. Due to the rapid development of Moocs and the rapid growth of digital data and video database over the Internet, it is becoming very difficult for learners to browsing and choosing the best training for them. The scientific community has increased the amount of research into new technologies, with a view to improve the digital video utilization: its archiving, indexing, accessibility, acquisition, store and even its process and usability. All these parts of the video utilization entail the necessity of the extraction of all important information of a video, especially in cases of lack of metadata information. Web video mining is retrieving the content using data mining techniques from World Wide Web. There are two approaches for web video mining using traditional image processing (signal processing) and metadata based approach. In this paper, we present the various research makes in this subject and we propose an effective methodology to extract the metadata from Moocs videos and classify them based on the extracted metadata by applying data mining techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090450"
    },
    {
        "title": "Text mining with HathiTrust",
        "authors": [
            "Eleanor Dickson Koehl",
            "Ryan Dubnicek"
        ],
        "date": "June 2019",
        "source": "JCDL '19: Proceedings of the 18th Joint Conference on Digital Libraries",
        "abstract": "This tutorial will introduce attendees to the HathiTrust Research Center and its tools and services for computational text analysis research. HTRC leverages the scope and scale of the HathiTrust Digital Library collection to create opportunities for researchers to perform text data mining on subsets of the corpus. Attendees will develop skills that will allow them to conduct basic text analysis research using HathiTrust data, learn techniques for engaging with scholars on their text data mining research, and explore strategies for computational access to large-scale digital collections.",
        "link": "https://dl.acm.org/doi/10.1109/JCDL.2019.00115"
    },
    {
        "title": "Text mining for plagiarism detection: multivariate pattern detection for recognition of text similarities",
        "authors": [
            "Konstantinos Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2018",
        "source": "ASONAM '18: Proceedings of the 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "The problem of plagiarism the recent years has been intensified by the availability of information in digital form and the accessibility of the electronic libraries through the Internet. As a result, plagiarism detection has been transformed into a big data analytics problem since the number of digital sources is extravagant and a new document needs to be compared with millions of other existing documents. In this paper, a text mining methodology is proposed that can detect all common patterns between a document and the documents in a reference database. The technique is based on a pattern detection algorithm and the corresponding data structure that enables the algorithm to detect all common patterns. The methodology has been applied in a well-defined dataset providing very promising results identifying difficult cases of plagiarism such as technical disguise.",
        "link": "https://dl.acm.org/doi/10.5555/3382225.3382424"
    },
    {
        "title": "Petroleum Engineering Data Text Classification Using Convolutional Neural Network Based Classifier",
        "authors": [
            "Tzuhan Hsu",
            "Yaoqin Zhang"
        ],
        "date": "May 2018",
        "source": "ICMLT '18: Proceedings of the 2018 International Conference on Machine Learning Technologies",
        "abstract": "In this paper, authors introduce a convolutional neural network (CNN) based classification approach for petroleum engineering event. Recently, text classification problem has raised substantial attention in the field of machine learning. The aim of text classification process is to assign correct label to a text on the basis of its content. However, due to the imprecise of feature representation, most of former text classification methods cannot export satisfying result. To solve this problem, authors propose a model based on convolutional neural network. Authors conduct experiments with real world petroleum engineering data text, artificially labelled into 6 categories. Beside purposed CNN-based model, the authors also implement other baseline machine learning approaches: k nearest neighbor, random forest, support vector machines, recurrent neural network, gated recurrent units, long short-term memory. Among all these models, our CNN-based model reach best performance in text classification task. In the last part of this paper, authors compare and analysis their purpose and baseline model.",
        "link": "https://dl.acm.org/doi/10.1145/3231884.3231898"
    },
    {
        "title": "Classifying Indonesian Online Articles as Advertisement Placement Base Using Text Mining",
        "authors": [
            "Nadhira Tasya",
            "Arian Dhini"
        ],
        "date": "July 2017",
        "source": "ICBIM 2017: Proceedings of the International Conference on Business and Information Management",
        "abstract": "Rapid development in technological aspect resulting in growing level of human needs for the latest news, so that emerged a new trend of publishing and accessing news through online media or usually called online journalism. In addition, the number of people who sell and purchase through online sites also continues to increase and this opportunity is utilized by the company and the advertiser by implementing targeted web advertising. However, the high number of articles that have been published and accessed leads to great opportunities for errors in determining where to place the ads. Therefore, it needs a system that can categorize articles accessed by users as the basis of advertisement placement by the company and this classification system can be done by applying the method of Data Mining and Text Mining. This research uses document data in the form of article content that will be categorized into twenty categories of class of advertisement by using Text Mining technique with Support Vector Machine algorithm. The results of this study may be used by companies or advertisers as a basis for placement of ads on selected online media sites.",
        "link": "https://dl.acm.org/doi/10.1145/3134271.3134288"
    },
    {
        "title": "Mining Free-Text Medical Notes for Suicide Risk Assessment",
        "authors": [
            "Marios Adamou",
            "Grigoris Antoniou",
            "Elissavet Greasidou",
            "Vincenzo Lagani",
            "Paulos Charonyktakis",
            "Ioannis Tsamardinos"
        ],
        "date": "July 2018",
        "source": "SETN '18: Proceedings of the 10th Hellenic Conference on Artificial Intelligence",
        "abstract": "Suicide has been considered as an important public health issue for a very long time, and is one of the main causes of death worldwide. Despite suicide prevention strategies being applied, the rate of suicide has not changed substantially over the past decades. Advances in machine learning make it possible to attempt to predict suicide based on the analysis of relevant data to inform clinical practice. This paper reports on findings from the analysis of data of patients who died by suicide in the period 2013-2016 and made use of both structured data and free-text medical notes. We focus on examining various text-mining approaches to support risk assessment. The results show that using advance machine learning and text-mining techniques, it is possible to predict within a specified period which people are most at risk of taking their own life at the time of referral to a mental health service.",
        "link": "https://dl.acm.org/doi/10.1145/3200947.3201020"
    },
    {
        "title": "Integration of data mining techniques in e-learning systems: Clustering Profil of Lerners and Recommender Course System",
        "authors": [
            "A. El Moustamid",
            "E. En-Naimi",
            "J. El Bouhdidi"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "Data mining aims at the extraction of knowledge from large quantities of data by automatic or semi-automatic methods. In the field of education and training e-learning, Data Mining techniques as clustering (The aim of this technique is to identify groups that are similar in some aspect) [21], prediction (is used for developing model where combination of data viewed from different aspects can effect single viewed data. Here, is important to watch how predictors variables are effecting predicted variable)[21], classification are applied in e-learning to improve the level of learners by providing learning according to the learners' requirement) are used to analyze learners profile, to predict and improve student performance. The aim of our project is to develop a system capable of analyzing learners profile and indexing web videos in order to offer learners a rich database with courses that correspond to their levels.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090453"
    },
    {
        "title": "Multi-level mining and visualization of scientific text collections: Exploring a bi-lingual scientific repository",
        "authors": [
            "Pablo Accuosto",
            "Francesco Ronzano",
            "Daniel Ferrés",
            "Horacio Saggion"
        ],
        "date": "December 2017",
        "source": "WOSP 2017: Proceedings of the 6th International Workshop on Mining Scientific Publications",
        "abstract": "We present a system to mine and visualize collections of scientific documents by semantically browsing information extracted from single publications or aggregated throughout corpora of articles. The text mining tool performs deep analysis of document collections allowing the extraction and interpretation of research paper's contents. In addition to the extraction and enrichment of documents with metadata (titles, authors, affiliations, etc), the deep analysis performed comprises semantic interpretation, rhetorical analysis of sentences, triple-based information extraction, and text summarization. The visualization components allow geographical-based exploration of collections, topic-evolution interpretation, and collaborative network analysis among others. The paper presents a case study of a bi-lingual collection in the field of Natural Language Processing (NLP).",
        "link": "https://dl.acm.org/doi/10.1145/3127526.3127529"
    },
    {
        "title": "Hierarchical Topic Mining via Joint Spherical Tree and Text Embedding",
        "authors": [
            "Yu Meng",
            "Yunyi Zhang",
            "Jiaxin Huang",
            "Yu Zhang",
            "Chao Zhang",
            "Jiawei Han"
        ],
        "date": "August 2020",
        "source": "KDD '20: Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Mining a set of meaningful topics organized into a hierarchy is intuitively appealing since topic correlations are ubiquitous in massive text corpora. To account for potential hierarchical topic structures, hierarchical topic models generalize flat topic models by incorporating latent topic hierarchies into their generative modeling process. However, due to their purely unsupervised nature, the learned topic hierarchy often deviates from users' particular needs or interests. To guide the hierarchical topic discovery process with minimal user supervision, we propose a new task, Hierarchical Topic Mining, which takes a category tree described by category names only, and aims to mine a set of representative terms for each category from a text corpus to help a user comprehend his/her interested topics. We develop a novel joint tree and text embedding method along with a principled optimization procedure that allows simultaneous modeling of the category tree structure and the corpus generative process in the spherical space for effective category-representative term discovery. Our comprehensive experiments show that our model, named JoSH, mines a high-quality set of hierarchical topics with high efficiency and benefits weakly-supervised hierarchical text classification tasks.",
        "link": "https://dl.acm.org/doi/10.1145/3394486.3403242"
    },
    {
        "title": "Unsupervised behavioural mining and clustering for malware family identification",
        "authors": [
            "Khanh Huu The Dam",
            "Thomas Given-Wilson",
            "Axel Legay"
        ],
        "date": "March 2021",
        "source": "SAC '21: Proceedings of the 36th Annual ACM Symposium on Applied Computing",
        "abstract": "More accurate and advanced detection and classification of malware requires exploiting program behaviour and not merely syntactic or static features. One approach is to use system call dependency graphs (SCDGs) that represent the program behaviour by interactions with the system, and the relations between these interactions. These SCDGs have been used with supervised learning techniques to very accurately detect and classify malware. This works considers the unsupervised learning challenge of mining for common clusters of behaviour without a priori knowledge. This allows for clustering of similar programs by behaviour, that can then be used for either classification or further analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3412841.3441919"
    },
    {
        "title": "Text Mining for Evaluating Authors' Birth and Death Years",
        "authors": [
            "Dror Moghaz",
            "Yaakov Hacohen-Kerner",
            "Dov Gabbay"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "This article presents a unique method in text and data mining for finding the era, i.e., mining temporal data, in which an anonymous author was living. Finding this era can assist in the examination of a fake document or extracting the time period in which a writer lived. The study and the experiments concern Hebrew, and in some parts, Aramaic and Yiddish rabbinic texts. The rabbinic texts are undated and contain no bibliographic sections, posing an interesting challenge. This work proposes algorithms using key phrases and key words that allow the temporal organization of citations together with linguistic patterns. Based on these key phrases, key words, and the references, we established several types of “Iron-clad,” Heuristic and Greedy rules for estimating the years of birth and death of a writer in an interesting classification task. Experiments were conducted on corpora, including documents authored by 12, 24, and 36 rabbinic writers and demonstrated promising results.",
        "link": "https://dl.acm.org/doi/10.1145/3281631"
    },
    {
        "title": "Transfer Learning to Timed Text Based Video Classification Using CNN",
        "authors": [
            "Zenun Kastrati",
            "Ali Shariq Imran",
            "Arianit Kurti"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "Open educational video resources are gaining popularity with a growing number of massive open online courses (MOOCs). This has created a niche for content providers to adopt effective solutions in automatically organizing and structuring of educational resources for maximum visibility. Recent advances in deep learning techniques are proving useful in managing and classifying resources into appropriate categories. This paper proposes one such convolutional neural network (CNN) model for classifying video lectures in a MOOC setting using a transfer learning approach. The model uses a time-aligned text transcripts corresponding to video lectures from six broader subject categories. Video lectures and their corresponding transcript dataset is gathered from the Coursera MOOC platform. Two different CNN models are proposed: i) CNN based classification using embeddings learned from our MOOC dataset, ii) CNN based classification using transfer learning. Word embeddings generated from two well known state-of-the-art pre-trained models Word2Vec and GloVe, are used in the transfer learning approach for the second case. The proposed CNN models are evaluated using precision, recall, and F1 score and the obtained performance is compared with both conventional and deep learning classifiers. The proposed CNN models have an F1 score improvement of 10-22 percentage points over DNN and conventional classifiers",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326483"
    },
    {
        "title": "Evolutionary role mining in complex networks by ensemble clustering",
        "authors": [
            "Sarvenaz Choobdar",
            "Pedro Ribeiro",
            "Fernando Silva"
        ],
        "date": "April 2017",
        "source": "SAC '17: Proceedings of the Symposium on Applied Computing",
        "abstract": "The structural patterns in the neighborhood of nodes assign unique roles to the nodes. Mining the set of existing roles in a network provides a descriptive profile of the network and draws its general picture. This paper proposes a new method to determine structural roles in a dynamic network based on the current position of nodes and their historic behavior. We develop a temporal ensemble clustering technique to dynamically find groups of nodes, holding similar tempo-structural roles. We compare two weighting functions, based on age and distribution of data, to incorporate temporal behavior of nodes in the role discovery. To evaluate the performance of the proposed method, we assess the results from two points of view: 1) goodness of fit to current structure of the network; 2) consistency with historic data. We conduct the evaluation using different ensemble clustering techniques. The results on real world networks demonstrate that our method can detect tempo-structural roles that simultaneously depict the topology of a network and reflect its dynamics with high accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3019612.3019815"
    },
    {
        "title": "Analyzing used-car web listings via text mining",
        "authors": [
            "Ayhan Demiriz",
            "Fatma Cantaş"
        ],
        "date": "October 2017",
        "source": "IML '17: Proceedings of the 1st International Conference on Internet of Things and Machine Learning",
        "abstract": "Used car trade is one of the major components of the world economies. It is not uncommon to sell a car by placing an internet advertisement irrespective of the geography in these days. A typical content of an advertisement is usually composed of two parts namely the structured and the free text data. The structured data may include some information about the asking price, make, model, year, mileage of the car and the contact info. In most cases, seller may give important clues about the car's current conditions in the free text data where the title (head) of the advertisement can be included as free text too. This paper reports preliminary results from a text mining study conducted on 75K used car internet listings collected from two major car listing web sites in Turkey. As expected, the words and the phrases related to the description of the car are observed to be frequent. The leading concepts in the free text are found to be regarding how to describe the current condition of a car, for example \"no crash history\".",
        "link": "https://dl.acm.org/doi/10.1145/3109761.3109782"
    },
    {
        "title": "Text classification of Diseases Treated by Traditional Chinese Medicine Prescription based on machine learning",
        "authors": [
            "Hanqing Zhao",
            "Bo Han",
            "Chen Li"
        ],
        "date": "September 2020",
        "source": "ISAIMS '20: Proceedings of the 1st International Symposium on Artificial Intelligence in Medical Sciences",
        "abstract": "OBJECTIVE: To explore the application of machine learning in th e identification of diseases treated by traditional Chinese medicine prescriptions. METHODS: Based on the composition of the text document of Chinese medicine prescriptions, the prescriptions were divided into cough, headache and diarrhea. THUTCT were introduced to establish and train two machine learning text classification models, LibSVM and LibLinear, and the prescriptions to be deter mined were put into the model for classification and prediction. R ESULTS: The Precision rate of LibSVM model and Liblinear mod el were 0.7283 and 0.6690. Seven prescriptions were classified an d predicted, and the results were in line with expectations. CONCLUSION: THUCTC has good universality for the content of TCM prescriptions, high classification accuracy and fast testing speed, which is suitable for the text classification and discrimination research of TCM prescriptions for diseases.",
        "link": "https://dl.acm.org/doi/10.1145/3429889.3429895"
    },
    {
        "title": "Evaluation of retweet clustering method classification method using retweets on Twitter without text data",
        "authors": [
            "K. Uchida",
            "F. Toriumi",
            "T. Sakaki"
        ],
        "date": "August 2017",
        "source": "WI '17: Proceedings of the International Conference on Web Intelligence",
        "abstract": "Burst phenomena, which frequently occur on social media, are caused by such social events as flaming on the internet, elections, and natural disasters. To understand people's thoughts and feelings, we must classify their opinions from burst phenomena. Therefore, classification methods that categorize tweets are critical. However, since most classification methods focus on text mining, they cannot group tweets by topics because each tweet has poor linguistic similarities. We used a non-text-based classification method proposed by Baba et al. that groups tweets by topics, even if they have poor linguistic similarities, and verified its validity by comparing it with a text-based classification method in two different evaluations: qualitative and quantitative. In the qualitative evaluation part, we did a questionnaire survey and validated the suitability of the topic clusters created using both the non-and text-based methods. Since evaluating the similarity of every pair of tweets in each topic is difficult, we evaluated the similarity between sampled pairs in the survey and acquired more appropriate topic clustering results using the non-text-based method than the text-based method. In the quantitative evaluation part, we focused on the robustness of each method against data reduction. Many approaches analyze social media data, especially because collecting data from social media is comparatively easy. However, since collecting the whole data of burst phenomena is very costly due to the vast amounts of available social media data, robustness against data reduction is an important index to evaluate classification methods. With the non-text-based method, over 55% of the pairs of tweets in the same cluster were also included in the same cluster even when the data were reduced to 10% in all three of our example cases. In this paper, as a source we focus on Twitter, one of the most popular microblogging services. Using clustering to conduct detailed case analyses, we scrutinized three burst cases that include natural disasters and flaming on the internet and found that a non-text-based method more effectively classified tweets in burst phenomena than a text-based method.",
        "link": "https://dl.acm.org/doi/10.1145/3106426.3106451"
    },
    {
        "title": "Using Spark for Text Mining on Large Scale Liver Cancer Literature",
        "authors": [
            "Ming-Yen Lin",
            "Yu-Ju Lin",
            "Sue-Chen Hsueh"
        ],
        "date": "January 2021",
        "source": "BDET 2021: 2021 the 3rd International Conference on Big Data Engineering and Technology (BDET)",
        "abstract": "Cancer is one of the main causes of death. The number of known cancer to-date is more than one hundred. Liver cancer, ranked after trachea and lung cancer, has long been high in the cancer leading cause in Taiwan and even ranked as second in 2016. The number of scientific articles related to cancer proliferates every year. The number reaches as high as 20 million in PubMed so that discovering useful information from the massive collection is very difficult. In addition, using a single machine to sift through these articles is very time-consuming. Therefore, we present a big data analytic framework using the distributed Apache Spark platform for text mining in PubMed literature. We establish a prediction model for liver cancer articles so that researchers may effectively validate whether an article is related to liver cancer or not. Classification models in Spark MLlib including Linear Support Vector Machines (SVM), Logistic Regression, are used in our experiments. Relevancy to liver cancer is further confirmed by using MeSH (Medical Subject Headings) terms. Logistic regression is about 3 times faster than SVMs and the accuracy of both methods is close to 95% in the experiments using hold-out validation. When max_features is 500 and min_df ≤ 0.1 (or min_df = 1), the accuracy may reach 96%. In the experiments with K-fold cross-validation, the accuracy of SVMs methods is 96%. The experimental results show that that our prediction model may effectively classify liver cancer articles.",
        "link": "https://dl.acm.org/doi/10.1145/3474944.3474958"
    },
    {
        "title": "Automatic topic classification of test cases using text mining at an Android smartphone vendor",
        "authors": [
            "Junji Shimagaki",
            "Yasutaka Kamei",
            "Naoyasu Ubayashi",
            "Abram Hindle"
        ],
        "date": "October 2018",
        "source": "ESEM '18: Proceedings of the 12th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Background: An Android smartphone is an ecosystem of applications, drivers, operating system components, and assets. The volume of the software is large and the number of test cases needed to cover the functionality of an Android system is substantial. Enormous effort has been already taken to properly quantify \"what features and apps were tested and verified?\". This insight is provided by dashboards that summarize test coverage and results per feature. One method to achieve this is to manually tag or label test cases with the topic or function they cover, much like function points. At the studied Android smartphone vendor, tests are labelled with manually defined tags, so-called \"feature labels (FLs)\", and the FLs serve to categorize 100s to 1000s test cases into 10 to 50 groups. Aim: Unfortunately for developers, manual assignment of FLs to 1000s of test cases is a time consuming task, leading to inaccurately labeled test cases, which will render the dashboard useless. We created an automated system that suggests tags/labels to the developers for their test cases rather than manual labeling. Method: We use machine learning models to predict and label the functionality tested by 10,000 test cases developed at the company. Results: Through the quantitative experiments, our models achieved acceptable F-1 performance of 0.3 to 0.88. Also through the qualitative studies with expert teams, we showed that the hierarchy and path of tests was a good predictor of a feature's label. Conclusions: We find that this method can reduce tedious manual effort that software developers spent classifying test cases, while providing more accurate classification results.",
        "link": "https://dl.acm.org/doi/10.1145/3239235.3268927"
    },
    {
        "title": "To apply Data Mining for Classification of Crowd sourced Software Requirements",
        "authors": [
            "Soonh Taj",
            "Qasim Arain",
            "Imran Memon",
            "Asma Zubedi"
        ],
        "date": "April 2019",
        "source": "ICSIE '19: Proceedings of the 8th International Conference on Software and Information Engineering",
        "abstract": "Now a day's main focus of developers is to build quality software that works according to customer needs and for this reason it is necessary to gather right requirements as requirement elicitation is the critical step that impacts on the success of software project as misinterpreted requirements leads to the failure of software project. By keeping this in mind a research is carried out on improving requirements elicitation process and automating the process of classifying requirements. In this research, a model is proposed which will help in this scenario for requirements elicitation and requirement classification. This paper presents a model in which crowd sourcing approach is used so that customers, end users, stakeholders, developers and software engineers can make active participation for requirement elicitation process and requirements gathered using crowdsourcing approach are used by model for classification process i.e. classification of requirements into functional and non-functional requirements. For the proof of proposed model a case study is conducted. Results of case study provided the usefulness and efficiency of proposed model for classification of crowd sourced software requirements.",
        "link": "https://dl.acm.org/doi/10.1145/3328833.3328837"
    },
    {
        "title": "Text classification using Fuzzy TF-IDF and Machine Learning Models",
        "authors": [
            "Mariem Bounabi",
            "Karim El Moutaouakil",
            "Khalid Satori"
        ],
        "date": "October 2019",
        "source": "BDIoT'19: Proceedings of the 4th International Conference on Big Data and Internet of Things",
        "abstract": "The representation of the information has an important impact on the text classification task. Several weighting methods were proposed in the literature, and the term frequency-inverse term frequency (TFIDF), the most know on the text treatment field. The FTF-IDF is a vector representation where the components of the TFIDF are presented as inputs to the Fuzzy Inference System (FIS). In this work, we compare several Machin Learning algorithms such as Naïve Bayes and its derivatives, SVM and Random forest classifiers, using the FTF-IDF representation. To improve the quality of the used classifiers, we call sum attribute selection methods. The recognition rate, for the tested systems, is satisfied, where the system based on naïve Bayes classifier, the FTF-IDF weighting terms, and the info gain select attributes method gives 98.7% as accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3372938.3372956"
    },
    {
        "title": "Text Mining in Cybersecurity: A Systematic Literature Review",
        "authors": [
            "Luciano Ignaczak",
            "Guilherme Goldschmidt",
            "Cristiano André Da Costa",
            "Rodrigo Da Rosa Righi"
        ],
        "date": null,
        "source": "ACM Computing Surveys",
        "abstract": "The growth of data volume has changed cybersecurity activities, demanding a higher level of automation. In this new cybersecurity landscape, text mining emerged as an alternative to improve the efficiency of the activities involving unstructured data. This article proposes a Systematic Literature Review (SLR) to present the application of text mining in the cybersecurity domain. Using a systematic protocol, we identified 2,196 studies, out of which 83 were summarized. As a contribution, we propose a taxonomy to demonstrate the different activities in the cybersecurity domain supported by text mining. We also detail the strategies evaluated in the application of text mining tasks and the use of neural networks to support activities involving unstructured data. The work also discusses text classification performance aiming its application in real-world solutions. The SLR also highlights open gaps for future research, such as the analysis of non-English content and the intensification in the usage of neural networks.",
        "link": "https://dl.acm.org/doi/10.1145/3462477"
    },
    {
        "title": "Using Text Mining to Analyze the Financial Patents",
        "authors": [
            "Yung-Feng Lu",
            "Jia-Lang Xu",
            "Mu-Yen Chen"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "With the advancement and development of science and technology, Nowadays society is the development of information technology, cloud computing, mobile Internet, Internet of Things, block chain and information technology.The combination of finance and technology has made financial technology terms gradually emerging.. This research analyzes the patents of financial technology. However, Taiwan's financial technology patents are mostly G06Q-20 (payment plan, architecture or agreement), G06Q-30 (commercial, such as marketing, shopping, payment, auction or e-commerce). G06Q-40 (financial, investment or tax treatment, insurance).The research use text mining technology to analyze and understand Taiwan's trends and developments in financial technology. The research results show that G06Q-20 patents are biased towards information security and mobile payment. G06Q-30 patents favor shopping websites and advertising. G06Q-40 patents are biased towards diversity, such as securities, insurance, finance, and stocks.",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326478"
    },
    {
        "title": "Identifying Online Profiles of Distance Learning Students Using Data Mining Techniques",
        "authors": [
            "Osama Islam",
            "Muazzam Siddiqui",
            "Naif Radi Aljohani"
        ],
        "date": "October 2019",
        "source": "ICDTE '19: Proceedings of the 3rd International Conference on Digital Technology in Education",
        "abstract": "Educational data has grown over the years with the increased use of technology within educational environments. This has led to a huge amount of data being stored in various data sources representing the student, his/her activities, and other aspects relevant to the learning process. To meet this analytical need, Educational Data Mining (EDM) has emerged to assist educational institutions in identifying key benefits such as students at risk, the level of student engagement or predicting student performance. The aim of this research was to explore the various aspects of student interaction data using data mining techniques to identify relevant patterns of behaviors and possible key attributes that have higher degrees of influence on distance learning students. The main findings identified several patterns of user online profiles based on a set of adopted learning strategies, the research proposed a framework for analyzing such interaction data based on R and Hadoop platforms to correlate online profiles with student performance.",
        "link": "https://dl.acm.org/doi/10.1145/3369199.3369249"
    },
    {
        "title": "Extremely Fast Decision Tree Mining for Evolving Data Streams",
        "authors": [
            "Albert Bifet",
            "Jiajin Zhang",
            "Wei Fan",
            "Cheng He",
            "Jianfeng Zhang",
            "Jianfeng Qian",
            "Geoff Holmes",
            "Bernhard Pfahringer"
        ],
        "date": "August 2017",
        "source": "KDD '17: Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
        "abstract": "Nowadays real-time industrial applications are generating a huge amount of data continuously every day. To process these large data streams, we need fast and efficient methodologies and systems. A useful feature desired for data scientists and analysts is to have easy to visualize and understand machine learning models. Decision trees are preferred in many real-time applications for this reason, and also, because combined in an ensemble, they are one of the most powerful methods in machine learning. In this paper, we present a new system called STREAMDM-C++, that implements decision trees for data streams in C++, and that has been used extensively at Huawei. Streaming decision trees adapt to changes on streams, a huge advantage since standard decision trees are built using a snapshot of data, and can not evolve over time. STREAMDM-C++ is easy to extend, and contains more powerful ensemble methods, and a more efficient and easy to use adaptive decision trees. We compare our new implementation with VFML, the current state of the art implementation in C, and show how our new system outperforms VFML in speed using less resources.",
        "link": "https://dl.acm.org/doi/10.1145/3097983.3098139"
    },
    {
        "title": "Data miners' little helper: data transformation activity cues for cluster analysis on document collections",
        "authors": [
            "Tania Cerquitelli",
            "Evelina Di Corso",
            "Francesco Ventura",
            "Silvia Chiusano"
        ],
        "date": "June 2017",
        "source": "WIMS '17: Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "In this paper we propose a new self-learning engine to streamline the analytics process, as it enables analysts to mine massive data repositories with minimal user intervention. In the context of cluster analysis on a collection of documents this new system, named SELF-DATA (SELF-learning DAta TrAnsformation), suggests to the analyst how to configure the whole mining process for a given dataset. SELF-DATA relies on an engine exploring different data weighting schemas (e.g., normalized term frequencies) and data transformation methods (e.g., PCA) before applying the cluster analysis, evaluating and comparing solutions through different quality indices (e.g., weighted Silhouette), and presenting the k-top solutions to the analyst. SELF-DATA will also include a knowledge base storing results of experiments on previously processed datasets, and a classification algorithm trained on the knowledge base content to forecast the best configuration for the whole mining process for an unexplored dataset. The first development of SELF-DATA running on Apache Spark has been validated on 5 collections of documents. Experimental results highlight that TF-IDF and logarithmic entropy are effective to measure item relevance with sparse datasets, and the LSI method outperforms PCA with a large dictionary.",
        "link": "https://dl.acm.org/doi/10.1145/3102254.3102288"
    },
    {
        "title": "Interactive Visual Graph Mining and Learning",
        "authors": [
            "Ryan A. Rossi",
            "Nesreen K. Ahmed",
            "Rong Zhou",
            "Hoda Eldardiry"
        ],
        "date": null,
        "source": "ACM Transactions on Intelligent Systems and Technology",
        "abstract": "This article presents a platform for interactive graph mining and relational machine learning called GraphVis. The platform combines interactive visual representations with state-of-the-art graph mining and relational machine learning techniques to aid in revealing important insights quickly as well as learning an appropriate and highly predictive model for a particular task (e.g., classification, link prediction, discovering the roles of nodes, and finding influential nodes). Visual representations and interaction techniques and tools are developed for simple, fast, and intuitive real-time interactive exploration, mining, and modeling of graph data. In particular, we propose techniques for interactive relational learning (e.g., node/link classification), interactive link prediction and weighting, role discovery and community detection, higher-order network analysis (via graphlets, network motifs), among others. GraphVis also allows for the refinement and tuning of graph mining and relational learning methods for specific application domains and constraints via an end-to-end interactive visual analytic pipeline that learns, infers, and provides rapid interactive visualization with immediate feedback at each change/prediction in real-time. Other key aspects include interactive filtering, querying, ranking, manipulating, exporting, as well as tools for dynamic network analysis and visualization, interactive graph generators (including new block model approaches), and a variety of multi-level network analysis techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3200764"
    },
    {
        "title": "Data Mining and Opinion Mining: A Tool in Educational Context",
        "authors": [
            "Myriam Peñafiel",
            "Stefanie Vásquez",
            "Diego Vásquez",
            "Juan Zaldumbide",
            "Sergio Luján-Mora"
        ],
        "date": "July 2018",
        "source": "ICoMS '18: Proceedings of the 2018 1st International Conference on Mathematics and Statistics",
        "abstract": "The use of the web as a universal communication platform generates large volumes of data (Big data), which in many cases, need to be processed so that they can become useful knowledge in face of the sceptics who have doubts about the credibility of such information. The use of web data that comes from educational contexts needs to be addressed, since that large amount of unstructured information is not being valued, losing valuable information that can be used. To solve this problem, we propose the use of data mining techniques such as sentiment analysis to validate the information that comes from the educational platforms. The objective of this research is to propose a methodology that allows the user to apply sentiment analysis in a simple way, because although some researchers have done it, very few do with data in the educational context. The results obtained prove that the proposal can be used in similar cases.",
        "link": "https://dl.acm.org/doi/10.1145/3274250.3274263"
    },
    {
        "title": "Research Hotspots Mining and Visualized Analysis Based on Linking Cluster and K-Core Decomposition",
        "authors": [
            "Ying Cai",
            "Fang Huang",
            "Shuai Wang",
            "Hao Zhang",
            "Chunxiu Du"
        ],
        "date": "May 2018",
        "source": "ICDPA 2018: Proceedings of the International Conference on Data Processing and Applications",
        "abstract": "In this paper, we focus on research hotspots mining with knowledge semantic features on discipline topic word networks, which are extracted from scientific and technical documents. For the topic networks, a novel method of combining linking community clustering with k-core decomposition is proposed to mine the hotspot community with knowledge hierarchy structure. And as a post-processing, the hierarchical community identification is implemented by k-core decomposition in order to discover the internal connection with knowledge semantic hierarchy in the research hotspot. Finally, the community density and visualization method is utilized to analyze the core and sub- problem in the research hotspot community. In experiments, the topic networks for a single discipline and interdisciplinary research hotspot are taken as examples of analysis, and the results show the proposed approach can effectively realize the objective of mining and analyzing the discipline research hotspot on the academic topic word network.",
        "link": "https://dl.acm.org/doi/10.1145/3224207.3224216"
    },
    {
        "title": "The Application of Data Mining Algorithms in the Construction of Travel Recommendation System",
        "authors": [
            "Chao Lou",
            "Zhaonan Mu",
            "Mengzhu Liu"
        ],
        "date": "January 2021",
        "source": "CONF-CDS 2021: The 2nd International Conference on Computing and Data Science",
        "abstract": "Photos shared by users of online social network often associated with tags, texts, geographic and time information. These data are ideal research resource which can be used by researchers to observe people's interests and behaviors. They can also be used to analyze people's travel behaviors and interests because it can reflect people's travel activities and experiences directly. This project aimed to consolidate people into groups based on their travel preferences of travel season, travel time span, scenery type of travel location and then analyze popular travel places for different people groups. The travel preferences were analyzed from extracted data applying PHP language and Naive Bayes classification approach. According to the analyzed data, people were divided into groups. Then, DBSCAN clustering was used to get popular locations based on the different groups. As a result, 80 thousand photo data of 5 cities have been collected from Flickr by Phython. It has been showed that people's preference of season in most of the mined cities was spring and summer and the preference of time span was short. Moreover, mined people has been divided into 16 groups according to their preference and based on the groups popular locations have been calculated by DBSCAN clustering. Theses locations have been provided in a recommendation website which was built in this project. For the future work, mining Flickr data associated with other travel social network sites could be a good way to build a more effective recommendation system.",
        "link": "https://dl.acm.org/doi/10.1145/3448734.3450477"
    },
    {
        "title": "Compare Machine Learning Models in Text Classification Using Steam User Reviews",
        "authors": [
            "Youchen Miao",
            "Zeyu Jin",
            "Yumeng Zhang",
            "Yuchen Chen",
            "Junren Lai"
        ],
        "date": "November 2021",
        "source": "ICSED '21: Proceedings of the 2021 3rd International Conference on Software Engineering and Development",
        "abstract": "Text Classification and Sentiment Analysis of game reviews are viewed as important parts in not only academic fields but also in game studies. In this paper, with more than 400 thousand game reviews on Steam platform, we preprocess the data using different libraries (sklearn, nltk, and spaCy) and use them as inputs to build three sentiment classification models based on different algorithms (Naive Bayes, SVM, and Random Forest). In contrast to previous studies that only focus on different sentiment analysis models, our paper also highlights the use of different APIs to preprocess the data and their corresponding model performance. The results show that no matter which API we choose, Random Forest models always perform the best. However, in terms of training time, Naive Bayes is the fastest. This work can be used to apply grid search for researchers to automatically find the optimum API before conducting sentiment analysis in the future.",
        "link": "https://dl.acm.org/doi/10.1145/3507473.3507480"
    },
    {
        "title": "Lost in Transduction: Transductive Transfer Learning in Text Classification",
        "authors": [
            "Alejandro Moreo",
            "Andrea Esuli",
            "Fabrizio Sebastiani"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Obtaining high-quality labelled data for training a classifier in a new application domain is often costly. Transfer Learning\n(a.k.a. “Inductive Transfer”) tries to alleviate these costs by transferring, to the “target” domain of interest, knowledge available from a different “source” domain. In transfer learning the lack of labelled information from the target domain is compensated by the availability at training time of a set of unlabelled examples from the target distribution. Transductive Transfer Learning denotes the transfer learning setting in which the only set of target documents that we are interested in classifying is known and available at training time. Although this definition is indeed in line with Vapnik’s original definition of “transduction”, current terminology in the field is confused. In this article, we discuss how the term “transduction” has been misused in the transfer learning literature, and propose a clarification consistent with the original characterization of this term given by Vapnik. We go on to observe that the above terminology misuse has brought about misleading experimental comparisons, with inductive transfer learning methods that have been incorrectly compared with transductive transfer learning methods. We then, give empirical evidence that the difference in performance between the inductive version and the transductive version of a transfer learning method can indeed be statistically significant (i.e., that knowing at training time the only data one needs to classify indeed gives an advantage). Our clarification allows a reassessment of the field, and of the relative merits of the major, state-of-the-art algorithms for transfer learning in text classification.",
        "link": "https://dl.acm.org/doi/10.1145/3453146"
    },
    {
        "title": "Machine Learning on Mining Potential Adverse Drug Reactions for Pharmacovigilance",
        "authors": [
            "Ke Jia"
        ],
        "date": "July 2021",
        "source": "DSIT 2021: 2021 4th International Conference on Data Science and Information Technology",
        "abstract": "Drugs are related to the lives and health of patients. Mining and discovering potential adverse drug reactions from a large number of unstructured texts play an important role in drug research and pharmacovigilance. It is a hot research issue in the field of biomedicine in recent years. A large number of scholars have discovered a large amount of information on adverse drug reactions from different data sources through effective machine learning methods. This paper reviews the research progress in recent years to provide a reference for future research.",
        "link": "https://dl.acm.org/doi/10.1145/3478905.3478964"
    },
    {
        "title": "TACAM: Topic And Context Aware Argument Mining",
        "authors": [
            "Michael Fromm",
            "Evgeniy Faerman",
            "Thomas Seidl"
        ],
        "date": "October 2019",
        "source": "WI '19: IEEE/WIC/ACM International Conference on Web Intelligence",
        "abstract": "In this work we address the problem of argument search. The purpose of argument search is the distillation of pro and contra arguments for requested topics from large text corpora. In previous works, the usual approach is to use a standard search engine to extract text parts which are relevant to the given topic and subsequently use an argument recognition algorithm to select arguments from them. The main challenge in the argument recognition task, which is also known as argument mining, is that often sentences containing arguments are structurally similar to purely informative sentences without any stance about the topic. In fact, they only differ semantically. Most approaches use topic or search term information only for the first search step and therefore assume that arguments can be classified independently of a topic. We argue that topic information is crucial for argument mining, since the topic defines the semantic context of an argument. Precisely, we propose different models for the classification of arguments, which take information about a topic of an argument into account. Moreover, to enrich the context of a topic and to let models understand the context of the potential argument better, we integrate information from different external sources such as Knowledge Graphs or pre-trained NLP models. Our evaluation shows that considering topic information, especially in connection with external information, provides a significant performance boost for the argument mining task.",
        "link": "https://dl.acm.org/doi/10.1145/3350546.3352506"
    },
    {
        "title": "Text mining for incoming tasks based on the urgency/importance factors and task classification using machine learning tools",
        "authors": [
            "Yasser Ali Alshehri"
        ],
        "date": "March 2020",
        "source": "ICCDA 2020: Proceedings of the 2020 the 4th International Conference on Compute and Data Analysis",
        "abstract": "In workplaces, there is a massive amount of unstructured data from different sources. In this paper, we present a case study that explains how can through communications between employees, we can help to prioritize tasks requests to increase the efficiency of their works for both technical and non-technical workers. This involves managing daily incoming tasks based on their level of urgency and importance.To allow all workers to utilize the urgency-importance matrix as a time-management tool, we need to automate this tool. The textual content of incoming tasks are analyzed, and metrics related to urgency and importance are extracted. A third factor (i.e., the response variable) is defined based on the two input variables (urgency and importance). Then, machine learning applied to the data to predict the class of incoming tasks based on data outcome desired. We used ordinal regression, neural networks, and decision tree algorithms to predict the four levels of task priority. We measure the performance of all using recalls, precisions, and F-scores. All classifiers perform higher than 89% in terms of all measures.",
        "link": "https://dl.acm.org/doi/10.1145/3388142.3388153"
    },
    {
        "title": "Text mining for malware classification using multivariate all repeated patterns detection",
        "authors": [
            "Konstantinos F. Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2019",
        "source": "ASONAM '19: Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Mobile phones have become nowadays a commodity to the majority of people. Using them, people are able to access the world of Internet and connect with their friends, their colleagues at work or even unknown people with common interests. This proliferation of the mobile devices has also been seen as an opportunity for the cyber criminals to deceive smartphone users and steel their money directly or indirectly, respectively, by accessing their bank accounts through the smartphones or by blackmailing them or selling their private data such as photos, credit card data, etc. to third parties. This is usually achieved by installing malware to smartphones masking their malevolent payload as a legitimate application and advertise it to the users with the hope that mobile users will install it in their devices. Thus, any existing application can easily be modified by integrating a malware and then presented it as a legitimate one. In response to this, scientists have proposed a number of malware detection and classification methods using a variety of techniques. Even though, several of them achieve relatively high precision in malware classification, there is still space for improvement. In this paper, we propose a text mining all repeated pattern detection method which uses the decompiled files of an application in order to classify a suspicious application into one of the known malware families. Based on the experimental results using a real malware dataset, the methodology tries to correctly classify (without any misclassification) all randomly selected malware applications of 3 categories with 3 different families each.",
        "link": "https://dl.acm.org/doi/10.1145/3341161.3350841"
    },
    {
        "title": "Comparative Study between Traditional Machine Learning and Deep Learning Approaches for Text Classification",
        "authors": [
            "Cannannore Nidhi Kamath",
            "Syed Saqib Bukhari",
            "Andreas Dengel"
        ],
        "date": "August 2018",
        "source": "DocEng '18: Proceedings of the ACM Symposium on Document Engineering 2018",
        "abstract": "In this contemporaneous world, it is an obligation for any organization working with documents to end up with the insipid task of classifying truckload of documents, which is the nascent stage of venturing into the realm of information retrieval and data mining. But classification of such humongous documents into multiple classes, calls for a lot of time and labor. Hence a system which could classify these documents with acceptable accuracy would be of an unfathomable help in document engineering. We have created multiple classifiers for document classification and compared their accuracy on raw and processed data. We have garnered data used in a corporate organization as well as publicly available data for comparison. Data is processed by removing the stop-words and stemming is implemented to produce root words. Multiple traditional machine learning techniques like Naive Bayes, Logistic Regression, Support Vector Machine, Random forest Classifier and Multi-Layer Perceptron are used for classification of documents. Classifiers are applied on raw and processed data separately and their accuracy is noted. Along with this, Deep learning technique such as Convolution Neural Network is also used to classify the data and its accuracy is compared with that of traditional machine learning techniques. We are also exploring hierarchical classifiers for classification of classes and subclasses. The system classifies the data faster and with better accuracy than if done manually. The results are discussed in the results and evaluation section.",
        "link": "https://dl.acm.org/doi/10.1145/3209280.3209526"
    },
    {
        "title": "Identification of Overpricing in the Purchase of Medication by the Federal Government of Brazil, Using Text Mining and Clustering Based on Ontology",
        "authors": [
            "Marco Aurelio O. S. Correa",
            "Adriano Galindo Leal"
        ],
        "date": "August 2018",
        "source": "ICCBDC '18: Proceedings of the 2018 2nd International Conference on Cloud and Big Data Computing",
        "abstract": "Increasing the transparency level in his actions and spending is one of the primary duties of the Brazilian Federal Government. The creation of laws that oblige full disclose of all its expenditures through transparency portals enables citizens to supervise all government entities. However, only the dissemination of these data, without a definite standard or the availability of data analysis tools, does not guarantee that the citizen is empowered to play his role. Therefore hence, the objective of this work is to identify overprice in the acquisition of products purchased by the federal government of Brazil using the unstructured data available on the Transparency Portal. The last two-years' worth of purchasing data, available in the Transparency Portal, were extracted, processed and stored. Due to his diverse nature and high volume of data, this study focused only on medicines purchased by the Ministry of Health. Ontology-based text mining and clustering techniques were applied for automatic identification and classification of products. The processing of this information was done through text mining and clustering, based on the ontology registered in another database of the Brazilian government. Because of this work, a consolidated price base per medication was created to allow the identification of distortions in prices practised, facilitating the identification of cases that merit further investigation to unravel fraud to the treasury.",
        "link": "https://dl.acm.org/doi/10.1145/3264560.3264569"
    },
    {
        "title": "Computational Estimation by Scientific Data Mining with Classical Methods to Automate Learning Strategies of Scientists",
        "authors": [
            "Aparna S. Varde"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Experimental results are often plotted as 2-dimensional graphical plots (aka graphs) in scientific domains depicting dependent versus independent variables to aid visual analysis of processes. Repeatedly performing laboratory experiments consumes significant time and resources, motivating the need for computational estimation. The goals are to estimate the graph obtained in an experiment given its input conditions, and to estimate the conditions that would lead to a desired graph. Existing estimation approaches often do not meet accuracy and efficiency needs of targeted applications. We develop a computational estimation approach called AutoDomainMine that integrates clustering and classification over complex scientific data in a framework so as to automate classical learning methods of scientists. Knowledge discovered thereby from a database of existing experiments serves as the basis for estimation. Challenges include preserving domain semantics in clustering, finding matching strategies in classification, striking a good balance between elaboration and conciseness while displaying estimation results based on needs of targeted users, and deriving objective measures to capture subjective user interests. These and other challenges are addressed in this work. The AutoDomainMine approach is used to build a computational estimation system, rigorously evaluated with real data in Materials Science. Our evaluation confirms that AutoDomainMine provides desired accuracy and efficiency in computational estimation. It is extendable to other science and engineering domains as proved by adaptation of its sub-processes within fields such as Bioinformatics and Nanotechnology.",
        "link": "https://dl.acm.org/doi/10.1145/3502736"
    },
    {
        "title": "Where is the road for issue reports classification based on text mining?",
        "authors": [
            "Qiang Fan",
            "Yue Yu",
            "Gang Yin",
            "Tao Wang",
            "Huaimin Wang"
        ],
        "date": "November 2017",
        "source": "ESEM '17: Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Currently, open source projects receive various kinds of issues daily, because of the extreme openness of Issue Tracking System (ITS) in GitHub. ITS is a labor-intensive and time-consuming task of issue categorization for project managers. However, a contributor is only required a short textual abstract to report an issue in GitHub. Thus, most traditional classification approaches based on detailed and structured data (e.g., priority, severity, software version and so on) are difficult to adopt. In this paper, issue classification approaches on a large-scale dataset, including 80 popular projects and over 252,000 issue reports collected from GitHub, were investigated. First, four traditional text-based classification methods and their performances were discussed. Semantic perplexity (i.e., an issues description confuses bug-related sentences with nonbug-related sentences) is a crucial factor that affects the classification performances based on quantitative and qualitative study. Finally, A two-stage classifier framework based on the novel metrics of semantic perplexity of issue reports was designed. Results show that our two-stage classification can significantly improve issue classification performances.",
        "link": "https://dl.acm.org/doi/10.1109/ESEM.2017.19"
    },
    {
        "title": "Feature-based Facebook reviews process model for e-management using data mining",
        "authors": [
            "Anish Kumar Varudharajulu",
            "Yongsheng Ma"
        ],
        "date": "January 2019",
        "source": "IC4E '19: Proceedings of the 10th International Conference on E-Education, E-Business, E-Management and E-Learning",
        "abstract": "The data generated from online communication acts as potential gold mines for discovering knowledge for researchers. A large amount of data is also generated in the form of web documents, emails, blogs, and feedback, etc. Text analytics is being significantly employed to mine important information. Opinion mining is the process of extracting human thoughts and perceptions from unstructured texts. The showstopper for designing an opinion mining system for analyzing reviews arise from the fact that customer reviews are often noisy. These reviews are informally written. In addition, they are subjected to spelling mistakes, grammatical errors, improper punctuation and irrational capitalization. This paper focuses on analyzing the different classification and clustering algorithms aimed at extracting and consolidating opinions of customers from social media sites like Facebook, Twitter and through surveys, at multiple levels of granularity to monitor and measure customer satisfaction. Ours is an automated approach, in which the system aids in the process of knowledge assimilation for knowledge-based building and also performs the analytics. Domain experts ratify the knowledge base and also provide training datasets for the system to intuitively gather more instances for ratification. The system identifies opinion expressions as phrases containing opinion words, opinionated features and also opinion modifiers. These expressions are categorized as positive, negative or neutral. Opinion expressions are identified and categorized using localized linguistic techniques. Opinions can be congregated at any desired level of specificity i.e. feature level or product level, user level or service level, etc. We have developed a system based on this approach, which provides the user with a platform to analyze opinion expressions crawled from a set of pre-defined datasets.",
        "link": "https://dl.acm.org/doi/10.1145/3306500.3306514"
    },
    {
        "title": "Discriminative Topic Mining via Category-Name Guided Text Embedding",
        "authors": [
            "Yu Meng",
            "Jiaxin Huang",
            "Guangyuan Wang",
            "Zihan Wang",
            "Chao Zhang",
            "Yu Zhang",
            "Jiawei Han"
        ],
        "date": "April 2020",
        "source": "WWW '20: Proceedings of The Web Conference 2020",
        "abstract": "Mining a set of meaningful and distinctive topics automatically from massive text corpora has broad applications. Existing topic models, however, typically work in a purely unsupervised way, which often generate topics that do not fit users’ particular needs and yield suboptimal performance on downstream tasks. We propose a new task, discriminative topic mining, which leverages a set of user-provided category names to mine discriminative topics from text corpora. This new task not only helps a user understand clearly and distinctively the topics he/she is most interested in, but also benefits directly keyword-driven classification tasks. We develop CatE, a novel category-name guided text embedding method for discriminative topic mining, which effectively leverages minimal user guidance to learn a discriminative embedding space and discover category representative terms in an iterative manner. We conduct a comprehensive set of experiments to show that CatE mines high-quality set of topics guided by category names only, and benefits a variety of downstream applications including weakly-supervised classification and lexical entailment direction identification.",
        "link": "https://dl.acm.org/doi/10.1145/3366423.3380278"
    },
    {
        "title": "Comparative Study of Heart Disease Diagnosis Using Top Ten Data Mining Classification Algorithms",
        "authors": [
            "I. Ketut Agung Enriko"
        ],
        "date": "June 2019",
        "source": "ICFET '19: Proceedings of the 5th International Conference on Frontiers of Educational Technologies",
        "abstract": "Data mining has been used for many purposes, especially for prediction system. In healthcare, data mining algorithms often used in disease diagnosis. Meanwhile, heart disease is known as a primary cause of death over the years. Many studies have been performed in heart disease diagnosis using data mining methods. There are some popular data mining algorithms that can be used in heart disease diagnosis, for example, k-Nearest Neighbor, CART, and AdaBoost. The algorithms are used to analyze a sample of cardiovascular patients data and predict the heart disease type that they suffer. Some parameters are taken from the patient, including EKG morphology, blood pressure, and information about the existence of chest pain, shortness of breath, palpitation, and cold sweat. In this study, medical records data are collected from Harapan Kita Hospital and utilized as a dataset sample in this research. Top ten data mining classification algorithms are used in diagnosing heart disease from Harapan Kita Hospital data and examining their performance by checking the accuracy and speed.",
        "link": "https://dl.acm.org/doi/10.1145/3338188.3338220"
    },
    {
        "title": "Text Mining Approach for Identifying Research Trends",
        "authors": [
            "Snezhana Sulova"
        ],
        "date": "June 2021",
        "source": "CompSysTech '21: Proceedings of the 22nd International Conference on Computer Systems and Technologies",
        "abstract": "With the increase of unstructured data, the issues connected with automatic text processing, the categorization of documents and the discovery of topics have become objects of growing interest. In order to improve the process of grouping and processing research publications, we would like to propose a method based upon natural language processing. It is based on text mining technologies which aim to identify key tendencies in documents. It processes the content of publications by clustering and identifies the topics of each identified group. This analysis helps by identifying key tendencies as well as discovering emerging new areas of research. Publications from the research literature database, Scopus, were used to test the approach. The topic of the publications is “the application of digital technologies in the logistics business”. The experiments were completed using the RapidMiner Studio software.",
        "link": "https://dl.acm.org/doi/10.1145/3472410.3472433"
    },
    {
        "title": "Assessment of Vulnerability Severity using Text Mining",
        "authors": [
            "Georgios Spanos",
            "Lefteris Angelis",
            "Dimitrios Toloudis"
        ],
        "date": "September 2017",
        "source": "PCI '17: Proceedings of the 21st Pan-Hellenic Conference on Informatics",
        "abstract": "Software1 vulnerabilities are closely associated with information systems security, a major and critical field in today's technology. Vulnerabilities constitute a constant and increasing threat for various aspects of everyday life, especially for safety and economy, since the social impact from the problems that they cause is complicated and often unpredictable. Although there is an entire research branch in software engineering that deals with the identification and elimination of vulnerabilities, the growing complexity of software products and the variability of software production procedures are factors contributing to the ongoing occurrence of vulnerabilities, Hence, another area that is being developed in parallel focuses on the study and management of the vulnerabilities that have already been reported and registered in databases. The information contained in such databases includes, a textual description and a number of metrics related to vulnerabilities. The purpose of this paper is to investigate to what extend the assessment of the vulnerability severity can be inferred directly from the corresponding textual description, or in other words, to examine the informative power of the description with respect to the vulnerability severity. For this purpose, text mining techniques, i.e. text analysis and three different classification methods (decision trees, neural networks and support vector machines) were employed. The application of text mining to a sample of 70,678 vulnerabilities from a public data source shows that the description itself is a reliable and highly accurate source of information for vulnerability prioritization.",
        "link": "https://dl.acm.org/doi/10.1145/3139367.3139390"
    },
    {
        "title": "Clustering Algorithms for Spatial Data Mining",
        "authors": [
            "Chetashri Bhadane",
            "Ketan Shah"
        ],
        "date": "April 2020",
        "source": "ICGDA '20: Proceedings of the 2020 3rd International Conference on Geoinformatics and Data Analysis",
        "abstract": "With the advances in mobile and wireless technologies, there has been a rise in applications that track and share the users' geospatial data. People use several social networking sites such as Twitter, Facebook and Flickr, where they share their status updates. With the integration of Global Positioning System (GPS) with mobile phones, it is now possible to share one's locations on these social networks. GPS allows us to record and track a person's movement along with the timestamp. The data set obtained from these GPS logs is vast and is widely used to analyze the users' movement patterns. Specifically, we can find out significant locations based on the number of users present at that location and the time spent by them at such places. Once significant places have been identified, it is also possible to identify the semantic importance of these locations. This paper presents an overview of the clustering techniques used to find important places of interest using large GPS based mobility datasets. Four clustering algorithms, K-Means, DBSCAN, OPTICS and Hierarchical, are implemented, and performance is tested using real-time data of 50 users collected over 2--5 years. Performance summary depicts that K-Means and DBSCAN perform well for spatial data.",
        "link": "https://dl.acm.org/doi/10.1145/3397056.3397068"
    },
    {
        "title": "Feature Selection based Arabic Text Classification using Different Machine Learning Algorithms: Comparative Study",
        "authors": [
            "Sakina Rim Bennabi",
            "Zakaria Elberrichi"
        ],
        "date": "June 2020",
        "source": "ICIST '20: Proceedings of the 10th International Conference on Information Systems and Technologies",
        "abstract": "Feature selection is a method of data pre-processing widely used when mining large data, such as textual classification. Several studies have been conducted to compare the different methods of feature selection applied to corpora in English. Unfortunately, a small number of works concern the Arabic language. This article aims to present a comparative study of different feature selection techniques including: Chi2, the ANOVA method and mutual information, applied on a corpus in Arabic language, while also diversifying the machine learning algorithms (Naive Bayes, SVM and KNN). This experimental study has shown in general that reducing dimensionality with feature selection techniques has slightly affected the performance of textual classification, reducing the size of the corpus by up to 1%.",
        "link": "https://dl.acm.org/doi/10.1145/3447568.3448531"
    },
    {
        "title": "Intelligent Data Analysis using Optimized Support Vector Machine Based Data Mining Approach for Tourism Industry",
        "authors": [
            "Ms Promila Sharma",
            "Uma Meena",
            "Girish Kumar Sharma"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Data analysis involves the deployment of sophisticated approaches from data mining methods, information theory, and artificial intelligence in various fields like tourism, hospitality, and so on for the extraction of knowledge from the gathered and preprocessed data. In tourism, pattern analysis or data analysis using classification is significant for finding the patterns that represent new and potentially useful information or knowledge about the destination and other data. Several data mining techniques are introduced for the classification of data or patterns. However, overfitting, less accuracy, local minima, sensitive to noise are the drawbacks in some existing data mining classification methods. To overcome these challenges, Support vector machine with Red deer optimization (SVM-RDO) based data mining strategy is proposed in this article. Extended Kalman filter (EKF) is utilized in the first phase, i.e., data cleaning to remove the noise and missing values from the input data. Mantaray foraging algorithm (MaFA) is used in the data selection phase, in which the significant data are selected for the further process to reduce the computational complexity. The final phase is the classification, in which SVM-RDO is proposed to access the useful pattern from the selected data. PYTHON is the implementation tool used for the experiment of the proposed model. The experimental analysis is done to show the efficacy of the proposed work. From the experimental results, the proposed SVM-RDO achieved better accuracy, precision, recall, and F1 score than the existing methods for the tourism dataset. Thus, it is showed the effectiveness of the proposed SVM-RDO for pattern analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3494566"
    },
    {
        "title": "Applying text mining to predict learners' cognitive engagement",
        "authors": [
            "Hayati Hind",
            "Mohammed Khalidi Idrissi",
            "Samir Bennani"
        ],
        "date": "October 2017",
        "source": "SCAMS '17: Proceedings of the Mediterranean Symposium on Smart City Application",
        "abstract": "In academic process, engagement represent one of the important success' factor. This paper proposes a predictive system for learners' cognitive engagement based on their online discussion forums participation. In the first level the ontology OWL and the LSA method are used to perform a semantic classification system of the threads according to a specific context chosen by the tutor. Then the Text mining, as a predictive method, is applied to the classified threads and learners' participation in the forums.",
        "link": "https://dl.acm.org/doi/10.1145/3175628.3175655"
    },
    {
        "title": "Investigate Transitions into Drug Addiction through Text Mining of Reddit Data",
        "authors": [
            "John Lu",
            "Sumati Sridhar",
            "Ritika Pandey",
            "Mohammad Al Hasan",
            "Georege Mohler"
        ],
        "date": "July 2019",
        "source": "KDD '19: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Increasing rates of opioid drug abuse and heightened prevalence of online support communities underscore the necessity of employing data mining techniques to better understand drug addiction using these rapidly developing online resources. In this work, we obtained data from Reddit, an online collection of forums, to gather insight into drug use/misuse using text snippets from users narratives. Specifically, using users' posts, we trained a binary classifier which predicts a user's transitions from casual drug discussion forums to drug recovery forums. We also proposed a Cox regression model that outputs likelihoods of such transitions. In doing so, we found that utterances of select drugs and certain linguistic features contained in one's posts can help predict these transitions. Using unfiltered drug-related posts, our research delineates drugs that are associated with higher rates of transitions from recreational drug discussion to support/recovery discussion, offers insight into modern drug culture, and provides tools with potential applications in combating the opioid crisis.",
        "link": "https://dl.acm.org/doi/10.1145/3292500.3330737"
    },
    {
        "title": "Analysing digital banking reviews using text mining",
        "authors": [
            "Li Chen Cheng",
            "Legaspi Rhea Sharmayne"
        ],
        "date": "December 2020",
        "source": "ASONAM '20: Proceedings of the 12th IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Digital banks are new entrants in the banking industry in the Philippines as they only started late 2018. Since then, a handful of players have and are still emerging. With more and more people becoming technologically savvy, it is very critical for financial institutions to develop a digital banking application that will stand out from the competition. This paper aims to use text mining methods to analyse digital banking application reviews. This study will perform topic modelling using LDA to explore customer concerns and will mine association rules between the digital banking features with the review score. The results will reveal which areas the digital banking application can further optimize for customer satisfaction and retention.",
        "link": "https://dl.acm.org/doi/10.1109/ASONAM49781.2020.9381429"
    },
    {
        "title": "Mining Moocs Videos Metadata Using Classification Techniques",
        "authors": [
            "El Harrak Othman",
            "Ghadi Abderrahim",
            "El Bouhdidi Jaber"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "In few years, the internet has become the first source of information for most people, Today MOOCs makes it possible for everyone to access to the education over the world, it's represents an emerging methodology of online teaching and an important development in open education. Due to the rapid development of Moocs and the rapid growth of digital data and video database over the Internet, it is becoming very difficult for learners to browsing and choosing the best training for them. The scientific community has increased the amount of research into new technologies, with a view to improve the digital video utilization: its archiving, indexing, accessibility, acquisition, store and even its process and usability. All these parts of the video utilization entail the necessity of the extraction of all important information of a video, especially in cases of lack of metadata information. Web video mining is retrieving the content using data mining techniques from World Wide Web. There are two approaches for web video mining using traditional image processing (signal processing) and metadata based approach. In this paper, we present the various research makes in this subject and we propose an effective methodology to extract the metadata from Moocs videos and classify them based on the extracted metadata by applying data mining techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090450"
    },
    {
        "title": "Text mining with HathiTrust",
        "authors": [
            "Eleanor Dickson Koehl",
            "Ryan Dubnicek"
        ],
        "date": "June 2019",
        "source": "JCDL '19: Proceedings of the 18th Joint Conference on Digital Libraries",
        "abstract": "This tutorial will introduce attendees to the HathiTrust Research Center and its tools and services for computational text analysis research. HTRC leverages the scope and scale of the HathiTrust Digital Library collection to create opportunities for researchers to perform text data mining on subsets of the corpus. Attendees will develop skills that will allow them to conduct basic text analysis research using HathiTrust data, learn techniques for engaging with scholars on their text data mining research, and explore strategies for computational access to large-scale digital collections.",
        "link": "https://dl.acm.org/doi/10.1109/JCDL.2019.00115"
    },
    {
        "title": "Text mining for plagiarism detection: multivariate pattern detection for recognition of text similarities",
        "authors": [
            "Konstantinos Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2018",
        "source": "ASONAM '18: Proceedings of the 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "The problem of plagiarism the recent years has been intensified by the availability of information in digital form and the accessibility of the electronic libraries through the Internet. As a result, plagiarism detection has been transformed into a big data analytics problem since the number of digital sources is extravagant and a new document needs to be compared with millions of other existing documents. In this paper, a text mining methodology is proposed that can detect all common patterns between a document and the documents in a reference database. The technique is based on a pattern detection algorithm and the corresponding data structure that enables the algorithm to detect all common patterns. The methodology has been applied in a well-defined dataset providing very promising results identifying difficult cases of plagiarism such as technical disguise.",
        "link": "https://dl.acm.org/doi/10.5555/3382225.3382424"
    },
    {
        "title": "Petroleum Engineering Data Text Classification Using Convolutional Neural Network Based Classifier",
        "authors": [
            "Tzuhan Hsu",
            "Yaoqin Zhang"
        ],
        "date": "May 2018",
        "source": "ICMLT '18: Proceedings of the 2018 International Conference on Machine Learning Technologies",
        "abstract": "In this paper, authors introduce a convolutional neural network (CNN) based classification approach for petroleum engineering event. Recently, text classification problem has raised substantial attention in the field of machine learning. The aim of text classification process is to assign correct label to a text on the basis of its content. However, due to the imprecise of feature representation, most of former text classification methods cannot export satisfying result. To solve this problem, authors propose a model based on convolutional neural network. Authors conduct experiments with real world petroleum engineering data text, artificially labelled into 6 categories. Beside purposed CNN-based model, the authors also implement other baseline machine learning approaches: k nearest neighbor, random forest, support vector machines, recurrent neural network, gated recurrent units, long short-term memory. Among all these models, our CNN-based model reach best performance in text classification task. In the last part of this paper, authors compare and analysis their purpose and baseline model.",
        "link": "https://dl.acm.org/doi/10.1145/3231884.3231898"
    },
    {
        "title": "Classifying Indonesian Online Articles as Advertisement Placement Base Using Text Mining",
        "authors": [
            "Nadhira Tasya",
            "Arian Dhini"
        ],
        "date": "July 2017",
        "source": "ICBIM 2017: Proceedings of the International Conference on Business and Information Management",
        "abstract": "Rapid development in technological aspect resulting in growing level of human needs for the latest news, so that emerged a new trend of publishing and accessing news through online media or usually called online journalism. In addition, the number of people who sell and purchase through online sites also continues to increase and this opportunity is utilized by the company and the advertiser by implementing targeted web advertising. However, the high number of articles that have been published and accessed leads to great opportunities for errors in determining where to place the ads. Therefore, it needs a system that can categorize articles accessed by users as the basis of advertisement placement by the company and this classification system can be done by applying the method of Data Mining and Text Mining. This research uses document data in the form of article content that will be categorized into twenty categories of class of advertisement by using Text Mining technique with Support Vector Machine algorithm. The results of this study may be used by companies or advertisers as a basis for placement of ads on selected online media sites.",
        "link": "https://dl.acm.org/doi/10.1145/3134271.3134288"
    },
    {
        "title": "Mining Free-Text Medical Notes for Suicide Risk Assessment",
        "authors": [
            "Marios Adamou",
            "Grigoris Antoniou",
            "Elissavet Greasidou",
            "Vincenzo Lagani",
            "Paulos Charonyktakis",
            "Ioannis Tsamardinos"
        ],
        "date": "July 2018",
        "source": "SETN '18: Proceedings of the 10th Hellenic Conference on Artificial Intelligence",
        "abstract": "Suicide has been considered as an important public health issue for a very long time, and is one of the main causes of death worldwide. Despite suicide prevention strategies being applied, the rate of suicide has not changed substantially over the past decades. Advances in machine learning make it possible to attempt to predict suicide based on the analysis of relevant data to inform clinical practice. This paper reports on findings from the analysis of data of patients who died by suicide in the period 2013-2016 and made use of both structured data and free-text medical notes. We focus on examining various text-mining approaches to support risk assessment. The results show that using advance machine learning and text-mining techniques, it is possible to predict within a specified period which people are most at risk of taking their own life at the time of referral to a mental health service.",
        "link": "https://dl.acm.org/doi/10.1145/3200947.3201020"
    },
    {
        "title": "Integration of data mining techniques in e-learning systems: Clustering Profil of Lerners and Recommender Course System",
        "authors": [
            "A. El Moustamid",
            "E. En-Naimi",
            "J. El Bouhdidi"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "Data mining aims at the extraction of knowledge from large quantities of data by automatic or semi-automatic methods. In the field of education and training e-learning, Data Mining techniques as clustering (The aim of this technique is to identify groups that are similar in some aspect) [21], prediction (is used for developing model where combination of data viewed from different aspects can effect single viewed data. Here, is important to watch how predictors variables are effecting predicted variable)[21], classification are applied in e-learning to improve the level of learners by providing learning according to the learners' requirement) are used to analyze learners profile, to predict and improve student performance. The aim of our project is to develop a system capable of analyzing learners profile and indexing web videos in order to offer learners a rich database with courses that correspond to their levels.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090453"
    },
    {
        "title": "Multi-level mining and visualization of scientific text collections: Exploring a bi-lingual scientific repository",
        "authors": [
            "Pablo Accuosto",
            "Francesco Ronzano",
            "Daniel Ferrés",
            "Horacio Saggion"
        ],
        "date": "December 2017",
        "source": "WOSP 2017: Proceedings of the 6th International Workshop on Mining Scientific Publications",
        "abstract": "We present a system to mine and visualize collections of scientific documents by semantically browsing information extracted from single publications or aggregated throughout corpora of articles. The text mining tool performs deep analysis of document collections allowing the extraction and interpretation of research paper's contents. In addition to the extraction and enrichment of documents with metadata (titles, authors, affiliations, etc), the deep analysis performed comprises semantic interpretation, rhetorical analysis of sentences, triple-based information extraction, and text summarization. The visualization components allow geographical-based exploration of collections, topic-evolution interpretation, and collaborative network analysis among others. The paper presents a case study of a bi-lingual collection in the field of Natural Language Processing (NLP).",
        "link": "https://dl.acm.org/doi/10.1145/3127526.3127529"
    },
    {
        "title": "Hierarchical Topic Mining via Joint Spherical Tree and Text Embedding",
        "authors": [
            "Yu Meng",
            "Yunyi Zhang",
            "Jiaxin Huang",
            "Yu Zhang",
            "Chao Zhang",
            "Jiawei Han"
        ],
        "date": "August 2020",
        "source": "KDD '20: Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Mining a set of meaningful topics organized into a hierarchy is intuitively appealing since topic correlations are ubiquitous in massive text corpora. To account for potential hierarchical topic structures, hierarchical topic models generalize flat topic models by incorporating latent topic hierarchies into their generative modeling process. However, due to their purely unsupervised nature, the learned topic hierarchy often deviates from users' particular needs or interests. To guide the hierarchical topic discovery process with minimal user supervision, we propose a new task, Hierarchical Topic Mining, which takes a category tree described by category names only, and aims to mine a set of representative terms for each category from a text corpus to help a user comprehend his/her interested topics. We develop a novel joint tree and text embedding method along with a principled optimization procedure that allows simultaneous modeling of the category tree structure and the corpus generative process in the spherical space for effective category-representative term discovery. Our comprehensive experiments show that our model, named JoSH, mines a high-quality set of hierarchical topics with high efficiency and benefits weakly-supervised hierarchical text classification tasks.",
        "link": "https://dl.acm.org/doi/10.1145/3394486.3403242"
    },
    {
        "title": "Unsupervised behavioural mining and clustering for malware family identification",
        "authors": [
            "Khanh Huu The Dam",
            "Thomas Given-Wilson",
            "Axel Legay"
        ],
        "date": "March 2021",
        "source": "SAC '21: Proceedings of the 36th Annual ACM Symposium on Applied Computing",
        "abstract": "More accurate and advanced detection and classification of malware requires exploiting program behaviour and not merely syntactic or static features. One approach is to use system call dependency graphs (SCDGs) that represent the program behaviour by interactions with the system, and the relations between these interactions. These SCDGs have been used with supervised learning techniques to very accurately detect and classify malware. This works considers the unsupervised learning challenge of mining for common clusters of behaviour without a priori knowledge. This allows for clustering of similar programs by behaviour, that can then be used for either classification or further analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3412841.3441919"
    },
    {
        "title": "Text Mining for Evaluating Authors' Birth and Death Years",
        "authors": [
            "Dror Moghaz",
            "Yaakov Hacohen-Kerner",
            "Dov Gabbay"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "This article presents a unique method in text and data mining for finding the era, i.e., mining temporal data, in which an anonymous author was living. Finding this era can assist in the examination of a fake document or extracting the time period in which a writer lived. The study and the experiments concern Hebrew, and in some parts, Aramaic and Yiddish rabbinic texts. The rabbinic texts are undated and contain no bibliographic sections, posing an interesting challenge. This work proposes algorithms using key phrases and key words that allow the temporal organization of citations together with linguistic patterns. Based on these key phrases, key words, and the references, we established several types of “Iron-clad,” Heuristic and Greedy rules for estimating the years of birth and death of a writer in an interesting classification task. Experiments were conducted on corpora, including documents authored by 12, 24, and 36 rabbinic writers and demonstrated promising results.",
        "link": "https://dl.acm.org/doi/10.1145/3281631"
    },
    {
        "title": "Transfer Learning to Timed Text Based Video Classification Using CNN",
        "authors": [
            "Zenun Kastrati",
            "Ali Shariq Imran",
            "Arianit Kurti"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "Open educational video resources are gaining popularity with a growing number of massive open online courses (MOOCs). This has created a niche for content providers to adopt effective solutions in automatically organizing and structuring of educational resources for maximum visibility. Recent advances in deep learning techniques are proving useful in managing and classifying resources into appropriate categories. This paper proposes one such convolutional neural network (CNN) model for classifying video lectures in a MOOC setting using a transfer learning approach. The model uses a time-aligned text transcripts corresponding to video lectures from six broader subject categories. Video lectures and their corresponding transcript dataset is gathered from the Coursera MOOC platform. Two different CNN models are proposed: i) CNN based classification using embeddings learned from our MOOC dataset, ii) CNN based classification using transfer learning. Word embeddings generated from two well known state-of-the-art pre-trained models Word2Vec and GloVe, are used in the transfer learning approach for the second case. The proposed CNN models are evaluated using precision, recall, and F1 score and the obtained performance is compared with both conventional and deep learning classifiers. The proposed CNN models have an F1 score improvement of 10-22 percentage points over DNN and conventional classifiers",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326483"
    },
    {
        "title": "Evolutionary role mining in complex networks by ensemble clustering",
        "authors": [
            "Sarvenaz Choobdar",
            "Pedro Ribeiro",
            "Fernando Silva"
        ],
        "date": "April 2017",
        "source": "SAC '17: Proceedings of the Symposium on Applied Computing",
        "abstract": "The structural patterns in the neighborhood of nodes assign unique roles to the nodes. Mining the set of existing roles in a network provides a descriptive profile of the network and draws its general picture. This paper proposes a new method to determine structural roles in a dynamic network based on the current position of nodes and their historic behavior. We develop a temporal ensemble clustering technique to dynamically find groups of nodes, holding similar tempo-structural roles. We compare two weighting functions, based on age and distribution of data, to incorporate temporal behavior of nodes in the role discovery. To evaluate the performance of the proposed method, we assess the results from two points of view: 1) goodness of fit to current structure of the network; 2) consistency with historic data. We conduct the evaluation using different ensemble clustering techniques. The results on real world networks demonstrate that our method can detect tempo-structural roles that simultaneously depict the topology of a network and reflect its dynamics with high accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3019612.3019815"
    },
    {
        "title": "Analyzing used-car web listings via text mining",
        "authors": [
            "Ayhan Demiriz",
            "Fatma Cantaş"
        ],
        "date": "October 2017",
        "source": "IML '17: Proceedings of the 1st International Conference on Internet of Things and Machine Learning",
        "abstract": "Used car trade is one of the major components of the world economies. It is not uncommon to sell a car by placing an internet advertisement irrespective of the geography in these days. A typical content of an advertisement is usually composed of two parts namely the structured and the free text data. The structured data may include some information about the asking price, make, model, year, mileage of the car and the contact info. In most cases, seller may give important clues about the car's current conditions in the free text data where the title (head) of the advertisement can be included as free text too. This paper reports preliminary results from a text mining study conducted on 75K used car internet listings collected from two major car listing web sites in Turkey. As expected, the words and the phrases related to the description of the car are observed to be frequent. The leading concepts in the free text are found to be regarding how to describe the current condition of a car, for example \"no crash history\".",
        "link": "https://dl.acm.org/doi/10.1145/3109761.3109782"
    },
    {
        "title": "Text classification of Diseases Treated by Traditional Chinese Medicine Prescription based on machine learning",
        "authors": [
            "Hanqing Zhao",
            "Bo Han",
            "Chen Li"
        ],
        "date": "September 2020",
        "source": "ISAIMS '20: Proceedings of the 1st International Symposium on Artificial Intelligence in Medical Sciences",
        "abstract": "OBJECTIVE: To explore the application of machine learning in th e identification of diseases treated by traditional Chinese medicine prescriptions. METHODS: Based on the composition of the text document of Chinese medicine prescriptions, the prescriptions were divided into cough, headache and diarrhea. THUTCT were introduced to establish and train two machine learning text classification models, LibSVM and LibLinear, and the prescriptions to be deter mined were put into the model for classification and prediction. R ESULTS: The Precision rate of LibSVM model and Liblinear mod el were 0.7283 and 0.6690. Seven prescriptions were classified an d predicted, and the results were in line with expectations. CONCLUSION: THUCTC has good universality for the content of TCM prescriptions, high classification accuracy and fast testing speed, which is suitable for the text classification and discrimination research of TCM prescriptions for diseases.",
        "link": "https://dl.acm.org/doi/10.1145/3429889.3429895"
    },
    {
        "title": "Evaluation of retweet clustering method classification method using retweets on Twitter without text data",
        "authors": [
            "K. Uchida",
            "F. Toriumi",
            "T. Sakaki"
        ],
        "date": "August 2017",
        "source": "WI '17: Proceedings of the International Conference on Web Intelligence",
        "abstract": "Burst phenomena, which frequently occur on social media, are caused by such social events as flaming on the internet, elections, and natural disasters. To understand people's thoughts and feelings, we must classify their opinions from burst phenomena. Therefore, classification methods that categorize tweets are critical. However, since most classification methods focus on text mining, they cannot group tweets by topics because each tweet has poor linguistic similarities. We used a non-text-based classification method proposed by Baba et al. that groups tweets by topics, even if they have poor linguistic similarities, and verified its validity by comparing it with a text-based classification method in two different evaluations: qualitative and quantitative. In the qualitative evaluation part, we did a questionnaire survey and validated the suitability of the topic clusters created using both the non-and text-based methods. Since evaluating the similarity of every pair of tweets in each topic is difficult, we evaluated the similarity between sampled pairs in the survey and acquired more appropriate topic clustering results using the non-text-based method than the text-based method. In the quantitative evaluation part, we focused on the robustness of each method against data reduction. Many approaches analyze social media data, especially because collecting data from social media is comparatively easy. However, since collecting the whole data of burst phenomena is very costly due to the vast amounts of available social media data, robustness against data reduction is an important index to evaluate classification methods. With the non-text-based method, over 55% of the pairs of tweets in the same cluster were also included in the same cluster even when the data were reduced to 10% in all three of our example cases. In this paper, as a source we focus on Twitter, one of the most popular microblogging services. Using clustering to conduct detailed case analyses, we scrutinized three burst cases that include natural disasters and flaming on the internet and found that a non-text-based method more effectively classified tweets in burst phenomena than a text-based method.",
        "link": "https://dl.acm.org/doi/10.1145/3106426.3106451"
    },
    {
        "title": "Using Spark for Text Mining on Large Scale Liver Cancer Literature",
        "authors": [
            "Ming-Yen Lin",
            "Yu-Ju Lin",
            "Sue-Chen Hsueh"
        ],
        "date": "January 2021",
        "source": "BDET 2021: 2021 the 3rd International Conference on Big Data Engineering and Technology (BDET)",
        "abstract": "Cancer is one of the main causes of death. The number of known cancer to-date is more than one hundred. Liver cancer, ranked after trachea and lung cancer, has long been high in the cancer leading cause in Taiwan and even ranked as second in 2016. The number of scientific articles related to cancer proliferates every year. The number reaches as high as 20 million in PubMed so that discovering useful information from the massive collection is very difficult. In addition, using a single machine to sift through these articles is very time-consuming. Therefore, we present a big data analytic framework using the distributed Apache Spark platform for text mining in PubMed literature. We establish a prediction model for liver cancer articles so that researchers may effectively validate whether an article is related to liver cancer or not. Classification models in Spark MLlib including Linear Support Vector Machines (SVM), Logistic Regression, are used in our experiments. Relevancy to liver cancer is further confirmed by using MeSH (Medical Subject Headings) terms. Logistic regression is about 3 times faster than SVMs and the accuracy of both methods is close to 95% in the experiments using hold-out validation. When max_features is 500 and min_df ≤ 0.1 (or min_df = 1), the accuracy may reach 96%. In the experiments with K-fold cross-validation, the accuracy of SVMs methods is 96%. The experimental results show that that our prediction model may effectively classify liver cancer articles.",
        "link": "https://dl.acm.org/doi/10.1145/3474944.3474958"
    },
    {
        "title": "Automatic topic classification of test cases using text mining at an Android smartphone vendor",
        "authors": [
            "Junji Shimagaki",
            "Yasutaka Kamei",
            "Naoyasu Ubayashi",
            "Abram Hindle"
        ],
        "date": "October 2018",
        "source": "ESEM '18: Proceedings of the 12th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Background: An Android smartphone is an ecosystem of applications, drivers, operating system components, and assets. The volume of the software is large and the number of test cases needed to cover the functionality of an Android system is substantial. Enormous effort has been already taken to properly quantify \"what features and apps were tested and verified?\". This insight is provided by dashboards that summarize test coverage and results per feature. One method to achieve this is to manually tag or label test cases with the topic or function they cover, much like function points. At the studied Android smartphone vendor, tests are labelled with manually defined tags, so-called \"feature labels (FLs)\", and the FLs serve to categorize 100s to 1000s test cases into 10 to 50 groups. Aim: Unfortunately for developers, manual assignment of FLs to 1000s of test cases is a time consuming task, leading to inaccurately labeled test cases, which will render the dashboard useless. We created an automated system that suggests tags/labels to the developers for their test cases rather than manual labeling. Method: We use machine learning models to predict and label the functionality tested by 10,000 test cases developed at the company. Results: Through the quantitative experiments, our models achieved acceptable F-1 performance of 0.3 to 0.88. Also through the qualitative studies with expert teams, we showed that the hierarchy and path of tests was a good predictor of a feature's label. Conclusions: We find that this method can reduce tedious manual effort that software developers spent classifying test cases, while providing more accurate classification results.",
        "link": "https://dl.acm.org/doi/10.1145/3239235.3268927"
    },
    {
        "title": "To apply Data Mining for Classification of Crowd sourced Software Requirements",
        "authors": [
            "Soonh Taj",
            "Qasim Arain",
            "Imran Memon",
            "Asma Zubedi"
        ],
        "date": "April 2019",
        "source": "ICSIE '19: Proceedings of the 8th International Conference on Software and Information Engineering",
        "abstract": "Now a day's main focus of developers is to build quality software that works according to customer needs and for this reason it is necessary to gather right requirements as requirement elicitation is the critical step that impacts on the success of software project as misinterpreted requirements leads to the failure of software project. By keeping this in mind a research is carried out on improving requirements elicitation process and automating the process of classifying requirements. In this research, a model is proposed which will help in this scenario for requirements elicitation and requirement classification. This paper presents a model in which crowd sourcing approach is used so that customers, end users, stakeholders, developers and software engineers can make active participation for requirement elicitation process and requirements gathered using crowdsourcing approach are used by model for classification process i.e. classification of requirements into functional and non-functional requirements. For the proof of proposed model a case study is conducted. Results of case study provided the usefulness and efficiency of proposed model for classification of crowd sourced software requirements.",
        "link": "https://dl.acm.org/doi/10.1145/3328833.3328837"
    },
    {
        "title": "Text classification using Fuzzy TF-IDF and Machine Learning Models",
        "authors": [
            "Mariem Bounabi",
            "Karim El Moutaouakil",
            "Khalid Satori"
        ],
        "date": "October 2019",
        "source": "BDIoT'19: Proceedings of the 4th International Conference on Big Data and Internet of Things",
        "abstract": "The representation of the information has an important impact on the text classification task. Several weighting methods were proposed in the literature, and the term frequency-inverse term frequency (TFIDF), the most know on the text treatment field. The FTF-IDF is a vector representation where the components of the TFIDF are presented as inputs to the Fuzzy Inference System (FIS). In this work, we compare several Machin Learning algorithms such as Naïve Bayes and its derivatives, SVM and Random forest classifiers, using the FTF-IDF representation. To improve the quality of the used classifiers, we call sum attribute selection methods. The recognition rate, for the tested systems, is satisfied, where the system based on naïve Bayes classifier, the FTF-IDF weighting terms, and the info gain select attributes method gives 98.7% as accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3372938.3372956"
    },
    {
        "title": "Text Mining in Cybersecurity: A Systematic Literature Review",
        "authors": [
            "Luciano Ignaczak",
            "Guilherme Goldschmidt",
            "Cristiano André Da Costa",
            "Rodrigo Da Rosa Righi"
        ],
        "date": null,
        "source": "ACM Computing Surveys",
        "abstract": "The growth of data volume has changed cybersecurity activities, demanding a higher level of automation. In this new cybersecurity landscape, text mining emerged as an alternative to improve the efficiency of the activities involving unstructured data. This article proposes a Systematic Literature Review (SLR) to present the application of text mining in the cybersecurity domain. Using a systematic protocol, we identified 2,196 studies, out of which 83 were summarized. As a contribution, we propose a taxonomy to demonstrate the different activities in the cybersecurity domain supported by text mining. We also detail the strategies evaluated in the application of text mining tasks and the use of neural networks to support activities involving unstructured data. The work also discusses text classification performance aiming its application in real-world solutions. The SLR also highlights open gaps for future research, such as the analysis of non-English content and the intensification in the usage of neural networks.",
        "link": "https://dl.acm.org/doi/10.1145/3462477"
    },
    {
        "title": "Using Text Mining to Analyze the Financial Patents",
        "authors": [
            "Yung-Feng Lu",
            "Jia-Lang Xu",
            "Mu-Yen Chen"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "With the advancement and development of science and technology, Nowadays society is the development of information technology, cloud computing, mobile Internet, Internet of Things, block chain and information technology.The combination of finance and technology has made financial technology terms gradually emerging.. This research analyzes the patents of financial technology. However, Taiwan's financial technology patents are mostly G06Q-20 (payment plan, architecture or agreement), G06Q-30 (commercial, such as marketing, shopping, payment, auction or e-commerce). G06Q-40 (financial, investment or tax treatment, insurance).The research use text mining technology to analyze and understand Taiwan's trends and developments in financial technology. The research results show that G06Q-20 patents are biased towards information security and mobile payment. G06Q-30 patents favor shopping websites and advertising. G06Q-40 patents are biased towards diversity, such as securities, insurance, finance, and stocks.",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326478"
    },
    {
        "title": "Identifying Online Profiles of Distance Learning Students Using Data Mining Techniques",
        "authors": [
            "Osama Islam",
            "Muazzam Siddiqui",
            "Naif Radi Aljohani"
        ],
        "date": "October 2019",
        "source": "ICDTE '19: Proceedings of the 3rd International Conference on Digital Technology in Education",
        "abstract": "Educational data has grown over the years with the increased use of technology within educational environments. This has led to a huge amount of data being stored in various data sources representing the student, his/her activities, and other aspects relevant to the learning process. To meet this analytical need, Educational Data Mining (EDM) has emerged to assist educational institutions in identifying key benefits such as students at risk, the level of student engagement or predicting student performance. The aim of this research was to explore the various aspects of student interaction data using data mining techniques to identify relevant patterns of behaviors and possible key attributes that have higher degrees of influence on distance learning students. The main findings identified several patterns of user online profiles based on a set of adopted learning strategies, the research proposed a framework for analyzing such interaction data based on R and Hadoop platforms to correlate online profiles with student performance.",
        "link": "https://dl.acm.org/doi/10.1145/3369199.3369249"
    },
    {
        "title": "Extremely Fast Decision Tree Mining for Evolving Data Streams",
        "authors": [
            "Albert Bifet",
            "Jiajin Zhang",
            "Wei Fan",
            "Cheng He",
            "Jianfeng Zhang",
            "Jianfeng Qian",
            "Geoff Holmes",
            "Bernhard Pfahringer"
        ],
        "date": "August 2017",
        "source": "KDD '17: Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
        "abstract": "Nowadays real-time industrial applications are generating a huge amount of data continuously every day. To process these large data streams, we need fast and efficient methodologies and systems. A useful feature desired for data scientists and analysts is to have easy to visualize and understand machine learning models. Decision trees are preferred in many real-time applications for this reason, and also, because combined in an ensemble, they are one of the most powerful methods in machine learning. In this paper, we present a new system called STREAMDM-C++, that implements decision trees for data streams in C++, and that has been used extensively at Huawei. Streaming decision trees adapt to changes on streams, a huge advantage since standard decision trees are built using a snapshot of data, and can not evolve over time. STREAMDM-C++ is easy to extend, and contains more powerful ensemble methods, and a more efficient and easy to use adaptive decision trees. We compare our new implementation with VFML, the current state of the art implementation in C, and show how our new system outperforms VFML in speed using less resources.",
        "link": "https://dl.acm.org/doi/10.1145/3097983.3098139"
    },
    {
        "title": "Data miners' little helper: data transformation activity cues for cluster analysis on document collections",
        "authors": [
            "Tania Cerquitelli",
            "Evelina Di Corso",
            "Francesco Ventura",
            "Silvia Chiusano"
        ],
        "date": "June 2017",
        "source": "WIMS '17: Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "In this paper we propose a new self-learning engine to streamline the analytics process, as it enables analysts to mine massive data repositories with minimal user intervention. In the context of cluster analysis on a collection of documents this new system, named SELF-DATA (SELF-learning DAta TrAnsformation), suggests to the analyst how to configure the whole mining process for a given dataset. SELF-DATA relies on an engine exploring different data weighting schemas (e.g., normalized term frequencies) and data transformation methods (e.g., PCA) before applying the cluster analysis, evaluating and comparing solutions through different quality indices (e.g., weighted Silhouette), and presenting the k-top solutions to the analyst. SELF-DATA will also include a knowledge base storing results of experiments on previously processed datasets, and a classification algorithm trained on the knowledge base content to forecast the best configuration for the whole mining process for an unexplored dataset. The first development of SELF-DATA running on Apache Spark has been validated on 5 collections of documents. Experimental results highlight that TF-IDF and logarithmic entropy are effective to measure item relevance with sparse datasets, and the LSI method outperforms PCA with a large dictionary.",
        "link": "https://dl.acm.org/doi/10.1145/3102254.3102288"
    },
    {
        "title": "Interactive Visual Graph Mining and Learning",
        "authors": [
            "Ryan A. Rossi",
            "Nesreen K. Ahmed",
            "Rong Zhou",
            "Hoda Eldardiry"
        ],
        "date": null,
        "source": "ACM Transactions on Intelligent Systems and Technology",
        "abstract": "This article presents a platform for interactive graph mining and relational machine learning called GraphVis. The platform combines interactive visual representations with state-of-the-art graph mining and relational machine learning techniques to aid in revealing important insights quickly as well as learning an appropriate and highly predictive model for a particular task (e.g., classification, link prediction, discovering the roles of nodes, and finding influential nodes). Visual representations and interaction techniques and tools are developed for simple, fast, and intuitive real-time interactive exploration, mining, and modeling of graph data. In particular, we propose techniques for interactive relational learning (e.g., node/link classification), interactive link prediction and weighting, role discovery and community detection, higher-order network analysis (via graphlets, network motifs), among others. GraphVis also allows for the refinement and tuning of graph mining and relational learning methods for specific application domains and constraints via an end-to-end interactive visual analytic pipeline that learns, infers, and provides rapid interactive visualization with immediate feedback at each change/prediction in real-time. Other key aspects include interactive filtering, querying, ranking, manipulating, exporting, as well as tools for dynamic network analysis and visualization, interactive graph generators (including new block model approaches), and a variety of multi-level network analysis techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3200764"
    },
    {
        "title": "Data Mining and Opinion Mining: A Tool in Educational Context",
        "authors": [
            "Myriam Peñafiel",
            "Stefanie Vásquez",
            "Diego Vásquez",
            "Juan Zaldumbide",
            "Sergio Luján-Mora"
        ],
        "date": "July 2018",
        "source": "ICoMS '18: Proceedings of the 2018 1st International Conference on Mathematics and Statistics",
        "abstract": "The use of the web as a universal communication platform generates large volumes of data (Big data), which in many cases, need to be processed so that they can become useful knowledge in face of the sceptics who have doubts about the credibility of such information. The use of web data that comes from educational contexts needs to be addressed, since that large amount of unstructured information is not being valued, losing valuable information that can be used. To solve this problem, we propose the use of data mining techniques such as sentiment analysis to validate the information that comes from the educational platforms. The objective of this research is to propose a methodology that allows the user to apply sentiment analysis in a simple way, because although some researchers have done it, very few do with data in the educational context. The results obtained prove that the proposal can be used in similar cases.",
        "link": "https://dl.acm.org/doi/10.1145/3274250.3274263"
    },
    {
        "title": "Research Hotspots Mining and Visualized Analysis Based on Linking Cluster and K-Core Decomposition",
        "authors": [
            "Ying Cai",
            "Fang Huang",
            "Shuai Wang",
            "Hao Zhang",
            "Chunxiu Du"
        ],
        "date": "May 2018",
        "source": "ICDPA 2018: Proceedings of the International Conference on Data Processing and Applications",
        "abstract": "In this paper, we focus on research hotspots mining with knowledge semantic features on discipline topic word networks, which are extracted from scientific and technical documents. For the topic networks, a novel method of combining linking community clustering with k-core decomposition is proposed to mine the hotspot community with knowledge hierarchy structure. And as a post-processing, the hierarchical community identification is implemented by k-core decomposition in order to discover the internal connection with knowledge semantic hierarchy in the research hotspot. Finally, the community density and visualization method is utilized to analyze the core and sub- problem in the research hotspot community. In experiments, the topic networks for a single discipline and interdisciplinary research hotspot are taken as examples of analysis, and the results show the proposed approach can effectively realize the objective of mining and analyzing the discipline research hotspot on the academic topic word network.",
        "link": "https://dl.acm.org/doi/10.1145/3224207.3224216"
    },
    {
        "title": "The Application of Data Mining Algorithms in the Construction of Travel Recommendation System",
        "authors": [
            "Chao Lou",
            "Zhaonan Mu",
            "Mengzhu Liu"
        ],
        "date": "January 2021",
        "source": "CONF-CDS 2021: The 2nd International Conference on Computing and Data Science",
        "abstract": "Photos shared by users of online social network often associated with tags, texts, geographic and time information. These data are ideal research resource which can be used by researchers to observe people's interests and behaviors. They can also be used to analyze people's travel behaviors and interests because it can reflect people's travel activities and experiences directly. This project aimed to consolidate people into groups based on their travel preferences of travel season, travel time span, scenery type of travel location and then analyze popular travel places for different people groups. The travel preferences were analyzed from extracted data applying PHP language and Naive Bayes classification approach. According to the analyzed data, people were divided into groups. Then, DBSCAN clustering was used to get popular locations based on the different groups. As a result, 80 thousand photo data of 5 cities have been collected from Flickr by Phython. It has been showed that people's preference of season in most of the mined cities was spring and summer and the preference of time span was short. Moreover, mined people has been divided into 16 groups according to their preference and based on the groups popular locations have been calculated by DBSCAN clustering. Theses locations have been provided in a recommendation website which was built in this project. For the future work, mining Flickr data associated with other travel social network sites could be a good way to build a more effective recommendation system.",
        "link": "https://dl.acm.org/doi/10.1145/3448734.3450477"
    },
    {
        "title": "Compare Machine Learning Models in Text Classification Using Steam User Reviews",
        "authors": [
            "Youchen Miao",
            "Zeyu Jin",
            "Yumeng Zhang",
            "Yuchen Chen",
            "Junren Lai"
        ],
        "date": "November 2021",
        "source": "ICSED '21: Proceedings of the 2021 3rd International Conference on Software Engineering and Development",
        "abstract": "Text Classification and Sentiment Analysis of game reviews are viewed as important parts in not only academic fields but also in game studies. In this paper, with more than 400 thousand game reviews on Steam platform, we preprocess the data using different libraries (sklearn, nltk, and spaCy) and use them as inputs to build three sentiment classification models based on different algorithms (Naive Bayes, SVM, and Random Forest). In contrast to previous studies that only focus on different sentiment analysis models, our paper also highlights the use of different APIs to preprocess the data and their corresponding model performance. The results show that no matter which API we choose, Random Forest models always perform the best. However, in terms of training time, Naive Bayes is the fastest. This work can be used to apply grid search for researchers to automatically find the optimum API before conducting sentiment analysis in the future.",
        "link": "https://dl.acm.org/doi/10.1145/3507473.3507480"
    },
    {
        "title": "Lost in Transduction: Transductive Transfer Learning in Text Classification",
        "authors": [
            "Alejandro Moreo",
            "Andrea Esuli",
            "Fabrizio Sebastiani"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Obtaining high-quality labelled data for training a classifier in a new application domain is often costly. Transfer Learning\n(a.k.a. “Inductive Transfer”) tries to alleviate these costs by transferring, to the “target” domain of interest, knowledge available from a different “source” domain. In transfer learning the lack of labelled information from the target domain is compensated by the availability at training time of a set of unlabelled examples from the target distribution. Transductive Transfer Learning denotes the transfer learning setting in which the only set of target documents that we are interested in classifying is known and available at training time. Although this definition is indeed in line with Vapnik’s original definition of “transduction”, current terminology in the field is confused. In this article, we discuss how the term “transduction” has been misused in the transfer learning literature, and propose a clarification consistent with the original characterization of this term given by Vapnik. We go on to observe that the above terminology misuse has brought about misleading experimental comparisons, with inductive transfer learning methods that have been incorrectly compared with transductive transfer learning methods. We then, give empirical evidence that the difference in performance between the inductive version and the transductive version of a transfer learning method can indeed be statistically significant (i.e., that knowing at training time the only data one needs to classify indeed gives an advantage). Our clarification allows a reassessment of the field, and of the relative merits of the major, state-of-the-art algorithms for transfer learning in text classification.",
        "link": "https://dl.acm.org/doi/10.1145/3453146"
    },
    {
        "title": "Machine Learning on Mining Potential Adverse Drug Reactions for Pharmacovigilance",
        "authors": [
            "Ke Jia"
        ],
        "date": "July 2021",
        "source": "DSIT 2021: 2021 4th International Conference on Data Science and Information Technology",
        "abstract": "Drugs are related to the lives and health of patients. Mining and discovering potential adverse drug reactions from a large number of unstructured texts play an important role in drug research and pharmacovigilance. It is a hot research issue in the field of biomedicine in recent years. A large number of scholars have discovered a large amount of information on adverse drug reactions from different data sources through effective machine learning methods. This paper reviews the research progress in recent years to provide a reference for future research.",
        "link": "https://dl.acm.org/doi/10.1145/3478905.3478964"
    },
    {
        "title": "TACAM: Topic And Context Aware Argument Mining",
        "authors": [
            "Michael Fromm",
            "Evgeniy Faerman",
            "Thomas Seidl"
        ],
        "date": "October 2019",
        "source": "WI '19: IEEE/WIC/ACM International Conference on Web Intelligence",
        "abstract": "In this work we address the problem of argument search. The purpose of argument search is the distillation of pro and contra arguments for requested topics from large text corpora. In previous works, the usual approach is to use a standard search engine to extract text parts which are relevant to the given topic and subsequently use an argument recognition algorithm to select arguments from them. The main challenge in the argument recognition task, which is also known as argument mining, is that often sentences containing arguments are structurally similar to purely informative sentences without any stance about the topic. In fact, they only differ semantically. Most approaches use topic or search term information only for the first search step and therefore assume that arguments can be classified independently of a topic. We argue that topic information is crucial for argument mining, since the topic defines the semantic context of an argument. Precisely, we propose different models for the classification of arguments, which take information about a topic of an argument into account. Moreover, to enrich the context of a topic and to let models understand the context of the potential argument better, we integrate information from different external sources such as Knowledge Graphs or pre-trained NLP models. Our evaluation shows that considering topic information, especially in connection with external information, provides a significant performance boost for the argument mining task.",
        "link": "https://dl.acm.org/doi/10.1145/3350546.3352506"
    },
    {
        "title": "Text mining for incoming tasks based on the urgency/importance factors and task classification using machine learning tools",
        "authors": [
            "Yasser Ali Alshehri"
        ],
        "date": "March 2020",
        "source": "ICCDA 2020: Proceedings of the 2020 the 4th International Conference on Compute and Data Analysis",
        "abstract": "In workplaces, there is a massive amount of unstructured data from different sources. In this paper, we present a case study that explains how can through communications between employees, we can help to prioritize tasks requests to increase the efficiency of their works for both technical and non-technical workers. This involves managing daily incoming tasks based on their level of urgency and importance.To allow all workers to utilize the urgency-importance matrix as a time-management tool, we need to automate this tool. The textual content of incoming tasks are analyzed, and metrics related to urgency and importance are extracted. A third factor (i.e., the response variable) is defined based on the two input variables (urgency and importance). Then, machine learning applied to the data to predict the class of incoming tasks based on data outcome desired. We used ordinal regression, neural networks, and decision tree algorithms to predict the four levels of task priority. We measure the performance of all using recalls, precisions, and F-scores. All classifiers perform higher than 89% in terms of all measures.",
        "link": "https://dl.acm.org/doi/10.1145/3388142.3388153"
    },
    {
        "title": "Text mining for malware classification using multivariate all repeated patterns detection",
        "authors": [
            "Konstantinos F. Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2019",
        "source": "ASONAM '19: Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Mobile phones have become nowadays a commodity to the majority of people. Using them, people are able to access the world of Internet and connect with their friends, their colleagues at work or even unknown people with common interests. This proliferation of the mobile devices has also been seen as an opportunity for the cyber criminals to deceive smartphone users and steel their money directly or indirectly, respectively, by accessing their bank accounts through the smartphones or by blackmailing them or selling their private data such as photos, credit card data, etc. to third parties. This is usually achieved by installing malware to smartphones masking their malevolent payload as a legitimate application and advertise it to the users with the hope that mobile users will install it in their devices. Thus, any existing application can easily be modified by integrating a malware and then presented it as a legitimate one. In response to this, scientists have proposed a number of malware detection and classification methods using a variety of techniques. Even though, several of them achieve relatively high precision in malware classification, there is still space for improvement. In this paper, we propose a text mining all repeated pattern detection method which uses the decompiled files of an application in order to classify a suspicious application into one of the known malware families. Based on the experimental results using a real malware dataset, the methodology tries to correctly classify (without any misclassification) all randomly selected malware applications of 3 categories with 3 different families each.",
        "link": "https://dl.acm.org/doi/10.1145/3341161.3350841"
    },
    {
        "title": "Comparative Study between Traditional Machine Learning and Deep Learning Approaches for Text Classification",
        "authors": [
            "Cannannore Nidhi Kamath",
            "Syed Saqib Bukhari",
            "Andreas Dengel"
        ],
        "date": "August 2018",
        "source": "DocEng '18: Proceedings of the ACM Symposium on Document Engineering 2018",
        "abstract": "In this contemporaneous world, it is an obligation for any organization working with documents to end up with the insipid task of classifying truckload of documents, which is the nascent stage of venturing into the realm of information retrieval and data mining. But classification of such humongous documents into multiple classes, calls for a lot of time and labor. Hence a system which could classify these documents with acceptable accuracy would be of an unfathomable help in document engineering. We have created multiple classifiers for document classification and compared their accuracy on raw and processed data. We have garnered data used in a corporate organization as well as publicly available data for comparison. Data is processed by removing the stop-words and stemming is implemented to produce root words. Multiple traditional machine learning techniques like Naive Bayes, Logistic Regression, Support Vector Machine, Random forest Classifier and Multi-Layer Perceptron are used for classification of documents. Classifiers are applied on raw and processed data separately and their accuracy is noted. Along with this, Deep learning technique such as Convolution Neural Network is also used to classify the data and its accuracy is compared with that of traditional machine learning techniques. We are also exploring hierarchical classifiers for classification of classes and subclasses. The system classifies the data faster and with better accuracy than if done manually. The results are discussed in the results and evaluation section.",
        "link": "https://dl.acm.org/doi/10.1145/3209280.3209526"
    },
    {
        "title": "Identification of Overpricing in the Purchase of Medication by the Federal Government of Brazil, Using Text Mining and Clustering Based on Ontology",
        "authors": [
            "Marco Aurelio O. S. Correa",
            "Adriano Galindo Leal"
        ],
        "date": "August 2018",
        "source": "ICCBDC '18: Proceedings of the 2018 2nd International Conference on Cloud and Big Data Computing",
        "abstract": "Increasing the transparency level in his actions and spending is one of the primary duties of the Brazilian Federal Government. The creation of laws that oblige full disclose of all its expenditures through transparency portals enables citizens to supervise all government entities. However, only the dissemination of these data, without a definite standard or the availability of data analysis tools, does not guarantee that the citizen is empowered to play his role. Therefore hence, the objective of this work is to identify overprice in the acquisition of products purchased by the federal government of Brazil using the unstructured data available on the Transparency Portal. The last two-years' worth of purchasing data, available in the Transparency Portal, were extracted, processed and stored. Due to his diverse nature and high volume of data, this study focused only on medicines purchased by the Ministry of Health. Ontology-based text mining and clustering techniques were applied for automatic identification and classification of products. The processing of this information was done through text mining and clustering, based on the ontology registered in another database of the Brazilian government. Because of this work, a consolidated price base per medication was created to allow the identification of distortions in prices practised, facilitating the identification of cases that merit further investigation to unravel fraud to the treasury.",
        "link": "https://dl.acm.org/doi/10.1145/3264560.3264569"
    },
    {
        "title": "Computational Estimation by Scientific Data Mining with Classical Methods to Automate Learning Strategies of Scientists",
        "authors": [
            "Aparna S. Varde"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Experimental results are often plotted as 2-dimensional graphical plots (aka graphs) in scientific domains depicting dependent versus independent variables to aid visual analysis of processes. Repeatedly performing laboratory experiments consumes significant time and resources, motivating the need for computational estimation. The goals are to estimate the graph obtained in an experiment given its input conditions, and to estimate the conditions that would lead to a desired graph. Existing estimation approaches often do not meet accuracy and efficiency needs of targeted applications. We develop a computational estimation approach called AutoDomainMine that integrates clustering and classification over complex scientific data in a framework so as to automate classical learning methods of scientists. Knowledge discovered thereby from a database of existing experiments serves as the basis for estimation. Challenges include preserving domain semantics in clustering, finding matching strategies in classification, striking a good balance between elaboration and conciseness while displaying estimation results based on needs of targeted users, and deriving objective measures to capture subjective user interests. These and other challenges are addressed in this work. The AutoDomainMine approach is used to build a computational estimation system, rigorously evaluated with real data in Materials Science. Our evaluation confirms that AutoDomainMine provides desired accuracy and efficiency in computational estimation. It is extendable to other science and engineering domains as proved by adaptation of its sub-processes within fields such as Bioinformatics and Nanotechnology.",
        "link": "https://dl.acm.org/doi/10.1145/3502736"
    },
    {
        "title": "Where is the road for issue reports classification based on text mining?",
        "authors": [
            "Qiang Fan",
            "Yue Yu",
            "Gang Yin",
            "Tao Wang",
            "Huaimin Wang"
        ],
        "date": "November 2017",
        "source": "ESEM '17: Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Currently, open source projects receive various kinds of issues daily, because of the extreme openness of Issue Tracking System (ITS) in GitHub. ITS is a labor-intensive and time-consuming task of issue categorization for project managers. However, a contributor is only required a short textual abstract to report an issue in GitHub. Thus, most traditional classification approaches based on detailed and structured data (e.g., priority, severity, software version and so on) are difficult to adopt. In this paper, issue classification approaches on a large-scale dataset, including 80 popular projects and over 252,000 issue reports collected from GitHub, were investigated. First, four traditional text-based classification methods and their performances were discussed. Semantic perplexity (i.e., an issues description confuses bug-related sentences with nonbug-related sentences) is a crucial factor that affects the classification performances based on quantitative and qualitative study. Finally, A two-stage classifier framework based on the novel metrics of semantic perplexity of issue reports was designed. Results show that our two-stage classification can significantly improve issue classification performances.",
        "link": "https://dl.acm.org/doi/10.1109/ESEM.2017.19"
    },
    {
        "title": "Feature-based Facebook reviews process model for e-management using data mining",
        "authors": [
            "Anish Kumar Varudharajulu",
            "Yongsheng Ma"
        ],
        "date": "January 2019",
        "source": "IC4E '19: Proceedings of the 10th International Conference on E-Education, E-Business, E-Management and E-Learning",
        "abstract": "The data generated from online communication acts as potential gold mines for discovering knowledge for researchers. A large amount of data is also generated in the form of web documents, emails, blogs, and feedback, etc. Text analytics is being significantly employed to mine important information. Opinion mining is the process of extracting human thoughts and perceptions from unstructured texts. The showstopper for designing an opinion mining system for analyzing reviews arise from the fact that customer reviews are often noisy. These reviews are informally written. In addition, they are subjected to spelling mistakes, grammatical errors, improper punctuation and irrational capitalization. This paper focuses on analyzing the different classification and clustering algorithms aimed at extracting and consolidating opinions of customers from social media sites like Facebook, Twitter and through surveys, at multiple levels of granularity to monitor and measure customer satisfaction. Ours is an automated approach, in which the system aids in the process of knowledge assimilation for knowledge-based building and also performs the analytics. Domain experts ratify the knowledge base and also provide training datasets for the system to intuitively gather more instances for ratification. The system identifies opinion expressions as phrases containing opinion words, opinionated features and also opinion modifiers. These expressions are categorized as positive, negative or neutral. Opinion expressions are identified and categorized using localized linguistic techniques. Opinions can be congregated at any desired level of specificity i.e. feature level or product level, user level or service level, etc. We have developed a system based on this approach, which provides the user with a platform to analyze opinion expressions crawled from a set of pre-defined datasets.",
        "link": "https://dl.acm.org/doi/10.1145/3306500.3306514"
    },
    {
        "title": "Discriminative Topic Mining via Category-Name Guided Text Embedding",
        "authors": [
            "Yu Meng",
            "Jiaxin Huang",
            "Guangyuan Wang",
            "Zihan Wang",
            "Chao Zhang",
            "Yu Zhang",
            "Jiawei Han"
        ],
        "date": "April 2020",
        "source": "WWW '20: Proceedings of The Web Conference 2020",
        "abstract": "Mining a set of meaningful and distinctive topics automatically from massive text corpora has broad applications. Existing topic models, however, typically work in a purely unsupervised way, which often generate topics that do not fit users’ particular needs and yield suboptimal performance on downstream tasks. We propose a new task, discriminative topic mining, which leverages a set of user-provided category names to mine discriminative topics from text corpora. This new task not only helps a user understand clearly and distinctively the topics he/she is most interested in, but also benefits directly keyword-driven classification tasks. We develop CatE, a novel category-name guided text embedding method for discriminative topic mining, which effectively leverages minimal user guidance to learn a discriminative embedding space and discover category representative terms in an iterative manner. We conduct a comprehensive set of experiments to show that CatE mines high-quality set of topics guided by category names only, and benefits a variety of downstream applications including weakly-supervised classification and lexical entailment direction identification.",
        "link": "https://dl.acm.org/doi/10.1145/3366423.3380278"
    },
    {
        "title": "Comparative Study of Heart Disease Diagnosis Using Top Ten Data Mining Classification Algorithms",
        "authors": [
            "I. Ketut Agung Enriko"
        ],
        "date": "June 2019",
        "source": "ICFET '19: Proceedings of the 5th International Conference on Frontiers of Educational Technologies",
        "abstract": "Data mining has been used for many purposes, especially for prediction system. In healthcare, data mining algorithms often used in disease diagnosis. Meanwhile, heart disease is known as a primary cause of death over the years. Many studies have been performed in heart disease diagnosis using data mining methods. There are some popular data mining algorithms that can be used in heart disease diagnosis, for example, k-Nearest Neighbor, CART, and AdaBoost. The algorithms are used to analyze a sample of cardiovascular patients data and predict the heart disease type that they suffer. Some parameters are taken from the patient, including EKG morphology, blood pressure, and information about the existence of chest pain, shortness of breath, palpitation, and cold sweat. In this study, medical records data are collected from Harapan Kita Hospital and utilized as a dataset sample in this research. Top ten data mining classification algorithms are used in diagnosing heart disease from Harapan Kita Hospital data and examining their performance by checking the accuracy and speed.",
        "link": "https://dl.acm.org/doi/10.1145/3338188.3338220"
    },
    {
        "title": "Text Mining Approach for Identifying Research Trends",
        "authors": [
            "Snezhana Sulova"
        ],
        "date": "June 2021",
        "source": "CompSysTech '21: Proceedings of the 22nd International Conference on Computer Systems and Technologies",
        "abstract": "With the increase of unstructured data, the issues connected with automatic text processing, the categorization of documents and the discovery of topics have become objects of growing interest. In order to improve the process of grouping and processing research publications, we would like to propose a method based upon natural language processing. It is based on text mining technologies which aim to identify key tendencies in documents. It processes the content of publications by clustering and identifies the topics of each identified group. This analysis helps by identifying key tendencies as well as discovering emerging new areas of research. Publications from the research literature database, Scopus, were used to test the approach. The topic of the publications is “the application of digital technologies in the logistics business”. The experiments were completed using the RapidMiner Studio software.",
        "link": "https://dl.acm.org/doi/10.1145/3472410.3472433"
    },
    {
        "title": "Assessment of Vulnerability Severity using Text Mining",
        "authors": [
            "Georgios Spanos",
            "Lefteris Angelis",
            "Dimitrios Toloudis"
        ],
        "date": "September 2017",
        "source": "PCI '17: Proceedings of the 21st Pan-Hellenic Conference on Informatics",
        "abstract": "Software1 vulnerabilities are closely associated with information systems security, a major and critical field in today's technology. Vulnerabilities constitute a constant and increasing threat for various aspects of everyday life, especially for safety and economy, since the social impact from the problems that they cause is complicated and often unpredictable. Although there is an entire research branch in software engineering that deals with the identification and elimination of vulnerabilities, the growing complexity of software products and the variability of software production procedures are factors contributing to the ongoing occurrence of vulnerabilities, Hence, another area that is being developed in parallel focuses on the study and management of the vulnerabilities that have already been reported and registered in databases. The information contained in such databases includes, a textual description and a number of metrics related to vulnerabilities. The purpose of this paper is to investigate to what extend the assessment of the vulnerability severity can be inferred directly from the corresponding textual description, or in other words, to examine the informative power of the description with respect to the vulnerability severity. For this purpose, text mining techniques, i.e. text analysis and three different classification methods (decision trees, neural networks and support vector machines) were employed. The application of text mining to a sample of 70,678 vulnerabilities from a public data source shows that the description itself is a reliable and highly accurate source of information for vulnerability prioritization.",
        "link": "https://dl.acm.org/doi/10.1145/3139367.3139390"
    },
    {
        "title": "Clustering Algorithms for Spatial Data Mining",
        "authors": [
            "Chetashri Bhadane",
            "Ketan Shah"
        ],
        "date": "April 2020",
        "source": "ICGDA '20: Proceedings of the 2020 3rd International Conference on Geoinformatics and Data Analysis",
        "abstract": "With the advances in mobile and wireless technologies, there has been a rise in applications that track and share the users' geospatial data. People use several social networking sites such as Twitter, Facebook and Flickr, where they share their status updates. With the integration of Global Positioning System (GPS) with mobile phones, it is now possible to share one's locations on these social networks. GPS allows us to record and track a person's movement along with the timestamp. The data set obtained from these GPS logs is vast and is widely used to analyze the users' movement patterns. Specifically, we can find out significant locations based on the number of users present at that location and the time spent by them at such places. Once significant places have been identified, it is also possible to identify the semantic importance of these locations. This paper presents an overview of the clustering techniques used to find important places of interest using large GPS based mobility datasets. Four clustering algorithms, K-Means, DBSCAN, OPTICS and Hierarchical, are implemented, and performance is tested using real-time data of 50 users collected over 2--5 years. Performance summary depicts that K-Means and DBSCAN perform well for spatial data.",
        "link": "https://dl.acm.org/doi/10.1145/3397056.3397068"
    },
    {
        "title": "Feature Selection based Arabic Text Classification using Different Machine Learning Algorithms: Comparative Study",
        "authors": [
            "Sakina Rim Bennabi",
            "Zakaria Elberrichi"
        ],
        "date": "June 2020",
        "source": "ICIST '20: Proceedings of the 10th International Conference on Information Systems and Technologies",
        "abstract": "Feature selection is a method of data pre-processing widely used when mining large data, such as textual classification. Several studies have been conducted to compare the different methods of feature selection applied to corpora in English. Unfortunately, a small number of works concern the Arabic language. This article aims to present a comparative study of different feature selection techniques including: Chi2, the ANOVA method and mutual information, applied on a corpus in Arabic language, while also diversifying the machine learning algorithms (Naive Bayes, SVM and KNN). This experimental study has shown in general that reducing dimensionality with feature selection techniques has slightly affected the performance of textual classification, reducing the size of the corpus by up to 1%.",
        "link": "https://dl.acm.org/doi/10.1145/3447568.3448531"
    },
    {
        "title": "Intelligent Data Analysis using Optimized Support Vector Machine Based Data Mining Approach for Tourism Industry",
        "authors": [
            "Ms Promila Sharma",
            "Uma Meena",
            "Girish Kumar Sharma"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Data analysis involves the deployment of sophisticated approaches from data mining methods, information theory, and artificial intelligence in various fields like tourism, hospitality, and so on for the extraction of knowledge from the gathered and preprocessed data. In tourism, pattern analysis or data analysis using classification is significant for finding the patterns that represent new and potentially useful information or knowledge about the destination and other data. Several data mining techniques are introduced for the classification of data or patterns. However, overfitting, less accuracy, local minima, sensitive to noise are the drawbacks in some existing data mining classification methods. To overcome these challenges, Support vector machine with Red deer optimization (SVM-RDO) based data mining strategy is proposed in this article. Extended Kalman filter (EKF) is utilized in the first phase, i.e., data cleaning to remove the noise and missing values from the input data. Mantaray foraging algorithm (MaFA) is used in the data selection phase, in which the significant data are selected for the further process to reduce the computational complexity. The final phase is the classification, in which SVM-RDO is proposed to access the useful pattern from the selected data. PYTHON is the implementation tool used for the experiment of the proposed model. The experimental analysis is done to show the efficacy of the proposed work. From the experimental results, the proposed SVM-RDO achieved better accuracy, precision, recall, and F1 score than the existing methods for the tourism dataset. Thus, it is showed the effectiveness of the proposed SVM-RDO for pattern analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3494566"
    },
    {
        "title": "Applying text mining to predict learners' cognitive engagement",
        "authors": [
            "Hayati Hind",
            "Mohammed Khalidi Idrissi",
            "Samir Bennani"
        ],
        "date": "October 2017",
        "source": "SCAMS '17: Proceedings of the Mediterranean Symposium on Smart City Application",
        "abstract": "In academic process, engagement represent one of the important success' factor. This paper proposes a predictive system for learners' cognitive engagement based on their online discussion forums participation. In the first level the ontology OWL and the LSA method are used to perform a semantic classification system of the threads according to a specific context chosen by the tutor. Then the Text mining, as a predictive method, is applied to the classified threads and learners' participation in the forums.",
        "link": "https://dl.acm.org/doi/10.1145/3175628.3175655"
    },
    {
        "title": "Investigate Transitions into Drug Addiction through Text Mining of Reddit Data",
        "authors": [
            "John Lu",
            "Sumati Sridhar",
            "Ritika Pandey",
            "Mohammad Al Hasan",
            "Georege Mohler"
        ],
        "date": "July 2019",
        "source": "KDD '19: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Increasing rates of opioid drug abuse and heightened prevalence of online support communities underscore the necessity of employing data mining techniques to better understand drug addiction using these rapidly developing online resources. In this work, we obtained data from Reddit, an online collection of forums, to gather insight into drug use/misuse using text snippets from users narratives. Specifically, using users' posts, we trained a binary classifier which predicts a user's transitions from casual drug discussion forums to drug recovery forums. We also proposed a Cox regression model that outputs likelihoods of such transitions. In doing so, we found that utterances of select drugs and certain linguistic features contained in one's posts can help predict these transitions. Using unfiltered drug-related posts, our research delineates drugs that are associated with higher rates of transitions from recreational drug discussion to support/recovery discussion, offers insight into modern drug culture, and provides tools with potential applications in combating the opioid crisis.",
        "link": "https://dl.acm.org/doi/10.1145/3292500.3330737"
    },
    {
        "title": "Analysing digital banking reviews using text mining",
        "authors": [
            "Li Chen Cheng",
            "Legaspi Rhea Sharmayne"
        ],
        "date": "December 2020",
        "source": "ASONAM '20: Proceedings of the 12th IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Digital banks are new entrants in the banking industry in the Philippines as they only started late 2018. Since then, a handful of players have and are still emerging. With more and more people becoming technologically savvy, it is very critical for financial institutions to develop a digital banking application that will stand out from the competition. This paper aims to use text mining methods to analyse digital banking application reviews. This study will perform topic modelling using LDA to explore customer concerns and will mine association rules between the digital banking features with the review score. The results will reveal which areas the digital banking application can further optimize for customer satisfaction and retention.",
        "link": "https://dl.acm.org/doi/10.1109/ASONAM49781.2020.9381429"
    },
    {
        "title": "Mining Moocs Videos Metadata Using Classification Techniques",
        "authors": [
            "El Harrak Othman",
            "Ghadi Abderrahim",
            "El Bouhdidi Jaber"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "In few years, the internet has become the first source of information for most people, Today MOOCs makes it possible for everyone to access to the education over the world, it's represents an emerging methodology of online teaching and an important development in open education. Due to the rapid development of Moocs and the rapid growth of digital data and video database over the Internet, it is becoming very difficult for learners to browsing and choosing the best training for them. The scientific community has increased the amount of research into new technologies, with a view to improve the digital video utilization: its archiving, indexing, accessibility, acquisition, store and even its process and usability. All these parts of the video utilization entail the necessity of the extraction of all important information of a video, especially in cases of lack of metadata information. Web video mining is retrieving the content using data mining techniques from World Wide Web. There are two approaches for web video mining using traditional image processing (signal processing) and metadata based approach. In this paper, we present the various research makes in this subject and we propose an effective methodology to extract the metadata from Moocs videos and classify them based on the extracted metadata by applying data mining techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090450"
    },
    {
        "title": "Text mining with HathiTrust",
        "authors": [
            "Eleanor Dickson Koehl",
            "Ryan Dubnicek"
        ],
        "date": "June 2019",
        "source": "JCDL '19: Proceedings of the 18th Joint Conference on Digital Libraries",
        "abstract": "This tutorial will introduce attendees to the HathiTrust Research Center and its tools and services for computational text analysis research. HTRC leverages the scope and scale of the HathiTrust Digital Library collection to create opportunities for researchers to perform text data mining on subsets of the corpus. Attendees will develop skills that will allow them to conduct basic text analysis research using HathiTrust data, learn techniques for engaging with scholars on their text data mining research, and explore strategies for computational access to large-scale digital collections.",
        "link": "https://dl.acm.org/doi/10.1109/JCDL.2019.00115"
    },
    {
        "title": "Text mining for plagiarism detection: multivariate pattern detection for recognition of text similarities",
        "authors": [
            "Konstantinos Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2018",
        "source": "ASONAM '18: Proceedings of the 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "The problem of plagiarism the recent years has been intensified by the availability of information in digital form and the accessibility of the electronic libraries through the Internet. As a result, plagiarism detection has been transformed into a big data analytics problem since the number of digital sources is extravagant and a new document needs to be compared with millions of other existing documents. In this paper, a text mining methodology is proposed that can detect all common patterns between a document and the documents in a reference database. The technique is based on a pattern detection algorithm and the corresponding data structure that enables the algorithm to detect all common patterns. The methodology has been applied in a well-defined dataset providing very promising results identifying difficult cases of plagiarism such as technical disguise.",
        "link": "https://dl.acm.org/doi/10.5555/3382225.3382424"
    },
    {
        "title": "Petroleum Engineering Data Text Classification Using Convolutional Neural Network Based Classifier",
        "authors": [
            "Tzuhan Hsu",
            "Yaoqin Zhang"
        ],
        "date": "May 2018",
        "source": "ICMLT '18: Proceedings of the 2018 International Conference on Machine Learning Technologies",
        "abstract": "In this paper, authors introduce a convolutional neural network (CNN) based classification approach for petroleum engineering event. Recently, text classification problem has raised substantial attention in the field of machine learning. The aim of text classification process is to assign correct label to a text on the basis of its content. However, due to the imprecise of feature representation, most of former text classification methods cannot export satisfying result. To solve this problem, authors propose a model based on convolutional neural network. Authors conduct experiments with real world petroleum engineering data text, artificially labelled into 6 categories. Beside purposed CNN-based model, the authors also implement other baseline machine learning approaches: k nearest neighbor, random forest, support vector machines, recurrent neural network, gated recurrent units, long short-term memory. Among all these models, our CNN-based model reach best performance in text classification task. In the last part of this paper, authors compare and analysis their purpose and baseline model.",
        "link": "https://dl.acm.org/doi/10.1145/3231884.3231898"
    },
    {
        "title": "Classifying Indonesian Online Articles as Advertisement Placement Base Using Text Mining",
        "authors": [
            "Nadhira Tasya",
            "Arian Dhini"
        ],
        "date": "July 2017",
        "source": "ICBIM 2017: Proceedings of the International Conference on Business and Information Management",
        "abstract": "Rapid development in technological aspect resulting in growing level of human needs for the latest news, so that emerged a new trend of publishing and accessing news through online media or usually called online journalism. In addition, the number of people who sell and purchase through online sites also continues to increase and this opportunity is utilized by the company and the advertiser by implementing targeted web advertising. However, the high number of articles that have been published and accessed leads to great opportunities for errors in determining where to place the ads. Therefore, it needs a system that can categorize articles accessed by users as the basis of advertisement placement by the company and this classification system can be done by applying the method of Data Mining and Text Mining. This research uses document data in the form of article content that will be categorized into twenty categories of class of advertisement by using Text Mining technique with Support Vector Machine algorithm. The results of this study may be used by companies or advertisers as a basis for placement of ads on selected online media sites.",
        "link": "https://dl.acm.org/doi/10.1145/3134271.3134288"
    },
    {
        "title": "Mining Free-Text Medical Notes for Suicide Risk Assessment",
        "authors": [
            "Marios Adamou",
            "Grigoris Antoniou",
            "Elissavet Greasidou",
            "Vincenzo Lagani",
            "Paulos Charonyktakis",
            "Ioannis Tsamardinos"
        ],
        "date": "July 2018",
        "source": "SETN '18: Proceedings of the 10th Hellenic Conference on Artificial Intelligence",
        "abstract": "Suicide has been considered as an important public health issue for a very long time, and is one of the main causes of death worldwide. Despite suicide prevention strategies being applied, the rate of suicide has not changed substantially over the past decades. Advances in machine learning make it possible to attempt to predict suicide based on the analysis of relevant data to inform clinical practice. This paper reports on findings from the analysis of data of patients who died by suicide in the period 2013-2016 and made use of both structured data and free-text medical notes. We focus on examining various text-mining approaches to support risk assessment. The results show that using advance machine learning and text-mining techniques, it is possible to predict within a specified period which people are most at risk of taking their own life at the time of referral to a mental health service.",
        "link": "https://dl.acm.org/doi/10.1145/3200947.3201020"
    },
    {
        "title": "Integration of data mining techniques in e-learning systems: Clustering Profil of Lerners and Recommender Course System",
        "authors": [
            "A. El Moustamid",
            "E. En-Naimi",
            "J. El Bouhdidi"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "Data mining aims at the extraction of knowledge from large quantities of data by automatic or semi-automatic methods. In the field of education and training e-learning, Data Mining techniques as clustering (The aim of this technique is to identify groups that are similar in some aspect) [21], prediction (is used for developing model where combination of data viewed from different aspects can effect single viewed data. Here, is important to watch how predictors variables are effecting predicted variable)[21], classification are applied in e-learning to improve the level of learners by providing learning according to the learners' requirement) are used to analyze learners profile, to predict and improve student performance. The aim of our project is to develop a system capable of analyzing learners profile and indexing web videos in order to offer learners a rich database with courses that correspond to their levels.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090453"
    },
    {
        "title": "Multi-level mining and visualization of scientific text collections: Exploring a bi-lingual scientific repository",
        "authors": [
            "Pablo Accuosto",
            "Francesco Ronzano",
            "Daniel Ferrés",
            "Horacio Saggion"
        ],
        "date": "December 2017",
        "source": "WOSP 2017: Proceedings of the 6th International Workshop on Mining Scientific Publications",
        "abstract": "We present a system to mine and visualize collections of scientific documents by semantically browsing information extracted from single publications or aggregated throughout corpora of articles. The text mining tool performs deep analysis of document collections allowing the extraction and interpretation of research paper's contents. In addition to the extraction and enrichment of documents with metadata (titles, authors, affiliations, etc), the deep analysis performed comprises semantic interpretation, rhetorical analysis of sentences, triple-based information extraction, and text summarization. The visualization components allow geographical-based exploration of collections, topic-evolution interpretation, and collaborative network analysis among others. The paper presents a case study of a bi-lingual collection in the field of Natural Language Processing (NLP).",
        "link": "https://dl.acm.org/doi/10.1145/3127526.3127529"
    },
    {
        "title": "Hierarchical Topic Mining via Joint Spherical Tree and Text Embedding",
        "authors": [
            "Yu Meng",
            "Yunyi Zhang",
            "Jiaxin Huang",
            "Yu Zhang",
            "Chao Zhang",
            "Jiawei Han"
        ],
        "date": "August 2020",
        "source": "KDD '20: Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Mining a set of meaningful topics organized into a hierarchy is intuitively appealing since topic correlations are ubiquitous in massive text corpora. To account for potential hierarchical topic structures, hierarchical topic models generalize flat topic models by incorporating latent topic hierarchies into their generative modeling process. However, due to their purely unsupervised nature, the learned topic hierarchy often deviates from users' particular needs or interests. To guide the hierarchical topic discovery process with minimal user supervision, we propose a new task, Hierarchical Topic Mining, which takes a category tree described by category names only, and aims to mine a set of representative terms for each category from a text corpus to help a user comprehend his/her interested topics. We develop a novel joint tree and text embedding method along with a principled optimization procedure that allows simultaneous modeling of the category tree structure and the corpus generative process in the spherical space for effective category-representative term discovery. Our comprehensive experiments show that our model, named JoSH, mines a high-quality set of hierarchical topics with high efficiency and benefits weakly-supervised hierarchical text classification tasks.",
        "link": "https://dl.acm.org/doi/10.1145/3394486.3403242"
    },
    {
        "title": "Unsupervised behavioural mining and clustering for malware family identification",
        "authors": [
            "Khanh Huu The Dam",
            "Thomas Given-Wilson",
            "Axel Legay"
        ],
        "date": "March 2021",
        "source": "SAC '21: Proceedings of the 36th Annual ACM Symposium on Applied Computing",
        "abstract": "More accurate and advanced detection and classification of malware requires exploiting program behaviour and not merely syntactic or static features. One approach is to use system call dependency graphs (SCDGs) that represent the program behaviour by interactions with the system, and the relations between these interactions. These SCDGs have been used with supervised learning techniques to very accurately detect and classify malware. This works considers the unsupervised learning challenge of mining for common clusters of behaviour without a priori knowledge. This allows for clustering of similar programs by behaviour, that can then be used for either classification or further analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3412841.3441919"
    },
    {
        "title": "Text Mining for Evaluating Authors' Birth and Death Years",
        "authors": [
            "Dror Moghaz",
            "Yaakov Hacohen-Kerner",
            "Dov Gabbay"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "This article presents a unique method in text and data mining for finding the era, i.e., mining temporal data, in which an anonymous author was living. Finding this era can assist in the examination of a fake document or extracting the time period in which a writer lived. The study and the experiments concern Hebrew, and in some parts, Aramaic and Yiddish rabbinic texts. The rabbinic texts are undated and contain no bibliographic sections, posing an interesting challenge. This work proposes algorithms using key phrases and key words that allow the temporal organization of citations together with linguistic patterns. Based on these key phrases, key words, and the references, we established several types of “Iron-clad,” Heuristic and Greedy rules for estimating the years of birth and death of a writer in an interesting classification task. Experiments were conducted on corpora, including documents authored by 12, 24, and 36 rabbinic writers and demonstrated promising results.",
        "link": "https://dl.acm.org/doi/10.1145/3281631"
    },
    {
        "title": "Transfer Learning to Timed Text Based Video Classification Using CNN",
        "authors": [
            "Zenun Kastrati",
            "Ali Shariq Imran",
            "Arianit Kurti"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "Open educational video resources are gaining popularity with a growing number of massive open online courses (MOOCs). This has created a niche for content providers to adopt effective solutions in automatically organizing and structuring of educational resources for maximum visibility. Recent advances in deep learning techniques are proving useful in managing and classifying resources into appropriate categories. This paper proposes one such convolutional neural network (CNN) model for classifying video lectures in a MOOC setting using a transfer learning approach. The model uses a time-aligned text transcripts corresponding to video lectures from six broader subject categories. Video lectures and their corresponding transcript dataset is gathered from the Coursera MOOC platform. Two different CNN models are proposed: i) CNN based classification using embeddings learned from our MOOC dataset, ii) CNN based classification using transfer learning. Word embeddings generated from two well known state-of-the-art pre-trained models Word2Vec and GloVe, are used in the transfer learning approach for the second case. The proposed CNN models are evaluated using precision, recall, and F1 score and the obtained performance is compared with both conventional and deep learning classifiers. The proposed CNN models have an F1 score improvement of 10-22 percentage points over DNN and conventional classifiers",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326483"
    },
    {
        "title": "Evolutionary role mining in complex networks by ensemble clustering",
        "authors": [
            "Sarvenaz Choobdar",
            "Pedro Ribeiro",
            "Fernando Silva"
        ],
        "date": "April 2017",
        "source": "SAC '17: Proceedings of the Symposium on Applied Computing",
        "abstract": "The structural patterns in the neighborhood of nodes assign unique roles to the nodes. Mining the set of existing roles in a network provides a descriptive profile of the network and draws its general picture. This paper proposes a new method to determine structural roles in a dynamic network based on the current position of nodes and their historic behavior. We develop a temporal ensemble clustering technique to dynamically find groups of nodes, holding similar tempo-structural roles. We compare two weighting functions, based on age and distribution of data, to incorporate temporal behavior of nodes in the role discovery. To evaluate the performance of the proposed method, we assess the results from two points of view: 1) goodness of fit to current structure of the network; 2) consistency with historic data. We conduct the evaluation using different ensemble clustering techniques. The results on real world networks demonstrate that our method can detect tempo-structural roles that simultaneously depict the topology of a network and reflect its dynamics with high accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3019612.3019815"
    },
    {
        "title": "Analyzing used-car web listings via text mining",
        "authors": [
            "Ayhan Demiriz",
            "Fatma Cantaş"
        ],
        "date": "October 2017",
        "source": "IML '17: Proceedings of the 1st International Conference on Internet of Things and Machine Learning",
        "abstract": "Used car trade is one of the major components of the world economies. It is not uncommon to sell a car by placing an internet advertisement irrespective of the geography in these days. A typical content of an advertisement is usually composed of two parts namely the structured and the free text data. The structured data may include some information about the asking price, make, model, year, mileage of the car and the contact info. In most cases, seller may give important clues about the car's current conditions in the free text data where the title (head) of the advertisement can be included as free text too. This paper reports preliminary results from a text mining study conducted on 75K used car internet listings collected from two major car listing web sites in Turkey. As expected, the words and the phrases related to the description of the car are observed to be frequent. The leading concepts in the free text are found to be regarding how to describe the current condition of a car, for example \"no crash history\".",
        "link": "https://dl.acm.org/doi/10.1145/3109761.3109782"
    },
    {
        "title": "Text classification of Diseases Treated by Traditional Chinese Medicine Prescription based on machine learning",
        "authors": [
            "Hanqing Zhao",
            "Bo Han",
            "Chen Li"
        ],
        "date": "September 2020",
        "source": "ISAIMS '20: Proceedings of the 1st International Symposium on Artificial Intelligence in Medical Sciences",
        "abstract": "OBJECTIVE: To explore the application of machine learning in th e identification of diseases treated by traditional Chinese medicine prescriptions. METHODS: Based on the composition of the text document of Chinese medicine prescriptions, the prescriptions were divided into cough, headache and diarrhea. THUTCT were introduced to establish and train two machine learning text classification models, LibSVM and LibLinear, and the prescriptions to be deter mined were put into the model for classification and prediction. R ESULTS: The Precision rate of LibSVM model and Liblinear mod el were 0.7283 and 0.6690. Seven prescriptions were classified an d predicted, and the results were in line with expectations. CONCLUSION: THUCTC has good universality for the content of TCM prescriptions, high classification accuracy and fast testing speed, which is suitable for the text classification and discrimination research of TCM prescriptions for diseases.",
        "link": "https://dl.acm.org/doi/10.1145/3429889.3429895"
    },
    {
        "title": "Evaluation of retweet clustering method classification method using retweets on Twitter without text data",
        "authors": [
            "K. Uchida",
            "F. Toriumi",
            "T. Sakaki"
        ],
        "date": "August 2017",
        "source": "WI '17: Proceedings of the International Conference on Web Intelligence",
        "abstract": "Burst phenomena, which frequently occur on social media, are caused by such social events as flaming on the internet, elections, and natural disasters. To understand people's thoughts and feelings, we must classify their opinions from burst phenomena. Therefore, classification methods that categorize tweets are critical. However, since most classification methods focus on text mining, they cannot group tweets by topics because each tweet has poor linguistic similarities. We used a non-text-based classification method proposed by Baba et al. that groups tweets by topics, even if they have poor linguistic similarities, and verified its validity by comparing it with a text-based classification method in two different evaluations: qualitative and quantitative. In the qualitative evaluation part, we did a questionnaire survey and validated the suitability of the topic clusters created using both the non-and text-based methods. Since evaluating the similarity of every pair of tweets in each topic is difficult, we evaluated the similarity between sampled pairs in the survey and acquired more appropriate topic clustering results using the non-text-based method than the text-based method. In the quantitative evaluation part, we focused on the robustness of each method against data reduction. Many approaches analyze social media data, especially because collecting data from social media is comparatively easy. However, since collecting the whole data of burst phenomena is very costly due to the vast amounts of available social media data, robustness against data reduction is an important index to evaluate classification methods. With the non-text-based method, over 55% of the pairs of tweets in the same cluster were also included in the same cluster even when the data were reduced to 10% in all three of our example cases. In this paper, as a source we focus on Twitter, one of the most popular microblogging services. Using clustering to conduct detailed case analyses, we scrutinized three burst cases that include natural disasters and flaming on the internet and found that a non-text-based method more effectively classified tweets in burst phenomena than a text-based method.",
        "link": "https://dl.acm.org/doi/10.1145/3106426.3106451"
    },
    {
        "title": "Using Spark for Text Mining on Large Scale Liver Cancer Literature",
        "authors": [
            "Ming-Yen Lin",
            "Yu-Ju Lin",
            "Sue-Chen Hsueh"
        ],
        "date": "January 2021",
        "source": "BDET 2021: 2021 the 3rd International Conference on Big Data Engineering and Technology (BDET)",
        "abstract": "Cancer is one of the main causes of death. The number of known cancer to-date is more than one hundred. Liver cancer, ranked after trachea and lung cancer, has long been high in the cancer leading cause in Taiwan and even ranked as second in 2016. The number of scientific articles related to cancer proliferates every year. The number reaches as high as 20 million in PubMed so that discovering useful information from the massive collection is very difficult. In addition, using a single machine to sift through these articles is very time-consuming. Therefore, we present a big data analytic framework using the distributed Apache Spark platform for text mining in PubMed literature. We establish a prediction model for liver cancer articles so that researchers may effectively validate whether an article is related to liver cancer or not. Classification models in Spark MLlib including Linear Support Vector Machines (SVM), Logistic Regression, are used in our experiments. Relevancy to liver cancer is further confirmed by using MeSH (Medical Subject Headings) terms. Logistic regression is about 3 times faster than SVMs and the accuracy of both methods is close to 95% in the experiments using hold-out validation. When max_features is 500 and min_df ≤ 0.1 (or min_df = 1), the accuracy may reach 96%. In the experiments with K-fold cross-validation, the accuracy of SVMs methods is 96%. The experimental results show that that our prediction model may effectively classify liver cancer articles.",
        "link": "https://dl.acm.org/doi/10.1145/3474944.3474958"
    },
    {
        "title": "Automatic topic classification of test cases using text mining at an Android smartphone vendor",
        "authors": [
            "Junji Shimagaki",
            "Yasutaka Kamei",
            "Naoyasu Ubayashi",
            "Abram Hindle"
        ],
        "date": "October 2018",
        "source": "ESEM '18: Proceedings of the 12th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Background: An Android smartphone is an ecosystem of applications, drivers, operating system components, and assets. The volume of the software is large and the number of test cases needed to cover the functionality of an Android system is substantial. Enormous effort has been already taken to properly quantify \"what features and apps were tested and verified?\". This insight is provided by dashboards that summarize test coverage and results per feature. One method to achieve this is to manually tag or label test cases with the topic or function they cover, much like function points. At the studied Android smartphone vendor, tests are labelled with manually defined tags, so-called \"feature labels (FLs)\", and the FLs serve to categorize 100s to 1000s test cases into 10 to 50 groups. Aim: Unfortunately for developers, manual assignment of FLs to 1000s of test cases is a time consuming task, leading to inaccurately labeled test cases, which will render the dashboard useless. We created an automated system that suggests tags/labels to the developers for their test cases rather than manual labeling. Method: We use machine learning models to predict and label the functionality tested by 10,000 test cases developed at the company. Results: Through the quantitative experiments, our models achieved acceptable F-1 performance of 0.3 to 0.88. Also through the qualitative studies with expert teams, we showed that the hierarchy and path of tests was a good predictor of a feature's label. Conclusions: We find that this method can reduce tedious manual effort that software developers spent classifying test cases, while providing more accurate classification results.",
        "link": "https://dl.acm.org/doi/10.1145/3239235.3268927"
    },
    {
        "title": "To apply Data Mining for Classification of Crowd sourced Software Requirements",
        "authors": [
            "Soonh Taj",
            "Qasim Arain",
            "Imran Memon",
            "Asma Zubedi"
        ],
        "date": "April 2019",
        "source": "ICSIE '19: Proceedings of the 8th International Conference on Software and Information Engineering",
        "abstract": "Now a day's main focus of developers is to build quality software that works according to customer needs and for this reason it is necessary to gather right requirements as requirement elicitation is the critical step that impacts on the success of software project as misinterpreted requirements leads to the failure of software project. By keeping this in mind a research is carried out on improving requirements elicitation process and automating the process of classifying requirements. In this research, a model is proposed which will help in this scenario for requirements elicitation and requirement classification. This paper presents a model in which crowd sourcing approach is used so that customers, end users, stakeholders, developers and software engineers can make active participation for requirement elicitation process and requirements gathered using crowdsourcing approach are used by model for classification process i.e. classification of requirements into functional and non-functional requirements. For the proof of proposed model a case study is conducted. Results of case study provided the usefulness and efficiency of proposed model for classification of crowd sourced software requirements.",
        "link": "https://dl.acm.org/doi/10.1145/3328833.3328837"
    },
    {
        "title": "Text classification using Fuzzy TF-IDF and Machine Learning Models",
        "authors": [
            "Mariem Bounabi",
            "Karim El Moutaouakil",
            "Khalid Satori"
        ],
        "date": "October 2019",
        "source": "BDIoT'19: Proceedings of the 4th International Conference on Big Data and Internet of Things",
        "abstract": "The representation of the information has an important impact on the text classification task. Several weighting methods were proposed in the literature, and the term frequency-inverse term frequency (TFIDF), the most know on the text treatment field. The FTF-IDF is a vector representation where the components of the TFIDF are presented as inputs to the Fuzzy Inference System (FIS). In this work, we compare several Machin Learning algorithms such as Naïve Bayes and its derivatives, SVM and Random forest classifiers, using the FTF-IDF representation. To improve the quality of the used classifiers, we call sum attribute selection methods. The recognition rate, for the tested systems, is satisfied, where the system based on naïve Bayes classifier, the FTF-IDF weighting terms, and the info gain select attributes method gives 98.7% as accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3372938.3372956"
    },
    {
        "title": "Text Mining in Cybersecurity: A Systematic Literature Review",
        "authors": [
            "Luciano Ignaczak",
            "Guilherme Goldschmidt",
            "Cristiano André Da Costa",
            "Rodrigo Da Rosa Righi"
        ],
        "date": null,
        "source": "ACM Computing Surveys",
        "abstract": "The growth of data volume has changed cybersecurity activities, demanding a higher level of automation. In this new cybersecurity landscape, text mining emerged as an alternative to improve the efficiency of the activities involving unstructured data. This article proposes a Systematic Literature Review (SLR) to present the application of text mining in the cybersecurity domain. Using a systematic protocol, we identified 2,196 studies, out of which 83 were summarized. As a contribution, we propose a taxonomy to demonstrate the different activities in the cybersecurity domain supported by text mining. We also detail the strategies evaluated in the application of text mining tasks and the use of neural networks to support activities involving unstructured data. The work also discusses text classification performance aiming its application in real-world solutions. The SLR also highlights open gaps for future research, such as the analysis of non-English content and the intensification in the usage of neural networks.",
        "link": "https://dl.acm.org/doi/10.1145/3462477"
    },
    {
        "title": "Using Text Mining to Analyze the Financial Patents",
        "authors": [
            "Yung-Feng Lu",
            "Jia-Lang Xu",
            "Mu-Yen Chen"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "With the advancement and development of science and technology, Nowadays society is the development of information technology, cloud computing, mobile Internet, Internet of Things, block chain and information technology.The combination of finance and technology has made financial technology terms gradually emerging.. This research analyzes the patents of financial technology. However, Taiwan's financial technology patents are mostly G06Q-20 (payment plan, architecture or agreement), G06Q-30 (commercial, such as marketing, shopping, payment, auction or e-commerce). G06Q-40 (financial, investment or tax treatment, insurance).The research use text mining technology to analyze and understand Taiwan's trends and developments in financial technology. The research results show that G06Q-20 patents are biased towards information security and mobile payment. G06Q-30 patents favor shopping websites and advertising. G06Q-40 patents are biased towards diversity, such as securities, insurance, finance, and stocks.",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326478"
    },
    {
        "title": "Identifying Online Profiles of Distance Learning Students Using Data Mining Techniques",
        "authors": [
            "Osama Islam",
            "Muazzam Siddiqui",
            "Naif Radi Aljohani"
        ],
        "date": "October 2019",
        "source": "ICDTE '19: Proceedings of the 3rd International Conference on Digital Technology in Education",
        "abstract": "Educational data has grown over the years with the increased use of technology within educational environments. This has led to a huge amount of data being stored in various data sources representing the student, his/her activities, and other aspects relevant to the learning process. To meet this analytical need, Educational Data Mining (EDM) has emerged to assist educational institutions in identifying key benefits such as students at risk, the level of student engagement or predicting student performance. The aim of this research was to explore the various aspects of student interaction data using data mining techniques to identify relevant patterns of behaviors and possible key attributes that have higher degrees of influence on distance learning students. The main findings identified several patterns of user online profiles based on a set of adopted learning strategies, the research proposed a framework for analyzing such interaction data based on R and Hadoop platforms to correlate online profiles with student performance.",
        "link": "https://dl.acm.org/doi/10.1145/3369199.3369249"
    },
    {
        "title": "Extremely Fast Decision Tree Mining for Evolving Data Streams",
        "authors": [
            "Albert Bifet",
            "Jiajin Zhang",
            "Wei Fan",
            "Cheng He",
            "Jianfeng Zhang",
            "Jianfeng Qian",
            "Geoff Holmes",
            "Bernhard Pfahringer"
        ],
        "date": "August 2017",
        "source": "KDD '17: Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
        "abstract": "Nowadays real-time industrial applications are generating a huge amount of data continuously every day. To process these large data streams, we need fast and efficient methodologies and systems. A useful feature desired for data scientists and analysts is to have easy to visualize and understand machine learning models. Decision trees are preferred in many real-time applications for this reason, and also, because combined in an ensemble, they are one of the most powerful methods in machine learning. In this paper, we present a new system called STREAMDM-C++, that implements decision trees for data streams in C++, and that has been used extensively at Huawei. Streaming decision trees adapt to changes on streams, a huge advantage since standard decision trees are built using a snapshot of data, and can not evolve over time. STREAMDM-C++ is easy to extend, and contains more powerful ensemble methods, and a more efficient and easy to use adaptive decision trees. We compare our new implementation with VFML, the current state of the art implementation in C, and show how our new system outperforms VFML in speed using less resources.",
        "link": "https://dl.acm.org/doi/10.1145/3097983.3098139"
    },
    {
        "title": "Data miners' little helper: data transformation activity cues for cluster analysis on document collections",
        "authors": [
            "Tania Cerquitelli",
            "Evelina Di Corso",
            "Francesco Ventura",
            "Silvia Chiusano"
        ],
        "date": "June 2017",
        "source": "WIMS '17: Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "In this paper we propose a new self-learning engine to streamline the analytics process, as it enables analysts to mine massive data repositories with minimal user intervention. In the context of cluster analysis on a collection of documents this new system, named SELF-DATA (SELF-learning DAta TrAnsformation), suggests to the analyst how to configure the whole mining process for a given dataset. SELF-DATA relies on an engine exploring different data weighting schemas (e.g., normalized term frequencies) and data transformation methods (e.g., PCA) before applying the cluster analysis, evaluating and comparing solutions through different quality indices (e.g., weighted Silhouette), and presenting the k-top solutions to the analyst. SELF-DATA will also include a knowledge base storing results of experiments on previously processed datasets, and a classification algorithm trained on the knowledge base content to forecast the best configuration for the whole mining process for an unexplored dataset. The first development of SELF-DATA running on Apache Spark has been validated on 5 collections of documents. Experimental results highlight that TF-IDF and logarithmic entropy are effective to measure item relevance with sparse datasets, and the LSI method outperforms PCA with a large dictionary.",
        "link": "https://dl.acm.org/doi/10.1145/3102254.3102288"
    },
    {
        "title": "Interactive Visual Graph Mining and Learning",
        "authors": [
            "Ryan A. Rossi",
            "Nesreen K. Ahmed",
            "Rong Zhou",
            "Hoda Eldardiry"
        ],
        "date": null,
        "source": "ACM Transactions on Intelligent Systems and Technology",
        "abstract": "This article presents a platform for interactive graph mining and relational machine learning called GraphVis. The platform combines interactive visual representations with state-of-the-art graph mining and relational machine learning techniques to aid in revealing important insights quickly as well as learning an appropriate and highly predictive model for a particular task (e.g., classification, link prediction, discovering the roles of nodes, and finding influential nodes). Visual representations and interaction techniques and tools are developed for simple, fast, and intuitive real-time interactive exploration, mining, and modeling of graph data. In particular, we propose techniques for interactive relational learning (e.g., node/link classification), interactive link prediction and weighting, role discovery and community detection, higher-order network analysis (via graphlets, network motifs), among others. GraphVis also allows for the refinement and tuning of graph mining and relational learning methods for specific application domains and constraints via an end-to-end interactive visual analytic pipeline that learns, infers, and provides rapid interactive visualization with immediate feedback at each change/prediction in real-time. Other key aspects include interactive filtering, querying, ranking, manipulating, exporting, as well as tools for dynamic network analysis and visualization, interactive graph generators (including new block model approaches), and a variety of multi-level network analysis techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3200764"
    },
    {
        "title": "Data Mining and Opinion Mining: A Tool in Educational Context",
        "authors": [
            "Myriam Peñafiel",
            "Stefanie Vásquez",
            "Diego Vásquez",
            "Juan Zaldumbide",
            "Sergio Luján-Mora"
        ],
        "date": "July 2018",
        "source": "ICoMS '18: Proceedings of the 2018 1st International Conference on Mathematics and Statistics",
        "abstract": "The use of the web as a universal communication platform generates large volumes of data (Big data), which in many cases, need to be processed so that they can become useful knowledge in face of the sceptics who have doubts about the credibility of such information. The use of web data that comes from educational contexts needs to be addressed, since that large amount of unstructured information is not being valued, losing valuable information that can be used. To solve this problem, we propose the use of data mining techniques such as sentiment analysis to validate the information that comes from the educational platforms. The objective of this research is to propose a methodology that allows the user to apply sentiment analysis in a simple way, because although some researchers have done it, very few do with data in the educational context. The results obtained prove that the proposal can be used in similar cases.",
        "link": "https://dl.acm.org/doi/10.1145/3274250.3274263"
    },
    {
        "title": "Research Hotspots Mining and Visualized Analysis Based on Linking Cluster and K-Core Decomposition",
        "authors": [
            "Ying Cai",
            "Fang Huang",
            "Shuai Wang",
            "Hao Zhang",
            "Chunxiu Du"
        ],
        "date": "May 2018",
        "source": "ICDPA 2018: Proceedings of the International Conference on Data Processing and Applications",
        "abstract": "In this paper, we focus on research hotspots mining with knowledge semantic features on discipline topic word networks, which are extracted from scientific and technical documents. For the topic networks, a novel method of combining linking community clustering with k-core decomposition is proposed to mine the hotspot community with knowledge hierarchy structure. And as a post-processing, the hierarchical community identification is implemented by k-core decomposition in order to discover the internal connection with knowledge semantic hierarchy in the research hotspot. Finally, the community density and visualization method is utilized to analyze the core and sub- problem in the research hotspot community. In experiments, the topic networks for a single discipline and interdisciplinary research hotspot are taken as examples of analysis, and the results show the proposed approach can effectively realize the objective of mining and analyzing the discipline research hotspot on the academic topic word network.",
        "link": "https://dl.acm.org/doi/10.1145/3224207.3224216"
    },
    {
        "title": "The Application of Data Mining Algorithms in the Construction of Travel Recommendation System",
        "authors": [
            "Chao Lou",
            "Zhaonan Mu",
            "Mengzhu Liu"
        ],
        "date": "January 2021",
        "source": "CONF-CDS 2021: The 2nd International Conference on Computing and Data Science",
        "abstract": "Photos shared by users of online social network often associated with tags, texts, geographic and time information. These data are ideal research resource which can be used by researchers to observe people's interests and behaviors. They can also be used to analyze people's travel behaviors and interests because it can reflect people's travel activities and experiences directly. This project aimed to consolidate people into groups based on their travel preferences of travel season, travel time span, scenery type of travel location and then analyze popular travel places for different people groups. The travel preferences were analyzed from extracted data applying PHP language and Naive Bayes classification approach. According to the analyzed data, people were divided into groups. Then, DBSCAN clustering was used to get popular locations based on the different groups. As a result, 80 thousand photo data of 5 cities have been collected from Flickr by Phython. It has been showed that people's preference of season in most of the mined cities was spring and summer and the preference of time span was short. Moreover, mined people has been divided into 16 groups according to their preference and based on the groups popular locations have been calculated by DBSCAN clustering. Theses locations have been provided in a recommendation website which was built in this project. For the future work, mining Flickr data associated with other travel social network sites could be a good way to build a more effective recommendation system.",
        "link": "https://dl.acm.org/doi/10.1145/3448734.3450477"
    },
    {
        "title": "Compare Machine Learning Models in Text Classification Using Steam User Reviews",
        "authors": [
            "Youchen Miao",
            "Zeyu Jin",
            "Yumeng Zhang",
            "Yuchen Chen",
            "Junren Lai"
        ],
        "date": "November 2021",
        "source": "ICSED '21: Proceedings of the 2021 3rd International Conference on Software Engineering and Development",
        "abstract": "Text Classification and Sentiment Analysis of game reviews are viewed as important parts in not only academic fields but also in game studies. In this paper, with more than 400 thousand game reviews on Steam platform, we preprocess the data using different libraries (sklearn, nltk, and spaCy) and use them as inputs to build three sentiment classification models based on different algorithms (Naive Bayes, SVM, and Random Forest). In contrast to previous studies that only focus on different sentiment analysis models, our paper also highlights the use of different APIs to preprocess the data and their corresponding model performance. The results show that no matter which API we choose, Random Forest models always perform the best. However, in terms of training time, Naive Bayes is the fastest. This work can be used to apply grid search for researchers to automatically find the optimum API before conducting sentiment analysis in the future.",
        "link": "https://dl.acm.org/doi/10.1145/3507473.3507480"
    },
    {
        "title": "Lost in Transduction: Transductive Transfer Learning in Text Classification",
        "authors": [
            "Alejandro Moreo",
            "Andrea Esuli",
            "Fabrizio Sebastiani"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Obtaining high-quality labelled data for training a classifier in a new application domain is often costly. Transfer Learning\n(a.k.a. “Inductive Transfer”) tries to alleviate these costs by transferring, to the “target” domain of interest, knowledge available from a different “source” domain. In transfer learning the lack of labelled information from the target domain is compensated by the availability at training time of a set of unlabelled examples from the target distribution. Transductive Transfer Learning denotes the transfer learning setting in which the only set of target documents that we are interested in classifying is known and available at training time. Although this definition is indeed in line with Vapnik’s original definition of “transduction”, current terminology in the field is confused. In this article, we discuss how the term “transduction” has been misused in the transfer learning literature, and propose a clarification consistent with the original characterization of this term given by Vapnik. We go on to observe that the above terminology misuse has brought about misleading experimental comparisons, with inductive transfer learning methods that have been incorrectly compared with transductive transfer learning methods. We then, give empirical evidence that the difference in performance between the inductive version and the transductive version of a transfer learning method can indeed be statistically significant (i.e., that knowing at training time the only data one needs to classify indeed gives an advantage). Our clarification allows a reassessment of the field, and of the relative merits of the major, state-of-the-art algorithms for transfer learning in text classification.",
        "link": "https://dl.acm.org/doi/10.1145/3453146"
    },
    {
        "title": "Machine Learning on Mining Potential Adverse Drug Reactions for Pharmacovigilance",
        "authors": [
            "Ke Jia"
        ],
        "date": "July 2021",
        "source": "DSIT 2021: 2021 4th International Conference on Data Science and Information Technology",
        "abstract": "Drugs are related to the lives and health of patients. Mining and discovering potential adverse drug reactions from a large number of unstructured texts play an important role in drug research and pharmacovigilance. It is a hot research issue in the field of biomedicine in recent years. A large number of scholars have discovered a large amount of information on adverse drug reactions from different data sources through effective machine learning methods. This paper reviews the research progress in recent years to provide a reference for future research.",
        "link": "https://dl.acm.org/doi/10.1145/3478905.3478964"
    },
    {
        "title": "TACAM: Topic And Context Aware Argument Mining",
        "authors": [
            "Michael Fromm",
            "Evgeniy Faerman",
            "Thomas Seidl"
        ],
        "date": "October 2019",
        "source": "WI '19: IEEE/WIC/ACM International Conference on Web Intelligence",
        "abstract": "In this work we address the problem of argument search. The purpose of argument search is the distillation of pro and contra arguments for requested topics from large text corpora. In previous works, the usual approach is to use a standard search engine to extract text parts which are relevant to the given topic and subsequently use an argument recognition algorithm to select arguments from them. The main challenge in the argument recognition task, which is also known as argument mining, is that often sentences containing arguments are structurally similar to purely informative sentences without any stance about the topic. In fact, they only differ semantically. Most approaches use topic or search term information only for the first search step and therefore assume that arguments can be classified independently of a topic. We argue that topic information is crucial for argument mining, since the topic defines the semantic context of an argument. Precisely, we propose different models for the classification of arguments, which take information about a topic of an argument into account. Moreover, to enrich the context of a topic and to let models understand the context of the potential argument better, we integrate information from different external sources such as Knowledge Graphs or pre-trained NLP models. Our evaluation shows that considering topic information, especially in connection with external information, provides a significant performance boost for the argument mining task.",
        "link": "https://dl.acm.org/doi/10.1145/3350546.3352506"
    },
    {
        "title": "Text mining for incoming tasks based on the urgency/importance factors and task classification using machine learning tools",
        "authors": [
            "Yasser Ali Alshehri"
        ],
        "date": "March 2020",
        "source": "ICCDA 2020: Proceedings of the 2020 the 4th International Conference on Compute and Data Analysis",
        "abstract": "In workplaces, there is a massive amount of unstructured data from different sources. In this paper, we present a case study that explains how can through communications between employees, we can help to prioritize tasks requests to increase the efficiency of their works for both technical and non-technical workers. This involves managing daily incoming tasks based on their level of urgency and importance.To allow all workers to utilize the urgency-importance matrix as a time-management tool, we need to automate this tool. The textual content of incoming tasks are analyzed, and metrics related to urgency and importance are extracted. A third factor (i.e., the response variable) is defined based on the two input variables (urgency and importance). Then, machine learning applied to the data to predict the class of incoming tasks based on data outcome desired. We used ordinal regression, neural networks, and decision tree algorithms to predict the four levels of task priority. We measure the performance of all using recalls, precisions, and F-scores. All classifiers perform higher than 89% in terms of all measures.",
        "link": "https://dl.acm.org/doi/10.1145/3388142.3388153"
    },
    {
        "title": "Text mining for malware classification using multivariate all repeated patterns detection",
        "authors": [
            "Konstantinos F. Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2019",
        "source": "ASONAM '19: Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Mobile phones have become nowadays a commodity to the majority of people. Using them, people are able to access the world of Internet and connect with their friends, their colleagues at work or even unknown people with common interests. This proliferation of the mobile devices has also been seen as an opportunity for the cyber criminals to deceive smartphone users and steel their money directly or indirectly, respectively, by accessing their bank accounts through the smartphones or by blackmailing them or selling their private data such as photos, credit card data, etc. to third parties. This is usually achieved by installing malware to smartphones masking their malevolent payload as a legitimate application and advertise it to the users with the hope that mobile users will install it in their devices. Thus, any existing application can easily be modified by integrating a malware and then presented it as a legitimate one. In response to this, scientists have proposed a number of malware detection and classification methods using a variety of techniques. Even though, several of them achieve relatively high precision in malware classification, there is still space for improvement. In this paper, we propose a text mining all repeated pattern detection method which uses the decompiled files of an application in order to classify a suspicious application into one of the known malware families. Based on the experimental results using a real malware dataset, the methodology tries to correctly classify (without any misclassification) all randomly selected malware applications of 3 categories with 3 different families each.",
        "link": "https://dl.acm.org/doi/10.1145/3341161.3350841"
    },
    {
        "title": "Comparative Study between Traditional Machine Learning and Deep Learning Approaches for Text Classification",
        "authors": [
            "Cannannore Nidhi Kamath",
            "Syed Saqib Bukhari",
            "Andreas Dengel"
        ],
        "date": "August 2018",
        "source": "DocEng '18: Proceedings of the ACM Symposium on Document Engineering 2018",
        "abstract": "In this contemporaneous world, it is an obligation for any organization working with documents to end up with the insipid task of classifying truckload of documents, which is the nascent stage of venturing into the realm of information retrieval and data mining. But classification of such humongous documents into multiple classes, calls for a lot of time and labor. Hence a system which could classify these documents with acceptable accuracy would be of an unfathomable help in document engineering. We have created multiple classifiers for document classification and compared their accuracy on raw and processed data. We have garnered data used in a corporate organization as well as publicly available data for comparison. Data is processed by removing the stop-words and stemming is implemented to produce root words. Multiple traditional machine learning techniques like Naive Bayes, Logistic Regression, Support Vector Machine, Random forest Classifier and Multi-Layer Perceptron are used for classification of documents. Classifiers are applied on raw and processed data separately and their accuracy is noted. Along with this, Deep learning technique such as Convolution Neural Network is also used to classify the data and its accuracy is compared with that of traditional machine learning techniques. We are also exploring hierarchical classifiers for classification of classes and subclasses. The system classifies the data faster and with better accuracy than if done manually. The results are discussed in the results and evaluation section.",
        "link": "https://dl.acm.org/doi/10.1145/3209280.3209526"
    },
    {
        "title": "Identification of Overpricing in the Purchase of Medication by the Federal Government of Brazil, Using Text Mining and Clustering Based on Ontology",
        "authors": [
            "Marco Aurelio O. S. Correa",
            "Adriano Galindo Leal"
        ],
        "date": "August 2018",
        "source": "ICCBDC '18: Proceedings of the 2018 2nd International Conference on Cloud and Big Data Computing",
        "abstract": "Increasing the transparency level in his actions and spending is one of the primary duties of the Brazilian Federal Government. The creation of laws that oblige full disclose of all its expenditures through transparency portals enables citizens to supervise all government entities. However, only the dissemination of these data, without a definite standard or the availability of data analysis tools, does not guarantee that the citizen is empowered to play his role. Therefore hence, the objective of this work is to identify overprice in the acquisition of products purchased by the federal government of Brazil using the unstructured data available on the Transparency Portal. The last two-years' worth of purchasing data, available in the Transparency Portal, were extracted, processed and stored. Due to his diverse nature and high volume of data, this study focused only on medicines purchased by the Ministry of Health. Ontology-based text mining and clustering techniques were applied for automatic identification and classification of products. The processing of this information was done through text mining and clustering, based on the ontology registered in another database of the Brazilian government. Because of this work, a consolidated price base per medication was created to allow the identification of distortions in prices practised, facilitating the identification of cases that merit further investigation to unravel fraud to the treasury.",
        "link": "https://dl.acm.org/doi/10.1145/3264560.3264569"
    },
    {
        "title": "Computational Estimation by Scientific Data Mining with Classical Methods to Automate Learning Strategies of Scientists",
        "authors": [
            "Aparna S. Varde"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Experimental results are often plotted as 2-dimensional graphical plots (aka graphs) in scientific domains depicting dependent versus independent variables to aid visual analysis of processes. Repeatedly performing laboratory experiments consumes significant time and resources, motivating the need for computational estimation. The goals are to estimate the graph obtained in an experiment given its input conditions, and to estimate the conditions that would lead to a desired graph. Existing estimation approaches often do not meet accuracy and efficiency needs of targeted applications. We develop a computational estimation approach called AutoDomainMine that integrates clustering and classification over complex scientific data in a framework so as to automate classical learning methods of scientists. Knowledge discovered thereby from a database of existing experiments serves as the basis for estimation. Challenges include preserving domain semantics in clustering, finding matching strategies in classification, striking a good balance between elaboration and conciseness while displaying estimation results based on needs of targeted users, and deriving objective measures to capture subjective user interests. These and other challenges are addressed in this work. The AutoDomainMine approach is used to build a computational estimation system, rigorously evaluated with real data in Materials Science. Our evaluation confirms that AutoDomainMine provides desired accuracy and efficiency in computational estimation. It is extendable to other science and engineering domains as proved by adaptation of its sub-processes within fields such as Bioinformatics and Nanotechnology.",
        "link": "https://dl.acm.org/doi/10.1145/3502736"
    },
    {
        "title": "Where is the road for issue reports classification based on text mining?",
        "authors": [
            "Qiang Fan",
            "Yue Yu",
            "Gang Yin",
            "Tao Wang",
            "Huaimin Wang"
        ],
        "date": "November 2017",
        "source": "ESEM '17: Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Currently, open source projects receive various kinds of issues daily, because of the extreme openness of Issue Tracking System (ITS) in GitHub. ITS is a labor-intensive and time-consuming task of issue categorization for project managers. However, a contributor is only required a short textual abstract to report an issue in GitHub. Thus, most traditional classification approaches based on detailed and structured data (e.g., priority, severity, software version and so on) are difficult to adopt. In this paper, issue classification approaches on a large-scale dataset, including 80 popular projects and over 252,000 issue reports collected from GitHub, were investigated. First, four traditional text-based classification methods and their performances were discussed. Semantic perplexity (i.e., an issues description confuses bug-related sentences with nonbug-related sentences) is a crucial factor that affects the classification performances based on quantitative and qualitative study. Finally, A two-stage classifier framework based on the novel metrics of semantic perplexity of issue reports was designed. Results show that our two-stage classification can significantly improve issue classification performances.",
        "link": "https://dl.acm.org/doi/10.1109/ESEM.2017.19"
    },
    {
        "title": "Feature-based Facebook reviews process model for e-management using data mining",
        "authors": [
            "Anish Kumar Varudharajulu",
            "Yongsheng Ma"
        ],
        "date": "January 2019",
        "source": "IC4E '19: Proceedings of the 10th International Conference on E-Education, E-Business, E-Management and E-Learning",
        "abstract": "The data generated from online communication acts as potential gold mines for discovering knowledge for researchers. A large amount of data is also generated in the form of web documents, emails, blogs, and feedback, etc. Text analytics is being significantly employed to mine important information. Opinion mining is the process of extracting human thoughts and perceptions from unstructured texts. The showstopper for designing an opinion mining system for analyzing reviews arise from the fact that customer reviews are often noisy. These reviews are informally written. In addition, they are subjected to spelling mistakes, grammatical errors, improper punctuation and irrational capitalization. This paper focuses on analyzing the different classification and clustering algorithms aimed at extracting and consolidating opinions of customers from social media sites like Facebook, Twitter and through surveys, at multiple levels of granularity to monitor and measure customer satisfaction. Ours is an automated approach, in which the system aids in the process of knowledge assimilation for knowledge-based building and also performs the analytics. Domain experts ratify the knowledge base and also provide training datasets for the system to intuitively gather more instances for ratification. The system identifies opinion expressions as phrases containing opinion words, opinionated features and also opinion modifiers. These expressions are categorized as positive, negative or neutral. Opinion expressions are identified and categorized using localized linguistic techniques. Opinions can be congregated at any desired level of specificity i.e. feature level or product level, user level or service level, etc. We have developed a system based on this approach, which provides the user with a platform to analyze opinion expressions crawled from a set of pre-defined datasets.",
        "link": "https://dl.acm.org/doi/10.1145/3306500.3306514"
    },
    {
        "title": "Discriminative Topic Mining via Category-Name Guided Text Embedding",
        "authors": [
            "Yu Meng",
            "Jiaxin Huang",
            "Guangyuan Wang",
            "Zihan Wang",
            "Chao Zhang",
            "Yu Zhang",
            "Jiawei Han"
        ],
        "date": "April 2020",
        "source": "WWW '20: Proceedings of The Web Conference 2020",
        "abstract": "Mining a set of meaningful and distinctive topics automatically from massive text corpora has broad applications. Existing topic models, however, typically work in a purely unsupervised way, which often generate topics that do not fit users’ particular needs and yield suboptimal performance on downstream tasks. We propose a new task, discriminative topic mining, which leverages a set of user-provided category names to mine discriminative topics from text corpora. This new task not only helps a user understand clearly and distinctively the topics he/she is most interested in, but also benefits directly keyword-driven classification tasks. We develop CatE, a novel category-name guided text embedding method for discriminative topic mining, which effectively leverages minimal user guidance to learn a discriminative embedding space and discover category representative terms in an iterative manner. We conduct a comprehensive set of experiments to show that CatE mines high-quality set of topics guided by category names only, and benefits a variety of downstream applications including weakly-supervised classification and lexical entailment direction identification.",
        "link": "https://dl.acm.org/doi/10.1145/3366423.3380278"
    },
    {
        "title": "Comparative Study of Heart Disease Diagnosis Using Top Ten Data Mining Classification Algorithms",
        "authors": [
            "I. Ketut Agung Enriko"
        ],
        "date": "June 2019",
        "source": "ICFET '19: Proceedings of the 5th International Conference on Frontiers of Educational Technologies",
        "abstract": "Data mining has been used for many purposes, especially for prediction system. In healthcare, data mining algorithms often used in disease diagnosis. Meanwhile, heart disease is known as a primary cause of death over the years. Many studies have been performed in heart disease diagnosis using data mining methods. There are some popular data mining algorithms that can be used in heart disease diagnosis, for example, k-Nearest Neighbor, CART, and AdaBoost. The algorithms are used to analyze a sample of cardiovascular patients data and predict the heart disease type that they suffer. Some parameters are taken from the patient, including EKG morphology, blood pressure, and information about the existence of chest pain, shortness of breath, palpitation, and cold sweat. In this study, medical records data are collected from Harapan Kita Hospital and utilized as a dataset sample in this research. Top ten data mining classification algorithms are used in diagnosing heart disease from Harapan Kita Hospital data and examining their performance by checking the accuracy and speed.",
        "link": "https://dl.acm.org/doi/10.1145/3338188.3338220"
    },
    {
        "title": "Text Mining Approach for Identifying Research Trends",
        "authors": [
            "Snezhana Sulova"
        ],
        "date": "June 2021",
        "source": "CompSysTech '21: Proceedings of the 22nd International Conference on Computer Systems and Technologies",
        "abstract": "With the increase of unstructured data, the issues connected with automatic text processing, the categorization of documents and the discovery of topics have become objects of growing interest. In order to improve the process of grouping and processing research publications, we would like to propose a method based upon natural language processing. It is based on text mining technologies which aim to identify key tendencies in documents. It processes the content of publications by clustering and identifies the topics of each identified group. This analysis helps by identifying key tendencies as well as discovering emerging new areas of research. Publications from the research literature database, Scopus, were used to test the approach. The topic of the publications is “the application of digital technologies in the logistics business”. The experiments were completed using the RapidMiner Studio software.",
        "link": "https://dl.acm.org/doi/10.1145/3472410.3472433"
    },
    {
        "title": "Assessment of Vulnerability Severity using Text Mining",
        "authors": [
            "Georgios Spanos",
            "Lefteris Angelis",
            "Dimitrios Toloudis"
        ],
        "date": "September 2017",
        "source": "PCI '17: Proceedings of the 21st Pan-Hellenic Conference on Informatics",
        "abstract": "Software1 vulnerabilities are closely associated with information systems security, a major and critical field in today's technology. Vulnerabilities constitute a constant and increasing threat for various aspects of everyday life, especially for safety and economy, since the social impact from the problems that they cause is complicated and often unpredictable. Although there is an entire research branch in software engineering that deals with the identification and elimination of vulnerabilities, the growing complexity of software products and the variability of software production procedures are factors contributing to the ongoing occurrence of vulnerabilities, Hence, another area that is being developed in parallel focuses on the study and management of the vulnerabilities that have already been reported and registered in databases. The information contained in such databases includes, a textual description and a number of metrics related to vulnerabilities. The purpose of this paper is to investigate to what extend the assessment of the vulnerability severity can be inferred directly from the corresponding textual description, or in other words, to examine the informative power of the description with respect to the vulnerability severity. For this purpose, text mining techniques, i.e. text analysis and three different classification methods (decision trees, neural networks and support vector machines) were employed. The application of text mining to a sample of 70,678 vulnerabilities from a public data source shows that the description itself is a reliable and highly accurate source of information for vulnerability prioritization.",
        "link": "https://dl.acm.org/doi/10.1145/3139367.3139390"
    },
    {
        "title": "Clustering Algorithms for Spatial Data Mining",
        "authors": [
            "Chetashri Bhadane",
            "Ketan Shah"
        ],
        "date": "April 2020",
        "source": "ICGDA '20: Proceedings of the 2020 3rd International Conference on Geoinformatics and Data Analysis",
        "abstract": "With the advances in mobile and wireless technologies, there has been a rise in applications that track and share the users' geospatial data. People use several social networking sites such as Twitter, Facebook and Flickr, where they share their status updates. With the integration of Global Positioning System (GPS) with mobile phones, it is now possible to share one's locations on these social networks. GPS allows us to record and track a person's movement along with the timestamp. The data set obtained from these GPS logs is vast and is widely used to analyze the users' movement patterns. Specifically, we can find out significant locations based on the number of users present at that location and the time spent by them at such places. Once significant places have been identified, it is also possible to identify the semantic importance of these locations. This paper presents an overview of the clustering techniques used to find important places of interest using large GPS based mobility datasets. Four clustering algorithms, K-Means, DBSCAN, OPTICS and Hierarchical, are implemented, and performance is tested using real-time data of 50 users collected over 2--5 years. Performance summary depicts that K-Means and DBSCAN perform well for spatial data.",
        "link": "https://dl.acm.org/doi/10.1145/3397056.3397068"
    },
    {
        "title": "Feature Selection based Arabic Text Classification using Different Machine Learning Algorithms: Comparative Study",
        "authors": [
            "Sakina Rim Bennabi",
            "Zakaria Elberrichi"
        ],
        "date": "June 2020",
        "source": "ICIST '20: Proceedings of the 10th International Conference on Information Systems and Technologies",
        "abstract": "Feature selection is a method of data pre-processing widely used when mining large data, such as textual classification. Several studies have been conducted to compare the different methods of feature selection applied to corpora in English. Unfortunately, a small number of works concern the Arabic language. This article aims to present a comparative study of different feature selection techniques including: Chi2, the ANOVA method and mutual information, applied on a corpus in Arabic language, while also diversifying the machine learning algorithms (Naive Bayes, SVM and KNN). This experimental study has shown in general that reducing dimensionality with feature selection techniques has slightly affected the performance of textual classification, reducing the size of the corpus by up to 1%.",
        "link": "https://dl.acm.org/doi/10.1145/3447568.3448531"
    },
    {
        "title": "Intelligent Data Analysis using Optimized Support Vector Machine Based Data Mining Approach for Tourism Industry",
        "authors": [
            "Ms Promila Sharma",
            "Uma Meena",
            "Girish Kumar Sharma"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Data analysis involves the deployment of sophisticated approaches from data mining methods, information theory, and artificial intelligence in various fields like tourism, hospitality, and so on for the extraction of knowledge from the gathered and preprocessed data. In tourism, pattern analysis or data analysis using classification is significant for finding the patterns that represent new and potentially useful information or knowledge about the destination and other data. Several data mining techniques are introduced for the classification of data or patterns. However, overfitting, less accuracy, local minima, sensitive to noise are the drawbacks in some existing data mining classification methods. To overcome these challenges, Support vector machine with Red deer optimization (SVM-RDO) based data mining strategy is proposed in this article. Extended Kalman filter (EKF) is utilized in the first phase, i.e., data cleaning to remove the noise and missing values from the input data. Mantaray foraging algorithm (MaFA) is used in the data selection phase, in which the significant data are selected for the further process to reduce the computational complexity. The final phase is the classification, in which SVM-RDO is proposed to access the useful pattern from the selected data. PYTHON is the implementation tool used for the experiment of the proposed model. The experimental analysis is done to show the efficacy of the proposed work. From the experimental results, the proposed SVM-RDO achieved better accuracy, precision, recall, and F1 score than the existing methods for the tourism dataset. Thus, it is showed the effectiveness of the proposed SVM-RDO for pattern analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3494566"
    },
    {
        "title": "Applying text mining to predict learners' cognitive engagement",
        "authors": [
            "Hayati Hind",
            "Mohammed Khalidi Idrissi",
            "Samir Bennani"
        ],
        "date": "October 2017",
        "source": "SCAMS '17: Proceedings of the Mediterranean Symposium on Smart City Application",
        "abstract": "In academic process, engagement represent one of the important success' factor. This paper proposes a predictive system for learners' cognitive engagement based on their online discussion forums participation. In the first level the ontology OWL and the LSA method are used to perform a semantic classification system of the threads according to a specific context chosen by the tutor. Then the Text mining, as a predictive method, is applied to the classified threads and learners' participation in the forums.",
        "link": "https://dl.acm.org/doi/10.1145/3175628.3175655"
    },
    {
        "title": "Investigate Transitions into Drug Addiction through Text Mining of Reddit Data",
        "authors": [
            "John Lu",
            "Sumati Sridhar",
            "Ritika Pandey",
            "Mohammad Al Hasan",
            "Georege Mohler"
        ],
        "date": "July 2019",
        "source": "KDD '19: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Increasing rates of opioid drug abuse and heightened prevalence of online support communities underscore the necessity of employing data mining techniques to better understand drug addiction using these rapidly developing online resources. In this work, we obtained data from Reddit, an online collection of forums, to gather insight into drug use/misuse using text snippets from users narratives. Specifically, using users' posts, we trained a binary classifier which predicts a user's transitions from casual drug discussion forums to drug recovery forums. We also proposed a Cox regression model that outputs likelihoods of such transitions. In doing so, we found that utterances of select drugs and certain linguistic features contained in one's posts can help predict these transitions. Using unfiltered drug-related posts, our research delineates drugs that are associated with higher rates of transitions from recreational drug discussion to support/recovery discussion, offers insight into modern drug culture, and provides tools with potential applications in combating the opioid crisis.",
        "link": "https://dl.acm.org/doi/10.1145/3292500.3330737"
    },
    {
        "title": "Analysing digital banking reviews using text mining",
        "authors": [
            "Li Chen Cheng",
            "Legaspi Rhea Sharmayne"
        ],
        "date": "December 2020",
        "source": "ASONAM '20: Proceedings of the 12th IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Digital banks are new entrants in the banking industry in the Philippines as they only started late 2018. Since then, a handful of players have and are still emerging. With more and more people becoming technologically savvy, it is very critical for financial institutions to develop a digital banking application that will stand out from the competition. This paper aims to use text mining methods to analyse digital banking application reviews. This study will perform topic modelling using LDA to explore customer concerns and will mine association rules between the digital banking features with the review score. The results will reveal which areas the digital banking application can further optimize for customer satisfaction and retention.",
        "link": "https://dl.acm.org/doi/10.1109/ASONAM49781.2020.9381429"
    },
    {
        "title": "Mining Moocs Videos Metadata Using Classification Techniques",
        "authors": [
            "El Harrak Othman",
            "Ghadi Abderrahim",
            "El Bouhdidi Jaber"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "In few years, the internet has become the first source of information for most people, Today MOOCs makes it possible for everyone to access to the education over the world, it's represents an emerging methodology of online teaching and an important development in open education. Due to the rapid development of Moocs and the rapid growth of digital data and video database over the Internet, it is becoming very difficult for learners to browsing and choosing the best training for them. The scientific community has increased the amount of research into new technologies, with a view to improve the digital video utilization: its archiving, indexing, accessibility, acquisition, store and even its process and usability. All these parts of the video utilization entail the necessity of the extraction of all important information of a video, especially in cases of lack of metadata information. Web video mining is retrieving the content using data mining techniques from World Wide Web. There are two approaches for web video mining using traditional image processing (signal processing) and metadata based approach. In this paper, we present the various research makes in this subject and we propose an effective methodology to extract the metadata from Moocs videos and classify them based on the extracted metadata by applying data mining techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090450"
    },
    {
        "title": "Text mining with HathiTrust",
        "authors": [
            "Eleanor Dickson Koehl",
            "Ryan Dubnicek"
        ],
        "date": "June 2019",
        "source": "JCDL '19: Proceedings of the 18th Joint Conference on Digital Libraries",
        "abstract": "This tutorial will introduce attendees to the HathiTrust Research Center and its tools and services for computational text analysis research. HTRC leverages the scope and scale of the HathiTrust Digital Library collection to create opportunities for researchers to perform text data mining on subsets of the corpus. Attendees will develop skills that will allow them to conduct basic text analysis research using HathiTrust data, learn techniques for engaging with scholars on their text data mining research, and explore strategies for computational access to large-scale digital collections.",
        "link": "https://dl.acm.org/doi/10.1109/JCDL.2019.00115"
    },
    {
        "title": "Text mining for plagiarism detection: multivariate pattern detection for recognition of text similarities",
        "authors": [
            "Konstantinos Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2018",
        "source": "ASONAM '18: Proceedings of the 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "The problem of plagiarism the recent years has been intensified by the availability of information in digital form and the accessibility of the electronic libraries through the Internet. As a result, plagiarism detection has been transformed into a big data analytics problem since the number of digital sources is extravagant and a new document needs to be compared with millions of other existing documents. In this paper, a text mining methodology is proposed that can detect all common patterns between a document and the documents in a reference database. The technique is based on a pattern detection algorithm and the corresponding data structure that enables the algorithm to detect all common patterns. The methodology has been applied in a well-defined dataset providing very promising results identifying difficult cases of plagiarism such as technical disguise.",
        "link": "https://dl.acm.org/doi/10.5555/3382225.3382424"
    },
    {
        "title": "Petroleum Engineering Data Text Classification Using Convolutional Neural Network Based Classifier",
        "authors": [
            "Tzuhan Hsu",
            "Yaoqin Zhang"
        ],
        "date": "May 2018",
        "source": "ICMLT '18: Proceedings of the 2018 International Conference on Machine Learning Technologies",
        "abstract": "In this paper, authors introduce a convolutional neural network (CNN) based classification approach for petroleum engineering event. Recently, text classification problem has raised substantial attention in the field of machine learning. The aim of text classification process is to assign correct label to a text on the basis of its content. However, due to the imprecise of feature representation, most of former text classification methods cannot export satisfying result. To solve this problem, authors propose a model based on convolutional neural network. Authors conduct experiments with real world petroleum engineering data text, artificially labelled into 6 categories. Beside purposed CNN-based model, the authors also implement other baseline machine learning approaches: k nearest neighbor, random forest, support vector machines, recurrent neural network, gated recurrent units, long short-term memory. Among all these models, our CNN-based model reach best performance in text classification task. In the last part of this paper, authors compare and analysis their purpose and baseline model.",
        "link": "https://dl.acm.org/doi/10.1145/3231884.3231898"
    },
    {
        "title": "Classifying Indonesian Online Articles as Advertisement Placement Base Using Text Mining",
        "authors": [
            "Nadhira Tasya",
            "Arian Dhini"
        ],
        "date": "July 2017",
        "source": "ICBIM 2017: Proceedings of the International Conference on Business and Information Management",
        "abstract": "Rapid development in technological aspect resulting in growing level of human needs for the latest news, so that emerged a new trend of publishing and accessing news through online media or usually called online journalism. In addition, the number of people who sell and purchase through online sites also continues to increase and this opportunity is utilized by the company and the advertiser by implementing targeted web advertising. However, the high number of articles that have been published and accessed leads to great opportunities for errors in determining where to place the ads. Therefore, it needs a system that can categorize articles accessed by users as the basis of advertisement placement by the company and this classification system can be done by applying the method of Data Mining and Text Mining. This research uses document data in the form of article content that will be categorized into twenty categories of class of advertisement by using Text Mining technique with Support Vector Machine algorithm. The results of this study may be used by companies or advertisers as a basis for placement of ads on selected online media sites.",
        "link": "https://dl.acm.org/doi/10.1145/3134271.3134288"
    },
    {
        "title": "Mining Free-Text Medical Notes for Suicide Risk Assessment",
        "authors": [
            "Marios Adamou",
            "Grigoris Antoniou",
            "Elissavet Greasidou",
            "Vincenzo Lagani",
            "Paulos Charonyktakis",
            "Ioannis Tsamardinos"
        ],
        "date": "July 2018",
        "source": "SETN '18: Proceedings of the 10th Hellenic Conference on Artificial Intelligence",
        "abstract": "Suicide has been considered as an important public health issue for a very long time, and is one of the main causes of death worldwide. Despite suicide prevention strategies being applied, the rate of suicide has not changed substantially over the past decades. Advances in machine learning make it possible to attempt to predict suicide based on the analysis of relevant data to inform clinical practice. This paper reports on findings from the analysis of data of patients who died by suicide in the period 2013-2016 and made use of both structured data and free-text medical notes. We focus on examining various text-mining approaches to support risk assessment. The results show that using advance machine learning and text-mining techniques, it is possible to predict within a specified period which people are most at risk of taking their own life at the time of referral to a mental health service.",
        "link": "https://dl.acm.org/doi/10.1145/3200947.3201020"
    },
    {
        "title": "Integration of data mining techniques in e-learning systems: Clustering Profil of Lerners and Recommender Course System",
        "authors": [
            "A. El Moustamid",
            "E. En-Naimi",
            "J. El Bouhdidi"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "Data mining aims at the extraction of knowledge from large quantities of data by automatic or semi-automatic methods. In the field of education and training e-learning, Data Mining techniques as clustering (The aim of this technique is to identify groups that are similar in some aspect) [21], prediction (is used for developing model where combination of data viewed from different aspects can effect single viewed data. Here, is important to watch how predictors variables are effecting predicted variable)[21], classification are applied in e-learning to improve the level of learners by providing learning according to the learners' requirement) are used to analyze learners profile, to predict and improve student performance. The aim of our project is to develop a system capable of analyzing learners profile and indexing web videos in order to offer learners a rich database with courses that correspond to their levels.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090453"
    },
    {
        "title": "Multi-level mining and visualization of scientific text collections: Exploring a bi-lingual scientific repository",
        "authors": [
            "Pablo Accuosto",
            "Francesco Ronzano",
            "Daniel Ferrés",
            "Horacio Saggion"
        ],
        "date": "December 2017",
        "source": "WOSP 2017: Proceedings of the 6th International Workshop on Mining Scientific Publications",
        "abstract": "We present a system to mine and visualize collections of scientific documents by semantically browsing information extracted from single publications or aggregated throughout corpora of articles. The text mining tool performs deep analysis of document collections allowing the extraction and interpretation of research paper's contents. In addition to the extraction and enrichment of documents with metadata (titles, authors, affiliations, etc), the deep analysis performed comprises semantic interpretation, rhetorical analysis of sentences, triple-based information extraction, and text summarization. The visualization components allow geographical-based exploration of collections, topic-evolution interpretation, and collaborative network analysis among others. The paper presents a case study of a bi-lingual collection in the field of Natural Language Processing (NLP).",
        "link": "https://dl.acm.org/doi/10.1145/3127526.3127529"
    },
    {
        "title": "Hierarchical Topic Mining via Joint Spherical Tree and Text Embedding",
        "authors": [
            "Yu Meng",
            "Yunyi Zhang",
            "Jiaxin Huang",
            "Yu Zhang",
            "Chao Zhang",
            "Jiawei Han"
        ],
        "date": "August 2020",
        "source": "KDD '20: Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Mining a set of meaningful topics organized into a hierarchy is intuitively appealing since topic correlations are ubiquitous in massive text corpora. To account for potential hierarchical topic structures, hierarchical topic models generalize flat topic models by incorporating latent topic hierarchies into their generative modeling process. However, due to their purely unsupervised nature, the learned topic hierarchy often deviates from users' particular needs or interests. To guide the hierarchical topic discovery process with minimal user supervision, we propose a new task, Hierarchical Topic Mining, which takes a category tree described by category names only, and aims to mine a set of representative terms for each category from a text corpus to help a user comprehend his/her interested topics. We develop a novel joint tree and text embedding method along with a principled optimization procedure that allows simultaneous modeling of the category tree structure and the corpus generative process in the spherical space for effective category-representative term discovery. Our comprehensive experiments show that our model, named JoSH, mines a high-quality set of hierarchical topics with high efficiency and benefits weakly-supervised hierarchical text classification tasks.",
        "link": "https://dl.acm.org/doi/10.1145/3394486.3403242"
    },
    {
        "title": "Unsupervised behavioural mining and clustering for malware family identification",
        "authors": [
            "Khanh Huu The Dam",
            "Thomas Given-Wilson",
            "Axel Legay"
        ],
        "date": "March 2021",
        "source": "SAC '21: Proceedings of the 36th Annual ACM Symposium on Applied Computing",
        "abstract": "More accurate and advanced detection and classification of malware requires exploiting program behaviour and not merely syntactic or static features. One approach is to use system call dependency graphs (SCDGs) that represent the program behaviour by interactions with the system, and the relations between these interactions. These SCDGs have been used with supervised learning techniques to very accurately detect and classify malware. This works considers the unsupervised learning challenge of mining for common clusters of behaviour without a priori knowledge. This allows for clustering of similar programs by behaviour, that can then be used for either classification or further analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3412841.3441919"
    },
    {
        "title": "Text Mining for Evaluating Authors' Birth and Death Years",
        "authors": [
            "Dror Moghaz",
            "Yaakov Hacohen-Kerner",
            "Dov Gabbay"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "This article presents a unique method in text and data mining for finding the era, i.e., mining temporal data, in which an anonymous author was living. Finding this era can assist in the examination of a fake document or extracting the time period in which a writer lived. The study and the experiments concern Hebrew, and in some parts, Aramaic and Yiddish rabbinic texts. The rabbinic texts are undated and contain no bibliographic sections, posing an interesting challenge. This work proposes algorithms using key phrases and key words that allow the temporal organization of citations together with linguistic patterns. Based on these key phrases, key words, and the references, we established several types of “Iron-clad,” Heuristic and Greedy rules for estimating the years of birth and death of a writer in an interesting classification task. Experiments were conducted on corpora, including documents authored by 12, 24, and 36 rabbinic writers and demonstrated promising results.",
        "link": "https://dl.acm.org/doi/10.1145/3281631"
    },
    {
        "title": "Transfer Learning to Timed Text Based Video Classification Using CNN",
        "authors": [
            "Zenun Kastrati",
            "Ali Shariq Imran",
            "Arianit Kurti"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "Open educational video resources are gaining popularity with a growing number of massive open online courses (MOOCs). This has created a niche for content providers to adopt effective solutions in automatically organizing and structuring of educational resources for maximum visibility. Recent advances in deep learning techniques are proving useful in managing and classifying resources into appropriate categories. This paper proposes one such convolutional neural network (CNN) model for classifying video lectures in a MOOC setting using a transfer learning approach. The model uses a time-aligned text transcripts corresponding to video lectures from six broader subject categories. Video lectures and their corresponding transcript dataset is gathered from the Coursera MOOC platform. Two different CNN models are proposed: i) CNN based classification using embeddings learned from our MOOC dataset, ii) CNN based classification using transfer learning. Word embeddings generated from two well known state-of-the-art pre-trained models Word2Vec and GloVe, are used in the transfer learning approach for the second case. The proposed CNN models are evaluated using precision, recall, and F1 score and the obtained performance is compared with both conventional and deep learning classifiers. The proposed CNN models have an F1 score improvement of 10-22 percentage points over DNN and conventional classifiers",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326483"
    },
    {
        "title": "Evolutionary role mining in complex networks by ensemble clustering",
        "authors": [
            "Sarvenaz Choobdar",
            "Pedro Ribeiro",
            "Fernando Silva"
        ],
        "date": "April 2017",
        "source": "SAC '17: Proceedings of the Symposium on Applied Computing",
        "abstract": "The structural patterns in the neighborhood of nodes assign unique roles to the nodes. Mining the set of existing roles in a network provides a descriptive profile of the network and draws its general picture. This paper proposes a new method to determine structural roles in a dynamic network based on the current position of nodes and their historic behavior. We develop a temporal ensemble clustering technique to dynamically find groups of nodes, holding similar tempo-structural roles. We compare two weighting functions, based on age and distribution of data, to incorporate temporal behavior of nodes in the role discovery. To evaluate the performance of the proposed method, we assess the results from two points of view: 1) goodness of fit to current structure of the network; 2) consistency with historic data. We conduct the evaluation using different ensemble clustering techniques. The results on real world networks demonstrate that our method can detect tempo-structural roles that simultaneously depict the topology of a network and reflect its dynamics with high accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3019612.3019815"
    },
    {
        "title": "Analyzing used-car web listings via text mining",
        "authors": [
            "Ayhan Demiriz",
            "Fatma Cantaş"
        ],
        "date": "October 2017",
        "source": "IML '17: Proceedings of the 1st International Conference on Internet of Things and Machine Learning",
        "abstract": "Used car trade is one of the major components of the world economies. It is not uncommon to sell a car by placing an internet advertisement irrespective of the geography in these days. A typical content of an advertisement is usually composed of two parts namely the structured and the free text data. The structured data may include some information about the asking price, make, model, year, mileage of the car and the contact info. In most cases, seller may give important clues about the car's current conditions in the free text data where the title (head) of the advertisement can be included as free text too. This paper reports preliminary results from a text mining study conducted on 75K used car internet listings collected from two major car listing web sites in Turkey. As expected, the words and the phrases related to the description of the car are observed to be frequent. The leading concepts in the free text are found to be regarding how to describe the current condition of a car, for example \"no crash history\".",
        "link": "https://dl.acm.org/doi/10.1145/3109761.3109782"
    },
    {
        "title": "Text classification of Diseases Treated by Traditional Chinese Medicine Prescription based on machine learning",
        "authors": [
            "Hanqing Zhao",
            "Bo Han",
            "Chen Li"
        ],
        "date": "September 2020",
        "source": "ISAIMS '20: Proceedings of the 1st International Symposium on Artificial Intelligence in Medical Sciences",
        "abstract": "OBJECTIVE: To explore the application of machine learning in th e identification of diseases treated by traditional Chinese medicine prescriptions. METHODS: Based on the composition of the text document of Chinese medicine prescriptions, the prescriptions were divided into cough, headache and diarrhea. THUTCT were introduced to establish and train two machine learning text classification models, LibSVM and LibLinear, and the prescriptions to be deter mined were put into the model for classification and prediction. R ESULTS: The Precision rate of LibSVM model and Liblinear mod el were 0.7283 and 0.6690. Seven prescriptions were classified an d predicted, and the results were in line with expectations. CONCLUSION: THUCTC has good universality for the content of TCM prescriptions, high classification accuracy and fast testing speed, which is suitable for the text classification and discrimination research of TCM prescriptions for diseases.",
        "link": "https://dl.acm.org/doi/10.1145/3429889.3429895"
    },
    {
        "title": "Evaluation of retweet clustering method classification method using retweets on Twitter without text data",
        "authors": [
            "K. Uchida",
            "F. Toriumi",
            "T. Sakaki"
        ],
        "date": "August 2017",
        "source": "WI '17: Proceedings of the International Conference on Web Intelligence",
        "abstract": "Burst phenomena, which frequently occur on social media, are caused by such social events as flaming on the internet, elections, and natural disasters. To understand people's thoughts and feelings, we must classify their opinions from burst phenomena. Therefore, classification methods that categorize tweets are critical. However, since most classification methods focus on text mining, they cannot group tweets by topics because each tweet has poor linguistic similarities. We used a non-text-based classification method proposed by Baba et al. that groups tweets by topics, even if they have poor linguistic similarities, and verified its validity by comparing it with a text-based classification method in two different evaluations: qualitative and quantitative. In the qualitative evaluation part, we did a questionnaire survey and validated the suitability of the topic clusters created using both the non-and text-based methods. Since evaluating the similarity of every pair of tweets in each topic is difficult, we evaluated the similarity between sampled pairs in the survey and acquired more appropriate topic clustering results using the non-text-based method than the text-based method. In the quantitative evaluation part, we focused on the robustness of each method against data reduction. Many approaches analyze social media data, especially because collecting data from social media is comparatively easy. However, since collecting the whole data of burst phenomena is very costly due to the vast amounts of available social media data, robustness against data reduction is an important index to evaluate classification methods. With the non-text-based method, over 55% of the pairs of tweets in the same cluster were also included in the same cluster even when the data were reduced to 10% in all three of our example cases. In this paper, as a source we focus on Twitter, one of the most popular microblogging services. Using clustering to conduct detailed case analyses, we scrutinized three burst cases that include natural disasters and flaming on the internet and found that a non-text-based method more effectively classified tweets in burst phenomena than a text-based method.",
        "link": "https://dl.acm.org/doi/10.1145/3106426.3106451"
    },
    {
        "title": "Using Spark for Text Mining on Large Scale Liver Cancer Literature",
        "authors": [
            "Ming-Yen Lin",
            "Yu-Ju Lin",
            "Sue-Chen Hsueh"
        ],
        "date": "January 2021",
        "source": "BDET 2021: 2021 the 3rd International Conference on Big Data Engineering and Technology (BDET)",
        "abstract": "Cancer is one of the main causes of death. The number of known cancer to-date is more than one hundred. Liver cancer, ranked after trachea and lung cancer, has long been high in the cancer leading cause in Taiwan and even ranked as second in 2016. The number of scientific articles related to cancer proliferates every year. The number reaches as high as 20 million in PubMed so that discovering useful information from the massive collection is very difficult. In addition, using a single machine to sift through these articles is very time-consuming. Therefore, we present a big data analytic framework using the distributed Apache Spark platform for text mining in PubMed literature. We establish a prediction model for liver cancer articles so that researchers may effectively validate whether an article is related to liver cancer or not. Classification models in Spark MLlib including Linear Support Vector Machines (SVM), Logistic Regression, are used in our experiments. Relevancy to liver cancer is further confirmed by using MeSH (Medical Subject Headings) terms. Logistic regression is about 3 times faster than SVMs and the accuracy of both methods is close to 95% in the experiments using hold-out validation. When max_features is 500 and min_df ≤ 0.1 (or min_df = 1), the accuracy may reach 96%. In the experiments with K-fold cross-validation, the accuracy of SVMs methods is 96%. The experimental results show that that our prediction model may effectively classify liver cancer articles.",
        "link": "https://dl.acm.org/doi/10.1145/3474944.3474958"
    },
    {
        "title": "Automatic topic classification of test cases using text mining at an Android smartphone vendor",
        "authors": [
            "Junji Shimagaki",
            "Yasutaka Kamei",
            "Naoyasu Ubayashi",
            "Abram Hindle"
        ],
        "date": "October 2018",
        "source": "ESEM '18: Proceedings of the 12th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Background: An Android smartphone is an ecosystem of applications, drivers, operating system components, and assets. The volume of the software is large and the number of test cases needed to cover the functionality of an Android system is substantial. Enormous effort has been already taken to properly quantify \"what features and apps were tested and verified?\". This insight is provided by dashboards that summarize test coverage and results per feature. One method to achieve this is to manually tag or label test cases with the topic or function they cover, much like function points. At the studied Android smartphone vendor, tests are labelled with manually defined tags, so-called \"feature labels (FLs)\", and the FLs serve to categorize 100s to 1000s test cases into 10 to 50 groups. Aim: Unfortunately for developers, manual assignment of FLs to 1000s of test cases is a time consuming task, leading to inaccurately labeled test cases, which will render the dashboard useless. We created an automated system that suggests tags/labels to the developers for their test cases rather than manual labeling. Method: We use machine learning models to predict and label the functionality tested by 10,000 test cases developed at the company. Results: Through the quantitative experiments, our models achieved acceptable F-1 performance of 0.3 to 0.88. Also through the qualitative studies with expert teams, we showed that the hierarchy and path of tests was a good predictor of a feature's label. Conclusions: We find that this method can reduce tedious manual effort that software developers spent classifying test cases, while providing more accurate classification results.",
        "link": "https://dl.acm.org/doi/10.1145/3239235.3268927"
    },
    {
        "title": "To apply Data Mining for Classification of Crowd sourced Software Requirements",
        "authors": [
            "Soonh Taj",
            "Qasim Arain",
            "Imran Memon",
            "Asma Zubedi"
        ],
        "date": "April 2019",
        "source": "ICSIE '19: Proceedings of the 8th International Conference on Software and Information Engineering",
        "abstract": "Now a day's main focus of developers is to build quality software that works according to customer needs and for this reason it is necessary to gather right requirements as requirement elicitation is the critical step that impacts on the success of software project as misinterpreted requirements leads to the failure of software project. By keeping this in mind a research is carried out on improving requirements elicitation process and automating the process of classifying requirements. In this research, a model is proposed which will help in this scenario for requirements elicitation and requirement classification. This paper presents a model in which crowd sourcing approach is used so that customers, end users, stakeholders, developers and software engineers can make active participation for requirement elicitation process and requirements gathered using crowdsourcing approach are used by model for classification process i.e. classification of requirements into functional and non-functional requirements. For the proof of proposed model a case study is conducted. Results of case study provided the usefulness and efficiency of proposed model for classification of crowd sourced software requirements.",
        "link": "https://dl.acm.org/doi/10.1145/3328833.3328837"
    },
    {
        "title": "Text classification using Fuzzy TF-IDF and Machine Learning Models",
        "authors": [
            "Mariem Bounabi",
            "Karim El Moutaouakil",
            "Khalid Satori"
        ],
        "date": "October 2019",
        "source": "BDIoT'19: Proceedings of the 4th International Conference on Big Data and Internet of Things",
        "abstract": "The representation of the information has an important impact on the text classification task. Several weighting methods were proposed in the literature, and the term frequency-inverse term frequency (TFIDF), the most know on the text treatment field. The FTF-IDF is a vector representation where the components of the TFIDF are presented as inputs to the Fuzzy Inference System (FIS). In this work, we compare several Machin Learning algorithms such as Naïve Bayes and its derivatives, SVM and Random forest classifiers, using the FTF-IDF representation. To improve the quality of the used classifiers, we call sum attribute selection methods. The recognition rate, for the tested systems, is satisfied, where the system based on naïve Bayes classifier, the FTF-IDF weighting terms, and the info gain select attributes method gives 98.7% as accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3372938.3372956"
    },
    {
        "title": "Text Mining in Cybersecurity: A Systematic Literature Review",
        "authors": [
            "Luciano Ignaczak",
            "Guilherme Goldschmidt",
            "Cristiano André Da Costa",
            "Rodrigo Da Rosa Righi"
        ],
        "date": null,
        "source": "ACM Computing Surveys",
        "abstract": "The growth of data volume has changed cybersecurity activities, demanding a higher level of automation. In this new cybersecurity landscape, text mining emerged as an alternative to improve the efficiency of the activities involving unstructured data. This article proposes a Systematic Literature Review (SLR) to present the application of text mining in the cybersecurity domain. Using a systematic protocol, we identified 2,196 studies, out of which 83 were summarized. As a contribution, we propose a taxonomy to demonstrate the different activities in the cybersecurity domain supported by text mining. We also detail the strategies evaluated in the application of text mining tasks and the use of neural networks to support activities involving unstructured data. The work also discusses text classification performance aiming its application in real-world solutions. The SLR also highlights open gaps for future research, such as the analysis of non-English content and the intensification in the usage of neural networks.",
        "link": "https://dl.acm.org/doi/10.1145/3462477"
    },
    {
        "title": "Using Text Mining to Analyze the Financial Patents",
        "authors": [
            "Yung-Feng Lu",
            "Jia-Lang Xu",
            "Mu-Yen Chen"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "With the advancement and development of science and technology, Nowadays society is the development of information technology, cloud computing, mobile Internet, Internet of Things, block chain and information technology.The combination of finance and technology has made financial technology terms gradually emerging.. This research analyzes the patents of financial technology. However, Taiwan's financial technology patents are mostly G06Q-20 (payment plan, architecture or agreement), G06Q-30 (commercial, such as marketing, shopping, payment, auction or e-commerce). G06Q-40 (financial, investment or tax treatment, insurance).The research use text mining technology to analyze and understand Taiwan's trends and developments in financial technology. The research results show that G06Q-20 patents are biased towards information security and mobile payment. G06Q-30 patents favor shopping websites and advertising. G06Q-40 patents are biased towards diversity, such as securities, insurance, finance, and stocks.",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326478"
    },
    {
        "title": "Identifying Online Profiles of Distance Learning Students Using Data Mining Techniques",
        "authors": [
            "Osama Islam",
            "Muazzam Siddiqui",
            "Naif Radi Aljohani"
        ],
        "date": "October 2019",
        "source": "ICDTE '19: Proceedings of the 3rd International Conference on Digital Technology in Education",
        "abstract": "Educational data has grown over the years with the increased use of technology within educational environments. This has led to a huge amount of data being stored in various data sources representing the student, his/her activities, and other aspects relevant to the learning process. To meet this analytical need, Educational Data Mining (EDM) has emerged to assist educational institutions in identifying key benefits such as students at risk, the level of student engagement or predicting student performance. The aim of this research was to explore the various aspects of student interaction data using data mining techniques to identify relevant patterns of behaviors and possible key attributes that have higher degrees of influence on distance learning students. The main findings identified several patterns of user online profiles based on a set of adopted learning strategies, the research proposed a framework for analyzing such interaction data based on R and Hadoop platforms to correlate online profiles with student performance.",
        "link": "https://dl.acm.org/doi/10.1145/3369199.3369249"
    },
    {
        "title": "Extremely Fast Decision Tree Mining for Evolving Data Streams",
        "authors": [
            "Albert Bifet",
            "Jiajin Zhang",
            "Wei Fan",
            "Cheng He",
            "Jianfeng Zhang",
            "Jianfeng Qian",
            "Geoff Holmes",
            "Bernhard Pfahringer"
        ],
        "date": "August 2017",
        "source": "KDD '17: Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
        "abstract": "Nowadays real-time industrial applications are generating a huge amount of data continuously every day. To process these large data streams, we need fast and efficient methodologies and systems. A useful feature desired for data scientists and analysts is to have easy to visualize and understand machine learning models. Decision trees are preferred in many real-time applications for this reason, and also, because combined in an ensemble, they are one of the most powerful methods in machine learning. In this paper, we present a new system called STREAMDM-C++, that implements decision trees for data streams in C++, and that has been used extensively at Huawei. Streaming decision trees adapt to changes on streams, a huge advantage since standard decision trees are built using a snapshot of data, and can not evolve over time. STREAMDM-C++ is easy to extend, and contains more powerful ensemble methods, and a more efficient and easy to use adaptive decision trees. We compare our new implementation with VFML, the current state of the art implementation in C, and show how our new system outperforms VFML in speed using less resources.",
        "link": "https://dl.acm.org/doi/10.1145/3097983.3098139"
    },
    {
        "title": "Data miners' little helper: data transformation activity cues for cluster analysis on document collections",
        "authors": [
            "Tania Cerquitelli",
            "Evelina Di Corso",
            "Francesco Ventura",
            "Silvia Chiusano"
        ],
        "date": "June 2017",
        "source": "WIMS '17: Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "In this paper we propose a new self-learning engine to streamline the analytics process, as it enables analysts to mine massive data repositories with minimal user intervention. In the context of cluster analysis on a collection of documents this new system, named SELF-DATA (SELF-learning DAta TrAnsformation), suggests to the analyst how to configure the whole mining process for a given dataset. SELF-DATA relies on an engine exploring different data weighting schemas (e.g., normalized term frequencies) and data transformation methods (e.g., PCA) before applying the cluster analysis, evaluating and comparing solutions through different quality indices (e.g., weighted Silhouette), and presenting the k-top solutions to the analyst. SELF-DATA will also include a knowledge base storing results of experiments on previously processed datasets, and a classification algorithm trained on the knowledge base content to forecast the best configuration for the whole mining process for an unexplored dataset. The first development of SELF-DATA running on Apache Spark has been validated on 5 collections of documents. Experimental results highlight that TF-IDF and logarithmic entropy are effective to measure item relevance with sparse datasets, and the LSI method outperforms PCA with a large dictionary.",
        "link": "https://dl.acm.org/doi/10.1145/3102254.3102288"
    },
    {
        "title": "Interactive Visual Graph Mining and Learning",
        "authors": [
            "Ryan A. Rossi",
            "Nesreen K. Ahmed",
            "Rong Zhou",
            "Hoda Eldardiry"
        ],
        "date": null,
        "source": "ACM Transactions on Intelligent Systems and Technology",
        "abstract": "This article presents a platform for interactive graph mining and relational machine learning called GraphVis. The platform combines interactive visual representations with state-of-the-art graph mining and relational machine learning techniques to aid in revealing important insights quickly as well as learning an appropriate and highly predictive model for a particular task (e.g., classification, link prediction, discovering the roles of nodes, and finding influential nodes). Visual representations and interaction techniques and tools are developed for simple, fast, and intuitive real-time interactive exploration, mining, and modeling of graph data. In particular, we propose techniques for interactive relational learning (e.g., node/link classification), interactive link prediction and weighting, role discovery and community detection, higher-order network analysis (via graphlets, network motifs), among others. GraphVis also allows for the refinement and tuning of graph mining and relational learning methods for specific application domains and constraints via an end-to-end interactive visual analytic pipeline that learns, infers, and provides rapid interactive visualization with immediate feedback at each change/prediction in real-time. Other key aspects include interactive filtering, querying, ranking, manipulating, exporting, as well as tools for dynamic network analysis and visualization, interactive graph generators (including new block model approaches), and a variety of multi-level network analysis techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3200764"
    },
    {
        "title": "Data Mining and Opinion Mining: A Tool in Educational Context",
        "authors": [
            "Myriam Peñafiel",
            "Stefanie Vásquez",
            "Diego Vásquez",
            "Juan Zaldumbide",
            "Sergio Luján-Mora"
        ],
        "date": "July 2018",
        "source": "ICoMS '18: Proceedings of the 2018 1st International Conference on Mathematics and Statistics",
        "abstract": "The use of the web as a universal communication platform generates large volumes of data (Big data), which in many cases, need to be processed so that they can become useful knowledge in face of the sceptics who have doubts about the credibility of such information. The use of web data that comes from educational contexts needs to be addressed, since that large amount of unstructured information is not being valued, losing valuable information that can be used. To solve this problem, we propose the use of data mining techniques such as sentiment analysis to validate the information that comes from the educational platforms. The objective of this research is to propose a methodology that allows the user to apply sentiment analysis in a simple way, because although some researchers have done it, very few do with data in the educational context. The results obtained prove that the proposal can be used in similar cases.",
        "link": "https://dl.acm.org/doi/10.1145/3274250.3274263"
    },
    {
        "title": "Research Hotspots Mining and Visualized Analysis Based on Linking Cluster and K-Core Decomposition",
        "authors": [
            "Ying Cai",
            "Fang Huang",
            "Shuai Wang",
            "Hao Zhang",
            "Chunxiu Du"
        ],
        "date": "May 2018",
        "source": "ICDPA 2018: Proceedings of the International Conference on Data Processing and Applications",
        "abstract": "In this paper, we focus on research hotspots mining with knowledge semantic features on discipline topic word networks, which are extracted from scientific and technical documents. For the topic networks, a novel method of combining linking community clustering with k-core decomposition is proposed to mine the hotspot community with knowledge hierarchy structure. And as a post-processing, the hierarchical community identification is implemented by k-core decomposition in order to discover the internal connection with knowledge semantic hierarchy in the research hotspot. Finally, the community density and visualization method is utilized to analyze the core and sub- problem in the research hotspot community. In experiments, the topic networks for a single discipline and interdisciplinary research hotspot are taken as examples of analysis, and the results show the proposed approach can effectively realize the objective of mining and analyzing the discipline research hotspot on the academic topic word network.",
        "link": "https://dl.acm.org/doi/10.1145/3224207.3224216"
    },
    {
        "title": "The Application of Data Mining Algorithms in the Construction of Travel Recommendation System",
        "authors": [
            "Chao Lou",
            "Zhaonan Mu",
            "Mengzhu Liu"
        ],
        "date": "January 2021",
        "source": "CONF-CDS 2021: The 2nd International Conference on Computing and Data Science",
        "abstract": "Photos shared by users of online social network often associated with tags, texts, geographic and time information. These data are ideal research resource which can be used by researchers to observe people's interests and behaviors. They can also be used to analyze people's travel behaviors and interests because it can reflect people's travel activities and experiences directly. This project aimed to consolidate people into groups based on their travel preferences of travel season, travel time span, scenery type of travel location and then analyze popular travel places for different people groups. The travel preferences were analyzed from extracted data applying PHP language and Naive Bayes classification approach. According to the analyzed data, people were divided into groups. Then, DBSCAN clustering was used to get popular locations based on the different groups. As a result, 80 thousand photo data of 5 cities have been collected from Flickr by Phython. It has been showed that people's preference of season in most of the mined cities was spring and summer and the preference of time span was short. Moreover, mined people has been divided into 16 groups according to their preference and based on the groups popular locations have been calculated by DBSCAN clustering. Theses locations have been provided in a recommendation website which was built in this project. For the future work, mining Flickr data associated with other travel social network sites could be a good way to build a more effective recommendation system.",
        "link": "https://dl.acm.org/doi/10.1145/3448734.3450477"
    },
    {
        "title": "Compare Machine Learning Models in Text Classification Using Steam User Reviews",
        "authors": [
            "Youchen Miao",
            "Zeyu Jin",
            "Yumeng Zhang",
            "Yuchen Chen",
            "Junren Lai"
        ],
        "date": "November 2021",
        "source": "ICSED '21: Proceedings of the 2021 3rd International Conference on Software Engineering and Development",
        "abstract": "Text Classification and Sentiment Analysis of game reviews are viewed as important parts in not only academic fields but also in game studies. In this paper, with more than 400 thousand game reviews on Steam platform, we preprocess the data using different libraries (sklearn, nltk, and spaCy) and use them as inputs to build three sentiment classification models based on different algorithms (Naive Bayes, SVM, and Random Forest). In contrast to previous studies that only focus on different sentiment analysis models, our paper also highlights the use of different APIs to preprocess the data and their corresponding model performance. The results show that no matter which API we choose, Random Forest models always perform the best. However, in terms of training time, Naive Bayes is the fastest. This work can be used to apply grid search for researchers to automatically find the optimum API before conducting sentiment analysis in the future.",
        "link": "https://dl.acm.org/doi/10.1145/3507473.3507480"
    },
    {
        "title": "Lost in Transduction: Transductive Transfer Learning in Text Classification",
        "authors": [
            "Alejandro Moreo",
            "Andrea Esuli",
            "Fabrizio Sebastiani"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Obtaining high-quality labelled data for training a classifier in a new application domain is often costly. Transfer Learning\n(a.k.a. “Inductive Transfer”) tries to alleviate these costs by transferring, to the “target” domain of interest, knowledge available from a different “source” domain. In transfer learning the lack of labelled information from the target domain is compensated by the availability at training time of a set of unlabelled examples from the target distribution. Transductive Transfer Learning denotes the transfer learning setting in which the only set of target documents that we are interested in classifying is known and available at training time. Although this definition is indeed in line with Vapnik’s original definition of “transduction”, current terminology in the field is confused. In this article, we discuss how the term “transduction” has been misused in the transfer learning literature, and propose a clarification consistent with the original characterization of this term given by Vapnik. We go on to observe that the above terminology misuse has brought about misleading experimental comparisons, with inductive transfer learning methods that have been incorrectly compared with transductive transfer learning methods. We then, give empirical evidence that the difference in performance between the inductive version and the transductive version of a transfer learning method can indeed be statistically significant (i.e., that knowing at training time the only data one needs to classify indeed gives an advantage). Our clarification allows a reassessment of the field, and of the relative merits of the major, state-of-the-art algorithms for transfer learning in text classification.",
        "link": "https://dl.acm.org/doi/10.1145/3453146"
    },
    {
        "title": "Machine Learning on Mining Potential Adverse Drug Reactions for Pharmacovigilance",
        "authors": [
            "Ke Jia"
        ],
        "date": "July 2021",
        "source": "DSIT 2021: 2021 4th International Conference on Data Science and Information Technology",
        "abstract": "Drugs are related to the lives and health of patients. Mining and discovering potential adverse drug reactions from a large number of unstructured texts play an important role in drug research and pharmacovigilance. It is a hot research issue in the field of biomedicine in recent years. A large number of scholars have discovered a large amount of information on adverse drug reactions from different data sources through effective machine learning methods. This paper reviews the research progress in recent years to provide a reference for future research.",
        "link": "https://dl.acm.org/doi/10.1145/3478905.3478964"
    },
    {
        "title": "TACAM: Topic And Context Aware Argument Mining",
        "authors": [
            "Michael Fromm",
            "Evgeniy Faerman",
            "Thomas Seidl"
        ],
        "date": "October 2019",
        "source": "WI '19: IEEE/WIC/ACM International Conference on Web Intelligence",
        "abstract": "In this work we address the problem of argument search. The purpose of argument search is the distillation of pro and contra arguments for requested topics from large text corpora. In previous works, the usual approach is to use a standard search engine to extract text parts which are relevant to the given topic and subsequently use an argument recognition algorithm to select arguments from them. The main challenge in the argument recognition task, which is also known as argument mining, is that often sentences containing arguments are structurally similar to purely informative sentences without any stance about the topic. In fact, they only differ semantically. Most approaches use topic or search term information only for the first search step and therefore assume that arguments can be classified independently of a topic. We argue that topic information is crucial for argument mining, since the topic defines the semantic context of an argument. Precisely, we propose different models for the classification of arguments, which take information about a topic of an argument into account. Moreover, to enrich the context of a topic and to let models understand the context of the potential argument better, we integrate information from different external sources such as Knowledge Graphs or pre-trained NLP models. Our evaluation shows that considering topic information, especially in connection with external information, provides a significant performance boost for the argument mining task.",
        "link": "https://dl.acm.org/doi/10.1145/3350546.3352506"
    },
    {
        "title": "Text mining for incoming tasks based on the urgency/importance factors and task classification using machine learning tools",
        "authors": [
            "Yasser Ali Alshehri"
        ],
        "date": "March 2020",
        "source": "ICCDA 2020: Proceedings of the 2020 the 4th International Conference on Compute and Data Analysis",
        "abstract": "In workplaces, there is a massive amount of unstructured data from different sources. In this paper, we present a case study that explains how can through communications between employees, we can help to prioritize tasks requests to increase the efficiency of their works for both technical and non-technical workers. This involves managing daily incoming tasks based on their level of urgency and importance.To allow all workers to utilize the urgency-importance matrix as a time-management tool, we need to automate this tool. The textual content of incoming tasks are analyzed, and metrics related to urgency and importance are extracted. A third factor (i.e., the response variable) is defined based on the two input variables (urgency and importance). Then, machine learning applied to the data to predict the class of incoming tasks based on data outcome desired. We used ordinal regression, neural networks, and decision tree algorithms to predict the four levels of task priority. We measure the performance of all using recalls, precisions, and F-scores. All classifiers perform higher than 89% in terms of all measures.",
        "link": "https://dl.acm.org/doi/10.1145/3388142.3388153"
    },
    {
        "title": "Text mining for malware classification using multivariate all repeated patterns detection",
        "authors": [
            "Konstantinos F. Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2019",
        "source": "ASONAM '19: Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Mobile phones have become nowadays a commodity to the majority of people. Using them, people are able to access the world of Internet and connect with their friends, their colleagues at work or even unknown people with common interests. This proliferation of the mobile devices has also been seen as an opportunity for the cyber criminals to deceive smartphone users and steel their money directly or indirectly, respectively, by accessing their bank accounts through the smartphones or by blackmailing them or selling their private data such as photos, credit card data, etc. to third parties. This is usually achieved by installing malware to smartphones masking their malevolent payload as a legitimate application and advertise it to the users with the hope that mobile users will install it in their devices. Thus, any existing application can easily be modified by integrating a malware and then presented it as a legitimate one. In response to this, scientists have proposed a number of malware detection and classification methods using a variety of techniques. Even though, several of them achieve relatively high precision in malware classification, there is still space for improvement. In this paper, we propose a text mining all repeated pattern detection method which uses the decompiled files of an application in order to classify a suspicious application into one of the known malware families. Based on the experimental results using a real malware dataset, the methodology tries to correctly classify (without any misclassification) all randomly selected malware applications of 3 categories with 3 different families each.",
        "link": "https://dl.acm.org/doi/10.1145/3341161.3350841"
    },
    {
        "title": "Comparative Study between Traditional Machine Learning and Deep Learning Approaches for Text Classification",
        "authors": [
            "Cannannore Nidhi Kamath",
            "Syed Saqib Bukhari",
            "Andreas Dengel"
        ],
        "date": "August 2018",
        "source": "DocEng '18: Proceedings of the ACM Symposium on Document Engineering 2018",
        "abstract": "In this contemporaneous world, it is an obligation for any organization working with documents to end up with the insipid task of classifying truckload of documents, which is the nascent stage of venturing into the realm of information retrieval and data mining. But classification of such humongous documents into multiple classes, calls for a lot of time and labor. Hence a system which could classify these documents with acceptable accuracy would be of an unfathomable help in document engineering. We have created multiple classifiers for document classification and compared their accuracy on raw and processed data. We have garnered data used in a corporate organization as well as publicly available data for comparison. Data is processed by removing the stop-words and stemming is implemented to produce root words. Multiple traditional machine learning techniques like Naive Bayes, Logistic Regression, Support Vector Machine, Random forest Classifier and Multi-Layer Perceptron are used for classification of documents. Classifiers are applied on raw and processed data separately and their accuracy is noted. Along with this, Deep learning technique such as Convolution Neural Network is also used to classify the data and its accuracy is compared with that of traditional machine learning techniques. We are also exploring hierarchical classifiers for classification of classes and subclasses. The system classifies the data faster and with better accuracy than if done manually. The results are discussed in the results and evaluation section.",
        "link": "https://dl.acm.org/doi/10.1145/3209280.3209526"
    },
    {
        "title": "Identification of Overpricing in the Purchase of Medication by the Federal Government of Brazil, Using Text Mining and Clustering Based on Ontology",
        "authors": [
            "Marco Aurelio O. S. Correa",
            "Adriano Galindo Leal"
        ],
        "date": "August 2018",
        "source": "ICCBDC '18: Proceedings of the 2018 2nd International Conference on Cloud and Big Data Computing",
        "abstract": "Increasing the transparency level in his actions and spending is one of the primary duties of the Brazilian Federal Government. The creation of laws that oblige full disclose of all its expenditures through transparency portals enables citizens to supervise all government entities. However, only the dissemination of these data, without a definite standard or the availability of data analysis tools, does not guarantee that the citizen is empowered to play his role. Therefore hence, the objective of this work is to identify overprice in the acquisition of products purchased by the federal government of Brazil using the unstructured data available on the Transparency Portal. The last two-years' worth of purchasing data, available in the Transparency Portal, were extracted, processed and stored. Due to his diverse nature and high volume of data, this study focused only on medicines purchased by the Ministry of Health. Ontology-based text mining and clustering techniques were applied for automatic identification and classification of products. The processing of this information was done through text mining and clustering, based on the ontology registered in another database of the Brazilian government. Because of this work, a consolidated price base per medication was created to allow the identification of distortions in prices practised, facilitating the identification of cases that merit further investigation to unravel fraud to the treasury.",
        "link": "https://dl.acm.org/doi/10.1145/3264560.3264569"
    },
    {
        "title": "Computational Estimation by Scientific Data Mining with Classical Methods to Automate Learning Strategies of Scientists",
        "authors": [
            "Aparna S. Varde"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Experimental results are often plotted as 2-dimensional graphical plots (aka graphs) in scientific domains depicting dependent versus independent variables to aid visual analysis of processes. Repeatedly performing laboratory experiments consumes significant time and resources, motivating the need for computational estimation. The goals are to estimate the graph obtained in an experiment given its input conditions, and to estimate the conditions that would lead to a desired graph. Existing estimation approaches often do not meet accuracy and efficiency needs of targeted applications. We develop a computational estimation approach called AutoDomainMine that integrates clustering and classification over complex scientific data in a framework so as to automate classical learning methods of scientists. Knowledge discovered thereby from a database of existing experiments serves as the basis for estimation. Challenges include preserving domain semantics in clustering, finding matching strategies in classification, striking a good balance between elaboration and conciseness while displaying estimation results based on needs of targeted users, and deriving objective measures to capture subjective user interests. These and other challenges are addressed in this work. The AutoDomainMine approach is used to build a computational estimation system, rigorously evaluated with real data in Materials Science. Our evaluation confirms that AutoDomainMine provides desired accuracy and efficiency in computational estimation. It is extendable to other science and engineering domains as proved by adaptation of its sub-processes within fields such as Bioinformatics and Nanotechnology.",
        "link": "https://dl.acm.org/doi/10.1145/3502736"
    },
    {
        "title": "Where is the road for issue reports classification based on text mining?",
        "authors": [
            "Qiang Fan",
            "Yue Yu",
            "Gang Yin",
            "Tao Wang",
            "Huaimin Wang"
        ],
        "date": "November 2017",
        "source": "ESEM '17: Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Currently, open source projects receive various kinds of issues daily, because of the extreme openness of Issue Tracking System (ITS) in GitHub. ITS is a labor-intensive and time-consuming task of issue categorization for project managers. However, a contributor is only required a short textual abstract to report an issue in GitHub. Thus, most traditional classification approaches based on detailed and structured data (e.g., priority, severity, software version and so on) are difficult to adopt. In this paper, issue classification approaches on a large-scale dataset, including 80 popular projects and over 252,000 issue reports collected from GitHub, were investigated. First, four traditional text-based classification methods and their performances were discussed. Semantic perplexity (i.e., an issues description confuses bug-related sentences with nonbug-related sentences) is a crucial factor that affects the classification performances based on quantitative and qualitative study. Finally, A two-stage classifier framework based on the novel metrics of semantic perplexity of issue reports was designed. Results show that our two-stage classification can significantly improve issue classification performances.",
        "link": "https://dl.acm.org/doi/10.1109/ESEM.2017.19"
    },
    {
        "title": "Feature-based Facebook reviews process model for e-management using data mining",
        "authors": [
            "Anish Kumar Varudharajulu",
            "Yongsheng Ma"
        ],
        "date": "January 2019",
        "source": "IC4E '19: Proceedings of the 10th International Conference on E-Education, E-Business, E-Management and E-Learning",
        "abstract": "The data generated from online communication acts as potential gold mines for discovering knowledge for researchers. A large amount of data is also generated in the form of web documents, emails, blogs, and feedback, etc. Text analytics is being significantly employed to mine important information. Opinion mining is the process of extracting human thoughts and perceptions from unstructured texts. The showstopper for designing an opinion mining system for analyzing reviews arise from the fact that customer reviews are often noisy. These reviews are informally written. In addition, they are subjected to spelling mistakes, grammatical errors, improper punctuation and irrational capitalization. This paper focuses on analyzing the different classification and clustering algorithms aimed at extracting and consolidating opinions of customers from social media sites like Facebook, Twitter and through surveys, at multiple levels of granularity to monitor and measure customer satisfaction. Ours is an automated approach, in which the system aids in the process of knowledge assimilation for knowledge-based building and also performs the analytics. Domain experts ratify the knowledge base and also provide training datasets for the system to intuitively gather more instances for ratification. The system identifies opinion expressions as phrases containing opinion words, opinionated features and also opinion modifiers. These expressions are categorized as positive, negative or neutral. Opinion expressions are identified and categorized using localized linguistic techniques. Opinions can be congregated at any desired level of specificity i.e. feature level or product level, user level or service level, etc. We have developed a system based on this approach, which provides the user with a platform to analyze opinion expressions crawled from a set of pre-defined datasets.",
        "link": "https://dl.acm.org/doi/10.1145/3306500.3306514"
    },
    {
        "title": "Discriminative Topic Mining via Category-Name Guided Text Embedding",
        "authors": [
            "Yu Meng",
            "Jiaxin Huang",
            "Guangyuan Wang",
            "Zihan Wang",
            "Chao Zhang",
            "Yu Zhang",
            "Jiawei Han"
        ],
        "date": "April 2020",
        "source": "WWW '20: Proceedings of The Web Conference 2020",
        "abstract": "Mining a set of meaningful and distinctive topics automatically from massive text corpora has broad applications. Existing topic models, however, typically work in a purely unsupervised way, which often generate topics that do not fit users’ particular needs and yield suboptimal performance on downstream tasks. We propose a new task, discriminative topic mining, which leverages a set of user-provided category names to mine discriminative topics from text corpora. This new task not only helps a user understand clearly and distinctively the topics he/she is most interested in, but also benefits directly keyword-driven classification tasks. We develop CatE, a novel category-name guided text embedding method for discriminative topic mining, which effectively leverages minimal user guidance to learn a discriminative embedding space and discover category representative terms in an iterative manner. We conduct a comprehensive set of experiments to show that CatE mines high-quality set of topics guided by category names only, and benefits a variety of downstream applications including weakly-supervised classification and lexical entailment direction identification.",
        "link": "https://dl.acm.org/doi/10.1145/3366423.3380278"
    },
    {
        "title": "Comparative Study of Heart Disease Diagnosis Using Top Ten Data Mining Classification Algorithms",
        "authors": [
            "I. Ketut Agung Enriko"
        ],
        "date": "June 2019",
        "source": "ICFET '19: Proceedings of the 5th International Conference on Frontiers of Educational Technologies",
        "abstract": "Data mining has been used for many purposes, especially for prediction system. In healthcare, data mining algorithms often used in disease diagnosis. Meanwhile, heart disease is known as a primary cause of death over the years. Many studies have been performed in heart disease diagnosis using data mining methods. There are some popular data mining algorithms that can be used in heart disease diagnosis, for example, k-Nearest Neighbor, CART, and AdaBoost. The algorithms are used to analyze a sample of cardiovascular patients data and predict the heart disease type that they suffer. Some parameters are taken from the patient, including EKG morphology, blood pressure, and information about the existence of chest pain, shortness of breath, palpitation, and cold sweat. In this study, medical records data are collected from Harapan Kita Hospital and utilized as a dataset sample in this research. Top ten data mining classification algorithms are used in diagnosing heart disease from Harapan Kita Hospital data and examining their performance by checking the accuracy and speed.",
        "link": "https://dl.acm.org/doi/10.1145/3338188.3338220"
    },
    {
        "title": "Text Mining Approach for Identifying Research Trends",
        "authors": [
            "Snezhana Sulova"
        ],
        "date": "June 2021",
        "source": "CompSysTech '21: Proceedings of the 22nd International Conference on Computer Systems and Technologies",
        "abstract": "With the increase of unstructured data, the issues connected with automatic text processing, the categorization of documents and the discovery of topics have become objects of growing interest. In order to improve the process of grouping and processing research publications, we would like to propose a method based upon natural language processing. It is based on text mining technologies which aim to identify key tendencies in documents. It processes the content of publications by clustering and identifies the topics of each identified group. This analysis helps by identifying key tendencies as well as discovering emerging new areas of research. Publications from the research literature database, Scopus, were used to test the approach. The topic of the publications is “the application of digital technologies in the logistics business”. The experiments were completed using the RapidMiner Studio software.",
        "link": "https://dl.acm.org/doi/10.1145/3472410.3472433"
    },
    {
        "title": "Assessment of Vulnerability Severity using Text Mining",
        "authors": [
            "Georgios Spanos",
            "Lefteris Angelis",
            "Dimitrios Toloudis"
        ],
        "date": "September 2017",
        "source": "PCI '17: Proceedings of the 21st Pan-Hellenic Conference on Informatics",
        "abstract": "Software1 vulnerabilities are closely associated with information systems security, a major and critical field in today's technology. Vulnerabilities constitute a constant and increasing threat for various aspects of everyday life, especially for safety and economy, since the social impact from the problems that they cause is complicated and often unpredictable. Although there is an entire research branch in software engineering that deals with the identification and elimination of vulnerabilities, the growing complexity of software products and the variability of software production procedures are factors contributing to the ongoing occurrence of vulnerabilities, Hence, another area that is being developed in parallel focuses on the study and management of the vulnerabilities that have already been reported and registered in databases. The information contained in such databases includes, a textual description and a number of metrics related to vulnerabilities. The purpose of this paper is to investigate to what extend the assessment of the vulnerability severity can be inferred directly from the corresponding textual description, or in other words, to examine the informative power of the description with respect to the vulnerability severity. For this purpose, text mining techniques, i.e. text analysis and three different classification methods (decision trees, neural networks and support vector machines) were employed. The application of text mining to a sample of 70,678 vulnerabilities from a public data source shows that the description itself is a reliable and highly accurate source of information for vulnerability prioritization.",
        "link": "https://dl.acm.org/doi/10.1145/3139367.3139390"
    },
    {
        "title": "Clustering Algorithms for Spatial Data Mining",
        "authors": [
            "Chetashri Bhadane",
            "Ketan Shah"
        ],
        "date": "April 2020",
        "source": "ICGDA '20: Proceedings of the 2020 3rd International Conference on Geoinformatics and Data Analysis",
        "abstract": "With the advances in mobile and wireless technologies, there has been a rise in applications that track and share the users' geospatial data. People use several social networking sites such as Twitter, Facebook and Flickr, where they share their status updates. With the integration of Global Positioning System (GPS) with mobile phones, it is now possible to share one's locations on these social networks. GPS allows us to record and track a person's movement along with the timestamp. The data set obtained from these GPS logs is vast and is widely used to analyze the users' movement patterns. Specifically, we can find out significant locations based on the number of users present at that location and the time spent by them at such places. Once significant places have been identified, it is also possible to identify the semantic importance of these locations. This paper presents an overview of the clustering techniques used to find important places of interest using large GPS based mobility datasets. Four clustering algorithms, K-Means, DBSCAN, OPTICS and Hierarchical, are implemented, and performance is tested using real-time data of 50 users collected over 2--5 years. Performance summary depicts that K-Means and DBSCAN perform well for spatial data.",
        "link": "https://dl.acm.org/doi/10.1145/3397056.3397068"
    },
    {
        "title": "Feature Selection based Arabic Text Classification using Different Machine Learning Algorithms: Comparative Study",
        "authors": [
            "Sakina Rim Bennabi",
            "Zakaria Elberrichi"
        ],
        "date": "June 2020",
        "source": "ICIST '20: Proceedings of the 10th International Conference on Information Systems and Technologies",
        "abstract": "Feature selection is a method of data pre-processing widely used when mining large data, such as textual classification. Several studies have been conducted to compare the different methods of feature selection applied to corpora in English. Unfortunately, a small number of works concern the Arabic language. This article aims to present a comparative study of different feature selection techniques including: Chi2, the ANOVA method and mutual information, applied on a corpus in Arabic language, while also diversifying the machine learning algorithms (Naive Bayes, SVM and KNN). This experimental study has shown in general that reducing dimensionality with feature selection techniques has slightly affected the performance of textual classification, reducing the size of the corpus by up to 1%.",
        "link": "https://dl.acm.org/doi/10.1145/3447568.3448531"
    },
    {
        "title": "Intelligent Data Analysis using Optimized Support Vector Machine Based Data Mining Approach for Tourism Industry",
        "authors": [
            "Ms Promila Sharma",
            "Uma Meena",
            "Girish Kumar Sharma"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Data analysis involves the deployment of sophisticated approaches from data mining methods, information theory, and artificial intelligence in various fields like tourism, hospitality, and so on for the extraction of knowledge from the gathered and preprocessed data. In tourism, pattern analysis or data analysis using classification is significant for finding the patterns that represent new and potentially useful information or knowledge about the destination and other data. Several data mining techniques are introduced for the classification of data or patterns. However, overfitting, less accuracy, local minima, sensitive to noise are the drawbacks in some existing data mining classification methods. To overcome these challenges, Support vector machine with Red deer optimization (SVM-RDO) based data mining strategy is proposed in this article. Extended Kalman filter (EKF) is utilized in the first phase, i.e., data cleaning to remove the noise and missing values from the input data. Mantaray foraging algorithm (MaFA) is used in the data selection phase, in which the significant data are selected for the further process to reduce the computational complexity. The final phase is the classification, in which SVM-RDO is proposed to access the useful pattern from the selected data. PYTHON is the implementation tool used for the experiment of the proposed model. The experimental analysis is done to show the efficacy of the proposed work. From the experimental results, the proposed SVM-RDO achieved better accuracy, precision, recall, and F1 score than the existing methods for the tourism dataset. Thus, it is showed the effectiveness of the proposed SVM-RDO for pattern analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3494566"
    },
    {
        "title": "Applying text mining to predict learners' cognitive engagement",
        "authors": [
            "Hayati Hind",
            "Mohammed Khalidi Idrissi",
            "Samir Bennani"
        ],
        "date": "October 2017",
        "source": "SCAMS '17: Proceedings of the Mediterranean Symposium on Smart City Application",
        "abstract": "In academic process, engagement represent one of the important success' factor. This paper proposes a predictive system for learners' cognitive engagement based on their online discussion forums participation. In the first level the ontology OWL and the LSA method are used to perform a semantic classification system of the threads according to a specific context chosen by the tutor. Then the Text mining, as a predictive method, is applied to the classified threads and learners' participation in the forums.",
        "link": "https://dl.acm.org/doi/10.1145/3175628.3175655"
    },
    {
        "title": "Investigate Transitions into Drug Addiction through Text Mining of Reddit Data",
        "authors": [
            "John Lu",
            "Sumati Sridhar",
            "Ritika Pandey",
            "Mohammad Al Hasan",
            "Georege Mohler"
        ],
        "date": "July 2019",
        "source": "KDD '19: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Increasing rates of opioid drug abuse and heightened prevalence of online support communities underscore the necessity of employing data mining techniques to better understand drug addiction using these rapidly developing online resources. In this work, we obtained data from Reddit, an online collection of forums, to gather insight into drug use/misuse using text snippets from users narratives. Specifically, using users' posts, we trained a binary classifier which predicts a user's transitions from casual drug discussion forums to drug recovery forums. We also proposed a Cox regression model that outputs likelihoods of such transitions. In doing so, we found that utterances of select drugs and certain linguistic features contained in one's posts can help predict these transitions. Using unfiltered drug-related posts, our research delineates drugs that are associated with higher rates of transitions from recreational drug discussion to support/recovery discussion, offers insight into modern drug culture, and provides tools with potential applications in combating the opioid crisis.",
        "link": "https://dl.acm.org/doi/10.1145/3292500.3330737"
    },
    {
        "title": "Analysing digital banking reviews using text mining",
        "authors": [
            "Li Chen Cheng",
            "Legaspi Rhea Sharmayne"
        ],
        "date": "December 2020",
        "source": "ASONAM '20: Proceedings of the 12th IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Digital banks are new entrants in the banking industry in the Philippines as they only started late 2018. Since then, a handful of players have and are still emerging. With more and more people becoming technologically savvy, it is very critical for financial institutions to develop a digital banking application that will stand out from the competition. This paper aims to use text mining methods to analyse digital banking application reviews. This study will perform topic modelling using LDA to explore customer concerns and will mine association rules between the digital banking features with the review score. The results will reveal which areas the digital banking application can further optimize for customer satisfaction and retention.",
        "link": "https://dl.acm.org/doi/10.1109/ASONAM49781.2020.9381429"
    },
    {
        "title": "Mining Moocs Videos Metadata Using Classification Techniques",
        "authors": [
            "El Harrak Othman",
            "Ghadi Abderrahim",
            "El Bouhdidi Jaber"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "In few years, the internet has become the first source of information for most people, Today MOOCs makes it possible for everyone to access to the education over the world, it's represents an emerging methodology of online teaching and an important development in open education. Due to the rapid development of Moocs and the rapid growth of digital data and video database over the Internet, it is becoming very difficult for learners to browsing and choosing the best training for them. The scientific community has increased the amount of research into new technologies, with a view to improve the digital video utilization: its archiving, indexing, accessibility, acquisition, store and even its process and usability. All these parts of the video utilization entail the necessity of the extraction of all important information of a video, especially in cases of lack of metadata information. Web video mining is retrieving the content using data mining techniques from World Wide Web. There are two approaches for web video mining using traditional image processing (signal processing) and metadata based approach. In this paper, we present the various research makes in this subject and we propose an effective methodology to extract the metadata from Moocs videos and classify them based on the extracted metadata by applying data mining techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090450"
    },
    {
        "title": "Text mining with HathiTrust",
        "authors": [
            "Eleanor Dickson Koehl",
            "Ryan Dubnicek"
        ],
        "date": "June 2019",
        "source": "JCDL '19: Proceedings of the 18th Joint Conference on Digital Libraries",
        "abstract": "This tutorial will introduce attendees to the HathiTrust Research Center and its tools and services for computational text analysis research. HTRC leverages the scope and scale of the HathiTrust Digital Library collection to create opportunities for researchers to perform text data mining on subsets of the corpus. Attendees will develop skills that will allow them to conduct basic text analysis research using HathiTrust data, learn techniques for engaging with scholars on their text data mining research, and explore strategies for computational access to large-scale digital collections.",
        "link": "https://dl.acm.org/doi/10.1109/JCDL.2019.00115"
    },
    {
        "title": "Text mining for plagiarism detection: multivariate pattern detection for recognition of text similarities",
        "authors": [
            "Konstantinos Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2018",
        "source": "ASONAM '18: Proceedings of the 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "The problem of plagiarism the recent years has been intensified by the availability of information in digital form and the accessibility of the electronic libraries through the Internet. As a result, plagiarism detection has been transformed into a big data analytics problem since the number of digital sources is extravagant and a new document needs to be compared with millions of other existing documents. In this paper, a text mining methodology is proposed that can detect all common patterns between a document and the documents in a reference database. The technique is based on a pattern detection algorithm and the corresponding data structure that enables the algorithm to detect all common patterns. The methodology has been applied in a well-defined dataset providing very promising results identifying difficult cases of plagiarism such as technical disguise.",
        "link": "https://dl.acm.org/doi/10.5555/3382225.3382424"
    },
    {
        "title": "Petroleum Engineering Data Text Classification Using Convolutional Neural Network Based Classifier",
        "authors": [
            "Tzuhan Hsu",
            "Yaoqin Zhang"
        ],
        "date": "May 2018",
        "source": "ICMLT '18: Proceedings of the 2018 International Conference on Machine Learning Technologies",
        "abstract": "In this paper, authors introduce a convolutional neural network (CNN) based classification approach for petroleum engineering event. Recently, text classification problem has raised substantial attention in the field of machine learning. The aim of text classification process is to assign correct label to a text on the basis of its content. However, due to the imprecise of feature representation, most of former text classification methods cannot export satisfying result. To solve this problem, authors propose a model based on convolutional neural network. Authors conduct experiments with real world petroleum engineering data text, artificially labelled into 6 categories. Beside purposed CNN-based model, the authors also implement other baseline machine learning approaches: k nearest neighbor, random forest, support vector machines, recurrent neural network, gated recurrent units, long short-term memory. Among all these models, our CNN-based model reach best performance in text classification task. In the last part of this paper, authors compare and analysis their purpose and baseline model.",
        "link": "https://dl.acm.org/doi/10.1145/3231884.3231898"
    },
    {
        "title": "Classifying Indonesian Online Articles as Advertisement Placement Base Using Text Mining",
        "authors": [
            "Nadhira Tasya",
            "Arian Dhini"
        ],
        "date": "July 2017",
        "source": "ICBIM 2017: Proceedings of the International Conference on Business and Information Management",
        "abstract": "Rapid development in technological aspect resulting in growing level of human needs for the latest news, so that emerged a new trend of publishing and accessing news through online media or usually called online journalism. In addition, the number of people who sell and purchase through online sites also continues to increase and this opportunity is utilized by the company and the advertiser by implementing targeted web advertising. However, the high number of articles that have been published and accessed leads to great opportunities for errors in determining where to place the ads. Therefore, it needs a system that can categorize articles accessed by users as the basis of advertisement placement by the company and this classification system can be done by applying the method of Data Mining and Text Mining. This research uses document data in the form of article content that will be categorized into twenty categories of class of advertisement by using Text Mining technique with Support Vector Machine algorithm. The results of this study may be used by companies or advertisers as a basis for placement of ads on selected online media sites.",
        "link": "https://dl.acm.org/doi/10.1145/3134271.3134288"
    },
    {
        "title": "Mining Free-Text Medical Notes for Suicide Risk Assessment",
        "authors": [
            "Marios Adamou",
            "Grigoris Antoniou",
            "Elissavet Greasidou",
            "Vincenzo Lagani",
            "Paulos Charonyktakis",
            "Ioannis Tsamardinos"
        ],
        "date": "July 2018",
        "source": "SETN '18: Proceedings of the 10th Hellenic Conference on Artificial Intelligence",
        "abstract": "Suicide has been considered as an important public health issue for a very long time, and is one of the main causes of death worldwide. Despite suicide prevention strategies being applied, the rate of suicide has not changed substantially over the past decades. Advances in machine learning make it possible to attempt to predict suicide based on the analysis of relevant data to inform clinical practice. This paper reports on findings from the analysis of data of patients who died by suicide in the period 2013-2016 and made use of both structured data and free-text medical notes. We focus on examining various text-mining approaches to support risk assessment. The results show that using advance machine learning and text-mining techniques, it is possible to predict within a specified period which people are most at risk of taking their own life at the time of referral to a mental health service.",
        "link": "https://dl.acm.org/doi/10.1145/3200947.3201020"
    },
    {
        "title": "Integration of data mining techniques in e-learning systems: Clustering Profil of Lerners and Recommender Course System",
        "authors": [
            "A. El Moustamid",
            "E. En-Naimi",
            "J. El Bouhdidi"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "Data mining aims at the extraction of knowledge from large quantities of data by automatic or semi-automatic methods. In the field of education and training e-learning, Data Mining techniques as clustering (The aim of this technique is to identify groups that are similar in some aspect) [21], prediction (is used for developing model where combination of data viewed from different aspects can effect single viewed data. Here, is important to watch how predictors variables are effecting predicted variable)[21], classification are applied in e-learning to improve the level of learners by providing learning according to the learners' requirement) are used to analyze learners profile, to predict and improve student performance. The aim of our project is to develop a system capable of analyzing learners profile and indexing web videos in order to offer learners a rich database with courses that correspond to their levels.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090453"
    },
    {
        "title": "Multi-level mining and visualization of scientific text collections: Exploring a bi-lingual scientific repository",
        "authors": [
            "Pablo Accuosto",
            "Francesco Ronzano",
            "Daniel Ferrés",
            "Horacio Saggion"
        ],
        "date": "December 2017",
        "source": "WOSP 2017: Proceedings of the 6th International Workshop on Mining Scientific Publications",
        "abstract": "We present a system to mine and visualize collections of scientific documents by semantically browsing information extracted from single publications or aggregated throughout corpora of articles. The text mining tool performs deep analysis of document collections allowing the extraction and interpretation of research paper's contents. In addition to the extraction and enrichment of documents with metadata (titles, authors, affiliations, etc), the deep analysis performed comprises semantic interpretation, rhetorical analysis of sentences, triple-based information extraction, and text summarization. The visualization components allow geographical-based exploration of collections, topic-evolution interpretation, and collaborative network analysis among others. The paper presents a case study of a bi-lingual collection in the field of Natural Language Processing (NLP).",
        "link": "https://dl.acm.org/doi/10.1145/3127526.3127529"
    },
    {
        "title": "Hierarchical Topic Mining via Joint Spherical Tree and Text Embedding",
        "authors": [
            "Yu Meng",
            "Yunyi Zhang",
            "Jiaxin Huang",
            "Yu Zhang",
            "Chao Zhang",
            "Jiawei Han"
        ],
        "date": "August 2020",
        "source": "KDD '20: Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Mining a set of meaningful topics organized into a hierarchy is intuitively appealing since topic correlations are ubiquitous in massive text corpora. To account for potential hierarchical topic structures, hierarchical topic models generalize flat topic models by incorporating latent topic hierarchies into their generative modeling process. However, due to their purely unsupervised nature, the learned topic hierarchy often deviates from users' particular needs or interests. To guide the hierarchical topic discovery process with minimal user supervision, we propose a new task, Hierarchical Topic Mining, which takes a category tree described by category names only, and aims to mine a set of representative terms for each category from a text corpus to help a user comprehend his/her interested topics. We develop a novel joint tree and text embedding method along with a principled optimization procedure that allows simultaneous modeling of the category tree structure and the corpus generative process in the spherical space for effective category-representative term discovery. Our comprehensive experiments show that our model, named JoSH, mines a high-quality set of hierarchical topics with high efficiency and benefits weakly-supervised hierarchical text classification tasks.",
        "link": "https://dl.acm.org/doi/10.1145/3394486.3403242"
    },
    {
        "title": "Unsupervised behavioural mining and clustering for malware family identification",
        "authors": [
            "Khanh Huu The Dam",
            "Thomas Given-Wilson",
            "Axel Legay"
        ],
        "date": "March 2021",
        "source": "SAC '21: Proceedings of the 36th Annual ACM Symposium on Applied Computing",
        "abstract": "More accurate and advanced detection and classification of malware requires exploiting program behaviour and not merely syntactic or static features. One approach is to use system call dependency graphs (SCDGs) that represent the program behaviour by interactions with the system, and the relations between these interactions. These SCDGs have been used with supervised learning techniques to very accurately detect and classify malware. This works considers the unsupervised learning challenge of mining for common clusters of behaviour without a priori knowledge. This allows for clustering of similar programs by behaviour, that can then be used for either classification or further analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3412841.3441919"
    },
    {
        "title": "Text Mining for Evaluating Authors' Birth and Death Years",
        "authors": [
            "Dror Moghaz",
            "Yaakov Hacohen-Kerner",
            "Dov Gabbay"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "This article presents a unique method in text and data mining for finding the era, i.e., mining temporal data, in which an anonymous author was living. Finding this era can assist in the examination of a fake document or extracting the time period in which a writer lived. The study and the experiments concern Hebrew, and in some parts, Aramaic and Yiddish rabbinic texts. The rabbinic texts are undated and contain no bibliographic sections, posing an interesting challenge. This work proposes algorithms using key phrases and key words that allow the temporal organization of citations together with linguistic patterns. Based on these key phrases, key words, and the references, we established several types of “Iron-clad,” Heuristic and Greedy rules for estimating the years of birth and death of a writer in an interesting classification task. Experiments were conducted on corpora, including documents authored by 12, 24, and 36 rabbinic writers and demonstrated promising results.",
        "link": "https://dl.acm.org/doi/10.1145/3281631"
    },
    {
        "title": "Transfer Learning to Timed Text Based Video Classification Using CNN",
        "authors": [
            "Zenun Kastrati",
            "Ali Shariq Imran",
            "Arianit Kurti"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "Open educational video resources are gaining popularity with a growing number of massive open online courses (MOOCs). This has created a niche for content providers to adopt effective solutions in automatically organizing and structuring of educational resources for maximum visibility. Recent advances in deep learning techniques are proving useful in managing and classifying resources into appropriate categories. This paper proposes one such convolutional neural network (CNN) model for classifying video lectures in a MOOC setting using a transfer learning approach. The model uses a time-aligned text transcripts corresponding to video lectures from six broader subject categories. Video lectures and their corresponding transcript dataset is gathered from the Coursera MOOC platform. Two different CNN models are proposed: i) CNN based classification using embeddings learned from our MOOC dataset, ii) CNN based classification using transfer learning. Word embeddings generated from two well known state-of-the-art pre-trained models Word2Vec and GloVe, are used in the transfer learning approach for the second case. The proposed CNN models are evaluated using precision, recall, and F1 score and the obtained performance is compared with both conventional and deep learning classifiers. The proposed CNN models have an F1 score improvement of 10-22 percentage points over DNN and conventional classifiers",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326483"
    },
    {
        "title": "Evolutionary role mining in complex networks by ensemble clustering",
        "authors": [
            "Sarvenaz Choobdar",
            "Pedro Ribeiro",
            "Fernando Silva"
        ],
        "date": "April 2017",
        "source": "SAC '17: Proceedings of the Symposium on Applied Computing",
        "abstract": "The structural patterns in the neighborhood of nodes assign unique roles to the nodes. Mining the set of existing roles in a network provides a descriptive profile of the network and draws its general picture. This paper proposes a new method to determine structural roles in a dynamic network based on the current position of nodes and their historic behavior. We develop a temporal ensemble clustering technique to dynamically find groups of nodes, holding similar tempo-structural roles. We compare two weighting functions, based on age and distribution of data, to incorporate temporal behavior of nodes in the role discovery. To evaluate the performance of the proposed method, we assess the results from two points of view: 1) goodness of fit to current structure of the network; 2) consistency with historic data. We conduct the evaluation using different ensemble clustering techniques. The results on real world networks demonstrate that our method can detect tempo-structural roles that simultaneously depict the topology of a network and reflect its dynamics with high accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3019612.3019815"
    },
    {
        "title": "Analyzing used-car web listings via text mining",
        "authors": [
            "Ayhan Demiriz",
            "Fatma Cantaş"
        ],
        "date": "October 2017",
        "source": "IML '17: Proceedings of the 1st International Conference on Internet of Things and Machine Learning",
        "abstract": "Used car trade is one of the major components of the world economies. It is not uncommon to sell a car by placing an internet advertisement irrespective of the geography in these days. A typical content of an advertisement is usually composed of two parts namely the structured and the free text data. The structured data may include some information about the asking price, make, model, year, mileage of the car and the contact info. In most cases, seller may give important clues about the car's current conditions in the free text data where the title (head) of the advertisement can be included as free text too. This paper reports preliminary results from a text mining study conducted on 75K used car internet listings collected from two major car listing web sites in Turkey. As expected, the words and the phrases related to the description of the car are observed to be frequent. The leading concepts in the free text are found to be regarding how to describe the current condition of a car, for example \"no crash history\".",
        "link": "https://dl.acm.org/doi/10.1145/3109761.3109782"
    },
    {
        "title": "Text classification of Diseases Treated by Traditional Chinese Medicine Prescription based on machine learning",
        "authors": [
            "Hanqing Zhao",
            "Bo Han",
            "Chen Li"
        ],
        "date": "September 2020",
        "source": "ISAIMS '20: Proceedings of the 1st International Symposium on Artificial Intelligence in Medical Sciences",
        "abstract": "OBJECTIVE: To explore the application of machine learning in th e identification of diseases treated by traditional Chinese medicine prescriptions. METHODS: Based on the composition of the text document of Chinese medicine prescriptions, the prescriptions were divided into cough, headache and diarrhea. THUTCT were introduced to establish and train two machine learning text classification models, LibSVM and LibLinear, and the prescriptions to be deter mined were put into the model for classification and prediction. R ESULTS: The Precision rate of LibSVM model and Liblinear mod el were 0.7283 and 0.6690. Seven prescriptions were classified an d predicted, and the results were in line with expectations. CONCLUSION: THUCTC has good universality for the content of TCM prescriptions, high classification accuracy and fast testing speed, which is suitable for the text classification and discrimination research of TCM prescriptions for diseases.",
        "link": "https://dl.acm.org/doi/10.1145/3429889.3429895"
    },
    {
        "title": "Evaluation of retweet clustering method classification method using retweets on Twitter without text data",
        "authors": [
            "K. Uchida",
            "F. Toriumi",
            "T. Sakaki"
        ],
        "date": "August 2017",
        "source": "WI '17: Proceedings of the International Conference on Web Intelligence",
        "abstract": "Burst phenomena, which frequently occur on social media, are caused by such social events as flaming on the internet, elections, and natural disasters. To understand people's thoughts and feelings, we must classify their opinions from burst phenomena. Therefore, classification methods that categorize tweets are critical. However, since most classification methods focus on text mining, they cannot group tweets by topics because each tweet has poor linguistic similarities. We used a non-text-based classification method proposed by Baba et al. that groups tweets by topics, even if they have poor linguistic similarities, and verified its validity by comparing it with a text-based classification method in two different evaluations: qualitative and quantitative. In the qualitative evaluation part, we did a questionnaire survey and validated the suitability of the topic clusters created using both the non-and text-based methods. Since evaluating the similarity of every pair of tweets in each topic is difficult, we evaluated the similarity between sampled pairs in the survey and acquired more appropriate topic clustering results using the non-text-based method than the text-based method. In the quantitative evaluation part, we focused on the robustness of each method against data reduction. Many approaches analyze social media data, especially because collecting data from social media is comparatively easy. However, since collecting the whole data of burst phenomena is very costly due to the vast amounts of available social media data, robustness against data reduction is an important index to evaluate classification methods. With the non-text-based method, over 55% of the pairs of tweets in the same cluster were also included in the same cluster even when the data were reduced to 10% in all three of our example cases. In this paper, as a source we focus on Twitter, one of the most popular microblogging services. Using clustering to conduct detailed case analyses, we scrutinized three burst cases that include natural disasters and flaming on the internet and found that a non-text-based method more effectively classified tweets in burst phenomena than a text-based method.",
        "link": "https://dl.acm.org/doi/10.1145/3106426.3106451"
    },
    {
        "title": "Using Spark for Text Mining on Large Scale Liver Cancer Literature",
        "authors": [
            "Ming-Yen Lin",
            "Yu-Ju Lin",
            "Sue-Chen Hsueh"
        ],
        "date": "January 2021",
        "source": "BDET 2021: 2021 the 3rd International Conference on Big Data Engineering and Technology (BDET)",
        "abstract": "Cancer is one of the main causes of death. The number of known cancer to-date is more than one hundred. Liver cancer, ranked after trachea and lung cancer, has long been high in the cancer leading cause in Taiwan and even ranked as second in 2016. The number of scientific articles related to cancer proliferates every year. The number reaches as high as 20 million in PubMed so that discovering useful information from the massive collection is very difficult. In addition, using a single machine to sift through these articles is very time-consuming. Therefore, we present a big data analytic framework using the distributed Apache Spark platform for text mining in PubMed literature. We establish a prediction model for liver cancer articles so that researchers may effectively validate whether an article is related to liver cancer or not. Classification models in Spark MLlib including Linear Support Vector Machines (SVM), Logistic Regression, are used in our experiments. Relevancy to liver cancer is further confirmed by using MeSH (Medical Subject Headings) terms. Logistic regression is about 3 times faster than SVMs and the accuracy of both methods is close to 95% in the experiments using hold-out validation. When max_features is 500 and min_df ≤ 0.1 (or min_df = 1), the accuracy may reach 96%. In the experiments with K-fold cross-validation, the accuracy of SVMs methods is 96%. The experimental results show that that our prediction model may effectively classify liver cancer articles.",
        "link": "https://dl.acm.org/doi/10.1145/3474944.3474958"
    },
    {
        "title": "Automatic topic classification of test cases using text mining at an Android smartphone vendor",
        "authors": [
            "Junji Shimagaki",
            "Yasutaka Kamei",
            "Naoyasu Ubayashi",
            "Abram Hindle"
        ],
        "date": "October 2018",
        "source": "ESEM '18: Proceedings of the 12th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Background: An Android smartphone is an ecosystem of applications, drivers, operating system components, and assets. The volume of the software is large and the number of test cases needed to cover the functionality of an Android system is substantial. Enormous effort has been already taken to properly quantify \"what features and apps were tested and verified?\". This insight is provided by dashboards that summarize test coverage and results per feature. One method to achieve this is to manually tag or label test cases with the topic or function they cover, much like function points. At the studied Android smartphone vendor, tests are labelled with manually defined tags, so-called \"feature labels (FLs)\", and the FLs serve to categorize 100s to 1000s test cases into 10 to 50 groups. Aim: Unfortunately for developers, manual assignment of FLs to 1000s of test cases is a time consuming task, leading to inaccurately labeled test cases, which will render the dashboard useless. We created an automated system that suggests tags/labels to the developers for their test cases rather than manual labeling. Method: We use machine learning models to predict and label the functionality tested by 10,000 test cases developed at the company. Results: Through the quantitative experiments, our models achieved acceptable F-1 performance of 0.3 to 0.88. Also through the qualitative studies with expert teams, we showed that the hierarchy and path of tests was a good predictor of a feature's label. Conclusions: We find that this method can reduce tedious manual effort that software developers spent classifying test cases, while providing more accurate classification results.",
        "link": "https://dl.acm.org/doi/10.1145/3239235.3268927"
    },
    {
        "title": "To apply Data Mining for Classification of Crowd sourced Software Requirements",
        "authors": [
            "Soonh Taj",
            "Qasim Arain",
            "Imran Memon",
            "Asma Zubedi"
        ],
        "date": "April 2019",
        "source": "ICSIE '19: Proceedings of the 8th International Conference on Software and Information Engineering",
        "abstract": "Now a day's main focus of developers is to build quality software that works according to customer needs and for this reason it is necessary to gather right requirements as requirement elicitation is the critical step that impacts on the success of software project as misinterpreted requirements leads to the failure of software project. By keeping this in mind a research is carried out on improving requirements elicitation process and automating the process of classifying requirements. In this research, a model is proposed which will help in this scenario for requirements elicitation and requirement classification. This paper presents a model in which crowd sourcing approach is used so that customers, end users, stakeholders, developers and software engineers can make active participation for requirement elicitation process and requirements gathered using crowdsourcing approach are used by model for classification process i.e. classification of requirements into functional and non-functional requirements. For the proof of proposed model a case study is conducted. Results of case study provided the usefulness and efficiency of proposed model for classification of crowd sourced software requirements.",
        "link": "https://dl.acm.org/doi/10.1145/3328833.3328837"
    },
    {
        "title": "Text classification using Fuzzy TF-IDF and Machine Learning Models",
        "authors": [
            "Mariem Bounabi",
            "Karim El Moutaouakil",
            "Khalid Satori"
        ],
        "date": "October 2019",
        "source": "BDIoT'19: Proceedings of the 4th International Conference on Big Data and Internet of Things",
        "abstract": "The representation of the information has an important impact on the text classification task. Several weighting methods were proposed in the literature, and the term frequency-inverse term frequency (TFIDF), the most know on the text treatment field. The FTF-IDF is a vector representation where the components of the TFIDF are presented as inputs to the Fuzzy Inference System (FIS). In this work, we compare several Machin Learning algorithms such as Naïve Bayes and its derivatives, SVM and Random forest classifiers, using the FTF-IDF representation. To improve the quality of the used classifiers, we call sum attribute selection methods. The recognition rate, for the tested systems, is satisfied, where the system based on naïve Bayes classifier, the FTF-IDF weighting terms, and the info gain select attributes method gives 98.7% as accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3372938.3372956"
    },
    {
        "title": "Text Mining in Cybersecurity: A Systematic Literature Review",
        "authors": [
            "Luciano Ignaczak",
            "Guilherme Goldschmidt",
            "Cristiano André Da Costa",
            "Rodrigo Da Rosa Righi"
        ],
        "date": null,
        "source": "ACM Computing Surveys",
        "abstract": "The growth of data volume has changed cybersecurity activities, demanding a higher level of automation. In this new cybersecurity landscape, text mining emerged as an alternative to improve the efficiency of the activities involving unstructured data. This article proposes a Systematic Literature Review (SLR) to present the application of text mining in the cybersecurity domain. Using a systematic protocol, we identified 2,196 studies, out of which 83 were summarized. As a contribution, we propose a taxonomy to demonstrate the different activities in the cybersecurity domain supported by text mining. We also detail the strategies evaluated in the application of text mining tasks and the use of neural networks to support activities involving unstructured data. The work also discusses text classification performance aiming its application in real-world solutions. The SLR also highlights open gaps for future research, such as the analysis of non-English content and the intensification in the usage of neural networks.",
        "link": "https://dl.acm.org/doi/10.1145/3462477"
    },
    {
        "title": "Using Text Mining to Analyze the Financial Patents",
        "authors": [
            "Yung-Feng Lu",
            "Jia-Lang Xu",
            "Mu-Yen Chen"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "With the advancement and development of science and technology, Nowadays society is the development of information technology, cloud computing, mobile Internet, Internet of Things, block chain and information technology.The combination of finance and technology has made financial technology terms gradually emerging.. This research analyzes the patents of financial technology. However, Taiwan's financial technology patents are mostly G06Q-20 (payment plan, architecture or agreement), G06Q-30 (commercial, such as marketing, shopping, payment, auction or e-commerce). G06Q-40 (financial, investment or tax treatment, insurance).The research use text mining technology to analyze and understand Taiwan's trends and developments in financial technology. The research results show that G06Q-20 patents are biased towards information security and mobile payment. G06Q-30 patents favor shopping websites and advertising. G06Q-40 patents are biased towards diversity, such as securities, insurance, finance, and stocks.",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326478"
    },
    {
        "title": "Identifying Online Profiles of Distance Learning Students Using Data Mining Techniques",
        "authors": [
            "Osama Islam",
            "Muazzam Siddiqui",
            "Naif Radi Aljohani"
        ],
        "date": "October 2019",
        "source": "ICDTE '19: Proceedings of the 3rd International Conference on Digital Technology in Education",
        "abstract": "Educational data has grown over the years with the increased use of technology within educational environments. This has led to a huge amount of data being stored in various data sources representing the student, his/her activities, and other aspects relevant to the learning process. To meet this analytical need, Educational Data Mining (EDM) has emerged to assist educational institutions in identifying key benefits such as students at risk, the level of student engagement or predicting student performance. The aim of this research was to explore the various aspects of student interaction data using data mining techniques to identify relevant patterns of behaviors and possible key attributes that have higher degrees of influence on distance learning students. The main findings identified several patterns of user online profiles based on a set of adopted learning strategies, the research proposed a framework for analyzing such interaction data based on R and Hadoop platforms to correlate online profiles with student performance.",
        "link": "https://dl.acm.org/doi/10.1145/3369199.3369249"
    },
    {
        "title": "Extremely Fast Decision Tree Mining for Evolving Data Streams",
        "authors": [
            "Albert Bifet",
            "Jiajin Zhang",
            "Wei Fan",
            "Cheng He",
            "Jianfeng Zhang",
            "Jianfeng Qian",
            "Geoff Holmes",
            "Bernhard Pfahringer"
        ],
        "date": "August 2017",
        "source": "KDD '17: Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
        "abstract": "Nowadays real-time industrial applications are generating a huge amount of data continuously every day. To process these large data streams, we need fast and efficient methodologies and systems. A useful feature desired for data scientists and analysts is to have easy to visualize and understand machine learning models. Decision trees are preferred in many real-time applications for this reason, and also, because combined in an ensemble, they are one of the most powerful methods in machine learning. In this paper, we present a new system called STREAMDM-C++, that implements decision trees for data streams in C++, and that has been used extensively at Huawei. Streaming decision trees adapt to changes on streams, a huge advantage since standard decision trees are built using a snapshot of data, and can not evolve over time. STREAMDM-C++ is easy to extend, and contains more powerful ensemble methods, and a more efficient and easy to use adaptive decision trees. We compare our new implementation with VFML, the current state of the art implementation in C, and show how our new system outperforms VFML in speed using less resources.",
        "link": "https://dl.acm.org/doi/10.1145/3097983.3098139"
    },
    {
        "title": "Data miners' little helper: data transformation activity cues for cluster analysis on document collections",
        "authors": [
            "Tania Cerquitelli",
            "Evelina Di Corso",
            "Francesco Ventura",
            "Silvia Chiusano"
        ],
        "date": "June 2017",
        "source": "WIMS '17: Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "In this paper we propose a new self-learning engine to streamline the analytics process, as it enables analysts to mine massive data repositories with minimal user intervention. In the context of cluster analysis on a collection of documents this new system, named SELF-DATA (SELF-learning DAta TrAnsformation), suggests to the analyst how to configure the whole mining process for a given dataset. SELF-DATA relies on an engine exploring different data weighting schemas (e.g., normalized term frequencies) and data transformation methods (e.g., PCA) before applying the cluster analysis, evaluating and comparing solutions through different quality indices (e.g., weighted Silhouette), and presenting the k-top solutions to the analyst. SELF-DATA will also include a knowledge base storing results of experiments on previously processed datasets, and a classification algorithm trained on the knowledge base content to forecast the best configuration for the whole mining process for an unexplored dataset. The first development of SELF-DATA running on Apache Spark has been validated on 5 collections of documents. Experimental results highlight that TF-IDF and logarithmic entropy are effective to measure item relevance with sparse datasets, and the LSI method outperforms PCA with a large dictionary.",
        "link": "https://dl.acm.org/doi/10.1145/3102254.3102288"
    },
    {
        "title": "Interactive Visual Graph Mining and Learning",
        "authors": [
            "Ryan A. Rossi",
            "Nesreen K. Ahmed",
            "Rong Zhou",
            "Hoda Eldardiry"
        ],
        "date": null,
        "source": "ACM Transactions on Intelligent Systems and Technology",
        "abstract": "This article presents a platform for interactive graph mining and relational machine learning called GraphVis. The platform combines interactive visual representations with state-of-the-art graph mining and relational machine learning techniques to aid in revealing important insights quickly as well as learning an appropriate and highly predictive model for a particular task (e.g., classification, link prediction, discovering the roles of nodes, and finding influential nodes). Visual representations and interaction techniques and tools are developed for simple, fast, and intuitive real-time interactive exploration, mining, and modeling of graph data. In particular, we propose techniques for interactive relational learning (e.g., node/link classification), interactive link prediction and weighting, role discovery and community detection, higher-order network analysis (via graphlets, network motifs), among others. GraphVis also allows for the refinement and tuning of graph mining and relational learning methods for specific application domains and constraints via an end-to-end interactive visual analytic pipeline that learns, infers, and provides rapid interactive visualization with immediate feedback at each change/prediction in real-time. Other key aspects include interactive filtering, querying, ranking, manipulating, exporting, as well as tools for dynamic network analysis and visualization, interactive graph generators (including new block model approaches), and a variety of multi-level network analysis techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3200764"
    },
    {
        "title": "Data Mining and Opinion Mining: A Tool in Educational Context",
        "authors": [
            "Myriam Peñafiel",
            "Stefanie Vásquez",
            "Diego Vásquez",
            "Juan Zaldumbide",
            "Sergio Luján-Mora"
        ],
        "date": "July 2018",
        "source": "ICoMS '18: Proceedings of the 2018 1st International Conference on Mathematics and Statistics",
        "abstract": "The use of the web as a universal communication platform generates large volumes of data (Big data), which in many cases, need to be processed so that they can become useful knowledge in face of the sceptics who have doubts about the credibility of such information. The use of web data that comes from educational contexts needs to be addressed, since that large amount of unstructured information is not being valued, losing valuable information that can be used. To solve this problem, we propose the use of data mining techniques such as sentiment analysis to validate the information that comes from the educational platforms. The objective of this research is to propose a methodology that allows the user to apply sentiment analysis in a simple way, because although some researchers have done it, very few do with data in the educational context. The results obtained prove that the proposal can be used in similar cases.",
        "link": "https://dl.acm.org/doi/10.1145/3274250.3274263"
    },
    {
        "title": "Research Hotspots Mining and Visualized Analysis Based on Linking Cluster and K-Core Decomposition",
        "authors": [
            "Ying Cai",
            "Fang Huang",
            "Shuai Wang",
            "Hao Zhang",
            "Chunxiu Du"
        ],
        "date": "May 2018",
        "source": "ICDPA 2018: Proceedings of the International Conference on Data Processing and Applications",
        "abstract": "In this paper, we focus on research hotspots mining with knowledge semantic features on discipline topic word networks, which are extracted from scientific and technical documents. For the topic networks, a novel method of combining linking community clustering with k-core decomposition is proposed to mine the hotspot community with knowledge hierarchy structure. And as a post-processing, the hierarchical community identification is implemented by k-core decomposition in order to discover the internal connection with knowledge semantic hierarchy in the research hotspot. Finally, the community density and visualization method is utilized to analyze the core and sub- problem in the research hotspot community. In experiments, the topic networks for a single discipline and interdisciplinary research hotspot are taken as examples of analysis, and the results show the proposed approach can effectively realize the objective of mining and analyzing the discipline research hotspot on the academic topic word network.",
        "link": "https://dl.acm.org/doi/10.1145/3224207.3224216"
    },
    {
        "title": "The Application of Data Mining Algorithms in the Construction of Travel Recommendation System",
        "authors": [
            "Chao Lou",
            "Zhaonan Mu",
            "Mengzhu Liu"
        ],
        "date": "January 2021",
        "source": "CONF-CDS 2021: The 2nd International Conference on Computing and Data Science",
        "abstract": "Photos shared by users of online social network often associated with tags, texts, geographic and time information. These data are ideal research resource which can be used by researchers to observe people's interests and behaviors. They can also be used to analyze people's travel behaviors and interests because it can reflect people's travel activities and experiences directly. This project aimed to consolidate people into groups based on their travel preferences of travel season, travel time span, scenery type of travel location and then analyze popular travel places for different people groups. The travel preferences were analyzed from extracted data applying PHP language and Naive Bayes classification approach. According to the analyzed data, people were divided into groups. Then, DBSCAN clustering was used to get popular locations based on the different groups. As a result, 80 thousand photo data of 5 cities have been collected from Flickr by Phython. It has been showed that people's preference of season in most of the mined cities was spring and summer and the preference of time span was short. Moreover, mined people has been divided into 16 groups according to their preference and based on the groups popular locations have been calculated by DBSCAN clustering. Theses locations have been provided in a recommendation website which was built in this project. For the future work, mining Flickr data associated with other travel social network sites could be a good way to build a more effective recommendation system.",
        "link": "https://dl.acm.org/doi/10.1145/3448734.3450477"
    },
    {
        "title": "Compare Machine Learning Models in Text Classification Using Steam User Reviews",
        "authors": [
            "Youchen Miao",
            "Zeyu Jin",
            "Yumeng Zhang",
            "Yuchen Chen",
            "Junren Lai"
        ],
        "date": "November 2021",
        "source": "ICSED '21: Proceedings of the 2021 3rd International Conference on Software Engineering and Development",
        "abstract": "Text Classification and Sentiment Analysis of game reviews are viewed as important parts in not only academic fields but also in game studies. In this paper, with more than 400 thousand game reviews on Steam platform, we preprocess the data using different libraries (sklearn, nltk, and spaCy) and use them as inputs to build three sentiment classification models based on different algorithms (Naive Bayes, SVM, and Random Forest). In contrast to previous studies that only focus on different sentiment analysis models, our paper also highlights the use of different APIs to preprocess the data and their corresponding model performance. The results show that no matter which API we choose, Random Forest models always perform the best. However, in terms of training time, Naive Bayes is the fastest. This work can be used to apply grid search for researchers to automatically find the optimum API before conducting sentiment analysis in the future.",
        "link": "https://dl.acm.org/doi/10.1145/3507473.3507480"
    },
    {
        "title": "Lost in Transduction: Transductive Transfer Learning in Text Classification",
        "authors": [
            "Alejandro Moreo",
            "Andrea Esuli",
            "Fabrizio Sebastiani"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Obtaining high-quality labelled data for training a classifier in a new application domain is often costly. Transfer Learning\n(a.k.a. “Inductive Transfer”) tries to alleviate these costs by transferring, to the “target” domain of interest, knowledge available from a different “source” domain. In transfer learning the lack of labelled information from the target domain is compensated by the availability at training time of a set of unlabelled examples from the target distribution. Transductive Transfer Learning denotes the transfer learning setting in which the only set of target documents that we are interested in classifying is known and available at training time. Although this definition is indeed in line with Vapnik’s original definition of “transduction”, current terminology in the field is confused. In this article, we discuss how the term “transduction” has been misused in the transfer learning literature, and propose a clarification consistent with the original characterization of this term given by Vapnik. We go on to observe that the above terminology misuse has brought about misleading experimental comparisons, with inductive transfer learning methods that have been incorrectly compared with transductive transfer learning methods. We then, give empirical evidence that the difference in performance between the inductive version and the transductive version of a transfer learning method can indeed be statistically significant (i.e., that knowing at training time the only data one needs to classify indeed gives an advantage). Our clarification allows a reassessment of the field, and of the relative merits of the major, state-of-the-art algorithms for transfer learning in text classification.",
        "link": "https://dl.acm.org/doi/10.1145/3453146"
    },
    {
        "title": "Machine Learning on Mining Potential Adverse Drug Reactions for Pharmacovigilance",
        "authors": [
            "Ke Jia"
        ],
        "date": "July 2021",
        "source": "DSIT 2021: 2021 4th International Conference on Data Science and Information Technology",
        "abstract": "Drugs are related to the lives and health of patients. Mining and discovering potential adverse drug reactions from a large number of unstructured texts play an important role in drug research and pharmacovigilance. It is a hot research issue in the field of biomedicine in recent years. A large number of scholars have discovered a large amount of information on adverse drug reactions from different data sources through effective machine learning methods. This paper reviews the research progress in recent years to provide a reference for future research.",
        "link": "https://dl.acm.org/doi/10.1145/3478905.3478964"
    },
    {
        "title": "TACAM: Topic And Context Aware Argument Mining",
        "authors": [
            "Michael Fromm",
            "Evgeniy Faerman",
            "Thomas Seidl"
        ],
        "date": "October 2019",
        "source": "WI '19: IEEE/WIC/ACM International Conference on Web Intelligence",
        "abstract": "In this work we address the problem of argument search. The purpose of argument search is the distillation of pro and contra arguments for requested topics from large text corpora. In previous works, the usual approach is to use a standard search engine to extract text parts which are relevant to the given topic and subsequently use an argument recognition algorithm to select arguments from them. The main challenge in the argument recognition task, which is also known as argument mining, is that often sentences containing arguments are structurally similar to purely informative sentences without any stance about the topic. In fact, they only differ semantically. Most approaches use topic or search term information only for the first search step and therefore assume that arguments can be classified independently of a topic. We argue that topic information is crucial for argument mining, since the topic defines the semantic context of an argument. Precisely, we propose different models for the classification of arguments, which take information about a topic of an argument into account. Moreover, to enrich the context of a topic and to let models understand the context of the potential argument better, we integrate information from different external sources such as Knowledge Graphs or pre-trained NLP models. Our evaluation shows that considering topic information, especially in connection with external information, provides a significant performance boost for the argument mining task.",
        "link": "https://dl.acm.org/doi/10.1145/3350546.3352506"
    },
    {
        "title": "Text mining for incoming tasks based on the urgency/importance factors and task classification using machine learning tools",
        "authors": [
            "Yasser Ali Alshehri"
        ],
        "date": "March 2020",
        "source": "ICCDA 2020: Proceedings of the 2020 the 4th International Conference on Compute and Data Analysis",
        "abstract": "In workplaces, there is a massive amount of unstructured data from different sources. In this paper, we present a case study that explains how can through communications between employees, we can help to prioritize tasks requests to increase the efficiency of their works for both technical and non-technical workers. This involves managing daily incoming tasks based on their level of urgency and importance.To allow all workers to utilize the urgency-importance matrix as a time-management tool, we need to automate this tool. The textual content of incoming tasks are analyzed, and metrics related to urgency and importance are extracted. A third factor (i.e., the response variable) is defined based on the two input variables (urgency and importance). Then, machine learning applied to the data to predict the class of incoming tasks based on data outcome desired. We used ordinal regression, neural networks, and decision tree algorithms to predict the four levels of task priority. We measure the performance of all using recalls, precisions, and F-scores. All classifiers perform higher than 89% in terms of all measures.",
        "link": "https://dl.acm.org/doi/10.1145/3388142.3388153"
    },
    {
        "title": "Text mining for malware classification using multivariate all repeated patterns detection",
        "authors": [
            "Konstantinos F. Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2019",
        "source": "ASONAM '19: Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Mobile phones have become nowadays a commodity to the majority of people. Using them, people are able to access the world of Internet and connect with their friends, their colleagues at work or even unknown people with common interests. This proliferation of the mobile devices has also been seen as an opportunity for the cyber criminals to deceive smartphone users and steel their money directly or indirectly, respectively, by accessing their bank accounts through the smartphones or by blackmailing them or selling their private data such as photos, credit card data, etc. to third parties. This is usually achieved by installing malware to smartphones masking their malevolent payload as a legitimate application and advertise it to the users with the hope that mobile users will install it in their devices. Thus, any existing application can easily be modified by integrating a malware and then presented it as a legitimate one. In response to this, scientists have proposed a number of malware detection and classification methods using a variety of techniques. Even though, several of them achieve relatively high precision in malware classification, there is still space for improvement. In this paper, we propose a text mining all repeated pattern detection method which uses the decompiled files of an application in order to classify a suspicious application into one of the known malware families. Based on the experimental results using a real malware dataset, the methodology tries to correctly classify (without any misclassification) all randomly selected malware applications of 3 categories with 3 different families each.",
        "link": "https://dl.acm.org/doi/10.1145/3341161.3350841"
    },
    {
        "title": "Comparative Study between Traditional Machine Learning and Deep Learning Approaches for Text Classification",
        "authors": [
            "Cannannore Nidhi Kamath",
            "Syed Saqib Bukhari",
            "Andreas Dengel"
        ],
        "date": "August 2018",
        "source": "DocEng '18: Proceedings of the ACM Symposium on Document Engineering 2018",
        "abstract": "In this contemporaneous world, it is an obligation for any organization working with documents to end up with the insipid task of classifying truckload of documents, which is the nascent stage of venturing into the realm of information retrieval and data mining. But classification of such humongous documents into multiple classes, calls for a lot of time and labor. Hence a system which could classify these documents with acceptable accuracy would be of an unfathomable help in document engineering. We have created multiple classifiers for document classification and compared their accuracy on raw and processed data. We have garnered data used in a corporate organization as well as publicly available data for comparison. Data is processed by removing the stop-words and stemming is implemented to produce root words. Multiple traditional machine learning techniques like Naive Bayes, Logistic Regression, Support Vector Machine, Random forest Classifier and Multi-Layer Perceptron are used for classification of documents. Classifiers are applied on raw and processed data separately and their accuracy is noted. Along with this, Deep learning technique such as Convolution Neural Network is also used to classify the data and its accuracy is compared with that of traditional machine learning techniques. We are also exploring hierarchical classifiers for classification of classes and subclasses. The system classifies the data faster and with better accuracy than if done manually. The results are discussed in the results and evaluation section.",
        "link": "https://dl.acm.org/doi/10.1145/3209280.3209526"
    },
    {
        "title": "Identification of Overpricing in the Purchase of Medication by the Federal Government of Brazil, Using Text Mining and Clustering Based on Ontology",
        "authors": [
            "Marco Aurelio O. S. Correa",
            "Adriano Galindo Leal"
        ],
        "date": "August 2018",
        "source": "ICCBDC '18: Proceedings of the 2018 2nd International Conference on Cloud and Big Data Computing",
        "abstract": "Increasing the transparency level in his actions and spending is one of the primary duties of the Brazilian Federal Government. The creation of laws that oblige full disclose of all its expenditures through transparency portals enables citizens to supervise all government entities. However, only the dissemination of these data, without a definite standard or the availability of data analysis tools, does not guarantee that the citizen is empowered to play his role. Therefore hence, the objective of this work is to identify overprice in the acquisition of products purchased by the federal government of Brazil using the unstructured data available on the Transparency Portal. The last two-years' worth of purchasing data, available in the Transparency Portal, were extracted, processed and stored. Due to his diverse nature and high volume of data, this study focused only on medicines purchased by the Ministry of Health. Ontology-based text mining and clustering techniques were applied for automatic identification and classification of products. The processing of this information was done through text mining and clustering, based on the ontology registered in another database of the Brazilian government. Because of this work, a consolidated price base per medication was created to allow the identification of distortions in prices practised, facilitating the identification of cases that merit further investigation to unravel fraud to the treasury.",
        "link": "https://dl.acm.org/doi/10.1145/3264560.3264569"
    },
    {
        "title": "Computational Estimation by Scientific Data Mining with Classical Methods to Automate Learning Strategies of Scientists",
        "authors": [
            "Aparna S. Varde"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Experimental results are often plotted as 2-dimensional graphical plots (aka graphs) in scientific domains depicting dependent versus independent variables to aid visual analysis of processes. Repeatedly performing laboratory experiments consumes significant time and resources, motivating the need for computational estimation. The goals are to estimate the graph obtained in an experiment given its input conditions, and to estimate the conditions that would lead to a desired graph. Existing estimation approaches often do not meet accuracy and efficiency needs of targeted applications. We develop a computational estimation approach called AutoDomainMine that integrates clustering and classification over complex scientific data in a framework so as to automate classical learning methods of scientists. Knowledge discovered thereby from a database of existing experiments serves as the basis for estimation. Challenges include preserving domain semantics in clustering, finding matching strategies in classification, striking a good balance between elaboration and conciseness while displaying estimation results based on needs of targeted users, and deriving objective measures to capture subjective user interests. These and other challenges are addressed in this work. The AutoDomainMine approach is used to build a computational estimation system, rigorously evaluated with real data in Materials Science. Our evaluation confirms that AutoDomainMine provides desired accuracy and efficiency in computational estimation. It is extendable to other science and engineering domains as proved by adaptation of its sub-processes within fields such as Bioinformatics and Nanotechnology.",
        "link": "https://dl.acm.org/doi/10.1145/3502736"
    },
    {
        "title": "Where is the road for issue reports classification based on text mining?",
        "authors": [
            "Qiang Fan",
            "Yue Yu",
            "Gang Yin",
            "Tao Wang",
            "Huaimin Wang"
        ],
        "date": "November 2017",
        "source": "ESEM '17: Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Currently, open source projects receive various kinds of issues daily, because of the extreme openness of Issue Tracking System (ITS) in GitHub. ITS is a labor-intensive and time-consuming task of issue categorization for project managers. However, a contributor is only required a short textual abstract to report an issue in GitHub. Thus, most traditional classification approaches based on detailed and structured data (e.g., priority, severity, software version and so on) are difficult to adopt. In this paper, issue classification approaches on a large-scale dataset, including 80 popular projects and over 252,000 issue reports collected from GitHub, were investigated. First, four traditional text-based classification methods and their performances were discussed. Semantic perplexity (i.e., an issues description confuses bug-related sentences with nonbug-related sentences) is a crucial factor that affects the classification performances based on quantitative and qualitative study. Finally, A two-stage classifier framework based on the novel metrics of semantic perplexity of issue reports was designed. Results show that our two-stage classification can significantly improve issue classification performances.",
        "link": "https://dl.acm.org/doi/10.1109/ESEM.2017.19"
    },
    {
        "title": "Feature-based Facebook reviews process model for e-management using data mining",
        "authors": [
            "Anish Kumar Varudharajulu",
            "Yongsheng Ma"
        ],
        "date": "January 2019",
        "source": "IC4E '19: Proceedings of the 10th International Conference on E-Education, E-Business, E-Management and E-Learning",
        "abstract": "The data generated from online communication acts as potential gold mines for discovering knowledge for researchers. A large amount of data is also generated in the form of web documents, emails, blogs, and feedback, etc. Text analytics is being significantly employed to mine important information. Opinion mining is the process of extracting human thoughts and perceptions from unstructured texts. The showstopper for designing an opinion mining system for analyzing reviews arise from the fact that customer reviews are often noisy. These reviews are informally written. In addition, they are subjected to spelling mistakes, grammatical errors, improper punctuation and irrational capitalization. This paper focuses on analyzing the different classification and clustering algorithms aimed at extracting and consolidating opinions of customers from social media sites like Facebook, Twitter and through surveys, at multiple levels of granularity to monitor and measure customer satisfaction. Ours is an automated approach, in which the system aids in the process of knowledge assimilation for knowledge-based building and also performs the analytics. Domain experts ratify the knowledge base and also provide training datasets for the system to intuitively gather more instances for ratification. The system identifies opinion expressions as phrases containing opinion words, opinionated features and also opinion modifiers. These expressions are categorized as positive, negative or neutral. Opinion expressions are identified and categorized using localized linguistic techniques. Opinions can be congregated at any desired level of specificity i.e. feature level or product level, user level or service level, etc. We have developed a system based on this approach, which provides the user with a platform to analyze opinion expressions crawled from a set of pre-defined datasets.",
        "link": "https://dl.acm.org/doi/10.1145/3306500.3306514"
    },
    {
        "title": "Discriminative Topic Mining via Category-Name Guided Text Embedding",
        "authors": [
            "Yu Meng",
            "Jiaxin Huang",
            "Guangyuan Wang",
            "Zihan Wang",
            "Chao Zhang",
            "Yu Zhang",
            "Jiawei Han"
        ],
        "date": "April 2020",
        "source": "WWW '20: Proceedings of The Web Conference 2020",
        "abstract": "Mining a set of meaningful and distinctive topics automatically from massive text corpora has broad applications. Existing topic models, however, typically work in a purely unsupervised way, which often generate topics that do not fit users’ particular needs and yield suboptimal performance on downstream tasks. We propose a new task, discriminative topic mining, which leverages a set of user-provided category names to mine discriminative topics from text corpora. This new task not only helps a user understand clearly and distinctively the topics he/she is most interested in, but also benefits directly keyword-driven classification tasks. We develop CatE, a novel category-name guided text embedding method for discriminative topic mining, which effectively leverages minimal user guidance to learn a discriminative embedding space and discover category representative terms in an iterative manner. We conduct a comprehensive set of experiments to show that CatE mines high-quality set of topics guided by category names only, and benefits a variety of downstream applications including weakly-supervised classification and lexical entailment direction identification.",
        "link": "https://dl.acm.org/doi/10.1145/3366423.3380278"
    },
    {
        "title": "Comparative Study of Heart Disease Diagnosis Using Top Ten Data Mining Classification Algorithms",
        "authors": [
            "I. Ketut Agung Enriko"
        ],
        "date": "June 2019",
        "source": "ICFET '19: Proceedings of the 5th International Conference on Frontiers of Educational Technologies",
        "abstract": "Data mining has been used for many purposes, especially for prediction system. In healthcare, data mining algorithms often used in disease diagnosis. Meanwhile, heart disease is known as a primary cause of death over the years. Many studies have been performed in heart disease diagnosis using data mining methods. There are some popular data mining algorithms that can be used in heart disease diagnosis, for example, k-Nearest Neighbor, CART, and AdaBoost. The algorithms are used to analyze a sample of cardiovascular patients data and predict the heart disease type that they suffer. Some parameters are taken from the patient, including EKG morphology, blood pressure, and information about the existence of chest pain, shortness of breath, palpitation, and cold sweat. In this study, medical records data are collected from Harapan Kita Hospital and utilized as a dataset sample in this research. Top ten data mining classification algorithms are used in diagnosing heart disease from Harapan Kita Hospital data and examining their performance by checking the accuracy and speed.",
        "link": "https://dl.acm.org/doi/10.1145/3338188.3338220"
    },
    {
        "title": "Text Mining Approach for Identifying Research Trends",
        "authors": [
            "Snezhana Sulova"
        ],
        "date": "June 2021",
        "source": "CompSysTech '21: Proceedings of the 22nd International Conference on Computer Systems and Technologies",
        "abstract": "With the increase of unstructured data, the issues connected with automatic text processing, the categorization of documents and the discovery of topics have become objects of growing interest. In order to improve the process of grouping and processing research publications, we would like to propose a method based upon natural language processing. It is based on text mining technologies which aim to identify key tendencies in documents. It processes the content of publications by clustering and identifies the topics of each identified group. This analysis helps by identifying key tendencies as well as discovering emerging new areas of research. Publications from the research literature database, Scopus, were used to test the approach. The topic of the publications is “the application of digital technologies in the logistics business”. The experiments were completed using the RapidMiner Studio software.",
        "link": "https://dl.acm.org/doi/10.1145/3472410.3472433"
    },
    {
        "title": "Assessment of Vulnerability Severity using Text Mining",
        "authors": [
            "Georgios Spanos",
            "Lefteris Angelis",
            "Dimitrios Toloudis"
        ],
        "date": "September 2017",
        "source": "PCI '17: Proceedings of the 21st Pan-Hellenic Conference on Informatics",
        "abstract": "Software1 vulnerabilities are closely associated with information systems security, a major and critical field in today's technology. Vulnerabilities constitute a constant and increasing threat for various aspects of everyday life, especially for safety and economy, since the social impact from the problems that they cause is complicated and often unpredictable. Although there is an entire research branch in software engineering that deals with the identification and elimination of vulnerabilities, the growing complexity of software products and the variability of software production procedures are factors contributing to the ongoing occurrence of vulnerabilities, Hence, another area that is being developed in parallel focuses on the study and management of the vulnerabilities that have already been reported and registered in databases. The information contained in such databases includes, a textual description and a number of metrics related to vulnerabilities. The purpose of this paper is to investigate to what extend the assessment of the vulnerability severity can be inferred directly from the corresponding textual description, or in other words, to examine the informative power of the description with respect to the vulnerability severity. For this purpose, text mining techniques, i.e. text analysis and three different classification methods (decision trees, neural networks and support vector machines) were employed. The application of text mining to a sample of 70,678 vulnerabilities from a public data source shows that the description itself is a reliable and highly accurate source of information for vulnerability prioritization.",
        "link": "https://dl.acm.org/doi/10.1145/3139367.3139390"
    },
    {
        "title": "Clustering Algorithms for Spatial Data Mining",
        "authors": [
            "Chetashri Bhadane",
            "Ketan Shah"
        ],
        "date": "April 2020",
        "source": "ICGDA '20: Proceedings of the 2020 3rd International Conference on Geoinformatics and Data Analysis",
        "abstract": "With the advances in mobile and wireless technologies, there has been a rise in applications that track and share the users' geospatial data. People use several social networking sites such as Twitter, Facebook and Flickr, where they share their status updates. With the integration of Global Positioning System (GPS) with mobile phones, it is now possible to share one's locations on these social networks. GPS allows us to record and track a person's movement along with the timestamp. The data set obtained from these GPS logs is vast and is widely used to analyze the users' movement patterns. Specifically, we can find out significant locations based on the number of users present at that location and the time spent by them at such places. Once significant places have been identified, it is also possible to identify the semantic importance of these locations. This paper presents an overview of the clustering techniques used to find important places of interest using large GPS based mobility datasets. Four clustering algorithms, K-Means, DBSCAN, OPTICS and Hierarchical, are implemented, and performance is tested using real-time data of 50 users collected over 2--5 years. Performance summary depicts that K-Means and DBSCAN perform well for spatial data.",
        "link": "https://dl.acm.org/doi/10.1145/3397056.3397068"
    },
    {
        "title": "Feature Selection based Arabic Text Classification using Different Machine Learning Algorithms: Comparative Study",
        "authors": [
            "Sakina Rim Bennabi",
            "Zakaria Elberrichi"
        ],
        "date": "June 2020",
        "source": "ICIST '20: Proceedings of the 10th International Conference on Information Systems and Technologies",
        "abstract": "Feature selection is a method of data pre-processing widely used when mining large data, such as textual classification. Several studies have been conducted to compare the different methods of feature selection applied to corpora in English. Unfortunately, a small number of works concern the Arabic language. This article aims to present a comparative study of different feature selection techniques including: Chi2, the ANOVA method and mutual information, applied on a corpus in Arabic language, while also diversifying the machine learning algorithms (Naive Bayes, SVM and KNN). This experimental study has shown in general that reducing dimensionality with feature selection techniques has slightly affected the performance of textual classification, reducing the size of the corpus by up to 1%.",
        "link": "https://dl.acm.org/doi/10.1145/3447568.3448531"
    },
    {
        "title": "Intelligent Data Analysis using Optimized Support Vector Machine Based Data Mining Approach for Tourism Industry",
        "authors": [
            "Ms Promila Sharma",
            "Uma Meena",
            "Girish Kumar Sharma"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Data analysis involves the deployment of sophisticated approaches from data mining methods, information theory, and artificial intelligence in various fields like tourism, hospitality, and so on for the extraction of knowledge from the gathered and preprocessed data. In tourism, pattern analysis or data analysis using classification is significant for finding the patterns that represent new and potentially useful information or knowledge about the destination and other data. Several data mining techniques are introduced for the classification of data or patterns. However, overfitting, less accuracy, local minima, sensitive to noise are the drawbacks in some existing data mining classification methods. To overcome these challenges, Support vector machine with Red deer optimization (SVM-RDO) based data mining strategy is proposed in this article. Extended Kalman filter (EKF) is utilized in the first phase, i.e., data cleaning to remove the noise and missing values from the input data. Mantaray foraging algorithm (MaFA) is used in the data selection phase, in which the significant data are selected for the further process to reduce the computational complexity. The final phase is the classification, in which SVM-RDO is proposed to access the useful pattern from the selected data. PYTHON is the implementation tool used for the experiment of the proposed model. The experimental analysis is done to show the efficacy of the proposed work. From the experimental results, the proposed SVM-RDO achieved better accuracy, precision, recall, and F1 score than the existing methods for the tourism dataset. Thus, it is showed the effectiveness of the proposed SVM-RDO for pattern analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3494566"
    },
    {
        "title": "Applying text mining to predict learners' cognitive engagement",
        "authors": [
            "Hayati Hind",
            "Mohammed Khalidi Idrissi",
            "Samir Bennani"
        ],
        "date": "October 2017",
        "source": "SCAMS '17: Proceedings of the Mediterranean Symposium on Smart City Application",
        "abstract": "In academic process, engagement represent one of the important success' factor. This paper proposes a predictive system for learners' cognitive engagement based on their online discussion forums participation. In the first level the ontology OWL and the LSA method are used to perform a semantic classification system of the threads according to a specific context chosen by the tutor. Then the Text mining, as a predictive method, is applied to the classified threads and learners' participation in the forums.",
        "link": "https://dl.acm.org/doi/10.1145/3175628.3175655"
    },
    {
        "title": "Investigate Transitions into Drug Addiction through Text Mining of Reddit Data",
        "authors": [
            "John Lu",
            "Sumati Sridhar",
            "Ritika Pandey",
            "Mohammad Al Hasan",
            "Georege Mohler"
        ],
        "date": "July 2019",
        "source": "KDD '19: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Increasing rates of opioid drug abuse and heightened prevalence of online support communities underscore the necessity of employing data mining techniques to better understand drug addiction using these rapidly developing online resources. In this work, we obtained data from Reddit, an online collection of forums, to gather insight into drug use/misuse using text snippets from users narratives. Specifically, using users' posts, we trained a binary classifier which predicts a user's transitions from casual drug discussion forums to drug recovery forums. We also proposed a Cox regression model that outputs likelihoods of such transitions. In doing so, we found that utterances of select drugs and certain linguistic features contained in one's posts can help predict these transitions. Using unfiltered drug-related posts, our research delineates drugs that are associated with higher rates of transitions from recreational drug discussion to support/recovery discussion, offers insight into modern drug culture, and provides tools with potential applications in combating the opioid crisis.",
        "link": "https://dl.acm.org/doi/10.1145/3292500.3330737"
    },
    {
        "title": "Analysing digital banking reviews using text mining",
        "authors": [
            "Li Chen Cheng",
            "Legaspi Rhea Sharmayne"
        ],
        "date": "December 2020",
        "source": "ASONAM '20: Proceedings of the 12th IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Digital banks are new entrants in the banking industry in the Philippines as they only started late 2018. Since then, a handful of players have and are still emerging. With more and more people becoming technologically savvy, it is very critical for financial institutions to develop a digital banking application that will stand out from the competition. This paper aims to use text mining methods to analyse digital banking application reviews. This study will perform topic modelling using LDA to explore customer concerns and will mine association rules between the digital banking features with the review score. The results will reveal which areas the digital banking application can further optimize for customer satisfaction and retention.",
        "link": "https://dl.acm.org/doi/10.1109/ASONAM49781.2020.9381429"
    },
    {
        "title": "Mining Moocs Videos Metadata Using Classification Techniques",
        "authors": [
            "El Harrak Othman",
            "Ghadi Abderrahim",
            "El Bouhdidi Jaber"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "In few years, the internet has become the first source of information for most people, Today MOOCs makes it possible for everyone to access to the education over the world, it's represents an emerging methodology of online teaching and an important development in open education. Due to the rapid development of Moocs and the rapid growth of digital data and video database over the Internet, it is becoming very difficult for learners to browsing and choosing the best training for them. The scientific community has increased the amount of research into new technologies, with a view to improve the digital video utilization: its archiving, indexing, accessibility, acquisition, store and even its process and usability. All these parts of the video utilization entail the necessity of the extraction of all important information of a video, especially in cases of lack of metadata information. Web video mining is retrieving the content using data mining techniques from World Wide Web. There are two approaches for web video mining using traditional image processing (signal processing) and metadata based approach. In this paper, we present the various research makes in this subject and we propose an effective methodology to extract the metadata from Moocs videos and classify them based on the extracted metadata by applying data mining techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090450"
    },
    {
        "title": "Text mining with HathiTrust",
        "authors": [
            "Eleanor Dickson Koehl",
            "Ryan Dubnicek"
        ],
        "date": "June 2019",
        "source": "JCDL '19: Proceedings of the 18th Joint Conference on Digital Libraries",
        "abstract": "This tutorial will introduce attendees to the HathiTrust Research Center and its tools and services for computational text analysis research. HTRC leverages the scope and scale of the HathiTrust Digital Library collection to create opportunities for researchers to perform text data mining on subsets of the corpus. Attendees will develop skills that will allow them to conduct basic text analysis research using HathiTrust data, learn techniques for engaging with scholars on their text data mining research, and explore strategies for computational access to large-scale digital collections.",
        "link": "https://dl.acm.org/doi/10.1109/JCDL.2019.00115"
    },
    {
        "title": "Text mining for plagiarism detection: multivariate pattern detection for recognition of text similarities",
        "authors": [
            "Konstantinos Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2018",
        "source": "ASONAM '18: Proceedings of the 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "The problem of plagiarism the recent years has been intensified by the availability of information in digital form and the accessibility of the electronic libraries through the Internet. As a result, plagiarism detection has been transformed into a big data analytics problem since the number of digital sources is extravagant and a new document needs to be compared with millions of other existing documents. In this paper, a text mining methodology is proposed that can detect all common patterns between a document and the documents in a reference database. The technique is based on a pattern detection algorithm and the corresponding data structure that enables the algorithm to detect all common patterns. The methodology has been applied in a well-defined dataset providing very promising results identifying difficult cases of plagiarism such as technical disguise.",
        "link": "https://dl.acm.org/doi/10.5555/3382225.3382424"
    },
    {
        "title": "Petroleum Engineering Data Text Classification Using Convolutional Neural Network Based Classifier",
        "authors": [
            "Tzuhan Hsu",
            "Yaoqin Zhang"
        ],
        "date": "May 2018",
        "source": "ICMLT '18: Proceedings of the 2018 International Conference on Machine Learning Technologies",
        "abstract": "In this paper, authors introduce a convolutional neural network (CNN) based classification approach for petroleum engineering event. Recently, text classification problem has raised substantial attention in the field of machine learning. The aim of text classification process is to assign correct label to a text on the basis of its content. However, due to the imprecise of feature representation, most of former text classification methods cannot export satisfying result. To solve this problem, authors propose a model based on convolutional neural network. Authors conduct experiments with real world petroleum engineering data text, artificially labelled into 6 categories. Beside purposed CNN-based model, the authors also implement other baseline machine learning approaches: k nearest neighbor, random forest, support vector machines, recurrent neural network, gated recurrent units, long short-term memory. Among all these models, our CNN-based model reach best performance in text classification task. In the last part of this paper, authors compare and analysis their purpose and baseline model.",
        "link": "https://dl.acm.org/doi/10.1145/3231884.3231898"
    },
    {
        "title": "Classifying Indonesian Online Articles as Advertisement Placement Base Using Text Mining",
        "authors": [
            "Nadhira Tasya",
            "Arian Dhini"
        ],
        "date": "July 2017",
        "source": "ICBIM 2017: Proceedings of the International Conference on Business and Information Management",
        "abstract": "Rapid development in technological aspect resulting in growing level of human needs for the latest news, so that emerged a new trend of publishing and accessing news through online media or usually called online journalism. In addition, the number of people who sell and purchase through online sites also continues to increase and this opportunity is utilized by the company and the advertiser by implementing targeted web advertising. However, the high number of articles that have been published and accessed leads to great opportunities for errors in determining where to place the ads. Therefore, it needs a system that can categorize articles accessed by users as the basis of advertisement placement by the company and this classification system can be done by applying the method of Data Mining and Text Mining. This research uses document data in the form of article content that will be categorized into twenty categories of class of advertisement by using Text Mining technique with Support Vector Machine algorithm. The results of this study may be used by companies or advertisers as a basis for placement of ads on selected online media sites.",
        "link": "https://dl.acm.org/doi/10.1145/3134271.3134288"
    },
    {
        "title": "Mining Free-Text Medical Notes for Suicide Risk Assessment",
        "authors": [
            "Marios Adamou",
            "Grigoris Antoniou",
            "Elissavet Greasidou",
            "Vincenzo Lagani",
            "Paulos Charonyktakis",
            "Ioannis Tsamardinos"
        ],
        "date": "July 2018",
        "source": "SETN '18: Proceedings of the 10th Hellenic Conference on Artificial Intelligence",
        "abstract": "Suicide has been considered as an important public health issue for a very long time, and is one of the main causes of death worldwide. Despite suicide prevention strategies being applied, the rate of suicide has not changed substantially over the past decades. Advances in machine learning make it possible to attempt to predict suicide based on the analysis of relevant data to inform clinical practice. This paper reports on findings from the analysis of data of patients who died by suicide in the period 2013-2016 and made use of both structured data and free-text medical notes. We focus on examining various text-mining approaches to support risk assessment. The results show that using advance machine learning and text-mining techniques, it is possible to predict within a specified period which people are most at risk of taking their own life at the time of referral to a mental health service.",
        "link": "https://dl.acm.org/doi/10.1145/3200947.3201020"
    },
    {
        "title": "Integration of data mining techniques in e-learning systems: Clustering Profil of Lerners and Recommender Course System",
        "authors": [
            "A. El Moustamid",
            "E. En-Naimi",
            "J. El Bouhdidi"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "Data mining aims at the extraction of knowledge from large quantities of data by automatic or semi-automatic methods. In the field of education and training e-learning, Data Mining techniques as clustering (The aim of this technique is to identify groups that are similar in some aspect) [21], prediction (is used for developing model where combination of data viewed from different aspects can effect single viewed data. Here, is important to watch how predictors variables are effecting predicted variable)[21], classification are applied in e-learning to improve the level of learners by providing learning according to the learners' requirement) are used to analyze learners profile, to predict and improve student performance. The aim of our project is to develop a system capable of analyzing learners profile and indexing web videos in order to offer learners a rich database with courses that correspond to their levels.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090453"
    },
    {
        "title": "Multi-level mining and visualization of scientific text collections: Exploring a bi-lingual scientific repository",
        "authors": [
            "Pablo Accuosto",
            "Francesco Ronzano",
            "Daniel Ferrés",
            "Horacio Saggion"
        ],
        "date": "December 2017",
        "source": "WOSP 2017: Proceedings of the 6th International Workshop on Mining Scientific Publications",
        "abstract": "We present a system to mine and visualize collections of scientific documents by semantically browsing information extracted from single publications or aggregated throughout corpora of articles. The text mining tool performs deep analysis of document collections allowing the extraction and interpretation of research paper's contents. In addition to the extraction and enrichment of documents with metadata (titles, authors, affiliations, etc), the deep analysis performed comprises semantic interpretation, rhetorical analysis of sentences, triple-based information extraction, and text summarization. The visualization components allow geographical-based exploration of collections, topic-evolution interpretation, and collaborative network analysis among others. The paper presents a case study of a bi-lingual collection in the field of Natural Language Processing (NLP).",
        "link": "https://dl.acm.org/doi/10.1145/3127526.3127529"
    },
    {
        "title": "Hierarchical Topic Mining via Joint Spherical Tree and Text Embedding",
        "authors": [
            "Yu Meng",
            "Yunyi Zhang",
            "Jiaxin Huang",
            "Yu Zhang",
            "Chao Zhang",
            "Jiawei Han"
        ],
        "date": "August 2020",
        "source": "KDD '20: Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Mining a set of meaningful topics organized into a hierarchy is intuitively appealing since topic correlations are ubiquitous in massive text corpora. To account for potential hierarchical topic structures, hierarchical topic models generalize flat topic models by incorporating latent topic hierarchies into their generative modeling process. However, due to their purely unsupervised nature, the learned topic hierarchy often deviates from users' particular needs or interests. To guide the hierarchical topic discovery process with minimal user supervision, we propose a new task, Hierarchical Topic Mining, which takes a category tree described by category names only, and aims to mine a set of representative terms for each category from a text corpus to help a user comprehend his/her interested topics. We develop a novel joint tree and text embedding method along with a principled optimization procedure that allows simultaneous modeling of the category tree structure and the corpus generative process in the spherical space for effective category-representative term discovery. Our comprehensive experiments show that our model, named JoSH, mines a high-quality set of hierarchical topics with high efficiency and benefits weakly-supervised hierarchical text classification tasks.",
        "link": "https://dl.acm.org/doi/10.1145/3394486.3403242"
    },
    {
        "title": "Unsupervised behavioural mining and clustering for malware family identification",
        "authors": [
            "Khanh Huu The Dam",
            "Thomas Given-Wilson",
            "Axel Legay"
        ],
        "date": "March 2021",
        "source": "SAC '21: Proceedings of the 36th Annual ACM Symposium on Applied Computing",
        "abstract": "More accurate and advanced detection and classification of malware requires exploiting program behaviour and not merely syntactic or static features. One approach is to use system call dependency graphs (SCDGs) that represent the program behaviour by interactions with the system, and the relations between these interactions. These SCDGs have been used with supervised learning techniques to very accurately detect and classify malware. This works considers the unsupervised learning challenge of mining for common clusters of behaviour without a priori knowledge. This allows for clustering of similar programs by behaviour, that can then be used for either classification or further analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3412841.3441919"
    },
    {
        "title": "Text Mining for Evaluating Authors' Birth and Death Years",
        "authors": [
            "Dror Moghaz",
            "Yaakov Hacohen-Kerner",
            "Dov Gabbay"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "This article presents a unique method in text and data mining for finding the era, i.e., mining temporal data, in which an anonymous author was living. Finding this era can assist in the examination of a fake document or extracting the time period in which a writer lived. The study and the experiments concern Hebrew, and in some parts, Aramaic and Yiddish rabbinic texts. The rabbinic texts are undated and contain no bibliographic sections, posing an interesting challenge. This work proposes algorithms using key phrases and key words that allow the temporal organization of citations together with linguistic patterns. Based on these key phrases, key words, and the references, we established several types of “Iron-clad,” Heuristic and Greedy rules for estimating the years of birth and death of a writer in an interesting classification task. Experiments were conducted on corpora, including documents authored by 12, 24, and 36 rabbinic writers and demonstrated promising results.",
        "link": "https://dl.acm.org/doi/10.1145/3281631"
    },
    {
        "title": "Transfer Learning to Timed Text Based Video Classification Using CNN",
        "authors": [
            "Zenun Kastrati",
            "Ali Shariq Imran",
            "Arianit Kurti"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "Open educational video resources are gaining popularity with a growing number of massive open online courses (MOOCs). This has created a niche for content providers to adopt effective solutions in automatically organizing and structuring of educational resources for maximum visibility. Recent advances in deep learning techniques are proving useful in managing and classifying resources into appropriate categories. This paper proposes one such convolutional neural network (CNN) model for classifying video lectures in a MOOC setting using a transfer learning approach. The model uses a time-aligned text transcripts corresponding to video lectures from six broader subject categories. Video lectures and their corresponding transcript dataset is gathered from the Coursera MOOC platform. Two different CNN models are proposed: i) CNN based classification using embeddings learned from our MOOC dataset, ii) CNN based classification using transfer learning. Word embeddings generated from two well known state-of-the-art pre-trained models Word2Vec and GloVe, are used in the transfer learning approach for the second case. The proposed CNN models are evaluated using precision, recall, and F1 score and the obtained performance is compared with both conventional and deep learning classifiers. The proposed CNN models have an F1 score improvement of 10-22 percentage points over DNN and conventional classifiers",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326483"
    },
    {
        "title": "Evolutionary role mining in complex networks by ensemble clustering",
        "authors": [
            "Sarvenaz Choobdar",
            "Pedro Ribeiro",
            "Fernando Silva"
        ],
        "date": "April 2017",
        "source": "SAC '17: Proceedings of the Symposium on Applied Computing",
        "abstract": "The structural patterns in the neighborhood of nodes assign unique roles to the nodes. Mining the set of existing roles in a network provides a descriptive profile of the network and draws its general picture. This paper proposes a new method to determine structural roles in a dynamic network based on the current position of nodes and their historic behavior. We develop a temporal ensemble clustering technique to dynamically find groups of nodes, holding similar tempo-structural roles. We compare two weighting functions, based on age and distribution of data, to incorporate temporal behavior of nodes in the role discovery. To evaluate the performance of the proposed method, we assess the results from two points of view: 1) goodness of fit to current structure of the network; 2) consistency with historic data. We conduct the evaluation using different ensemble clustering techniques. The results on real world networks demonstrate that our method can detect tempo-structural roles that simultaneously depict the topology of a network and reflect its dynamics with high accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3019612.3019815"
    },
    {
        "title": "Analyzing used-car web listings via text mining",
        "authors": [
            "Ayhan Demiriz",
            "Fatma Cantaş"
        ],
        "date": "October 2017",
        "source": "IML '17: Proceedings of the 1st International Conference on Internet of Things and Machine Learning",
        "abstract": "Used car trade is one of the major components of the world economies. It is not uncommon to sell a car by placing an internet advertisement irrespective of the geography in these days. A typical content of an advertisement is usually composed of two parts namely the structured and the free text data. The structured data may include some information about the asking price, make, model, year, mileage of the car and the contact info. In most cases, seller may give important clues about the car's current conditions in the free text data where the title (head) of the advertisement can be included as free text too. This paper reports preliminary results from a text mining study conducted on 75K used car internet listings collected from two major car listing web sites in Turkey. As expected, the words and the phrases related to the description of the car are observed to be frequent. The leading concepts in the free text are found to be regarding how to describe the current condition of a car, for example \"no crash history\".",
        "link": "https://dl.acm.org/doi/10.1145/3109761.3109782"
    },
    {
        "title": "Text classification of Diseases Treated by Traditional Chinese Medicine Prescription based on machine learning",
        "authors": [
            "Hanqing Zhao",
            "Bo Han",
            "Chen Li"
        ],
        "date": "September 2020",
        "source": "ISAIMS '20: Proceedings of the 1st International Symposium on Artificial Intelligence in Medical Sciences",
        "abstract": "OBJECTIVE: To explore the application of machine learning in th e identification of diseases treated by traditional Chinese medicine prescriptions. METHODS: Based on the composition of the text document of Chinese medicine prescriptions, the prescriptions were divided into cough, headache and diarrhea. THUTCT were introduced to establish and train two machine learning text classification models, LibSVM and LibLinear, and the prescriptions to be deter mined were put into the model for classification and prediction. R ESULTS: The Precision rate of LibSVM model and Liblinear mod el were 0.7283 and 0.6690. Seven prescriptions were classified an d predicted, and the results were in line with expectations. CONCLUSION: THUCTC has good universality for the content of TCM prescriptions, high classification accuracy and fast testing speed, which is suitable for the text classification and discrimination research of TCM prescriptions for diseases.",
        "link": "https://dl.acm.org/doi/10.1145/3429889.3429895"
    },
    {
        "title": "Evaluation of retweet clustering method classification method using retweets on Twitter without text data",
        "authors": [
            "K. Uchida",
            "F. Toriumi",
            "T. Sakaki"
        ],
        "date": "August 2017",
        "source": "WI '17: Proceedings of the International Conference on Web Intelligence",
        "abstract": "Burst phenomena, which frequently occur on social media, are caused by such social events as flaming on the internet, elections, and natural disasters. To understand people's thoughts and feelings, we must classify their opinions from burst phenomena. Therefore, classification methods that categorize tweets are critical. However, since most classification methods focus on text mining, they cannot group tweets by topics because each tweet has poor linguistic similarities. We used a non-text-based classification method proposed by Baba et al. that groups tweets by topics, even if they have poor linguistic similarities, and verified its validity by comparing it with a text-based classification method in two different evaluations: qualitative and quantitative. In the qualitative evaluation part, we did a questionnaire survey and validated the suitability of the topic clusters created using both the non-and text-based methods. Since evaluating the similarity of every pair of tweets in each topic is difficult, we evaluated the similarity between sampled pairs in the survey and acquired more appropriate topic clustering results using the non-text-based method than the text-based method. In the quantitative evaluation part, we focused on the robustness of each method against data reduction. Many approaches analyze social media data, especially because collecting data from social media is comparatively easy. However, since collecting the whole data of burst phenomena is very costly due to the vast amounts of available social media data, robustness against data reduction is an important index to evaluate classification methods. With the non-text-based method, over 55% of the pairs of tweets in the same cluster were also included in the same cluster even when the data were reduced to 10% in all three of our example cases. In this paper, as a source we focus on Twitter, one of the most popular microblogging services. Using clustering to conduct detailed case analyses, we scrutinized three burst cases that include natural disasters and flaming on the internet and found that a non-text-based method more effectively classified tweets in burst phenomena than a text-based method.",
        "link": "https://dl.acm.org/doi/10.1145/3106426.3106451"
    },
    {
        "title": "Using Spark for Text Mining on Large Scale Liver Cancer Literature",
        "authors": [
            "Ming-Yen Lin",
            "Yu-Ju Lin",
            "Sue-Chen Hsueh"
        ],
        "date": "January 2021",
        "source": "BDET 2021: 2021 the 3rd International Conference on Big Data Engineering and Technology (BDET)",
        "abstract": "Cancer is one of the main causes of death. The number of known cancer to-date is more than one hundred. Liver cancer, ranked after trachea and lung cancer, has long been high in the cancer leading cause in Taiwan and even ranked as second in 2016. The number of scientific articles related to cancer proliferates every year. The number reaches as high as 20 million in PubMed so that discovering useful information from the massive collection is very difficult. In addition, using a single machine to sift through these articles is very time-consuming. Therefore, we present a big data analytic framework using the distributed Apache Spark platform for text mining in PubMed literature. We establish a prediction model for liver cancer articles so that researchers may effectively validate whether an article is related to liver cancer or not. Classification models in Spark MLlib including Linear Support Vector Machines (SVM), Logistic Regression, are used in our experiments. Relevancy to liver cancer is further confirmed by using MeSH (Medical Subject Headings) terms. Logistic regression is about 3 times faster than SVMs and the accuracy of both methods is close to 95% in the experiments using hold-out validation. When max_features is 500 and min_df ≤ 0.1 (or min_df = 1), the accuracy may reach 96%. In the experiments with K-fold cross-validation, the accuracy of SVMs methods is 96%. The experimental results show that that our prediction model may effectively classify liver cancer articles.",
        "link": "https://dl.acm.org/doi/10.1145/3474944.3474958"
    },
    {
        "title": "Automatic topic classification of test cases using text mining at an Android smartphone vendor",
        "authors": [
            "Junji Shimagaki",
            "Yasutaka Kamei",
            "Naoyasu Ubayashi",
            "Abram Hindle"
        ],
        "date": "October 2018",
        "source": "ESEM '18: Proceedings of the 12th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Background: An Android smartphone is an ecosystem of applications, drivers, operating system components, and assets. The volume of the software is large and the number of test cases needed to cover the functionality of an Android system is substantial. Enormous effort has been already taken to properly quantify \"what features and apps were tested and verified?\". This insight is provided by dashboards that summarize test coverage and results per feature. One method to achieve this is to manually tag or label test cases with the topic or function they cover, much like function points. At the studied Android smartphone vendor, tests are labelled with manually defined tags, so-called \"feature labels (FLs)\", and the FLs serve to categorize 100s to 1000s test cases into 10 to 50 groups. Aim: Unfortunately for developers, manual assignment of FLs to 1000s of test cases is a time consuming task, leading to inaccurately labeled test cases, which will render the dashboard useless. We created an automated system that suggests tags/labels to the developers for their test cases rather than manual labeling. Method: We use machine learning models to predict and label the functionality tested by 10,000 test cases developed at the company. Results: Through the quantitative experiments, our models achieved acceptable F-1 performance of 0.3 to 0.88. Also through the qualitative studies with expert teams, we showed that the hierarchy and path of tests was a good predictor of a feature's label. Conclusions: We find that this method can reduce tedious manual effort that software developers spent classifying test cases, while providing more accurate classification results.",
        "link": "https://dl.acm.org/doi/10.1145/3239235.3268927"
    },
    {
        "title": "To apply Data Mining for Classification of Crowd sourced Software Requirements",
        "authors": [
            "Soonh Taj",
            "Qasim Arain",
            "Imran Memon",
            "Asma Zubedi"
        ],
        "date": "April 2019",
        "source": "ICSIE '19: Proceedings of the 8th International Conference on Software and Information Engineering",
        "abstract": "Now a day's main focus of developers is to build quality software that works according to customer needs and for this reason it is necessary to gather right requirements as requirement elicitation is the critical step that impacts on the success of software project as misinterpreted requirements leads to the failure of software project. By keeping this in mind a research is carried out on improving requirements elicitation process and automating the process of classifying requirements. In this research, a model is proposed which will help in this scenario for requirements elicitation and requirement classification. This paper presents a model in which crowd sourcing approach is used so that customers, end users, stakeholders, developers and software engineers can make active participation for requirement elicitation process and requirements gathered using crowdsourcing approach are used by model for classification process i.e. classification of requirements into functional and non-functional requirements. For the proof of proposed model a case study is conducted. Results of case study provided the usefulness and efficiency of proposed model for classification of crowd sourced software requirements.",
        "link": "https://dl.acm.org/doi/10.1145/3328833.3328837"
    },
    {
        "title": "Text classification using Fuzzy TF-IDF and Machine Learning Models",
        "authors": [
            "Mariem Bounabi",
            "Karim El Moutaouakil",
            "Khalid Satori"
        ],
        "date": "October 2019",
        "source": "BDIoT'19: Proceedings of the 4th International Conference on Big Data and Internet of Things",
        "abstract": "The representation of the information has an important impact on the text classification task. Several weighting methods were proposed in the literature, and the term frequency-inverse term frequency (TFIDF), the most know on the text treatment field. The FTF-IDF is a vector representation where the components of the TFIDF are presented as inputs to the Fuzzy Inference System (FIS). In this work, we compare several Machin Learning algorithms such as Naïve Bayes and its derivatives, SVM and Random forest classifiers, using the FTF-IDF representation. To improve the quality of the used classifiers, we call sum attribute selection methods. The recognition rate, for the tested systems, is satisfied, where the system based on naïve Bayes classifier, the FTF-IDF weighting terms, and the info gain select attributes method gives 98.7% as accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3372938.3372956"
    },
    {
        "title": "Text Mining in Cybersecurity: A Systematic Literature Review",
        "authors": [
            "Luciano Ignaczak",
            "Guilherme Goldschmidt",
            "Cristiano André Da Costa",
            "Rodrigo Da Rosa Righi"
        ],
        "date": null,
        "source": "ACM Computing Surveys",
        "abstract": "The growth of data volume has changed cybersecurity activities, demanding a higher level of automation. In this new cybersecurity landscape, text mining emerged as an alternative to improve the efficiency of the activities involving unstructured data. This article proposes a Systematic Literature Review (SLR) to present the application of text mining in the cybersecurity domain. Using a systematic protocol, we identified 2,196 studies, out of which 83 were summarized. As a contribution, we propose a taxonomy to demonstrate the different activities in the cybersecurity domain supported by text mining. We also detail the strategies evaluated in the application of text mining tasks and the use of neural networks to support activities involving unstructured data. The work also discusses text classification performance aiming its application in real-world solutions. The SLR also highlights open gaps for future research, such as the analysis of non-English content and the intensification in the usage of neural networks.",
        "link": "https://dl.acm.org/doi/10.1145/3462477"
    },
    {
        "title": "Using Text Mining to Analyze the Financial Patents",
        "authors": [
            "Yung-Feng Lu",
            "Jia-Lang Xu",
            "Mu-Yen Chen"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "With the advancement and development of science and technology, Nowadays society is the development of information technology, cloud computing, mobile Internet, Internet of Things, block chain and information technology.The combination of finance and technology has made financial technology terms gradually emerging.. This research analyzes the patents of financial technology. However, Taiwan's financial technology patents are mostly G06Q-20 (payment plan, architecture or agreement), G06Q-30 (commercial, such as marketing, shopping, payment, auction or e-commerce). G06Q-40 (financial, investment or tax treatment, insurance).The research use text mining technology to analyze and understand Taiwan's trends and developments in financial technology. The research results show that G06Q-20 patents are biased towards information security and mobile payment. G06Q-30 patents favor shopping websites and advertising. G06Q-40 patents are biased towards diversity, such as securities, insurance, finance, and stocks.",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326478"
    },
    {
        "title": "Identifying Online Profiles of Distance Learning Students Using Data Mining Techniques",
        "authors": [
            "Osama Islam",
            "Muazzam Siddiqui",
            "Naif Radi Aljohani"
        ],
        "date": "October 2019",
        "source": "ICDTE '19: Proceedings of the 3rd International Conference on Digital Technology in Education",
        "abstract": "Educational data has grown over the years with the increased use of technology within educational environments. This has led to a huge amount of data being stored in various data sources representing the student, his/her activities, and other aspects relevant to the learning process. To meet this analytical need, Educational Data Mining (EDM) has emerged to assist educational institutions in identifying key benefits such as students at risk, the level of student engagement or predicting student performance. The aim of this research was to explore the various aspects of student interaction data using data mining techniques to identify relevant patterns of behaviors and possible key attributes that have higher degrees of influence on distance learning students. The main findings identified several patterns of user online profiles based on a set of adopted learning strategies, the research proposed a framework for analyzing such interaction data based on R and Hadoop platforms to correlate online profiles with student performance.",
        "link": "https://dl.acm.org/doi/10.1145/3369199.3369249"
    },
    {
        "title": "Extremely Fast Decision Tree Mining for Evolving Data Streams",
        "authors": [
            "Albert Bifet",
            "Jiajin Zhang",
            "Wei Fan",
            "Cheng He",
            "Jianfeng Zhang",
            "Jianfeng Qian",
            "Geoff Holmes",
            "Bernhard Pfahringer"
        ],
        "date": "August 2017",
        "source": "KDD '17: Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
        "abstract": "Nowadays real-time industrial applications are generating a huge amount of data continuously every day. To process these large data streams, we need fast and efficient methodologies and systems. A useful feature desired for data scientists and analysts is to have easy to visualize and understand machine learning models. Decision trees are preferred in many real-time applications for this reason, and also, because combined in an ensemble, they are one of the most powerful methods in machine learning. In this paper, we present a new system called STREAMDM-C++, that implements decision trees for data streams in C++, and that has been used extensively at Huawei. Streaming decision trees adapt to changes on streams, a huge advantage since standard decision trees are built using a snapshot of data, and can not evolve over time. STREAMDM-C++ is easy to extend, and contains more powerful ensemble methods, and a more efficient and easy to use adaptive decision trees. We compare our new implementation with VFML, the current state of the art implementation in C, and show how our new system outperforms VFML in speed using less resources.",
        "link": "https://dl.acm.org/doi/10.1145/3097983.3098139"
    },
    {
        "title": "Data miners' little helper: data transformation activity cues for cluster analysis on document collections",
        "authors": [
            "Tania Cerquitelli",
            "Evelina Di Corso",
            "Francesco Ventura",
            "Silvia Chiusano"
        ],
        "date": "June 2017",
        "source": "WIMS '17: Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "In this paper we propose a new self-learning engine to streamline the analytics process, as it enables analysts to mine massive data repositories with minimal user intervention. In the context of cluster analysis on a collection of documents this new system, named SELF-DATA (SELF-learning DAta TrAnsformation), suggests to the analyst how to configure the whole mining process for a given dataset. SELF-DATA relies on an engine exploring different data weighting schemas (e.g., normalized term frequencies) and data transformation methods (e.g., PCA) before applying the cluster analysis, evaluating and comparing solutions through different quality indices (e.g., weighted Silhouette), and presenting the k-top solutions to the analyst. SELF-DATA will also include a knowledge base storing results of experiments on previously processed datasets, and a classification algorithm trained on the knowledge base content to forecast the best configuration for the whole mining process for an unexplored dataset. The first development of SELF-DATA running on Apache Spark has been validated on 5 collections of documents. Experimental results highlight that TF-IDF and logarithmic entropy are effective to measure item relevance with sparse datasets, and the LSI method outperforms PCA with a large dictionary.",
        "link": "https://dl.acm.org/doi/10.1145/3102254.3102288"
    },
    {
        "title": "Interactive Visual Graph Mining and Learning",
        "authors": [
            "Ryan A. Rossi",
            "Nesreen K. Ahmed",
            "Rong Zhou",
            "Hoda Eldardiry"
        ],
        "date": null,
        "source": "ACM Transactions on Intelligent Systems and Technology",
        "abstract": "This article presents a platform for interactive graph mining and relational machine learning called GraphVis. The platform combines interactive visual representations with state-of-the-art graph mining and relational machine learning techniques to aid in revealing important insights quickly as well as learning an appropriate and highly predictive model for a particular task (e.g., classification, link prediction, discovering the roles of nodes, and finding influential nodes). Visual representations and interaction techniques and tools are developed for simple, fast, and intuitive real-time interactive exploration, mining, and modeling of graph data. In particular, we propose techniques for interactive relational learning (e.g., node/link classification), interactive link prediction and weighting, role discovery and community detection, higher-order network analysis (via graphlets, network motifs), among others. GraphVis also allows for the refinement and tuning of graph mining and relational learning methods for specific application domains and constraints via an end-to-end interactive visual analytic pipeline that learns, infers, and provides rapid interactive visualization with immediate feedback at each change/prediction in real-time. Other key aspects include interactive filtering, querying, ranking, manipulating, exporting, as well as tools for dynamic network analysis and visualization, interactive graph generators (including new block model approaches), and a variety of multi-level network analysis techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3200764"
    },
    {
        "title": "Data Mining and Opinion Mining: A Tool in Educational Context",
        "authors": [
            "Myriam Peñafiel",
            "Stefanie Vásquez",
            "Diego Vásquez",
            "Juan Zaldumbide",
            "Sergio Luján-Mora"
        ],
        "date": "July 2018",
        "source": "ICoMS '18: Proceedings of the 2018 1st International Conference on Mathematics and Statistics",
        "abstract": "The use of the web as a universal communication platform generates large volumes of data (Big data), which in many cases, need to be processed so that they can become useful knowledge in face of the sceptics who have doubts about the credibility of such information. The use of web data that comes from educational contexts needs to be addressed, since that large amount of unstructured information is not being valued, losing valuable information that can be used. To solve this problem, we propose the use of data mining techniques such as sentiment analysis to validate the information that comes from the educational platforms. The objective of this research is to propose a methodology that allows the user to apply sentiment analysis in a simple way, because although some researchers have done it, very few do with data in the educational context. The results obtained prove that the proposal can be used in similar cases.",
        "link": "https://dl.acm.org/doi/10.1145/3274250.3274263"
    },
    {
        "title": "Research Hotspots Mining and Visualized Analysis Based on Linking Cluster and K-Core Decomposition",
        "authors": [
            "Ying Cai",
            "Fang Huang",
            "Shuai Wang",
            "Hao Zhang",
            "Chunxiu Du"
        ],
        "date": "May 2018",
        "source": "ICDPA 2018: Proceedings of the International Conference on Data Processing and Applications",
        "abstract": "In this paper, we focus on research hotspots mining with knowledge semantic features on discipline topic word networks, which are extracted from scientific and technical documents. For the topic networks, a novel method of combining linking community clustering with k-core decomposition is proposed to mine the hotspot community with knowledge hierarchy structure. And as a post-processing, the hierarchical community identification is implemented by k-core decomposition in order to discover the internal connection with knowledge semantic hierarchy in the research hotspot. Finally, the community density and visualization method is utilized to analyze the core and sub- problem in the research hotspot community. In experiments, the topic networks for a single discipline and interdisciplinary research hotspot are taken as examples of analysis, and the results show the proposed approach can effectively realize the objective of mining and analyzing the discipline research hotspot on the academic topic word network.",
        "link": "https://dl.acm.org/doi/10.1145/3224207.3224216"
    },
    {
        "title": "The Application of Data Mining Algorithms in the Construction of Travel Recommendation System",
        "authors": [
            "Chao Lou",
            "Zhaonan Mu",
            "Mengzhu Liu"
        ],
        "date": "January 2021",
        "source": "CONF-CDS 2021: The 2nd International Conference on Computing and Data Science",
        "abstract": "Photos shared by users of online social network often associated with tags, texts, geographic and time information. These data are ideal research resource which can be used by researchers to observe people's interests and behaviors. They can also be used to analyze people's travel behaviors and interests because it can reflect people's travel activities and experiences directly. This project aimed to consolidate people into groups based on their travel preferences of travel season, travel time span, scenery type of travel location and then analyze popular travel places for different people groups. The travel preferences were analyzed from extracted data applying PHP language and Naive Bayes classification approach. According to the analyzed data, people were divided into groups. Then, DBSCAN clustering was used to get popular locations based on the different groups. As a result, 80 thousand photo data of 5 cities have been collected from Flickr by Phython. It has been showed that people's preference of season in most of the mined cities was spring and summer and the preference of time span was short. Moreover, mined people has been divided into 16 groups according to their preference and based on the groups popular locations have been calculated by DBSCAN clustering. Theses locations have been provided in a recommendation website which was built in this project. For the future work, mining Flickr data associated with other travel social network sites could be a good way to build a more effective recommendation system.",
        "link": "https://dl.acm.org/doi/10.1145/3448734.3450477"
    },
    {
        "title": "Compare Machine Learning Models in Text Classification Using Steam User Reviews",
        "authors": [
            "Youchen Miao",
            "Zeyu Jin",
            "Yumeng Zhang",
            "Yuchen Chen",
            "Junren Lai"
        ],
        "date": "November 2021",
        "source": "ICSED '21: Proceedings of the 2021 3rd International Conference on Software Engineering and Development",
        "abstract": "Text Classification and Sentiment Analysis of game reviews are viewed as important parts in not only academic fields but also in game studies. In this paper, with more than 400 thousand game reviews on Steam platform, we preprocess the data using different libraries (sklearn, nltk, and spaCy) and use them as inputs to build three sentiment classification models based on different algorithms (Naive Bayes, SVM, and Random Forest). In contrast to previous studies that only focus on different sentiment analysis models, our paper also highlights the use of different APIs to preprocess the data and their corresponding model performance. The results show that no matter which API we choose, Random Forest models always perform the best. However, in terms of training time, Naive Bayes is the fastest. This work can be used to apply grid search for researchers to automatically find the optimum API before conducting sentiment analysis in the future.",
        "link": "https://dl.acm.org/doi/10.1145/3507473.3507480"
    },
    {
        "title": "Lost in Transduction: Transductive Transfer Learning in Text Classification",
        "authors": [
            "Alejandro Moreo",
            "Andrea Esuli",
            "Fabrizio Sebastiani"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Obtaining high-quality labelled data for training a classifier in a new application domain is often costly. Transfer Learning\n(a.k.a. “Inductive Transfer”) tries to alleviate these costs by transferring, to the “target” domain of interest, knowledge available from a different “source” domain. In transfer learning the lack of labelled information from the target domain is compensated by the availability at training time of a set of unlabelled examples from the target distribution. Transductive Transfer Learning denotes the transfer learning setting in which the only set of target documents that we are interested in classifying is known and available at training time. Although this definition is indeed in line with Vapnik’s original definition of “transduction”, current terminology in the field is confused. In this article, we discuss how the term “transduction” has been misused in the transfer learning literature, and propose a clarification consistent with the original characterization of this term given by Vapnik. We go on to observe that the above terminology misuse has brought about misleading experimental comparisons, with inductive transfer learning methods that have been incorrectly compared with transductive transfer learning methods. We then, give empirical evidence that the difference in performance between the inductive version and the transductive version of a transfer learning method can indeed be statistically significant (i.e., that knowing at training time the only data one needs to classify indeed gives an advantage). Our clarification allows a reassessment of the field, and of the relative merits of the major, state-of-the-art algorithms for transfer learning in text classification.",
        "link": "https://dl.acm.org/doi/10.1145/3453146"
    },
    {
        "title": "Machine Learning on Mining Potential Adverse Drug Reactions for Pharmacovigilance",
        "authors": [
            "Ke Jia"
        ],
        "date": "July 2021",
        "source": "DSIT 2021: 2021 4th International Conference on Data Science and Information Technology",
        "abstract": "Drugs are related to the lives and health of patients. Mining and discovering potential adverse drug reactions from a large number of unstructured texts play an important role in drug research and pharmacovigilance. It is a hot research issue in the field of biomedicine in recent years. A large number of scholars have discovered a large amount of information on adverse drug reactions from different data sources through effective machine learning methods. This paper reviews the research progress in recent years to provide a reference for future research.",
        "link": "https://dl.acm.org/doi/10.1145/3478905.3478964"
    },
    {
        "title": "TACAM: Topic And Context Aware Argument Mining",
        "authors": [
            "Michael Fromm",
            "Evgeniy Faerman",
            "Thomas Seidl"
        ],
        "date": "October 2019",
        "source": "WI '19: IEEE/WIC/ACM International Conference on Web Intelligence",
        "abstract": "In this work we address the problem of argument search. The purpose of argument search is the distillation of pro and contra arguments for requested topics from large text corpora. In previous works, the usual approach is to use a standard search engine to extract text parts which are relevant to the given topic and subsequently use an argument recognition algorithm to select arguments from them. The main challenge in the argument recognition task, which is also known as argument mining, is that often sentences containing arguments are structurally similar to purely informative sentences without any stance about the topic. In fact, they only differ semantically. Most approaches use topic or search term information only for the first search step and therefore assume that arguments can be classified independently of a topic. We argue that topic information is crucial for argument mining, since the topic defines the semantic context of an argument. Precisely, we propose different models for the classification of arguments, which take information about a topic of an argument into account. Moreover, to enrich the context of a topic and to let models understand the context of the potential argument better, we integrate information from different external sources such as Knowledge Graphs or pre-trained NLP models. Our evaluation shows that considering topic information, especially in connection with external information, provides a significant performance boost for the argument mining task.",
        "link": "https://dl.acm.org/doi/10.1145/3350546.3352506"
    },
    {
        "title": "Text mining for incoming tasks based on the urgency/importance factors and task classification using machine learning tools",
        "authors": [
            "Yasser Ali Alshehri"
        ],
        "date": "March 2020",
        "source": "ICCDA 2020: Proceedings of the 2020 the 4th International Conference on Compute and Data Analysis",
        "abstract": "In workplaces, there is a massive amount of unstructured data from different sources. In this paper, we present a case study that explains how can through communications between employees, we can help to prioritize tasks requests to increase the efficiency of their works for both technical and non-technical workers. This involves managing daily incoming tasks based on their level of urgency and importance.To allow all workers to utilize the urgency-importance matrix as a time-management tool, we need to automate this tool. The textual content of incoming tasks are analyzed, and metrics related to urgency and importance are extracted. A third factor (i.e., the response variable) is defined based on the two input variables (urgency and importance). Then, machine learning applied to the data to predict the class of incoming tasks based on data outcome desired. We used ordinal regression, neural networks, and decision tree algorithms to predict the four levels of task priority. We measure the performance of all using recalls, precisions, and F-scores. All classifiers perform higher than 89% in terms of all measures.",
        "link": "https://dl.acm.org/doi/10.1145/3388142.3388153"
    },
    {
        "title": "Text mining for malware classification using multivariate all repeated patterns detection",
        "authors": [
            "Konstantinos F. Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2019",
        "source": "ASONAM '19: Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Mobile phones have become nowadays a commodity to the majority of people. Using them, people are able to access the world of Internet and connect with their friends, their colleagues at work or even unknown people with common interests. This proliferation of the mobile devices has also been seen as an opportunity for the cyber criminals to deceive smartphone users and steel their money directly or indirectly, respectively, by accessing their bank accounts through the smartphones or by blackmailing them or selling their private data such as photos, credit card data, etc. to third parties. This is usually achieved by installing malware to smartphones masking their malevolent payload as a legitimate application and advertise it to the users with the hope that mobile users will install it in their devices. Thus, any existing application can easily be modified by integrating a malware and then presented it as a legitimate one. In response to this, scientists have proposed a number of malware detection and classification methods using a variety of techniques. Even though, several of them achieve relatively high precision in malware classification, there is still space for improvement. In this paper, we propose a text mining all repeated pattern detection method which uses the decompiled files of an application in order to classify a suspicious application into one of the known malware families. Based on the experimental results using a real malware dataset, the methodology tries to correctly classify (without any misclassification) all randomly selected malware applications of 3 categories with 3 different families each.",
        "link": "https://dl.acm.org/doi/10.1145/3341161.3350841"
    },
    {
        "title": "Comparative Study between Traditional Machine Learning and Deep Learning Approaches for Text Classification",
        "authors": [
            "Cannannore Nidhi Kamath",
            "Syed Saqib Bukhari",
            "Andreas Dengel"
        ],
        "date": "August 2018",
        "source": "DocEng '18: Proceedings of the ACM Symposium on Document Engineering 2018",
        "abstract": "In this contemporaneous world, it is an obligation for any organization working with documents to end up with the insipid task of classifying truckload of documents, which is the nascent stage of venturing into the realm of information retrieval and data mining. But classification of such humongous documents into multiple classes, calls for a lot of time and labor. Hence a system which could classify these documents with acceptable accuracy would be of an unfathomable help in document engineering. We have created multiple classifiers for document classification and compared their accuracy on raw and processed data. We have garnered data used in a corporate organization as well as publicly available data for comparison. Data is processed by removing the stop-words and stemming is implemented to produce root words. Multiple traditional machine learning techniques like Naive Bayes, Logistic Regression, Support Vector Machine, Random forest Classifier and Multi-Layer Perceptron are used for classification of documents. Classifiers are applied on raw and processed data separately and their accuracy is noted. Along with this, Deep learning technique such as Convolution Neural Network is also used to classify the data and its accuracy is compared with that of traditional machine learning techniques. We are also exploring hierarchical classifiers for classification of classes and subclasses. The system classifies the data faster and with better accuracy than if done manually. The results are discussed in the results and evaluation section.",
        "link": "https://dl.acm.org/doi/10.1145/3209280.3209526"
    },
    {
        "title": "Identification of Overpricing in the Purchase of Medication by the Federal Government of Brazil, Using Text Mining and Clustering Based on Ontology",
        "authors": [
            "Marco Aurelio O. S. Correa",
            "Adriano Galindo Leal"
        ],
        "date": "August 2018",
        "source": "ICCBDC '18: Proceedings of the 2018 2nd International Conference on Cloud and Big Data Computing",
        "abstract": "Increasing the transparency level in his actions and spending is one of the primary duties of the Brazilian Federal Government. The creation of laws that oblige full disclose of all its expenditures through transparency portals enables citizens to supervise all government entities. However, only the dissemination of these data, without a definite standard or the availability of data analysis tools, does not guarantee that the citizen is empowered to play his role. Therefore hence, the objective of this work is to identify overprice in the acquisition of products purchased by the federal government of Brazil using the unstructured data available on the Transparency Portal. The last two-years' worth of purchasing data, available in the Transparency Portal, were extracted, processed and stored. Due to his diverse nature and high volume of data, this study focused only on medicines purchased by the Ministry of Health. Ontology-based text mining and clustering techniques were applied for automatic identification and classification of products. The processing of this information was done through text mining and clustering, based on the ontology registered in another database of the Brazilian government. Because of this work, a consolidated price base per medication was created to allow the identification of distortions in prices practised, facilitating the identification of cases that merit further investigation to unravel fraud to the treasury.",
        "link": "https://dl.acm.org/doi/10.1145/3264560.3264569"
    },
    {
        "title": "Computational Estimation by Scientific Data Mining with Classical Methods to Automate Learning Strategies of Scientists",
        "authors": [
            "Aparna S. Varde"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Experimental results are often plotted as 2-dimensional graphical plots (aka graphs) in scientific domains depicting dependent versus independent variables to aid visual analysis of processes. Repeatedly performing laboratory experiments consumes significant time and resources, motivating the need for computational estimation. The goals are to estimate the graph obtained in an experiment given its input conditions, and to estimate the conditions that would lead to a desired graph. Existing estimation approaches often do not meet accuracy and efficiency needs of targeted applications. We develop a computational estimation approach called AutoDomainMine that integrates clustering and classification over complex scientific data in a framework so as to automate classical learning methods of scientists. Knowledge discovered thereby from a database of existing experiments serves as the basis for estimation. Challenges include preserving domain semantics in clustering, finding matching strategies in classification, striking a good balance between elaboration and conciseness while displaying estimation results based on needs of targeted users, and deriving objective measures to capture subjective user interests. These and other challenges are addressed in this work. The AutoDomainMine approach is used to build a computational estimation system, rigorously evaluated with real data in Materials Science. Our evaluation confirms that AutoDomainMine provides desired accuracy and efficiency in computational estimation. It is extendable to other science and engineering domains as proved by adaptation of its sub-processes within fields such as Bioinformatics and Nanotechnology.",
        "link": "https://dl.acm.org/doi/10.1145/3502736"
    },
    {
        "title": "Where is the road for issue reports classification based on text mining?",
        "authors": [
            "Qiang Fan",
            "Yue Yu",
            "Gang Yin",
            "Tao Wang",
            "Huaimin Wang"
        ],
        "date": "November 2017",
        "source": "ESEM '17: Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Currently, open source projects receive various kinds of issues daily, because of the extreme openness of Issue Tracking System (ITS) in GitHub. ITS is a labor-intensive and time-consuming task of issue categorization for project managers. However, a contributor is only required a short textual abstract to report an issue in GitHub. Thus, most traditional classification approaches based on detailed and structured data (e.g., priority, severity, software version and so on) are difficult to adopt. In this paper, issue classification approaches on a large-scale dataset, including 80 popular projects and over 252,000 issue reports collected from GitHub, were investigated. First, four traditional text-based classification methods and their performances were discussed. Semantic perplexity (i.e., an issues description confuses bug-related sentences with nonbug-related sentences) is a crucial factor that affects the classification performances based on quantitative and qualitative study. Finally, A two-stage classifier framework based on the novel metrics of semantic perplexity of issue reports was designed. Results show that our two-stage classification can significantly improve issue classification performances.",
        "link": "https://dl.acm.org/doi/10.1109/ESEM.2017.19"
    },
    {
        "title": "Feature-based Facebook reviews process model for e-management using data mining",
        "authors": [
            "Anish Kumar Varudharajulu",
            "Yongsheng Ma"
        ],
        "date": "January 2019",
        "source": "IC4E '19: Proceedings of the 10th International Conference on E-Education, E-Business, E-Management and E-Learning",
        "abstract": "The data generated from online communication acts as potential gold mines for discovering knowledge for researchers. A large amount of data is also generated in the form of web documents, emails, blogs, and feedback, etc. Text analytics is being significantly employed to mine important information. Opinion mining is the process of extracting human thoughts and perceptions from unstructured texts. The showstopper for designing an opinion mining system for analyzing reviews arise from the fact that customer reviews are often noisy. These reviews are informally written. In addition, they are subjected to spelling mistakes, grammatical errors, improper punctuation and irrational capitalization. This paper focuses on analyzing the different classification and clustering algorithms aimed at extracting and consolidating opinions of customers from social media sites like Facebook, Twitter and through surveys, at multiple levels of granularity to monitor and measure customer satisfaction. Ours is an automated approach, in which the system aids in the process of knowledge assimilation for knowledge-based building and also performs the analytics. Domain experts ratify the knowledge base and also provide training datasets for the system to intuitively gather more instances for ratification. The system identifies opinion expressions as phrases containing opinion words, opinionated features and also opinion modifiers. These expressions are categorized as positive, negative or neutral. Opinion expressions are identified and categorized using localized linguistic techniques. Opinions can be congregated at any desired level of specificity i.e. feature level or product level, user level or service level, etc. We have developed a system based on this approach, which provides the user with a platform to analyze opinion expressions crawled from a set of pre-defined datasets.",
        "link": "https://dl.acm.org/doi/10.1145/3306500.3306514"
    },
    {
        "title": "Discriminative Topic Mining via Category-Name Guided Text Embedding",
        "authors": [
            "Yu Meng",
            "Jiaxin Huang",
            "Guangyuan Wang",
            "Zihan Wang",
            "Chao Zhang",
            "Yu Zhang",
            "Jiawei Han"
        ],
        "date": "April 2020",
        "source": "WWW '20: Proceedings of The Web Conference 2020",
        "abstract": "Mining a set of meaningful and distinctive topics automatically from massive text corpora has broad applications. Existing topic models, however, typically work in a purely unsupervised way, which often generate topics that do not fit users’ particular needs and yield suboptimal performance on downstream tasks. We propose a new task, discriminative topic mining, which leverages a set of user-provided category names to mine discriminative topics from text corpora. This new task not only helps a user understand clearly and distinctively the topics he/she is most interested in, but also benefits directly keyword-driven classification tasks. We develop CatE, a novel category-name guided text embedding method for discriminative topic mining, which effectively leverages minimal user guidance to learn a discriminative embedding space and discover category representative terms in an iterative manner. We conduct a comprehensive set of experiments to show that CatE mines high-quality set of topics guided by category names only, and benefits a variety of downstream applications including weakly-supervised classification and lexical entailment direction identification.",
        "link": "https://dl.acm.org/doi/10.1145/3366423.3380278"
    },
    {
        "title": "Comparative Study of Heart Disease Diagnosis Using Top Ten Data Mining Classification Algorithms",
        "authors": [
            "I. Ketut Agung Enriko"
        ],
        "date": "June 2019",
        "source": "ICFET '19: Proceedings of the 5th International Conference on Frontiers of Educational Technologies",
        "abstract": "Data mining has been used for many purposes, especially for prediction system. In healthcare, data mining algorithms often used in disease diagnosis. Meanwhile, heart disease is known as a primary cause of death over the years. Many studies have been performed in heart disease diagnosis using data mining methods. There are some popular data mining algorithms that can be used in heart disease diagnosis, for example, k-Nearest Neighbor, CART, and AdaBoost. The algorithms are used to analyze a sample of cardiovascular patients data and predict the heart disease type that they suffer. Some parameters are taken from the patient, including EKG morphology, blood pressure, and information about the existence of chest pain, shortness of breath, palpitation, and cold sweat. In this study, medical records data are collected from Harapan Kita Hospital and utilized as a dataset sample in this research. Top ten data mining classification algorithms are used in diagnosing heart disease from Harapan Kita Hospital data and examining their performance by checking the accuracy and speed.",
        "link": "https://dl.acm.org/doi/10.1145/3338188.3338220"
    },
    {
        "title": "Text Mining Approach for Identifying Research Trends",
        "authors": [
            "Snezhana Sulova"
        ],
        "date": "June 2021",
        "source": "CompSysTech '21: Proceedings of the 22nd International Conference on Computer Systems and Technologies",
        "abstract": "With the increase of unstructured data, the issues connected with automatic text processing, the categorization of documents and the discovery of topics have become objects of growing interest. In order to improve the process of grouping and processing research publications, we would like to propose a method based upon natural language processing. It is based on text mining technologies which aim to identify key tendencies in documents. It processes the content of publications by clustering and identifies the topics of each identified group. This analysis helps by identifying key tendencies as well as discovering emerging new areas of research. Publications from the research literature database, Scopus, were used to test the approach. The topic of the publications is “the application of digital technologies in the logistics business”. The experiments were completed using the RapidMiner Studio software.",
        "link": "https://dl.acm.org/doi/10.1145/3472410.3472433"
    },
    {
        "title": "Assessment of Vulnerability Severity using Text Mining",
        "authors": [
            "Georgios Spanos",
            "Lefteris Angelis",
            "Dimitrios Toloudis"
        ],
        "date": "September 2017",
        "source": "PCI '17: Proceedings of the 21st Pan-Hellenic Conference on Informatics",
        "abstract": "Software1 vulnerabilities are closely associated with information systems security, a major and critical field in today's technology. Vulnerabilities constitute a constant and increasing threat for various aspects of everyday life, especially for safety and economy, since the social impact from the problems that they cause is complicated and often unpredictable. Although there is an entire research branch in software engineering that deals with the identification and elimination of vulnerabilities, the growing complexity of software products and the variability of software production procedures are factors contributing to the ongoing occurrence of vulnerabilities, Hence, another area that is being developed in parallel focuses on the study and management of the vulnerabilities that have already been reported and registered in databases. The information contained in such databases includes, a textual description and a number of metrics related to vulnerabilities. The purpose of this paper is to investigate to what extend the assessment of the vulnerability severity can be inferred directly from the corresponding textual description, or in other words, to examine the informative power of the description with respect to the vulnerability severity. For this purpose, text mining techniques, i.e. text analysis and three different classification methods (decision trees, neural networks and support vector machines) were employed. The application of text mining to a sample of 70,678 vulnerabilities from a public data source shows that the description itself is a reliable and highly accurate source of information for vulnerability prioritization.",
        "link": "https://dl.acm.org/doi/10.1145/3139367.3139390"
    },
    {
        "title": "Clustering Algorithms for Spatial Data Mining",
        "authors": [
            "Chetashri Bhadane",
            "Ketan Shah"
        ],
        "date": "April 2020",
        "source": "ICGDA '20: Proceedings of the 2020 3rd International Conference on Geoinformatics and Data Analysis",
        "abstract": "With the advances in mobile and wireless technologies, there has been a rise in applications that track and share the users' geospatial data. People use several social networking sites such as Twitter, Facebook and Flickr, where they share their status updates. With the integration of Global Positioning System (GPS) with mobile phones, it is now possible to share one's locations on these social networks. GPS allows us to record and track a person's movement along with the timestamp. The data set obtained from these GPS logs is vast and is widely used to analyze the users' movement patterns. Specifically, we can find out significant locations based on the number of users present at that location and the time spent by them at such places. Once significant places have been identified, it is also possible to identify the semantic importance of these locations. This paper presents an overview of the clustering techniques used to find important places of interest using large GPS based mobility datasets. Four clustering algorithms, K-Means, DBSCAN, OPTICS and Hierarchical, are implemented, and performance is tested using real-time data of 50 users collected over 2--5 years. Performance summary depicts that K-Means and DBSCAN perform well for spatial data.",
        "link": "https://dl.acm.org/doi/10.1145/3397056.3397068"
    },
    {
        "title": "Feature Selection based Arabic Text Classification using Different Machine Learning Algorithms: Comparative Study",
        "authors": [
            "Sakina Rim Bennabi",
            "Zakaria Elberrichi"
        ],
        "date": "June 2020",
        "source": "ICIST '20: Proceedings of the 10th International Conference on Information Systems and Technologies",
        "abstract": "Feature selection is a method of data pre-processing widely used when mining large data, such as textual classification. Several studies have been conducted to compare the different methods of feature selection applied to corpora in English. Unfortunately, a small number of works concern the Arabic language. This article aims to present a comparative study of different feature selection techniques including: Chi2, the ANOVA method and mutual information, applied on a corpus in Arabic language, while also diversifying the machine learning algorithms (Naive Bayes, SVM and KNN). This experimental study has shown in general that reducing dimensionality with feature selection techniques has slightly affected the performance of textual classification, reducing the size of the corpus by up to 1%.",
        "link": "https://dl.acm.org/doi/10.1145/3447568.3448531"
    },
    {
        "title": "Intelligent Data Analysis using Optimized Support Vector Machine Based Data Mining Approach for Tourism Industry",
        "authors": [
            "Ms Promila Sharma",
            "Uma Meena",
            "Girish Kumar Sharma"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Data analysis involves the deployment of sophisticated approaches from data mining methods, information theory, and artificial intelligence in various fields like tourism, hospitality, and so on for the extraction of knowledge from the gathered and preprocessed data. In tourism, pattern analysis or data analysis using classification is significant for finding the patterns that represent new and potentially useful information or knowledge about the destination and other data. Several data mining techniques are introduced for the classification of data or patterns. However, overfitting, less accuracy, local minima, sensitive to noise are the drawbacks in some existing data mining classification methods. To overcome these challenges, Support vector machine with Red deer optimization (SVM-RDO) based data mining strategy is proposed in this article. Extended Kalman filter (EKF) is utilized in the first phase, i.e., data cleaning to remove the noise and missing values from the input data. Mantaray foraging algorithm (MaFA) is used in the data selection phase, in which the significant data are selected for the further process to reduce the computational complexity. The final phase is the classification, in which SVM-RDO is proposed to access the useful pattern from the selected data. PYTHON is the implementation tool used for the experiment of the proposed model. The experimental analysis is done to show the efficacy of the proposed work. From the experimental results, the proposed SVM-RDO achieved better accuracy, precision, recall, and F1 score than the existing methods for the tourism dataset. Thus, it is showed the effectiveness of the proposed SVM-RDO for pattern analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3494566"
    },
    {
        "title": "Applying text mining to predict learners' cognitive engagement",
        "authors": [
            "Hayati Hind",
            "Mohammed Khalidi Idrissi",
            "Samir Bennani"
        ],
        "date": "October 2017",
        "source": "SCAMS '17: Proceedings of the Mediterranean Symposium on Smart City Application",
        "abstract": "In academic process, engagement represent one of the important success' factor. This paper proposes a predictive system for learners' cognitive engagement based on their online discussion forums participation. In the first level the ontology OWL and the LSA method are used to perform a semantic classification system of the threads according to a specific context chosen by the tutor. Then the Text mining, as a predictive method, is applied to the classified threads and learners' participation in the forums.",
        "link": "https://dl.acm.org/doi/10.1145/3175628.3175655"
    },
    {
        "title": "Investigate Transitions into Drug Addiction through Text Mining of Reddit Data",
        "authors": [
            "John Lu",
            "Sumati Sridhar",
            "Ritika Pandey",
            "Mohammad Al Hasan",
            "Georege Mohler"
        ],
        "date": "July 2019",
        "source": "KDD '19: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Increasing rates of opioid drug abuse and heightened prevalence of online support communities underscore the necessity of employing data mining techniques to better understand drug addiction using these rapidly developing online resources. In this work, we obtained data from Reddit, an online collection of forums, to gather insight into drug use/misuse using text snippets from users narratives. Specifically, using users' posts, we trained a binary classifier which predicts a user's transitions from casual drug discussion forums to drug recovery forums. We also proposed a Cox regression model that outputs likelihoods of such transitions. In doing so, we found that utterances of select drugs and certain linguistic features contained in one's posts can help predict these transitions. Using unfiltered drug-related posts, our research delineates drugs that are associated with higher rates of transitions from recreational drug discussion to support/recovery discussion, offers insight into modern drug culture, and provides tools with potential applications in combating the opioid crisis.",
        "link": "https://dl.acm.org/doi/10.1145/3292500.3330737"
    },
    {
        "title": "Analysing digital banking reviews using text mining",
        "authors": [
            "Li Chen Cheng",
            "Legaspi Rhea Sharmayne"
        ],
        "date": "December 2020",
        "source": "ASONAM '20: Proceedings of the 12th IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Digital banks are new entrants in the banking industry in the Philippines as they only started late 2018. Since then, a handful of players have and are still emerging. With more and more people becoming technologically savvy, it is very critical for financial institutions to develop a digital banking application that will stand out from the competition. This paper aims to use text mining methods to analyse digital banking application reviews. This study will perform topic modelling using LDA to explore customer concerns and will mine association rules between the digital banking features with the review score. The results will reveal which areas the digital banking application can further optimize for customer satisfaction and retention.",
        "link": "https://dl.acm.org/doi/10.1109/ASONAM49781.2020.9381429"
    },
    {
        "title": "Mining Moocs Videos Metadata Using Classification Techniques",
        "authors": [
            "El Harrak Othman",
            "Ghadi Abderrahim",
            "El Bouhdidi Jaber"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "In few years, the internet has become the first source of information for most people, Today MOOCs makes it possible for everyone to access to the education over the world, it's represents an emerging methodology of online teaching and an important development in open education. Due to the rapid development of Moocs and the rapid growth of digital data and video database over the Internet, it is becoming very difficult for learners to browsing and choosing the best training for them. The scientific community has increased the amount of research into new technologies, with a view to improve the digital video utilization: its archiving, indexing, accessibility, acquisition, store and even its process and usability. All these parts of the video utilization entail the necessity of the extraction of all important information of a video, especially in cases of lack of metadata information. Web video mining is retrieving the content using data mining techniques from World Wide Web. There are two approaches for web video mining using traditional image processing (signal processing) and metadata based approach. In this paper, we present the various research makes in this subject and we propose an effective methodology to extract the metadata from Moocs videos and classify them based on the extracted metadata by applying data mining techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090450"
    },
    {
        "title": "Text mining with HathiTrust",
        "authors": [
            "Eleanor Dickson Koehl",
            "Ryan Dubnicek"
        ],
        "date": "June 2019",
        "source": "JCDL '19: Proceedings of the 18th Joint Conference on Digital Libraries",
        "abstract": "This tutorial will introduce attendees to the HathiTrust Research Center and its tools and services for computational text analysis research. HTRC leverages the scope and scale of the HathiTrust Digital Library collection to create opportunities for researchers to perform text data mining on subsets of the corpus. Attendees will develop skills that will allow them to conduct basic text analysis research using HathiTrust data, learn techniques for engaging with scholars on their text data mining research, and explore strategies for computational access to large-scale digital collections.",
        "link": "https://dl.acm.org/doi/10.1109/JCDL.2019.00115"
    },
    {
        "title": "Text mining for plagiarism detection: multivariate pattern detection for recognition of text similarities",
        "authors": [
            "Konstantinos Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2018",
        "source": "ASONAM '18: Proceedings of the 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "The problem of plagiarism the recent years has been intensified by the availability of information in digital form and the accessibility of the electronic libraries through the Internet. As a result, plagiarism detection has been transformed into a big data analytics problem since the number of digital sources is extravagant and a new document needs to be compared with millions of other existing documents. In this paper, a text mining methodology is proposed that can detect all common patterns between a document and the documents in a reference database. The technique is based on a pattern detection algorithm and the corresponding data structure that enables the algorithm to detect all common patterns. The methodology has been applied in a well-defined dataset providing very promising results identifying difficult cases of plagiarism such as technical disguise.",
        "link": "https://dl.acm.org/doi/10.5555/3382225.3382424"
    },
    {
        "title": "Petroleum Engineering Data Text Classification Using Convolutional Neural Network Based Classifier",
        "authors": [
            "Tzuhan Hsu",
            "Yaoqin Zhang"
        ],
        "date": "May 2018",
        "source": "ICMLT '18: Proceedings of the 2018 International Conference on Machine Learning Technologies",
        "abstract": "In this paper, authors introduce a convolutional neural network (CNN) based classification approach for petroleum engineering event. Recently, text classification problem has raised substantial attention in the field of machine learning. The aim of text classification process is to assign correct label to a text on the basis of its content. However, due to the imprecise of feature representation, most of former text classification methods cannot export satisfying result. To solve this problem, authors propose a model based on convolutional neural network. Authors conduct experiments with real world petroleum engineering data text, artificially labelled into 6 categories. Beside purposed CNN-based model, the authors also implement other baseline machine learning approaches: k nearest neighbor, random forest, support vector machines, recurrent neural network, gated recurrent units, long short-term memory. Among all these models, our CNN-based model reach best performance in text classification task. In the last part of this paper, authors compare and analysis their purpose and baseline model.",
        "link": "https://dl.acm.org/doi/10.1145/3231884.3231898"
    },
    {
        "title": "Classifying Indonesian Online Articles as Advertisement Placement Base Using Text Mining",
        "authors": [
            "Nadhira Tasya",
            "Arian Dhini"
        ],
        "date": "July 2017",
        "source": "ICBIM 2017: Proceedings of the International Conference on Business and Information Management",
        "abstract": "Rapid development in technological aspect resulting in growing level of human needs for the latest news, so that emerged a new trend of publishing and accessing news through online media or usually called online journalism. In addition, the number of people who sell and purchase through online sites also continues to increase and this opportunity is utilized by the company and the advertiser by implementing targeted web advertising. However, the high number of articles that have been published and accessed leads to great opportunities for errors in determining where to place the ads. Therefore, it needs a system that can categorize articles accessed by users as the basis of advertisement placement by the company and this classification system can be done by applying the method of Data Mining and Text Mining. This research uses document data in the form of article content that will be categorized into twenty categories of class of advertisement by using Text Mining technique with Support Vector Machine algorithm. The results of this study may be used by companies or advertisers as a basis for placement of ads on selected online media sites.",
        "link": "https://dl.acm.org/doi/10.1145/3134271.3134288"
    },
    {
        "title": "Mining Free-Text Medical Notes for Suicide Risk Assessment",
        "authors": [
            "Marios Adamou",
            "Grigoris Antoniou",
            "Elissavet Greasidou",
            "Vincenzo Lagani",
            "Paulos Charonyktakis",
            "Ioannis Tsamardinos"
        ],
        "date": "July 2018",
        "source": "SETN '18: Proceedings of the 10th Hellenic Conference on Artificial Intelligence",
        "abstract": "Suicide has been considered as an important public health issue for a very long time, and is one of the main causes of death worldwide. Despite suicide prevention strategies being applied, the rate of suicide has not changed substantially over the past decades. Advances in machine learning make it possible to attempt to predict suicide based on the analysis of relevant data to inform clinical practice. This paper reports on findings from the analysis of data of patients who died by suicide in the period 2013-2016 and made use of both structured data and free-text medical notes. We focus on examining various text-mining approaches to support risk assessment. The results show that using advance machine learning and text-mining techniques, it is possible to predict within a specified period which people are most at risk of taking their own life at the time of referral to a mental health service.",
        "link": "https://dl.acm.org/doi/10.1145/3200947.3201020"
    },
    {
        "title": "Integration of data mining techniques in e-learning systems: Clustering Profil of Lerners and Recommender Course System",
        "authors": [
            "A. El Moustamid",
            "E. En-Naimi",
            "J. El Bouhdidi"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "Data mining aims at the extraction of knowledge from large quantities of data by automatic or semi-automatic methods. In the field of education and training e-learning, Data Mining techniques as clustering (The aim of this technique is to identify groups that are similar in some aspect) [21], prediction (is used for developing model where combination of data viewed from different aspects can effect single viewed data. Here, is important to watch how predictors variables are effecting predicted variable)[21], classification are applied in e-learning to improve the level of learners by providing learning according to the learners' requirement) are used to analyze learners profile, to predict and improve student performance. The aim of our project is to develop a system capable of analyzing learners profile and indexing web videos in order to offer learners a rich database with courses that correspond to their levels.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090453"
    },
    {
        "title": "Multi-level mining and visualization of scientific text collections: Exploring a bi-lingual scientific repository",
        "authors": [
            "Pablo Accuosto",
            "Francesco Ronzano",
            "Daniel Ferrés",
            "Horacio Saggion"
        ],
        "date": "December 2017",
        "source": "WOSP 2017: Proceedings of the 6th International Workshop on Mining Scientific Publications",
        "abstract": "We present a system to mine and visualize collections of scientific documents by semantically browsing information extracted from single publications or aggregated throughout corpora of articles. The text mining tool performs deep analysis of document collections allowing the extraction and interpretation of research paper's contents. In addition to the extraction and enrichment of documents with metadata (titles, authors, affiliations, etc), the deep analysis performed comprises semantic interpretation, rhetorical analysis of sentences, triple-based information extraction, and text summarization. The visualization components allow geographical-based exploration of collections, topic-evolution interpretation, and collaborative network analysis among others. The paper presents a case study of a bi-lingual collection in the field of Natural Language Processing (NLP).",
        "link": "https://dl.acm.org/doi/10.1145/3127526.3127529"
    },
    {
        "title": "Hierarchical Topic Mining via Joint Spherical Tree and Text Embedding",
        "authors": [
            "Yu Meng",
            "Yunyi Zhang",
            "Jiaxin Huang",
            "Yu Zhang",
            "Chao Zhang",
            "Jiawei Han"
        ],
        "date": "August 2020",
        "source": "KDD '20: Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Mining a set of meaningful topics organized into a hierarchy is intuitively appealing since topic correlations are ubiquitous in massive text corpora. To account for potential hierarchical topic structures, hierarchical topic models generalize flat topic models by incorporating latent topic hierarchies into their generative modeling process. However, due to their purely unsupervised nature, the learned topic hierarchy often deviates from users' particular needs or interests. To guide the hierarchical topic discovery process with minimal user supervision, we propose a new task, Hierarchical Topic Mining, which takes a category tree described by category names only, and aims to mine a set of representative terms for each category from a text corpus to help a user comprehend his/her interested topics. We develop a novel joint tree and text embedding method along with a principled optimization procedure that allows simultaneous modeling of the category tree structure and the corpus generative process in the spherical space for effective category-representative term discovery. Our comprehensive experiments show that our model, named JoSH, mines a high-quality set of hierarchical topics with high efficiency and benefits weakly-supervised hierarchical text classification tasks.",
        "link": "https://dl.acm.org/doi/10.1145/3394486.3403242"
    },
    {
        "title": "Unsupervised behavioural mining and clustering for malware family identification",
        "authors": [
            "Khanh Huu The Dam",
            "Thomas Given-Wilson",
            "Axel Legay"
        ],
        "date": "March 2021",
        "source": "SAC '21: Proceedings of the 36th Annual ACM Symposium on Applied Computing",
        "abstract": "More accurate and advanced detection and classification of malware requires exploiting program behaviour and not merely syntactic or static features. One approach is to use system call dependency graphs (SCDGs) that represent the program behaviour by interactions with the system, and the relations between these interactions. These SCDGs have been used with supervised learning techniques to very accurately detect and classify malware. This works considers the unsupervised learning challenge of mining for common clusters of behaviour without a priori knowledge. This allows for clustering of similar programs by behaviour, that can then be used for either classification or further analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3412841.3441919"
    },
    {
        "title": "Text Mining for Evaluating Authors' Birth and Death Years",
        "authors": [
            "Dror Moghaz",
            "Yaakov Hacohen-Kerner",
            "Dov Gabbay"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "This article presents a unique method in text and data mining for finding the era, i.e., mining temporal data, in which an anonymous author was living. Finding this era can assist in the examination of a fake document or extracting the time period in which a writer lived. The study and the experiments concern Hebrew, and in some parts, Aramaic and Yiddish rabbinic texts. The rabbinic texts are undated and contain no bibliographic sections, posing an interesting challenge. This work proposes algorithms using key phrases and key words that allow the temporal organization of citations together with linguistic patterns. Based on these key phrases, key words, and the references, we established several types of “Iron-clad,” Heuristic and Greedy rules for estimating the years of birth and death of a writer in an interesting classification task. Experiments were conducted on corpora, including documents authored by 12, 24, and 36 rabbinic writers and demonstrated promising results.",
        "link": "https://dl.acm.org/doi/10.1145/3281631"
    },
    {
        "title": "Transfer Learning to Timed Text Based Video Classification Using CNN",
        "authors": [
            "Zenun Kastrati",
            "Ali Shariq Imran",
            "Arianit Kurti"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "Open educational video resources are gaining popularity with a growing number of massive open online courses (MOOCs). This has created a niche for content providers to adopt effective solutions in automatically organizing and structuring of educational resources for maximum visibility. Recent advances in deep learning techniques are proving useful in managing and classifying resources into appropriate categories. This paper proposes one such convolutional neural network (CNN) model for classifying video lectures in a MOOC setting using a transfer learning approach. The model uses a time-aligned text transcripts corresponding to video lectures from six broader subject categories. Video lectures and their corresponding transcript dataset is gathered from the Coursera MOOC platform. Two different CNN models are proposed: i) CNN based classification using embeddings learned from our MOOC dataset, ii) CNN based classification using transfer learning. Word embeddings generated from two well known state-of-the-art pre-trained models Word2Vec and GloVe, are used in the transfer learning approach for the second case. The proposed CNN models are evaluated using precision, recall, and F1 score and the obtained performance is compared with both conventional and deep learning classifiers. The proposed CNN models have an F1 score improvement of 10-22 percentage points over DNN and conventional classifiers",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326483"
    },
    {
        "title": "Evolutionary role mining in complex networks by ensemble clustering",
        "authors": [
            "Sarvenaz Choobdar",
            "Pedro Ribeiro",
            "Fernando Silva"
        ],
        "date": "April 2017",
        "source": "SAC '17: Proceedings of the Symposium on Applied Computing",
        "abstract": "The structural patterns in the neighborhood of nodes assign unique roles to the nodes. Mining the set of existing roles in a network provides a descriptive profile of the network and draws its general picture. This paper proposes a new method to determine structural roles in a dynamic network based on the current position of nodes and their historic behavior. We develop a temporal ensemble clustering technique to dynamically find groups of nodes, holding similar tempo-structural roles. We compare two weighting functions, based on age and distribution of data, to incorporate temporal behavior of nodes in the role discovery. To evaluate the performance of the proposed method, we assess the results from two points of view: 1) goodness of fit to current structure of the network; 2) consistency with historic data. We conduct the evaluation using different ensemble clustering techniques. The results on real world networks demonstrate that our method can detect tempo-structural roles that simultaneously depict the topology of a network and reflect its dynamics with high accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3019612.3019815"
    },
    {
        "title": "Analyzing used-car web listings via text mining",
        "authors": [
            "Ayhan Demiriz",
            "Fatma Cantaş"
        ],
        "date": "October 2017",
        "source": "IML '17: Proceedings of the 1st International Conference on Internet of Things and Machine Learning",
        "abstract": "Used car trade is one of the major components of the world economies. It is not uncommon to sell a car by placing an internet advertisement irrespective of the geography in these days. A typical content of an advertisement is usually composed of two parts namely the structured and the free text data. The structured data may include some information about the asking price, make, model, year, mileage of the car and the contact info. In most cases, seller may give important clues about the car's current conditions in the free text data where the title (head) of the advertisement can be included as free text too. This paper reports preliminary results from a text mining study conducted on 75K used car internet listings collected from two major car listing web sites in Turkey. As expected, the words and the phrases related to the description of the car are observed to be frequent. The leading concepts in the free text are found to be regarding how to describe the current condition of a car, for example \"no crash history\".",
        "link": "https://dl.acm.org/doi/10.1145/3109761.3109782"
    },
    {
        "title": "Text classification of Diseases Treated by Traditional Chinese Medicine Prescription based on machine learning",
        "authors": [
            "Hanqing Zhao",
            "Bo Han",
            "Chen Li"
        ],
        "date": "September 2020",
        "source": "ISAIMS '20: Proceedings of the 1st International Symposium on Artificial Intelligence in Medical Sciences",
        "abstract": "OBJECTIVE: To explore the application of machine learning in th e identification of diseases treated by traditional Chinese medicine prescriptions. METHODS: Based on the composition of the text document of Chinese medicine prescriptions, the prescriptions were divided into cough, headache and diarrhea. THUTCT were introduced to establish and train two machine learning text classification models, LibSVM and LibLinear, and the prescriptions to be deter mined were put into the model for classification and prediction. R ESULTS: The Precision rate of LibSVM model and Liblinear mod el were 0.7283 and 0.6690. Seven prescriptions were classified an d predicted, and the results were in line with expectations. CONCLUSION: THUCTC has good universality for the content of TCM prescriptions, high classification accuracy and fast testing speed, which is suitable for the text classification and discrimination research of TCM prescriptions for diseases.",
        "link": "https://dl.acm.org/doi/10.1145/3429889.3429895"
    },
    {
        "title": "Evaluation of retweet clustering method classification method using retweets on Twitter without text data",
        "authors": [
            "K. Uchida",
            "F. Toriumi",
            "T. Sakaki"
        ],
        "date": "August 2017",
        "source": "WI '17: Proceedings of the International Conference on Web Intelligence",
        "abstract": "Burst phenomena, which frequently occur on social media, are caused by such social events as flaming on the internet, elections, and natural disasters. To understand people's thoughts and feelings, we must classify their opinions from burst phenomena. Therefore, classification methods that categorize tweets are critical. However, since most classification methods focus on text mining, they cannot group tweets by topics because each tweet has poor linguistic similarities. We used a non-text-based classification method proposed by Baba et al. that groups tweets by topics, even if they have poor linguistic similarities, and verified its validity by comparing it with a text-based classification method in two different evaluations: qualitative and quantitative. In the qualitative evaluation part, we did a questionnaire survey and validated the suitability of the topic clusters created using both the non-and text-based methods. Since evaluating the similarity of every pair of tweets in each topic is difficult, we evaluated the similarity between sampled pairs in the survey and acquired more appropriate topic clustering results using the non-text-based method than the text-based method. In the quantitative evaluation part, we focused on the robustness of each method against data reduction. Many approaches analyze social media data, especially because collecting data from social media is comparatively easy. However, since collecting the whole data of burst phenomena is very costly due to the vast amounts of available social media data, robustness against data reduction is an important index to evaluate classification methods. With the non-text-based method, over 55% of the pairs of tweets in the same cluster were also included in the same cluster even when the data were reduced to 10% in all three of our example cases. In this paper, as a source we focus on Twitter, one of the most popular microblogging services. Using clustering to conduct detailed case analyses, we scrutinized three burst cases that include natural disasters and flaming on the internet and found that a non-text-based method more effectively classified tweets in burst phenomena than a text-based method.",
        "link": "https://dl.acm.org/doi/10.1145/3106426.3106451"
    },
    {
        "title": "Using Spark for Text Mining on Large Scale Liver Cancer Literature",
        "authors": [
            "Ming-Yen Lin",
            "Yu-Ju Lin",
            "Sue-Chen Hsueh"
        ],
        "date": "January 2021",
        "source": "BDET 2021: 2021 the 3rd International Conference on Big Data Engineering and Technology (BDET)",
        "abstract": "Cancer is one of the main causes of death. The number of known cancer to-date is more than one hundred. Liver cancer, ranked after trachea and lung cancer, has long been high in the cancer leading cause in Taiwan and even ranked as second in 2016. The number of scientific articles related to cancer proliferates every year. The number reaches as high as 20 million in PubMed so that discovering useful information from the massive collection is very difficult. In addition, using a single machine to sift through these articles is very time-consuming. Therefore, we present a big data analytic framework using the distributed Apache Spark platform for text mining in PubMed literature. We establish a prediction model for liver cancer articles so that researchers may effectively validate whether an article is related to liver cancer or not. Classification models in Spark MLlib including Linear Support Vector Machines (SVM), Logistic Regression, are used in our experiments. Relevancy to liver cancer is further confirmed by using MeSH (Medical Subject Headings) terms. Logistic regression is about 3 times faster than SVMs and the accuracy of both methods is close to 95% in the experiments using hold-out validation. When max_features is 500 and min_df ≤ 0.1 (or min_df = 1), the accuracy may reach 96%. In the experiments with K-fold cross-validation, the accuracy of SVMs methods is 96%. The experimental results show that that our prediction model may effectively classify liver cancer articles.",
        "link": "https://dl.acm.org/doi/10.1145/3474944.3474958"
    },
    {
        "title": "Automatic topic classification of test cases using text mining at an Android smartphone vendor",
        "authors": [
            "Junji Shimagaki",
            "Yasutaka Kamei",
            "Naoyasu Ubayashi",
            "Abram Hindle"
        ],
        "date": "October 2018",
        "source": "ESEM '18: Proceedings of the 12th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Background: An Android smartphone is an ecosystem of applications, drivers, operating system components, and assets. The volume of the software is large and the number of test cases needed to cover the functionality of an Android system is substantial. Enormous effort has been already taken to properly quantify \"what features and apps were tested and verified?\". This insight is provided by dashboards that summarize test coverage and results per feature. One method to achieve this is to manually tag or label test cases with the topic or function they cover, much like function points. At the studied Android smartphone vendor, tests are labelled with manually defined tags, so-called \"feature labels (FLs)\", and the FLs serve to categorize 100s to 1000s test cases into 10 to 50 groups. Aim: Unfortunately for developers, manual assignment of FLs to 1000s of test cases is a time consuming task, leading to inaccurately labeled test cases, which will render the dashboard useless. We created an automated system that suggests tags/labels to the developers for their test cases rather than manual labeling. Method: We use machine learning models to predict and label the functionality tested by 10,000 test cases developed at the company. Results: Through the quantitative experiments, our models achieved acceptable F-1 performance of 0.3 to 0.88. Also through the qualitative studies with expert teams, we showed that the hierarchy and path of tests was a good predictor of a feature's label. Conclusions: We find that this method can reduce tedious manual effort that software developers spent classifying test cases, while providing more accurate classification results.",
        "link": "https://dl.acm.org/doi/10.1145/3239235.3268927"
    },
    {
        "title": "To apply Data Mining for Classification of Crowd sourced Software Requirements",
        "authors": [
            "Soonh Taj",
            "Qasim Arain",
            "Imran Memon",
            "Asma Zubedi"
        ],
        "date": "April 2019",
        "source": "ICSIE '19: Proceedings of the 8th International Conference on Software and Information Engineering",
        "abstract": "Now a day's main focus of developers is to build quality software that works according to customer needs and for this reason it is necessary to gather right requirements as requirement elicitation is the critical step that impacts on the success of software project as misinterpreted requirements leads to the failure of software project. By keeping this in mind a research is carried out on improving requirements elicitation process and automating the process of classifying requirements. In this research, a model is proposed which will help in this scenario for requirements elicitation and requirement classification. This paper presents a model in which crowd sourcing approach is used so that customers, end users, stakeholders, developers and software engineers can make active participation for requirement elicitation process and requirements gathered using crowdsourcing approach are used by model for classification process i.e. classification of requirements into functional and non-functional requirements. For the proof of proposed model a case study is conducted. Results of case study provided the usefulness and efficiency of proposed model for classification of crowd sourced software requirements.",
        "link": "https://dl.acm.org/doi/10.1145/3328833.3328837"
    },
    {
        "title": "Text classification using Fuzzy TF-IDF and Machine Learning Models",
        "authors": [
            "Mariem Bounabi",
            "Karim El Moutaouakil",
            "Khalid Satori"
        ],
        "date": "October 2019",
        "source": "BDIoT'19: Proceedings of the 4th International Conference on Big Data and Internet of Things",
        "abstract": "The representation of the information has an important impact on the text classification task. Several weighting methods were proposed in the literature, and the term frequency-inverse term frequency (TFIDF), the most know on the text treatment field. The FTF-IDF is a vector representation where the components of the TFIDF are presented as inputs to the Fuzzy Inference System (FIS). In this work, we compare several Machin Learning algorithms such as Naïve Bayes and its derivatives, SVM and Random forest classifiers, using the FTF-IDF representation. To improve the quality of the used classifiers, we call sum attribute selection methods. The recognition rate, for the tested systems, is satisfied, where the system based on naïve Bayes classifier, the FTF-IDF weighting terms, and the info gain select attributes method gives 98.7% as accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3372938.3372956"
    },
    {
        "title": "Text Mining in Cybersecurity: A Systematic Literature Review",
        "authors": [
            "Luciano Ignaczak",
            "Guilherme Goldschmidt",
            "Cristiano André Da Costa",
            "Rodrigo Da Rosa Righi"
        ],
        "date": null,
        "source": "ACM Computing Surveys",
        "abstract": "The growth of data volume has changed cybersecurity activities, demanding a higher level of automation. In this new cybersecurity landscape, text mining emerged as an alternative to improve the efficiency of the activities involving unstructured data. This article proposes a Systematic Literature Review (SLR) to present the application of text mining in the cybersecurity domain. Using a systematic protocol, we identified 2,196 studies, out of which 83 were summarized. As a contribution, we propose a taxonomy to demonstrate the different activities in the cybersecurity domain supported by text mining. We also detail the strategies evaluated in the application of text mining tasks and the use of neural networks to support activities involving unstructured data. The work also discusses text classification performance aiming its application in real-world solutions. The SLR also highlights open gaps for future research, such as the analysis of non-English content and the intensification in the usage of neural networks.",
        "link": "https://dl.acm.org/doi/10.1145/3462477"
    },
    {
        "title": "Using Text Mining to Analyze the Financial Patents",
        "authors": [
            "Yung-Feng Lu",
            "Jia-Lang Xu",
            "Mu-Yen Chen"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "With the advancement and development of science and technology, Nowadays society is the development of information technology, cloud computing, mobile Internet, Internet of Things, block chain and information technology.The combination of finance and technology has made financial technology terms gradually emerging.. This research analyzes the patents of financial technology. However, Taiwan's financial technology patents are mostly G06Q-20 (payment plan, architecture or agreement), G06Q-30 (commercial, such as marketing, shopping, payment, auction or e-commerce). G06Q-40 (financial, investment or tax treatment, insurance).The research use text mining technology to analyze and understand Taiwan's trends and developments in financial technology. The research results show that G06Q-20 patents are biased towards information security and mobile payment. G06Q-30 patents favor shopping websites and advertising. G06Q-40 patents are biased towards diversity, such as securities, insurance, finance, and stocks.",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326478"
    },
    {
        "title": "Identifying Online Profiles of Distance Learning Students Using Data Mining Techniques",
        "authors": [
            "Osama Islam",
            "Muazzam Siddiqui",
            "Naif Radi Aljohani"
        ],
        "date": "October 2019",
        "source": "ICDTE '19: Proceedings of the 3rd International Conference on Digital Technology in Education",
        "abstract": "Educational data has grown over the years with the increased use of technology within educational environments. This has led to a huge amount of data being stored in various data sources representing the student, his/her activities, and other aspects relevant to the learning process. To meet this analytical need, Educational Data Mining (EDM) has emerged to assist educational institutions in identifying key benefits such as students at risk, the level of student engagement or predicting student performance. The aim of this research was to explore the various aspects of student interaction data using data mining techniques to identify relevant patterns of behaviors and possible key attributes that have higher degrees of influence on distance learning students. The main findings identified several patterns of user online profiles based on a set of adopted learning strategies, the research proposed a framework for analyzing such interaction data based on R and Hadoop platforms to correlate online profiles with student performance.",
        "link": "https://dl.acm.org/doi/10.1145/3369199.3369249"
    },
    {
        "title": "Extremely Fast Decision Tree Mining for Evolving Data Streams",
        "authors": [
            "Albert Bifet",
            "Jiajin Zhang",
            "Wei Fan",
            "Cheng He",
            "Jianfeng Zhang",
            "Jianfeng Qian",
            "Geoff Holmes",
            "Bernhard Pfahringer"
        ],
        "date": "August 2017",
        "source": "KDD '17: Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
        "abstract": "Nowadays real-time industrial applications are generating a huge amount of data continuously every day. To process these large data streams, we need fast and efficient methodologies and systems. A useful feature desired for data scientists and analysts is to have easy to visualize and understand machine learning models. Decision trees are preferred in many real-time applications for this reason, and also, because combined in an ensemble, they are one of the most powerful methods in machine learning. In this paper, we present a new system called STREAMDM-C++, that implements decision trees for data streams in C++, and that has been used extensively at Huawei. Streaming decision trees adapt to changes on streams, a huge advantage since standard decision trees are built using a snapshot of data, and can not evolve over time. STREAMDM-C++ is easy to extend, and contains more powerful ensemble methods, and a more efficient and easy to use adaptive decision trees. We compare our new implementation with VFML, the current state of the art implementation in C, and show how our new system outperforms VFML in speed using less resources.",
        "link": "https://dl.acm.org/doi/10.1145/3097983.3098139"
    },
    {
        "title": "Data miners' little helper: data transformation activity cues for cluster analysis on document collections",
        "authors": [
            "Tania Cerquitelli",
            "Evelina Di Corso",
            "Francesco Ventura",
            "Silvia Chiusano"
        ],
        "date": "June 2017",
        "source": "WIMS '17: Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "In this paper we propose a new self-learning engine to streamline the analytics process, as it enables analysts to mine massive data repositories with minimal user intervention. In the context of cluster analysis on a collection of documents this new system, named SELF-DATA (SELF-learning DAta TrAnsformation), suggests to the analyst how to configure the whole mining process for a given dataset. SELF-DATA relies on an engine exploring different data weighting schemas (e.g., normalized term frequencies) and data transformation methods (e.g., PCA) before applying the cluster analysis, evaluating and comparing solutions through different quality indices (e.g., weighted Silhouette), and presenting the k-top solutions to the analyst. SELF-DATA will also include a knowledge base storing results of experiments on previously processed datasets, and a classification algorithm trained on the knowledge base content to forecast the best configuration for the whole mining process for an unexplored dataset. The first development of SELF-DATA running on Apache Spark has been validated on 5 collections of documents. Experimental results highlight that TF-IDF and logarithmic entropy are effective to measure item relevance with sparse datasets, and the LSI method outperforms PCA with a large dictionary.",
        "link": "https://dl.acm.org/doi/10.1145/3102254.3102288"
    },
    {
        "title": "Interactive Visual Graph Mining and Learning",
        "authors": [
            "Ryan A. Rossi",
            "Nesreen K. Ahmed",
            "Rong Zhou",
            "Hoda Eldardiry"
        ],
        "date": null,
        "source": "ACM Transactions on Intelligent Systems and Technology",
        "abstract": "This article presents a platform for interactive graph mining and relational machine learning called GraphVis. The platform combines interactive visual representations with state-of-the-art graph mining and relational machine learning techniques to aid in revealing important insights quickly as well as learning an appropriate and highly predictive model for a particular task (e.g., classification, link prediction, discovering the roles of nodes, and finding influential nodes). Visual representations and interaction techniques and tools are developed for simple, fast, and intuitive real-time interactive exploration, mining, and modeling of graph data. In particular, we propose techniques for interactive relational learning (e.g., node/link classification), interactive link prediction and weighting, role discovery and community detection, higher-order network analysis (via graphlets, network motifs), among others. GraphVis also allows for the refinement and tuning of graph mining and relational learning methods for specific application domains and constraints via an end-to-end interactive visual analytic pipeline that learns, infers, and provides rapid interactive visualization with immediate feedback at each change/prediction in real-time. Other key aspects include interactive filtering, querying, ranking, manipulating, exporting, as well as tools for dynamic network analysis and visualization, interactive graph generators (including new block model approaches), and a variety of multi-level network analysis techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3200764"
    },
    {
        "title": "Data Mining and Opinion Mining: A Tool in Educational Context",
        "authors": [
            "Myriam Peñafiel",
            "Stefanie Vásquez",
            "Diego Vásquez",
            "Juan Zaldumbide",
            "Sergio Luján-Mora"
        ],
        "date": "July 2018",
        "source": "ICoMS '18: Proceedings of the 2018 1st International Conference on Mathematics and Statistics",
        "abstract": "The use of the web as a universal communication platform generates large volumes of data (Big data), which in many cases, need to be processed so that they can become useful knowledge in face of the sceptics who have doubts about the credibility of such information. The use of web data that comes from educational contexts needs to be addressed, since that large amount of unstructured information is not being valued, losing valuable information that can be used. To solve this problem, we propose the use of data mining techniques such as sentiment analysis to validate the information that comes from the educational platforms. The objective of this research is to propose a methodology that allows the user to apply sentiment analysis in a simple way, because although some researchers have done it, very few do with data in the educational context. The results obtained prove that the proposal can be used in similar cases.",
        "link": "https://dl.acm.org/doi/10.1145/3274250.3274263"
    },
    {
        "title": "Research Hotspots Mining and Visualized Analysis Based on Linking Cluster and K-Core Decomposition",
        "authors": [
            "Ying Cai",
            "Fang Huang",
            "Shuai Wang",
            "Hao Zhang",
            "Chunxiu Du"
        ],
        "date": "May 2018",
        "source": "ICDPA 2018: Proceedings of the International Conference on Data Processing and Applications",
        "abstract": "In this paper, we focus on research hotspots mining with knowledge semantic features on discipline topic word networks, which are extracted from scientific and technical documents. For the topic networks, a novel method of combining linking community clustering with k-core decomposition is proposed to mine the hotspot community with knowledge hierarchy structure. And as a post-processing, the hierarchical community identification is implemented by k-core decomposition in order to discover the internal connection with knowledge semantic hierarchy in the research hotspot. Finally, the community density and visualization method is utilized to analyze the core and sub- problem in the research hotspot community. In experiments, the topic networks for a single discipline and interdisciplinary research hotspot are taken as examples of analysis, and the results show the proposed approach can effectively realize the objective of mining and analyzing the discipline research hotspot on the academic topic word network.",
        "link": "https://dl.acm.org/doi/10.1145/3224207.3224216"
    },
    {
        "title": "The Application of Data Mining Algorithms in the Construction of Travel Recommendation System",
        "authors": [
            "Chao Lou",
            "Zhaonan Mu",
            "Mengzhu Liu"
        ],
        "date": "January 2021",
        "source": "CONF-CDS 2021: The 2nd International Conference on Computing and Data Science",
        "abstract": "Photos shared by users of online social network often associated with tags, texts, geographic and time information. These data are ideal research resource which can be used by researchers to observe people's interests and behaviors. They can also be used to analyze people's travel behaviors and interests because it can reflect people's travel activities and experiences directly. This project aimed to consolidate people into groups based on their travel preferences of travel season, travel time span, scenery type of travel location and then analyze popular travel places for different people groups. The travel preferences were analyzed from extracted data applying PHP language and Naive Bayes classification approach. According to the analyzed data, people were divided into groups. Then, DBSCAN clustering was used to get popular locations based on the different groups. As a result, 80 thousand photo data of 5 cities have been collected from Flickr by Phython. It has been showed that people's preference of season in most of the mined cities was spring and summer and the preference of time span was short. Moreover, mined people has been divided into 16 groups according to their preference and based on the groups popular locations have been calculated by DBSCAN clustering. Theses locations have been provided in a recommendation website which was built in this project. For the future work, mining Flickr data associated with other travel social network sites could be a good way to build a more effective recommendation system.",
        "link": "https://dl.acm.org/doi/10.1145/3448734.3450477"
    },
    {
        "title": "Compare Machine Learning Models in Text Classification Using Steam User Reviews",
        "authors": [
            "Youchen Miao",
            "Zeyu Jin",
            "Yumeng Zhang",
            "Yuchen Chen",
            "Junren Lai"
        ],
        "date": "November 2021",
        "source": "ICSED '21: Proceedings of the 2021 3rd International Conference on Software Engineering and Development",
        "abstract": "Text Classification and Sentiment Analysis of game reviews are viewed as important parts in not only academic fields but also in game studies. In this paper, with more than 400 thousand game reviews on Steam platform, we preprocess the data using different libraries (sklearn, nltk, and spaCy) and use them as inputs to build three sentiment classification models based on different algorithms (Naive Bayes, SVM, and Random Forest). In contrast to previous studies that only focus on different sentiment analysis models, our paper also highlights the use of different APIs to preprocess the data and their corresponding model performance. The results show that no matter which API we choose, Random Forest models always perform the best. However, in terms of training time, Naive Bayes is the fastest. This work can be used to apply grid search for researchers to automatically find the optimum API before conducting sentiment analysis in the future.",
        "link": "https://dl.acm.org/doi/10.1145/3507473.3507480"
    },
    {
        "title": "Lost in Transduction: Transductive Transfer Learning in Text Classification",
        "authors": [
            "Alejandro Moreo",
            "Andrea Esuli",
            "Fabrizio Sebastiani"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Obtaining high-quality labelled data for training a classifier in a new application domain is often costly. Transfer Learning\n(a.k.a. “Inductive Transfer”) tries to alleviate these costs by transferring, to the “target” domain of interest, knowledge available from a different “source” domain. In transfer learning the lack of labelled information from the target domain is compensated by the availability at training time of a set of unlabelled examples from the target distribution. Transductive Transfer Learning denotes the transfer learning setting in which the only set of target documents that we are interested in classifying is known and available at training time. Although this definition is indeed in line with Vapnik’s original definition of “transduction”, current terminology in the field is confused. In this article, we discuss how the term “transduction” has been misused in the transfer learning literature, and propose a clarification consistent with the original characterization of this term given by Vapnik. We go on to observe that the above terminology misuse has brought about misleading experimental comparisons, with inductive transfer learning methods that have been incorrectly compared with transductive transfer learning methods. We then, give empirical evidence that the difference in performance between the inductive version and the transductive version of a transfer learning method can indeed be statistically significant (i.e., that knowing at training time the only data one needs to classify indeed gives an advantage). Our clarification allows a reassessment of the field, and of the relative merits of the major, state-of-the-art algorithms for transfer learning in text classification.",
        "link": "https://dl.acm.org/doi/10.1145/3453146"
    },
    {
        "title": "Machine Learning on Mining Potential Adverse Drug Reactions for Pharmacovigilance",
        "authors": [
            "Ke Jia"
        ],
        "date": "July 2021",
        "source": "DSIT 2021: 2021 4th International Conference on Data Science and Information Technology",
        "abstract": "Drugs are related to the lives and health of patients. Mining and discovering potential adverse drug reactions from a large number of unstructured texts play an important role in drug research and pharmacovigilance. It is a hot research issue in the field of biomedicine in recent years. A large number of scholars have discovered a large amount of information on adverse drug reactions from different data sources through effective machine learning methods. This paper reviews the research progress in recent years to provide a reference for future research.",
        "link": "https://dl.acm.org/doi/10.1145/3478905.3478964"
    },
    {
        "title": "TACAM: Topic And Context Aware Argument Mining",
        "authors": [
            "Michael Fromm",
            "Evgeniy Faerman",
            "Thomas Seidl"
        ],
        "date": "October 2019",
        "source": "WI '19: IEEE/WIC/ACM International Conference on Web Intelligence",
        "abstract": "In this work we address the problem of argument search. The purpose of argument search is the distillation of pro and contra arguments for requested topics from large text corpora. In previous works, the usual approach is to use a standard search engine to extract text parts which are relevant to the given topic and subsequently use an argument recognition algorithm to select arguments from them. The main challenge in the argument recognition task, which is also known as argument mining, is that often sentences containing arguments are structurally similar to purely informative sentences without any stance about the topic. In fact, they only differ semantically. Most approaches use topic or search term information only for the first search step and therefore assume that arguments can be classified independently of a topic. We argue that topic information is crucial for argument mining, since the topic defines the semantic context of an argument. Precisely, we propose different models for the classification of arguments, which take information about a topic of an argument into account. Moreover, to enrich the context of a topic and to let models understand the context of the potential argument better, we integrate information from different external sources such as Knowledge Graphs or pre-trained NLP models. Our evaluation shows that considering topic information, especially in connection with external information, provides a significant performance boost for the argument mining task.",
        "link": "https://dl.acm.org/doi/10.1145/3350546.3352506"
    },
    {
        "title": "Text mining for incoming tasks based on the urgency/importance factors and task classification using machine learning tools",
        "authors": [
            "Yasser Ali Alshehri"
        ],
        "date": "March 2020",
        "source": "ICCDA 2020: Proceedings of the 2020 the 4th International Conference on Compute and Data Analysis",
        "abstract": "In workplaces, there is a massive amount of unstructured data from different sources. In this paper, we present a case study that explains how can through communications between employees, we can help to prioritize tasks requests to increase the efficiency of their works for both technical and non-technical workers. This involves managing daily incoming tasks based on their level of urgency and importance.To allow all workers to utilize the urgency-importance matrix as a time-management tool, we need to automate this tool. The textual content of incoming tasks are analyzed, and metrics related to urgency and importance are extracted. A third factor (i.e., the response variable) is defined based on the two input variables (urgency and importance). Then, machine learning applied to the data to predict the class of incoming tasks based on data outcome desired. We used ordinal regression, neural networks, and decision tree algorithms to predict the four levels of task priority. We measure the performance of all using recalls, precisions, and F-scores. All classifiers perform higher than 89% in terms of all measures.",
        "link": "https://dl.acm.org/doi/10.1145/3388142.3388153"
    },
    {
        "title": "Text mining for malware classification using multivariate all repeated patterns detection",
        "authors": [
            "Konstantinos F. Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2019",
        "source": "ASONAM '19: Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Mobile phones have become nowadays a commodity to the majority of people. Using them, people are able to access the world of Internet and connect with their friends, their colleagues at work or even unknown people with common interests. This proliferation of the mobile devices has also been seen as an opportunity for the cyber criminals to deceive smartphone users and steel their money directly or indirectly, respectively, by accessing their bank accounts through the smartphones or by blackmailing them or selling their private data such as photos, credit card data, etc. to third parties. This is usually achieved by installing malware to smartphones masking their malevolent payload as a legitimate application and advertise it to the users with the hope that mobile users will install it in their devices. Thus, any existing application can easily be modified by integrating a malware and then presented it as a legitimate one. In response to this, scientists have proposed a number of malware detection and classification methods using a variety of techniques. Even though, several of them achieve relatively high precision in malware classification, there is still space for improvement. In this paper, we propose a text mining all repeated pattern detection method which uses the decompiled files of an application in order to classify a suspicious application into one of the known malware families. Based on the experimental results using a real malware dataset, the methodology tries to correctly classify (without any misclassification) all randomly selected malware applications of 3 categories with 3 different families each.",
        "link": "https://dl.acm.org/doi/10.1145/3341161.3350841"
    },
    {
        "title": "Comparative Study between Traditional Machine Learning and Deep Learning Approaches for Text Classification",
        "authors": [
            "Cannannore Nidhi Kamath",
            "Syed Saqib Bukhari",
            "Andreas Dengel"
        ],
        "date": "August 2018",
        "source": "DocEng '18: Proceedings of the ACM Symposium on Document Engineering 2018",
        "abstract": "In this contemporaneous world, it is an obligation for any organization working with documents to end up with the insipid task of classifying truckload of documents, which is the nascent stage of venturing into the realm of information retrieval and data mining. But classification of such humongous documents into multiple classes, calls for a lot of time and labor. Hence a system which could classify these documents with acceptable accuracy would be of an unfathomable help in document engineering. We have created multiple classifiers for document classification and compared their accuracy on raw and processed data. We have garnered data used in a corporate organization as well as publicly available data for comparison. Data is processed by removing the stop-words and stemming is implemented to produce root words. Multiple traditional machine learning techniques like Naive Bayes, Logistic Regression, Support Vector Machine, Random forest Classifier and Multi-Layer Perceptron are used for classification of documents. Classifiers are applied on raw and processed data separately and their accuracy is noted. Along with this, Deep learning technique such as Convolution Neural Network is also used to classify the data and its accuracy is compared with that of traditional machine learning techniques. We are also exploring hierarchical classifiers for classification of classes and subclasses. The system classifies the data faster and with better accuracy than if done manually. The results are discussed in the results and evaluation section.",
        "link": "https://dl.acm.org/doi/10.1145/3209280.3209526"
    },
    {
        "title": "Identification of Overpricing in the Purchase of Medication by the Federal Government of Brazil, Using Text Mining and Clustering Based on Ontology",
        "authors": [
            "Marco Aurelio O. S. Correa",
            "Adriano Galindo Leal"
        ],
        "date": "August 2018",
        "source": "ICCBDC '18: Proceedings of the 2018 2nd International Conference on Cloud and Big Data Computing",
        "abstract": "Increasing the transparency level in his actions and spending is one of the primary duties of the Brazilian Federal Government. The creation of laws that oblige full disclose of all its expenditures through transparency portals enables citizens to supervise all government entities. However, only the dissemination of these data, without a definite standard or the availability of data analysis tools, does not guarantee that the citizen is empowered to play his role. Therefore hence, the objective of this work is to identify overprice in the acquisition of products purchased by the federal government of Brazil using the unstructured data available on the Transparency Portal. The last two-years' worth of purchasing data, available in the Transparency Portal, were extracted, processed and stored. Due to his diverse nature and high volume of data, this study focused only on medicines purchased by the Ministry of Health. Ontology-based text mining and clustering techniques were applied for automatic identification and classification of products. The processing of this information was done through text mining and clustering, based on the ontology registered in another database of the Brazilian government. Because of this work, a consolidated price base per medication was created to allow the identification of distortions in prices practised, facilitating the identification of cases that merit further investigation to unravel fraud to the treasury.",
        "link": "https://dl.acm.org/doi/10.1145/3264560.3264569"
    },
    {
        "title": "Computational Estimation by Scientific Data Mining with Classical Methods to Automate Learning Strategies of Scientists",
        "authors": [
            "Aparna S. Varde"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Experimental results are often plotted as 2-dimensional graphical plots (aka graphs) in scientific domains depicting dependent versus independent variables to aid visual analysis of processes. Repeatedly performing laboratory experiments consumes significant time and resources, motivating the need for computational estimation. The goals are to estimate the graph obtained in an experiment given its input conditions, and to estimate the conditions that would lead to a desired graph. Existing estimation approaches often do not meet accuracy and efficiency needs of targeted applications. We develop a computational estimation approach called AutoDomainMine that integrates clustering and classification over complex scientific data in a framework so as to automate classical learning methods of scientists. Knowledge discovered thereby from a database of existing experiments serves as the basis for estimation. Challenges include preserving domain semantics in clustering, finding matching strategies in classification, striking a good balance between elaboration and conciseness while displaying estimation results based on needs of targeted users, and deriving objective measures to capture subjective user interests. These and other challenges are addressed in this work. The AutoDomainMine approach is used to build a computational estimation system, rigorously evaluated with real data in Materials Science. Our evaluation confirms that AutoDomainMine provides desired accuracy and efficiency in computational estimation. It is extendable to other science and engineering domains as proved by adaptation of its sub-processes within fields such as Bioinformatics and Nanotechnology.",
        "link": "https://dl.acm.org/doi/10.1145/3502736"
    },
    {
        "title": "Where is the road for issue reports classification based on text mining?",
        "authors": [
            "Qiang Fan",
            "Yue Yu",
            "Gang Yin",
            "Tao Wang",
            "Huaimin Wang"
        ],
        "date": "November 2017",
        "source": "ESEM '17: Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Currently, open source projects receive various kinds of issues daily, because of the extreme openness of Issue Tracking System (ITS) in GitHub. ITS is a labor-intensive and time-consuming task of issue categorization for project managers. However, a contributor is only required a short textual abstract to report an issue in GitHub. Thus, most traditional classification approaches based on detailed and structured data (e.g., priority, severity, software version and so on) are difficult to adopt. In this paper, issue classification approaches on a large-scale dataset, including 80 popular projects and over 252,000 issue reports collected from GitHub, were investigated. First, four traditional text-based classification methods and their performances were discussed. Semantic perplexity (i.e., an issues description confuses bug-related sentences with nonbug-related sentences) is a crucial factor that affects the classification performances based on quantitative and qualitative study. Finally, A two-stage classifier framework based on the novel metrics of semantic perplexity of issue reports was designed. Results show that our two-stage classification can significantly improve issue classification performances.",
        "link": "https://dl.acm.org/doi/10.1109/ESEM.2017.19"
    },
    {
        "title": "Feature-based Facebook reviews process model for e-management using data mining",
        "authors": [
            "Anish Kumar Varudharajulu",
            "Yongsheng Ma"
        ],
        "date": "January 2019",
        "source": "IC4E '19: Proceedings of the 10th International Conference on E-Education, E-Business, E-Management and E-Learning",
        "abstract": "The data generated from online communication acts as potential gold mines for discovering knowledge for researchers. A large amount of data is also generated in the form of web documents, emails, blogs, and feedback, etc. Text analytics is being significantly employed to mine important information. Opinion mining is the process of extracting human thoughts and perceptions from unstructured texts. The showstopper for designing an opinion mining system for analyzing reviews arise from the fact that customer reviews are often noisy. These reviews are informally written. In addition, they are subjected to spelling mistakes, grammatical errors, improper punctuation and irrational capitalization. This paper focuses on analyzing the different classification and clustering algorithms aimed at extracting and consolidating opinions of customers from social media sites like Facebook, Twitter and through surveys, at multiple levels of granularity to monitor and measure customer satisfaction. Ours is an automated approach, in which the system aids in the process of knowledge assimilation for knowledge-based building and also performs the analytics. Domain experts ratify the knowledge base and also provide training datasets for the system to intuitively gather more instances for ratification. The system identifies opinion expressions as phrases containing opinion words, opinionated features and also opinion modifiers. These expressions are categorized as positive, negative or neutral. Opinion expressions are identified and categorized using localized linguistic techniques. Opinions can be congregated at any desired level of specificity i.e. feature level or product level, user level or service level, etc. We have developed a system based on this approach, which provides the user with a platform to analyze opinion expressions crawled from a set of pre-defined datasets.",
        "link": "https://dl.acm.org/doi/10.1145/3306500.3306514"
    },
    {
        "title": "Discriminative Topic Mining via Category-Name Guided Text Embedding",
        "authors": [
            "Yu Meng",
            "Jiaxin Huang",
            "Guangyuan Wang",
            "Zihan Wang",
            "Chao Zhang",
            "Yu Zhang",
            "Jiawei Han"
        ],
        "date": "April 2020",
        "source": "WWW '20: Proceedings of The Web Conference 2020",
        "abstract": "Mining a set of meaningful and distinctive topics automatically from massive text corpora has broad applications. Existing topic models, however, typically work in a purely unsupervised way, which often generate topics that do not fit users’ particular needs and yield suboptimal performance on downstream tasks. We propose a new task, discriminative topic mining, which leverages a set of user-provided category names to mine discriminative topics from text corpora. This new task not only helps a user understand clearly and distinctively the topics he/she is most interested in, but also benefits directly keyword-driven classification tasks. We develop CatE, a novel category-name guided text embedding method for discriminative topic mining, which effectively leverages minimal user guidance to learn a discriminative embedding space and discover category representative terms in an iterative manner. We conduct a comprehensive set of experiments to show that CatE mines high-quality set of topics guided by category names only, and benefits a variety of downstream applications including weakly-supervised classification and lexical entailment direction identification.",
        "link": "https://dl.acm.org/doi/10.1145/3366423.3380278"
    },
    {
        "title": "Comparative Study of Heart Disease Diagnosis Using Top Ten Data Mining Classification Algorithms",
        "authors": [
            "I. Ketut Agung Enriko"
        ],
        "date": "June 2019",
        "source": "ICFET '19: Proceedings of the 5th International Conference on Frontiers of Educational Technologies",
        "abstract": "Data mining has been used for many purposes, especially for prediction system. In healthcare, data mining algorithms often used in disease diagnosis. Meanwhile, heart disease is known as a primary cause of death over the years. Many studies have been performed in heart disease diagnosis using data mining methods. There are some popular data mining algorithms that can be used in heart disease diagnosis, for example, k-Nearest Neighbor, CART, and AdaBoost. The algorithms are used to analyze a sample of cardiovascular patients data and predict the heart disease type that they suffer. Some parameters are taken from the patient, including EKG morphology, blood pressure, and information about the existence of chest pain, shortness of breath, palpitation, and cold sweat. In this study, medical records data are collected from Harapan Kita Hospital and utilized as a dataset sample in this research. Top ten data mining classification algorithms are used in diagnosing heart disease from Harapan Kita Hospital data and examining their performance by checking the accuracy and speed.",
        "link": "https://dl.acm.org/doi/10.1145/3338188.3338220"
    },
    {
        "title": "Text Mining Approach for Identifying Research Trends",
        "authors": [
            "Snezhana Sulova"
        ],
        "date": "June 2021",
        "source": "CompSysTech '21: Proceedings of the 22nd International Conference on Computer Systems and Technologies",
        "abstract": "With the increase of unstructured data, the issues connected with automatic text processing, the categorization of documents and the discovery of topics have become objects of growing interest. In order to improve the process of grouping and processing research publications, we would like to propose a method based upon natural language processing. It is based on text mining technologies which aim to identify key tendencies in documents. It processes the content of publications by clustering and identifies the topics of each identified group. This analysis helps by identifying key tendencies as well as discovering emerging new areas of research. Publications from the research literature database, Scopus, were used to test the approach. The topic of the publications is “the application of digital technologies in the logistics business”. The experiments were completed using the RapidMiner Studio software.",
        "link": "https://dl.acm.org/doi/10.1145/3472410.3472433"
    },
    {
        "title": "Assessment of Vulnerability Severity using Text Mining",
        "authors": [
            "Georgios Spanos",
            "Lefteris Angelis",
            "Dimitrios Toloudis"
        ],
        "date": "September 2017",
        "source": "PCI '17: Proceedings of the 21st Pan-Hellenic Conference on Informatics",
        "abstract": "Software1 vulnerabilities are closely associated with information systems security, a major and critical field in today's technology. Vulnerabilities constitute a constant and increasing threat for various aspects of everyday life, especially for safety and economy, since the social impact from the problems that they cause is complicated and often unpredictable. Although there is an entire research branch in software engineering that deals with the identification and elimination of vulnerabilities, the growing complexity of software products and the variability of software production procedures are factors contributing to the ongoing occurrence of vulnerabilities, Hence, another area that is being developed in parallel focuses on the study and management of the vulnerabilities that have already been reported and registered in databases. The information contained in such databases includes, a textual description and a number of metrics related to vulnerabilities. The purpose of this paper is to investigate to what extend the assessment of the vulnerability severity can be inferred directly from the corresponding textual description, or in other words, to examine the informative power of the description with respect to the vulnerability severity. For this purpose, text mining techniques, i.e. text analysis and three different classification methods (decision trees, neural networks and support vector machines) were employed. The application of text mining to a sample of 70,678 vulnerabilities from a public data source shows that the description itself is a reliable and highly accurate source of information for vulnerability prioritization.",
        "link": "https://dl.acm.org/doi/10.1145/3139367.3139390"
    },
    {
        "title": "Clustering Algorithms for Spatial Data Mining",
        "authors": [
            "Chetashri Bhadane",
            "Ketan Shah"
        ],
        "date": "April 2020",
        "source": "ICGDA '20: Proceedings of the 2020 3rd International Conference on Geoinformatics and Data Analysis",
        "abstract": "With the advances in mobile and wireless technologies, there has been a rise in applications that track and share the users' geospatial data. People use several social networking sites such as Twitter, Facebook and Flickr, where they share their status updates. With the integration of Global Positioning System (GPS) with mobile phones, it is now possible to share one's locations on these social networks. GPS allows us to record and track a person's movement along with the timestamp. The data set obtained from these GPS logs is vast and is widely used to analyze the users' movement patterns. Specifically, we can find out significant locations based on the number of users present at that location and the time spent by them at such places. Once significant places have been identified, it is also possible to identify the semantic importance of these locations. This paper presents an overview of the clustering techniques used to find important places of interest using large GPS based mobility datasets. Four clustering algorithms, K-Means, DBSCAN, OPTICS and Hierarchical, are implemented, and performance is tested using real-time data of 50 users collected over 2--5 years. Performance summary depicts that K-Means and DBSCAN perform well for spatial data.",
        "link": "https://dl.acm.org/doi/10.1145/3397056.3397068"
    },
    {
        "title": "Feature Selection based Arabic Text Classification using Different Machine Learning Algorithms: Comparative Study",
        "authors": [
            "Sakina Rim Bennabi",
            "Zakaria Elberrichi"
        ],
        "date": "June 2020",
        "source": "ICIST '20: Proceedings of the 10th International Conference on Information Systems and Technologies",
        "abstract": "Feature selection is a method of data pre-processing widely used when mining large data, such as textual classification. Several studies have been conducted to compare the different methods of feature selection applied to corpora in English. Unfortunately, a small number of works concern the Arabic language. This article aims to present a comparative study of different feature selection techniques including: Chi2, the ANOVA method and mutual information, applied on a corpus in Arabic language, while also diversifying the machine learning algorithms (Naive Bayes, SVM and KNN). This experimental study has shown in general that reducing dimensionality with feature selection techniques has slightly affected the performance of textual classification, reducing the size of the corpus by up to 1%.",
        "link": "https://dl.acm.org/doi/10.1145/3447568.3448531"
    },
    {
        "title": "Intelligent Data Analysis using Optimized Support Vector Machine Based Data Mining Approach for Tourism Industry",
        "authors": [
            "Ms Promila Sharma",
            "Uma Meena",
            "Girish Kumar Sharma"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Data analysis involves the deployment of sophisticated approaches from data mining methods, information theory, and artificial intelligence in various fields like tourism, hospitality, and so on for the extraction of knowledge from the gathered and preprocessed data. In tourism, pattern analysis or data analysis using classification is significant for finding the patterns that represent new and potentially useful information or knowledge about the destination and other data. Several data mining techniques are introduced for the classification of data or patterns. However, overfitting, less accuracy, local minima, sensitive to noise are the drawbacks in some existing data mining classification methods. To overcome these challenges, Support vector machine with Red deer optimization (SVM-RDO) based data mining strategy is proposed in this article. Extended Kalman filter (EKF) is utilized in the first phase, i.e., data cleaning to remove the noise and missing values from the input data. Mantaray foraging algorithm (MaFA) is used in the data selection phase, in which the significant data are selected for the further process to reduce the computational complexity. The final phase is the classification, in which SVM-RDO is proposed to access the useful pattern from the selected data. PYTHON is the implementation tool used for the experiment of the proposed model. The experimental analysis is done to show the efficacy of the proposed work. From the experimental results, the proposed SVM-RDO achieved better accuracy, precision, recall, and F1 score than the existing methods for the tourism dataset. Thus, it is showed the effectiveness of the proposed SVM-RDO for pattern analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3494566"
    },
    {
        "title": "Applying text mining to predict learners' cognitive engagement",
        "authors": [
            "Hayati Hind",
            "Mohammed Khalidi Idrissi",
            "Samir Bennani"
        ],
        "date": "October 2017",
        "source": "SCAMS '17: Proceedings of the Mediterranean Symposium on Smart City Application",
        "abstract": "In academic process, engagement represent one of the important success' factor. This paper proposes a predictive system for learners' cognitive engagement based on their online discussion forums participation. In the first level the ontology OWL and the LSA method are used to perform a semantic classification system of the threads according to a specific context chosen by the tutor. Then the Text mining, as a predictive method, is applied to the classified threads and learners' participation in the forums.",
        "link": "https://dl.acm.org/doi/10.1145/3175628.3175655"
    },
    {
        "title": "Investigate Transitions into Drug Addiction through Text Mining of Reddit Data",
        "authors": [
            "John Lu",
            "Sumati Sridhar",
            "Ritika Pandey",
            "Mohammad Al Hasan",
            "Georege Mohler"
        ],
        "date": "July 2019",
        "source": "KDD '19: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Increasing rates of opioid drug abuse and heightened prevalence of online support communities underscore the necessity of employing data mining techniques to better understand drug addiction using these rapidly developing online resources. In this work, we obtained data from Reddit, an online collection of forums, to gather insight into drug use/misuse using text snippets from users narratives. Specifically, using users' posts, we trained a binary classifier which predicts a user's transitions from casual drug discussion forums to drug recovery forums. We also proposed a Cox regression model that outputs likelihoods of such transitions. In doing so, we found that utterances of select drugs and certain linguistic features contained in one's posts can help predict these transitions. Using unfiltered drug-related posts, our research delineates drugs that are associated with higher rates of transitions from recreational drug discussion to support/recovery discussion, offers insight into modern drug culture, and provides tools with potential applications in combating the opioid crisis.",
        "link": "https://dl.acm.org/doi/10.1145/3292500.3330737"
    },
    {
        "title": "Analysing digital banking reviews using text mining",
        "authors": [
            "Li Chen Cheng",
            "Legaspi Rhea Sharmayne"
        ],
        "date": "December 2020",
        "source": "ASONAM '20: Proceedings of the 12th IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Digital banks are new entrants in the banking industry in the Philippines as they only started late 2018. Since then, a handful of players have and are still emerging. With more and more people becoming technologically savvy, it is very critical for financial institutions to develop a digital banking application that will stand out from the competition. This paper aims to use text mining methods to analyse digital banking application reviews. This study will perform topic modelling using LDA to explore customer concerns and will mine association rules between the digital banking features with the review score. The results will reveal which areas the digital banking application can further optimize for customer satisfaction and retention.",
        "link": "https://dl.acm.org/doi/10.1109/ASONAM49781.2020.9381429"
    },
    {
        "title": "Mining Moocs Videos Metadata Using Classification Techniques",
        "authors": [
            "El Harrak Othman",
            "Ghadi Abderrahim",
            "El Bouhdidi Jaber"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "In few years, the internet has become the first source of information for most people, Today MOOCs makes it possible for everyone to access to the education over the world, it's represents an emerging methodology of online teaching and an important development in open education. Due to the rapid development of Moocs and the rapid growth of digital data and video database over the Internet, it is becoming very difficult for learners to browsing and choosing the best training for them. The scientific community has increased the amount of research into new technologies, with a view to improve the digital video utilization: its archiving, indexing, accessibility, acquisition, store and even its process and usability. All these parts of the video utilization entail the necessity of the extraction of all important information of a video, especially in cases of lack of metadata information. Web video mining is retrieving the content using data mining techniques from World Wide Web. There are two approaches for web video mining using traditional image processing (signal processing) and metadata based approach. In this paper, we present the various research makes in this subject and we propose an effective methodology to extract the metadata from Moocs videos and classify them based on the extracted metadata by applying data mining techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090450"
    },
    {
        "title": "Text mining with HathiTrust",
        "authors": [
            "Eleanor Dickson Koehl",
            "Ryan Dubnicek"
        ],
        "date": "June 2019",
        "source": "JCDL '19: Proceedings of the 18th Joint Conference on Digital Libraries",
        "abstract": "This tutorial will introduce attendees to the HathiTrust Research Center and its tools and services for computational text analysis research. HTRC leverages the scope and scale of the HathiTrust Digital Library collection to create opportunities for researchers to perform text data mining on subsets of the corpus. Attendees will develop skills that will allow them to conduct basic text analysis research using HathiTrust data, learn techniques for engaging with scholars on their text data mining research, and explore strategies for computational access to large-scale digital collections.",
        "link": "https://dl.acm.org/doi/10.1109/JCDL.2019.00115"
    },
    {
        "title": "Text mining for plagiarism detection: multivariate pattern detection for recognition of text similarities",
        "authors": [
            "Konstantinos Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2018",
        "source": "ASONAM '18: Proceedings of the 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "The problem of plagiarism the recent years has been intensified by the availability of information in digital form and the accessibility of the electronic libraries through the Internet. As a result, plagiarism detection has been transformed into a big data analytics problem since the number of digital sources is extravagant and a new document needs to be compared with millions of other existing documents. In this paper, a text mining methodology is proposed that can detect all common patterns between a document and the documents in a reference database. The technique is based on a pattern detection algorithm and the corresponding data structure that enables the algorithm to detect all common patterns. The methodology has been applied in a well-defined dataset providing very promising results identifying difficult cases of plagiarism such as technical disguise.",
        "link": "https://dl.acm.org/doi/10.5555/3382225.3382424"
    },
    {
        "title": "Petroleum Engineering Data Text Classification Using Convolutional Neural Network Based Classifier",
        "authors": [
            "Tzuhan Hsu",
            "Yaoqin Zhang"
        ],
        "date": "May 2018",
        "source": "ICMLT '18: Proceedings of the 2018 International Conference on Machine Learning Technologies",
        "abstract": "In this paper, authors introduce a convolutional neural network (CNN) based classification approach for petroleum engineering event. Recently, text classification problem has raised substantial attention in the field of machine learning. The aim of text classification process is to assign correct label to a text on the basis of its content. However, due to the imprecise of feature representation, most of former text classification methods cannot export satisfying result. To solve this problem, authors propose a model based on convolutional neural network. Authors conduct experiments with real world petroleum engineering data text, artificially labelled into 6 categories. Beside purposed CNN-based model, the authors also implement other baseline machine learning approaches: k nearest neighbor, random forest, support vector machines, recurrent neural network, gated recurrent units, long short-term memory. Among all these models, our CNN-based model reach best performance in text classification task. In the last part of this paper, authors compare and analysis their purpose and baseline model.",
        "link": "https://dl.acm.org/doi/10.1145/3231884.3231898"
    },
    {
        "title": "Classifying Indonesian Online Articles as Advertisement Placement Base Using Text Mining",
        "authors": [
            "Nadhira Tasya",
            "Arian Dhini"
        ],
        "date": "July 2017",
        "source": "ICBIM 2017: Proceedings of the International Conference on Business and Information Management",
        "abstract": "Rapid development in technological aspect resulting in growing level of human needs for the latest news, so that emerged a new trend of publishing and accessing news through online media or usually called online journalism. In addition, the number of people who sell and purchase through online sites also continues to increase and this opportunity is utilized by the company and the advertiser by implementing targeted web advertising. However, the high number of articles that have been published and accessed leads to great opportunities for errors in determining where to place the ads. Therefore, it needs a system that can categorize articles accessed by users as the basis of advertisement placement by the company and this classification system can be done by applying the method of Data Mining and Text Mining. This research uses document data in the form of article content that will be categorized into twenty categories of class of advertisement by using Text Mining technique with Support Vector Machine algorithm. The results of this study may be used by companies or advertisers as a basis for placement of ads on selected online media sites.",
        "link": "https://dl.acm.org/doi/10.1145/3134271.3134288"
    },
    {
        "title": "Mining Free-Text Medical Notes for Suicide Risk Assessment",
        "authors": [
            "Marios Adamou",
            "Grigoris Antoniou",
            "Elissavet Greasidou",
            "Vincenzo Lagani",
            "Paulos Charonyktakis",
            "Ioannis Tsamardinos"
        ],
        "date": "July 2018",
        "source": "SETN '18: Proceedings of the 10th Hellenic Conference on Artificial Intelligence",
        "abstract": "Suicide has been considered as an important public health issue for a very long time, and is one of the main causes of death worldwide. Despite suicide prevention strategies being applied, the rate of suicide has not changed substantially over the past decades. Advances in machine learning make it possible to attempt to predict suicide based on the analysis of relevant data to inform clinical practice. This paper reports on findings from the analysis of data of patients who died by suicide in the period 2013-2016 and made use of both structured data and free-text medical notes. We focus on examining various text-mining approaches to support risk assessment. The results show that using advance machine learning and text-mining techniques, it is possible to predict within a specified period which people are most at risk of taking their own life at the time of referral to a mental health service.",
        "link": "https://dl.acm.org/doi/10.1145/3200947.3201020"
    },
    {
        "title": "Integration of data mining techniques in e-learning systems: Clustering Profil of Lerners and Recommender Course System",
        "authors": [
            "A. El Moustamid",
            "E. En-Naimi",
            "J. El Bouhdidi"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "Data mining aims at the extraction of knowledge from large quantities of data by automatic or semi-automatic methods. In the field of education and training e-learning, Data Mining techniques as clustering (The aim of this technique is to identify groups that are similar in some aspect) [21], prediction (is used for developing model where combination of data viewed from different aspects can effect single viewed data. Here, is important to watch how predictors variables are effecting predicted variable)[21], classification are applied in e-learning to improve the level of learners by providing learning according to the learners' requirement) are used to analyze learners profile, to predict and improve student performance. The aim of our project is to develop a system capable of analyzing learners profile and indexing web videos in order to offer learners a rich database with courses that correspond to their levels.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090453"
    },
    {
        "title": "Multi-level mining and visualization of scientific text collections: Exploring a bi-lingual scientific repository",
        "authors": [
            "Pablo Accuosto",
            "Francesco Ronzano",
            "Daniel Ferrés",
            "Horacio Saggion"
        ],
        "date": "December 2017",
        "source": "WOSP 2017: Proceedings of the 6th International Workshop on Mining Scientific Publications",
        "abstract": "We present a system to mine and visualize collections of scientific documents by semantically browsing information extracted from single publications or aggregated throughout corpora of articles. The text mining tool performs deep analysis of document collections allowing the extraction and interpretation of research paper's contents. In addition to the extraction and enrichment of documents with metadata (titles, authors, affiliations, etc), the deep analysis performed comprises semantic interpretation, rhetorical analysis of sentences, triple-based information extraction, and text summarization. The visualization components allow geographical-based exploration of collections, topic-evolution interpretation, and collaborative network analysis among others. The paper presents a case study of a bi-lingual collection in the field of Natural Language Processing (NLP).",
        "link": "https://dl.acm.org/doi/10.1145/3127526.3127529"
    },
    {
        "title": "Hierarchical Topic Mining via Joint Spherical Tree and Text Embedding",
        "authors": [
            "Yu Meng",
            "Yunyi Zhang",
            "Jiaxin Huang",
            "Yu Zhang",
            "Chao Zhang",
            "Jiawei Han"
        ],
        "date": "August 2020",
        "source": "KDD '20: Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Mining a set of meaningful topics organized into a hierarchy is intuitively appealing since topic correlations are ubiquitous in massive text corpora. To account for potential hierarchical topic structures, hierarchical topic models generalize flat topic models by incorporating latent topic hierarchies into their generative modeling process. However, due to their purely unsupervised nature, the learned topic hierarchy often deviates from users' particular needs or interests. To guide the hierarchical topic discovery process with minimal user supervision, we propose a new task, Hierarchical Topic Mining, which takes a category tree described by category names only, and aims to mine a set of representative terms for each category from a text corpus to help a user comprehend his/her interested topics. We develop a novel joint tree and text embedding method along with a principled optimization procedure that allows simultaneous modeling of the category tree structure and the corpus generative process in the spherical space for effective category-representative term discovery. Our comprehensive experiments show that our model, named JoSH, mines a high-quality set of hierarchical topics with high efficiency and benefits weakly-supervised hierarchical text classification tasks.",
        "link": "https://dl.acm.org/doi/10.1145/3394486.3403242"
    },
    {
        "title": "Unsupervised behavioural mining and clustering for malware family identification",
        "authors": [
            "Khanh Huu The Dam",
            "Thomas Given-Wilson",
            "Axel Legay"
        ],
        "date": "March 2021",
        "source": "SAC '21: Proceedings of the 36th Annual ACM Symposium on Applied Computing",
        "abstract": "More accurate and advanced detection and classification of malware requires exploiting program behaviour and not merely syntactic or static features. One approach is to use system call dependency graphs (SCDGs) that represent the program behaviour by interactions with the system, and the relations between these interactions. These SCDGs have been used with supervised learning techniques to very accurately detect and classify malware. This works considers the unsupervised learning challenge of mining for common clusters of behaviour without a priori knowledge. This allows for clustering of similar programs by behaviour, that can then be used for either classification or further analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3412841.3441919"
    },
    {
        "title": "Text Mining for Evaluating Authors' Birth and Death Years",
        "authors": [
            "Dror Moghaz",
            "Yaakov Hacohen-Kerner",
            "Dov Gabbay"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "This article presents a unique method in text and data mining for finding the era, i.e., mining temporal data, in which an anonymous author was living. Finding this era can assist in the examination of a fake document or extracting the time period in which a writer lived. The study and the experiments concern Hebrew, and in some parts, Aramaic and Yiddish rabbinic texts. The rabbinic texts are undated and contain no bibliographic sections, posing an interesting challenge. This work proposes algorithms using key phrases and key words that allow the temporal organization of citations together with linguistic patterns. Based on these key phrases, key words, and the references, we established several types of “Iron-clad,” Heuristic and Greedy rules for estimating the years of birth and death of a writer in an interesting classification task. Experiments were conducted on corpora, including documents authored by 12, 24, and 36 rabbinic writers and demonstrated promising results.",
        "link": "https://dl.acm.org/doi/10.1145/3281631"
    },
    {
        "title": "Transfer Learning to Timed Text Based Video Classification Using CNN",
        "authors": [
            "Zenun Kastrati",
            "Ali Shariq Imran",
            "Arianit Kurti"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "Open educational video resources are gaining popularity with a growing number of massive open online courses (MOOCs). This has created a niche for content providers to adopt effective solutions in automatically organizing and structuring of educational resources for maximum visibility. Recent advances in deep learning techniques are proving useful in managing and classifying resources into appropriate categories. This paper proposes one such convolutional neural network (CNN) model for classifying video lectures in a MOOC setting using a transfer learning approach. The model uses a time-aligned text transcripts corresponding to video lectures from six broader subject categories. Video lectures and their corresponding transcript dataset is gathered from the Coursera MOOC platform. Two different CNN models are proposed: i) CNN based classification using embeddings learned from our MOOC dataset, ii) CNN based classification using transfer learning. Word embeddings generated from two well known state-of-the-art pre-trained models Word2Vec and GloVe, are used in the transfer learning approach for the second case. The proposed CNN models are evaluated using precision, recall, and F1 score and the obtained performance is compared with both conventional and deep learning classifiers. The proposed CNN models have an F1 score improvement of 10-22 percentage points over DNN and conventional classifiers",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326483"
    },
    {
        "title": "Evolutionary role mining in complex networks by ensemble clustering",
        "authors": [
            "Sarvenaz Choobdar",
            "Pedro Ribeiro",
            "Fernando Silva"
        ],
        "date": "April 2017",
        "source": "SAC '17: Proceedings of the Symposium on Applied Computing",
        "abstract": "The structural patterns in the neighborhood of nodes assign unique roles to the nodes. Mining the set of existing roles in a network provides a descriptive profile of the network and draws its general picture. This paper proposes a new method to determine structural roles in a dynamic network based on the current position of nodes and their historic behavior. We develop a temporal ensemble clustering technique to dynamically find groups of nodes, holding similar tempo-structural roles. We compare two weighting functions, based on age and distribution of data, to incorporate temporal behavior of nodes in the role discovery. To evaluate the performance of the proposed method, we assess the results from two points of view: 1) goodness of fit to current structure of the network; 2) consistency with historic data. We conduct the evaluation using different ensemble clustering techniques. The results on real world networks demonstrate that our method can detect tempo-structural roles that simultaneously depict the topology of a network and reflect its dynamics with high accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3019612.3019815"
    },
    {
        "title": "Analyzing used-car web listings via text mining",
        "authors": [
            "Ayhan Demiriz",
            "Fatma Cantaş"
        ],
        "date": "October 2017",
        "source": "IML '17: Proceedings of the 1st International Conference on Internet of Things and Machine Learning",
        "abstract": "Used car trade is one of the major components of the world economies. It is not uncommon to sell a car by placing an internet advertisement irrespective of the geography in these days. A typical content of an advertisement is usually composed of two parts namely the structured and the free text data. The structured data may include some information about the asking price, make, model, year, mileage of the car and the contact info. In most cases, seller may give important clues about the car's current conditions in the free text data where the title (head) of the advertisement can be included as free text too. This paper reports preliminary results from a text mining study conducted on 75K used car internet listings collected from two major car listing web sites in Turkey. As expected, the words and the phrases related to the description of the car are observed to be frequent. The leading concepts in the free text are found to be regarding how to describe the current condition of a car, for example \"no crash history\".",
        "link": "https://dl.acm.org/doi/10.1145/3109761.3109782"
    },
    {
        "title": "Text classification of Diseases Treated by Traditional Chinese Medicine Prescription based on machine learning",
        "authors": [
            "Hanqing Zhao",
            "Bo Han",
            "Chen Li"
        ],
        "date": "September 2020",
        "source": "ISAIMS '20: Proceedings of the 1st International Symposium on Artificial Intelligence in Medical Sciences",
        "abstract": "OBJECTIVE: To explore the application of machine learning in th e identification of diseases treated by traditional Chinese medicine prescriptions. METHODS: Based on the composition of the text document of Chinese medicine prescriptions, the prescriptions were divided into cough, headache and diarrhea. THUTCT were introduced to establish and train two machine learning text classification models, LibSVM and LibLinear, and the prescriptions to be deter mined were put into the model for classification and prediction. R ESULTS: The Precision rate of LibSVM model and Liblinear mod el were 0.7283 and 0.6690. Seven prescriptions were classified an d predicted, and the results were in line with expectations. CONCLUSION: THUCTC has good universality for the content of TCM prescriptions, high classification accuracy and fast testing speed, which is suitable for the text classification and discrimination research of TCM prescriptions for diseases.",
        "link": "https://dl.acm.org/doi/10.1145/3429889.3429895"
    },
    {
        "title": "Evaluation of retweet clustering method classification method using retweets on Twitter without text data",
        "authors": [
            "K. Uchida",
            "F. Toriumi",
            "T. Sakaki"
        ],
        "date": "August 2017",
        "source": "WI '17: Proceedings of the International Conference on Web Intelligence",
        "abstract": "Burst phenomena, which frequently occur on social media, are caused by such social events as flaming on the internet, elections, and natural disasters. To understand people's thoughts and feelings, we must classify their opinions from burst phenomena. Therefore, classification methods that categorize tweets are critical. However, since most classification methods focus on text mining, they cannot group tweets by topics because each tweet has poor linguistic similarities. We used a non-text-based classification method proposed by Baba et al. that groups tweets by topics, even if they have poor linguistic similarities, and verified its validity by comparing it with a text-based classification method in two different evaluations: qualitative and quantitative. In the qualitative evaluation part, we did a questionnaire survey and validated the suitability of the topic clusters created using both the non-and text-based methods. Since evaluating the similarity of every pair of tweets in each topic is difficult, we evaluated the similarity between sampled pairs in the survey and acquired more appropriate topic clustering results using the non-text-based method than the text-based method. In the quantitative evaluation part, we focused on the robustness of each method against data reduction. Many approaches analyze social media data, especially because collecting data from social media is comparatively easy. However, since collecting the whole data of burst phenomena is very costly due to the vast amounts of available social media data, robustness against data reduction is an important index to evaluate classification methods. With the non-text-based method, over 55% of the pairs of tweets in the same cluster were also included in the same cluster even when the data were reduced to 10% in all three of our example cases. In this paper, as a source we focus on Twitter, one of the most popular microblogging services. Using clustering to conduct detailed case analyses, we scrutinized three burst cases that include natural disasters and flaming on the internet and found that a non-text-based method more effectively classified tweets in burst phenomena than a text-based method.",
        "link": "https://dl.acm.org/doi/10.1145/3106426.3106451"
    },
    {
        "title": "Using Spark for Text Mining on Large Scale Liver Cancer Literature",
        "authors": [
            "Ming-Yen Lin",
            "Yu-Ju Lin",
            "Sue-Chen Hsueh"
        ],
        "date": "January 2021",
        "source": "BDET 2021: 2021 the 3rd International Conference on Big Data Engineering and Technology (BDET)",
        "abstract": "Cancer is one of the main causes of death. The number of known cancer to-date is more than one hundred. Liver cancer, ranked after trachea and lung cancer, has long been high in the cancer leading cause in Taiwan and even ranked as second in 2016. The number of scientific articles related to cancer proliferates every year. The number reaches as high as 20 million in PubMed so that discovering useful information from the massive collection is very difficult. In addition, using a single machine to sift through these articles is very time-consuming. Therefore, we present a big data analytic framework using the distributed Apache Spark platform for text mining in PubMed literature. We establish a prediction model for liver cancer articles so that researchers may effectively validate whether an article is related to liver cancer or not. Classification models in Spark MLlib including Linear Support Vector Machines (SVM), Logistic Regression, are used in our experiments. Relevancy to liver cancer is further confirmed by using MeSH (Medical Subject Headings) terms. Logistic regression is about 3 times faster than SVMs and the accuracy of both methods is close to 95% in the experiments using hold-out validation. When max_features is 500 and min_df ≤ 0.1 (or min_df = 1), the accuracy may reach 96%. In the experiments with K-fold cross-validation, the accuracy of SVMs methods is 96%. The experimental results show that that our prediction model may effectively classify liver cancer articles.",
        "link": "https://dl.acm.org/doi/10.1145/3474944.3474958"
    },
    {
        "title": "Automatic topic classification of test cases using text mining at an Android smartphone vendor",
        "authors": [
            "Junji Shimagaki",
            "Yasutaka Kamei",
            "Naoyasu Ubayashi",
            "Abram Hindle"
        ],
        "date": "October 2018",
        "source": "ESEM '18: Proceedings of the 12th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Background: An Android smartphone is an ecosystem of applications, drivers, operating system components, and assets. The volume of the software is large and the number of test cases needed to cover the functionality of an Android system is substantial. Enormous effort has been already taken to properly quantify \"what features and apps were tested and verified?\". This insight is provided by dashboards that summarize test coverage and results per feature. One method to achieve this is to manually tag or label test cases with the topic or function they cover, much like function points. At the studied Android smartphone vendor, tests are labelled with manually defined tags, so-called \"feature labels (FLs)\", and the FLs serve to categorize 100s to 1000s test cases into 10 to 50 groups. Aim: Unfortunately for developers, manual assignment of FLs to 1000s of test cases is a time consuming task, leading to inaccurately labeled test cases, which will render the dashboard useless. We created an automated system that suggests tags/labels to the developers for their test cases rather than manual labeling. Method: We use machine learning models to predict and label the functionality tested by 10,000 test cases developed at the company. Results: Through the quantitative experiments, our models achieved acceptable F-1 performance of 0.3 to 0.88. Also through the qualitative studies with expert teams, we showed that the hierarchy and path of tests was a good predictor of a feature's label. Conclusions: We find that this method can reduce tedious manual effort that software developers spent classifying test cases, while providing more accurate classification results.",
        "link": "https://dl.acm.org/doi/10.1145/3239235.3268927"
    },
    {
        "title": "To apply Data Mining for Classification of Crowd sourced Software Requirements",
        "authors": [
            "Soonh Taj",
            "Qasim Arain",
            "Imran Memon",
            "Asma Zubedi"
        ],
        "date": "April 2019",
        "source": "ICSIE '19: Proceedings of the 8th International Conference on Software and Information Engineering",
        "abstract": "Now a day's main focus of developers is to build quality software that works according to customer needs and for this reason it is necessary to gather right requirements as requirement elicitation is the critical step that impacts on the success of software project as misinterpreted requirements leads to the failure of software project. By keeping this in mind a research is carried out on improving requirements elicitation process and automating the process of classifying requirements. In this research, a model is proposed which will help in this scenario for requirements elicitation and requirement classification. This paper presents a model in which crowd sourcing approach is used so that customers, end users, stakeholders, developers and software engineers can make active participation for requirement elicitation process and requirements gathered using crowdsourcing approach are used by model for classification process i.e. classification of requirements into functional and non-functional requirements. For the proof of proposed model a case study is conducted. Results of case study provided the usefulness and efficiency of proposed model for classification of crowd sourced software requirements.",
        "link": "https://dl.acm.org/doi/10.1145/3328833.3328837"
    },
    {
        "title": "Text classification using Fuzzy TF-IDF and Machine Learning Models",
        "authors": [
            "Mariem Bounabi",
            "Karim El Moutaouakil",
            "Khalid Satori"
        ],
        "date": "October 2019",
        "source": "BDIoT'19: Proceedings of the 4th International Conference on Big Data and Internet of Things",
        "abstract": "The representation of the information has an important impact on the text classification task. Several weighting methods were proposed in the literature, and the term frequency-inverse term frequency (TFIDF), the most know on the text treatment field. The FTF-IDF is a vector representation where the components of the TFIDF are presented as inputs to the Fuzzy Inference System (FIS). In this work, we compare several Machin Learning algorithms such as Naïve Bayes and its derivatives, SVM and Random forest classifiers, using the FTF-IDF representation. To improve the quality of the used classifiers, we call sum attribute selection methods. The recognition rate, for the tested systems, is satisfied, where the system based on naïve Bayes classifier, the FTF-IDF weighting terms, and the info gain select attributes method gives 98.7% as accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3372938.3372956"
    },
    {
        "title": "Text Mining in Cybersecurity: A Systematic Literature Review",
        "authors": [
            "Luciano Ignaczak",
            "Guilherme Goldschmidt",
            "Cristiano André Da Costa",
            "Rodrigo Da Rosa Righi"
        ],
        "date": null,
        "source": "ACM Computing Surveys",
        "abstract": "The growth of data volume has changed cybersecurity activities, demanding a higher level of automation. In this new cybersecurity landscape, text mining emerged as an alternative to improve the efficiency of the activities involving unstructured data. This article proposes a Systematic Literature Review (SLR) to present the application of text mining in the cybersecurity domain. Using a systematic protocol, we identified 2,196 studies, out of which 83 were summarized. As a contribution, we propose a taxonomy to demonstrate the different activities in the cybersecurity domain supported by text mining. We also detail the strategies evaluated in the application of text mining tasks and the use of neural networks to support activities involving unstructured data. The work also discusses text classification performance aiming its application in real-world solutions. The SLR also highlights open gaps for future research, such as the analysis of non-English content and the intensification in the usage of neural networks.",
        "link": "https://dl.acm.org/doi/10.1145/3462477"
    },
    {
        "title": "Using Text Mining to Analyze the Financial Patents",
        "authors": [
            "Yung-Feng Lu",
            "Jia-Lang Xu",
            "Mu-Yen Chen"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "With the advancement and development of science and technology, Nowadays society is the development of information technology, cloud computing, mobile Internet, Internet of Things, block chain and information technology.The combination of finance and technology has made financial technology terms gradually emerging.. This research analyzes the patents of financial technology. However, Taiwan's financial technology patents are mostly G06Q-20 (payment plan, architecture or agreement), G06Q-30 (commercial, such as marketing, shopping, payment, auction or e-commerce). G06Q-40 (financial, investment or tax treatment, insurance).The research use text mining technology to analyze and understand Taiwan's trends and developments in financial technology. The research results show that G06Q-20 patents are biased towards information security and mobile payment. G06Q-30 patents favor shopping websites and advertising. G06Q-40 patents are biased towards diversity, such as securities, insurance, finance, and stocks.",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326478"
    },
    {
        "title": "Identifying Online Profiles of Distance Learning Students Using Data Mining Techniques",
        "authors": [
            "Osama Islam",
            "Muazzam Siddiqui",
            "Naif Radi Aljohani"
        ],
        "date": "October 2019",
        "source": "ICDTE '19: Proceedings of the 3rd International Conference on Digital Technology in Education",
        "abstract": "Educational data has grown over the years with the increased use of technology within educational environments. This has led to a huge amount of data being stored in various data sources representing the student, his/her activities, and other aspects relevant to the learning process. To meet this analytical need, Educational Data Mining (EDM) has emerged to assist educational institutions in identifying key benefits such as students at risk, the level of student engagement or predicting student performance. The aim of this research was to explore the various aspects of student interaction data using data mining techniques to identify relevant patterns of behaviors and possible key attributes that have higher degrees of influence on distance learning students. The main findings identified several patterns of user online profiles based on a set of adopted learning strategies, the research proposed a framework for analyzing such interaction data based on R and Hadoop platforms to correlate online profiles with student performance.",
        "link": "https://dl.acm.org/doi/10.1145/3369199.3369249"
    },
    {
        "title": "Extremely Fast Decision Tree Mining for Evolving Data Streams",
        "authors": [
            "Albert Bifet",
            "Jiajin Zhang",
            "Wei Fan",
            "Cheng He",
            "Jianfeng Zhang",
            "Jianfeng Qian",
            "Geoff Holmes",
            "Bernhard Pfahringer"
        ],
        "date": "August 2017",
        "source": "KDD '17: Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
        "abstract": "Nowadays real-time industrial applications are generating a huge amount of data continuously every day. To process these large data streams, we need fast and efficient methodologies and systems. A useful feature desired for data scientists and analysts is to have easy to visualize and understand machine learning models. Decision trees are preferred in many real-time applications for this reason, and also, because combined in an ensemble, they are one of the most powerful methods in machine learning. In this paper, we present a new system called STREAMDM-C++, that implements decision trees for data streams in C++, and that has been used extensively at Huawei. Streaming decision trees adapt to changes on streams, a huge advantage since standard decision trees are built using a snapshot of data, and can not evolve over time. STREAMDM-C++ is easy to extend, and contains more powerful ensemble methods, and a more efficient and easy to use adaptive decision trees. We compare our new implementation with VFML, the current state of the art implementation in C, and show how our new system outperforms VFML in speed using less resources.",
        "link": "https://dl.acm.org/doi/10.1145/3097983.3098139"
    },
    {
        "title": "Data miners' little helper: data transformation activity cues for cluster analysis on document collections",
        "authors": [
            "Tania Cerquitelli",
            "Evelina Di Corso",
            "Francesco Ventura",
            "Silvia Chiusano"
        ],
        "date": "June 2017",
        "source": "WIMS '17: Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "In this paper we propose a new self-learning engine to streamline the analytics process, as it enables analysts to mine massive data repositories with minimal user intervention. In the context of cluster analysis on a collection of documents this new system, named SELF-DATA (SELF-learning DAta TrAnsformation), suggests to the analyst how to configure the whole mining process for a given dataset. SELF-DATA relies on an engine exploring different data weighting schemas (e.g., normalized term frequencies) and data transformation methods (e.g., PCA) before applying the cluster analysis, evaluating and comparing solutions through different quality indices (e.g., weighted Silhouette), and presenting the k-top solutions to the analyst. SELF-DATA will also include a knowledge base storing results of experiments on previously processed datasets, and a classification algorithm trained on the knowledge base content to forecast the best configuration for the whole mining process for an unexplored dataset. The first development of SELF-DATA running on Apache Spark has been validated on 5 collections of documents. Experimental results highlight that TF-IDF and logarithmic entropy are effective to measure item relevance with sparse datasets, and the LSI method outperforms PCA with a large dictionary.",
        "link": "https://dl.acm.org/doi/10.1145/3102254.3102288"
    },
    {
        "title": "Interactive Visual Graph Mining and Learning",
        "authors": [
            "Ryan A. Rossi",
            "Nesreen K. Ahmed",
            "Rong Zhou",
            "Hoda Eldardiry"
        ],
        "date": null,
        "source": "ACM Transactions on Intelligent Systems and Technology",
        "abstract": "This article presents a platform for interactive graph mining and relational machine learning called GraphVis. The platform combines interactive visual representations with state-of-the-art graph mining and relational machine learning techniques to aid in revealing important insights quickly as well as learning an appropriate and highly predictive model for a particular task (e.g., classification, link prediction, discovering the roles of nodes, and finding influential nodes). Visual representations and interaction techniques and tools are developed for simple, fast, and intuitive real-time interactive exploration, mining, and modeling of graph data. In particular, we propose techniques for interactive relational learning (e.g., node/link classification), interactive link prediction and weighting, role discovery and community detection, higher-order network analysis (via graphlets, network motifs), among others. GraphVis also allows for the refinement and tuning of graph mining and relational learning methods for specific application domains and constraints via an end-to-end interactive visual analytic pipeline that learns, infers, and provides rapid interactive visualization with immediate feedback at each change/prediction in real-time. Other key aspects include interactive filtering, querying, ranking, manipulating, exporting, as well as tools for dynamic network analysis and visualization, interactive graph generators (including new block model approaches), and a variety of multi-level network analysis techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3200764"
    },
    {
        "title": "Data Mining and Opinion Mining: A Tool in Educational Context",
        "authors": [
            "Myriam Peñafiel",
            "Stefanie Vásquez",
            "Diego Vásquez",
            "Juan Zaldumbide",
            "Sergio Luján-Mora"
        ],
        "date": "July 2018",
        "source": "ICoMS '18: Proceedings of the 2018 1st International Conference on Mathematics and Statistics",
        "abstract": "The use of the web as a universal communication platform generates large volumes of data (Big data), which in many cases, need to be processed so that they can become useful knowledge in face of the sceptics who have doubts about the credibility of such information. The use of web data that comes from educational contexts needs to be addressed, since that large amount of unstructured information is not being valued, losing valuable information that can be used. To solve this problem, we propose the use of data mining techniques such as sentiment analysis to validate the information that comes from the educational platforms. The objective of this research is to propose a methodology that allows the user to apply sentiment analysis in a simple way, because although some researchers have done it, very few do with data in the educational context. The results obtained prove that the proposal can be used in similar cases.",
        "link": "https://dl.acm.org/doi/10.1145/3274250.3274263"
    },
    {
        "title": "Research Hotspots Mining and Visualized Analysis Based on Linking Cluster and K-Core Decomposition",
        "authors": [
            "Ying Cai",
            "Fang Huang",
            "Shuai Wang",
            "Hao Zhang",
            "Chunxiu Du"
        ],
        "date": "May 2018",
        "source": "ICDPA 2018: Proceedings of the International Conference on Data Processing and Applications",
        "abstract": "In this paper, we focus on research hotspots mining with knowledge semantic features on discipline topic word networks, which are extracted from scientific and technical documents. For the topic networks, a novel method of combining linking community clustering with k-core decomposition is proposed to mine the hotspot community with knowledge hierarchy structure. And as a post-processing, the hierarchical community identification is implemented by k-core decomposition in order to discover the internal connection with knowledge semantic hierarchy in the research hotspot. Finally, the community density and visualization method is utilized to analyze the core and sub- problem in the research hotspot community. In experiments, the topic networks for a single discipline and interdisciplinary research hotspot are taken as examples of analysis, and the results show the proposed approach can effectively realize the objective of mining and analyzing the discipline research hotspot on the academic topic word network.",
        "link": "https://dl.acm.org/doi/10.1145/3224207.3224216"
    },
    {
        "title": "The Application of Data Mining Algorithms in the Construction of Travel Recommendation System",
        "authors": [
            "Chao Lou",
            "Zhaonan Mu",
            "Mengzhu Liu"
        ],
        "date": "January 2021",
        "source": "CONF-CDS 2021: The 2nd International Conference on Computing and Data Science",
        "abstract": "Photos shared by users of online social network often associated with tags, texts, geographic and time information. These data are ideal research resource which can be used by researchers to observe people's interests and behaviors. They can also be used to analyze people's travel behaviors and interests because it can reflect people's travel activities and experiences directly. This project aimed to consolidate people into groups based on their travel preferences of travel season, travel time span, scenery type of travel location and then analyze popular travel places for different people groups. The travel preferences were analyzed from extracted data applying PHP language and Naive Bayes classification approach. According to the analyzed data, people were divided into groups. Then, DBSCAN clustering was used to get popular locations based on the different groups. As a result, 80 thousand photo data of 5 cities have been collected from Flickr by Phython. It has been showed that people's preference of season in most of the mined cities was spring and summer and the preference of time span was short. Moreover, mined people has been divided into 16 groups according to their preference and based on the groups popular locations have been calculated by DBSCAN clustering. Theses locations have been provided in a recommendation website which was built in this project. For the future work, mining Flickr data associated with other travel social network sites could be a good way to build a more effective recommendation system.",
        "link": "https://dl.acm.org/doi/10.1145/3448734.3450477"
    },
    {
        "title": "Compare Machine Learning Models in Text Classification Using Steam User Reviews",
        "authors": [
            "Youchen Miao",
            "Zeyu Jin",
            "Yumeng Zhang",
            "Yuchen Chen",
            "Junren Lai"
        ],
        "date": "November 2021",
        "source": "ICSED '21: Proceedings of the 2021 3rd International Conference on Software Engineering and Development",
        "abstract": "Text Classification and Sentiment Analysis of game reviews are viewed as important parts in not only academic fields but also in game studies. In this paper, with more than 400 thousand game reviews on Steam platform, we preprocess the data using different libraries (sklearn, nltk, and spaCy) and use them as inputs to build three sentiment classification models based on different algorithms (Naive Bayes, SVM, and Random Forest). In contrast to previous studies that only focus on different sentiment analysis models, our paper also highlights the use of different APIs to preprocess the data and their corresponding model performance. The results show that no matter which API we choose, Random Forest models always perform the best. However, in terms of training time, Naive Bayes is the fastest. This work can be used to apply grid search for researchers to automatically find the optimum API before conducting sentiment analysis in the future.",
        "link": "https://dl.acm.org/doi/10.1145/3507473.3507480"
    },
    {
        "title": "Lost in Transduction: Transductive Transfer Learning in Text Classification",
        "authors": [
            "Alejandro Moreo",
            "Andrea Esuli",
            "Fabrizio Sebastiani"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Obtaining high-quality labelled data for training a classifier in a new application domain is often costly. Transfer Learning\n(a.k.a. “Inductive Transfer”) tries to alleviate these costs by transferring, to the “target” domain of interest, knowledge available from a different “source” domain. In transfer learning the lack of labelled information from the target domain is compensated by the availability at training time of a set of unlabelled examples from the target distribution. Transductive Transfer Learning denotes the transfer learning setting in which the only set of target documents that we are interested in classifying is known and available at training time. Although this definition is indeed in line with Vapnik’s original definition of “transduction”, current terminology in the field is confused. In this article, we discuss how the term “transduction” has been misused in the transfer learning literature, and propose a clarification consistent with the original characterization of this term given by Vapnik. We go on to observe that the above terminology misuse has brought about misleading experimental comparisons, with inductive transfer learning methods that have been incorrectly compared with transductive transfer learning methods. We then, give empirical evidence that the difference in performance between the inductive version and the transductive version of a transfer learning method can indeed be statistically significant (i.e., that knowing at training time the only data one needs to classify indeed gives an advantage). Our clarification allows a reassessment of the field, and of the relative merits of the major, state-of-the-art algorithms for transfer learning in text classification.",
        "link": "https://dl.acm.org/doi/10.1145/3453146"
    },
    {
        "title": "Machine Learning on Mining Potential Adverse Drug Reactions for Pharmacovigilance",
        "authors": [
            "Ke Jia"
        ],
        "date": "July 2021",
        "source": "DSIT 2021: 2021 4th International Conference on Data Science and Information Technology",
        "abstract": "Drugs are related to the lives and health of patients. Mining and discovering potential adverse drug reactions from a large number of unstructured texts play an important role in drug research and pharmacovigilance. It is a hot research issue in the field of biomedicine in recent years. A large number of scholars have discovered a large amount of information on adverse drug reactions from different data sources through effective machine learning methods. This paper reviews the research progress in recent years to provide a reference for future research.",
        "link": "https://dl.acm.org/doi/10.1145/3478905.3478964"
    },
    {
        "title": "TACAM: Topic And Context Aware Argument Mining",
        "authors": [
            "Michael Fromm",
            "Evgeniy Faerman",
            "Thomas Seidl"
        ],
        "date": "October 2019",
        "source": "WI '19: IEEE/WIC/ACM International Conference on Web Intelligence",
        "abstract": "In this work we address the problem of argument search. The purpose of argument search is the distillation of pro and contra arguments for requested topics from large text corpora. In previous works, the usual approach is to use a standard search engine to extract text parts which are relevant to the given topic and subsequently use an argument recognition algorithm to select arguments from them. The main challenge in the argument recognition task, which is also known as argument mining, is that often sentences containing arguments are structurally similar to purely informative sentences without any stance about the topic. In fact, they only differ semantically. Most approaches use topic or search term information only for the first search step and therefore assume that arguments can be classified independently of a topic. We argue that topic information is crucial for argument mining, since the topic defines the semantic context of an argument. Precisely, we propose different models for the classification of arguments, which take information about a topic of an argument into account. Moreover, to enrich the context of a topic and to let models understand the context of the potential argument better, we integrate information from different external sources such as Knowledge Graphs or pre-trained NLP models. Our evaluation shows that considering topic information, especially in connection with external information, provides a significant performance boost for the argument mining task.",
        "link": "https://dl.acm.org/doi/10.1145/3350546.3352506"
    },
    {
        "title": "Text mining for incoming tasks based on the urgency/importance factors and task classification using machine learning tools",
        "authors": [
            "Yasser Ali Alshehri"
        ],
        "date": "March 2020",
        "source": "ICCDA 2020: Proceedings of the 2020 the 4th International Conference on Compute and Data Analysis",
        "abstract": "In workplaces, there is a massive amount of unstructured data from different sources. In this paper, we present a case study that explains how can through communications between employees, we can help to prioritize tasks requests to increase the efficiency of their works for both technical and non-technical workers. This involves managing daily incoming tasks based on their level of urgency and importance.To allow all workers to utilize the urgency-importance matrix as a time-management tool, we need to automate this tool. The textual content of incoming tasks are analyzed, and metrics related to urgency and importance are extracted. A third factor (i.e., the response variable) is defined based on the two input variables (urgency and importance). Then, machine learning applied to the data to predict the class of incoming tasks based on data outcome desired. We used ordinal regression, neural networks, and decision tree algorithms to predict the four levels of task priority. We measure the performance of all using recalls, precisions, and F-scores. All classifiers perform higher than 89% in terms of all measures.",
        "link": "https://dl.acm.org/doi/10.1145/3388142.3388153"
    },
    {
        "title": "Text mining for malware classification using multivariate all repeated patterns detection",
        "authors": [
            "Konstantinos F. Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2019",
        "source": "ASONAM '19: Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Mobile phones have become nowadays a commodity to the majority of people. Using them, people are able to access the world of Internet and connect with their friends, their colleagues at work or even unknown people with common interests. This proliferation of the mobile devices has also been seen as an opportunity for the cyber criminals to deceive smartphone users and steel their money directly or indirectly, respectively, by accessing their bank accounts through the smartphones or by blackmailing them or selling their private data such as photos, credit card data, etc. to third parties. This is usually achieved by installing malware to smartphones masking their malevolent payload as a legitimate application and advertise it to the users with the hope that mobile users will install it in their devices. Thus, any existing application can easily be modified by integrating a malware and then presented it as a legitimate one. In response to this, scientists have proposed a number of malware detection and classification methods using a variety of techniques. Even though, several of them achieve relatively high precision in malware classification, there is still space for improvement. In this paper, we propose a text mining all repeated pattern detection method which uses the decompiled files of an application in order to classify a suspicious application into one of the known malware families. Based on the experimental results using a real malware dataset, the methodology tries to correctly classify (without any misclassification) all randomly selected malware applications of 3 categories with 3 different families each.",
        "link": "https://dl.acm.org/doi/10.1145/3341161.3350841"
    },
    {
        "title": "Comparative Study between Traditional Machine Learning and Deep Learning Approaches for Text Classification",
        "authors": [
            "Cannannore Nidhi Kamath",
            "Syed Saqib Bukhari",
            "Andreas Dengel"
        ],
        "date": "August 2018",
        "source": "DocEng '18: Proceedings of the ACM Symposium on Document Engineering 2018",
        "abstract": "In this contemporaneous world, it is an obligation for any organization working with documents to end up with the insipid task of classifying truckload of documents, which is the nascent stage of venturing into the realm of information retrieval and data mining. But classification of such humongous documents into multiple classes, calls for a lot of time and labor. Hence a system which could classify these documents with acceptable accuracy would be of an unfathomable help in document engineering. We have created multiple classifiers for document classification and compared their accuracy on raw and processed data. We have garnered data used in a corporate organization as well as publicly available data for comparison. Data is processed by removing the stop-words and stemming is implemented to produce root words. Multiple traditional machine learning techniques like Naive Bayes, Logistic Regression, Support Vector Machine, Random forest Classifier and Multi-Layer Perceptron are used for classification of documents. Classifiers are applied on raw and processed data separately and their accuracy is noted. Along with this, Deep learning technique such as Convolution Neural Network is also used to classify the data and its accuracy is compared with that of traditional machine learning techniques. We are also exploring hierarchical classifiers for classification of classes and subclasses. The system classifies the data faster and with better accuracy than if done manually. The results are discussed in the results and evaluation section.",
        "link": "https://dl.acm.org/doi/10.1145/3209280.3209526"
    },
    {
        "title": "Identification of Overpricing in the Purchase of Medication by the Federal Government of Brazil, Using Text Mining and Clustering Based on Ontology",
        "authors": [
            "Marco Aurelio O. S. Correa",
            "Adriano Galindo Leal"
        ],
        "date": "August 2018",
        "source": "ICCBDC '18: Proceedings of the 2018 2nd International Conference on Cloud and Big Data Computing",
        "abstract": "Increasing the transparency level in his actions and spending is one of the primary duties of the Brazilian Federal Government. The creation of laws that oblige full disclose of all its expenditures through transparency portals enables citizens to supervise all government entities. However, only the dissemination of these data, without a definite standard or the availability of data analysis tools, does not guarantee that the citizen is empowered to play his role. Therefore hence, the objective of this work is to identify overprice in the acquisition of products purchased by the federal government of Brazil using the unstructured data available on the Transparency Portal. The last two-years' worth of purchasing data, available in the Transparency Portal, were extracted, processed and stored. Due to his diverse nature and high volume of data, this study focused only on medicines purchased by the Ministry of Health. Ontology-based text mining and clustering techniques were applied for automatic identification and classification of products. The processing of this information was done through text mining and clustering, based on the ontology registered in another database of the Brazilian government. Because of this work, a consolidated price base per medication was created to allow the identification of distortions in prices practised, facilitating the identification of cases that merit further investigation to unravel fraud to the treasury.",
        "link": "https://dl.acm.org/doi/10.1145/3264560.3264569"
    },
    {
        "title": "Computational Estimation by Scientific Data Mining with Classical Methods to Automate Learning Strategies of Scientists",
        "authors": [
            "Aparna S. Varde"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Experimental results are often plotted as 2-dimensional graphical plots (aka graphs) in scientific domains depicting dependent versus independent variables to aid visual analysis of processes. Repeatedly performing laboratory experiments consumes significant time and resources, motivating the need for computational estimation. The goals are to estimate the graph obtained in an experiment given its input conditions, and to estimate the conditions that would lead to a desired graph. Existing estimation approaches often do not meet accuracy and efficiency needs of targeted applications. We develop a computational estimation approach called AutoDomainMine that integrates clustering and classification over complex scientific data in a framework so as to automate classical learning methods of scientists. Knowledge discovered thereby from a database of existing experiments serves as the basis for estimation. Challenges include preserving domain semantics in clustering, finding matching strategies in classification, striking a good balance between elaboration and conciseness while displaying estimation results based on needs of targeted users, and deriving objective measures to capture subjective user interests. These and other challenges are addressed in this work. The AutoDomainMine approach is used to build a computational estimation system, rigorously evaluated with real data in Materials Science. Our evaluation confirms that AutoDomainMine provides desired accuracy and efficiency in computational estimation. It is extendable to other science and engineering domains as proved by adaptation of its sub-processes within fields such as Bioinformatics and Nanotechnology.",
        "link": "https://dl.acm.org/doi/10.1145/3502736"
    },
    {
        "title": "Where is the road for issue reports classification based on text mining?",
        "authors": [
            "Qiang Fan",
            "Yue Yu",
            "Gang Yin",
            "Tao Wang",
            "Huaimin Wang"
        ],
        "date": "November 2017",
        "source": "ESEM '17: Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Currently, open source projects receive various kinds of issues daily, because of the extreme openness of Issue Tracking System (ITS) in GitHub. ITS is a labor-intensive and time-consuming task of issue categorization for project managers. However, a contributor is only required a short textual abstract to report an issue in GitHub. Thus, most traditional classification approaches based on detailed and structured data (e.g., priority, severity, software version and so on) are difficult to adopt. In this paper, issue classification approaches on a large-scale dataset, including 80 popular projects and over 252,000 issue reports collected from GitHub, were investigated. First, four traditional text-based classification methods and their performances were discussed. Semantic perplexity (i.e., an issues description confuses bug-related sentences with nonbug-related sentences) is a crucial factor that affects the classification performances based on quantitative and qualitative study. Finally, A two-stage classifier framework based on the novel metrics of semantic perplexity of issue reports was designed. Results show that our two-stage classification can significantly improve issue classification performances.",
        "link": "https://dl.acm.org/doi/10.1109/ESEM.2017.19"
    },
    {
        "title": "Feature-based Facebook reviews process model for e-management using data mining",
        "authors": [
            "Anish Kumar Varudharajulu",
            "Yongsheng Ma"
        ],
        "date": "January 2019",
        "source": "IC4E '19: Proceedings of the 10th International Conference on E-Education, E-Business, E-Management and E-Learning",
        "abstract": "The data generated from online communication acts as potential gold mines for discovering knowledge for researchers. A large amount of data is also generated in the form of web documents, emails, blogs, and feedback, etc. Text analytics is being significantly employed to mine important information. Opinion mining is the process of extracting human thoughts and perceptions from unstructured texts. The showstopper for designing an opinion mining system for analyzing reviews arise from the fact that customer reviews are often noisy. These reviews are informally written. In addition, they are subjected to spelling mistakes, grammatical errors, improper punctuation and irrational capitalization. This paper focuses on analyzing the different classification and clustering algorithms aimed at extracting and consolidating opinions of customers from social media sites like Facebook, Twitter and through surveys, at multiple levels of granularity to monitor and measure customer satisfaction. Ours is an automated approach, in which the system aids in the process of knowledge assimilation for knowledge-based building and also performs the analytics. Domain experts ratify the knowledge base and also provide training datasets for the system to intuitively gather more instances for ratification. The system identifies opinion expressions as phrases containing opinion words, opinionated features and also opinion modifiers. These expressions are categorized as positive, negative or neutral. Opinion expressions are identified and categorized using localized linguistic techniques. Opinions can be congregated at any desired level of specificity i.e. feature level or product level, user level or service level, etc. We have developed a system based on this approach, which provides the user with a platform to analyze opinion expressions crawled from a set of pre-defined datasets.",
        "link": "https://dl.acm.org/doi/10.1145/3306500.3306514"
    },
    {
        "title": "Discriminative Topic Mining via Category-Name Guided Text Embedding",
        "authors": [
            "Yu Meng",
            "Jiaxin Huang",
            "Guangyuan Wang",
            "Zihan Wang",
            "Chao Zhang",
            "Yu Zhang",
            "Jiawei Han"
        ],
        "date": "April 2020",
        "source": "WWW '20: Proceedings of The Web Conference 2020",
        "abstract": "Mining a set of meaningful and distinctive topics automatically from massive text corpora has broad applications. Existing topic models, however, typically work in a purely unsupervised way, which often generate topics that do not fit users’ particular needs and yield suboptimal performance on downstream tasks. We propose a new task, discriminative topic mining, which leverages a set of user-provided category names to mine discriminative topics from text corpora. This new task not only helps a user understand clearly and distinctively the topics he/she is most interested in, but also benefits directly keyword-driven classification tasks. We develop CatE, a novel category-name guided text embedding method for discriminative topic mining, which effectively leverages minimal user guidance to learn a discriminative embedding space and discover category representative terms in an iterative manner. We conduct a comprehensive set of experiments to show that CatE mines high-quality set of topics guided by category names only, and benefits a variety of downstream applications including weakly-supervised classification and lexical entailment direction identification.",
        "link": "https://dl.acm.org/doi/10.1145/3366423.3380278"
    },
    {
        "title": "Comparative Study of Heart Disease Diagnosis Using Top Ten Data Mining Classification Algorithms",
        "authors": [
            "I. Ketut Agung Enriko"
        ],
        "date": "June 2019",
        "source": "ICFET '19: Proceedings of the 5th International Conference on Frontiers of Educational Technologies",
        "abstract": "Data mining has been used for many purposes, especially for prediction system. In healthcare, data mining algorithms often used in disease diagnosis. Meanwhile, heart disease is known as a primary cause of death over the years. Many studies have been performed in heart disease diagnosis using data mining methods. There are some popular data mining algorithms that can be used in heart disease diagnosis, for example, k-Nearest Neighbor, CART, and AdaBoost. The algorithms are used to analyze a sample of cardiovascular patients data and predict the heart disease type that they suffer. Some parameters are taken from the patient, including EKG morphology, blood pressure, and information about the existence of chest pain, shortness of breath, palpitation, and cold sweat. In this study, medical records data are collected from Harapan Kita Hospital and utilized as a dataset sample in this research. Top ten data mining classification algorithms are used in diagnosing heart disease from Harapan Kita Hospital data and examining their performance by checking the accuracy and speed.",
        "link": "https://dl.acm.org/doi/10.1145/3338188.3338220"
    },
    {
        "title": "Text Mining Approach for Identifying Research Trends",
        "authors": [
            "Snezhana Sulova"
        ],
        "date": "June 2021",
        "source": "CompSysTech '21: Proceedings of the 22nd International Conference on Computer Systems and Technologies",
        "abstract": "With the increase of unstructured data, the issues connected with automatic text processing, the categorization of documents and the discovery of topics have become objects of growing interest. In order to improve the process of grouping and processing research publications, we would like to propose a method based upon natural language processing. It is based on text mining technologies which aim to identify key tendencies in documents. It processes the content of publications by clustering and identifies the topics of each identified group. This analysis helps by identifying key tendencies as well as discovering emerging new areas of research. Publications from the research literature database, Scopus, were used to test the approach. The topic of the publications is “the application of digital technologies in the logistics business”. The experiments were completed using the RapidMiner Studio software.",
        "link": "https://dl.acm.org/doi/10.1145/3472410.3472433"
    },
    {
        "title": "Assessment of Vulnerability Severity using Text Mining",
        "authors": [
            "Georgios Spanos",
            "Lefteris Angelis",
            "Dimitrios Toloudis"
        ],
        "date": "September 2017",
        "source": "PCI '17: Proceedings of the 21st Pan-Hellenic Conference on Informatics",
        "abstract": "Software1 vulnerabilities are closely associated with information systems security, a major and critical field in today's technology. Vulnerabilities constitute a constant and increasing threat for various aspects of everyday life, especially for safety and economy, since the social impact from the problems that they cause is complicated and often unpredictable. Although there is an entire research branch in software engineering that deals with the identification and elimination of vulnerabilities, the growing complexity of software products and the variability of software production procedures are factors contributing to the ongoing occurrence of vulnerabilities, Hence, another area that is being developed in parallel focuses on the study and management of the vulnerabilities that have already been reported and registered in databases. The information contained in such databases includes, a textual description and a number of metrics related to vulnerabilities. The purpose of this paper is to investigate to what extend the assessment of the vulnerability severity can be inferred directly from the corresponding textual description, or in other words, to examine the informative power of the description with respect to the vulnerability severity. For this purpose, text mining techniques, i.e. text analysis and three different classification methods (decision trees, neural networks and support vector machines) were employed. The application of text mining to a sample of 70,678 vulnerabilities from a public data source shows that the description itself is a reliable and highly accurate source of information for vulnerability prioritization.",
        "link": "https://dl.acm.org/doi/10.1145/3139367.3139390"
    },
    {
        "title": "Clustering Algorithms for Spatial Data Mining",
        "authors": [
            "Chetashri Bhadane",
            "Ketan Shah"
        ],
        "date": "April 2020",
        "source": "ICGDA '20: Proceedings of the 2020 3rd International Conference on Geoinformatics and Data Analysis",
        "abstract": "With the advances in mobile and wireless technologies, there has been a rise in applications that track and share the users' geospatial data. People use several social networking sites such as Twitter, Facebook and Flickr, where they share their status updates. With the integration of Global Positioning System (GPS) with mobile phones, it is now possible to share one's locations on these social networks. GPS allows us to record and track a person's movement along with the timestamp. The data set obtained from these GPS logs is vast and is widely used to analyze the users' movement patterns. Specifically, we can find out significant locations based on the number of users present at that location and the time spent by them at such places. Once significant places have been identified, it is also possible to identify the semantic importance of these locations. This paper presents an overview of the clustering techniques used to find important places of interest using large GPS based mobility datasets. Four clustering algorithms, K-Means, DBSCAN, OPTICS and Hierarchical, are implemented, and performance is tested using real-time data of 50 users collected over 2--5 years. Performance summary depicts that K-Means and DBSCAN perform well for spatial data.",
        "link": "https://dl.acm.org/doi/10.1145/3397056.3397068"
    },
    {
        "title": "Feature Selection based Arabic Text Classification using Different Machine Learning Algorithms: Comparative Study",
        "authors": [
            "Sakina Rim Bennabi",
            "Zakaria Elberrichi"
        ],
        "date": "June 2020",
        "source": "ICIST '20: Proceedings of the 10th International Conference on Information Systems and Technologies",
        "abstract": "Feature selection is a method of data pre-processing widely used when mining large data, such as textual classification. Several studies have been conducted to compare the different methods of feature selection applied to corpora in English. Unfortunately, a small number of works concern the Arabic language. This article aims to present a comparative study of different feature selection techniques including: Chi2, the ANOVA method and mutual information, applied on a corpus in Arabic language, while also diversifying the machine learning algorithms (Naive Bayes, SVM and KNN). This experimental study has shown in general that reducing dimensionality with feature selection techniques has slightly affected the performance of textual classification, reducing the size of the corpus by up to 1%.",
        "link": "https://dl.acm.org/doi/10.1145/3447568.3448531"
    },
    {
        "title": "Intelligent Data Analysis using Optimized Support Vector Machine Based Data Mining Approach for Tourism Industry",
        "authors": [
            "Ms Promila Sharma",
            "Uma Meena",
            "Girish Kumar Sharma"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Data analysis involves the deployment of sophisticated approaches from data mining methods, information theory, and artificial intelligence in various fields like tourism, hospitality, and so on for the extraction of knowledge from the gathered and preprocessed data. In tourism, pattern analysis or data analysis using classification is significant for finding the patterns that represent new and potentially useful information or knowledge about the destination and other data. Several data mining techniques are introduced for the classification of data or patterns. However, overfitting, less accuracy, local minima, sensitive to noise are the drawbacks in some existing data mining classification methods. To overcome these challenges, Support vector machine with Red deer optimization (SVM-RDO) based data mining strategy is proposed in this article. Extended Kalman filter (EKF) is utilized in the first phase, i.e., data cleaning to remove the noise and missing values from the input data. Mantaray foraging algorithm (MaFA) is used in the data selection phase, in which the significant data are selected for the further process to reduce the computational complexity. The final phase is the classification, in which SVM-RDO is proposed to access the useful pattern from the selected data. PYTHON is the implementation tool used for the experiment of the proposed model. The experimental analysis is done to show the efficacy of the proposed work. From the experimental results, the proposed SVM-RDO achieved better accuracy, precision, recall, and F1 score than the existing methods for the tourism dataset. Thus, it is showed the effectiveness of the proposed SVM-RDO for pattern analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3494566"
    },
    {
        "title": "Applying text mining to predict learners' cognitive engagement",
        "authors": [
            "Hayati Hind",
            "Mohammed Khalidi Idrissi",
            "Samir Bennani"
        ],
        "date": "October 2017",
        "source": "SCAMS '17: Proceedings of the Mediterranean Symposium on Smart City Application",
        "abstract": "In academic process, engagement represent one of the important success' factor. This paper proposes a predictive system for learners' cognitive engagement based on their online discussion forums participation. In the first level the ontology OWL and the LSA method are used to perform a semantic classification system of the threads according to a specific context chosen by the tutor. Then the Text mining, as a predictive method, is applied to the classified threads and learners' participation in the forums.",
        "link": "https://dl.acm.org/doi/10.1145/3175628.3175655"
    },
    {
        "title": "Investigate Transitions into Drug Addiction through Text Mining of Reddit Data",
        "authors": [
            "John Lu",
            "Sumati Sridhar",
            "Ritika Pandey",
            "Mohammad Al Hasan",
            "Georege Mohler"
        ],
        "date": "July 2019",
        "source": "KDD '19: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Increasing rates of opioid drug abuse and heightened prevalence of online support communities underscore the necessity of employing data mining techniques to better understand drug addiction using these rapidly developing online resources. In this work, we obtained data from Reddit, an online collection of forums, to gather insight into drug use/misuse using text snippets from users narratives. Specifically, using users' posts, we trained a binary classifier which predicts a user's transitions from casual drug discussion forums to drug recovery forums. We also proposed a Cox regression model that outputs likelihoods of such transitions. In doing so, we found that utterances of select drugs and certain linguistic features contained in one's posts can help predict these transitions. Using unfiltered drug-related posts, our research delineates drugs that are associated with higher rates of transitions from recreational drug discussion to support/recovery discussion, offers insight into modern drug culture, and provides tools with potential applications in combating the opioid crisis.",
        "link": "https://dl.acm.org/doi/10.1145/3292500.3330737"
    },
    {
        "title": "Analysing digital banking reviews using text mining",
        "authors": [
            "Li Chen Cheng",
            "Legaspi Rhea Sharmayne"
        ],
        "date": "December 2020",
        "source": "ASONAM '20: Proceedings of the 12th IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Digital banks are new entrants in the banking industry in the Philippines as they only started late 2018. Since then, a handful of players have and are still emerging. With more and more people becoming technologically savvy, it is very critical for financial institutions to develop a digital banking application that will stand out from the competition. This paper aims to use text mining methods to analyse digital banking application reviews. This study will perform topic modelling using LDA to explore customer concerns and will mine association rules between the digital banking features with the review score. The results will reveal which areas the digital banking application can further optimize for customer satisfaction and retention.",
        "link": "https://dl.acm.org/doi/10.1109/ASONAM49781.2020.9381429"
    },
    {
        "title": "Mining Moocs Videos Metadata Using Classification Techniques",
        "authors": [
            "El Harrak Othman",
            "Ghadi Abderrahim",
            "El Bouhdidi Jaber"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "In few years, the internet has become the first source of information for most people, Today MOOCs makes it possible for everyone to access to the education over the world, it's represents an emerging methodology of online teaching and an important development in open education. Due to the rapid development of Moocs and the rapid growth of digital data and video database over the Internet, it is becoming very difficult for learners to browsing and choosing the best training for them. The scientific community has increased the amount of research into new technologies, with a view to improve the digital video utilization: its archiving, indexing, accessibility, acquisition, store and even its process and usability. All these parts of the video utilization entail the necessity of the extraction of all important information of a video, especially in cases of lack of metadata information. Web video mining is retrieving the content using data mining techniques from World Wide Web. There are two approaches for web video mining using traditional image processing (signal processing) and metadata based approach. In this paper, we present the various research makes in this subject and we propose an effective methodology to extract the metadata from Moocs videos and classify them based on the extracted metadata by applying data mining techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090450"
    },
    {
        "title": "Text mining with HathiTrust",
        "authors": [
            "Eleanor Dickson Koehl",
            "Ryan Dubnicek"
        ],
        "date": "June 2019",
        "source": "JCDL '19: Proceedings of the 18th Joint Conference on Digital Libraries",
        "abstract": "This tutorial will introduce attendees to the HathiTrust Research Center and its tools and services for computational text analysis research. HTRC leverages the scope and scale of the HathiTrust Digital Library collection to create opportunities for researchers to perform text data mining on subsets of the corpus. Attendees will develop skills that will allow them to conduct basic text analysis research using HathiTrust data, learn techniques for engaging with scholars on their text data mining research, and explore strategies for computational access to large-scale digital collections.",
        "link": "https://dl.acm.org/doi/10.1109/JCDL.2019.00115"
    },
    {
        "title": "Text mining for plagiarism detection: multivariate pattern detection for recognition of text similarities",
        "authors": [
            "Konstantinos Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2018",
        "source": "ASONAM '18: Proceedings of the 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "The problem of plagiarism the recent years has been intensified by the availability of information in digital form and the accessibility of the electronic libraries through the Internet. As a result, plagiarism detection has been transformed into a big data analytics problem since the number of digital sources is extravagant and a new document needs to be compared with millions of other existing documents. In this paper, a text mining methodology is proposed that can detect all common patterns between a document and the documents in a reference database. The technique is based on a pattern detection algorithm and the corresponding data structure that enables the algorithm to detect all common patterns. The methodology has been applied in a well-defined dataset providing very promising results identifying difficult cases of plagiarism such as technical disguise.",
        "link": "https://dl.acm.org/doi/10.5555/3382225.3382424"
    },
    {
        "title": "Petroleum Engineering Data Text Classification Using Convolutional Neural Network Based Classifier",
        "authors": [
            "Tzuhan Hsu",
            "Yaoqin Zhang"
        ],
        "date": "May 2018",
        "source": "ICMLT '18: Proceedings of the 2018 International Conference on Machine Learning Technologies",
        "abstract": "In this paper, authors introduce a convolutional neural network (CNN) based classification approach for petroleum engineering event. Recently, text classification problem has raised substantial attention in the field of machine learning. The aim of text classification process is to assign correct label to a text on the basis of its content. However, due to the imprecise of feature representation, most of former text classification methods cannot export satisfying result. To solve this problem, authors propose a model based on convolutional neural network. Authors conduct experiments with real world petroleum engineering data text, artificially labelled into 6 categories. Beside purposed CNN-based model, the authors also implement other baseline machine learning approaches: k nearest neighbor, random forest, support vector machines, recurrent neural network, gated recurrent units, long short-term memory. Among all these models, our CNN-based model reach best performance in text classification task. In the last part of this paper, authors compare and analysis their purpose and baseline model.",
        "link": "https://dl.acm.org/doi/10.1145/3231884.3231898"
    },
    {
        "title": "Classifying Indonesian Online Articles as Advertisement Placement Base Using Text Mining",
        "authors": [
            "Nadhira Tasya",
            "Arian Dhini"
        ],
        "date": "July 2017",
        "source": "ICBIM 2017: Proceedings of the International Conference on Business and Information Management",
        "abstract": "Rapid development in technological aspect resulting in growing level of human needs for the latest news, so that emerged a new trend of publishing and accessing news through online media or usually called online journalism. In addition, the number of people who sell and purchase through online sites also continues to increase and this opportunity is utilized by the company and the advertiser by implementing targeted web advertising. However, the high number of articles that have been published and accessed leads to great opportunities for errors in determining where to place the ads. Therefore, it needs a system that can categorize articles accessed by users as the basis of advertisement placement by the company and this classification system can be done by applying the method of Data Mining and Text Mining. This research uses document data in the form of article content that will be categorized into twenty categories of class of advertisement by using Text Mining technique with Support Vector Machine algorithm. The results of this study may be used by companies or advertisers as a basis for placement of ads on selected online media sites.",
        "link": "https://dl.acm.org/doi/10.1145/3134271.3134288"
    },
    {
        "title": "Mining Free-Text Medical Notes for Suicide Risk Assessment",
        "authors": [
            "Marios Adamou",
            "Grigoris Antoniou",
            "Elissavet Greasidou",
            "Vincenzo Lagani",
            "Paulos Charonyktakis",
            "Ioannis Tsamardinos"
        ],
        "date": "July 2018",
        "source": "SETN '18: Proceedings of the 10th Hellenic Conference on Artificial Intelligence",
        "abstract": "Suicide has been considered as an important public health issue for a very long time, and is one of the main causes of death worldwide. Despite suicide prevention strategies being applied, the rate of suicide has not changed substantially over the past decades. Advances in machine learning make it possible to attempt to predict suicide based on the analysis of relevant data to inform clinical practice. This paper reports on findings from the analysis of data of patients who died by suicide in the period 2013-2016 and made use of both structured data and free-text medical notes. We focus on examining various text-mining approaches to support risk assessment. The results show that using advance machine learning and text-mining techniques, it is possible to predict within a specified period which people are most at risk of taking their own life at the time of referral to a mental health service.",
        "link": "https://dl.acm.org/doi/10.1145/3200947.3201020"
    },
    {
        "title": "Integration of data mining techniques in e-learning systems: Clustering Profil of Lerners and Recommender Course System",
        "authors": [
            "A. El Moustamid",
            "E. En-Naimi",
            "J. El Bouhdidi"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "Data mining aims at the extraction of knowledge from large quantities of data by automatic or semi-automatic methods. In the field of education and training e-learning, Data Mining techniques as clustering (The aim of this technique is to identify groups that are similar in some aspect) [21], prediction (is used for developing model where combination of data viewed from different aspects can effect single viewed data. Here, is important to watch how predictors variables are effecting predicted variable)[21], classification are applied in e-learning to improve the level of learners by providing learning according to the learners' requirement) are used to analyze learners profile, to predict and improve student performance. The aim of our project is to develop a system capable of analyzing learners profile and indexing web videos in order to offer learners a rich database with courses that correspond to their levels.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090453"
    },
    {
        "title": "Multi-level mining and visualization of scientific text collections: Exploring a bi-lingual scientific repository",
        "authors": [
            "Pablo Accuosto",
            "Francesco Ronzano",
            "Daniel Ferrés",
            "Horacio Saggion"
        ],
        "date": "December 2017",
        "source": "WOSP 2017: Proceedings of the 6th International Workshop on Mining Scientific Publications",
        "abstract": "We present a system to mine and visualize collections of scientific documents by semantically browsing information extracted from single publications or aggregated throughout corpora of articles. The text mining tool performs deep analysis of document collections allowing the extraction and interpretation of research paper's contents. In addition to the extraction and enrichment of documents with metadata (titles, authors, affiliations, etc), the deep analysis performed comprises semantic interpretation, rhetorical analysis of sentences, triple-based information extraction, and text summarization. The visualization components allow geographical-based exploration of collections, topic-evolution interpretation, and collaborative network analysis among others. The paper presents a case study of a bi-lingual collection in the field of Natural Language Processing (NLP).",
        "link": "https://dl.acm.org/doi/10.1145/3127526.3127529"
    },
    {
        "title": "Hierarchical Topic Mining via Joint Spherical Tree and Text Embedding",
        "authors": [
            "Yu Meng",
            "Yunyi Zhang",
            "Jiaxin Huang",
            "Yu Zhang",
            "Chao Zhang",
            "Jiawei Han"
        ],
        "date": "August 2020",
        "source": "KDD '20: Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Mining a set of meaningful topics organized into a hierarchy is intuitively appealing since topic correlations are ubiquitous in massive text corpora. To account for potential hierarchical topic structures, hierarchical topic models generalize flat topic models by incorporating latent topic hierarchies into their generative modeling process. However, due to their purely unsupervised nature, the learned topic hierarchy often deviates from users' particular needs or interests. To guide the hierarchical topic discovery process with minimal user supervision, we propose a new task, Hierarchical Topic Mining, which takes a category tree described by category names only, and aims to mine a set of representative terms for each category from a text corpus to help a user comprehend his/her interested topics. We develop a novel joint tree and text embedding method along with a principled optimization procedure that allows simultaneous modeling of the category tree structure and the corpus generative process in the spherical space for effective category-representative term discovery. Our comprehensive experiments show that our model, named JoSH, mines a high-quality set of hierarchical topics with high efficiency and benefits weakly-supervised hierarchical text classification tasks.",
        "link": "https://dl.acm.org/doi/10.1145/3394486.3403242"
    },
    {
        "title": "Unsupervised behavioural mining and clustering for malware family identification",
        "authors": [
            "Khanh Huu The Dam",
            "Thomas Given-Wilson",
            "Axel Legay"
        ],
        "date": "March 2021",
        "source": "SAC '21: Proceedings of the 36th Annual ACM Symposium on Applied Computing",
        "abstract": "More accurate and advanced detection and classification of malware requires exploiting program behaviour and not merely syntactic or static features. One approach is to use system call dependency graphs (SCDGs) that represent the program behaviour by interactions with the system, and the relations between these interactions. These SCDGs have been used with supervised learning techniques to very accurately detect and classify malware. This works considers the unsupervised learning challenge of mining for common clusters of behaviour without a priori knowledge. This allows for clustering of similar programs by behaviour, that can then be used for either classification or further analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3412841.3441919"
    },
    {
        "title": "Text Mining for Evaluating Authors' Birth and Death Years",
        "authors": [
            "Dror Moghaz",
            "Yaakov Hacohen-Kerner",
            "Dov Gabbay"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "This article presents a unique method in text and data mining for finding the era, i.e., mining temporal data, in which an anonymous author was living. Finding this era can assist in the examination of a fake document or extracting the time period in which a writer lived. The study and the experiments concern Hebrew, and in some parts, Aramaic and Yiddish rabbinic texts. The rabbinic texts are undated and contain no bibliographic sections, posing an interesting challenge. This work proposes algorithms using key phrases and key words that allow the temporal organization of citations together with linguistic patterns. Based on these key phrases, key words, and the references, we established several types of “Iron-clad,” Heuristic and Greedy rules for estimating the years of birth and death of a writer in an interesting classification task. Experiments were conducted on corpora, including documents authored by 12, 24, and 36 rabbinic writers and demonstrated promising results.",
        "link": "https://dl.acm.org/doi/10.1145/3281631"
    },
    {
        "title": "Transfer Learning to Timed Text Based Video Classification Using CNN",
        "authors": [
            "Zenun Kastrati",
            "Ali Shariq Imran",
            "Arianit Kurti"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "Open educational video resources are gaining popularity with a growing number of massive open online courses (MOOCs). This has created a niche for content providers to adopt effective solutions in automatically organizing and structuring of educational resources for maximum visibility. Recent advances in deep learning techniques are proving useful in managing and classifying resources into appropriate categories. This paper proposes one such convolutional neural network (CNN) model for classifying video lectures in a MOOC setting using a transfer learning approach. The model uses a time-aligned text transcripts corresponding to video lectures from six broader subject categories. Video lectures and their corresponding transcript dataset is gathered from the Coursera MOOC platform. Two different CNN models are proposed: i) CNN based classification using embeddings learned from our MOOC dataset, ii) CNN based classification using transfer learning. Word embeddings generated from two well known state-of-the-art pre-trained models Word2Vec and GloVe, are used in the transfer learning approach for the second case. The proposed CNN models are evaluated using precision, recall, and F1 score and the obtained performance is compared with both conventional and deep learning classifiers. The proposed CNN models have an F1 score improvement of 10-22 percentage points over DNN and conventional classifiers",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326483"
    },
    {
        "title": "Evolutionary role mining in complex networks by ensemble clustering",
        "authors": [
            "Sarvenaz Choobdar",
            "Pedro Ribeiro",
            "Fernando Silva"
        ],
        "date": "April 2017",
        "source": "SAC '17: Proceedings of the Symposium on Applied Computing",
        "abstract": "The structural patterns in the neighborhood of nodes assign unique roles to the nodes. Mining the set of existing roles in a network provides a descriptive profile of the network and draws its general picture. This paper proposes a new method to determine structural roles in a dynamic network based on the current position of nodes and their historic behavior. We develop a temporal ensemble clustering technique to dynamically find groups of nodes, holding similar tempo-structural roles. We compare two weighting functions, based on age and distribution of data, to incorporate temporal behavior of nodes in the role discovery. To evaluate the performance of the proposed method, we assess the results from two points of view: 1) goodness of fit to current structure of the network; 2) consistency with historic data. We conduct the evaluation using different ensemble clustering techniques. The results on real world networks demonstrate that our method can detect tempo-structural roles that simultaneously depict the topology of a network and reflect its dynamics with high accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3019612.3019815"
    },
    {
        "title": "Analyzing used-car web listings via text mining",
        "authors": [
            "Ayhan Demiriz",
            "Fatma Cantaş"
        ],
        "date": "October 2017",
        "source": "IML '17: Proceedings of the 1st International Conference on Internet of Things and Machine Learning",
        "abstract": "Used car trade is one of the major components of the world economies. It is not uncommon to sell a car by placing an internet advertisement irrespective of the geography in these days. A typical content of an advertisement is usually composed of two parts namely the structured and the free text data. The structured data may include some information about the asking price, make, model, year, mileage of the car and the contact info. In most cases, seller may give important clues about the car's current conditions in the free text data where the title (head) of the advertisement can be included as free text too. This paper reports preliminary results from a text mining study conducted on 75K used car internet listings collected from two major car listing web sites in Turkey. As expected, the words and the phrases related to the description of the car are observed to be frequent. The leading concepts in the free text are found to be regarding how to describe the current condition of a car, for example \"no crash history\".",
        "link": "https://dl.acm.org/doi/10.1145/3109761.3109782"
    },
    {
        "title": "Text classification of Diseases Treated by Traditional Chinese Medicine Prescription based on machine learning",
        "authors": [
            "Hanqing Zhao",
            "Bo Han",
            "Chen Li"
        ],
        "date": "September 2020",
        "source": "ISAIMS '20: Proceedings of the 1st International Symposium on Artificial Intelligence in Medical Sciences",
        "abstract": "OBJECTIVE: To explore the application of machine learning in th e identification of diseases treated by traditional Chinese medicine prescriptions. METHODS: Based on the composition of the text document of Chinese medicine prescriptions, the prescriptions were divided into cough, headache and diarrhea. THUTCT were introduced to establish and train two machine learning text classification models, LibSVM and LibLinear, and the prescriptions to be deter mined were put into the model for classification and prediction. R ESULTS: The Precision rate of LibSVM model and Liblinear mod el were 0.7283 and 0.6690. Seven prescriptions were classified an d predicted, and the results were in line with expectations. CONCLUSION: THUCTC has good universality for the content of TCM prescriptions, high classification accuracy and fast testing speed, which is suitable for the text classification and discrimination research of TCM prescriptions for diseases.",
        "link": "https://dl.acm.org/doi/10.1145/3429889.3429895"
    },
    {
        "title": "Evaluation of retweet clustering method classification method using retweets on Twitter without text data",
        "authors": [
            "K. Uchida",
            "F. Toriumi",
            "T. Sakaki"
        ],
        "date": "August 2017",
        "source": "WI '17: Proceedings of the International Conference on Web Intelligence",
        "abstract": "Burst phenomena, which frequently occur on social media, are caused by such social events as flaming on the internet, elections, and natural disasters. To understand people's thoughts and feelings, we must classify their opinions from burst phenomena. Therefore, classification methods that categorize tweets are critical. However, since most classification methods focus on text mining, they cannot group tweets by topics because each tweet has poor linguistic similarities. We used a non-text-based classification method proposed by Baba et al. that groups tweets by topics, even if they have poor linguistic similarities, and verified its validity by comparing it with a text-based classification method in two different evaluations: qualitative and quantitative. In the qualitative evaluation part, we did a questionnaire survey and validated the suitability of the topic clusters created using both the non-and text-based methods. Since evaluating the similarity of every pair of tweets in each topic is difficult, we evaluated the similarity between sampled pairs in the survey and acquired more appropriate topic clustering results using the non-text-based method than the text-based method. In the quantitative evaluation part, we focused on the robustness of each method against data reduction. Many approaches analyze social media data, especially because collecting data from social media is comparatively easy. However, since collecting the whole data of burst phenomena is very costly due to the vast amounts of available social media data, robustness against data reduction is an important index to evaluate classification methods. With the non-text-based method, over 55% of the pairs of tweets in the same cluster were also included in the same cluster even when the data were reduced to 10% in all three of our example cases. In this paper, as a source we focus on Twitter, one of the most popular microblogging services. Using clustering to conduct detailed case analyses, we scrutinized three burst cases that include natural disasters and flaming on the internet and found that a non-text-based method more effectively classified tweets in burst phenomena than a text-based method.",
        "link": "https://dl.acm.org/doi/10.1145/3106426.3106451"
    },
    {
        "title": "Using Spark for Text Mining on Large Scale Liver Cancer Literature",
        "authors": [
            "Ming-Yen Lin",
            "Yu-Ju Lin",
            "Sue-Chen Hsueh"
        ],
        "date": "January 2021",
        "source": "BDET 2021: 2021 the 3rd International Conference on Big Data Engineering and Technology (BDET)",
        "abstract": "Cancer is one of the main causes of death. The number of known cancer to-date is more than one hundred. Liver cancer, ranked after trachea and lung cancer, has long been high in the cancer leading cause in Taiwan and even ranked as second in 2016. The number of scientific articles related to cancer proliferates every year. The number reaches as high as 20 million in PubMed so that discovering useful information from the massive collection is very difficult. In addition, using a single machine to sift through these articles is very time-consuming. Therefore, we present a big data analytic framework using the distributed Apache Spark platform for text mining in PubMed literature. We establish a prediction model for liver cancer articles so that researchers may effectively validate whether an article is related to liver cancer or not. Classification models in Spark MLlib including Linear Support Vector Machines (SVM), Logistic Regression, are used in our experiments. Relevancy to liver cancer is further confirmed by using MeSH (Medical Subject Headings) terms. Logistic regression is about 3 times faster than SVMs and the accuracy of both methods is close to 95% in the experiments using hold-out validation. When max_features is 500 and min_df ≤ 0.1 (or min_df = 1), the accuracy may reach 96%. In the experiments with K-fold cross-validation, the accuracy of SVMs methods is 96%. The experimental results show that that our prediction model may effectively classify liver cancer articles.",
        "link": "https://dl.acm.org/doi/10.1145/3474944.3474958"
    },
    {
        "title": "Automatic topic classification of test cases using text mining at an Android smartphone vendor",
        "authors": [
            "Junji Shimagaki",
            "Yasutaka Kamei",
            "Naoyasu Ubayashi",
            "Abram Hindle"
        ],
        "date": "October 2018",
        "source": "ESEM '18: Proceedings of the 12th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Background: An Android smartphone is an ecosystem of applications, drivers, operating system components, and assets. The volume of the software is large and the number of test cases needed to cover the functionality of an Android system is substantial. Enormous effort has been already taken to properly quantify \"what features and apps were tested and verified?\". This insight is provided by dashboards that summarize test coverage and results per feature. One method to achieve this is to manually tag or label test cases with the topic or function they cover, much like function points. At the studied Android smartphone vendor, tests are labelled with manually defined tags, so-called \"feature labels (FLs)\", and the FLs serve to categorize 100s to 1000s test cases into 10 to 50 groups. Aim: Unfortunately for developers, manual assignment of FLs to 1000s of test cases is a time consuming task, leading to inaccurately labeled test cases, which will render the dashboard useless. We created an automated system that suggests tags/labels to the developers for their test cases rather than manual labeling. Method: We use machine learning models to predict and label the functionality tested by 10,000 test cases developed at the company. Results: Through the quantitative experiments, our models achieved acceptable F-1 performance of 0.3 to 0.88. Also through the qualitative studies with expert teams, we showed that the hierarchy and path of tests was a good predictor of a feature's label. Conclusions: We find that this method can reduce tedious manual effort that software developers spent classifying test cases, while providing more accurate classification results.",
        "link": "https://dl.acm.org/doi/10.1145/3239235.3268927"
    },
    {
        "title": "To apply Data Mining for Classification of Crowd sourced Software Requirements",
        "authors": [
            "Soonh Taj",
            "Qasim Arain",
            "Imran Memon",
            "Asma Zubedi"
        ],
        "date": "April 2019",
        "source": "ICSIE '19: Proceedings of the 8th International Conference on Software and Information Engineering",
        "abstract": "Now a day's main focus of developers is to build quality software that works according to customer needs and for this reason it is necessary to gather right requirements as requirement elicitation is the critical step that impacts on the success of software project as misinterpreted requirements leads to the failure of software project. By keeping this in mind a research is carried out on improving requirements elicitation process and automating the process of classifying requirements. In this research, a model is proposed which will help in this scenario for requirements elicitation and requirement classification. This paper presents a model in which crowd sourcing approach is used so that customers, end users, stakeholders, developers and software engineers can make active participation for requirement elicitation process and requirements gathered using crowdsourcing approach are used by model for classification process i.e. classification of requirements into functional and non-functional requirements. For the proof of proposed model a case study is conducted. Results of case study provided the usefulness and efficiency of proposed model for classification of crowd sourced software requirements.",
        "link": "https://dl.acm.org/doi/10.1145/3328833.3328837"
    },
    {
        "title": "Text classification using Fuzzy TF-IDF and Machine Learning Models",
        "authors": [
            "Mariem Bounabi",
            "Karim El Moutaouakil",
            "Khalid Satori"
        ],
        "date": "October 2019",
        "source": "BDIoT'19: Proceedings of the 4th International Conference on Big Data and Internet of Things",
        "abstract": "The representation of the information has an important impact on the text classification task. Several weighting methods were proposed in the literature, and the term frequency-inverse term frequency (TFIDF), the most know on the text treatment field. The FTF-IDF is a vector representation where the components of the TFIDF are presented as inputs to the Fuzzy Inference System (FIS). In this work, we compare several Machin Learning algorithms such as Naïve Bayes and its derivatives, SVM and Random forest classifiers, using the FTF-IDF representation. To improve the quality of the used classifiers, we call sum attribute selection methods. The recognition rate, for the tested systems, is satisfied, where the system based on naïve Bayes classifier, the FTF-IDF weighting terms, and the info gain select attributes method gives 98.7% as accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3372938.3372956"
    },
    {
        "title": "Text Mining in Cybersecurity: A Systematic Literature Review",
        "authors": [
            "Luciano Ignaczak",
            "Guilherme Goldschmidt",
            "Cristiano André Da Costa",
            "Rodrigo Da Rosa Righi"
        ],
        "date": null,
        "source": "ACM Computing Surveys",
        "abstract": "The growth of data volume has changed cybersecurity activities, demanding a higher level of automation. In this new cybersecurity landscape, text mining emerged as an alternative to improve the efficiency of the activities involving unstructured data. This article proposes a Systematic Literature Review (SLR) to present the application of text mining in the cybersecurity domain. Using a systematic protocol, we identified 2,196 studies, out of which 83 were summarized. As a contribution, we propose a taxonomy to demonstrate the different activities in the cybersecurity domain supported by text mining. We also detail the strategies evaluated in the application of text mining tasks and the use of neural networks to support activities involving unstructured data. The work also discusses text classification performance aiming its application in real-world solutions. The SLR also highlights open gaps for future research, such as the analysis of non-English content and the intensification in the usage of neural networks.",
        "link": "https://dl.acm.org/doi/10.1145/3462477"
    },
    {
        "title": "Using Text Mining to Analyze the Financial Patents",
        "authors": [
            "Yung-Feng Lu",
            "Jia-Lang Xu",
            "Mu-Yen Chen"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "With the advancement and development of science and technology, Nowadays society is the development of information technology, cloud computing, mobile Internet, Internet of Things, block chain and information technology.The combination of finance and technology has made financial technology terms gradually emerging.. This research analyzes the patents of financial technology. However, Taiwan's financial technology patents are mostly G06Q-20 (payment plan, architecture or agreement), G06Q-30 (commercial, such as marketing, shopping, payment, auction or e-commerce). G06Q-40 (financial, investment or tax treatment, insurance).The research use text mining technology to analyze and understand Taiwan's trends and developments in financial technology. The research results show that G06Q-20 patents are biased towards information security and mobile payment. G06Q-30 patents favor shopping websites and advertising. G06Q-40 patents are biased towards diversity, such as securities, insurance, finance, and stocks.",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326478"
    },
    {
        "title": "Identifying Online Profiles of Distance Learning Students Using Data Mining Techniques",
        "authors": [
            "Osama Islam",
            "Muazzam Siddiqui",
            "Naif Radi Aljohani"
        ],
        "date": "October 2019",
        "source": "ICDTE '19: Proceedings of the 3rd International Conference on Digital Technology in Education",
        "abstract": "Educational data has grown over the years with the increased use of technology within educational environments. This has led to a huge amount of data being stored in various data sources representing the student, his/her activities, and other aspects relevant to the learning process. To meet this analytical need, Educational Data Mining (EDM) has emerged to assist educational institutions in identifying key benefits such as students at risk, the level of student engagement or predicting student performance. The aim of this research was to explore the various aspects of student interaction data using data mining techniques to identify relevant patterns of behaviors and possible key attributes that have higher degrees of influence on distance learning students. The main findings identified several patterns of user online profiles based on a set of adopted learning strategies, the research proposed a framework for analyzing such interaction data based on R and Hadoop platforms to correlate online profiles with student performance.",
        "link": "https://dl.acm.org/doi/10.1145/3369199.3369249"
    },
    {
        "title": "Extremely Fast Decision Tree Mining for Evolving Data Streams",
        "authors": [
            "Albert Bifet",
            "Jiajin Zhang",
            "Wei Fan",
            "Cheng He",
            "Jianfeng Zhang",
            "Jianfeng Qian",
            "Geoff Holmes",
            "Bernhard Pfahringer"
        ],
        "date": "August 2017",
        "source": "KDD '17: Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
        "abstract": "Nowadays real-time industrial applications are generating a huge amount of data continuously every day. To process these large data streams, we need fast and efficient methodologies and systems. A useful feature desired for data scientists and analysts is to have easy to visualize and understand machine learning models. Decision trees are preferred in many real-time applications for this reason, and also, because combined in an ensemble, they are one of the most powerful methods in machine learning. In this paper, we present a new system called STREAMDM-C++, that implements decision trees for data streams in C++, and that has been used extensively at Huawei. Streaming decision trees adapt to changes on streams, a huge advantage since standard decision trees are built using a snapshot of data, and can not evolve over time. STREAMDM-C++ is easy to extend, and contains more powerful ensemble methods, and a more efficient and easy to use adaptive decision trees. We compare our new implementation with VFML, the current state of the art implementation in C, and show how our new system outperforms VFML in speed using less resources.",
        "link": "https://dl.acm.org/doi/10.1145/3097983.3098139"
    },
    {
        "title": "Data miners' little helper: data transformation activity cues for cluster analysis on document collections",
        "authors": [
            "Tania Cerquitelli",
            "Evelina Di Corso",
            "Francesco Ventura",
            "Silvia Chiusano"
        ],
        "date": "June 2017",
        "source": "WIMS '17: Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "In this paper we propose a new self-learning engine to streamline the analytics process, as it enables analysts to mine massive data repositories with minimal user intervention. In the context of cluster analysis on a collection of documents this new system, named SELF-DATA (SELF-learning DAta TrAnsformation), suggests to the analyst how to configure the whole mining process for a given dataset. SELF-DATA relies on an engine exploring different data weighting schemas (e.g., normalized term frequencies) and data transformation methods (e.g., PCA) before applying the cluster analysis, evaluating and comparing solutions through different quality indices (e.g., weighted Silhouette), and presenting the k-top solutions to the analyst. SELF-DATA will also include a knowledge base storing results of experiments on previously processed datasets, and a classification algorithm trained on the knowledge base content to forecast the best configuration for the whole mining process for an unexplored dataset. The first development of SELF-DATA running on Apache Spark has been validated on 5 collections of documents. Experimental results highlight that TF-IDF and logarithmic entropy are effective to measure item relevance with sparse datasets, and the LSI method outperforms PCA with a large dictionary.",
        "link": "https://dl.acm.org/doi/10.1145/3102254.3102288"
    },
    {
        "title": "Interactive Visual Graph Mining and Learning",
        "authors": [
            "Ryan A. Rossi",
            "Nesreen K. Ahmed",
            "Rong Zhou",
            "Hoda Eldardiry"
        ],
        "date": null,
        "source": "ACM Transactions on Intelligent Systems and Technology",
        "abstract": "This article presents a platform for interactive graph mining and relational machine learning called GraphVis. The platform combines interactive visual representations with state-of-the-art graph mining and relational machine learning techniques to aid in revealing important insights quickly as well as learning an appropriate and highly predictive model for a particular task (e.g., classification, link prediction, discovering the roles of nodes, and finding influential nodes). Visual representations and interaction techniques and tools are developed for simple, fast, and intuitive real-time interactive exploration, mining, and modeling of graph data. In particular, we propose techniques for interactive relational learning (e.g., node/link classification), interactive link prediction and weighting, role discovery and community detection, higher-order network analysis (via graphlets, network motifs), among others. GraphVis also allows for the refinement and tuning of graph mining and relational learning methods for specific application domains and constraints via an end-to-end interactive visual analytic pipeline that learns, infers, and provides rapid interactive visualization with immediate feedback at each change/prediction in real-time. Other key aspects include interactive filtering, querying, ranking, manipulating, exporting, as well as tools for dynamic network analysis and visualization, interactive graph generators (including new block model approaches), and a variety of multi-level network analysis techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3200764"
    },
    {
        "title": "Data Mining and Opinion Mining: A Tool in Educational Context",
        "authors": [
            "Myriam Peñafiel",
            "Stefanie Vásquez",
            "Diego Vásquez",
            "Juan Zaldumbide",
            "Sergio Luján-Mora"
        ],
        "date": "July 2018",
        "source": "ICoMS '18: Proceedings of the 2018 1st International Conference on Mathematics and Statistics",
        "abstract": "The use of the web as a universal communication platform generates large volumes of data (Big data), which in many cases, need to be processed so that they can become useful knowledge in face of the sceptics who have doubts about the credibility of such information. The use of web data that comes from educational contexts needs to be addressed, since that large amount of unstructured information is not being valued, losing valuable information that can be used. To solve this problem, we propose the use of data mining techniques such as sentiment analysis to validate the information that comes from the educational platforms. The objective of this research is to propose a methodology that allows the user to apply sentiment analysis in a simple way, because although some researchers have done it, very few do with data in the educational context. The results obtained prove that the proposal can be used in similar cases.",
        "link": "https://dl.acm.org/doi/10.1145/3274250.3274263"
    },
    {
        "title": "Research Hotspots Mining and Visualized Analysis Based on Linking Cluster and K-Core Decomposition",
        "authors": [
            "Ying Cai",
            "Fang Huang",
            "Shuai Wang",
            "Hao Zhang",
            "Chunxiu Du"
        ],
        "date": "May 2018",
        "source": "ICDPA 2018: Proceedings of the International Conference on Data Processing and Applications",
        "abstract": "In this paper, we focus on research hotspots mining with knowledge semantic features on discipline topic word networks, which are extracted from scientific and technical documents. For the topic networks, a novel method of combining linking community clustering with k-core decomposition is proposed to mine the hotspot community with knowledge hierarchy structure. And as a post-processing, the hierarchical community identification is implemented by k-core decomposition in order to discover the internal connection with knowledge semantic hierarchy in the research hotspot. Finally, the community density and visualization method is utilized to analyze the core and sub- problem in the research hotspot community. In experiments, the topic networks for a single discipline and interdisciplinary research hotspot are taken as examples of analysis, and the results show the proposed approach can effectively realize the objective of mining and analyzing the discipline research hotspot on the academic topic word network.",
        "link": "https://dl.acm.org/doi/10.1145/3224207.3224216"
    },
    {
        "title": "The Application of Data Mining Algorithms in the Construction of Travel Recommendation System",
        "authors": [
            "Chao Lou",
            "Zhaonan Mu",
            "Mengzhu Liu"
        ],
        "date": "January 2021",
        "source": "CONF-CDS 2021: The 2nd International Conference on Computing and Data Science",
        "abstract": "Photos shared by users of online social network often associated with tags, texts, geographic and time information. These data are ideal research resource which can be used by researchers to observe people's interests and behaviors. They can also be used to analyze people's travel behaviors and interests because it can reflect people's travel activities and experiences directly. This project aimed to consolidate people into groups based on their travel preferences of travel season, travel time span, scenery type of travel location and then analyze popular travel places for different people groups. The travel preferences were analyzed from extracted data applying PHP language and Naive Bayes classification approach. According to the analyzed data, people were divided into groups. Then, DBSCAN clustering was used to get popular locations based on the different groups. As a result, 80 thousand photo data of 5 cities have been collected from Flickr by Phython. It has been showed that people's preference of season in most of the mined cities was spring and summer and the preference of time span was short. Moreover, mined people has been divided into 16 groups according to their preference and based on the groups popular locations have been calculated by DBSCAN clustering. Theses locations have been provided in a recommendation website which was built in this project. For the future work, mining Flickr data associated with other travel social network sites could be a good way to build a more effective recommendation system.",
        "link": "https://dl.acm.org/doi/10.1145/3448734.3450477"
    },
    {
        "title": "Compare Machine Learning Models in Text Classification Using Steam User Reviews",
        "authors": [
            "Youchen Miao",
            "Zeyu Jin",
            "Yumeng Zhang",
            "Yuchen Chen",
            "Junren Lai"
        ],
        "date": "November 2021",
        "source": "ICSED '21: Proceedings of the 2021 3rd International Conference on Software Engineering and Development",
        "abstract": "Text Classification and Sentiment Analysis of game reviews are viewed as important parts in not only academic fields but also in game studies. In this paper, with more than 400 thousand game reviews on Steam platform, we preprocess the data using different libraries (sklearn, nltk, and spaCy) and use them as inputs to build three sentiment classification models based on different algorithms (Naive Bayes, SVM, and Random Forest). In contrast to previous studies that only focus on different sentiment analysis models, our paper also highlights the use of different APIs to preprocess the data and their corresponding model performance. The results show that no matter which API we choose, Random Forest models always perform the best. However, in terms of training time, Naive Bayes is the fastest. This work can be used to apply grid search for researchers to automatically find the optimum API before conducting sentiment analysis in the future.",
        "link": "https://dl.acm.org/doi/10.1145/3507473.3507480"
    },
    {
        "title": "Lost in Transduction: Transductive Transfer Learning in Text Classification",
        "authors": [
            "Alejandro Moreo",
            "Andrea Esuli",
            "Fabrizio Sebastiani"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Obtaining high-quality labelled data for training a classifier in a new application domain is often costly. Transfer Learning\n(a.k.a. “Inductive Transfer”) tries to alleviate these costs by transferring, to the “target” domain of interest, knowledge available from a different “source” domain. In transfer learning the lack of labelled information from the target domain is compensated by the availability at training time of a set of unlabelled examples from the target distribution. Transductive Transfer Learning denotes the transfer learning setting in which the only set of target documents that we are interested in classifying is known and available at training time. Although this definition is indeed in line with Vapnik’s original definition of “transduction”, current terminology in the field is confused. In this article, we discuss how the term “transduction” has been misused in the transfer learning literature, and propose a clarification consistent with the original characterization of this term given by Vapnik. We go on to observe that the above terminology misuse has brought about misleading experimental comparisons, with inductive transfer learning methods that have been incorrectly compared with transductive transfer learning methods. We then, give empirical evidence that the difference in performance between the inductive version and the transductive version of a transfer learning method can indeed be statistically significant (i.e., that knowing at training time the only data one needs to classify indeed gives an advantage). Our clarification allows a reassessment of the field, and of the relative merits of the major, state-of-the-art algorithms for transfer learning in text classification.",
        "link": "https://dl.acm.org/doi/10.1145/3453146"
    },
    {
        "title": "Machine Learning on Mining Potential Adverse Drug Reactions for Pharmacovigilance",
        "authors": [
            "Ke Jia"
        ],
        "date": "July 2021",
        "source": "DSIT 2021: 2021 4th International Conference on Data Science and Information Technology",
        "abstract": "Drugs are related to the lives and health of patients. Mining and discovering potential adverse drug reactions from a large number of unstructured texts play an important role in drug research and pharmacovigilance. It is a hot research issue in the field of biomedicine in recent years. A large number of scholars have discovered a large amount of information on adverse drug reactions from different data sources through effective machine learning methods. This paper reviews the research progress in recent years to provide a reference for future research.",
        "link": "https://dl.acm.org/doi/10.1145/3478905.3478964"
    },
    {
        "title": "TACAM: Topic And Context Aware Argument Mining",
        "authors": [
            "Michael Fromm",
            "Evgeniy Faerman",
            "Thomas Seidl"
        ],
        "date": "October 2019",
        "source": "WI '19: IEEE/WIC/ACM International Conference on Web Intelligence",
        "abstract": "In this work we address the problem of argument search. The purpose of argument search is the distillation of pro and contra arguments for requested topics from large text corpora. In previous works, the usual approach is to use a standard search engine to extract text parts which are relevant to the given topic and subsequently use an argument recognition algorithm to select arguments from them. The main challenge in the argument recognition task, which is also known as argument mining, is that often sentences containing arguments are structurally similar to purely informative sentences without any stance about the topic. In fact, they only differ semantically. Most approaches use topic or search term information only for the first search step and therefore assume that arguments can be classified independently of a topic. We argue that topic information is crucial for argument mining, since the topic defines the semantic context of an argument. Precisely, we propose different models for the classification of arguments, which take information about a topic of an argument into account. Moreover, to enrich the context of a topic and to let models understand the context of the potential argument better, we integrate information from different external sources such as Knowledge Graphs or pre-trained NLP models. Our evaluation shows that considering topic information, especially in connection with external information, provides a significant performance boost for the argument mining task.",
        "link": "https://dl.acm.org/doi/10.1145/3350546.3352506"
    },
    {
        "title": "Text mining for incoming tasks based on the urgency/importance factors and task classification using machine learning tools",
        "authors": [
            "Yasser Ali Alshehri"
        ],
        "date": "March 2020",
        "source": "ICCDA 2020: Proceedings of the 2020 the 4th International Conference on Compute and Data Analysis",
        "abstract": "In workplaces, there is a massive amount of unstructured data from different sources. In this paper, we present a case study that explains how can through communications between employees, we can help to prioritize tasks requests to increase the efficiency of their works for both technical and non-technical workers. This involves managing daily incoming tasks based on their level of urgency and importance.To allow all workers to utilize the urgency-importance matrix as a time-management tool, we need to automate this tool. The textual content of incoming tasks are analyzed, and metrics related to urgency and importance are extracted. A third factor (i.e., the response variable) is defined based on the two input variables (urgency and importance). Then, machine learning applied to the data to predict the class of incoming tasks based on data outcome desired. We used ordinal regression, neural networks, and decision tree algorithms to predict the four levels of task priority. We measure the performance of all using recalls, precisions, and F-scores. All classifiers perform higher than 89% in terms of all measures.",
        "link": "https://dl.acm.org/doi/10.1145/3388142.3388153"
    },
    {
        "title": "Text mining for malware classification using multivariate all repeated patterns detection",
        "authors": [
            "Konstantinos F. Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2019",
        "source": "ASONAM '19: Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Mobile phones have become nowadays a commodity to the majority of people. Using them, people are able to access the world of Internet and connect with their friends, their colleagues at work or even unknown people with common interests. This proliferation of the mobile devices has also been seen as an opportunity for the cyber criminals to deceive smartphone users and steel their money directly or indirectly, respectively, by accessing their bank accounts through the smartphones or by blackmailing them or selling their private data such as photos, credit card data, etc. to third parties. This is usually achieved by installing malware to smartphones masking their malevolent payload as a legitimate application and advertise it to the users with the hope that mobile users will install it in their devices. Thus, any existing application can easily be modified by integrating a malware and then presented it as a legitimate one. In response to this, scientists have proposed a number of malware detection and classification methods using a variety of techniques. Even though, several of them achieve relatively high precision in malware classification, there is still space for improvement. In this paper, we propose a text mining all repeated pattern detection method which uses the decompiled files of an application in order to classify a suspicious application into one of the known malware families. Based on the experimental results using a real malware dataset, the methodology tries to correctly classify (without any misclassification) all randomly selected malware applications of 3 categories with 3 different families each.",
        "link": "https://dl.acm.org/doi/10.1145/3341161.3350841"
    },
    {
        "title": "Comparative Study between Traditional Machine Learning and Deep Learning Approaches for Text Classification",
        "authors": [
            "Cannannore Nidhi Kamath",
            "Syed Saqib Bukhari",
            "Andreas Dengel"
        ],
        "date": "August 2018",
        "source": "DocEng '18: Proceedings of the ACM Symposium on Document Engineering 2018",
        "abstract": "In this contemporaneous world, it is an obligation for any organization working with documents to end up with the insipid task of classifying truckload of documents, which is the nascent stage of venturing into the realm of information retrieval and data mining. But classification of such humongous documents into multiple classes, calls for a lot of time and labor. Hence a system which could classify these documents with acceptable accuracy would be of an unfathomable help in document engineering. We have created multiple classifiers for document classification and compared their accuracy on raw and processed data. We have garnered data used in a corporate organization as well as publicly available data for comparison. Data is processed by removing the stop-words and stemming is implemented to produce root words. Multiple traditional machine learning techniques like Naive Bayes, Logistic Regression, Support Vector Machine, Random forest Classifier and Multi-Layer Perceptron are used for classification of documents. Classifiers are applied on raw and processed data separately and their accuracy is noted. Along with this, Deep learning technique such as Convolution Neural Network is also used to classify the data and its accuracy is compared with that of traditional machine learning techniques. We are also exploring hierarchical classifiers for classification of classes and subclasses. The system classifies the data faster and with better accuracy than if done manually. The results are discussed in the results and evaluation section.",
        "link": "https://dl.acm.org/doi/10.1145/3209280.3209526"
    },
    {
        "title": "Identification of Overpricing in the Purchase of Medication by the Federal Government of Brazil, Using Text Mining and Clustering Based on Ontology",
        "authors": [
            "Marco Aurelio O. S. Correa",
            "Adriano Galindo Leal"
        ],
        "date": "August 2018",
        "source": "ICCBDC '18: Proceedings of the 2018 2nd International Conference on Cloud and Big Data Computing",
        "abstract": "Increasing the transparency level in his actions and spending is one of the primary duties of the Brazilian Federal Government. The creation of laws that oblige full disclose of all its expenditures through transparency portals enables citizens to supervise all government entities. However, only the dissemination of these data, without a definite standard or the availability of data analysis tools, does not guarantee that the citizen is empowered to play his role. Therefore hence, the objective of this work is to identify overprice in the acquisition of products purchased by the federal government of Brazil using the unstructured data available on the Transparency Portal. The last two-years' worth of purchasing data, available in the Transparency Portal, were extracted, processed and stored. Due to his diverse nature and high volume of data, this study focused only on medicines purchased by the Ministry of Health. Ontology-based text mining and clustering techniques were applied for automatic identification and classification of products. The processing of this information was done through text mining and clustering, based on the ontology registered in another database of the Brazilian government. Because of this work, a consolidated price base per medication was created to allow the identification of distortions in prices practised, facilitating the identification of cases that merit further investigation to unravel fraud to the treasury.",
        "link": "https://dl.acm.org/doi/10.1145/3264560.3264569"
    },
    {
        "title": "Computational Estimation by Scientific Data Mining with Classical Methods to Automate Learning Strategies of Scientists",
        "authors": [
            "Aparna S. Varde"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Experimental results are often plotted as 2-dimensional graphical plots (aka graphs) in scientific domains depicting dependent versus independent variables to aid visual analysis of processes. Repeatedly performing laboratory experiments consumes significant time and resources, motivating the need for computational estimation. The goals are to estimate the graph obtained in an experiment given its input conditions, and to estimate the conditions that would lead to a desired graph. Existing estimation approaches often do not meet accuracy and efficiency needs of targeted applications. We develop a computational estimation approach called AutoDomainMine that integrates clustering and classification over complex scientific data in a framework so as to automate classical learning methods of scientists. Knowledge discovered thereby from a database of existing experiments serves as the basis for estimation. Challenges include preserving domain semantics in clustering, finding matching strategies in classification, striking a good balance between elaboration and conciseness while displaying estimation results based on needs of targeted users, and deriving objective measures to capture subjective user interests. These and other challenges are addressed in this work. The AutoDomainMine approach is used to build a computational estimation system, rigorously evaluated with real data in Materials Science. Our evaluation confirms that AutoDomainMine provides desired accuracy and efficiency in computational estimation. It is extendable to other science and engineering domains as proved by adaptation of its sub-processes within fields such as Bioinformatics and Nanotechnology.",
        "link": "https://dl.acm.org/doi/10.1145/3502736"
    },
    {
        "title": "Where is the road for issue reports classification based on text mining?",
        "authors": [
            "Qiang Fan",
            "Yue Yu",
            "Gang Yin",
            "Tao Wang",
            "Huaimin Wang"
        ],
        "date": "November 2017",
        "source": "ESEM '17: Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Currently, open source projects receive various kinds of issues daily, because of the extreme openness of Issue Tracking System (ITS) in GitHub. ITS is a labor-intensive and time-consuming task of issue categorization for project managers. However, a contributor is only required a short textual abstract to report an issue in GitHub. Thus, most traditional classification approaches based on detailed and structured data (e.g., priority, severity, software version and so on) are difficult to adopt. In this paper, issue classification approaches on a large-scale dataset, including 80 popular projects and over 252,000 issue reports collected from GitHub, were investigated. First, four traditional text-based classification methods and their performances were discussed. Semantic perplexity (i.e., an issues description confuses bug-related sentences with nonbug-related sentences) is a crucial factor that affects the classification performances based on quantitative and qualitative study. Finally, A two-stage classifier framework based on the novel metrics of semantic perplexity of issue reports was designed. Results show that our two-stage classification can significantly improve issue classification performances.",
        "link": "https://dl.acm.org/doi/10.1109/ESEM.2017.19"
    },
    {
        "title": "Feature-based Facebook reviews process model for e-management using data mining",
        "authors": [
            "Anish Kumar Varudharajulu",
            "Yongsheng Ma"
        ],
        "date": "January 2019",
        "source": "IC4E '19: Proceedings of the 10th International Conference on E-Education, E-Business, E-Management and E-Learning",
        "abstract": "The data generated from online communication acts as potential gold mines for discovering knowledge for researchers. A large amount of data is also generated in the form of web documents, emails, blogs, and feedback, etc. Text analytics is being significantly employed to mine important information. Opinion mining is the process of extracting human thoughts and perceptions from unstructured texts. The showstopper for designing an opinion mining system for analyzing reviews arise from the fact that customer reviews are often noisy. These reviews are informally written. In addition, they are subjected to spelling mistakes, grammatical errors, improper punctuation and irrational capitalization. This paper focuses on analyzing the different classification and clustering algorithms aimed at extracting and consolidating opinions of customers from social media sites like Facebook, Twitter and through surveys, at multiple levels of granularity to monitor and measure customer satisfaction. Ours is an automated approach, in which the system aids in the process of knowledge assimilation for knowledge-based building and also performs the analytics. Domain experts ratify the knowledge base and also provide training datasets for the system to intuitively gather more instances for ratification. The system identifies opinion expressions as phrases containing opinion words, opinionated features and also opinion modifiers. These expressions are categorized as positive, negative or neutral. Opinion expressions are identified and categorized using localized linguistic techniques. Opinions can be congregated at any desired level of specificity i.e. feature level or product level, user level or service level, etc. We have developed a system based on this approach, which provides the user with a platform to analyze opinion expressions crawled from a set of pre-defined datasets.",
        "link": "https://dl.acm.org/doi/10.1145/3306500.3306514"
    },
    {
        "title": "Discriminative Topic Mining via Category-Name Guided Text Embedding",
        "authors": [
            "Yu Meng",
            "Jiaxin Huang",
            "Guangyuan Wang",
            "Zihan Wang",
            "Chao Zhang",
            "Yu Zhang",
            "Jiawei Han"
        ],
        "date": "April 2020",
        "source": "WWW '20: Proceedings of The Web Conference 2020",
        "abstract": "Mining a set of meaningful and distinctive topics automatically from massive text corpora has broad applications. Existing topic models, however, typically work in a purely unsupervised way, which often generate topics that do not fit users’ particular needs and yield suboptimal performance on downstream tasks. We propose a new task, discriminative topic mining, which leverages a set of user-provided category names to mine discriminative topics from text corpora. This new task not only helps a user understand clearly and distinctively the topics he/she is most interested in, but also benefits directly keyword-driven classification tasks. We develop CatE, a novel category-name guided text embedding method for discriminative topic mining, which effectively leverages minimal user guidance to learn a discriminative embedding space and discover category representative terms in an iterative manner. We conduct a comprehensive set of experiments to show that CatE mines high-quality set of topics guided by category names only, and benefits a variety of downstream applications including weakly-supervised classification and lexical entailment direction identification.",
        "link": "https://dl.acm.org/doi/10.1145/3366423.3380278"
    },
    {
        "title": "Comparative Study of Heart Disease Diagnosis Using Top Ten Data Mining Classification Algorithms",
        "authors": [
            "I. Ketut Agung Enriko"
        ],
        "date": "June 2019",
        "source": "ICFET '19: Proceedings of the 5th International Conference on Frontiers of Educational Technologies",
        "abstract": "Data mining has been used for many purposes, especially for prediction system. In healthcare, data mining algorithms often used in disease diagnosis. Meanwhile, heart disease is known as a primary cause of death over the years. Many studies have been performed in heart disease diagnosis using data mining methods. There are some popular data mining algorithms that can be used in heart disease diagnosis, for example, k-Nearest Neighbor, CART, and AdaBoost. The algorithms are used to analyze a sample of cardiovascular patients data and predict the heart disease type that they suffer. Some parameters are taken from the patient, including EKG morphology, blood pressure, and information about the existence of chest pain, shortness of breath, palpitation, and cold sweat. In this study, medical records data are collected from Harapan Kita Hospital and utilized as a dataset sample in this research. Top ten data mining classification algorithms are used in diagnosing heart disease from Harapan Kita Hospital data and examining their performance by checking the accuracy and speed.",
        "link": "https://dl.acm.org/doi/10.1145/3338188.3338220"
    },
    {
        "title": "Text Mining Approach for Identifying Research Trends",
        "authors": [
            "Snezhana Sulova"
        ],
        "date": "June 2021",
        "source": "CompSysTech '21: Proceedings of the 22nd International Conference on Computer Systems and Technologies",
        "abstract": "With the increase of unstructured data, the issues connected with automatic text processing, the categorization of documents and the discovery of topics have become objects of growing interest. In order to improve the process of grouping and processing research publications, we would like to propose a method based upon natural language processing. It is based on text mining technologies which aim to identify key tendencies in documents. It processes the content of publications by clustering and identifies the topics of each identified group. This analysis helps by identifying key tendencies as well as discovering emerging new areas of research. Publications from the research literature database, Scopus, were used to test the approach. The topic of the publications is “the application of digital technologies in the logistics business”. The experiments were completed using the RapidMiner Studio software.",
        "link": "https://dl.acm.org/doi/10.1145/3472410.3472433"
    },
    {
        "title": "Assessment of Vulnerability Severity using Text Mining",
        "authors": [
            "Georgios Spanos",
            "Lefteris Angelis",
            "Dimitrios Toloudis"
        ],
        "date": "September 2017",
        "source": "PCI '17: Proceedings of the 21st Pan-Hellenic Conference on Informatics",
        "abstract": "Software1 vulnerabilities are closely associated with information systems security, a major and critical field in today's technology. Vulnerabilities constitute a constant and increasing threat for various aspects of everyday life, especially for safety and economy, since the social impact from the problems that they cause is complicated and often unpredictable. Although there is an entire research branch in software engineering that deals with the identification and elimination of vulnerabilities, the growing complexity of software products and the variability of software production procedures are factors contributing to the ongoing occurrence of vulnerabilities, Hence, another area that is being developed in parallel focuses on the study and management of the vulnerabilities that have already been reported and registered in databases. The information contained in such databases includes, a textual description and a number of metrics related to vulnerabilities. The purpose of this paper is to investigate to what extend the assessment of the vulnerability severity can be inferred directly from the corresponding textual description, or in other words, to examine the informative power of the description with respect to the vulnerability severity. For this purpose, text mining techniques, i.e. text analysis and three different classification methods (decision trees, neural networks and support vector machines) were employed. The application of text mining to a sample of 70,678 vulnerabilities from a public data source shows that the description itself is a reliable and highly accurate source of information for vulnerability prioritization.",
        "link": "https://dl.acm.org/doi/10.1145/3139367.3139390"
    },
    {
        "title": "Clustering Algorithms for Spatial Data Mining",
        "authors": [
            "Chetashri Bhadane",
            "Ketan Shah"
        ],
        "date": "April 2020",
        "source": "ICGDA '20: Proceedings of the 2020 3rd International Conference on Geoinformatics and Data Analysis",
        "abstract": "With the advances in mobile and wireless technologies, there has been a rise in applications that track and share the users' geospatial data. People use several social networking sites such as Twitter, Facebook and Flickr, where they share their status updates. With the integration of Global Positioning System (GPS) with mobile phones, it is now possible to share one's locations on these social networks. GPS allows us to record and track a person's movement along with the timestamp. The data set obtained from these GPS logs is vast and is widely used to analyze the users' movement patterns. Specifically, we can find out significant locations based on the number of users present at that location and the time spent by them at such places. Once significant places have been identified, it is also possible to identify the semantic importance of these locations. This paper presents an overview of the clustering techniques used to find important places of interest using large GPS based mobility datasets. Four clustering algorithms, K-Means, DBSCAN, OPTICS and Hierarchical, are implemented, and performance is tested using real-time data of 50 users collected over 2--5 years. Performance summary depicts that K-Means and DBSCAN perform well for spatial data.",
        "link": "https://dl.acm.org/doi/10.1145/3397056.3397068"
    },
    {
        "title": "Feature Selection based Arabic Text Classification using Different Machine Learning Algorithms: Comparative Study",
        "authors": [
            "Sakina Rim Bennabi",
            "Zakaria Elberrichi"
        ],
        "date": "June 2020",
        "source": "ICIST '20: Proceedings of the 10th International Conference on Information Systems and Technologies",
        "abstract": "Feature selection is a method of data pre-processing widely used when mining large data, such as textual classification. Several studies have been conducted to compare the different methods of feature selection applied to corpora in English. Unfortunately, a small number of works concern the Arabic language. This article aims to present a comparative study of different feature selection techniques including: Chi2, the ANOVA method and mutual information, applied on a corpus in Arabic language, while also diversifying the machine learning algorithms (Naive Bayes, SVM and KNN). This experimental study has shown in general that reducing dimensionality with feature selection techniques has slightly affected the performance of textual classification, reducing the size of the corpus by up to 1%.",
        "link": "https://dl.acm.org/doi/10.1145/3447568.3448531"
    },
    {
        "title": "Intelligent Data Analysis using Optimized Support Vector Machine Based Data Mining Approach for Tourism Industry",
        "authors": [
            "Ms Promila Sharma",
            "Uma Meena",
            "Girish Kumar Sharma"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Data analysis involves the deployment of sophisticated approaches from data mining methods, information theory, and artificial intelligence in various fields like tourism, hospitality, and so on for the extraction of knowledge from the gathered and preprocessed data. In tourism, pattern analysis or data analysis using classification is significant for finding the patterns that represent new and potentially useful information or knowledge about the destination and other data. Several data mining techniques are introduced for the classification of data or patterns. However, overfitting, less accuracy, local minima, sensitive to noise are the drawbacks in some existing data mining classification methods. To overcome these challenges, Support vector machine with Red deer optimization (SVM-RDO) based data mining strategy is proposed in this article. Extended Kalman filter (EKF) is utilized in the first phase, i.e., data cleaning to remove the noise and missing values from the input data. Mantaray foraging algorithm (MaFA) is used in the data selection phase, in which the significant data are selected for the further process to reduce the computational complexity. The final phase is the classification, in which SVM-RDO is proposed to access the useful pattern from the selected data. PYTHON is the implementation tool used for the experiment of the proposed model. The experimental analysis is done to show the efficacy of the proposed work. From the experimental results, the proposed SVM-RDO achieved better accuracy, precision, recall, and F1 score than the existing methods for the tourism dataset. Thus, it is showed the effectiveness of the proposed SVM-RDO for pattern analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3494566"
    },
    {
        "title": "Applying text mining to predict learners' cognitive engagement",
        "authors": [
            "Hayati Hind",
            "Mohammed Khalidi Idrissi",
            "Samir Bennani"
        ],
        "date": "October 2017",
        "source": "SCAMS '17: Proceedings of the Mediterranean Symposium on Smart City Application",
        "abstract": "In academic process, engagement represent one of the important success' factor. This paper proposes a predictive system for learners' cognitive engagement based on their online discussion forums participation. In the first level the ontology OWL and the LSA method are used to perform a semantic classification system of the threads according to a specific context chosen by the tutor. Then the Text mining, as a predictive method, is applied to the classified threads and learners' participation in the forums.",
        "link": "https://dl.acm.org/doi/10.1145/3175628.3175655"
    },
    {
        "title": "Investigate Transitions into Drug Addiction through Text Mining of Reddit Data",
        "authors": [
            "John Lu",
            "Sumati Sridhar",
            "Ritika Pandey",
            "Mohammad Al Hasan",
            "Georege Mohler"
        ],
        "date": "July 2019",
        "source": "KDD '19: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Increasing rates of opioid drug abuse and heightened prevalence of online support communities underscore the necessity of employing data mining techniques to better understand drug addiction using these rapidly developing online resources. In this work, we obtained data from Reddit, an online collection of forums, to gather insight into drug use/misuse using text snippets from users narratives. Specifically, using users' posts, we trained a binary classifier which predicts a user's transitions from casual drug discussion forums to drug recovery forums. We also proposed a Cox regression model that outputs likelihoods of such transitions. In doing so, we found that utterances of select drugs and certain linguistic features contained in one's posts can help predict these transitions. Using unfiltered drug-related posts, our research delineates drugs that are associated with higher rates of transitions from recreational drug discussion to support/recovery discussion, offers insight into modern drug culture, and provides tools with potential applications in combating the opioid crisis.",
        "link": "https://dl.acm.org/doi/10.1145/3292500.3330737"
    },
    {
        "title": "Analysing digital banking reviews using text mining",
        "authors": [
            "Li Chen Cheng",
            "Legaspi Rhea Sharmayne"
        ],
        "date": "December 2020",
        "source": "ASONAM '20: Proceedings of the 12th IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Digital banks are new entrants in the banking industry in the Philippines as they only started late 2018. Since then, a handful of players have and are still emerging. With more and more people becoming technologically savvy, it is very critical for financial institutions to develop a digital banking application that will stand out from the competition. This paper aims to use text mining methods to analyse digital banking application reviews. This study will perform topic modelling using LDA to explore customer concerns and will mine association rules between the digital banking features with the review score. The results will reveal which areas the digital banking application can further optimize for customer satisfaction and retention.",
        "link": "https://dl.acm.org/doi/10.1109/ASONAM49781.2020.9381429"
    },
    {
        "title": "Mining Moocs Videos Metadata Using Classification Techniques",
        "authors": [
            "El Harrak Othman",
            "Ghadi Abderrahim",
            "El Bouhdidi Jaber"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "In few years, the internet has become the first source of information for most people, Today MOOCs makes it possible for everyone to access to the education over the world, it's represents an emerging methodology of online teaching and an important development in open education. Due to the rapid development of Moocs and the rapid growth of digital data and video database over the Internet, it is becoming very difficult for learners to browsing and choosing the best training for them. The scientific community has increased the amount of research into new technologies, with a view to improve the digital video utilization: its archiving, indexing, accessibility, acquisition, store and even its process and usability. All these parts of the video utilization entail the necessity of the extraction of all important information of a video, especially in cases of lack of metadata information. Web video mining is retrieving the content using data mining techniques from World Wide Web. There are two approaches for web video mining using traditional image processing (signal processing) and metadata based approach. In this paper, we present the various research makes in this subject and we propose an effective methodology to extract the metadata from Moocs videos and classify them based on the extracted metadata by applying data mining techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090450"
    },
    {
        "title": "Text mining with HathiTrust",
        "authors": [
            "Eleanor Dickson Koehl",
            "Ryan Dubnicek"
        ],
        "date": "June 2019",
        "source": "JCDL '19: Proceedings of the 18th Joint Conference on Digital Libraries",
        "abstract": "This tutorial will introduce attendees to the HathiTrust Research Center and its tools and services for computational text analysis research. HTRC leverages the scope and scale of the HathiTrust Digital Library collection to create opportunities for researchers to perform text data mining on subsets of the corpus. Attendees will develop skills that will allow them to conduct basic text analysis research using HathiTrust data, learn techniques for engaging with scholars on their text data mining research, and explore strategies for computational access to large-scale digital collections.",
        "link": "https://dl.acm.org/doi/10.1109/JCDL.2019.00115"
    },
    {
        "title": "Text mining for plagiarism detection: multivariate pattern detection for recognition of text similarities",
        "authors": [
            "Konstantinos Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2018",
        "source": "ASONAM '18: Proceedings of the 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "The problem of plagiarism the recent years has been intensified by the availability of information in digital form and the accessibility of the electronic libraries through the Internet. As a result, plagiarism detection has been transformed into a big data analytics problem since the number of digital sources is extravagant and a new document needs to be compared with millions of other existing documents. In this paper, a text mining methodology is proposed that can detect all common patterns between a document and the documents in a reference database. The technique is based on a pattern detection algorithm and the corresponding data structure that enables the algorithm to detect all common patterns. The methodology has been applied in a well-defined dataset providing very promising results identifying difficult cases of plagiarism such as technical disguise.",
        "link": "https://dl.acm.org/doi/10.5555/3382225.3382424"
    },
    {
        "title": "Petroleum Engineering Data Text Classification Using Convolutional Neural Network Based Classifier",
        "authors": [
            "Tzuhan Hsu",
            "Yaoqin Zhang"
        ],
        "date": "May 2018",
        "source": "ICMLT '18: Proceedings of the 2018 International Conference on Machine Learning Technologies",
        "abstract": "In this paper, authors introduce a convolutional neural network (CNN) based classification approach for petroleum engineering event. Recently, text classification problem has raised substantial attention in the field of machine learning. The aim of text classification process is to assign correct label to a text on the basis of its content. However, due to the imprecise of feature representation, most of former text classification methods cannot export satisfying result. To solve this problem, authors propose a model based on convolutional neural network. Authors conduct experiments with real world petroleum engineering data text, artificially labelled into 6 categories. Beside purposed CNN-based model, the authors also implement other baseline machine learning approaches: k nearest neighbor, random forest, support vector machines, recurrent neural network, gated recurrent units, long short-term memory. Among all these models, our CNN-based model reach best performance in text classification task. In the last part of this paper, authors compare and analysis their purpose and baseline model.",
        "link": "https://dl.acm.org/doi/10.1145/3231884.3231898"
    },
    {
        "title": "Classifying Indonesian Online Articles as Advertisement Placement Base Using Text Mining",
        "authors": [
            "Nadhira Tasya",
            "Arian Dhini"
        ],
        "date": "July 2017",
        "source": "ICBIM 2017: Proceedings of the International Conference on Business and Information Management",
        "abstract": "Rapid development in technological aspect resulting in growing level of human needs for the latest news, so that emerged a new trend of publishing and accessing news through online media or usually called online journalism. In addition, the number of people who sell and purchase through online sites also continues to increase and this opportunity is utilized by the company and the advertiser by implementing targeted web advertising. However, the high number of articles that have been published and accessed leads to great opportunities for errors in determining where to place the ads. Therefore, it needs a system that can categorize articles accessed by users as the basis of advertisement placement by the company and this classification system can be done by applying the method of Data Mining and Text Mining. This research uses document data in the form of article content that will be categorized into twenty categories of class of advertisement by using Text Mining technique with Support Vector Machine algorithm. The results of this study may be used by companies or advertisers as a basis for placement of ads on selected online media sites.",
        "link": "https://dl.acm.org/doi/10.1145/3134271.3134288"
    },
    {
        "title": "Mining Free-Text Medical Notes for Suicide Risk Assessment",
        "authors": [
            "Marios Adamou",
            "Grigoris Antoniou",
            "Elissavet Greasidou",
            "Vincenzo Lagani",
            "Paulos Charonyktakis",
            "Ioannis Tsamardinos"
        ],
        "date": "July 2018",
        "source": "SETN '18: Proceedings of the 10th Hellenic Conference on Artificial Intelligence",
        "abstract": "Suicide has been considered as an important public health issue for a very long time, and is one of the main causes of death worldwide. Despite suicide prevention strategies being applied, the rate of suicide has not changed substantially over the past decades. Advances in machine learning make it possible to attempt to predict suicide based on the analysis of relevant data to inform clinical practice. This paper reports on findings from the analysis of data of patients who died by suicide in the period 2013-2016 and made use of both structured data and free-text medical notes. We focus on examining various text-mining approaches to support risk assessment. The results show that using advance machine learning and text-mining techniques, it is possible to predict within a specified period which people are most at risk of taking their own life at the time of referral to a mental health service.",
        "link": "https://dl.acm.org/doi/10.1145/3200947.3201020"
    },
    {
        "title": "Integration of data mining techniques in e-learning systems: Clustering Profil of Lerners and Recommender Course System",
        "authors": [
            "A. El Moustamid",
            "E. En-Naimi",
            "J. El Bouhdidi"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "Data mining aims at the extraction of knowledge from large quantities of data by automatic or semi-automatic methods. In the field of education and training e-learning, Data Mining techniques as clustering (The aim of this technique is to identify groups that are similar in some aspect) [21], prediction (is used for developing model where combination of data viewed from different aspects can effect single viewed data. Here, is important to watch how predictors variables are effecting predicted variable)[21], classification are applied in e-learning to improve the level of learners by providing learning according to the learners' requirement) are used to analyze learners profile, to predict and improve student performance. The aim of our project is to develop a system capable of analyzing learners profile and indexing web videos in order to offer learners a rich database with courses that correspond to their levels.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090453"
    },
    {
        "title": "Multi-level mining and visualization of scientific text collections: Exploring a bi-lingual scientific repository",
        "authors": [
            "Pablo Accuosto",
            "Francesco Ronzano",
            "Daniel Ferrés",
            "Horacio Saggion"
        ],
        "date": "December 2017",
        "source": "WOSP 2017: Proceedings of the 6th International Workshop on Mining Scientific Publications",
        "abstract": "We present a system to mine and visualize collections of scientific documents by semantically browsing information extracted from single publications or aggregated throughout corpora of articles. The text mining tool performs deep analysis of document collections allowing the extraction and interpretation of research paper's contents. In addition to the extraction and enrichment of documents with metadata (titles, authors, affiliations, etc), the deep analysis performed comprises semantic interpretation, rhetorical analysis of sentences, triple-based information extraction, and text summarization. The visualization components allow geographical-based exploration of collections, topic-evolution interpretation, and collaborative network analysis among others. The paper presents a case study of a bi-lingual collection in the field of Natural Language Processing (NLP).",
        "link": "https://dl.acm.org/doi/10.1145/3127526.3127529"
    },
    {
        "title": "Hierarchical Topic Mining via Joint Spherical Tree and Text Embedding",
        "authors": [
            "Yu Meng",
            "Yunyi Zhang",
            "Jiaxin Huang",
            "Yu Zhang",
            "Chao Zhang",
            "Jiawei Han"
        ],
        "date": "August 2020",
        "source": "KDD '20: Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Mining a set of meaningful topics organized into a hierarchy is intuitively appealing since topic correlations are ubiquitous in massive text corpora. To account for potential hierarchical topic structures, hierarchical topic models generalize flat topic models by incorporating latent topic hierarchies into their generative modeling process. However, due to their purely unsupervised nature, the learned topic hierarchy often deviates from users' particular needs or interests. To guide the hierarchical topic discovery process with minimal user supervision, we propose a new task, Hierarchical Topic Mining, which takes a category tree described by category names only, and aims to mine a set of representative terms for each category from a text corpus to help a user comprehend his/her interested topics. We develop a novel joint tree and text embedding method along with a principled optimization procedure that allows simultaneous modeling of the category tree structure and the corpus generative process in the spherical space for effective category-representative term discovery. Our comprehensive experiments show that our model, named JoSH, mines a high-quality set of hierarchical topics with high efficiency and benefits weakly-supervised hierarchical text classification tasks.",
        "link": "https://dl.acm.org/doi/10.1145/3394486.3403242"
    },
    {
        "title": "Unsupervised behavioural mining and clustering for malware family identification",
        "authors": [
            "Khanh Huu The Dam",
            "Thomas Given-Wilson",
            "Axel Legay"
        ],
        "date": "March 2021",
        "source": "SAC '21: Proceedings of the 36th Annual ACM Symposium on Applied Computing",
        "abstract": "More accurate and advanced detection and classification of malware requires exploiting program behaviour and not merely syntactic or static features. One approach is to use system call dependency graphs (SCDGs) that represent the program behaviour by interactions with the system, and the relations between these interactions. These SCDGs have been used with supervised learning techniques to very accurately detect and classify malware. This works considers the unsupervised learning challenge of mining for common clusters of behaviour without a priori knowledge. This allows for clustering of similar programs by behaviour, that can then be used for either classification or further analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3412841.3441919"
    },
    {
        "title": "Text Mining for Evaluating Authors' Birth and Death Years",
        "authors": [
            "Dror Moghaz",
            "Yaakov Hacohen-Kerner",
            "Dov Gabbay"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "This article presents a unique method in text and data mining for finding the era, i.e., mining temporal data, in which an anonymous author was living. Finding this era can assist in the examination of a fake document or extracting the time period in which a writer lived. The study and the experiments concern Hebrew, and in some parts, Aramaic and Yiddish rabbinic texts. The rabbinic texts are undated and contain no bibliographic sections, posing an interesting challenge. This work proposes algorithms using key phrases and key words that allow the temporal organization of citations together with linguistic patterns. Based on these key phrases, key words, and the references, we established several types of “Iron-clad,” Heuristic and Greedy rules for estimating the years of birth and death of a writer in an interesting classification task. Experiments were conducted on corpora, including documents authored by 12, 24, and 36 rabbinic writers and demonstrated promising results.",
        "link": "https://dl.acm.org/doi/10.1145/3281631"
    },
    {
        "title": "Transfer Learning to Timed Text Based Video Classification Using CNN",
        "authors": [
            "Zenun Kastrati",
            "Ali Shariq Imran",
            "Arianit Kurti"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "Open educational video resources are gaining popularity with a growing number of massive open online courses (MOOCs). This has created a niche for content providers to adopt effective solutions in automatically organizing and structuring of educational resources for maximum visibility. Recent advances in deep learning techniques are proving useful in managing and classifying resources into appropriate categories. This paper proposes one such convolutional neural network (CNN) model for classifying video lectures in a MOOC setting using a transfer learning approach. The model uses a time-aligned text transcripts corresponding to video lectures from six broader subject categories. Video lectures and their corresponding transcript dataset is gathered from the Coursera MOOC platform. Two different CNN models are proposed: i) CNN based classification using embeddings learned from our MOOC dataset, ii) CNN based classification using transfer learning. Word embeddings generated from two well known state-of-the-art pre-trained models Word2Vec and GloVe, are used in the transfer learning approach for the second case. The proposed CNN models are evaluated using precision, recall, and F1 score and the obtained performance is compared with both conventional and deep learning classifiers. The proposed CNN models have an F1 score improvement of 10-22 percentage points over DNN and conventional classifiers",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326483"
    },
    {
        "title": "Evolutionary role mining in complex networks by ensemble clustering",
        "authors": [
            "Sarvenaz Choobdar",
            "Pedro Ribeiro",
            "Fernando Silva"
        ],
        "date": "April 2017",
        "source": "SAC '17: Proceedings of the Symposium on Applied Computing",
        "abstract": "The structural patterns in the neighborhood of nodes assign unique roles to the nodes. Mining the set of existing roles in a network provides a descriptive profile of the network and draws its general picture. This paper proposes a new method to determine structural roles in a dynamic network based on the current position of nodes and their historic behavior. We develop a temporal ensemble clustering technique to dynamically find groups of nodes, holding similar tempo-structural roles. We compare two weighting functions, based on age and distribution of data, to incorporate temporal behavior of nodes in the role discovery. To evaluate the performance of the proposed method, we assess the results from two points of view: 1) goodness of fit to current structure of the network; 2) consistency with historic data. We conduct the evaluation using different ensemble clustering techniques. The results on real world networks demonstrate that our method can detect tempo-structural roles that simultaneously depict the topology of a network and reflect its dynamics with high accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3019612.3019815"
    },
    {
        "title": "Analyzing used-car web listings via text mining",
        "authors": [
            "Ayhan Demiriz",
            "Fatma Cantaş"
        ],
        "date": "October 2017",
        "source": "IML '17: Proceedings of the 1st International Conference on Internet of Things and Machine Learning",
        "abstract": "Used car trade is one of the major components of the world economies. It is not uncommon to sell a car by placing an internet advertisement irrespective of the geography in these days. A typical content of an advertisement is usually composed of two parts namely the structured and the free text data. The structured data may include some information about the asking price, make, model, year, mileage of the car and the contact info. In most cases, seller may give important clues about the car's current conditions in the free text data where the title (head) of the advertisement can be included as free text too. This paper reports preliminary results from a text mining study conducted on 75K used car internet listings collected from two major car listing web sites in Turkey. As expected, the words and the phrases related to the description of the car are observed to be frequent. The leading concepts in the free text are found to be regarding how to describe the current condition of a car, for example \"no crash history\".",
        "link": "https://dl.acm.org/doi/10.1145/3109761.3109782"
    },
    {
        "title": "Text classification of Diseases Treated by Traditional Chinese Medicine Prescription based on machine learning",
        "authors": [
            "Hanqing Zhao",
            "Bo Han",
            "Chen Li"
        ],
        "date": "September 2020",
        "source": "ISAIMS '20: Proceedings of the 1st International Symposium on Artificial Intelligence in Medical Sciences",
        "abstract": "OBJECTIVE: To explore the application of machine learning in th e identification of diseases treated by traditional Chinese medicine prescriptions. METHODS: Based on the composition of the text document of Chinese medicine prescriptions, the prescriptions were divided into cough, headache and diarrhea. THUTCT were introduced to establish and train two machine learning text classification models, LibSVM and LibLinear, and the prescriptions to be deter mined were put into the model for classification and prediction. R ESULTS: The Precision rate of LibSVM model and Liblinear mod el were 0.7283 and 0.6690. Seven prescriptions were classified an d predicted, and the results were in line with expectations. CONCLUSION: THUCTC has good universality for the content of TCM prescriptions, high classification accuracy and fast testing speed, which is suitable for the text classification and discrimination research of TCM prescriptions for diseases.",
        "link": "https://dl.acm.org/doi/10.1145/3429889.3429895"
    },
    {
        "title": "Evaluation of retweet clustering method classification method using retweets on Twitter without text data",
        "authors": [
            "K. Uchida",
            "F. Toriumi",
            "T. Sakaki"
        ],
        "date": "August 2017",
        "source": "WI '17: Proceedings of the International Conference on Web Intelligence",
        "abstract": "Burst phenomena, which frequently occur on social media, are caused by such social events as flaming on the internet, elections, and natural disasters. To understand people's thoughts and feelings, we must classify their opinions from burst phenomena. Therefore, classification methods that categorize tweets are critical. However, since most classification methods focus on text mining, they cannot group tweets by topics because each tweet has poor linguistic similarities. We used a non-text-based classification method proposed by Baba et al. that groups tweets by topics, even if they have poor linguistic similarities, and verified its validity by comparing it with a text-based classification method in two different evaluations: qualitative and quantitative. In the qualitative evaluation part, we did a questionnaire survey and validated the suitability of the topic clusters created using both the non-and text-based methods. Since evaluating the similarity of every pair of tweets in each topic is difficult, we evaluated the similarity between sampled pairs in the survey and acquired more appropriate topic clustering results using the non-text-based method than the text-based method. In the quantitative evaluation part, we focused on the robustness of each method against data reduction. Many approaches analyze social media data, especially because collecting data from social media is comparatively easy. However, since collecting the whole data of burst phenomena is very costly due to the vast amounts of available social media data, robustness against data reduction is an important index to evaluate classification methods. With the non-text-based method, over 55% of the pairs of tweets in the same cluster were also included in the same cluster even when the data were reduced to 10% in all three of our example cases. In this paper, as a source we focus on Twitter, one of the most popular microblogging services. Using clustering to conduct detailed case analyses, we scrutinized three burst cases that include natural disasters and flaming on the internet and found that a non-text-based method more effectively classified tweets in burst phenomena than a text-based method.",
        "link": "https://dl.acm.org/doi/10.1145/3106426.3106451"
    },
    {
        "title": "Using Spark for Text Mining on Large Scale Liver Cancer Literature",
        "authors": [
            "Ming-Yen Lin",
            "Yu-Ju Lin",
            "Sue-Chen Hsueh"
        ],
        "date": "January 2021",
        "source": "BDET 2021: 2021 the 3rd International Conference on Big Data Engineering and Technology (BDET)",
        "abstract": "Cancer is one of the main causes of death. The number of known cancer to-date is more than one hundred. Liver cancer, ranked after trachea and lung cancer, has long been high in the cancer leading cause in Taiwan and even ranked as second in 2016. The number of scientific articles related to cancer proliferates every year. The number reaches as high as 20 million in PubMed so that discovering useful information from the massive collection is very difficult. In addition, using a single machine to sift through these articles is very time-consuming. Therefore, we present a big data analytic framework using the distributed Apache Spark platform for text mining in PubMed literature. We establish a prediction model for liver cancer articles so that researchers may effectively validate whether an article is related to liver cancer or not. Classification models in Spark MLlib including Linear Support Vector Machines (SVM), Logistic Regression, are used in our experiments. Relevancy to liver cancer is further confirmed by using MeSH (Medical Subject Headings) terms. Logistic regression is about 3 times faster than SVMs and the accuracy of both methods is close to 95% in the experiments using hold-out validation. When max_features is 500 and min_df ≤ 0.1 (or min_df = 1), the accuracy may reach 96%. In the experiments with K-fold cross-validation, the accuracy of SVMs methods is 96%. The experimental results show that that our prediction model may effectively classify liver cancer articles.",
        "link": "https://dl.acm.org/doi/10.1145/3474944.3474958"
    },
    {
        "title": "Automatic topic classification of test cases using text mining at an Android smartphone vendor",
        "authors": [
            "Junji Shimagaki",
            "Yasutaka Kamei",
            "Naoyasu Ubayashi",
            "Abram Hindle"
        ],
        "date": "October 2018",
        "source": "ESEM '18: Proceedings of the 12th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Background: An Android smartphone is an ecosystem of applications, drivers, operating system components, and assets. The volume of the software is large and the number of test cases needed to cover the functionality of an Android system is substantial. Enormous effort has been already taken to properly quantify \"what features and apps were tested and verified?\". This insight is provided by dashboards that summarize test coverage and results per feature. One method to achieve this is to manually tag or label test cases with the topic or function they cover, much like function points. At the studied Android smartphone vendor, tests are labelled with manually defined tags, so-called \"feature labels (FLs)\", and the FLs serve to categorize 100s to 1000s test cases into 10 to 50 groups. Aim: Unfortunately for developers, manual assignment of FLs to 1000s of test cases is a time consuming task, leading to inaccurately labeled test cases, which will render the dashboard useless. We created an automated system that suggests tags/labels to the developers for their test cases rather than manual labeling. Method: We use machine learning models to predict and label the functionality tested by 10,000 test cases developed at the company. Results: Through the quantitative experiments, our models achieved acceptable F-1 performance of 0.3 to 0.88. Also through the qualitative studies with expert teams, we showed that the hierarchy and path of tests was a good predictor of a feature's label. Conclusions: We find that this method can reduce tedious manual effort that software developers spent classifying test cases, while providing more accurate classification results.",
        "link": "https://dl.acm.org/doi/10.1145/3239235.3268927"
    },
    {
        "title": "To apply Data Mining for Classification of Crowd sourced Software Requirements",
        "authors": [
            "Soonh Taj",
            "Qasim Arain",
            "Imran Memon",
            "Asma Zubedi"
        ],
        "date": "April 2019",
        "source": "ICSIE '19: Proceedings of the 8th International Conference on Software and Information Engineering",
        "abstract": "Now a day's main focus of developers is to build quality software that works according to customer needs and for this reason it is necessary to gather right requirements as requirement elicitation is the critical step that impacts on the success of software project as misinterpreted requirements leads to the failure of software project. By keeping this in mind a research is carried out on improving requirements elicitation process and automating the process of classifying requirements. In this research, a model is proposed which will help in this scenario for requirements elicitation and requirement classification. This paper presents a model in which crowd sourcing approach is used so that customers, end users, stakeholders, developers and software engineers can make active participation for requirement elicitation process and requirements gathered using crowdsourcing approach are used by model for classification process i.e. classification of requirements into functional and non-functional requirements. For the proof of proposed model a case study is conducted. Results of case study provided the usefulness and efficiency of proposed model for classification of crowd sourced software requirements.",
        "link": "https://dl.acm.org/doi/10.1145/3328833.3328837"
    },
    {
        "title": "Text classification using Fuzzy TF-IDF and Machine Learning Models",
        "authors": [
            "Mariem Bounabi",
            "Karim El Moutaouakil",
            "Khalid Satori"
        ],
        "date": "October 2019",
        "source": "BDIoT'19: Proceedings of the 4th International Conference on Big Data and Internet of Things",
        "abstract": "The representation of the information has an important impact on the text classification task. Several weighting methods were proposed in the literature, and the term frequency-inverse term frequency (TFIDF), the most know on the text treatment field. The FTF-IDF is a vector representation where the components of the TFIDF are presented as inputs to the Fuzzy Inference System (FIS). In this work, we compare several Machin Learning algorithms such as Naïve Bayes and its derivatives, SVM and Random forest classifiers, using the FTF-IDF representation. To improve the quality of the used classifiers, we call sum attribute selection methods. The recognition rate, for the tested systems, is satisfied, where the system based on naïve Bayes classifier, the FTF-IDF weighting terms, and the info gain select attributes method gives 98.7% as accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3372938.3372956"
    },
    {
        "title": "Text Mining in Cybersecurity: A Systematic Literature Review",
        "authors": [
            "Luciano Ignaczak",
            "Guilherme Goldschmidt",
            "Cristiano André Da Costa",
            "Rodrigo Da Rosa Righi"
        ],
        "date": null,
        "source": "ACM Computing Surveys",
        "abstract": "The growth of data volume has changed cybersecurity activities, demanding a higher level of automation. In this new cybersecurity landscape, text mining emerged as an alternative to improve the efficiency of the activities involving unstructured data. This article proposes a Systematic Literature Review (SLR) to present the application of text mining in the cybersecurity domain. Using a systematic protocol, we identified 2,196 studies, out of which 83 were summarized. As a contribution, we propose a taxonomy to demonstrate the different activities in the cybersecurity domain supported by text mining. We also detail the strategies evaluated in the application of text mining tasks and the use of neural networks to support activities involving unstructured data. The work also discusses text classification performance aiming its application in real-world solutions. The SLR also highlights open gaps for future research, such as the analysis of non-English content and the intensification in the usage of neural networks.",
        "link": "https://dl.acm.org/doi/10.1145/3462477"
    },
    {
        "title": "Using Text Mining to Analyze the Financial Patents",
        "authors": [
            "Yung-Feng Lu",
            "Jia-Lang Xu",
            "Mu-Yen Chen"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "With the advancement and development of science and technology, Nowadays society is the development of information technology, cloud computing, mobile Internet, Internet of Things, block chain and information technology.The combination of finance and technology has made financial technology terms gradually emerging.. This research analyzes the patents of financial technology. However, Taiwan's financial technology patents are mostly G06Q-20 (payment plan, architecture or agreement), G06Q-30 (commercial, such as marketing, shopping, payment, auction or e-commerce). G06Q-40 (financial, investment or tax treatment, insurance).The research use text mining technology to analyze and understand Taiwan's trends and developments in financial technology. The research results show that G06Q-20 patents are biased towards information security and mobile payment. G06Q-30 patents favor shopping websites and advertising. G06Q-40 patents are biased towards diversity, such as securities, insurance, finance, and stocks.",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326478"
    },
    {
        "title": "Identifying Online Profiles of Distance Learning Students Using Data Mining Techniques",
        "authors": [
            "Osama Islam",
            "Muazzam Siddiqui",
            "Naif Radi Aljohani"
        ],
        "date": "October 2019",
        "source": "ICDTE '19: Proceedings of the 3rd International Conference on Digital Technology in Education",
        "abstract": "Educational data has grown over the years with the increased use of technology within educational environments. This has led to a huge amount of data being stored in various data sources representing the student, his/her activities, and other aspects relevant to the learning process. To meet this analytical need, Educational Data Mining (EDM) has emerged to assist educational institutions in identifying key benefits such as students at risk, the level of student engagement or predicting student performance. The aim of this research was to explore the various aspects of student interaction data using data mining techniques to identify relevant patterns of behaviors and possible key attributes that have higher degrees of influence on distance learning students. The main findings identified several patterns of user online profiles based on a set of adopted learning strategies, the research proposed a framework for analyzing such interaction data based on R and Hadoop platforms to correlate online profiles with student performance.",
        "link": "https://dl.acm.org/doi/10.1145/3369199.3369249"
    },
    {
        "title": "Extremely Fast Decision Tree Mining for Evolving Data Streams",
        "authors": [
            "Albert Bifet",
            "Jiajin Zhang",
            "Wei Fan",
            "Cheng He",
            "Jianfeng Zhang",
            "Jianfeng Qian",
            "Geoff Holmes",
            "Bernhard Pfahringer"
        ],
        "date": "August 2017",
        "source": "KDD '17: Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
        "abstract": "Nowadays real-time industrial applications are generating a huge amount of data continuously every day. To process these large data streams, we need fast and efficient methodologies and systems. A useful feature desired for data scientists and analysts is to have easy to visualize and understand machine learning models. Decision trees are preferred in many real-time applications for this reason, and also, because combined in an ensemble, they are one of the most powerful methods in machine learning. In this paper, we present a new system called STREAMDM-C++, that implements decision trees for data streams in C++, and that has been used extensively at Huawei. Streaming decision trees adapt to changes on streams, a huge advantage since standard decision trees are built using a snapshot of data, and can not evolve over time. STREAMDM-C++ is easy to extend, and contains more powerful ensemble methods, and a more efficient and easy to use adaptive decision trees. We compare our new implementation with VFML, the current state of the art implementation in C, and show how our new system outperforms VFML in speed using less resources.",
        "link": "https://dl.acm.org/doi/10.1145/3097983.3098139"
    },
    {
        "title": "Data miners' little helper: data transformation activity cues for cluster analysis on document collections",
        "authors": [
            "Tania Cerquitelli",
            "Evelina Di Corso",
            "Francesco Ventura",
            "Silvia Chiusano"
        ],
        "date": "June 2017",
        "source": "WIMS '17: Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "In this paper we propose a new self-learning engine to streamline the analytics process, as it enables analysts to mine massive data repositories with minimal user intervention. In the context of cluster analysis on a collection of documents this new system, named SELF-DATA (SELF-learning DAta TrAnsformation), suggests to the analyst how to configure the whole mining process for a given dataset. SELF-DATA relies on an engine exploring different data weighting schemas (e.g., normalized term frequencies) and data transformation methods (e.g., PCA) before applying the cluster analysis, evaluating and comparing solutions through different quality indices (e.g., weighted Silhouette), and presenting the k-top solutions to the analyst. SELF-DATA will also include a knowledge base storing results of experiments on previously processed datasets, and a classification algorithm trained on the knowledge base content to forecast the best configuration for the whole mining process for an unexplored dataset. The first development of SELF-DATA running on Apache Spark has been validated on 5 collections of documents. Experimental results highlight that TF-IDF and logarithmic entropy are effective to measure item relevance with sparse datasets, and the LSI method outperforms PCA with a large dictionary.",
        "link": "https://dl.acm.org/doi/10.1145/3102254.3102288"
    },
    {
        "title": "Interactive Visual Graph Mining and Learning",
        "authors": [
            "Ryan A. Rossi",
            "Nesreen K. Ahmed",
            "Rong Zhou",
            "Hoda Eldardiry"
        ],
        "date": null,
        "source": "ACM Transactions on Intelligent Systems and Technology",
        "abstract": "This article presents a platform for interactive graph mining and relational machine learning called GraphVis. The platform combines interactive visual representations with state-of-the-art graph mining and relational machine learning techniques to aid in revealing important insights quickly as well as learning an appropriate and highly predictive model for a particular task (e.g., classification, link prediction, discovering the roles of nodes, and finding influential nodes). Visual representations and interaction techniques and tools are developed for simple, fast, and intuitive real-time interactive exploration, mining, and modeling of graph data. In particular, we propose techniques for interactive relational learning (e.g., node/link classification), interactive link prediction and weighting, role discovery and community detection, higher-order network analysis (via graphlets, network motifs), among others. GraphVis also allows for the refinement and tuning of graph mining and relational learning methods for specific application domains and constraints via an end-to-end interactive visual analytic pipeline that learns, infers, and provides rapid interactive visualization with immediate feedback at each change/prediction in real-time. Other key aspects include interactive filtering, querying, ranking, manipulating, exporting, as well as tools for dynamic network analysis and visualization, interactive graph generators (including new block model approaches), and a variety of multi-level network analysis techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3200764"
    },
    {
        "title": "Data Mining and Opinion Mining: A Tool in Educational Context",
        "authors": [
            "Myriam Peñafiel",
            "Stefanie Vásquez",
            "Diego Vásquez",
            "Juan Zaldumbide",
            "Sergio Luján-Mora"
        ],
        "date": "July 2018",
        "source": "ICoMS '18: Proceedings of the 2018 1st International Conference on Mathematics and Statistics",
        "abstract": "The use of the web as a universal communication platform generates large volumes of data (Big data), which in many cases, need to be processed so that they can become useful knowledge in face of the sceptics who have doubts about the credibility of such information. The use of web data that comes from educational contexts needs to be addressed, since that large amount of unstructured information is not being valued, losing valuable information that can be used. To solve this problem, we propose the use of data mining techniques such as sentiment analysis to validate the information that comes from the educational platforms. The objective of this research is to propose a methodology that allows the user to apply sentiment analysis in a simple way, because although some researchers have done it, very few do with data in the educational context. The results obtained prove that the proposal can be used in similar cases.",
        "link": "https://dl.acm.org/doi/10.1145/3274250.3274263"
    },
    {
        "title": "Research Hotspots Mining and Visualized Analysis Based on Linking Cluster and K-Core Decomposition",
        "authors": [
            "Ying Cai",
            "Fang Huang",
            "Shuai Wang",
            "Hao Zhang",
            "Chunxiu Du"
        ],
        "date": "May 2018",
        "source": "ICDPA 2018: Proceedings of the International Conference on Data Processing and Applications",
        "abstract": "In this paper, we focus on research hotspots mining with knowledge semantic features on discipline topic word networks, which are extracted from scientific and technical documents. For the topic networks, a novel method of combining linking community clustering with k-core decomposition is proposed to mine the hotspot community with knowledge hierarchy structure. And as a post-processing, the hierarchical community identification is implemented by k-core decomposition in order to discover the internal connection with knowledge semantic hierarchy in the research hotspot. Finally, the community density and visualization method is utilized to analyze the core and sub- problem in the research hotspot community. In experiments, the topic networks for a single discipline and interdisciplinary research hotspot are taken as examples of analysis, and the results show the proposed approach can effectively realize the objective of mining and analyzing the discipline research hotspot on the academic topic word network.",
        "link": "https://dl.acm.org/doi/10.1145/3224207.3224216"
    },
    {
        "title": "The Application of Data Mining Algorithms in the Construction of Travel Recommendation System",
        "authors": [
            "Chao Lou",
            "Zhaonan Mu",
            "Mengzhu Liu"
        ],
        "date": "January 2021",
        "source": "CONF-CDS 2021: The 2nd International Conference on Computing and Data Science",
        "abstract": "Photos shared by users of online social network often associated with tags, texts, geographic and time information. These data are ideal research resource which can be used by researchers to observe people's interests and behaviors. They can also be used to analyze people's travel behaviors and interests because it can reflect people's travel activities and experiences directly. This project aimed to consolidate people into groups based on their travel preferences of travel season, travel time span, scenery type of travel location and then analyze popular travel places for different people groups. The travel preferences were analyzed from extracted data applying PHP language and Naive Bayes classification approach. According to the analyzed data, people were divided into groups. Then, DBSCAN clustering was used to get popular locations based on the different groups. As a result, 80 thousand photo data of 5 cities have been collected from Flickr by Phython. It has been showed that people's preference of season in most of the mined cities was spring and summer and the preference of time span was short. Moreover, mined people has been divided into 16 groups according to their preference and based on the groups popular locations have been calculated by DBSCAN clustering. Theses locations have been provided in a recommendation website which was built in this project. For the future work, mining Flickr data associated with other travel social network sites could be a good way to build a more effective recommendation system.",
        "link": "https://dl.acm.org/doi/10.1145/3448734.3450477"
    },
    {
        "title": "Compare Machine Learning Models in Text Classification Using Steam User Reviews",
        "authors": [
            "Youchen Miao",
            "Zeyu Jin",
            "Yumeng Zhang",
            "Yuchen Chen",
            "Junren Lai"
        ],
        "date": "November 2021",
        "source": "ICSED '21: Proceedings of the 2021 3rd International Conference on Software Engineering and Development",
        "abstract": "Text Classification and Sentiment Analysis of game reviews are viewed as important parts in not only academic fields but also in game studies. In this paper, with more than 400 thousand game reviews on Steam platform, we preprocess the data using different libraries (sklearn, nltk, and spaCy) and use them as inputs to build three sentiment classification models based on different algorithms (Naive Bayes, SVM, and Random Forest). In contrast to previous studies that only focus on different sentiment analysis models, our paper also highlights the use of different APIs to preprocess the data and their corresponding model performance. The results show that no matter which API we choose, Random Forest models always perform the best. However, in terms of training time, Naive Bayes is the fastest. This work can be used to apply grid search for researchers to automatically find the optimum API before conducting sentiment analysis in the future.",
        "link": "https://dl.acm.org/doi/10.1145/3507473.3507480"
    },
    {
        "title": "Lost in Transduction: Transductive Transfer Learning in Text Classification",
        "authors": [
            "Alejandro Moreo",
            "Andrea Esuli",
            "Fabrizio Sebastiani"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Obtaining high-quality labelled data for training a classifier in a new application domain is often costly. Transfer Learning\n(a.k.a. “Inductive Transfer”) tries to alleviate these costs by transferring, to the “target” domain of interest, knowledge available from a different “source” domain. In transfer learning the lack of labelled information from the target domain is compensated by the availability at training time of a set of unlabelled examples from the target distribution. Transductive Transfer Learning denotes the transfer learning setting in which the only set of target documents that we are interested in classifying is known and available at training time. Although this definition is indeed in line with Vapnik’s original definition of “transduction”, current terminology in the field is confused. In this article, we discuss how the term “transduction” has been misused in the transfer learning literature, and propose a clarification consistent with the original characterization of this term given by Vapnik. We go on to observe that the above terminology misuse has brought about misleading experimental comparisons, with inductive transfer learning methods that have been incorrectly compared with transductive transfer learning methods. We then, give empirical evidence that the difference in performance between the inductive version and the transductive version of a transfer learning method can indeed be statistically significant (i.e., that knowing at training time the only data one needs to classify indeed gives an advantage). Our clarification allows a reassessment of the field, and of the relative merits of the major, state-of-the-art algorithms for transfer learning in text classification.",
        "link": "https://dl.acm.org/doi/10.1145/3453146"
    },
    {
        "title": "Machine Learning on Mining Potential Adverse Drug Reactions for Pharmacovigilance",
        "authors": [
            "Ke Jia"
        ],
        "date": "July 2021",
        "source": "DSIT 2021: 2021 4th International Conference on Data Science and Information Technology",
        "abstract": "Drugs are related to the lives and health of patients. Mining and discovering potential adverse drug reactions from a large number of unstructured texts play an important role in drug research and pharmacovigilance. It is a hot research issue in the field of biomedicine in recent years. A large number of scholars have discovered a large amount of information on adverse drug reactions from different data sources through effective machine learning methods. This paper reviews the research progress in recent years to provide a reference for future research.",
        "link": "https://dl.acm.org/doi/10.1145/3478905.3478964"
    },
    {
        "title": "TACAM: Topic And Context Aware Argument Mining",
        "authors": [
            "Michael Fromm",
            "Evgeniy Faerman",
            "Thomas Seidl"
        ],
        "date": "October 2019",
        "source": "WI '19: IEEE/WIC/ACM International Conference on Web Intelligence",
        "abstract": "In this work we address the problem of argument search. The purpose of argument search is the distillation of pro and contra arguments for requested topics from large text corpora. In previous works, the usual approach is to use a standard search engine to extract text parts which are relevant to the given topic and subsequently use an argument recognition algorithm to select arguments from them. The main challenge in the argument recognition task, which is also known as argument mining, is that often sentences containing arguments are structurally similar to purely informative sentences without any stance about the topic. In fact, they only differ semantically. Most approaches use topic or search term information only for the first search step and therefore assume that arguments can be classified independently of a topic. We argue that topic information is crucial for argument mining, since the topic defines the semantic context of an argument. Precisely, we propose different models for the classification of arguments, which take information about a topic of an argument into account. Moreover, to enrich the context of a topic and to let models understand the context of the potential argument better, we integrate information from different external sources such as Knowledge Graphs or pre-trained NLP models. Our evaluation shows that considering topic information, especially in connection with external information, provides a significant performance boost for the argument mining task.",
        "link": "https://dl.acm.org/doi/10.1145/3350546.3352506"
    },
    {
        "title": "Text mining for incoming tasks based on the urgency/importance factors and task classification using machine learning tools",
        "authors": [
            "Yasser Ali Alshehri"
        ],
        "date": "March 2020",
        "source": "ICCDA 2020: Proceedings of the 2020 the 4th International Conference on Compute and Data Analysis",
        "abstract": "In workplaces, there is a massive amount of unstructured data from different sources. In this paper, we present a case study that explains how can through communications between employees, we can help to prioritize tasks requests to increase the efficiency of their works for both technical and non-technical workers. This involves managing daily incoming tasks based on their level of urgency and importance.To allow all workers to utilize the urgency-importance matrix as a time-management tool, we need to automate this tool. The textual content of incoming tasks are analyzed, and metrics related to urgency and importance are extracted. A third factor (i.e., the response variable) is defined based on the two input variables (urgency and importance). Then, machine learning applied to the data to predict the class of incoming tasks based on data outcome desired. We used ordinal regression, neural networks, and decision tree algorithms to predict the four levels of task priority. We measure the performance of all using recalls, precisions, and F-scores. All classifiers perform higher than 89% in terms of all measures.",
        "link": "https://dl.acm.org/doi/10.1145/3388142.3388153"
    },
    {
        "title": "Text mining for malware classification using multivariate all repeated patterns detection",
        "authors": [
            "Konstantinos F. Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2019",
        "source": "ASONAM '19: Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Mobile phones have become nowadays a commodity to the majority of people. Using them, people are able to access the world of Internet and connect with their friends, their colleagues at work or even unknown people with common interests. This proliferation of the mobile devices has also been seen as an opportunity for the cyber criminals to deceive smartphone users and steel their money directly or indirectly, respectively, by accessing their bank accounts through the smartphones or by blackmailing them or selling their private data such as photos, credit card data, etc. to third parties. This is usually achieved by installing malware to smartphones masking their malevolent payload as a legitimate application and advertise it to the users with the hope that mobile users will install it in their devices. Thus, any existing application can easily be modified by integrating a malware and then presented it as a legitimate one. In response to this, scientists have proposed a number of malware detection and classification methods using a variety of techniques. Even though, several of them achieve relatively high precision in malware classification, there is still space for improvement. In this paper, we propose a text mining all repeated pattern detection method which uses the decompiled files of an application in order to classify a suspicious application into one of the known malware families. Based on the experimental results using a real malware dataset, the methodology tries to correctly classify (without any misclassification) all randomly selected malware applications of 3 categories with 3 different families each.",
        "link": "https://dl.acm.org/doi/10.1145/3341161.3350841"
    },
    {
        "title": "Comparative Study between Traditional Machine Learning and Deep Learning Approaches for Text Classification",
        "authors": [
            "Cannannore Nidhi Kamath",
            "Syed Saqib Bukhari",
            "Andreas Dengel"
        ],
        "date": "August 2018",
        "source": "DocEng '18: Proceedings of the ACM Symposium on Document Engineering 2018",
        "abstract": "In this contemporaneous world, it is an obligation for any organization working with documents to end up with the insipid task of classifying truckload of documents, which is the nascent stage of venturing into the realm of information retrieval and data mining. But classification of such humongous documents into multiple classes, calls for a lot of time and labor. Hence a system which could classify these documents with acceptable accuracy would be of an unfathomable help in document engineering. We have created multiple classifiers for document classification and compared their accuracy on raw and processed data. We have garnered data used in a corporate organization as well as publicly available data for comparison. Data is processed by removing the stop-words and stemming is implemented to produce root words. Multiple traditional machine learning techniques like Naive Bayes, Logistic Regression, Support Vector Machine, Random forest Classifier and Multi-Layer Perceptron are used for classification of documents. Classifiers are applied on raw and processed data separately and their accuracy is noted. Along with this, Deep learning technique such as Convolution Neural Network is also used to classify the data and its accuracy is compared with that of traditional machine learning techniques. We are also exploring hierarchical classifiers for classification of classes and subclasses. The system classifies the data faster and with better accuracy than if done manually. The results are discussed in the results and evaluation section.",
        "link": "https://dl.acm.org/doi/10.1145/3209280.3209526"
    },
    {
        "title": "Identification of Overpricing in the Purchase of Medication by the Federal Government of Brazil, Using Text Mining and Clustering Based on Ontology",
        "authors": [
            "Marco Aurelio O. S. Correa",
            "Adriano Galindo Leal"
        ],
        "date": "August 2018",
        "source": "ICCBDC '18: Proceedings of the 2018 2nd International Conference on Cloud and Big Data Computing",
        "abstract": "Increasing the transparency level in his actions and spending is one of the primary duties of the Brazilian Federal Government. The creation of laws that oblige full disclose of all its expenditures through transparency portals enables citizens to supervise all government entities. However, only the dissemination of these data, without a definite standard or the availability of data analysis tools, does not guarantee that the citizen is empowered to play his role. Therefore hence, the objective of this work is to identify overprice in the acquisition of products purchased by the federal government of Brazil using the unstructured data available on the Transparency Portal. The last two-years' worth of purchasing data, available in the Transparency Portal, were extracted, processed and stored. Due to his diverse nature and high volume of data, this study focused only on medicines purchased by the Ministry of Health. Ontology-based text mining and clustering techniques were applied for automatic identification and classification of products. The processing of this information was done through text mining and clustering, based on the ontology registered in another database of the Brazilian government. Because of this work, a consolidated price base per medication was created to allow the identification of distortions in prices practised, facilitating the identification of cases that merit further investigation to unravel fraud to the treasury.",
        "link": "https://dl.acm.org/doi/10.1145/3264560.3264569"
    },
    {
        "title": "Computational Estimation by Scientific Data Mining with Classical Methods to Automate Learning Strategies of Scientists",
        "authors": [
            "Aparna S. Varde"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Experimental results are often plotted as 2-dimensional graphical plots (aka graphs) in scientific domains depicting dependent versus independent variables to aid visual analysis of processes. Repeatedly performing laboratory experiments consumes significant time and resources, motivating the need for computational estimation. The goals are to estimate the graph obtained in an experiment given its input conditions, and to estimate the conditions that would lead to a desired graph. Existing estimation approaches often do not meet accuracy and efficiency needs of targeted applications. We develop a computational estimation approach called AutoDomainMine that integrates clustering and classification over complex scientific data in a framework so as to automate classical learning methods of scientists. Knowledge discovered thereby from a database of existing experiments serves as the basis for estimation. Challenges include preserving domain semantics in clustering, finding matching strategies in classification, striking a good balance between elaboration and conciseness while displaying estimation results based on needs of targeted users, and deriving objective measures to capture subjective user interests. These and other challenges are addressed in this work. The AutoDomainMine approach is used to build a computational estimation system, rigorously evaluated with real data in Materials Science. Our evaluation confirms that AutoDomainMine provides desired accuracy and efficiency in computational estimation. It is extendable to other science and engineering domains as proved by adaptation of its sub-processes within fields such as Bioinformatics and Nanotechnology.",
        "link": "https://dl.acm.org/doi/10.1145/3502736"
    },
    {
        "title": "Where is the road for issue reports classification based on text mining?",
        "authors": [
            "Qiang Fan",
            "Yue Yu",
            "Gang Yin",
            "Tao Wang",
            "Huaimin Wang"
        ],
        "date": "November 2017",
        "source": "ESEM '17: Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Currently, open source projects receive various kinds of issues daily, because of the extreme openness of Issue Tracking System (ITS) in GitHub. ITS is a labor-intensive and time-consuming task of issue categorization for project managers. However, a contributor is only required a short textual abstract to report an issue in GitHub. Thus, most traditional classification approaches based on detailed and structured data (e.g., priority, severity, software version and so on) are difficult to adopt. In this paper, issue classification approaches on a large-scale dataset, including 80 popular projects and over 252,000 issue reports collected from GitHub, were investigated. First, four traditional text-based classification methods and their performances were discussed. Semantic perplexity (i.e., an issues description confuses bug-related sentences with nonbug-related sentences) is a crucial factor that affects the classification performances based on quantitative and qualitative study. Finally, A two-stage classifier framework based on the novel metrics of semantic perplexity of issue reports was designed. Results show that our two-stage classification can significantly improve issue classification performances.",
        "link": "https://dl.acm.org/doi/10.1109/ESEM.2017.19"
    },
    {
        "title": "Feature-based Facebook reviews process model for e-management using data mining",
        "authors": [
            "Anish Kumar Varudharajulu",
            "Yongsheng Ma"
        ],
        "date": "January 2019",
        "source": "IC4E '19: Proceedings of the 10th International Conference on E-Education, E-Business, E-Management and E-Learning",
        "abstract": "The data generated from online communication acts as potential gold mines for discovering knowledge for researchers. A large amount of data is also generated in the form of web documents, emails, blogs, and feedback, etc. Text analytics is being significantly employed to mine important information. Opinion mining is the process of extracting human thoughts and perceptions from unstructured texts. The showstopper for designing an opinion mining system for analyzing reviews arise from the fact that customer reviews are often noisy. These reviews are informally written. In addition, they are subjected to spelling mistakes, grammatical errors, improper punctuation and irrational capitalization. This paper focuses on analyzing the different classification and clustering algorithms aimed at extracting and consolidating opinions of customers from social media sites like Facebook, Twitter and through surveys, at multiple levels of granularity to monitor and measure customer satisfaction. Ours is an automated approach, in which the system aids in the process of knowledge assimilation for knowledge-based building and also performs the analytics. Domain experts ratify the knowledge base and also provide training datasets for the system to intuitively gather more instances for ratification. The system identifies opinion expressions as phrases containing opinion words, opinionated features and also opinion modifiers. These expressions are categorized as positive, negative or neutral. Opinion expressions are identified and categorized using localized linguistic techniques. Opinions can be congregated at any desired level of specificity i.e. feature level or product level, user level or service level, etc. We have developed a system based on this approach, which provides the user with a platform to analyze opinion expressions crawled from a set of pre-defined datasets.",
        "link": "https://dl.acm.org/doi/10.1145/3306500.3306514"
    },
    {
        "title": "Discriminative Topic Mining via Category-Name Guided Text Embedding",
        "authors": [
            "Yu Meng",
            "Jiaxin Huang",
            "Guangyuan Wang",
            "Zihan Wang",
            "Chao Zhang",
            "Yu Zhang",
            "Jiawei Han"
        ],
        "date": "April 2020",
        "source": "WWW '20: Proceedings of The Web Conference 2020",
        "abstract": "Mining a set of meaningful and distinctive topics automatically from massive text corpora has broad applications. Existing topic models, however, typically work in a purely unsupervised way, which often generate topics that do not fit users’ particular needs and yield suboptimal performance on downstream tasks. We propose a new task, discriminative topic mining, which leverages a set of user-provided category names to mine discriminative topics from text corpora. This new task not only helps a user understand clearly and distinctively the topics he/she is most interested in, but also benefits directly keyword-driven classification tasks. We develop CatE, a novel category-name guided text embedding method for discriminative topic mining, which effectively leverages minimal user guidance to learn a discriminative embedding space and discover category representative terms in an iterative manner. We conduct a comprehensive set of experiments to show that CatE mines high-quality set of topics guided by category names only, and benefits a variety of downstream applications including weakly-supervised classification and lexical entailment direction identification.",
        "link": "https://dl.acm.org/doi/10.1145/3366423.3380278"
    },
    {
        "title": "Comparative Study of Heart Disease Diagnosis Using Top Ten Data Mining Classification Algorithms",
        "authors": [
            "I. Ketut Agung Enriko"
        ],
        "date": "June 2019",
        "source": "ICFET '19: Proceedings of the 5th International Conference on Frontiers of Educational Technologies",
        "abstract": "Data mining has been used for many purposes, especially for prediction system. In healthcare, data mining algorithms often used in disease diagnosis. Meanwhile, heart disease is known as a primary cause of death over the years. Many studies have been performed in heart disease diagnosis using data mining methods. There are some popular data mining algorithms that can be used in heart disease diagnosis, for example, k-Nearest Neighbor, CART, and AdaBoost. The algorithms are used to analyze a sample of cardiovascular patients data and predict the heart disease type that they suffer. Some parameters are taken from the patient, including EKG morphology, blood pressure, and information about the existence of chest pain, shortness of breath, palpitation, and cold sweat. In this study, medical records data are collected from Harapan Kita Hospital and utilized as a dataset sample in this research. Top ten data mining classification algorithms are used in diagnosing heart disease from Harapan Kita Hospital data and examining their performance by checking the accuracy and speed.",
        "link": "https://dl.acm.org/doi/10.1145/3338188.3338220"
    },
    {
        "title": "Text Mining Approach for Identifying Research Trends",
        "authors": [
            "Snezhana Sulova"
        ],
        "date": "June 2021",
        "source": "CompSysTech '21: Proceedings of the 22nd International Conference on Computer Systems and Technologies",
        "abstract": "With the increase of unstructured data, the issues connected with automatic text processing, the categorization of documents and the discovery of topics have become objects of growing interest. In order to improve the process of grouping and processing research publications, we would like to propose a method based upon natural language processing. It is based on text mining technologies which aim to identify key tendencies in documents. It processes the content of publications by clustering and identifies the topics of each identified group. This analysis helps by identifying key tendencies as well as discovering emerging new areas of research. Publications from the research literature database, Scopus, were used to test the approach. The topic of the publications is “the application of digital technologies in the logistics business”. The experiments were completed using the RapidMiner Studio software.",
        "link": "https://dl.acm.org/doi/10.1145/3472410.3472433"
    },
    {
        "title": "Assessment of Vulnerability Severity using Text Mining",
        "authors": [
            "Georgios Spanos",
            "Lefteris Angelis",
            "Dimitrios Toloudis"
        ],
        "date": "September 2017",
        "source": "PCI '17: Proceedings of the 21st Pan-Hellenic Conference on Informatics",
        "abstract": "Software1 vulnerabilities are closely associated with information systems security, a major and critical field in today's technology. Vulnerabilities constitute a constant and increasing threat for various aspects of everyday life, especially for safety and economy, since the social impact from the problems that they cause is complicated and often unpredictable. Although there is an entire research branch in software engineering that deals with the identification and elimination of vulnerabilities, the growing complexity of software products and the variability of software production procedures are factors contributing to the ongoing occurrence of vulnerabilities, Hence, another area that is being developed in parallel focuses on the study and management of the vulnerabilities that have already been reported and registered in databases. The information contained in such databases includes, a textual description and a number of metrics related to vulnerabilities. The purpose of this paper is to investigate to what extend the assessment of the vulnerability severity can be inferred directly from the corresponding textual description, or in other words, to examine the informative power of the description with respect to the vulnerability severity. For this purpose, text mining techniques, i.e. text analysis and three different classification methods (decision trees, neural networks and support vector machines) were employed. The application of text mining to a sample of 70,678 vulnerabilities from a public data source shows that the description itself is a reliable and highly accurate source of information for vulnerability prioritization.",
        "link": "https://dl.acm.org/doi/10.1145/3139367.3139390"
    },
    {
        "title": "Clustering Algorithms for Spatial Data Mining",
        "authors": [
            "Chetashri Bhadane",
            "Ketan Shah"
        ],
        "date": "April 2020",
        "source": "ICGDA '20: Proceedings of the 2020 3rd International Conference on Geoinformatics and Data Analysis",
        "abstract": "With the advances in mobile and wireless technologies, there has been a rise in applications that track and share the users' geospatial data. People use several social networking sites such as Twitter, Facebook and Flickr, where they share their status updates. With the integration of Global Positioning System (GPS) with mobile phones, it is now possible to share one's locations on these social networks. GPS allows us to record and track a person's movement along with the timestamp. The data set obtained from these GPS logs is vast and is widely used to analyze the users' movement patterns. Specifically, we can find out significant locations based on the number of users present at that location and the time spent by them at such places. Once significant places have been identified, it is also possible to identify the semantic importance of these locations. This paper presents an overview of the clustering techniques used to find important places of interest using large GPS based mobility datasets. Four clustering algorithms, K-Means, DBSCAN, OPTICS and Hierarchical, are implemented, and performance is tested using real-time data of 50 users collected over 2--5 years. Performance summary depicts that K-Means and DBSCAN perform well for spatial data.",
        "link": "https://dl.acm.org/doi/10.1145/3397056.3397068"
    },
    {
        "title": "Feature Selection based Arabic Text Classification using Different Machine Learning Algorithms: Comparative Study",
        "authors": [
            "Sakina Rim Bennabi",
            "Zakaria Elberrichi"
        ],
        "date": "June 2020",
        "source": "ICIST '20: Proceedings of the 10th International Conference on Information Systems and Technologies",
        "abstract": "Feature selection is a method of data pre-processing widely used when mining large data, such as textual classification. Several studies have been conducted to compare the different methods of feature selection applied to corpora in English. Unfortunately, a small number of works concern the Arabic language. This article aims to present a comparative study of different feature selection techniques including: Chi2, the ANOVA method and mutual information, applied on a corpus in Arabic language, while also diversifying the machine learning algorithms (Naive Bayes, SVM and KNN). This experimental study has shown in general that reducing dimensionality with feature selection techniques has slightly affected the performance of textual classification, reducing the size of the corpus by up to 1%.",
        "link": "https://dl.acm.org/doi/10.1145/3447568.3448531"
    },
    {
        "title": "Intelligent Data Analysis using Optimized Support Vector Machine Based Data Mining Approach for Tourism Industry",
        "authors": [
            "Ms Promila Sharma",
            "Uma Meena",
            "Girish Kumar Sharma"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Data analysis involves the deployment of sophisticated approaches from data mining methods, information theory, and artificial intelligence in various fields like tourism, hospitality, and so on for the extraction of knowledge from the gathered and preprocessed data. In tourism, pattern analysis or data analysis using classification is significant for finding the patterns that represent new and potentially useful information or knowledge about the destination and other data. Several data mining techniques are introduced for the classification of data or patterns. However, overfitting, less accuracy, local minima, sensitive to noise are the drawbacks in some existing data mining classification methods. To overcome these challenges, Support vector machine with Red deer optimization (SVM-RDO) based data mining strategy is proposed in this article. Extended Kalman filter (EKF) is utilized in the first phase, i.e., data cleaning to remove the noise and missing values from the input data. Mantaray foraging algorithm (MaFA) is used in the data selection phase, in which the significant data are selected for the further process to reduce the computational complexity. The final phase is the classification, in which SVM-RDO is proposed to access the useful pattern from the selected data. PYTHON is the implementation tool used for the experiment of the proposed model. The experimental analysis is done to show the efficacy of the proposed work. From the experimental results, the proposed SVM-RDO achieved better accuracy, precision, recall, and F1 score than the existing methods for the tourism dataset. Thus, it is showed the effectiveness of the proposed SVM-RDO for pattern analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3494566"
    },
    {
        "title": "Applying text mining to predict learners' cognitive engagement",
        "authors": [
            "Hayati Hind",
            "Mohammed Khalidi Idrissi",
            "Samir Bennani"
        ],
        "date": "October 2017",
        "source": "SCAMS '17: Proceedings of the Mediterranean Symposium on Smart City Application",
        "abstract": "In academic process, engagement represent one of the important success' factor. This paper proposes a predictive system for learners' cognitive engagement based on their online discussion forums participation. In the first level the ontology OWL and the LSA method are used to perform a semantic classification system of the threads according to a specific context chosen by the tutor. Then the Text mining, as a predictive method, is applied to the classified threads and learners' participation in the forums.",
        "link": "https://dl.acm.org/doi/10.1145/3175628.3175655"
    },
    {
        "title": "Investigate Transitions into Drug Addiction through Text Mining of Reddit Data",
        "authors": [
            "John Lu",
            "Sumati Sridhar",
            "Ritika Pandey",
            "Mohammad Al Hasan",
            "Georege Mohler"
        ],
        "date": "July 2019",
        "source": "KDD '19: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Increasing rates of opioid drug abuse and heightened prevalence of online support communities underscore the necessity of employing data mining techniques to better understand drug addiction using these rapidly developing online resources. In this work, we obtained data from Reddit, an online collection of forums, to gather insight into drug use/misuse using text snippets from users narratives. Specifically, using users' posts, we trained a binary classifier which predicts a user's transitions from casual drug discussion forums to drug recovery forums. We also proposed a Cox regression model that outputs likelihoods of such transitions. In doing so, we found that utterances of select drugs and certain linguistic features contained in one's posts can help predict these transitions. Using unfiltered drug-related posts, our research delineates drugs that are associated with higher rates of transitions from recreational drug discussion to support/recovery discussion, offers insight into modern drug culture, and provides tools with potential applications in combating the opioid crisis.",
        "link": "https://dl.acm.org/doi/10.1145/3292500.3330737"
    },
    {
        "title": "Analysing digital banking reviews using text mining",
        "authors": [
            "Li Chen Cheng",
            "Legaspi Rhea Sharmayne"
        ],
        "date": "December 2020",
        "source": "ASONAM '20: Proceedings of the 12th IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Digital banks are new entrants in the banking industry in the Philippines as they only started late 2018. Since then, a handful of players have and are still emerging. With more and more people becoming technologically savvy, it is very critical for financial institutions to develop a digital banking application that will stand out from the competition. This paper aims to use text mining methods to analyse digital banking application reviews. This study will perform topic modelling using LDA to explore customer concerns and will mine association rules between the digital banking features with the review score. The results will reveal which areas the digital banking application can further optimize for customer satisfaction and retention.",
        "link": "https://dl.acm.org/doi/10.1109/ASONAM49781.2020.9381429"
    },
    {
        "title": "Mining Moocs Videos Metadata Using Classification Techniques",
        "authors": [
            "El Harrak Othman",
            "Ghadi Abderrahim",
            "El Bouhdidi Jaber"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "In few years, the internet has become the first source of information for most people, Today MOOCs makes it possible for everyone to access to the education over the world, it's represents an emerging methodology of online teaching and an important development in open education. Due to the rapid development of Moocs and the rapid growth of digital data and video database over the Internet, it is becoming very difficult for learners to browsing and choosing the best training for them. The scientific community has increased the amount of research into new technologies, with a view to improve the digital video utilization: its archiving, indexing, accessibility, acquisition, store and even its process and usability. All these parts of the video utilization entail the necessity of the extraction of all important information of a video, especially in cases of lack of metadata information. Web video mining is retrieving the content using data mining techniques from World Wide Web. There are two approaches for web video mining using traditional image processing (signal processing) and metadata based approach. In this paper, we present the various research makes in this subject and we propose an effective methodology to extract the metadata from Moocs videos and classify them based on the extracted metadata by applying data mining techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090450"
    },
    {
        "title": "Text mining with HathiTrust",
        "authors": [
            "Eleanor Dickson Koehl",
            "Ryan Dubnicek"
        ],
        "date": "June 2019",
        "source": "JCDL '19: Proceedings of the 18th Joint Conference on Digital Libraries",
        "abstract": "This tutorial will introduce attendees to the HathiTrust Research Center and its tools and services for computational text analysis research. HTRC leverages the scope and scale of the HathiTrust Digital Library collection to create opportunities for researchers to perform text data mining on subsets of the corpus. Attendees will develop skills that will allow them to conduct basic text analysis research using HathiTrust data, learn techniques for engaging with scholars on their text data mining research, and explore strategies for computational access to large-scale digital collections.",
        "link": "https://dl.acm.org/doi/10.1109/JCDL.2019.00115"
    },
    {
        "title": "Text mining for plagiarism detection: multivariate pattern detection for recognition of text similarities",
        "authors": [
            "Konstantinos Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2018",
        "source": "ASONAM '18: Proceedings of the 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "The problem of plagiarism the recent years has been intensified by the availability of information in digital form and the accessibility of the electronic libraries through the Internet. As a result, plagiarism detection has been transformed into a big data analytics problem since the number of digital sources is extravagant and a new document needs to be compared with millions of other existing documents. In this paper, a text mining methodology is proposed that can detect all common patterns between a document and the documents in a reference database. The technique is based on a pattern detection algorithm and the corresponding data structure that enables the algorithm to detect all common patterns. The methodology has been applied in a well-defined dataset providing very promising results identifying difficult cases of plagiarism such as technical disguise.",
        "link": "https://dl.acm.org/doi/10.5555/3382225.3382424"
    },
    {
        "title": "Petroleum Engineering Data Text Classification Using Convolutional Neural Network Based Classifier",
        "authors": [
            "Tzuhan Hsu",
            "Yaoqin Zhang"
        ],
        "date": "May 2018",
        "source": "ICMLT '18: Proceedings of the 2018 International Conference on Machine Learning Technologies",
        "abstract": "In this paper, authors introduce a convolutional neural network (CNN) based classification approach for petroleum engineering event. Recently, text classification problem has raised substantial attention in the field of machine learning. The aim of text classification process is to assign correct label to a text on the basis of its content. However, due to the imprecise of feature representation, most of former text classification methods cannot export satisfying result. To solve this problem, authors propose a model based on convolutional neural network. Authors conduct experiments with real world petroleum engineering data text, artificially labelled into 6 categories. Beside purposed CNN-based model, the authors also implement other baseline machine learning approaches: k nearest neighbor, random forest, support vector machines, recurrent neural network, gated recurrent units, long short-term memory. Among all these models, our CNN-based model reach best performance in text classification task. In the last part of this paper, authors compare and analysis their purpose and baseline model.",
        "link": "https://dl.acm.org/doi/10.1145/3231884.3231898"
    },
    {
        "title": "Classifying Indonesian Online Articles as Advertisement Placement Base Using Text Mining",
        "authors": [
            "Nadhira Tasya",
            "Arian Dhini"
        ],
        "date": "July 2017",
        "source": "ICBIM 2017: Proceedings of the International Conference on Business and Information Management",
        "abstract": "Rapid development in technological aspect resulting in growing level of human needs for the latest news, so that emerged a new trend of publishing and accessing news through online media or usually called online journalism. In addition, the number of people who sell and purchase through online sites also continues to increase and this opportunity is utilized by the company and the advertiser by implementing targeted web advertising. However, the high number of articles that have been published and accessed leads to great opportunities for errors in determining where to place the ads. Therefore, it needs a system that can categorize articles accessed by users as the basis of advertisement placement by the company and this classification system can be done by applying the method of Data Mining and Text Mining. This research uses document data in the form of article content that will be categorized into twenty categories of class of advertisement by using Text Mining technique with Support Vector Machine algorithm. The results of this study may be used by companies or advertisers as a basis for placement of ads on selected online media sites.",
        "link": "https://dl.acm.org/doi/10.1145/3134271.3134288"
    },
    {
        "title": "Mining Free-Text Medical Notes for Suicide Risk Assessment",
        "authors": [
            "Marios Adamou",
            "Grigoris Antoniou",
            "Elissavet Greasidou",
            "Vincenzo Lagani",
            "Paulos Charonyktakis",
            "Ioannis Tsamardinos"
        ],
        "date": "July 2018",
        "source": "SETN '18: Proceedings of the 10th Hellenic Conference on Artificial Intelligence",
        "abstract": "Suicide has been considered as an important public health issue for a very long time, and is one of the main causes of death worldwide. Despite suicide prevention strategies being applied, the rate of suicide has not changed substantially over the past decades. Advances in machine learning make it possible to attempt to predict suicide based on the analysis of relevant data to inform clinical practice. This paper reports on findings from the analysis of data of patients who died by suicide in the period 2013-2016 and made use of both structured data and free-text medical notes. We focus on examining various text-mining approaches to support risk assessment. The results show that using advance machine learning and text-mining techniques, it is possible to predict within a specified period which people are most at risk of taking their own life at the time of referral to a mental health service.",
        "link": "https://dl.acm.org/doi/10.1145/3200947.3201020"
    },
    {
        "title": "Integration of data mining techniques in e-learning systems: Clustering Profil of Lerners and Recommender Course System",
        "authors": [
            "A. El Moustamid",
            "E. En-Naimi",
            "J. El Bouhdidi"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "Data mining aims at the extraction of knowledge from large quantities of data by automatic or semi-automatic methods. In the field of education and training e-learning, Data Mining techniques as clustering (The aim of this technique is to identify groups that are similar in some aspect) [21], prediction (is used for developing model where combination of data viewed from different aspects can effect single viewed data. Here, is important to watch how predictors variables are effecting predicted variable)[21], classification are applied in e-learning to improve the level of learners by providing learning according to the learners' requirement) are used to analyze learners profile, to predict and improve student performance. The aim of our project is to develop a system capable of analyzing learners profile and indexing web videos in order to offer learners a rich database with courses that correspond to their levels.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090453"
    },
    {
        "title": "Multi-level mining and visualization of scientific text collections: Exploring a bi-lingual scientific repository",
        "authors": [
            "Pablo Accuosto",
            "Francesco Ronzano",
            "Daniel Ferrés",
            "Horacio Saggion"
        ],
        "date": "December 2017",
        "source": "WOSP 2017: Proceedings of the 6th International Workshop on Mining Scientific Publications",
        "abstract": "We present a system to mine and visualize collections of scientific documents by semantically browsing information extracted from single publications or aggregated throughout corpora of articles. The text mining tool performs deep analysis of document collections allowing the extraction and interpretation of research paper's contents. In addition to the extraction and enrichment of documents with metadata (titles, authors, affiliations, etc), the deep analysis performed comprises semantic interpretation, rhetorical analysis of sentences, triple-based information extraction, and text summarization. The visualization components allow geographical-based exploration of collections, topic-evolution interpretation, and collaborative network analysis among others. The paper presents a case study of a bi-lingual collection in the field of Natural Language Processing (NLP).",
        "link": "https://dl.acm.org/doi/10.1145/3127526.3127529"
    },
    {
        "title": "Hierarchical Topic Mining via Joint Spherical Tree and Text Embedding",
        "authors": [
            "Yu Meng",
            "Yunyi Zhang",
            "Jiaxin Huang",
            "Yu Zhang",
            "Chao Zhang",
            "Jiawei Han"
        ],
        "date": "August 2020",
        "source": "KDD '20: Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Mining a set of meaningful topics organized into a hierarchy is intuitively appealing since topic correlations are ubiquitous in massive text corpora. To account for potential hierarchical topic structures, hierarchical topic models generalize flat topic models by incorporating latent topic hierarchies into their generative modeling process. However, due to their purely unsupervised nature, the learned topic hierarchy often deviates from users' particular needs or interests. To guide the hierarchical topic discovery process with minimal user supervision, we propose a new task, Hierarchical Topic Mining, which takes a category tree described by category names only, and aims to mine a set of representative terms for each category from a text corpus to help a user comprehend his/her interested topics. We develop a novel joint tree and text embedding method along with a principled optimization procedure that allows simultaneous modeling of the category tree structure and the corpus generative process in the spherical space for effective category-representative term discovery. Our comprehensive experiments show that our model, named JoSH, mines a high-quality set of hierarchical topics with high efficiency and benefits weakly-supervised hierarchical text classification tasks.",
        "link": "https://dl.acm.org/doi/10.1145/3394486.3403242"
    },
    {
        "title": "Unsupervised behavioural mining and clustering for malware family identification",
        "authors": [
            "Khanh Huu The Dam",
            "Thomas Given-Wilson",
            "Axel Legay"
        ],
        "date": "March 2021",
        "source": "SAC '21: Proceedings of the 36th Annual ACM Symposium on Applied Computing",
        "abstract": "More accurate and advanced detection and classification of malware requires exploiting program behaviour and not merely syntactic or static features. One approach is to use system call dependency graphs (SCDGs) that represent the program behaviour by interactions with the system, and the relations between these interactions. These SCDGs have been used with supervised learning techniques to very accurately detect and classify malware. This works considers the unsupervised learning challenge of mining for common clusters of behaviour without a priori knowledge. This allows for clustering of similar programs by behaviour, that can then be used for either classification or further analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3412841.3441919"
    },
    {
        "title": "Text Mining for Evaluating Authors' Birth and Death Years",
        "authors": [
            "Dror Moghaz",
            "Yaakov Hacohen-Kerner",
            "Dov Gabbay"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "This article presents a unique method in text and data mining for finding the era, i.e., mining temporal data, in which an anonymous author was living. Finding this era can assist in the examination of a fake document or extracting the time period in which a writer lived. The study and the experiments concern Hebrew, and in some parts, Aramaic and Yiddish rabbinic texts. The rabbinic texts are undated and contain no bibliographic sections, posing an interesting challenge. This work proposes algorithms using key phrases and key words that allow the temporal organization of citations together with linguistic patterns. Based on these key phrases, key words, and the references, we established several types of “Iron-clad,” Heuristic and Greedy rules for estimating the years of birth and death of a writer in an interesting classification task. Experiments were conducted on corpora, including documents authored by 12, 24, and 36 rabbinic writers and demonstrated promising results.",
        "link": "https://dl.acm.org/doi/10.1145/3281631"
    },
    {
        "title": "Transfer Learning to Timed Text Based Video Classification Using CNN",
        "authors": [
            "Zenun Kastrati",
            "Ali Shariq Imran",
            "Arianit Kurti"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "Open educational video resources are gaining popularity with a growing number of massive open online courses (MOOCs). This has created a niche for content providers to adopt effective solutions in automatically organizing and structuring of educational resources for maximum visibility. Recent advances in deep learning techniques are proving useful in managing and classifying resources into appropriate categories. This paper proposes one such convolutional neural network (CNN) model for classifying video lectures in a MOOC setting using a transfer learning approach. The model uses a time-aligned text transcripts corresponding to video lectures from six broader subject categories. Video lectures and their corresponding transcript dataset is gathered from the Coursera MOOC platform. Two different CNN models are proposed: i) CNN based classification using embeddings learned from our MOOC dataset, ii) CNN based classification using transfer learning. Word embeddings generated from two well known state-of-the-art pre-trained models Word2Vec and GloVe, are used in the transfer learning approach for the second case. The proposed CNN models are evaluated using precision, recall, and F1 score and the obtained performance is compared with both conventional and deep learning classifiers. The proposed CNN models have an F1 score improvement of 10-22 percentage points over DNN and conventional classifiers",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326483"
    },
    {
        "title": "Evolutionary role mining in complex networks by ensemble clustering",
        "authors": [
            "Sarvenaz Choobdar",
            "Pedro Ribeiro",
            "Fernando Silva"
        ],
        "date": "April 2017",
        "source": "SAC '17: Proceedings of the Symposium on Applied Computing",
        "abstract": "The structural patterns in the neighborhood of nodes assign unique roles to the nodes. Mining the set of existing roles in a network provides a descriptive profile of the network and draws its general picture. This paper proposes a new method to determine structural roles in a dynamic network based on the current position of nodes and their historic behavior. We develop a temporal ensemble clustering technique to dynamically find groups of nodes, holding similar tempo-structural roles. We compare two weighting functions, based on age and distribution of data, to incorporate temporal behavior of nodes in the role discovery. To evaluate the performance of the proposed method, we assess the results from two points of view: 1) goodness of fit to current structure of the network; 2) consistency with historic data. We conduct the evaluation using different ensemble clustering techniques. The results on real world networks demonstrate that our method can detect tempo-structural roles that simultaneously depict the topology of a network and reflect its dynamics with high accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3019612.3019815"
    },
    {
        "title": "Analyzing used-car web listings via text mining",
        "authors": [
            "Ayhan Demiriz",
            "Fatma Cantaş"
        ],
        "date": "October 2017",
        "source": "IML '17: Proceedings of the 1st International Conference on Internet of Things and Machine Learning",
        "abstract": "Used car trade is one of the major components of the world economies. It is not uncommon to sell a car by placing an internet advertisement irrespective of the geography in these days. A typical content of an advertisement is usually composed of two parts namely the structured and the free text data. The structured data may include some information about the asking price, make, model, year, mileage of the car and the contact info. In most cases, seller may give important clues about the car's current conditions in the free text data where the title (head) of the advertisement can be included as free text too. This paper reports preliminary results from a text mining study conducted on 75K used car internet listings collected from two major car listing web sites in Turkey. As expected, the words and the phrases related to the description of the car are observed to be frequent. The leading concepts in the free text are found to be regarding how to describe the current condition of a car, for example \"no crash history\".",
        "link": "https://dl.acm.org/doi/10.1145/3109761.3109782"
    },
    {
        "title": "Text classification of Diseases Treated by Traditional Chinese Medicine Prescription based on machine learning",
        "authors": [
            "Hanqing Zhao",
            "Bo Han",
            "Chen Li"
        ],
        "date": "September 2020",
        "source": "ISAIMS '20: Proceedings of the 1st International Symposium on Artificial Intelligence in Medical Sciences",
        "abstract": "OBJECTIVE: To explore the application of machine learning in th e identification of diseases treated by traditional Chinese medicine prescriptions. METHODS: Based on the composition of the text document of Chinese medicine prescriptions, the prescriptions were divided into cough, headache and diarrhea. THUTCT were introduced to establish and train two machine learning text classification models, LibSVM and LibLinear, and the prescriptions to be deter mined were put into the model for classification and prediction. R ESULTS: The Precision rate of LibSVM model and Liblinear mod el were 0.7283 and 0.6690. Seven prescriptions were classified an d predicted, and the results were in line with expectations. CONCLUSION: THUCTC has good universality for the content of TCM prescriptions, high classification accuracy and fast testing speed, which is suitable for the text classification and discrimination research of TCM prescriptions for diseases.",
        "link": "https://dl.acm.org/doi/10.1145/3429889.3429895"
    },
    {
        "title": "Evaluation of retweet clustering method classification method using retweets on Twitter without text data",
        "authors": [
            "K. Uchida",
            "F. Toriumi",
            "T. Sakaki"
        ],
        "date": "August 2017",
        "source": "WI '17: Proceedings of the International Conference on Web Intelligence",
        "abstract": "Burst phenomena, which frequently occur on social media, are caused by such social events as flaming on the internet, elections, and natural disasters. To understand people's thoughts and feelings, we must classify their opinions from burst phenomena. Therefore, classification methods that categorize tweets are critical. However, since most classification methods focus on text mining, they cannot group tweets by topics because each tweet has poor linguistic similarities. We used a non-text-based classification method proposed by Baba et al. that groups tweets by topics, even if they have poor linguistic similarities, and verified its validity by comparing it with a text-based classification method in two different evaluations: qualitative and quantitative. In the qualitative evaluation part, we did a questionnaire survey and validated the suitability of the topic clusters created using both the non-and text-based methods. Since evaluating the similarity of every pair of tweets in each topic is difficult, we evaluated the similarity between sampled pairs in the survey and acquired more appropriate topic clustering results using the non-text-based method than the text-based method. In the quantitative evaluation part, we focused on the robustness of each method against data reduction. Many approaches analyze social media data, especially because collecting data from social media is comparatively easy. However, since collecting the whole data of burst phenomena is very costly due to the vast amounts of available social media data, robustness against data reduction is an important index to evaluate classification methods. With the non-text-based method, over 55% of the pairs of tweets in the same cluster were also included in the same cluster even when the data were reduced to 10% in all three of our example cases. In this paper, as a source we focus on Twitter, one of the most popular microblogging services. Using clustering to conduct detailed case analyses, we scrutinized three burst cases that include natural disasters and flaming on the internet and found that a non-text-based method more effectively classified tweets in burst phenomena than a text-based method.",
        "link": "https://dl.acm.org/doi/10.1145/3106426.3106451"
    },
    {
        "title": "Using Spark for Text Mining on Large Scale Liver Cancer Literature",
        "authors": [
            "Ming-Yen Lin",
            "Yu-Ju Lin",
            "Sue-Chen Hsueh"
        ],
        "date": "January 2021",
        "source": "BDET 2021: 2021 the 3rd International Conference on Big Data Engineering and Technology (BDET)",
        "abstract": "Cancer is one of the main causes of death. The number of known cancer to-date is more than one hundred. Liver cancer, ranked after trachea and lung cancer, has long been high in the cancer leading cause in Taiwan and even ranked as second in 2016. The number of scientific articles related to cancer proliferates every year. The number reaches as high as 20 million in PubMed so that discovering useful information from the massive collection is very difficult. In addition, using a single machine to sift through these articles is very time-consuming. Therefore, we present a big data analytic framework using the distributed Apache Spark platform for text mining in PubMed literature. We establish a prediction model for liver cancer articles so that researchers may effectively validate whether an article is related to liver cancer or not. Classification models in Spark MLlib including Linear Support Vector Machines (SVM), Logistic Regression, are used in our experiments. Relevancy to liver cancer is further confirmed by using MeSH (Medical Subject Headings) terms. Logistic regression is about 3 times faster than SVMs and the accuracy of both methods is close to 95% in the experiments using hold-out validation. When max_features is 500 and min_df ≤ 0.1 (or min_df = 1), the accuracy may reach 96%. In the experiments with K-fold cross-validation, the accuracy of SVMs methods is 96%. The experimental results show that that our prediction model may effectively classify liver cancer articles.",
        "link": "https://dl.acm.org/doi/10.1145/3474944.3474958"
    },
    {
        "title": "Automatic topic classification of test cases using text mining at an Android smartphone vendor",
        "authors": [
            "Junji Shimagaki",
            "Yasutaka Kamei",
            "Naoyasu Ubayashi",
            "Abram Hindle"
        ],
        "date": "October 2018",
        "source": "ESEM '18: Proceedings of the 12th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Background: An Android smartphone is an ecosystem of applications, drivers, operating system components, and assets. The volume of the software is large and the number of test cases needed to cover the functionality of an Android system is substantial. Enormous effort has been already taken to properly quantify \"what features and apps were tested and verified?\". This insight is provided by dashboards that summarize test coverage and results per feature. One method to achieve this is to manually tag or label test cases with the topic or function they cover, much like function points. At the studied Android smartphone vendor, tests are labelled with manually defined tags, so-called \"feature labels (FLs)\", and the FLs serve to categorize 100s to 1000s test cases into 10 to 50 groups. Aim: Unfortunately for developers, manual assignment of FLs to 1000s of test cases is a time consuming task, leading to inaccurately labeled test cases, which will render the dashboard useless. We created an automated system that suggests tags/labels to the developers for their test cases rather than manual labeling. Method: We use machine learning models to predict and label the functionality tested by 10,000 test cases developed at the company. Results: Through the quantitative experiments, our models achieved acceptable F-1 performance of 0.3 to 0.88. Also through the qualitative studies with expert teams, we showed that the hierarchy and path of tests was a good predictor of a feature's label. Conclusions: We find that this method can reduce tedious manual effort that software developers spent classifying test cases, while providing more accurate classification results.",
        "link": "https://dl.acm.org/doi/10.1145/3239235.3268927"
    },
    {
        "title": "To apply Data Mining for Classification of Crowd sourced Software Requirements",
        "authors": [
            "Soonh Taj",
            "Qasim Arain",
            "Imran Memon",
            "Asma Zubedi"
        ],
        "date": "April 2019",
        "source": "ICSIE '19: Proceedings of the 8th International Conference on Software and Information Engineering",
        "abstract": "Now a day's main focus of developers is to build quality software that works according to customer needs and for this reason it is necessary to gather right requirements as requirement elicitation is the critical step that impacts on the success of software project as misinterpreted requirements leads to the failure of software project. By keeping this in mind a research is carried out on improving requirements elicitation process and automating the process of classifying requirements. In this research, a model is proposed which will help in this scenario for requirements elicitation and requirement classification. This paper presents a model in which crowd sourcing approach is used so that customers, end users, stakeholders, developers and software engineers can make active participation for requirement elicitation process and requirements gathered using crowdsourcing approach are used by model for classification process i.e. classification of requirements into functional and non-functional requirements. For the proof of proposed model a case study is conducted. Results of case study provided the usefulness and efficiency of proposed model for classification of crowd sourced software requirements.",
        "link": "https://dl.acm.org/doi/10.1145/3328833.3328837"
    },
    {
        "title": "Text classification using Fuzzy TF-IDF and Machine Learning Models",
        "authors": [
            "Mariem Bounabi",
            "Karim El Moutaouakil",
            "Khalid Satori"
        ],
        "date": "October 2019",
        "source": "BDIoT'19: Proceedings of the 4th International Conference on Big Data and Internet of Things",
        "abstract": "The representation of the information has an important impact on the text classification task. Several weighting methods were proposed in the literature, and the term frequency-inverse term frequency (TFIDF), the most know on the text treatment field. The FTF-IDF is a vector representation where the components of the TFIDF are presented as inputs to the Fuzzy Inference System (FIS). In this work, we compare several Machin Learning algorithms such as Naïve Bayes and its derivatives, SVM and Random forest classifiers, using the FTF-IDF representation. To improve the quality of the used classifiers, we call sum attribute selection methods. The recognition rate, for the tested systems, is satisfied, where the system based on naïve Bayes classifier, the FTF-IDF weighting terms, and the info gain select attributes method gives 98.7% as accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3372938.3372956"
    },
    {
        "title": "Text Mining in Cybersecurity: A Systematic Literature Review",
        "authors": [
            "Luciano Ignaczak",
            "Guilherme Goldschmidt",
            "Cristiano André Da Costa",
            "Rodrigo Da Rosa Righi"
        ],
        "date": null,
        "source": "ACM Computing Surveys",
        "abstract": "The growth of data volume has changed cybersecurity activities, demanding a higher level of automation. In this new cybersecurity landscape, text mining emerged as an alternative to improve the efficiency of the activities involving unstructured data. This article proposes a Systematic Literature Review (SLR) to present the application of text mining in the cybersecurity domain. Using a systematic protocol, we identified 2,196 studies, out of which 83 were summarized. As a contribution, we propose a taxonomy to demonstrate the different activities in the cybersecurity domain supported by text mining. We also detail the strategies evaluated in the application of text mining tasks and the use of neural networks to support activities involving unstructured data. The work also discusses text classification performance aiming its application in real-world solutions. The SLR also highlights open gaps for future research, such as the analysis of non-English content and the intensification in the usage of neural networks.",
        "link": "https://dl.acm.org/doi/10.1145/3462477"
    },
    {
        "title": "Using Text Mining to Analyze the Financial Patents",
        "authors": [
            "Yung-Feng Lu",
            "Jia-Lang Xu",
            "Mu-Yen Chen"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "With the advancement and development of science and technology, Nowadays society is the development of information technology, cloud computing, mobile Internet, Internet of Things, block chain and information technology.The combination of finance and technology has made financial technology terms gradually emerging.. This research analyzes the patents of financial technology. However, Taiwan's financial technology patents are mostly G06Q-20 (payment plan, architecture or agreement), G06Q-30 (commercial, such as marketing, shopping, payment, auction or e-commerce). G06Q-40 (financial, investment or tax treatment, insurance).The research use text mining technology to analyze and understand Taiwan's trends and developments in financial technology. The research results show that G06Q-20 patents are biased towards information security and mobile payment. G06Q-30 patents favor shopping websites and advertising. G06Q-40 patents are biased towards diversity, such as securities, insurance, finance, and stocks.",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326478"
    },
    {
        "title": "Identifying Online Profiles of Distance Learning Students Using Data Mining Techniques",
        "authors": [
            "Osama Islam",
            "Muazzam Siddiqui",
            "Naif Radi Aljohani"
        ],
        "date": "October 2019",
        "source": "ICDTE '19: Proceedings of the 3rd International Conference on Digital Technology in Education",
        "abstract": "Educational data has grown over the years with the increased use of technology within educational environments. This has led to a huge amount of data being stored in various data sources representing the student, his/her activities, and other aspects relevant to the learning process. To meet this analytical need, Educational Data Mining (EDM) has emerged to assist educational institutions in identifying key benefits such as students at risk, the level of student engagement or predicting student performance. The aim of this research was to explore the various aspects of student interaction data using data mining techniques to identify relevant patterns of behaviors and possible key attributes that have higher degrees of influence on distance learning students. The main findings identified several patterns of user online profiles based on a set of adopted learning strategies, the research proposed a framework for analyzing such interaction data based on R and Hadoop platforms to correlate online profiles with student performance.",
        "link": "https://dl.acm.org/doi/10.1145/3369199.3369249"
    },
    {
        "title": "Extremely Fast Decision Tree Mining for Evolving Data Streams",
        "authors": [
            "Albert Bifet",
            "Jiajin Zhang",
            "Wei Fan",
            "Cheng He",
            "Jianfeng Zhang",
            "Jianfeng Qian",
            "Geoff Holmes",
            "Bernhard Pfahringer"
        ],
        "date": "August 2017",
        "source": "KDD '17: Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
        "abstract": "Nowadays real-time industrial applications are generating a huge amount of data continuously every day. To process these large data streams, we need fast and efficient methodologies and systems. A useful feature desired for data scientists and analysts is to have easy to visualize and understand machine learning models. Decision trees are preferred in many real-time applications for this reason, and also, because combined in an ensemble, they are one of the most powerful methods in machine learning. In this paper, we present a new system called STREAMDM-C++, that implements decision trees for data streams in C++, and that has been used extensively at Huawei. Streaming decision trees adapt to changes on streams, a huge advantage since standard decision trees are built using a snapshot of data, and can not evolve over time. STREAMDM-C++ is easy to extend, and contains more powerful ensemble methods, and a more efficient and easy to use adaptive decision trees. We compare our new implementation with VFML, the current state of the art implementation in C, and show how our new system outperforms VFML in speed using less resources.",
        "link": "https://dl.acm.org/doi/10.1145/3097983.3098139"
    },
    {
        "title": "Data miners' little helper: data transformation activity cues for cluster analysis on document collections",
        "authors": [
            "Tania Cerquitelli",
            "Evelina Di Corso",
            "Francesco Ventura",
            "Silvia Chiusano"
        ],
        "date": "June 2017",
        "source": "WIMS '17: Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "In this paper we propose a new self-learning engine to streamline the analytics process, as it enables analysts to mine massive data repositories with minimal user intervention. In the context of cluster analysis on a collection of documents this new system, named SELF-DATA (SELF-learning DAta TrAnsformation), suggests to the analyst how to configure the whole mining process for a given dataset. SELF-DATA relies on an engine exploring different data weighting schemas (e.g., normalized term frequencies) and data transformation methods (e.g., PCA) before applying the cluster analysis, evaluating and comparing solutions through different quality indices (e.g., weighted Silhouette), and presenting the k-top solutions to the analyst. SELF-DATA will also include a knowledge base storing results of experiments on previously processed datasets, and a classification algorithm trained on the knowledge base content to forecast the best configuration for the whole mining process for an unexplored dataset. The first development of SELF-DATA running on Apache Spark has been validated on 5 collections of documents. Experimental results highlight that TF-IDF and logarithmic entropy are effective to measure item relevance with sparse datasets, and the LSI method outperforms PCA with a large dictionary.",
        "link": "https://dl.acm.org/doi/10.1145/3102254.3102288"
    },
    {
        "title": "Interactive Visual Graph Mining and Learning",
        "authors": [
            "Ryan A. Rossi",
            "Nesreen K. Ahmed",
            "Rong Zhou",
            "Hoda Eldardiry"
        ],
        "date": null,
        "source": "ACM Transactions on Intelligent Systems and Technology",
        "abstract": "This article presents a platform for interactive graph mining and relational machine learning called GraphVis. The platform combines interactive visual representations with state-of-the-art graph mining and relational machine learning techniques to aid in revealing important insights quickly as well as learning an appropriate and highly predictive model for a particular task (e.g., classification, link prediction, discovering the roles of nodes, and finding influential nodes). Visual representations and interaction techniques and tools are developed for simple, fast, and intuitive real-time interactive exploration, mining, and modeling of graph data. In particular, we propose techniques for interactive relational learning (e.g., node/link classification), interactive link prediction and weighting, role discovery and community detection, higher-order network analysis (via graphlets, network motifs), among others. GraphVis also allows for the refinement and tuning of graph mining and relational learning methods for specific application domains and constraints via an end-to-end interactive visual analytic pipeline that learns, infers, and provides rapid interactive visualization with immediate feedback at each change/prediction in real-time. Other key aspects include interactive filtering, querying, ranking, manipulating, exporting, as well as tools for dynamic network analysis and visualization, interactive graph generators (including new block model approaches), and a variety of multi-level network analysis techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3200764"
    },
    {
        "title": "Data Mining and Opinion Mining: A Tool in Educational Context",
        "authors": [
            "Myriam Peñafiel",
            "Stefanie Vásquez",
            "Diego Vásquez",
            "Juan Zaldumbide",
            "Sergio Luján-Mora"
        ],
        "date": "July 2018",
        "source": "ICoMS '18: Proceedings of the 2018 1st International Conference on Mathematics and Statistics",
        "abstract": "The use of the web as a universal communication platform generates large volumes of data (Big data), which in many cases, need to be processed so that they can become useful knowledge in face of the sceptics who have doubts about the credibility of such information. The use of web data that comes from educational contexts needs to be addressed, since that large amount of unstructured information is not being valued, losing valuable information that can be used. To solve this problem, we propose the use of data mining techniques such as sentiment analysis to validate the information that comes from the educational platforms. The objective of this research is to propose a methodology that allows the user to apply sentiment analysis in a simple way, because although some researchers have done it, very few do with data in the educational context. The results obtained prove that the proposal can be used in similar cases.",
        "link": "https://dl.acm.org/doi/10.1145/3274250.3274263"
    },
    {
        "title": "Research Hotspots Mining and Visualized Analysis Based on Linking Cluster and K-Core Decomposition",
        "authors": [
            "Ying Cai",
            "Fang Huang",
            "Shuai Wang",
            "Hao Zhang",
            "Chunxiu Du"
        ],
        "date": "May 2018",
        "source": "ICDPA 2018: Proceedings of the International Conference on Data Processing and Applications",
        "abstract": "In this paper, we focus on research hotspots mining with knowledge semantic features on discipline topic word networks, which are extracted from scientific and technical documents. For the topic networks, a novel method of combining linking community clustering with k-core decomposition is proposed to mine the hotspot community with knowledge hierarchy structure. And as a post-processing, the hierarchical community identification is implemented by k-core decomposition in order to discover the internal connection with knowledge semantic hierarchy in the research hotspot. Finally, the community density and visualization method is utilized to analyze the core and sub- problem in the research hotspot community. In experiments, the topic networks for a single discipline and interdisciplinary research hotspot are taken as examples of analysis, and the results show the proposed approach can effectively realize the objective of mining and analyzing the discipline research hotspot on the academic topic word network.",
        "link": "https://dl.acm.org/doi/10.1145/3224207.3224216"
    },
    {
        "title": "The Application of Data Mining Algorithms in the Construction of Travel Recommendation System",
        "authors": [
            "Chao Lou",
            "Zhaonan Mu",
            "Mengzhu Liu"
        ],
        "date": "January 2021",
        "source": "CONF-CDS 2021: The 2nd International Conference on Computing and Data Science",
        "abstract": "Photos shared by users of online social network often associated with tags, texts, geographic and time information. These data are ideal research resource which can be used by researchers to observe people's interests and behaviors. They can also be used to analyze people's travel behaviors and interests because it can reflect people's travel activities and experiences directly. This project aimed to consolidate people into groups based on their travel preferences of travel season, travel time span, scenery type of travel location and then analyze popular travel places for different people groups. The travel preferences were analyzed from extracted data applying PHP language and Naive Bayes classification approach. According to the analyzed data, people were divided into groups. Then, DBSCAN clustering was used to get popular locations based on the different groups. As a result, 80 thousand photo data of 5 cities have been collected from Flickr by Phython. It has been showed that people's preference of season in most of the mined cities was spring and summer and the preference of time span was short. Moreover, mined people has been divided into 16 groups according to their preference and based on the groups popular locations have been calculated by DBSCAN clustering. Theses locations have been provided in a recommendation website which was built in this project. For the future work, mining Flickr data associated with other travel social network sites could be a good way to build a more effective recommendation system.",
        "link": "https://dl.acm.org/doi/10.1145/3448734.3450477"
    },
    {
        "title": "Compare Machine Learning Models in Text Classification Using Steam User Reviews",
        "authors": [
            "Youchen Miao",
            "Zeyu Jin",
            "Yumeng Zhang",
            "Yuchen Chen",
            "Junren Lai"
        ],
        "date": "November 2021",
        "source": "ICSED '21: Proceedings of the 2021 3rd International Conference on Software Engineering and Development",
        "abstract": "Text Classification and Sentiment Analysis of game reviews are viewed as important parts in not only academic fields but also in game studies. In this paper, with more than 400 thousand game reviews on Steam platform, we preprocess the data using different libraries (sklearn, nltk, and spaCy) and use them as inputs to build three sentiment classification models based on different algorithms (Naive Bayes, SVM, and Random Forest). In contrast to previous studies that only focus on different sentiment analysis models, our paper also highlights the use of different APIs to preprocess the data and their corresponding model performance. The results show that no matter which API we choose, Random Forest models always perform the best. However, in terms of training time, Naive Bayes is the fastest. This work can be used to apply grid search for researchers to automatically find the optimum API before conducting sentiment analysis in the future.",
        "link": "https://dl.acm.org/doi/10.1145/3507473.3507480"
    },
    {
        "title": "Lost in Transduction: Transductive Transfer Learning in Text Classification",
        "authors": [
            "Alejandro Moreo",
            "Andrea Esuli",
            "Fabrizio Sebastiani"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Obtaining high-quality labelled data for training a classifier in a new application domain is often costly. Transfer Learning\n(a.k.a. “Inductive Transfer”) tries to alleviate these costs by transferring, to the “target” domain of interest, knowledge available from a different “source” domain. In transfer learning the lack of labelled information from the target domain is compensated by the availability at training time of a set of unlabelled examples from the target distribution. Transductive Transfer Learning denotes the transfer learning setting in which the only set of target documents that we are interested in classifying is known and available at training time. Although this definition is indeed in line with Vapnik’s original definition of “transduction”, current terminology in the field is confused. In this article, we discuss how the term “transduction” has been misused in the transfer learning literature, and propose a clarification consistent with the original characterization of this term given by Vapnik. We go on to observe that the above terminology misuse has brought about misleading experimental comparisons, with inductive transfer learning methods that have been incorrectly compared with transductive transfer learning methods. We then, give empirical evidence that the difference in performance between the inductive version and the transductive version of a transfer learning method can indeed be statistically significant (i.e., that knowing at training time the only data one needs to classify indeed gives an advantage). Our clarification allows a reassessment of the field, and of the relative merits of the major, state-of-the-art algorithms for transfer learning in text classification.",
        "link": "https://dl.acm.org/doi/10.1145/3453146"
    },
    {
        "title": "Machine Learning on Mining Potential Adverse Drug Reactions for Pharmacovigilance",
        "authors": [
            "Ke Jia"
        ],
        "date": "July 2021",
        "source": "DSIT 2021: 2021 4th International Conference on Data Science and Information Technology",
        "abstract": "Drugs are related to the lives and health of patients. Mining and discovering potential adverse drug reactions from a large number of unstructured texts play an important role in drug research and pharmacovigilance. It is a hot research issue in the field of biomedicine in recent years. A large number of scholars have discovered a large amount of information on adverse drug reactions from different data sources through effective machine learning methods. This paper reviews the research progress in recent years to provide a reference for future research.",
        "link": "https://dl.acm.org/doi/10.1145/3478905.3478964"
    },
    {
        "title": "TACAM: Topic And Context Aware Argument Mining",
        "authors": [
            "Michael Fromm",
            "Evgeniy Faerman",
            "Thomas Seidl"
        ],
        "date": "October 2019",
        "source": "WI '19: IEEE/WIC/ACM International Conference on Web Intelligence",
        "abstract": "In this work we address the problem of argument search. The purpose of argument search is the distillation of pro and contra arguments for requested topics from large text corpora. In previous works, the usual approach is to use a standard search engine to extract text parts which are relevant to the given topic and subsequently use an argument recognition algorithm to select arguments from them. The main challenge in the argument recognition task, which is also known as argument mining, is that often sentences containing arguments are structurally similar to purely informative sentences without any stance about the topic. In fact, they only differ semantically. Most approaches use topic or search term information only for the first search step and therefore assume that arguments can be classified independently of a topic. We argue that topic information is crucial for argument mining, since the topic defines the semantic context of an argument. Precisely, we propose different models for the classification of arguments, which take information about a topic of an argument into account. Moreover, to enrich the context of a topic and to let models understand the context of the potential argument better, we integrate information from different external sources such as Knowledge Graphs or pre-trained NLP models. Our evaluation shows that considering topic information, especially in connection with external information, provides a significant performance boost for the argument mining task.",
        "link": "https://dl.acm.org/doi/10.1145/3350546.3352506"
    },
    {
        "title": "Text mining for incoming tasks based on the urgency/importance factors and task classification using machine learning tools",
        "authors": [
            "Yasser Ali Alshehri"
        ],
        "date": "March 2020",
        "source": "ICCDA 2020: Proceedings of the 2020 the 4th International Conference on Compute and Data Analysis",
        "abstract": "In workplaces, there is a massive amount of unstructured data from different sources. In this paper, we present a case study that explains how can through communications between employees, we can help to prioritize tasks requests to increase the efficiency of their works for both technical and non-technical workers. This involves managing daily incoming tasks based on their level of urgency and importance.To allow all workers to utilize the urgency-importance matrix as a time-management tool, we need to automate this tool. The textual content of incoming tasks are analyzed, and metrics related to urgency and importance are extracted. A third factor (i.e., the response variable) is defined based on the two input variables (urgency and importance). Then, machine learning applied to the data to predict the class of incoming tasks based on data outcome desired. We used ordinal regression, neural networks, and decision tree algorithms to predict the four levels of task priority. We measure the performance of all using recalls, precisions, and F-scores. All classifiers perform higher than 89% in terms of all measures.",
        "link": "https://dl.acm.org/doi/10.1145/3388142.3388153"
    },
    {
        "title": "Text mining for malware classification using multivariate all repeated patterns detection",
        "authors": [
            "Konstantinos F. Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2019",
        "source": "ASONAM '19: Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Mobile phones have become nowadays a commodity to the majority of people. Using them, people are able to access the world of Internet and connect with their friends, their colleagues at work or even unknown people with common interests. This proliferation of the mobile devices has also been seen as an opportunity for the cyber criminals to deceive smartphone users and steel their money directly or indirectly, respectively, by accessing their bank accounts through the smartphones or by blackmailing them or selling their private data such as photos, credit card data, etc. to third parties. This is usually achieved by installing malware to smartphones masking their malevolent payload as a legitimate application and advertise it to the users with the hope that mobile users will install it in their devices. Thus, any existing application can easily be modified by integrating a malware and then presented it as a legitimate one. In response to this, scientists have proposed a number of malware detection and classification methods using a variety of techniques. Even though, several of them achieve relatively high precision in malware classification, there is still space for improvement. In this paper, we propose a text mining all repeated pattern detection method which uses the decompiled files of an application in order to classify a suspicious application into one of the known malware families. Based on the experimental results using a real malware dataset, the methodology tries to correctly classify (without any misclassification) all randomly selected malware applications of 3 categories with 3 different families each.",
        "link": "https://dl.acm.org/doi/10.1145/3341161.3350841"
    },
    {
        "title": "Comparative Study between Traditional Machine Learning and Deep Learning Approaches for Text Classification",
        "authors": [
            "Cannannore Nidhi Kamath",
            "Syed Saqib Bukhari",
            "Andreas Dengel"
        ],
        "date": "August 2018",
        "source": "DocEng '18: Proceedings of the ACM Symposium on Document Engineering 2018",
        "abstract": "In this contemporaneous world, it is an obligation for any organization working with documents to end up with the insipid task of classifying truckload of documents, which is the nascent stage of venturing into the realm of information retrieval and data mining. But classification of such humongous documents into multiple classes, calls for a lot of time and labor. Hence a system which could classify these documents with acceptable accuracy would be of an unfathomable help in document engineering. We have created multiple classifiers for document classification and compared their accuracy on raw and processed data. We have garnered data used in a corporate organization as well as publicly available data for comparison. Data is processed by removing the stop-words and stemming is implemented to produce root words. Multiple traditional machine learning techniques like Naive Bayes, Logistic Regression, Support Vector Machine, Random forest Classifier and Multi-Layer Perceptron are used for classification of documents. Classifiers are applied on raw and processed data separately and their accuracy is noted. Along with this, Deep learning technique such as Convolution Neural Network is also used to classify the data and its accuracy is compared with that of traditional machine learning techniques. We are also exploring hierarchical classifiers for classification of classes and subclasses. The system classifies the data faster and with better accuracy than if done manually. The results are discussed in the results and evaluation section.",
        "link": "https://dl.acm.org/doi/10.1145/3209280.3209526"
    },
    {
        "title": "Identification of Overpricing in the Purchase of Medication by the Federal Government of Brazil, Using Text Mining and Clustering Based on Ontology",
        "authors": [
            "Marco Aurelio O. S. Correa",
            "Adriano Galindo Leal"
        ],
        "date": "August 2018",
        "source": "ICCBDC '18: Proceedings of the 2018 2nd International Conference on Cloud and Big Data Computing",
        "abstract": "Increasing the transparency level in his actions and spending is one of the primary duties of the Brazilian Federal Government. The creation of laws that oblige full disclose of all its expenditures through transparency portals enables citizens to supervise all government entities. However, only the dissemination of these data, without a definite standard or the availability of data analysis tools, does not guarantee that the citizen is empowered to play his role. Therefore hence, the objective of this work is to identify overprice in the acquisition of products purchased by the federal government of Brazil using the unstructured data available on the Transparency Portal. The last two-years' worth of purchasing data, available in the Transparency Portal, were extracted, processed and stored. Due to his diverse nature and high volume of data, this study focused only on medicines purchased by the Ministry of Health. Ontology-based text mining and clustering techniques were applied for automatic identification and classification of products. The processing of this information was done through text mining and clustering, based on the ontology registered in another database of the Brazilian government. Because of this work, a consolidated price base per medication was created to allow the identification of distortions in prices practised, facilitating the identification of cases that merit further investigation to unravel fraud to the treasury.",
        "link": "https://dl.acm.org/doi/10.1145/3264560.3264569"
    },
    {
        "title": "Computational Estimation by Scientific Data Mining with Classical Methods to Automate Learning Strategies of Scientists",
        "authors": [
            "Aparna S. Varde"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Experimental results are often plotted as 2-dimensional graphical plots (aka graphs) in scientific domains depicting dependent versus independent variables to aid visual analysis of processes. Repeatedly performing laboratory experiments consumes significant time and resources, motivating the need for computational estimation. The goals are to estimate the graph obtained in an experiment given its input conditions, and to estimate the conditions that would lead to a desired graph. Existing estimation approaches often do not meet accuracy and efficiency needs of targeted applications. We develop a computational estimation approach called AutoDomainMine that integrates clustering and classification over complex scientific data in a framework so as to automate classical learning methods of scientists. Knowledge discovered thereby from a database of existing experiments serves as the basis for estimation. Challenges include preserving domain semantics in clustering, finding matching strategies in classification, striking a good balance between elaboration and conciseness while displaying estimation results based on needs of targeted users, and deriving objective measures to capture subjective user interests. These and other challenges are addressed in this work. The AutoDomainMine approach is used to build a computational estimation system, rigorously evaluated with real data in Materials Science. Our evaluation confirms that AutoDomainMine provides desired accuracy and efficiency in computational estimation. It is extendable to other science and engineering domains as proved by adaptation of its sub-processes within fields such as Bioinformatics and Nanotechnology.",
        "link": "https://dl.acm.org/doi/10.1145/3502736"
    },
    {
        "title": "Where is the road for issue reports classification based on text mining?",
        "authors": [
            "Qiang Fan",
            "Yue Yu",
            "Gang Yin",
            "Tao Wang",
            "Huaimin Wang"
        ],
        "date": "November 2017",
        "source": "ESEM '17: Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Currently, open source projects receive various kinds of issues daily, because of the extreme openness of Issue Tracking System (ITS) in GitHub. ITS is a labor-intensive and time-consuming task of issue categorization for project managers. However, a contributor is only required a short textual abstract to report an issue in GitHub. Thus, most traditional classification approaches based on detailed and structured data (e.g., priority, severity, software version and so on) are difficult to adopt. In this paper, issue classification approaches on a large-scale dataset, including 80 popular projects and over 252,000 issue reports collected from GitHub, were investigated. First, four traditional text-based classification methods and their performances were discussed. Semantic perplexity (i.e., an issues description confuses bug-related sentences with nonbug-related sentences) is a crucial factor that affects the classification performances based on quantitative and qualitative study. Finally, A two-stage classifier framework based on the novel metrics of semantic perplexity of issue reports was designed. Results show that our two-stage classification can significantly improve issue classification performances.",
        "link": "https://dl.acm.org/doi/10.1109/ESEM.2017.19"
    },
    {
        "title": "Feature-based Facebook reviews process model for e-management using data mining",
        "authors": [
            "Anish Kumar Varudharajulu",
            "Yongsheng Ma"
        ],
        "date": "January 2019",
        "source": "IC4E '19: Proceedings of the 10th International Conference on E-Education, E-Business, E-Management and E-Learning",
        "abstract": "The data generated from online communication acts as potential gold mines for discovering knowledge for researchers. A large amount of data is also generated in the form of web documents, emails, blogs, and feedback, etc. Text analytics is being significantly employed to mine important information. Opinion mining is the process of extracting human thoughts and perceptions from unstructured texts. The showstopper for designing an opinion mining system for analyzing reviews arise from the fact that customer reviews are often noisy. These reviews are informally written. In addition, they are subjected to spelling mistakes, grammatical errors, improper punctuation and irrational capitalization. This paper focuses on analyzing the different classification and clustering algorithms aimed at extracting and consolidating opinions of customers from social media sites like Facebook, Twitter and through surveys, at multiple levels of granularity to monitor and measure customer satisfaction. Ours is an automated approach, in which the system aids in the process of knowledge assimilation for knowledge-based building and also performs the analytics. Domain experts ratify the knowledge base and also provide training datasets for the system to intuitively gather more instances for ratification. The system identifies opinion expressions as phrases containing opinion words, opinionated features and also opinion modifiers. These expressions are categorized as positive, negative or neutral. Opinion expressions are identified and categorized using localized linguistic techniques. Opinions can be congregated at any desired level of specificity i.e. feature level or product level, user level or service level, etc. We have developed a system based on this approach, which provides the user with a platform to analyze opinion expressions crawled from a set of pre-defined datasets.",
        "link": "https://dl.acm.org/doi/10.1145/3306500.3306514"
    },
    {
        "title": "Discriminative Topic Mining via Category-Name Guided Text Embedding",
        "authors": [
            "Yu Meng",
            "Jiaxin Huang",
            "Guangyuan Wang",
            "Zihan Wang",
            "Chao Zhang",
            "Yu Zhang",
            "Jiawei Han"
        ],
        "date": "April 2020",
        "source": "WWW '20: Proceedings of The Web Conference 2020",
        "abstract": "Mining a set of meaningful and distinctive topics automatically from massive text corpora has broad applications. Existing topic models, however, typically work in a purely unsupervised way, which often generate topics that do not fit users’ particular needs and yield suboptimal performance on downstream tasks. We propose a new task, discriminative topic mining, which leverages a set of user-provided category names to mine discriminative topics from text corpora. This new task not only helps a user understand clearly and distinctively the topics he/she is most interested in, but also benefits directly keyword-driven classification tasks. We develop CatE, a novel category-name guided text embedding method for discriminative topic mining, which effectively leverages minimal user guidance to learn a discriminative embedding space and discover category representative terms in an iterative manner. We conduct a comprehensive set of experiments to show that CatE mines high-quality set of topics guided by category names only, and benefits a variety of downstream applications including weakly-supervised classification and lexical entailment direction identification.",
        "link": "https://dl.acm.org/doi/10.1145/3366423.3380278"
    },
    {
        "title": "Comparative Study of Heart Disease Diagnosis Using Top Ten Data Mining Classification Algorithms",
        "authors": [
            "I. Ketut Agung Enriko"
        ],
        "date": "June 2019",
        "source": "ICFET '19: Proceedings of the 5th International Conference on Frontiers of Educational Technologies",
        "abstract": "Data mining has been used for many purposes, especially for prediction system. In healthcare, data mining algorithms often used in disease diagnosis. Meanwhile, heart disease is known as a primary cause of death over the years. Many studies have been performed in heart disease diagnosis using data mining methods. There are some popular data mining algorithms that can be used in heart disease diagnosis, for example, k-Nearest Neighbor, CART, and AdaBoost. The algorithms are used to analyze a sample of cardiovascular patients data and predict the heart disease type that they suffer. Some parameters are taken from the patient, including EKG morphology, blood pressure, and information about the existence of chest pain, shortness of breath, palpitation, and cold sweat. In this study, medical records data are collected from Harapan Kita Hospital and utilized as a dataset sample in this research. Top ten data mining classification algorithms are used in diagnosing heart disease from Harapan Kita Hospital data and examining their performance by checking the accuracy and speed.",
        "link": "https://dl.acm.org/doi/10.1145/3338188.3338220"
    },
    {
        "title": "Text Mining Approach for Identifying Research Trends",
        "authors": [
            "Snezhana Sulova"
        ],
        "date": "June 2021",
        "source": "CompSysTech '21: Proceedings of the 22nd International Conference on Computer Systems and Technologies",
        "abstract": "With the increase of unstructured data, the issues connected with automatic text processing, the categorization of documents and the discovery of topics have become objects of growing interest. In order to improve the process of grouping and processing research publications, we would like to propose a method based upon natural language processing. It is based on text mining technologies which aim to identify key tendencies in documents. It processes the content of publications by clustering and identifies the topics of each identified group. This analysis helps by identifying key tendencies as well as discovering emerging new areas of research. Publications from the research literature database, Scopus, were used to test the approach. The topic of the publications is “the application of digital technologies in the logistics business”. The experiments were completed using the RapidMiner Studio software.",
        "link": "https://dl.acm.org/doi/10.1145/3472410.3472433"
    },
    {
        "title": "Assessment of Vulnerability Severity using Text Mining",
        "authors": [
            "Georgios Spanos",
            "Lefteris Angelis",
            "Dimitrios Toloudis"
        ],
        "date": "September 2017",
        "source": "PCI '17: Proceedings of the 21st Pan-Hellenic Conference on Informatics",
        "abstract": "Software1 vulnerabilities are closely associated with information systems security, a major and critical field in today's technology. Vulnerabilities constitute a constant and increasing threat for various aspects of everyday life, especially for safety and economy, since the social impact from the problems that they cause is complicated and often unpredictable. Although there is an entire research branch in software engineering that deals with the identification and elimination of vulnerabilities, the growing complexity of software products and the variability of software production procedures are factors contributing to the ongoing occurrence of vulnerabilities, Hence, another area that is being developed in parallel focuses on the study and management of the vulnerabilities that have already been reported and registered in databases. The information contained in such databases includes, a textual description and a number of metrics related to vulnerabilities. The purpose of this paper is to investigate to what extend the assessment of the vulnerability severity can be inferred directly from the corresponding textual description, or in other words, to examine the informative power of the description with respect to the vulnerability severity. For this purpose, text mining techniques, i.e. text analysis and three different classification methods (decision trees, neural networks and support vector machines) were employed. The application of text mining to a sample of 70,678 vulnerabilities from a public data source shows that the description itself is a reliable and highly accurate source of information for vulnerability prioritization.",
        "link": "https://dl.acm.org/doi/10.1145/3139367.3139390"
    },
    {
        "title": "Clustering Algorithms for Spatial Data Mining",
        "authors": [
            "Chetashri Bhadane",
            "Ketan Shah"
        ],
        "date": "April 2020",
        "source": "ICGDA '20: Proceedings of the 2020 3rd International Conference on Geoinformatics and Data Analysis",
        "abstract": "With the advances in mobile and wireless technologies, there has been a rise in applications that track and share the users' geospatial data. People use several social networking sites such as Twitter, Facebook and Flickr, where they share their status updates. With the integration of Global Positioning System (GPS) with mobile phones, it is now possible to share one's locations on these social networks. GPS allows us to record and track a person's movement along with the timestamp. The data set obtained from these GPS logs is vast and is widely used to analyze the users' movement patterns. Specifically, we can find out significant locations based on the number of users present at that location and the time spent by them at such places. Once significant places have been identified, it is also possible to identify the semantic importance of these locations. This paper presents an overview of the clustering techniques used to find important places of interest using large GPS based mobility datasets. Four clustering algorithms, K-Means, DBSCAN, OPTICS and Hierarchical, are implemented, and performance is tested using real-time data of 50 users collected over 2--5 years. Performance summary depicts that K-Means and DBSCAN perform well for spatial data.",
        "link": "https://dl.acm.org/doi/10.1145/3397056.3397068"
    },
    {
        "title": "Feature Selection based Arabic Text Classification using Different Machine Learning Algorithms: Comparative Study",
        "authors": [
            "Sakina Rim Bennabi",
            "Zakaria Elberrichi"
        ],
        "date": "June 2020",
        "source": "ICIST '20: Proceedings of the 10th International Conference on Information Systems and Technologies",
        "abstract": "Feature selection is a method of data pre-processing widely used when mining large data, such as textual classification. Several studies have been conducted to compare the different methods of feature selection applied to corpora in English. Unfortunately, a small number of works concern the Arabic language. This article aims to present a comparative study of different feature selection techniques including: Chi2, the ANOVA method and mutual information, applied on a corpus in Arabic language, while also diversifying the machine learning algorithms (Naive Bayes, SVM and KNN). This experimental study has shown in general that reducing dimensionality with feature selection techniques has slightly affected the performance of textual classification, reducing the size of the corpus by up to 1%.",
        "link": "https://dl.acm.org/doi/10.1145/3447568.3448531"
    },
    {
        "title": "Intelligent Data Analysis using Optimized Support Vector Machine Based Data Mining Approach for Tourism Industry",
        "authors": [
            "Ms Promila Sharma",
            "Uma Meena",
            "Girish Kumar Sharma"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Data analysis involves the deployment of sophisticated approaches from data mining methods, information theory, and artificial intelligence in various fields like tourism, hospitality, and so on for the extraction of knowledge from the gathered and preprocessed data. In tourism, pattern analysis or data analysis using classification is significant for finding the patterns that represent new and potentially useful information or knowledge about the destination and other data. Several data mining techniques are introduced for the classification of data or patterns. However, overfitting, less accuracy, local minima, sensitive to noise are the drawbacks in some existing data mining classification methods. To overcome these challenges, Support vector machine with Red deer optimization (SVM-RDO) based data mining strategy is proposed in this article. Extended Kalman filter (EKF) is utilized in the first phase, i.e., data cleaning to remove the noise and missing values from the input data. Mantaray foraging algorithm (MaFA) is used in the data selection phase, in which the significant data are selected for the further process to reduce the computational complexity. The final phase is the classification, in which SVM-RDO is proposed to access the useful pattern from the selected data. PYTHON is the implementation tool used for the experiment of the proposed model. The experimental analysis is done to show the efficacy of the proposed work. From the experimental results, the proposed SVM-RDO achieved better accuracy, precision, recall, and F1 score than the existing methods for the tourism dataset. Thus, it is showed the effectiveness of the proposed SVM-RDO for pattern analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3494566"
    },
    {
        "title": "Applying text mining to predict learners' cognitive engagement",
        "authors": [
            "Hayati Hind",
            "Mohammed Khalidi Idrissi",
            "Samir Bennani"
        ],
        "date": "October 2017",
        "source": "SCAMS '17: Proceedings of the Mediterranean Symposium on Smart City Application",
        "abstract": "In academic process, engagement represent one of the important success' factor. This paper proposes a predictive system for learners' cognitive engagement based on their online discussion forums participation. In the first level the ontology OWL and the LSA method are used to perform a semantic classification system of the threads according to a specific context chosen by the tutor. Then the Text mining, as a predictive method, is applied to the classified threads and learners' participation in the forums.",
        "link": "https://dl.acm.org/doi/10.1145/3175628.3175655"
    },
    {
        "title": "Investigate Transitions into Drug Addiction through Text Mining of Reddit Data",
        "authors": [
            "John Lu",
            "Sumati Sridhar",
            "Ritika Pandey",
            "Mohammad Al Hasan",
            "Georege Mohler"
        ],
        "date": "July 2019",
        "source": "KDD '19: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Increasing rates of opioid drug abuse and heightened prevalence of online support communities underscore the necessity of employing data mining techniques to better understand drug addiction using these rapidly developing online resources. In this work, we obtained data from Reddit, an online collection of forums, to gather insight into drug use/misuse using text snippets from users narratives. Specifically, using users' posts, we trained a binary classifier which predicts a user's transitions from casual drug discussion forums to drug recovery forums. We also proposed a Cox regression model that outputs likelihoods of such transitions. In doing so, we found that utterances of select drugs and certain linguistic features contained in one's posts can help predict these transitions. Using unfiltered drug-related posts, our research delineates drugs that are associated with higher rates of transitions from recreational drug discussion to support/recovery discussion, offers insight into modern drug culture, and provides tools with potential applications in combating the opioid crisis.",
        "link": "https://dl.acm.org/doi/10.1145/3292500.3330737"
    },
    {
        "title": "Analysing digital banking reviews using text mining",
        "authors": [
            "Li Chen Cheng",
            "Legaspi Rhea Sharmayne"
        ],
        "date": "December 2020",
        "source": "ASONAM '20: Proceedings of the 12th IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Digital banks are new entrants in the banking industry in the Philippines as they only started late 2018. Since then, a handful of players have and are still emerging. With more and more people becoming technologically savvy, it is very critical for financial institutions to develop a digital banking application that will stand out from the competition. This paper aims to use text mining methods to analyse digital banking application reviews. This study will perform topic modelling using LDA to explore customer concerns and will mine association rules between the digital banking features with the review score. The results will reveal which areas the digital banking application can further optimize for customer satisfaction and retention.",
        "link": "https://dl.acm.org/doi/10.1109/ASONAM49781.2020.9381429"
    },
    {
        "title": "Mining Moocs Videos Metadata Using Classification Techniques",
        "authors": [
            "El Harrak Othman",
            "Ghadi Abderrahim",
            "El Bouhdidi Jaber"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "In few years, the internet has become the first source of information for most people, Today MOOCs makes it possible for everyone to access to the education over the world, it's represents an emerging methodology of online teaching and an important development in open education. Due to the rapid development of Moocs and the rapid growth of digital data and video database over the Internet, it is becoming very difficult for learners to browsing and choosing the best training for them. The scientific community has increased the amount of research into new technologies, with a view to improve the digital video utilization: its archiving, indexing, accessibility, acquisition, store and even its process and usability. All these parts of the video utilization entail the necessity of the extraction of all important information of a video, especially in cases of lack of metadata information. Web video mining is retrieving the content using data mining techniques from World Wide Web. There are two approaches for web video mining using traditional image processing (signal processing) and metadata based approach. In this paper, we present the various research makes in this subject and we propose an effective methodology to extract the metadata from Moocs videos and classify them based on the extracted metadata by applying data mining techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090450"
    },
    {
        "title": "Text mining with HathiTrust",
        "authors": [
            "Eleanor Dickson Koehl",
            "Ryan Dubnicek"
        ],
        "date": "June 2019",
        "source": "JCDL '19: Proceedings of the 18th Joint Conference on Digital Libraries",
        "abstract": "This tutorial will introduce attendees to the HathiTrust Research Center and its tools and services for computational text analysis research. HTRC leverages the scope and scale of the HathiTrust Digital Library collection to create opportunities for researchers to perform text data mining on subsets of the corpus. Attendees will develop skills that will allow them to conduct basic text analysis research using HathiTrust data, learn techniques for engaging with scholars on their text data mining research, and explore strategies for computational access to large-scale digital collections.",
        "link": "https://dl.acm.org/doi/10.1109/JCDL.2019.00115"
    },
    {
        "title": "Text mining for plagiarism detection: multivariate pattern detection for recognition of text similarities",
        "authors": [
            "Konstantinos Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2018",
        "source": "ASONAM '18: Proceedings of the 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "The problem of plagiarism the recent years has been intensified by the availability of information in digital form and the accessibility of the electronic libraries through the Internet. As a result, plagiarism detection has been transformed into a big data analytics problem since the number of digital sources is extravagant and a new document needs to be compared with millions of other existing documents. In this paper, a text mining methodology is proposed that can detect all common patterns between a document and the documents in a reference database. The technique is based on a pattern detection algorithm and the corresponding data structure that enables the algorithm to detect all common patterns. The methodology has been applied in a well-defined dataset providing very promising results identifying difficult cases of plagiarism such as technical disguise.",
        "link": "https://dl.acm.org/doi/10.5555/3382225.3382424"
    },
    {
        "title": "Petroleum Engineering Data Text Classification Using Convolutional Neural Network Based Classifier",
        "authors": [
            "Tzuhan Hsu",
            "Yaoqin Zhang"
        ],
        "date": "May 2018",
        "source": "ICMLT '18: Proceedings of the 2018 International Conference on Machine Learning Technologies",
        "abstract": "In this paper, authors introduce a convolutional neural network (CNN) based classification approach for petroleum engineering event. Recently, text classification problem has raised substantial attention in the field of machine learning. The aim of text classification process is to assign correct label to a text on the basis of its content. However, due to the imprecise of feature representation, most of former text classification methods cannot export satisfying result. To solve this problem, authors propose a model based on convolutional neural network. Authors conduct experiments with real world petroleum engineering data text, artificially labelled into 6 categories. Beside purposed CNN-based model, the authors also implement other baseline machine learning approaches: k nearest neighbor, random forest, support vector machines, recurrent neural network, gated recurrent units, long short-term memory. Among all these models, our CNN-based model reach best performance in text classification task. In the last part of this paper, authors compare and analysis their purpose and baseline model.",
        "link": "https://dl.acm.org/doi/10.1145/3231884.3231898"
    },
    {
        "title": "Classifying Indonesian Online Articles as Advertisement Placement Base Using Text Mining",
        "authors": [
            "Nadhira Tasya",
            "Arian Dhini"
        ],
        "date": "July 2017",
        "source": "ICBIM 2017: Proceedings of the International Conference on Business and Information Management",
        "abstract": "Rapid development in technological aspect resulting in growing level of human needs for the latest news, so that emerged a new trend of publishing and accessing news through online media or usually called online journalism. In addition, the number of people who sell and purchase through online sites also continues to increase and this opportunity is utilized by the company and the advertiser by implementing targeted web advertising. However, the high number of articles that have been published and accessed leads to great opportunities for errors in determining where to place the ads. Therefore, it needs a system that can categorize articles accessed by users as the basis of advertisement placement by the company and this classification system can be done by applying the method of Data Mining and Text Mining. This research uses document data in the form of article content that will be categorized into twenty categories of class of advertisement by using Text Mining technique with Support Vector Machine algorithm. The results of this study may be used by companies or advertisers as a basis for placement of ads on selected online media sites.",
        "link": "https://dl.acm.org/doi/10.1145/3134271.3134288"
    },
    {
        "title": "Mining Free-Text Medical Notes for Suicide Risk Assessment",
        "authors": [
            "Marios Adamou",
            "Grigoris Antoniou",
            "Elissavet Greasidou",
            "Vincenzo Lagani",
            "Paulos Charonyktakis",
            "Ioannis Tsamardinos"
        ],
        "date": "July 2018",
        "source": "SETN '18: Proceedings of the 10th Hellenic Conference on Artificial Intelligence",
        "abstract": "Suicide has been considered as an important public health issue for a very long time, and is one of the main causes of death worldwide. Despite suicide prevention strategies being applied, the rate of suicide has not changed substantially over the past decades. Advances in machine learning make it possible to attempt to predict suicide based on the analysis of relevant data to inform clinical practice. This paper reports on findings from the analysis of data of patients who died by suicide in the period 2013-2016 and made use of both structured data and free-text medical notes. We focus on examining various text-mining approaches to support risk assessment. The results show that using advance machine learning and text-mining techniques, it is possible to predict within a specified period which people are most at risk of taking their own life at the time of referral to a mental health service.",
        "link": "https://dl.acm.org/doi/10.1145/3200947.3201020"
    },
    {
        "title": "Integration of data mining techniques in e-learning systems: Clustering Profil of Lerners and Recommender Course System",
        "authors": [
            "A. El Moustamid",
            "E. En-Naimi",
            "J. El Bouhdidi"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "Data mining aims at the extraction of knowledge from large quantities of data by automatic or semi-automatic methods. In the field of education and training e-learning, Data Mining techniques as clustering (The aim of this technique is to identify groups that are similar in some aspect) [21], prediction (is used for developing model where combination of data viewed from different aspects can effect single viewed data. Here, is important to watch how predictors variables are effecting predicted variable)[21], classification are applied in e-learning to improve the level of learners by providing learning according to the learners' requirement) are used to analyze learners profile, to predict and improve student performance. The aim of our project is to develop a system capable of analyzing learners profile and indexing web videos in order to offer learners a rich database with courses that correspond to their levels.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090453"
    },
    {
        "title": "Multi-level mining and visualization of scientific text collections: Exploring a bi-lingual scientific repository",
        "authors": [
            "Pablo Accuosto",
            "Francesco Ronzano",
            "Daniel Ferrés",
            "Horacio Saggion"
        ],
        "date": "December 2017",
        "source": "WOSP 2017: Proceedings of the 6th International Workshop on Mining Scientific Publications",
        "abstract": "We present a system to mine and visualize collections of scientific documents by semantically browsing information extracted from single publications or aggregated throughout corpora of articles. The text mining tool performs deep analysis of document collections allowing the extraction and interpretation of research paper's contents. In addition to the extraction and enrichment of documents with metadata (titles, authors, affiliations, etc), the deep analysis performed comprises semantic interpretation, rhetorical analysis of sentences, triple-based information extraction, and text summarization. The visualization components allow geographical-based exploration of collections, topic-evolution interpretation, and collaborative network analysis among others. The paper presents a case study of a bi-lingual collection in the field of Natural Language Processing (NLP).",
        "link": "https://dl.acm.org/doi/10.1145/3127526.3127529"
    },
    {
        "title": "Hierarchical Topic Mining via Joint Spherical Tree and Text Embedding",
        "authors": [
            "Yu Meng",
            "Yunyi Zhang",
            "Jiaxin Huang",
            "Yu Zhang",
            "Chao Zhang",
            "Jiawei Han"
        ],
        "date": "August 2020",
        "source": "KDD '20: Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Mining a set of meaningful topics organized into a hierarchy is intuitively appealing since topic correlations are ubiquitous in massive text corpora. To account for potential hierarchical topic structures, hierarchical topic models generalize flat topic models by incorporating latent topic hierarchies into their generative modeling process. However, due to their purely unsupervised nature, the learned topic hierarchy often deviates from users' particular needs or interests. To guide the hierarchical topic discovery process with minimal user supervision, we propose a new task, Hierarchical Topic Mining, which takes a category tree described by category names only, and aims to mine a set of representative terms for each category from a text corpus to help a user comprehend his/her interested topics. We develop a novel joint tree and text embedding method along with a principled optimization procedure that allows simultaneous modeling of the category tree structure and the corpus generative process in the spherical space for effective category-representative term discovery. Our comprehensive experiments show that our model, named JoSH, mines a high-quality set of hierarchical topics with high efficiency and benefits weakly-supervised hierarchical text classification tasks.",
        "link": "https://dl.acm.org/doi/10.1145/3394486.3403242"
    },
    {
        "title": "Unsupervised behavioural mining and clustering for malware family identification",
        "authors": [
            "Khanh Huu The Dam",
            "Thomas Given-Wilson",
            "Axel Legay"
        ],
        "date": "March 2021",
        "source": "SAC '21: Proceedings of the 36th Annual ACM Symposium on Applied Computing",
        "abstract": "More accurate and advanced detection and classification of malware requires exploiting program behaviour and not merely syntactic or static features. One approach is to use system call dependency graphs (SCDGs) that represent the program behaviour by interactions with the system, and the relations between these interactions. These SCDGs have been used with supervised learning techniques to very accurately detect and classify malware. This works considers the unsupervised learning challenge of mining for common clusters of behaviour without a priori knowledge. This allows for clustering of similar programs by behaviour, that can then be used for either classification or further analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3412841.3441919"
    },
    {
        "title": "Text Mining for Evaluating Authors' Birth and Death Years",
        "authors": [
            "Dror Moghaz",
            "Yaakov Hacohen-Kerner",
            "Dov Gabbay"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "This article presents a unique method in text and data mining for finding the era, i.e., mining temporal data, in which an anonymous author was living. Finding this era can assist in the examination of a fake document or extracting the time period in which a writer lived. The study and the experiments concern Hebrew, and in some parts, Aramaic and Yiddish rabbinic texts. The rabbinic texts are undated and contain no bibliographic sections, posing an interesting challenge. This work proposes algorithms using key phrases and key words that allow the temporal organization of citations together with linguistic patterns. Based on these key phrases, key words, and the references, we established several types of “Iron-clad,” Heuristic and Greedy rules for estimating the years of birth and death of a writer in an interesting classification task. Experiments were conducted on corpora, including documents authored by 12, 24, and 36 rabbinic writers and demonstrated promising results.",
        "link": "https://dl.acm.org/doi/10.1145/3281631"
    },
    {
        "title": "Transfer Learning to Timed Text Based Video Classification Using CNN",
        "authors": [
            "Zenun Kastrati",
            "Ali Shariq Imran",
            "Arianit Kurti"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "Open educational video resources are gaining popularity with a growing number of massive open online courses (MOOCs). This has created a niche for content providers to adopt effective solutions in automatically organizing and structuring of educational resources for maximum visibility. Recent advances in deep learning techniques are proving useful in managing and classifying resources into appropriate categories. This paper proposes one such convolutional neural network (CNN) model for classifying video lectures in a MOOC setting using a transfer learning approach. The model uses a time-aligned text transcripts corresponding to video lectures from six broader subject categories. Video lectures and their corresponding transcript dataset is gathered from the Coursera MOOC platform. Two different CNN models are proposed: i) CNN based classification using embeddings learned from our MOOC dataset, ii) CNN based classification using transfer learning. Word embeddings generated from two well known state-of-the-art pre-trained models Word2Vec and GloVe, are used in the transfer learning approach for the second case. The proposed CNN models are evaluated using precision, recall, and F1 score and the obtained performance is compared with both conventional and deep learning classifiers. The proposed CNN models have an F1 score improvement of 10-22 percentage points over DNN and conventional classifiers",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326483"
    },
    {
        "title": "Evolutionary role mining in complex networks by ensemble clustering",
        "authors": [
            "Sarvenaz Choobdar",
            "Pedro Ribeiro",
            "Fernando Silva"
        ],
        "date": "April 2017",
        "source": "SAC '17: Proceedings of the Symposium on Applied Computing",
        "abstract": "The structural patterns in the neighborhood of nodes assign unique roles to the nodes. Mining the set of existing roles in a network provides a descriptive profile of the network and draws its general picture. This paper proposes a new method to determine structural roles in a dynamic network based on the current position of nodes and their historic behavior. We develop a temporal ensemble clustering technique to dynamically find groups of nodes, holding similar tempo-structural roles. We compare two weighting functions, based on age and distribution of data, to incorporate temporal behavior of nodes in the role discovery. To evaluate the performance of the proposed method, we assess the results from two points of view: 1) goodness of fit to current structure of the network; 2) consistency with historic data. We conduct the evaluation using different ensemble clustering techniques. The results on real world networks demonstrate that our method can detect tempo-structural roles that simultaneously depict the topology of a network and reflect its dynamics with high accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3019612.3019815"
    },
    {
        "title": "Analyzing used-car web listings via text mining",
        "authors": [
            "Ayhan Demiriz",
            "Fatma Cantaş"
        ],
        "date": "October 2017",
        "source": "IML '17: Proceedings of the 1st International Conference on Internet of Things and Machine Learning",
        "abstract": "Used car trade is one of the major components of the world economies. It is not uncommon to sell a car by placing an internet advertisement irrespective of the geography in these days. A typical content of an advertisement is usually composed of two parts namely the structured and the free text data. The structured data may include some information about the asking price, make, model, year, mileage of the car and the contact info. In most cases, seller may give important clues about the car's current conditions in the free text data where the title (head) of the advertisement can be included as free text too. This paper reports preliminary results from a text mining study conducted on 75K used car internet listings collected from two major car listing web sites in Turkey. As expected, the words and the phrases related to the description of the car are observed to be frequent. The leading concepts in the free text are found to be regarding how to describe the current condition of a car, for example \"no crash history\".",
        "link": "https://dl.acm.org/doi/10.1145/3109761.3109782"
    },
    {
        "title": "Text classification of Diseases Treated by Traditional Chinese Medicine Prescription based on machine learning",
        "authors": [
            "Hanqing Zhao",
            "Bo Han",
            "Chen Li"
        ],
        "date": "September 2020",
        "source": "ISAIMS '20: Proceedings of the 1st International Symposium on Artificial Intelligence in Medical Sciences",
        "abstract": "OBJECTIVE: To explore the application of machine learning in th e identification of diseases treated by traditional Chinese medicine prescriptions. METHODS: Based on the composition of the text document of Chinese medicine prescriptions, the prescriptions were divided into cough, headache and diarrhea. THUTCT were introduced to establish and train two machine learning text classification models, LibSVM and LibLinear, and the prescriptions to be deter mined were put into the model for classification and prediction. R ESULTS: The Precision rate of LibSVM model and Liblinear mod el were 0.7283 and 0.6690. Seven prescriptions were classified an d predicted, and the results were in line with expectations. CONCLUSION: THUCTC has good universality for the content of TCM prescriptions, high classification accuracy and fast testing speed, which is suitable for the text classification and discrimination research of TCM prescriptions for diseases.",
        "link": "https://dl.acm.org/doi/10.1145/3429889.3429895"
    },
    {
        "title": "Evaluation of retweet clustering method classification method using retweets on Twitter without text data",
        "authors": [
            "K. Uchida",
            "F. Toriumi",
            "T. Sakaki"
        ],
        "date": "August 2017",
        "source": "WI '17: Proceedings of the International Conference on Web Intelligence",
        "abstract": "Burst phenomena, which frequently occur on social media, are caused by such social events as flaming on the internet, elections, and natural disasters. To understand people's thoughts and feelings, we must classify their opinions from burst phenomena. Therefore, classification methods that categorize tweets are critical. However, since most classification methods focus on text mining, they cannot group tweets by topics because each tweet has poor linguistic similarities. We used a non-text-based classification method proposed by Baba et al. that groups tweets by topics, even if they have poor linguistic similarities, and verified its validity by comparing it with a text-based classification method in two different evaluations: qualitative and quantitative. In the qualitative evaluation part, we did a questionnaire survey and validated the suitability of the topic clusters created using both the non-and text-based methods. Since evaluating the similarity of every pair of tweets in each topic is difficult, we evaluated the similarity between sampled pairs in the survey and acquired more appropriate topic clustering results using the non-text-based method than the text-based method. In the quantitative evaluation part, we focused on the robustness of each method against data reduction. Many approaches analyze social media data, especially because collecting data from social media is comparatively easy. However, since collecting the whole data of burst phenomena is very costly due to the vast amounts of available social media data, robustness against data reduction is an important index to evaluate classification methods. With the non-text-based method, over 55% of the pairs of tweets in the same cluster were also included in the same cluster even when the data were reduced to 10% in all three of our example cases. In this paper, as a source we focus on Twitter, one of the most popular microblogging services. Using clustering to conduct detailed case analyses, we scrutinized three burst cases that include natural disasters and flaming on the internet and found that a non-text-based method more effectively classified tweets in burst phenomena than a text-based method.",
        "link": "https://dl.acm.org/doi/10.1145/3106426.3106451"
    },
    {
        "title": "Using Spark for Text Mining on Large Scale Liver Cancer Literature",
        "authors": [
            "Ming-Yen Lin",
            "Yu-Ju Lin",
            "Sue-Chen Hsueh"
        ],
        "date": "January 2021",
        "source": "BDET 2021: 2021 the 3rd International Conference on Big Data Engineering and Technology (BDET)",
        "abstract": "Cancer is one of the main causes of death. The number of known cancer to-date is more than one hundred. Liver cancer, ranked after trachea and lung cancer, has long been high in the cancer leading cause in Taiwan and even ranked as second in 2016. The number of scientific articles related to cancer proliferates every year. The number reaches as high as 20 million in PubMed so that discovering useful information from the massive collection is very difficult. In addition, using a single machine to sift through these articles is very time-consuming. Therefore, we present a big data analytic framework using the distributed Apache Spark platform for text mining in PubMed literature. We establish a prediction model for liver cancer articles so that researchers may effectively validate whether an article is related to liver cancer or not. Classification models in Spark MLlib including Linear Support Vector Machines (SVM), Logistic Regression, are used in our experiments. Relevancy to liver cancer is further confirmed by using MeSH (Medical Subject Headings) terms. Logistic regression is about 3 times faster than SVMs and the accuracy of both methods is close to 95% in the experiments using hold-out validation. When max_features is 500 and min_df ≤ 0.1 (or min_df = 1), the accuracy may reach 96%. In the experiments with K-fold cross-validation, the accuracy of SVMs methods is 96%. The experimental results show that that our prediction model may effectively classify liver cancer articles.",
        "link": "https://dl.acm.org/doi/10.1145/3474944.3474958"
    },
    {
        "title": "Automatic topic classification of test cases using text mining at an Android smartphone vendor",
        "authors": [
            "Junji Shimagaki",
            "Yasutaka Kamei",
            "Naoyasu Ubayashi",
            "Abram Hindle"
        ],
        "date": "October 2018",
        "source": "ESEM '18: Proceedings of the 12th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Background: An Android smartphone is an ecosystem of applications, drivers, operating system components, and assets. The volume of the software is large and the number of test cases needed to cover the functionality of an Android system is substantial. Enormous effort has been already taken to properly quantify \"what features and apps were tested and verified?\". This insight is provided by dashboards that summarize test coverage and results per feature. One method to achieve this is to manually tag or label test cases with the topic or function they cover, much like function points. At the studied Android smartphone vendor, tests are labelled with manually defined tags, so-called \"feature labels (FLs)\", and the FLs serve to categorize 100s to 1000s test cases into 10 to 50 groups. Aim: Unfortunately for developers, manual assignment of FLs to 1000s of test cases is a time consuming task, leading to inaccurately labeled test cases, which will render the dashboard useless. We created an automated system that suggests tags/labels to the developers for their test cases rather than manual labeling. Method: We use machine learning models to predict and label the functionality tested by 10,000 test cases developed at the company. Results: Through the quantitative experiments, our models achieved acceptable F-1 performance of 0.3 to 0.88. Also through the qualitative studies with expert teams, we showed that the hierarchy and path of tests was a good predictor of a feature's label. Conclusions: We find that this method can reduce tedious manual effort that software developers spent classifying test cases, while providing more accurate classification results.",
        "link": "https://dl.acm.org/doi/10.1145/3239235.3268927"
    },
    {
        "title": "To apply Data Mining for Classification of Crowd sourced Software Requirements",
        "authors": [
            "Soonh Taj",
            "Qasim Arain",
            "Imran Memon",
            "Asma Zubedi"
        ],
        "date": "April 2019",
        "source": "ICSIE '19: Proceedings of the 8th International Conference on Software and Information Engineering",
        "abstract": "Now a day's main focus of developers is to build quality software that works according to customer needs and for this reason it is necessary to gather right requirements as requirement elicitation is the critical step that impacts on the success of software project as misinterpreted requirements leads to the failure of software project. By keeping this in mind a research is carried out on improving requirements elicitation process and automating the process of classifying requirements. In this research, a model is proposed which will help in this scenario for requirements elicitation and requirement classification. This paper presents a model in which crowd sourcing approach is used so that customers, end users, stakeholders, developers and software engineers can make active participation for requirement elicitation process and requirements gathered using crowdsourcing approach are used by model for classification process i.e. classification of requirements into functional and non-functional requirements. For the proof of proposed model a case study is conducted. Results of case study provided the usefulness and efficiency of proposed model for classification of crowd sourced software requirements.",
        "link": "https://dl.acm.org/doi/10.1145/3328833.3328837"
    },
    {
        "title": "Text classification using Fuzzy TF-IDF and Machine Learning Models",
        "authors": [
            "Mariem Bounabi",
            "Karim El Moutaouakil",
            "Khalid Satori"
        ],
        "date": "October 2019",
        "source": "BDIoT'19: Proceedings of the 4th International Conference on Big Data and Internet of Things",
        "abstract": "The representation of the information has an important impact on the text classification task. Several weighting methods were proposed in the literature, and the term frequency-inverse term frequency (TFIDF), the most know on the text treatment field. The FTF-IDF is a vector representation where the components of the TFIDF are presented as inputs to the Fuzzy Inference System (FIS). In this work, we compare several Machin Learning algorithms such as Naïve Bayes and its derivatives, SVM and Random forest classifiers, using the FTF-IDF representation. To improve the quality of the used classifiers, we call sum attribute selection methods. The recognition rate, for the tested systems, is satisfied, where the system based on naïve Bayes classifier, the FTF-IDF weighting terms, and the info gain select attributes method gives 98.7% as accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3372938.3372956"
    },
    {
        "title": "Text Mining in Cybersecurity: A Systematic Literature Review",
        "authors": [
            "Luciano Ignaczak",
            "Guilherme Goldschmidt",
            "Cristiano André Da Costa",
            "Rodrigo Da Rosa Righi"
        ],
        "date": null,
        "source": "ACM Computing Surveys",
        "abstract": "The growth of data volume has changed cybersecurity activities, demanding a higher level of automation. In this new cybersecurity landscape, text mining emerged as an alternative to improve the efficiency of the activities involving unstructured data. This article proposes a Systematic Literature Review (SLR) to present the application of text mining in the cybersecurity domain. Using a systematic protocol, we identified 2,196 studies, out of which 83 were summarized. As a contribution, we propose a taxonomy to demonstrate the different activities in the cybersecurity domain supported by text mining. We also detail the strategies evaluated in the application of text mining tasks and the use of neural networks to support activities involving unstructured data. The work also discusses text classification performance aiming its application in real-world solutions. The SLR also highlights open gaps for future research, such as the analysis of non-English content and the intensification in the usage of neural networks.",
        "link": "https://dl.acm.org/doi/10.1145/3462477"
    },
    {
        "title": "Using Text Mining to Analyze the Financial Patents",
        "authors": [
            "Yung-Feng Lu",
            "Jia-Lang Xu",
            "Mu-Yen Chen"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "With the advancement and development of science and technology, Nowadays society is the development of information technology, cloud computing, mobile Internet, Internet of Things, block chain and information technology.The combination of finance and technology has made financial technology terms gradually emerging.. This research analyzes the patents of financial technology. However, Taiwan's financial technology patents are mostly G06Q-20 (payment plan, architecture or agreement), G06Q-30 (commercial, such as marketing, shopping, payment, auction or e-commerce). G06Q-40 (financial, investment or tax treatment, insurance).The research use text mining technology to analyze and understand Taiwan's trends and developments in financial technology. The research results show that G06Q-20 patents are biased towards information security and mobile payment. G06Q-30 patents favor shopping websites and advertising. G06Q-40 patents are biased towards diversity, such as securities, insurance, finance, and stocks.",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326478"
    },
    {
        "title": "Identifying Online Profiles of Distance Learning Students Using Data Mining Techniques",
        "authors": [
            "Osama Islam",
            "Muazzam Siddiqui",
            "Naif Radi Aljohani"
        ],
        "date": "October 2019",
        "source": "ICDTE '19: Proceedings of the 3rd International Conference on Digital Technology in Education",
        "abstract": "Educational data has grown over the years with the increased use of technology within educational environments. This has led to a huge amount of data being stored in various data sources representing the student, his/her activities, and other aspects relevant to the learning process. To meet this analytical need, Educational Data Mining (EDM) has emerged to assist educational institutions in identifying key benefits such as students at risk, the level of student engagement or predicting student performance. The aim of this research was to explore the various aspects of student interaction data using data mining techniques to identify relevant patterns of behaviors and possible key attributes that have higher degrees of influence on distance learning students. The main findings identified several patterns of user online profiles based on a set of adopted learning strategies, the research proposed a framework for analyzing such interaction data based on R and Hadoop platforms to correlate online profiles with student performance.",
        "link": "https://dl.acm.org/doi/10.1145/3369199.3369249"
    },
    {
        "title": "Extremely Fast Decision Tree Mining for Evolving Data Streams",
        "authors": [
            "Albert Bifet",
            "Jiajin Zhang",
            "Wei Fan",
            "Cheng He",
            "Jianfeng Zhang",
            "Jianfeng Qian",
            "Geoff Holmes",
            "Bernhard Pfahringer"
        ],
        "date": "August 2017",
        "source": "KDD '17: Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
        "abstract": "Nowadays real-time industrial applications are generating a huge amount of data continuously every day. To process these large data streams, we need fast and efficient methodologies and systems. A useful feature desired for data scientists and analysts is to have easy to visualize and understand machine learning models. Decision trees are preferred in many real-time applications for this reason, and also, because combined in an ensemble, they are one of the most powerful methods in machine learning. In this paper, we present a new system called STREAMDM-C++, that implements decision trees for data streams in C++, and that has been used extensively at Huawei. Streaming decision trees adapt to changes on streams, a huge advantage since standard decision trees are built using a snapshot of data, and can not evolve over time. STREAMDM-C++ is easy to extend, and contains more powerful ensemble methods, and a more efficient and easy to use adaptive decision trees. We compare our new implementation with VFML, the current state of the art implementation in C, and show how our new system outperforms VFML in speed using less resources.",
        "link": "https://dl.acm.org/doi/10.1145/3097983.3098139"
    },
    {
        "title": "Data miners' little helper: data transformation activity cues for cluster analysis on document collections",
        "authors": [
            "Tania Cerquitelli",
            "Evelina Di Corso",
            "Francesco Ventura",
            "Silvia Chiusano"
        ],
        "date": "June 2017",
        "source": "WIMS '17: Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "In this paper we propose a new self-learning engine to streamline the analytics process, as it enables analysts to mine massive data repositories with minimal user intervention. In the context of cluster analysis on a collection of documents this new system, named SELF-DATA (SELF-learning DAta TrAnsformation), suggests to the analyst how to configure the whole mining process for a given dataset. SELF-DATA relies on an engine exploring different data weighting schemas (e.g., normalized term frequencies) and data transformation methods (e.g., PCA) before applying the cluster analysis, evaluating and comparing solutions through different quality indices (e.g., weighted Silhouette), and presenting the k-top solutions to the analyst. SELF-DATA will also include a knowledge base storing results of experiments on previously processed datasets, and a classification algorithm trained on the knowledge base content to forecast the best configuration for the whole mining process for an unexplored dataset. The first development of SELF-DATA running on Apache Spark has been validated on 5 collections of documents. Experimental results highlight that TF-IDF and logarithmic entropy are effective to measure item relevance with sparse datasets, and the LSI method outperforms PCA with a large dictionary.",
        "link": "https://dl.acm.org/doi/10.1145/3102254.3102288"
    },
    {
        "title": "Interactive Visual Graph Mining and Learning",
        "authors": [
            "Ryan A. Rossi",
            "Nesreen K. Ahmed",
            "Rong Zhou",
            "Hoda Eldardiry"
        ],
        "date": null,
        "source": "ACM Transactions on Intelligent Systems and Technology",
        "abstract": "This article presents a platform for interactive graph mining and relational machine learning called GraphVis. The platform combines interactive visual representations with state-of-the-art graph mining and relational machine learning techniques to aid in revealing important insights quickly as well as learning an appropriate and highly predictive model for a particular task (e.g., classification, link prediction, discovering the roles of nodes, and finding influential nodes). Visual representations and interaction techniques and tools are developed for simple, fast, and intuitive real-time interactive exploration, mining, and modeling of graph data. In particular, we propose techniques for interactive relational learning (e.g., node/link classification), interactive link prediction and weighting, role discovery and community detection, higher-order network analysis (via graphlets, network motifs), among others. GraphVis also allows for the refinement and tuning of graph mining and relational learning methods for specific application domains and constraints via an end-to-end interactive visual analytic pipeline that learns, infers, and provides rapid interactive visualization with immediate feedback at each change/prediction in real-time. Other key aspects include interactive filtering, querying, ranking, manipulating, exporting, as well as tools for dynamic network analysis and visualization, interactive graph generators (including new block model approaches), and a variety of multi-level network analysis techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3200764"
    },
    {
        "title": "Data Mining and Opinion Mining: A Tool in Educational Context",
        "authors": [
            "Myriam Peñafiel",
            "Stefanie Vásquez",
            "Diego Vásquez",
            "Juan Zaldumbide",
            "Sergio Luján-Mora"
        ],
        "date": "July 2018",
        "source": "ICoMS '18: Proceedings of the 2018 1st International Conference on Mathematics and Statistics",
        "abstract": "The use of the web as a universal communication platform generates large volumes of data (Big data), which in many cases, need to be processed so that they can become useful knowledge in face of the sceptics who have doubts about the credibility of such information. The use of web data that comes from educational contexts needs to be addressed, since that large amount of unstructured information is not being valued, losing valuable information that can be used. To solve this problem, we propose the use of data mining techniques such as sentiment analysis to validate the information that comes from the educational platforms. The objective of this research is to propose a methodology that allows the user to apply sentiment analysis in a simple way, because although some researchers have done it, very few do with data in the educational context. The results obtained prove that the proposal can be used in similar cases.",
        "link": "https://dl.acm.org/doi/10.1145/3274250.3274263"
    },
    {
        "title": "Research Hotspots Mining and Visualized Analysis Based on Linking Cluster and K-Core Decomposition",
        "authors": [
            "Ying Cai",
            "Fang Huang",
            "Shuai Wang",
            "Hao Zhang",
            "Chunxiu Du"
        ],
        "date": "May 2018",
        "source": "ICDPA 2018: Proceedings of the International Conference on Data Processing and Applications",
        "abstract": "In this paper, we focus on research hotspots mining with knowledge semantic features on discipline topic word networks, which are extracted from scientific and technical documents. For the topic networks, a novel method of combining linking community clustering with k-core decomposition is proposed to mine the hotspot community with knowledge hierarchy structure. And as a post-processing, the hierarchical community identification is implemented by k-core decomposition in order to discover the internal connection with knowledge semantic hierarchy in the research hotspot. Finally, the community density and visualization method is utilized to analyze the core and sub- problem in the research hotspot community. In experiments, the topic networks for a single discipline and interdisciplinary research hotspot are taken as examples of analysis, and the results show the proposed approach can effectively realize the objective of mining and analyzing the discipline research hotspot on the academic topic word network.",
        "link": "https://dl.acm.org/doi/10.1145/3224207.3224216"
    },
    {
        "title": "The Application of Data Mining Algorithms in the Construction of Travel Recommendation System",
        "authors": [
            "Chao Lou",
            "Zhaonan Mu",
            "Mengzhu Liu"
        ],
        "date": "January 2021",
        "source": "CONF-CDS 2021: The 2nd International Conference on Computing and Data Science",
        "abstract": "Photos shared by users of online social network often associated with tags, texts, geographic and time information. These data are ideal research resource which can be used by researchers to observe people's interests and behaviors. They can also be used to analyze people's travel behaviors and interests because it can reflect people's travel activities and experiences directly. This project aimed to consolidate people into groups based on their travel preferences of travel season, travel time span, scenery type of travel location and then analyze popular travel places for different people groups. The travel preferences were analyzed from extracted data applying PHP language and Naive Bayes classification approach. According to the analyzed data, people were divided into groups. Then, DBSCAN clustering was used to get popular locations based on the different groups. As a result, 80 thousand photo data of 5 cities have been collected from Flickr by Phython. It has been showed that people's preference of season in most of the mined cities was spring and summer and the preference of time span was short. Moreover, mined people has been divided into 16 groups according to their preference and based on the groups popular locations have been calculated by DBSCAN clustering. Theses locations have been provided in a recommendation website which was built in this project. For the future work, mining Flickr data associated with other travel social network sites could be a good way to build a more effective recommendation system.",
        "link": "https://dl.acm.org/doi/10.1145/3448734.3450477"
    },
    {
        "title": "Compare Machine Learning Models in Text Classification Using Steam User Reviews",
        "authors": [
            "Youchen Miao",
            "Zeyu Jin",
            "Yumeng Zhang",
            "Yuchen Chen",
            "Junren Lai"
        ],
        "date": "November 2021",
        "source": "ICSED '21: Proceedings of the 2021 3rd International Conference on Software Engineering and Development",
        "abstract": "Text Classification and Sentiment Analysis of game reviews are viewed as important parts in not only academic fields but also in game studies. In this paper, with more than 400 thousand game reviews on Steam platform, we preprocess the data using different libraries (sklearn, nltk, and spaCy) and use them as inputs to build three sentiment classification models based on different algorithms (Naive Bayes, SVM, and Random Forest). In contrast to previous studies that only focus on different sentiment analysis models, our paper also highlights the use of different APIs to preprocess the data and their corresponding model performance. The results show that no matter which API we choose, Random Forest models always perform the best. However, in terms of training time, Naive Bayes is the fastest. This work can be used to apply grid search for researchers to automatically find the optimum API before conducting sentiment analysis in the future.",
        "link": "https://dl.acm.org/doi/10.1145/3507473.3507480"
    },
    {
        "title": "Lost in Transduction: Transductive Transfer Learning in Text Classification",
        "authors": [
            "Alejandro Moreo",
            "Andrea Esuli",
            "Fabrizio Sebastiani"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Obtaining high-quality labelled data for training a classifier in a new application domain is often costly. Transfer Learning\n(a.k.a. “Inductive Transfer”) tries to alleviate these costs by transferring, to the “target” domain of interest, knowledge available from a different “source” domain. In transfer learning the lack of labelled information from the target domain is compensated by the availability at training time of a set of unlabelled examples from the target distribution. Transductive Transfer Learning denotes the transfer learning setting in which the only set of target documents that we are interested in classifying is known and available at training time. Although this definition is indeed in line with Vapnik’s original definition of “transduction”, current terminology in the field is confused. In this article, we discuss how the term “transduction” has been misused in the transfer learning literature, and propose a clarification consistent with the original characterization of this term given by Vapnik. We go on to observe that the above terminology misuse has brought about misleading experimental comparisons, with inductive transfer learning methods that have been incorrectly compared with transductive transfer learning methods. We then, give empirical evidence that the difference in performance between the inductive version and the transductive version of a transfer learning method can indeed be statistically significant (i.e., that knowing at training time the only data one needs to classify indeed gives an advantage). Our clarification allows a reassessment of the field, and of the relative merits of the major, state-of-the-art algorithms for transfer learning in text classification.",
        "link": "https://dl.acm.org/doi/10.1145/3453146"
    },
    {
        "title": "Machine Learning on Mining Potential Adverse Drug Reactions for Pharmacovigilance",
        "authors": [
            "Ke Jia"
        ],
        "date": "July 2021",
        "source": "DSIT 2021: 2021 4th International Conference on Data Science and Information Technology",
        "abstract": "Drugs are related to the lives and health of patients. Mining and discovering potential adverse drug reactions from a large number of unstructured texts play an important role in drug research and pharmacovigilance. It is a hot research issue in the field of biomedicine in recent years. A large number of scholars have discovered a large amount of information on adverse drug reactions from different data sources through effective machine learning methods. This paper reviews the research progress in recent years to provide a reference for future research.",
        "link": "https://dl.acm.org/doi/10.1145/3478905.3478964"
    },
    {
        "title": "TACAM: Topic And Context Aware Argument Mining",
        "authors": [
            "Michael Fromm",
            "Evgeniy Faerman",
            "Thomas Seidl"
        ],
        "date": "October 2019",
        "source": "WI '19: IEEE/WIC/ACM International Conference on Web Intelligence",
        "abstract": "In this work we address the problem of argument search. The purpose of argument search is the distillation of pro and contra arguments for requested topics from large text corpora. In previous works, the usual approach is to use a standard search engine to extract text parts which are relevant to the given topic and subsequently use an argument recognition algorithm to select arguments from them. The main challenge in the argument recognition task, which is also known as argument mining, is that often sentences containing arguments are structurally similar to purely informative sentences without any stance about the topic. In fact, they only differ semantically. Most approaches use topic or search term information only for the first search step and therefore assume that arguments can be classified independently of a topic. We argue that topic information is crucial for argument mining, since the topic defines the semantic context of an argument. Precisely, we propose different models for the classification of arguments, which take information about a topic of an argument into account. Moreover, to enrich the context of a topic and to let models understand the context of the potential argument better, we integrate information from different external sources such as Knowledge Graphs or pre-trained NLP models. Our evaluation shows that considering topic information, especially in connection with external information, provides a significant performance boost for the argument mining task.",
        "link": "https://dl.acm.org/doi/10.1145/3350546.3352506"
    },
    {
        "title": "Text mining for incoming tasks based on the urgency/importance factors and task classification using machine learning tools",
        "authors": [
            "Yasser Ali Alshehri"
        ],
        "date": "March 2020",
        "source": "ICCDA 2020: Proceedings of the 2020 the 4th International Conference on Compute and Data Analysis",
        "abstract": "In workplaces, there is a massive amount of unstructured data from different sources. In this paper, we present a case study that explains how can through communications between employees, we can help to prioritize tasks requests to increase the efficiency of their works for both technical and non-technical workers. This involves managing daily incoming tasks based on their level of urgency and importance.To allow all workers to utilize the urgency-importance matrix as a time-management tool, we need to automate this tool. The textual content of incoming tasks are analyzed, and metrics related to urgency and importance are extracted. A third factor (i.e., the response variable) is defined based on the two input variables (urgency and importance). Then, machine learning applied to the data to predict the class of incoming tasks based on data outcome desired. We used ordinal regression, neural networks, and decision tree algorithms to predict the four levels of task priority. We measure the performance of all using recalls, precisions, and F-scores. All classifiers perform higher than 89% in terms of all measures.",
        "link": "https://dl.acm.org/doi/10.1145/3388142.3388153"
    },
    {
        "title": "Text mining for malware classification using multivariate all repeated patterns detection",
        "authors": [
            "Konstantinos F. Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2019",
        "source": "ASONAM '19: Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Mobile phones have become nowadays a commodity to the majority of people. Using them, people are able to access the world of Internet and connect with their friends, their colleagues at work or even unknown people with common interests. This proliferation of the mobile devices has also been seen as an opportunity for the cyber criminals to deceive smartphone users and steel their money directly or indirectly, respectively, by accessing their bank accounts through the smartphones or by blackmailing them or selling their private data such as photos, credit card data, etc. to third parties. This is usually achieved by installing malware to smartphones masking their malevolent payload as a legitimate application and advertise it to the users with the hope that mobile users will install it in their devices. Thus, any existing application can easily be modified by integrating a malware and then presented it as a legitimate one. In response to this, scientists have proposed a number of malware detection and classification methods using a variety of techniques. Even though, several of them achieve relatively high precision in malware classification, there is still space for improvement. In this paper, we propose a text mining all repeated pattern detection method which uses the decompiled files of an application in order to classify a suspicious application into one of the known malware families. Based on the experimental results using a real malware dataset, the methodology tries to correctly classify (without any misclassification) all randomly selected malware applications of 3 categories with 3 different families each.",
        "link": "https://dl.acm.org/doi/10.1145/3341161.3350841"
    },
    {
        "title": "Comparative Study between Traditional Machine Learning and Deep Learning Approaches for Text Classification",
        "authors": [
            "Cannannore Nidhi Kamath",
            "Syed Saqib Bukhari",
            "Andreas Dengel"
        ],
        "date": "August 2018",
        "source": "DocEng '18: Proceedings of the ACM Symposium on Document Engineering 2018",
        "abstract": "In this contemporaneous world, it is an obligation for any organization working with documents to end up with the insipid task of classifying truckload of documents, which is the nascent stage of venturing into the realm of information retrieval and data mining. But classification of such humongous documents into multiple classes, calls for a lot of time and labor. Hence a system which could classify these documents with acceptable accuracy would be of an unfathomable help in document engineering. We have created multiple classifiers for document classification and compared their accuracy on raw and processed data. We have garnered data used in a corporate organization as well as publicly available data for comparison. Data is processed by removing the stop-words and stemming is implemented to produce root words. Multiple traditional machine learning techniques like Naive Bayes, Logistic Regression, Support Vector Machine, Random forest Classifier and Multi-Layer Perceptron are used for classification of documents. Classifiers are applied on raw and processed data separately and their accuracy is noted. Along with this, Deep learning technique such as Convolution Neural Network is also used to classify the data and its accuracy is compared with that of traditional machine learning techniques. We are also exploring hierarchical classifiers for classification of classes and subclasses. The system classifies the data faster and with better accuracy than if done manually. The results are discussed in the results and evaluation section.",
        "link": "https://dl.acm.org/doi/10.1145/3209280.3209526"
    },
    {
        "title": "Identification of Overpricing in the Purchase of Medication by the Federal Government of Brazil, Using Text Mining and Clustering Based on Ontology",
        "authors": [
            "Marco Aurelio O. S. Correa",
            "Adriano Galindo Leal"
        ],
        "date": "August 2018",
        "source": "ICCBDC '18: Proceedings of the 2018 2nd International Conference on Cloud and Big Data Computing",
        "abstract": "Increasing the transparency level in his actions and spending is one of the primary duties of the Brazilian Federal Government. The creation of laws that oblige full disclose of all its expenditures through transparency portals enables citizens to supervise all government entities. However, only the dissemination of these data, without a definite standard or the availability of data analysis tools, does not guarantee that the citizen is empowered to play his role. Therefore hence, the objective of this work is to identify overprice in the acquisition of products purchased by the federal government of Brazil using the unstructured data available on the Transparency Portal. The last two-years' worth of purchasing data, available in the Transparency Portal, were extracted, processed and stored. Due to his diverse nature and high volume of data, this study focused only on medicines purchased by the Ministry of Health. Ontology-based text mining and clustering techniques were applied for automatic identification and classification of products. The processing of this information was done through text mining and clustering, based on the ontology registered in another database of the Brazilian government. Because of this work, a consolidated price base per medication was created to allow the identification of distortions in prices practised, facilitating the identification of cases that merit further investigation to unravel fraud to the treasury.",
        "link": "https://dl.acm.org/doi/10.1145/3264560.3264569"
    },
    {
        "title": "Computational Estimation by Scientific Data Mining with Classical Methods to Automate Learning Strategies of Scientists",
        "authors": [
            "Aparna S. Varde"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Experimental results are often plotted as 2-dimensional graphical plots (aka graphs) in scientific domains depicting dependent versus independent variables to aid visual analysis of processes. Repeatedly performing laboratory experiments consumes significant time and resources, motivating the need for computational estimation. The goals are to estimate the graph obtained in an experiment given its input conditions, and to estimate the conditions that would lead to a desired graph. Existing estimation approaches often do not meet accuracy and efficiency needs of targeted applications. We develop a computational estimation approach called AutoDomainMine that integrates clustering and classification over complex scientific data in a framework so as to automate classical learning methods of scientists. Knowledge discovered thereby from a database of existing experiments serves as the basis for estimation. Challenges include preserving domain semantics in clustering, finding matching strategies in classification, striking a good balance between elaboration and conciseness while displaying estimation results based on needs of targeted users, and deriving objective measures to capture subjective user interests. These and other challenges are addressed in this work. The AutoDomainMine approach is used to build a computational estimation system, rigorously evaluated with real data in Materials Science. Our evaluation confirms that AutoDomainMine provides desired accuracy and efficiency in computational estimation. It is extendable to other science and engineering domains as proved by adaptation of its sub-processes within fields such as Bioinformatics and Nanotechnology.",
        "link": "https://dl.acm.org/doi/10.1145/3502736"
    },
    {
        "title": "Where is the road for issue reports classification based on text mining?",
        "authors": [
            "Qiang Fan",
            "Yue Yu",
            "Gang Yin",
            "Tao Wang",
            "Huaimin Wang"
        ],
        "date": "November 2017",
        "source": "ESEM '17: Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Currently, open source projects receive various kinds of issues daily, because of the extreme openness of Issue Tracking System (ITS) in GitHub. ITS is a labor-intensive and time-consuming task of issue categorization for project managers. However, a contributor is only required a short textual abstract to report an issue in GitHub. Thus, most traditional classification approaches based on detailed and structured data (e.g., priority, severity, software version and so on) are difficult to adopt. In this paper, issue classification approaches on a large-scale dataset, including 80 popular projects and over 252,000 issue reports collected from GitHub, were investigated. First, four traditional text-based classification methods and their performances were discussed. Semantic perplexity (i.e., an issues description confuses bug-related sentences with nonbug-related sentences) is a crucial factor that affects the classification performances based on quantitative and qualitative study. Finally, A two-stage classifier framework based on the novel metrics of semantic perplexity of issue reports was designed. Results show that our two-stage classification can significantly improve issue classification performances.",
        "link": "https://dl.acm.org/doi/10.1109/ESEM.2017.19"
    },
    {
        "title": "Feature-based Facebook reviews process model for e-management using data mining",
        "authors": [
            "Anish Kumar Varudharajulu",
            "Yongsheng Ma"
        ],
        "date": "January 2019",
        "source": "IC4E '19: Proceedings of the 10th International Conference on E-Education, E-Business, E-Management and E-Learning",
        "abstract": "The data generated from online communication acts as potential gold mines for discovering knowledge for researchers. A large amount of data is also generated in the form of web documents, emails, blogs, and feedback, etc. Text analytics is being significantly employed to mine important information. Opinion mining is the process of extracting human thoughts and perceptions from unstructured texts. The showstopper for designing an opinion mining system for analyzing reviews arise from the fact that customer reviews are often noisy. These reviews are informally written. In addition, they are subjected to spelling mistakes, grammatical errors, improper punctuation and irrational capitalization. This paper focuses on analyzing the different classification and clustering algorithms aimed at extracting and consolidating opinions of customers from social media sites like Facebook, Twitter and through surveys, at multiple levels of granularity to monitor and measure customer satisfaction. Ours is an automated approach, in which the system aids in the process of knowledge assimilation for knowledge-based building and also performs the analytics. Domain experts ratify the knowledge base and also provide training datasets for the system to intuitively gather more instances for ratification. The system identifies opinion expressions as phrases containing opinion words, opinionated features and also opinion modifiers. These expressions are categorized as positive, negative or neutral. Opinion expressions are identified and categorized using localized linguistic techniques. Opinions can be congregated at any desired level of specificity i.e. feature level or product level, user level or service level, etc. We have developed a system based on this approach, which provides the user with a platform to analyze opinion expressions crawled from a set of pre-defined datasets.",
        "link": "https://dl.acm.org/doi/10.1145/3306500.3306514"
    },
    {
        "title": "Discriminative Topic Mining via Category-Name Guided Text Embedding",
        "authors": [
            "Yu Meng",
            "Jiaxin Huang",
            "Guangyuan Wang",
            "Zihan Wang",
            "Chao Zhang",
            "Yu Zhang",
            "Jiawei Han"
        ],
        "date": "April 2020",
        "source": "WWW '20: Proceedings of The Web Conference 2020",
        "abstract": "Mining a set of meaningful and distinctive topics automatically from massive text corpora has broad applications. Existing topic models, however, typically work in a purely unsupervised way, which often generate topics that do not fit users’ particular needs and yield suboptimal performance on downstream tasks. We propose a new task, discriminative topic mining, which leverages a set of user-provided category names to mine discriminative topics from text corpora. This new task not only helps a user understand clearly and distinctively the topics he/she is most interested in, but also benefits directly keyword-driven classification tasks. We develop CatE, a novel category-name guided text embedding method for discriminative topic mining, which effectively leverages minimal user guidance to learn a discriminative embedding space and discover category representative terms in an iterative manner. We conduct a comprehensive set of experiments to show that CatE mines high-quality set of topics guided by category names only, and benefits a variety of downstream applications including weakly-supervised classification and lexical entailment direction identification.",
        "link": "https://dl.acm.org/doi/10.1145/3366423.3380278"
    },
    {
        "title": "Comparative Study of Heart Disease Diagnosis Using Top Ten Data Mining Classification Algorithms",
        "authors": [
            "I. Ketut Agung Enriko"
        ],
        "date": "June 2019",
        "source": "ICFET '19: Proceedings of the 5th International Conference on Frontiers of Educational Technologies",
        "abstract": "Data mining has been used for many purposes, especially for prediction system. In healthcare, data mining algorithms often used in disease diagnosis. Meanwhile, heart disease is known as a primary cause of death over the years. Many studies have been performed in heart disease diagnosis using data mining methods. There are some popular data mining algorithms that can be used in heart disease diagnosis, for example, k-Nearest Neighbor, CART, and AdaBoost. The algorithms are used to analyze a sample of cardiovascular patients data and predict the heart disease type that they suffer. Some parameters are taken from the patient, including EKG morphology, blood pressure, and information about the existence of chest pain, shortness of breath, palpitation, and cold sweat. In this study, medical records data are collected from Harapan Kita Hospital and utilized as a dataset sample in this research. Top ten data mining classification algorithms are used in diagnosing heart disease from Harapan Kita Hospital data and examining their performance by checking the accuracy and speed.",
        "link": "https://dl.acm.org/doi/10.1145/3338188.3338220"
    },
    {
        "title": "Text Mining Approach for Identifying Research Trends",
        "authors": [
            "Snezhana Sulova"
        ],
        "date": "June 2021",
        "source": "CompSysTech '21: Proceedings of the 22nd International Conference on Computer Systems and Technologies",
        "abstract": "With the increase of unstructured data, the issues connected with automatic text processing, the categorization of documents and the discovery of topics have become objects of growing interest. In order to improve the process of grouping and processing research publications, we would like to propose a method based upon natural language processing. It is based on text mining technologies which aim to identify key tendencies in documents. It processes the content of publications by clustering and identifies the topics of each identified group. This analysis helps by identifying key tendencies as well as discovering emerging new areas of research. Publications from the research literature database, Scopus, were used to test the approach. The topic of the publications is “the application of digital technologies in the logistics business”. The experiments were completed using the RapidMiner Studio software.",
        "link": "https://dl.acm.org/doi/10.1145/3472410.3472433"
    },
    {
        "title": "Assessment of Vulnerability Severity using Text Mining",
        "authors": [
            "Georgios Spanos",
            "Lefteris Angelis",
            "Dimitrios Toloudis"
        ],
        "date": "September 2017",
        "source": "PCI '17: Proceedings of the 21st Pan-Hellenic Conference on Informatics",
        "abstract": "Software1 vulnerabilities are closely associated with information systems security, a major and critical field in today's technology. Vulnerabilities constitute a constant and increasing threat for various aspects of everyday life, especially for safety and economy, since the social impact from the problems that they cause is complicated and often unpredictable. Although there is an entire research branch in software engineering that deals with the identification and elimination of vulnerabilities, the growing complexity of software products and the variability of software production procedures are factors contributing to the ongoing occurrence of vulnerabilities, Hence, another area that is being developed in parallel focuses on the study and management of the vulnerabilities that have already been reported and registered in databases. The information contained in such databases includes, a textual description and a number of metrics related to vulnerabilities. The purpose of this paper is to investigate to what extend the assessment of the vulnerability severity can be inferred directly from the corresponding textual description, or in other words, to examine the informative power of the description with respect to the vulnerability severity. For this purpose, text mining techniques, i.e. text analysis and three different classification methods (decision trees, neural networks and support vector machines) were employed. The application of text mining to a sample of 70,678 vulnerabilities from a public data source shows that the description itself is a reliable and highly accurate source of information for vulnerability prioritization.",
        "link": "https://dl.acm.org/doi/10.1145/3139367.3139390"
    },
    {
        "title": "Clustering Algorithms for Spatial Data Mining",
        "authors": [
            "Chetashri Bhadane",
            "Ketan Shah"
        ],
        "date": "April 2020",
        "source": "ICGDA '20: Proceedings of the 2020 3rd International Conference on Geoinformatics and Data Analysis",
        "abstract": "With the advances in mobile and wireless technologies, there has been a rise in applications that track and share the users' geospatial data. People use several social networking sites such as Twitter, Facebook and Flickr, where they share their status updates. With the integration of Global Positioning System (GPS) with mobile phones, it is now possible to share one's locations on these social networks. GPS allows us to record and track a person's movement along with the timestamp. The data set obtained from these GPS logs is vast and is widely used to analyze the users' movement patterns. Specifically, we can find out significant locations based on the number of users present at that location and the time spent by them at such places. Once significant places have been identified, it is also possible to identify the semantic importance of these locations. This paper presents an overview of the clustering techniques used to find important places of interest using large GPS based mobility datasets. Four clustering algorithms, K-Means, DBSCAN, OPTICS and Hierarchical, are implemented, and performance is tested using real-time data of 50 users collected over 2--5 years. Performance summary depicts that K-Means and DBSCAN perform well for spatial data.",
        "link": "https://dl.acm.org/doi/10.1145/3397056.3397068"
    },
    {
        "title": "Feature Selection based Arabic Text Classification using Different Machine Learning Algorithms: Comparative Study",
        "authors": [
            "Sakina Rim Bennabi",
            "Zakaria Elberrichi"
        ],
        "date": "June 2020",
        "source": "ICIST '20: Proceedings of the 10th International Conference on Information Systems and Technologies",
        "abstract": "Feature selection is a method of data pre-processing widely used when mining large data, such as textual classification. Several studies have been conducted to compare the different methods of feature selection applied to corpora in English. Unfortunately, a small number of works concern the Arabic language. This article aims to present a comparative study of different feature selection techniques including: Chi2, the ANOVA method and mutual information, applied on a corpus in Arabic language, while also diversifying the machine learning algorithms (Naive Bayes, SVM and KNN). This experimental study has shown in general that reducing dimensionality with feature selection techniques has slightly affected the performance of textual classification, reducing the size of the corpus by up to 1%.",
        "link": "https://dl.acm.org/doi/10.1145/3447568.3448531"
    },
    {
        "title": "Intelligent Data Analysis using Optimized Support Vector Machine Based Data Mining Approach for Tourism Industry",
        "authors": [
            "Ms Promila Sharma",
            "Uma Meena",
            "Girish Kumar Sharma"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Data analysis involves the deployment of sophisticated approaches from data mining methods, information theory, and artificial intelligence in various fields like tourism, hospitality, and so on for the extraction of knowledge from the gathered and preprocessed data. In tourism, pattern analysis or data analysis using classification is significant for finding the patterns that represent new and potentially useful information or knowledge about the destination and other data. Several data mining techniques are introduced for the classification of data or patterns. However, overfitting, less accuracy, local minima, sensitive to noise are the drawbacks in some existing data mining classification methods. To overcome these challenges, Support vector machine with Red deer optimization (SVM-RDO) based data mining strategy is proposed in this article. Extended Kalman filter (EKF) is utilized in the first phase, i.e., data cleaning to remove the noise and missing values from the input data. Mantaray foraging algorithm (MaFA) is used in the data selection phase, in which the significant data are selected for the further process to reduce the computational complexity. The final phase is the classification, in which SVM-RDO is proposed to access the useful pattern from the selected data. PYTHON is the implementation tool used for the experiment of the proposed model. The experimental analysis is done to show the efficacy of the proposed work. From the experimental results, the proposed SVM-RDO achieved better accuracy, precision, recall, and F1 score than the existing methods for the tourism dataset. Thus, it is showed the effectiveness of the proposed SVM-RDO for pattern analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3494566"
    },
    {
        "title": "Applying text mining to predict learners' cognitive engagement",
        "authors": [
            "Hayati Hind",
            "Mohammed Khalidi Idrissi",
            "Samir Bennani"
        ],
        "date": "October 2017",
        "source": "SCAMS '17: Proceedings of the Mediterranean Symposium on Smart City Application",
        "abstract": "In academic process, engagement represent one of the important success' factor. This paper proposes a predictive system for learners' cognitive engagement based on their online discussion forums participation. In the first level the ontology OWL and the LSA method are used to perform a semantic classification system of the threads according to a specific context chosen by the tutor. Then the Text mining, as a predictive method, is applied to the classified threads and learners' participation in the forums.",
        "link": "https://dl.acm.org/doi/10.1145/3175628.3175655"
    },
    {
        "title": "Investigate Transitions into Drug Addiction through Text Mining of Reddit Data",
        "authors": [
            "John Lu",
            "Sumati Sridhar",
            "Ritika Pandey",
            "Mohammad Al Hasan",
            "Georege Mohler"
        ],
        "date": "July 2019",
        "source": "KDD '19: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Increasing rates of opioid drug abuse and heightened prevalence of online support communities underscore the necessity of employing data mining techniques to better understand drug addiction using these rapidly developing online resources. In this work, we obtained data from Reddit, an online collection of forums, to gather insight into drug use/misuse using text snippets from users narratives. Specifically, using users' posts, we trained a binary classifier which predicts a user's transitions from casual drug discussion forums to drug recovery forums. We also proposed a Cox regression model that outputs likelihoods of such transitions. In doing so, we found that utterances of select drugs and certain linguistic features contained in one's posts can help predict these transitions. Using unfiltered drug-related posts, our research delineates drugs that are associated with higher rates of transitions from recreational drug discussion to support/recovery discussion, offers insight into modern drug culture, and provides tools with potential applications in combating the opioid crisis.",
        "link": "https://dl.acm.org/doi/10.1145/3292500.3330737"
    },
    {
        "title": "Analysing digital banking reviews using text mining",
        "authors": [
            "Li Chen Cheng",
            "Legaspi Rhea Sharmayne"
        ],
        "date": "December 2020",
        "source": "ASONAM '20: Proceedings of the 12th IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Digital banks are new entrants in the banking industry in the Philippines as they only started late 2018. Since then, a handful of players have and are still emerging. With more and more people becoming technologically savvy, it is very critical for financial institutions to develop a digital banking application that will stand out from the competition. This paper aims to use text mining methods to analyse digital banking application reviews. This study will perform topic modelling using LDA to explore customer concerns and will mine association rules between the digital banking features with the review score. The results will reveal which areas the digital banking application can further optimize for customer satisfaction and retention.",
        "link": "https://dl.acm.org/doi/10.1109/ASONAM49781.2020.9381429"
    },
    {
        "title": "Mining Moocs Videos Metadata Using Classification Techniques",
        "authors": [
            "El Harrak Othman",
            "Ghadi Abderrahim",
            "El Bouhdidi Jaber"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "In few years, the internet has become the first source of information for most people, Today MOOCs makes it possible for everyone to access to the education over the world, it's represents an emerging methodology of online teaching and an important development in open education. Due to the rapid development of Moocs and the rapid growth of digital data and video database over the Internet, it is becoming very difficult for learners to browsing and choosing the best training for them. The scientific community has increased the amount of research into new technologies, with a view to improve the digital video utilization: its archiving, indexing, accessibility, acquisition, store and even its process and usability. All these parts of the video utilization entail the necessity of the extraction of all important information of a video, especially in cases of lack of metadata information. Web video mining is retrieving the content using data mining techniques from World Wide Web. There are two approaches for web video mining using traditional image processing (signal processing) and metadata based approach. In this paper, we present the various research makes in this subject and we propose an effective methodology to extract the metadata from Moocs videos and classify them based on the extracted metadata by applying data mining techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090450"
    },
    {
        "title": "Text mining with HathiTrust",
        "authors": [
            "Eleanor Dickson Koehl",
            "Ryan Dubnicek"
        ],
        "date": "June 2019",
        "source": "JCDL '19: Proceedings of the 18th Joint Conference on Digital Libraries",
        "abstract": "This tutorial will introduce attendees to the HathiTrust Research Center and its tools and services for computational text analysis research. HTRC leverages the scope and scale of the HathiTrust Digital Library collection to create opportunities for researchers to perform text data mining on subsets of the corpus. Attendees will develop skills that will allow them to conduct basic text analysis research using HathiTrust data, learn techniques for engaging with scholars on their text data mining research, and explore strategies for computational access to large-scale digital collections.",
        "link": "https://dl.acm.org/doi/10.1109/JCDL.2019.00115"
    },
    {
        "title": "Text mining for plagiarism detection: multivariate pattern detection for recognition of text similarities",
        "authors": [
            "Konstantinos Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2018",
        "source": "ASONAM '18: Proceedings of the 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "The problem of plagiarism the recent years has been intensified by the availability of information in digital form and the accessibility of the electronic libraries through the Internet. As a result, plagiarism detection has been transformed into a big data analytics problem since the number of digital sources is extravagant and a new document needs to be compared with millions of other existing documents. In this paper, a text mining methodology is proposed that can detect all common patterns between a document and the documents in a reference database. The technique is based on a pattern detection algorithm and the corresponding data structure that enables the algorithm to detect all common patterns. The methodology has been applied in a well-defined dataset providing very promising results identifying difficult cases of plagiarism such as technical disguise.",
        "link": "https://dl.acm.org/doi/10.5555/3382225.3382424"
    },
    {
        "title": "Petroleum Engineering Data Text Classification Using Convolutional Neural Network Based Classifier",
        "authors": [
            "Tzuhan Hsu",
            "Yaoqin Zhang"
        ],
        "date": "May 2018",
        "source": "ICMLT '18: Proceedings of the 2018 International Conference on Machine Learning Technologies",
        "abstract": "In this paper, authors introduce a convolutional neural network (CNN) based classification approach for petroleum engineering event. Recently, text classification problem has raised substantial attention in the field of machine learning. The aim of text classification process is to assign correct label to a text on the basis of its content. However, due to the imprecise of feature representation, most of former text classification methods cannot export satisfying result. To solve this problem, authors propose a model based on convolutional neural network. Authors conduct experiments with real world petroleum engineering data text, artificially labelled into 6 categories. Beside purposed CNN-based model, the authors also implement other baseline machine learning approaches: k nearest neighbor, random forest, support vector machines, recurrent neural network, gated recurrent units, long short-term memory. Among all these models, our CNN-based model reach best performance in text classification task. In the last part of this paper, authors compare and analysis their purpose and baseline model.",
        "link": "https://dl.acm.org/doi/10.1145/3231884.3231898"
    },
    {
        "title": "Classifying Indonesian Online Articles as Advertisement Placement Base Using Text Mining",
        "authors": [
            "Nadhira Tasya",
            "Arian Dhini"
        ],
        "date": "July 2017",
        "source": "ICBIM 2017: Proceedings of the International Conference on Business and Information Management",
        "abstract": "Rapid development in technological aspect resulting in growing level of human needs for the latest news, so that emerged a new trend of publishing and accessing news through online media or usually called online journalism. In addition, the number of people who sell and purchase through online sites also continues to increase and this opportunity is utilized by the company and the advertiser by implementing targeted web advertising. However, the high number of articles that have been published and accessed leads to great opportunities for errors in determining where to place the ads. Therefore, it needs a system that can categorize articles accessed by users as the basis of advertisement placement by the company and this classification system can be done by applying the method of Data Mining and Text Mining. This research uses document data in the form of article content that will be categorized into twenty categories of class of advertisement by using Text Mining technique with Support Vector Machine algorithm. The results of this study may be used by companies or advertisers as a basis for placement of ads on selected online media sites.",
        "link": "https://dl.acm.org/doi/10.1145/3134271.3134288"
    },
    {
        "title": "Mining Free-Text Medical Notes for Suicide Risk Assessment",
        "authors": [
            "Marios Adamou",
            "Grigoris Antoniou",
            "Elissavet Greasidou",
            "Vincenzo Lagani",
            "Paulos Charonyktakis",
            "Ioannis Tsamardinos"
        ],
        "date": "July 2018",
        "source": "SETN '18: Proceedings of the 10th Hellenic Conference on Artificial Intelligence",
        "abstract": "Suicide has been considered as an important public health issue for a very long time, and is one of the main causes of death worldwide. Despite suicide prevention strategies being applied, the rate of suicide has not changed substantially over the past decades. Advances in machine learning make it possible to attempt to predict suicide based on the analysis of relevant data to inform clinical practice. This paper reports on findings from the analysis of data of patients who died by suicide in the period 2013-2016 and made use of both structured data and free-text medical notes. We focus on examining various text-mining approaches to support risk assessment. The results show that using advance machine learning and text-mining techniques, it is possible to predict within a specified period which people are most at risk of taking their own life at the time of referral to a mental health service.",
        "link": "https://dl.acm.org/doi/10.1145/3200947.3201020"
    },
    {
        "title": "Integration of data mining techniques in e-learning systems: Clustering Profil of Lerners and Recommender Course System",
        "authors": [
            "A. El Moustamid",
            "E. En-Naimi",
            "J. El Bouhdidi"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "Data mining aims at the extraction of knowledge from large quantities of data by automatic or semi-automatic methods. In the field of education and training e-learning, Data Mining techniques as clustering (The aim of this technique is to identify groups that are similar in some aspect) [21], prediction (is used for developing model where combination of data viewed from different aspects can effect single viewed data. Here, is important to watch how predictors variables are effecting predicted variable)[21], classification are applied in e-learning to improve the level of learners by providing learning according to the learners' requirement) are used to analyze learners profile, to predict and improve student performance. The aim of our project is to develop a system capable of analyzing learners profile and indexing web videos in order to offer learners a rich database with courses that correspond to their levels.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090453"
    },
    {
        "title": "Multi-level mining and visualization of scientific text collections: Exploring a bi-lingual scientific repository",
        "authors": [
            "Pablo Accuosto",
            "Francesco Ronzano",
            "Daniel Ferrés",
            "Horacio Saggion"
        ],
        "date": "December 2017",
        "source": "WOSP 2017: Proceedings of the 6th International Workshop on Mining Scientific Publications",
        "abstract": "We present a system to mine and visualize collections of scientific documents by semantically browsing information extracted from single publications or aggregated throughout corpora of articles. The text mining tool performs deep analysis of document collections allowing the extraction and interpretation of research paper's contents. In addition to the extraction and enrichment of documents with metadata (titles, authors, affiliations, etc), the deep analysis performed comprises semantic interpretation, rhetorical analysis of sentences, triple-based information extraction, and text summarization. The visualization components allow geographical-based exploration of collections, topic-evolution interpretation, and collaborative network analysis among others. The paper presents a case study of a bi-lingual collection in the field of Natural Language Processing (NLP).",
        "link": "https://dl.acm.org/doi/10.1145/3127526.3127529"
    },
    {
        "title": "Hierarchical Topic Mining via Joint Spherical Tree and Text Embedding",
        "authors": [
            "Yu Meng",
            "Yunyi Zhang",
            "Jiaxin Huang",
            "Yu Zhang",
            "Chao Zhang",
            "Jiawei Han"
        ],
        "date": "August 2020",
        "source": "KDD '20: Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Mining a set of meaningful topics organized into a hierarchy is intuitively appealing since topic correlations are ubiquitous in massive text corpora. To account for potential hierarchical topic structures, hierarchical topic models generalize flat topic models by incorporating latent topic hierarchies into their generative modeling process. However, due to their purely unsupervised nature, the learned topic hierarchy often deviates from users' particular needs or interests. To guide the hierarchical topic discovery process with minimal user supervision, we propose a new task, Hierarchical Topic Mining, which takes a category tree described by category names only, and aims to mine a set of representative terms for each category from a text corpus to help a user comprehend his/her interested topics. We develop a novel joint tree and text embedding method along with a principled optimization procedure that allows simultaneous modeling of the category tree structure and the corpus generative process in the spherical space for effective category-representative term discovery. Our comprehensive experiments show that our model, named JoSH, mines a high-quality set of hierarchical topics with high efficiency and benefits weakly-supervised hierarchical text classification tasks.",
        "link": "https://dl.acm.org/doi/10.1145/3394486.3403242"
    },
    {
        "title": "Unsupervised behavioural mining and clustering for malware family identification",
        "authors": [
            "Khanh Huu The Dam",
            "Thomas Given-Wilson",
            "Axel Legay"
        ],
        "date": "March 2021",
        "source": "SAC '21: Proceedings of the 36th Annual ACM Symposium on Applied Computing",
        "abstract": "More accurate and advanced detection and classification of malware requires exploiting program behaviour and not merely syntactic or static features. One approach is to use system call dependency graphs (SCDGs) that represent the program behaviour by interactions with the system, and the relations between these interactions. These SCDGs have been used with supervised learning techniques to very accurately detect and classify malware. This works considers the unsupervised learning challenge of mining for common clusters of behaviour without a priori knowledge. This allows for clustering of similar programs by behaviour, that can then be used for either classification or further analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3412841.3441919"
    },
    {
        "title": "Text Mining for Evaluating Authors' Birth and Death Years",
        "authors": [
            "Dror Moghaz",
            "Yaakov Hacohen-Kerner",
            "Dov Gabbay"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "This article presents a unique method in text and data mining for finding the era, i.e., mining temporal data, in which an anonymous author was living. Finding this era can assist in the examination of a fake document or extracting the time period in which a writer lived. The study and the experiments concern Hebrew, and in some parts, Aramaic and Yiddish rabbinic texts. The rabbinic texts are undated and contain no bibliographic sections, posing an interesting challenge. This work proposes algorithms using key phrases and key words that allow the temporal organization of citations together with linguistic patterns. Based on these key phrases, key words, and the references, we established several types of “Iron-clad,” Heuristic and Greedy rules for estimating the years of birth and death of a writer in an interesting classification task. Experiments were conducted on corpora, including documents authored by 12, 24, and 36 rabbinic writers and demonstrated promising results.",
        "link": "https://dl.acm.org/doi/10.1145/3281631"
    },
    {
        "title": "Transfer Learning to Timed Text Based Video Classification Using CNN",
        "authors": [
            "Zenun Kastrati",
            "Ali Shariq Imran",
            "Arianit Kurti"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "Open educational video resources are gaining popularity with a growing number of massive open online courses (MOOCs). This has created a niche for content providers to adopt effective solutions in automatically organizing and structuring of educational resources for maximum visibility. Recent advances in deep learning techniques are proving useful in managing and classifying resources into appropriate categories. This paper proposes one such convolutional neural network (CNN) model for classifying video lectures in a MOOC setting using a transfer learning approach. The model uses a time-aligned text transcripts corresponding to video lectures from six broader subject categories. Video lectures and their corresponding transcript dataset is gathered from the Coursera MOOC platform. Two different CNN models are proposed: i) CNN based classification using embeddings learned from our MOOC dataset, ii) CNN based classification using transfer learning. Word embeddings generated from two well known state-of-the-art pre-trained models Word2Vec and GloVe, are used in the transfer learning approach for the second case. The proposed CNN models are evaluated using precision, recall, and F1 score and the obtained performance is compared with both conventional and deep learning classifiers. The proposed CNN models have an F1 score improvement of 10-22 percentage points over DNN and conventional classifiers",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326483"
    },
    {
        "title": "Evolutionary role mining in complex networks by ensemble clustering",
        "authors": [
            "Sarvenaz Choobdar",
            "Pedro Ribeiro",
            "Fernando Silva"
        ],
        "date": "April 2017",
        "source": "SAC '17: Proceedings of the Symposium on Applied Computing",
        "abstract": "The structural patterns in the neighborhood of nodes assign unique roles to the nodes. Mining the set of existing roles in a network provides a descriptive profile of the network and draws its general picture. This paper proposes a new method to determine structural roles in a dynamic network based on the current position of nodes and their historic behavior. We develop a temporal ensemble clustering technique to dynamically find groups of nodes, holding similar tempo-structural roles. We compare two weighting functions, based on age and distribution of data, to incorporate temporal behavior of nodes in the role discovery. To evaluate the performance of the proposed method, we assess the results from two points of view: 1) goodness of fit to current structure of the network; 2) consistency with historic data. We conduct the evaluation using different ensemble clustering techniques. The results on real world networks demonstrate that our method can detect tempo-structural roles that simultaneously depict the topology of a network and reflect its dynamics with high accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3019612.3019815"
    },
    {
        "title": "Analyzing used-car web listings via text mining",
        "authors": [
            "Ayhan Demiriz",
            "Fatma Cantaş"
        ],
        "date": "October 2017",
        "source": "IML '17: Proceedings of the 1st International Conference on Internet of Things and Machine Learning",
        "abstract": "Used car trade is one of the major components of the world economies. It is not uncommon to sell a car by placing an internet advertisement irrespective of the geography in these days. A typical content of an advertisement is usually composed of two parts namely the structured and the free text data. The structured data may include some information about the asking price, make, model, year, mileage of the car and the contact info. In most cases, seller may give important clues about the car's current conditions in the free text data where the title (head) of the advertisement can be included as free text too. This paper reports preliminary results from a text mining study conducted on 75K used car internet listings collected from two major car listing web sites in Turkey. As expected, the words and the phrases related to the description of the car are observed to be frequent. The leading concepts in the free text are found to be regarding how to describe the current condition of a car, for example \"no crash history\".",
        "link": "https://dl.acm.org/doi/10.1145/3109761.3109782"
    },
    {
        "title": "Text classification of Diseases Treated by Traditional Chinese Medicine Prescription based on machine learning",
        "authors": [
            "Hanqing Zhao",
            "Bo Han",
            "Chen Li"
        ],
        "date": "September 2020",
        "source": "ISAIMS '20: Proceedings of the 1st International Symposium on Artificial Intelligence in Medical Sciences",
        "abstract": "OBJECTIVE: To explore the application of machine learning in th e identification of diseases treated by traditional Chinese medicine prescriptions. METHODS: Based on the composition of the text document of Chinese medicine prescriptions, the prescriptions were divided into cough, headache and diarrhea. THUTCT were introduced to establish and train two machine learning text classification models, LibSVM and LibLinear, and the prescriptions to be deter mined were put into the model for classification and prediction. R ESULTS: The Precision rate of LibSVM model and Liblinear mod el were 0.7283 and 0.6690. Seven prescriptions were classified an d predicted, and the results were in line with expectations. CONCLUSION: THUCTC has good universality for the content of TCM prescriptions, high classification accuracy and fast testing speed, which is suitable for the text classification and discrimination research of TCM prescriptions for diseases.",
        "link": "https://dl.acm.org/doi/10.1145/3429889.3429895"
    },
    {
        "title": "Evaluation of retweet clustering method classification method using retweets on Twitter without text data",
        "authors": [
            "K. Uchida",
            "F. Toriumi",
            "T. Sakaki"
        ],
        "date": "August 2017",
        "source": "WI '17: Proceedings of the International Conference on Web Intelligence",
        "abstract": "Burst phenomena, which frequently occur on social media, are caused by such social events as flaming on the internet, elections, and natural disasters. To understand people's thoughts and feelings, we must classify their opinions from burst phenomena. Therefore, classification methods that categorize tweets are critical. However, since most classification methods focus on text mining, they cannot group tweets by topics because each tweet has poor linguistic similarities. We used a non-text-based classification method proposed by Baba et al. that groups tweets by topics, even if they have poor linguistic similarities, and verified its validity by comparing it with a text-based classification method in two different evaluations: qualitative and quantitative. In the qualitative evaluation part, we did a questionnaire survey and validated the suitability of the topic clusters created using both the non-and text-based methods. Since evaluating the similarity of every pair of tweets in each topic is difficult, we evaluated the similarity between sampled pairs in the survey and acquired more appropriate topic clustering results using the non-text-based method than the text-based method. In the quantitative evaluation part, we focused on the robustness of each method against data reduction. Many approaches analyze social media data, especially because collecting data from social media is comparatively easy. However, since collecting the whole data of burst phenomena is very costly due to the vast amounts of available social media data, robustness against data reduction is an important index to evaluate classification methods. With the non-text-based method, over 55% of the pairs of tweets in the same cluster were also included in the same cluster even when the data were reduced to 10% in all three of our example cases. In this paper, as a source we focus on Twitter, one of the most popular microblogging services. Using clustering to conduct detailed case analyses, we scrutinized three burst cases that include natural disasters and flaming on the internet and found that a non-text-based method more effectively classified tweets in burst phenomena than a text-based method.",
        "link": "https://dl.acm.org/doi/10.1145/3106426.3106451"
    },
    {
        "title": "Using Spark for Text Mining on Large Scale Liver Cancer Literature",
        "authors": [
            "Ming-Yen Lin",
            "Yu-Ju Lin",
            "Sue-Chen Hsueh"
        ],
        "date": "January 2021",
        "source": "BDET 2021: 2021 the 3rd International Conference on Big Data Engineering and Technology (BDET)",
        "abstract": "Cancer is one of the main causes of death. The number of known cancer to-date is more than one hundred. Liver cancer, ranked after trachea and lung cancer, has long been high in the cancer leading cause in Taiwan and even ranked as second in 2016. The number of scientific articles related to cancer proliferates every year. The number reaches as high as 20 million in PubMed so that discovering useful information from the massive collection is very difficult. In addition, using a single machine to sift through these articles is very time-consuming. Therefore, we present a big data analytic framework using the distributed Apache Spark platform for text mining in PubMed literature. We establish a prediction model for liver cancer articles so that researchers may effectively validate whether an article is related to liver cancer or not. Classification models in Spark MLlib including Linear Support Vector Machines (SVM), Logistic Regression, are used in our experiments. Relevancy to liver cancer is further confirmed by using MeSH (Medical Subject Headings) terms. Logistic regression is about 3 times faster than SVMs and the accuracy of both methods is close to 95% in the experiments using hold-out validation. When max_features is 500 and min_df ≤ 0.1 (or min_df = 1), the accuracy may reach 96%. In the experiments with K-fold cross-validation, the accuracy of SVMs methods is 96%. The experimental results show that that our prediction model may effectively classify liver cancer articles.",
        "link": "https://dl.acm.org/doi/10.1145/3474944.3474958"
    },
    {
        "title": "Automatic topic classification of test cases using text mining at an Android smartphone vendor",
        "authors": [
            "Junji Shimagaki",
            "Yasutaka Kamei",
            "Naoyasu Ubayashi",
            "Abram Hindle"
        ],
        "date": "October 2018",
        "source": "ESEM '18: Proceedings of the 12th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Background: An Android smartphone is an ecosystem of applications, drivers, operating system components, and assets. The volume of the software is large and the number of test cases needed to cover the functionality of an Android system is substantial. Enormous effort has been already taken to properly quantify \"what features and apps were tested and verified?\". This insight is provided by dashboards that summarize test coverage and results per feature. One method to achieve this is to manually tag or label test cases with the topic or function they cover, much like function points. At the studied Android smartphone vendor, tests are labelled with manually defined tags, so-called \"feature labels (FLs)\", and the FLs serve to categorize 100s to 1000s test cases into 10 to 50 groups. Aim: Unfortunately for developers, manual assignment of FLs to 1000s of test cases is a time consuming task, leading to inaccurately labeled test cases, which will render the dashboard useless. We created an automated system that suggests tags/labels to the developers for their test cases rather than manual labeling. Method: We use machine learning models to predict and label the functionality tested by 10,000 test cases developed at the company. Results: Through the quantitative experiments, our models achieved acceptable F-1 performance of 0.3 to 0.88. Also through the qualitative studies with expert teams, we showed that the hierarchy and path of tests was a good predictor of a feature's label. Conclusions: We find that this method can reduce tedious manual effort that software developers spent classifying test cases, while providing more accurate classification results.",
        "link": "https://dl.acm.org/doi/10.1145/3239235.3268927"
    },
    {
        "title": "To apply Data Mining for Classification of Crowd sourced Software Requirements",
        "authors": [
            "Soonh Taj",
            "Qasim Arain",
            "Imran Memon",
            "Asma Zubedi"
        ],
        "date": "April 2019",
        "source": "ICSIE '19: Proceedings of the 8th International Conference on Software and Information Engineering",
        "abstract": "Now a day's main focus of developers is to build quality software that works according to customer needs and for this reason it is necessary to gather right requirements as requirement elicitation is the critical step that impacts on the success of software project as misinterpreted requirements leads to the failure of software project. By keeping this in mind a research is carried out on improving requirements elicitation process and automating the process of classifying requirements. In this research, a model is proposed which will help in this scenario for requirements elicitation and requirement classification. This paper presents a model in which crowd sourcing approach is used so that customers, end users, stakeholders, developers and software engineers can make active participation for requirement elicitation process and requirements gathered using crowdsourcing approach are used by model for classification process i.e. classification of requirements into functional and non-functional requirements. For the proof of proposed model a case study is conducted. Results of case study provided the usefulness and efficiency of proposed model for classification of crowd sourced software requirements.",
        "link": "https://dl.acm.org/doi/10.1145/3328833.3328837"
    },
    {
        "title": "Text classification using Fuzzy TF-IDF and Machine Learning Models",
        "authors": [
            "Mariem Bounabi",
            "Karim El Moutaouakil",
            "Khalid Satori"
        ],
        "date": "October 2019",
        "source": "BDIoT'19: Proceedings of the 4th International Conference on Big Data and Internet of Things",
        "abstract": "The representation of the information has an important impact on the text classification task. Several weighting methods were proposed in the literature, and the term frequency-inverse term frequency (TFIDF), the most know on the text treatment field. The FTF-IDF is a vector representation where the components of the TFIDF are presented as inputs to the Fuzzy Inference System (FIS). In this work, we compare several Machin Learning algorithms such as Naïve Bayes and its derivatives, SVM and Random forest classifiers, using the FTF-IDF representation. To improve the quality of the used classifiers, we call sum attribute selection methods. The recognition rate, for the tested systems, is satisfied, where the system based on naïve Bayes classifier, the FTF-IDF weighting terms, and the info gain select attributes method gives 98.7% as accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3372938.3372956"
    },
    {
        "title": "Text Mining in Cybersecurity: A Systematic Literature Review",
        "authors": [
            "Luciano Ignaczak",
            "Guilherme Goldschmidt",
            "Cristiano André Da Costa",
            "Rodrigo Da Rosa Righi"
        ],
        "date": null,
        "source": "ACM Computing Surveys",
        "abstract": "The growth of data volume has changed cybersecurity activities, demanding a higher level of automation. In this new cybersecurity landscape, text mining emerged as an alternative to improve the efficiency of the activities involving unstructured data. This article proposes a Systematic Literature Review (SLR) to present the application of text mining in the cybersecurity domain. Using a systematic protocol, we identified 2,196 studies, out of which 83 were summarized. As a contribution, we propose a taxonomy to demonstrate the different activities in the cybersecurity domain supported by text mining. We also detail the strategies evaluated in the application of text mining tasks and the use of neural networks to support activities involving unstructured data. The work also discusses text classification performance aiming its application in real-world solutions. The SLR also highlights open gaps for future research, such as the analysis of non-English content and the intensification in the usage of neural networks.",
        "link": "https://dl.acm.org/doi/10.1145/3462477"
    },
    {
        "title": "Using Text Mining to Analyze the Financial Patents",
        "authors": [
            "Yung-Feng Lu",
            "Jia-Lang Xu",
            "Mu-Yen Chen"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "With the advancement and development of science and technology, Nowadays society is the development of information technology, cloud computing, mobile Internet, Internet of Things, block chain and information technology.The combination of finance and technology has made financial technology terms gradually emerging.. This research analyzes the patents of financial technology. However, Taiwan's financial technology patents are mostly G06Q-20 (payment plan, architecture or agreement), G06Q-30 (commercial, such as marketing, shopping, payment, auction or e-commerce). G06Q-40 (financial, investment or tax treatment, insurance).The research use text mining technology to analyze and understand Taiwan's trends and developments in financial technology. The research results show that G06Q-20 patents are biased towards information security and mobile payment. G06Q-30 patents favor shopping websites and advertising. G06Q-40 patents are biased towards diversity, such as securities, insurance, finance, and stocks.",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326478"
    },
    {
        "title": "Identifying Online Profiles of Distance Learning Students Using Data Mining Techniques",
        "authors": [
            "Osama Islam",
            "Muazzam Siddiqui",
            "Naif Radi Aljohani"
        ],
        "date": "October 2019",
        "source": "ICDTE '19: Proceedings of the 3rd International Conference on Digital Technology in Education",
        "abstract": "Educational data has grown over the years with the increased use of technology within educational environments. This has led to a huge amount of data being stored in various data sources representing the student, his/her activities, and other aspects relevant to the learning process. To meet this analytical need, Educational Data Mining (EDM) has emerged to assist educational institutions in identifying key benefits such as students at risk, the level of student engagement or predicting student performance. The aim of this research was to explore the various aspects of student interaction data using data mining techniques to identify relevant patterns of behaviors and possible key attributes that have higher degrees of influence on distance learning students. The main findings identified several patterns of user online profiles based on a set of adopted learning strategies, the research proposed a framework for analyzing such interaction data based on R and Hadoop platforms to correlate online profiles with student performance.",
        "link": "https://dl.acm.org/doi/10.1145/3369199.3369249"
    },
    {
        "title": "Extremely Fast Decision Tree Mining for Evolving Data Streams",
        "authors": [
            "Albert Bifet",
            "Jiajin Zhang",
            "Wei Fan",
            "Cheng He",
            "Jianfeng Zhang",
            "Jianfeng Qian",
            "Geoff Holmes",
            "Bernhard Pfahringer"
        ],
        "date": "August 2017",
        "source": "KDD '17: Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
        "abstract": "Nowadays real-time industrial applications are generating a huge amount of data continuously every day. To process these large data streams, we need fast and efficient methodologies and systems. A useful feature desired for data scientists and analysts is to have easy to visualize and understand machine learning models. Decision trees are preferred in many real-time applications for this reason, and also, because combined in an ensemble, they are one of the most powerful methods in machine learning. In this paper, we present a new system called STREAMDM-C++, that implements decision trees for data streams in C++, and that has been used extensively at Huawei. Streaming decision trees adapt to changes on streams, a huge advantage since standard decision trees are built using a snapshot of data, and can not evolve over time. STREAMDM-C++ is easy to extend, and contains more powerful ensemble methods, and a more efficient and easy to use adaptive decision trees. We compare our new implementation with VFML, the current state of the art implementation in C, and show how our new system outperforms VFML in speed using less resources.",
        "link": "https://dl.acm.org/doi/10.1145/3097983.3098139"
    },
    {
        "title": "Data miners' little helper: data transformation activity cues for cluster analysis on document collections",
        "authors": [
            "Tania Cerquitelli",
            "Evelina Di Corso",
            "Francesco Ventura",
            "Silvia Chiusano"
        ],
        "date": "June 2017",
        "source": "WIMS '17: Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "In this paper we propose a new self-learning engine to streamline the analytics process, as it enables analysts to mine massive data repositories with minimal user intervention. In the context of cluster analysis on a collection of documents this new system, named SELF-DATA (SELF-learning DAta TrAnsformation), suggests to the analyst how to configure the whole mining process for a given dataset. SELF-DATA relies on an engine exploring different data weighting schemas (e.g., normalized term frequencies) and data transformation methods (e.g., PCA) before applying the cluster analysis, evaluating and comparing solutions through different quality indices (e.g., weighted Silhouette), and presenting the k-top solutions to the analyst. SELF-DATA will also include a knowledge base storing results of experiments on previously processed datasets, and a classification algorithm trained on the knowledge base content to forecast the best configuration for the whole mining process for an unexplored dataset. The first development of SELF-DATA running on Apache Spark has been validated on 5 collections of documents. Experimental results highlight that TF-IDF and logarithmic entropy are effective to measure item relevance with sparse datasets, and the LSI method outperforms PCA with a large dictionary.",
        "link": "https://dl.acm.org/doi/10.1145/3102254.3102288"
    },
    {
        "title": "Interactive Visual Graph Mining and Learning",
        "authors": [
            "Ryan A. Rossi",
            "Nesreen K. Ahmed",
            "Rong Zhou",
            "Hoda Eldardiry"
        ],
        "date": null,
        "source": "ACM Transactions on Intelligent Systems and Technology",
        "abstract": "This article presents a platform for interactive graph mining and relational machine learning called GraphVis. The platform combines interactive visual representations with state-of-the-art graph mining and relational machine learning techniques to aid in revealing important insights quickly as well as learning an appropriate and highly predictive model for a particular task (e.g., classification, link prediction, discovering the roles of nodes, and finding influential nodes). Visual representations and interaction techniques and tools are developed for simple, fast, and intuitive real-time interactive exploration, mining, and modeling of graph data. In particular, we propose techniques for interactive relational learning (e.g., node/link classification), interactive link prediction and weighting, role discovery and community detection, higher-order network analysis (via graphlets, network motifs), among others. GraphVis also allows for the refinement and tuning of graph mining and relational learning methods for specific application domains and constraints via an end-to-end interactive visual analytic pipeline that learns, infers, and provides rapid interactive visualization with immediate feedback at each change/prediction in real-time. Other key aspects include interactive filtering, querying, ranking, manipulating, exporting, as well as tools for dynamic network analysis and visualization, interactive graph generators (including new block model approaches), and a variety of multi-level network analysis techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3200764"
    },
    {
        "title": "Data Mining and Opinion Mining: A Tool in Educational Context",
        "authors": [
            "Myriam Peñafiel",
            "Stefanie Vásquez",
            "Diego Vásquez",
            "Juan Zaldumbide",
            "Sergio Luján-Mora"
        ],
        "date": "July 2018",
        "source": "ICoMS '18: Proceedings of the 2018 1st International Conference on Mathematics and Statistics",
        "abstract": "The use of the web as a universal communication platform generates large volumes of data (Big data), which in many cases, need to be processed so that they can become useful knowledge in face of the sceptics who have doubts about the credibility of such information. The use of web data that comes from educational contexts needs to be addressed, since that large amount of unstructured information is not being valued, losing valuable information that can be used. To solve this problem, we propose the use of data mining techniques such as sentiment analysis to validate the information that comes from the educational platforms. The objective of this research is to propose a methodology that allows the user to apply sentiment analysis in a simple way, because although some researchers have done it, very few do with data in the educational context. The results obtained prove that the proposal can be used in similar cases.",
        "link": "https://dl.acm.org/doi/10.1145/3274250.3274263"
    },
    {
        "title": "Research Hotspots Mining and Visualized Analysis Based on Linking Cluster and K-Core Decomposition",
        "authors": [
            "Ying Cai",
            "Fang Huang",
            "Shuai Wang",
            "Hao Zhang",
            "Chunxiu Du"
        ],
        "date": "May 2018",
        "source": "ICDPA 2018: Proceedings of the International Conference on Data Processing and Applications",
        "abstract": "In this paper, we focus on research hotspots mining with knowledge semantic features on discipline topic word networks, which are extracted from scientific and technical documents. For the topic networks, a novel method of combining linking community clustering with k-core decomposition is proposed to mine the hotspot community with knowledge hierarchy structure. And as a post-processing, the hierarchical community identification is implemented by k-core decomposition in order to discover the internal connection with knowledge semantic hierarchy in the research hotspot. Finally, the community density and visualization method is utilized to analyze the core and sub- problem in the research hotspot community. In experiments, the topic networks for a single discipline and interdisciplinary research hotspot are taken as examples of analysis, and the results show the proposed approach can effectively realize the objective of mining and analyzing the discipline research hotspot on the academic topic word network.",
        "link": "https://dl.acm.org/doi/10.1145/3224207.3224216"
    },
    {
        "title": "The Application of Data Mining Algorithms in the Construction of Travel Recommendation System",
        "authors": [
            "Chao Lou",
            "Zhaonan Mu",
            "Mengzhu Liu"
        ],
        "date": "January 2021",
        "source": "CONF-CDS 2021: The 2nd International Conference on Computing and Data Science",
        "abstract": "Photos shared by users of online social network often associated with tags, texts, geographic and time information. These data are ideal research resource which can be used by researchers to observe people's interests and behaviors. They can also be used to analyze people's travel behaviors and interests because it can reflect people's travel activities and experiences directly. This project aimed to consolidate people into groups based on their travel preferences of travel season, travel time span, scenery type of travel location and then analyze popular travel places for different people groups. The travel preferences were analyzed from extracted data applying PHP language and Naive Bayes classification approach. According to the analyzed data, people were divided into groups. Then, DBSCAN clustering was used to get popular locations based on the different groups. As a result, 80 thousand photo data of 5 cities have been collected from Flickr by Phython. It has been showed that people's preference of season in most of the mined cities was spring and summer and the preference of time span was short. Moreover, mined people has been divided into 16 groups according to their preference and based on the groups popular locations have been calculated by DBSCAN clustering. Theses locations have been provided in a recommendation website which was built in this project. For the future work, mining Flickr data associated with other travel social network sites could be a good way to build a more effective recommendation system.",
        "link": "https://dl.acm.org/doi/10.1145/3448734.3450477"
    },
    {
        "title": "Compare Machine Learning Models in Text Classification Using Steam User Reviews",
        "authors": [
            "Youchen Miao",
            "Zeyu Jin",
            "Yumeng Zhang",
            "Yuchen Chen",
            "Junren Lai"
        ],
        "date": "November 2021",
        "source": "ICSED '21: Proceedings of the 2021 3rd International Conference on Software Engineering and Development",
        "abstract": "Text Classification and Sentiment Analysis of game reviews are viewed as important parts in not only academic fields but also in game studies. In this paper, with more than 400 thousand game reviews on Steam platform, we preprocess the data using different libraries (sklearn, nltk, and spaCy) and use them as inputs to build three sentiment classification models based on different algorithms (Naive Bayes, SVM, and Random Forest). In contrast to previous studies that only focus on different sentiment analysis models, our paper also highlights the use of different APIs to preprocess the data and their corresponding model performance. The results show that no matter which API we choose, Random Forest models always perform the best. However, in terms of training time, Naive Bayes is the fastest. This work can be used to apply grid search for researchers to automatically find the optimum API before conducting sentiment analysis in the future.",
        "link": "https://dl.acm.org/doi/10.1145/3507473.3507480"
    },
    {
        "title": "Lost in Transduction: Transductive Transfer Learning in Text Classification",
        "authors": [
            "Alejandro Moreo",
            "Andrea Esuli",
            "Fabrizio Sebastiani"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Obtaining high-quality labelled data for training a classifier in a new application domain is often costly. Transfer Learning\n(a.k.a. “Inductive Transfer”) tries to alleviate these costs by transferring, to the “target” domain of interest, knowledge available from a different “source” domain. In transfer learning the lack of labelled information from the target domain is compensated by the availability at training time of a set of unlabelled examples from the target distribution. Transductive Transfer Learning denotes the transfer learning setting in which the only set of target documents that we are interested in classifying is known and available at training time. Although this definition is indeed in line with Vapnik’s original definition of “transduction”, current terminology in the field is confused. In this article, we discuss how the term “transduction” has been misused in the transfer learning literature, and propose a clarification consistent with the original characterization of this term given by Vapnik. We go on to observe that the above terminology misuse has brought about misleading experimental comparisons, with inductive transfer learning methods that have been incorrectly compared with transductive transfer learning methods. We then, give empirical evidence that the difference in performance between the inductive version and the transductive version of a transfer learning method can indeed be statistically significant (i.e., that knowing at training time the only data one needs to classify indeed gives an advantage). Our clarification allows a reassessment of the field, and of the relative merits of the major, state-of-the-art algorithms for transfer learning in text classification.",
        "link": "https://dl.acm.org/doi/10.1145/3453146"
    },
    {
        "title": "Machine Learning on Mining Potential Adverse Drug Reactions for Pharmacovigilance",
        "authors": [
            "Ke Jia"
        ],
        "date": "July 2021",
        "source": "DSIT 2021: 2021 4th International Conference on Data Science and Information Technology",
        "abstract": "Drugs are related to the lives and health of patients. Mining and discovering potential adverse drug reactions from a large number of unstructured texts play an important role in drug research and pharmacovigilance. It is a hot research issue in the field of biomedicine in recent years. A large number of scholars have discovered a large amount of information on adverse drug reactions from different data sources through effective machine learning methods. This paper reviews the research progress in recent years to provide a reference for future research.",
        "link": "https://dl.acm.org/doi/10.1145/3478905.3478964"
    },
    {
        "title": "TACAM: Topic And Context Aware Argument Mining",
        "authors": [
            "Michael Fromm",
            "Evgeniy Faerman",
            "Thomas Seidl"
        ],
        "date": "October 2019",
        "source": "WI '19: IEEE/WIC/ACM International Conference on Web Intelligence",
        "abstract": "In this work we address the problem of argument search. The purpose of argument search is the distillation of pro and contra arguments for requested topics from large text corpora. In previous works, the usual approach is to use a standard search engine to extract text parts which are relevant to the given topic and subsequently use an argument recognition algorithm to select arguments from them. The main challenge in the argument recognition task, which is also known as argument mining, is that often sentences containing arguments are structurally similar to purely informative sentences without any stance about the topic. In fact, they only differ semantically. Most approaches use topic or search term information only for the first search step and therefore assume that arguments can be classified independently of a topic. We argue that topic information is crucial for argument mining, since the topic defines the semantic context of an argument. Precisely, we propose different models for the classification of arguments, which take information about a topic of an argument into account. Moreover, to enrich the context of a topic and to let models understand the context of the potential argument better, we integrate information from different external sources such as Knowledge Graphs or pre-trained NLP models. Our evaluation shows that considering topic information, especially in connection with external information, provides a significant performance boost for the argument mining task.",
        "link": "https://dl.acm.org/doi/10.1145/3350546.3352506"
    },
    {
        "title": "Text mining for incoming tasks based on the urgency/importance factors and task classification using machine learning tools",
        "authors": [
            "Yasser Ali Alshehri"
        ],
        "date": "March 2020",
        "source": "ICCDA 2020: Proceedings of the 2020 the 4th International Conference on Compute and Data Analysis",
        "abstract": "In workplaces, there is a massive amount of unstructured data from different sources. In this paper, we present a case study that explains how can through communications between employees, we can help to prioritize tasks requests to increase the efficiency of their works for both technical and non-technical workers. This involves managing daily incoming tasks based on their level of urgency and importance.To allow all workers to utilize the urgency-importance matrix as a time-management tool, we need to automate this tool. The textual content of incoming tasks are analyzed, and metrics related to urgency and importance are extracted. A third factor (i.e., the response variable) is defined based on the two input variables (urgency and importance). Then, machine learning applied to the data to predict the class of incoming tasks based on data outcome desired. We used ordinal regression, neural networks, and decision tree algorithms to predict the four levels of task priority. We measure the performance of all using recalls, precisions, and F-scores. All classifiers perform higher than 89% in terms of all measures.",
        "link": "https://dl.acm.org/doi/10.1145/3388142.3388153"
    },
    {
        "title": "Text mining for malware classification using multivariate all repeated patterns detection",
        "authors": [
            "Konstantinos F. Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2019",
        "source": "ASONAM '19: Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Mobile phones have become nowadays a commodity to the majority of people. Using them, people are able to access the world of Internet and connect with their friends, their colleagues at work or even unknown people with common interests. This proliferation of the mobile devices has also been seen as an opportunity for the cyber criminals to deceive smartphone users and steel their money directly or indirectly, respectively, by accessing their bank accounts through the smartphones or by blackmailing them or selling their private data such as photos, credit card data, etc. to third parties. This is usually achieved by installing malware to smartphones masking their malevolent payload as a legitimate application and advertise it to the users with the hope that mobile users will install it in their devices. Thus, any existing application can easily be modified by integrating a malware and then presented it as a legitimate one. In response to this, scientists have proposed a number of malware detection and classification methods using a variety of techniques. Even though, several of them achieve relatively high precision in malware classification, there is still space for improvement. In this paper, we propose a text mining all repeated pattern detection method which uses the decompiled files of an application in order to classify a suspicious application into one of the known malware families. Based on the experimental results using a real malware dataset, the methodology tries to correctly classify (without any misclassification) all randomly selected malware applications of 3 categories with 3 different families each.",
        "link": "https://dl.acm.org/doi/10.1145/3341161.3350841"
    },
    {
        "title": "Comparative Study between Traditional Machine Learning and Deep Learning Approaches for Text Classification",
        "authors": [
            "Cannannore Nidhi Kamath",
            "Syed Saqib Bukhari",
            "Andreas Dengel"
        ],
        "date": "August 2018",
        "source": "DocEng '18: Proceedings of the ACM Symposium on Document Engineering 2018",
        "abstract": "In this contemporaneous world, it is an obligation for any organization working with documents to end up with the insipid task of classifying truckload of documents, which is the nascent stage of venturing into the realm of information retrieval and data mining. But classification of such humongous documents into multiple classes, calls for a lot of time and labor. Hence a system which could classify these documents with acceptable accuracy would be of an unfathomable help in document engineering. We have created multiple classifiers for document classification and compared their accuracy on raw and processed data. We have garnered data used in a corporate organization as well as publicly available data for comparison. Data is processed by removing the stop-words and stemming is implemented to produce root words. Multiple traditional machine learning techniques like Naive Bayes, Logistic Regression, Support Vector Machine, Random forest Classifier and Multi-Layer Perceptron are used for classification of documents. Classifiers are applied on raw and processed data separately and their accuracy is noted. Along with this, Deep learning technique such as Convolution Neural Network is also used to classify the data and its accuracy is compared with that of traditional machine learning techniques. We are also exploring hierarchical classifiers for classification of classes and subclasses. The system classifies the data faster and with better accuracy than if done manually. The results are discussed in the results and evaluation section.",
        "link": "https://dl.acm.org/doi/10.1145/3209280.3209526"
    },
    {
        "title": "Identification of Overpricing in the Purchase of Medication by the Federal Government of Brazil, Using Text Mining and Clustering Based on Ontology",
        "authors": [
            "Marco Aurelio O. S. Correa",
            "Adriano Galindo Leal"
        ],
        "date": "August 2018",
        "source": "ICCBDC '18: Proceedings of the 2018 2nd International Conference on Cloud and Big Data Computing",
        "abstract": "Increasing the transparency level in his actions and spending is one of the primary duties of the Brazilian Federal Government. The creation of laws that oblige full disclose of all its expenditures through transparency portals enables citizens to supervise all government entities. However, only the dissemination of these data, without a definite standard or the availability of data analysis tools, does not guarantee that the citizen is empowered to play his role. Therefore hence, the objective of this work is to identify overprice in the acquisition of products purchased by the federal government of Brazil using the unstructured data available on the Transparency Portal. The last two-years' worth of purchasing data, available in the Transparency Portal, were extracted, processed and stored. Due to his diverse nature and high volume of data, this study focused only on medicines purchased by the Ministry of Health. Ontology-based text mining and clustering techniques were applied for automatic identification and classification of products. The processing of this information was done through text mining and clustering, based on the ontology registered in another database of the Brazilian government. Because of this work, a consolidated price base per medication was created to allow the identification of distortions in prices practised, facilitating the identification of cases that merit further investigation to unravel fraud to the treasury.",
        "link": "https://dl.acm.org/doi/10.1145/3264560.3264569"
    },
    {
        "title": "Computational Estimation by Scientific Data Mining with Classical Methods to Automate Learning Strategies of Scientists",
        "authors": [
            "Aparna S. Varde"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Experimental results are often plotted as 2-dimensional graphical plots (aka graphs) in scientific domains depicting dependent versus independent variables to aid visual analysis of processes. Repeatedly performing laboratory experiments consumes significant time and resources, motivating the need for computational estimation. The goals are to estimate the graph obtained in an experiment given its input conditions, and to estimate the conditions that would lead to a desired graph. Existing estimation approaches often do not meet accuracy and efficiency needs of targeted applications. We develop a computational estimation approach called AutoDomainMine that integrates clustering and classification over complex scientific data in a framework so as to automate classical learning methods of scientists. Knowledge discovered thereby from a database of existing experiments serves as the basis for estimation. Challenges include preserving domain semantics in clustering, finding matching strategies in classification, striking a good balance between elaboration and conciseness while displaying estimation results based on needs of targeted users, and deriving objective measures to capture subjective user interests. These and other challenges are addressed in this work. The AutoDomainMine approach is used to build a computational estimation system, rigorously evaluated with real data in Materials Science. Our evaluation confirms that AutoDomainMine provides desired accuracy and efficiency in computational estimation. It is extendable to other science and engineering domains as proved by adaptation of its sub-processes within fields such as Bioinformatics and Nanotechnology.",
        "link": "https://dl.acm.org/doi/10.1145/3502736"
    },
    {
        "title": "Where is the road for issue reports classification based on text mining?",
        "authors": [
            "Qiang Fan",
            "Yue Yu",
            "Gang Yin",
            "Tao Wang",
            "Huaimin Wang"
        ],
        "date": "November 2017",
        "source": "ESEM '17: Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Currently, open source projects receive various kinds of issues daily, because of the extreme openness of Issue Tracking System (ITS) in GitHub. ITS is a labor-intensive and time-consuming task of issue categorization for project managers. However, a contributor is only required a short textual abstract to report an issue in GitHub. Thus, most traditional classification approaches based on detailed and structured data (e.g., priority, severity, software version and so on) are difficult to adopt. In this paper, issue classification approaches on a large-scale dataset, including 80 popular projects and over 252,000 issue reports collected from GitHub, were investigated. First, four traditional text-based classification methods and their performances were discussed. Semantic perplexity (i.e., an issues description confuses bug-related sentences with nonbug-related sentences) is a crucial factor that affects the classification performances based on quantitative and qualitative study. Finally, A two-stage classifier framework based on the novel metrics of semantic perplexity of issue reports was designed. Results show that our two-stage classification can significantly improve issue classification performances.",
        "link": "https://dl.acm.org/doi/10.1109/ESEM.2017.19"
    },
    {
        "title": "Feature-based Facebook reviews process model for e-management using data mining",
        "authors": [
            "Anish Kumar Varudharajulu",
            "Yongsheng Ma"
        ],
        "date": "January 2019",
        "source": "IC4E '19: Proceedings of the 10th International Conference on E-Education, E-Business, E-Management and E-Learning",
        "abstract": "The data generated from online communication acts as potential gold mines for discovering knowledge for researchers. A large amount of data is also generated in the form of web documents, emails, blogs, and feedback, etc. Text analytics is being significantly employed to mine important information. Opinion mining is the process of extracting human thoughts and perceptions from unstructured texts. The showstopper for designing an opinion mining system for analyzing reviews arise from the fact that customer reviews are often noisy. These reviews are informally written. In addition, they are subjected to spelling mistakes, grammatical errors, improper punctuation and irrational capitalization. This paper focuses on analyzing the different classification and clustering algorithms aimed at extracting and consolidating opinions of customers from social media sites like Facebook, Twitter and through surveys, at multiple levels of granularity to monitor and measure customer satisfaction. Ours is an automated approach, in which the system aids in the process of knowledge assimilation for knowledge-based building and also performs the analytics. Domain experts ratify the knowledge base and also provide training datasets for the system to intuitively gather more instances for ratification. The system identifies opinion expressions as phrases containing opinion words, opinionated features and also opinion modifiers. These expressions are categorized as positive, negative or neutral. Opinion expressions are identified and categorized using localized linguistic techniques. Opinions can be congregated at any desired level of specificity i.e. feature level or product level, user level or service level, etc. We have developed a system based on this approach, which provides the user with a platform to analyze opinion expressions crawled from a set of pre-defined datasets.",
        "link": "https://dl.acm.org/doi/10.1145/3306500.3306514"
    },
    {
        "title": "Discriminative Topic Mining via Category-Name Guided Text Embedding",
        "authors": [
            "Yu Meng",
            "Jiaxin Huang",
            "Guangyuan Wang",
            "Zihan Wang",
            "Chao Zhang",
            "Yu Zhang",
            "Jiawei Han"
        ],
        "date": "April 2020",
        "source": "WWW '20: Proceedings of The Web Conference 2020",
        "abstract": "Mining a set of meaningful and distinctive topics automatically from massive text corpora has broad applications. Existing topic models, however, typically work in a purely unsupervised way, which often generate topics that do not fit users’ particular needs and yield suboptimal performance on downstream tasks. We propose a new task, discriminative topic mining, which leverages a set of user-provided category names to mine discriminative topics from text corpora. This new task not only helps a user understand clearly and distinctively the topics he/she is most interested in, but also benefits directly keyword-driven classification tasks. We develop CatE, a novel category-name guided text embedding method for discriminative topic mining, which effectively leverages minimal user guidance to learn a discriminative embedding space and discover category representative terms in an iterative manner. We conduct a comprehensive set of experiments to show that CatE mines high-quality set of topics guided by category names only, and benefits a variety of downstream applications including weakly-supervised classification and lexical entailment direction identification.",
        "link": "https://dl.acm.org/doi/10.1145/3366423.3380278"
    },
    {
        "title": "Comparative Study of Heart Disease Diagnosis Using Top Ten Data Mining Classification Algorithms",
        "authors": [
            "I. Ketut Agung Enriko"
        ],
        "date": "June 2019",
        "source": "ICFET '19: Proceedings of the 5th International Conference on Frontiers of Educational Technologies",
        "abstract": "Data mining has been used for many purposes, especially for prediction system. In healthcare, data mining algorithms often used in disease diagnosis. Meanwhile, heart disease is known as a primary cause of death over the years. Many studies have been performed in heart disease diagnosis using data mining methods. There are some popular data mining algorithms that can be used in heart disease diagnosis, for example, k-Nearest Neighbor, CART, and AdaBoost. The algorithms are used to analyze a sample of cardiovascular patients data and predict the heart disease type that they suffer. Some parameters are taken from the patient, including EKG morphology, blood pressure, and information about the existence of chest pain, shortness of breath, palpitation, and cold sweat. In this study, medical records data are collected from Harapan Kita Hospital and utilized as a dataset sample in this research. Top ten data mining classification algorithms are used in diagnosing heart disease from Harapan Kita Hospital data and examining their performance by checking the accuracy and speed.",
        "link": "https://dl.acm.org/doi/10.1145/3338188.3338220"
    },
    {
        "title": "Text Mining Approach for Identifying Research Trends",
        "authors": [
            "Snezhana Sulova"
        ],
        "date": "June 2021",
        "source": "CompSysTech '21: Proceedings of the 22nd International Conference on Computer Systems and Technologies",
        "abstract": "With the increase of unstructured data, the issues connected with automatic text processing, the categorization of documents and the discovery of topics have become objects of growing interest. In order to improve the process of grouping and processing research publications, we would like to propose a method based upon natural language processing. It is based on text mining technologies which aim to identify key tendencies in documents. It processes the content of publications by clustering and identifies the topics of each identified group. This analysis helps by identifying key tendencies as well as discovering emerging new areas of research. Publications from the research literature database, Scopus, were used to test the approach. The topic of the publications is “the application of digital technologies in the logistics business”. The experiments were completed using the RapidMiner Studio software.",
        "link": "https://dl.acm.org/doi/10.1145/3472410.3472433"
    },
    {
        "title": "Assessment of Vulnerability Severity using Text Mining",
        "authors": [
            "Georgios Spanos",
            "Lefteris Angelis",
            "Dimitrios Toloudis"
        ],
        "date": "September 2017",
        "source": "PCI '17: Proceedings of the 21st Pan-Hellenic Conference on Informatics",
        "abstract": "Software1 vulnerabilities are closely associated with information systems security, a major and critical field in today's technology. Vulnerabilities constitute a constant and increasing threat for various aspects of everyday life, especially for safety and economy, since the social impact from the problems that they cause is complicated and often unpredictable. Although there is an entire research branch in software engineering that deals with the identification and elimination of vulnerabilities, the growing complexity of software products and the variability of software production procedures are factors contributing to the ongoing occurrence of vulnerabilities, Hence, another area that is being developed in parallel focuses on the study and management of the vulnerabilities that have already been reported and registered in databases. The information contained in such databases includes, a textual description and a number of metrics related to vulnerabilities. The purpose of this paper is to investigate to what extend the assessment of the vulnerability severity can be inferred directly from the corresponding textual description, or in other words, to examine the informative power of the description with respect to the vulnerability severity. For this purpose, text mining techniques, i.e. text analysis and three different classification methods (decision trees, neural networks and support vector machines) were employed. The application of text mining to a sample of 70,678 vulnerabilities from a public data source shows that the description itself is a reliable and highly accurate source of information for vulnerability prioritization.",
        "link": "https://dl.acm.org/doi/10.1145/3139367.3139390"
    },
    {
        "title": "Clustering Algorithms for Spatial Data Mining",
        "authors": [
            "Chetashri Bhadane",
            "Ketan Shah"
        ],
        "date": "April 2020",
        "source": "ICGDA '20: Proceedings of the 2020 3rd International Conference on Geoinformatics and Data Analysis",
        "abstract": "With the advances in mobile and wireless technologies, there has been a rise in applications that track and share the users' geospatial data. People use several social networking sites such as Twitter, Facebook and Flickr, where they share their status updates. With the integration of Global Positioning System (GPS) with mobile phones, it is now possible to share one's locations on these social networks. GPS allows us to record and track a person's movement along with the timestamp. The data set obtained from these GPS logs is vast and is widely used to analyze the users' movement patterns. Specifically, we can find out significant locations based on the number of users present at that location and the time spent by them at such places. Once significant places have been identified, it is also possible to identify the semantic importance of these locations. This paper presents an overview of the clustering techniques used to find important places of interest using large GPS based mobility datasets. Four clustering algorithms, K-Means, DBSCAN, OPTICS and Hierarchical, are implemented, and performance is tested using real-time data of 50 users collected over 2--5 years. Performance summary depicts that K-Means and DBSCAN perform well for spatial data.",
        "link": "https://dl.acm.org/doi/10.1145/3397056.3397068"
    },
    {
        "title": "Feature Selection based Arabic Text Classification using Different Machine Learning Algorithms: Comparative Study",
        "authors": [
            "Sakina Rim Bennabi",
            "Zakaria Elberrichi"
        ],
        "date": "June 2020",
        "source": "ICIST '20: Proceedings of the 10th International Conference on Information Systems and Technologies",
        "abstract": "Feature selection is a method of data pre-processing widely used when mining large data, such as textual classification. Several studies have been conducted to compare the different methods of feature selection applied to corpora in English. Unfortunately, a small number of works concern the Arabic language. This article aims to present a comparative study of different feature selection techniques including: Chi2, the ANOVA method and mutual information, applied on a corpus in Arabic language, while also diversifying the machine learning algorithms (Naive Bayes, SVM and KNN). This experimental study has shown in general that reducing dimensionality with feature selection techniques has slightly affected the performance of textual classification, reducing the size of the corpus by up to 1%.",
        "link": "https://dl.acm.org/doi/10.1145/3447568.3448531"
    },
    {
        "title": "Intelligent Data Analysis using Optimized Support Vector Machine Based Data Mining Approach for Tourism Industry",
        "authors": [
            "Ms Promila Sharma",
            "Uma Meena",
            "Girish Kumar Sharma"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Data analysis involves the deployment of sophisticated approaches from data mining methods, information theory, and artificial intelligence in various fields like tourism, hospitality, and so on for the extraction of knowledge from the gathered and preprocessed data. In tourism, pattern analysis or data analysis using classification is significant for finding the patterns that represent new and potentially useful information or knowledge about the destination and other data. Several data mining techniques are introduced for the classification of data or patterns. However, overfitting, less accuracy, local minima, sensitive to noise are the drawbacks in some existing data mining classification methods. To overcome these challenges, Support vector machine with Red deer optimization (SVM-RDO) based data mining strategy is proposed in this article. Extended Kalman filter (EKF) is utilized in the first phase, i.e., data cleaning to remove the noise and missing values from the input data. Mantaray foraging algorithm (MaFA) is used in the data selection phase, in which the significant data are selected for the further process to reduce the computational complexity. The final phase is the classification, in which SVM-RDO is proposed to access the useful pattern from the selected data. PYTHON is the implementation tool used for the experiment of the proposed model. The experimental analysis is done to show the efficacy of the proposed work. From the experimental results, the proposed SVM-RDO achieved better accuracy, precision, recall, and F1 score than the existing methods for the tourism dataset. Thus, it is showed the effectiveness of the proposed SVM-RDO for pattern analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3494566"
    },
    {
        "title": "Applying text mining to predict learners' cognitive engagement",
        "authors": [
            "Hayati Hind",
            "Mohammed Khalidi Idrissi",
            "Samir Bennani"
        ],
        "date": "October 2017",
        "source": "SCAMS '17: Proceedings of the Mediterranean Symposium on Smart City Application",
        "abstract": "In academic process, engagement represent one of the important success' factor. This paper proposes a predictive system for learners' cognitive engagement based on their online discussion forums participation. In the first level the ontology OWL and the LSA method are used to perform a semantic classification system of the threads according to a specific context chosen by the tutor. Then the Text mining, as a predictive method, is applied to the classified threads and learners' participation in the forums.",
        "link": "https://dl.acm.org/doi/10.1145/3175628.3175655"
    },
    {
        "title": "Investigate Transitions into Drug Addiction through Text Mining of Reddit Data",
        "authors": [
            "John Lu",
            "Sumati Sridhar",
            "Ritika Pandey",
            "Mohammad Al Hasan",
            "Georege Mohler"
        ],
        "date": "July 2019",
        "source": "KDD '19: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Increasing rates of opioid drug abuse and heightened prevalence of online support communities underscore the necessity of employing data mining techniques to better understand drug addiction using these rapidly developing online resources. In this work, we obtained data from Reddit, an online collection of forums, to gather insight into drug use/misuse using text snippets from users narratives. Specifically, using users' posts, we trained a binary classifier which predicts a user's transitions from casual drug discussion forums to drug recovery forums. We also proposed a Cox regression model that outputs likelihoods of such transitions. In doing so, we found that utterances of select drugs and certain linguistic features contained in one's posts can help predict these transitions. Using unfiltered drug-related posts, our research delineates drugs that are associated with higher rates of transitions from recreational drug discussion to support/recovery discussion, offers insight into modern drug culture, and provides tools with potential applications in combating the opioid crisis.",
        "link": "https://dl.acm.org/doi/10.1145/3292500.3330737"
    },
    {
        "title": "Analysing digital banking reviews using text mining",
        "authors": [
            "Li Chen Cheng",
            "Legaspi Rhea Sharmayne"
        ],
        "date": "December 2020",
        "source": "ASONAM '20: Proceedings of the 12th IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Digital banks are new entrants in the banking industry in the Philippines as they only started late 2018. Since then, a handful of players have and are still emerging. With more and more people becoming technologically savvy, it is very critical for financial institutions to develop a digital banking application that will stand out from the competition. This paper aims to use text mining methods to analyse digital banking application reviews. This study will perform topic modelling using LDA to explore customer concerns and will mine association rules between the digital banking features with the review score. The results will reveal which areas the digital banking application can further optimize for customer satisfaction and retention.",
        "link": "https://dl.acm.org/doi/10.1109/ASONAM49781.2020.9381429"
    },
    {
        "title": "Mining Moocs Videos Metadata Using Classification Techniques",
        "authors": [
            "El Harrak Othman",
            "Ghadi Abderrahim",
            "El Bouhdidi Jaber"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "In few years, the internet has become the first source of information for most people, Today MOOCs makes it possible for everyone to access to the education over the world, it's represents an emerging methodology of online teaching and an important development in open education. Due to the rapid development of Moocs and the rapid growth of digital data and video database over the Internet, it is becoming very difficult for learners to browsing and choosing the best training for them. The scientific community has increased the amount of research into new technologies, with a view to improve the digital video utilization: its archiving, indexing, accessibility, acquisition, store and even its process and usability. All these parts of the video utilization entail the necessity of the extraction of all important information of a video, especially in cases of lack of metadata information. Web video mining is retrieving the content using data mining techniques from World Wide Web. There are two approaches for web video mining using traditional image processing (signal processing) and metadata based approach. In this paper, we present the various research makes in this subject and we propose an effective methodology to extract the metadata from Moocs videos and classify them based on the extracted metadata by applying data mining techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090450"
    },
    {
        "title": "Text mining with HathiTrust",
        "authors": [
            "Eleanor Dickson Koehl",
            "Ryan Dubnicek"
        ],
        "date": "June 2019",
        "source": "JCDL '19: Proceedings of the 18th Joint Conference on Digital Libraries",
        "abstract": "This tutorial will introduce attendees to the HathiTrust Research Center and its tools and services for computational text analysis research. HTRC leverages the scope and scale of the HathiTrust Digital Library collection to create opportunities for researchers to perform text data mining on subsets of the corpus. Attendees will develop skills that will allow them to conduct basic text analysis research using HathiTrust data, learn techniques for engaging with scholars on their text data mining research, and explore strategies for computational access to large-scale digital collections.",
        "link": "https://dl.acm.org/doi/10.1109/JCDL.2019.00115"
    },
    {
        "title": "Text mining for plagiarism detection: multivariate pattern detection for recognition of text similarities",
        "authors": [
            "Konstantinos Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2018",
        "source": "ASONAM '18: Proceedings of the 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "The problem of plagiarism the recent years has been intensified by the availability of information in digital form and the accessibility of the electronic libraries through the Internet. As a result, plagiarism detection has been transformed into a big data analytics problem since the number of digital sources is extravagant and a new document needs to be compared with millions of other existing documents. In this paper, a text mining methodology is proposed that can detect all common patterns between a document and the documents in a reference database. The technique is based on a pattern detection algorithm and the corresponding data structure that enables the algorithm to detect all common patterns. The methodology has been applied in a well-defined dataset providing very promising results identifying difficult cases of plagiarism such as technical disguise.",
        "link": "https://dl.acm.org/doi/10.5555/3382225.3382424"
    },
    {
        "title": "Petroleum Engineering Data Text Classification Using Convolutional Neural Network Based Classifier",
        "authors": [
            "Tzuhan Hsu",
            "Yaoqin Zhang"
        ],
        "date": "May 2018",
        "source": "ICMLT '18: Proceedings of the 2018 International Conference on Machine Learning Technologies",
        "abstract": "In this paper, authors introduce a convolutional neural network (CNN) based classification approach for petroleum engineering event. Recently, text classification problem has raised substantial attention in the field of machine learning. The aim of text classification process is to assign correct label to a text on the basis of its content. However, due to the imprecise of feature representation, most of former text classification methods cannot export satisfying result. To solve this problem, authors propose a model based on convolutional neural network. Authors conduct experiments with real world petroleum engineering data text, artificially labelled into 6 categories. Beside purposed CNN-based model, the authors also implement other baseline machine learning approaches: k nearest neighbor, random forest, support vector machines, recurrent neural network, gated recurrent units, long short-term memory. Among all these models, our CNN-based model reach best performance in text classification task. In the last part of this paper, authors compare and analysis their purpose and baseline model.",
        "link": "https://dl.acm.org/doi/10.1145/3231884.3231898"
    },
    {
        "title": "Classifying Indonesian Online Articles as Advertisement Placement Base Using Text Mining",
        "authors": [
            "Nadhira Tasya",
            "Arian Dhini"
        ],
        "date": "July 2017",
        "source": "ICBIM 2017: Proceedings of the International Conference on Business and Information Management",
        "abstract": "Rapid development in technological aspect resulting in growing level of human needs for the latest news, so that emerged a new trend of publishing and accessing news through online media or usually called online journalism. In addition, the number of people who sell and purchase through online sites also continues to increase and this opportunity is utilized by the company and the advertiser by implementing targeted web advertising. However, the high number of articles that have been published and accessed leads to great opportunities for errors in determining where to place the ads. Therefore, it needs a system that can categorize articles accessed by users as the basis of advertisement placement by the company and this classification system can be done by applying the method of Data Mining and Text Mining. This research uses document data in the form of article content that will be categorized into twenty categories of class of advertisement by using Text Mining technique with Support Vector Machine algorithm. The results of this study may be used by companies or advertisers as a basis for placement of ads on selected online media sites.",
        "link": "https://dl.acm.org/doi/10.1145/3134271.3134288"
    },
    {
        "title": "Mining Free-Text Medical Notes for Suicide Risk Assessment",
        "authors": [
            "Marios Adamou",
            "Grigoris Antoniou",
            "Elissavet Greasidou",
            "Vincenzo Lagani",
            "Paulos Charonyktakis",
            "Ioannis Tsamardinos"
        ],
        "date": "July 2018",
        "source": "SETN '18: Proceedings of the 10th Hellenic Conference on Artificial Intelligence",
        "abstract": "Suicide has been considered as an important public health issue for a very long time, and is one of the main causes of death worldwide. Despite suicide prevention strategies being applied, the rate of suicide has not changed substantially over the past decades. Advances in machine learning make it possible to attempt to predict suicide based on the analysis of relevant data to inform clinical practice. This paper reports on findings from the analysis of data of patients who died by suicide in the period 2013-2016 and made use of both structured data and free-text medical notes. We focus on examining various text-mining approaches to support risk assessment. The results show that using advance machine learning and text-mining techniques, it is possible to predict within a specified period which people are most at risk of taking their own life at the time of referral to a mental health service.",
        "link": "https://dl.acm.org/doi/10.1145/3200947.3201020"
    },
    {
        "title": "Integration of data mining techniques in e-learning systems: Clustering Profil of Lerners and Recommender Course System",
        "authors": [
            "A. El Moustamid",
            "E. En-Naimi",
            "J. El Bouhdidi"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "Data mining aims at the extraction of knowledge from large quantities of data by automatic or semi-automatic methods. In the field of education and training e-learning, Data Mining techniques as clustering (The aim of this technique is to identify groups that are similar in some aspect) [21], prediction (is used for developing model where combination of data viewed from different aspects can effect single viewed data. Here, is important to watch how predictors variables are effecting predicted variable)[21], classification are applied in e-learning to improve the level of learners by providing learning according to the learners' requirement) are used to analyze learners profile, to predict and improve student performance. The aim of our project is to develop a system capable of analyzing learners profile and indexing web videos in order to offer learners a rich database with courses that correspond to their levels.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090453"
    },
    {
        "title": "Multi-level mining and visualization of scientific text collections: Exploring a bi-lingual scientific repository",
        "authors": [
            "Pablo Accuosto",
            "Francesco Ronzano",
            "Daniel Ferrés",
            "Horacio Saggion"
        ],
        "date": "December 2017",
        "source": "WOSP 2017: Proceedings of the 6th International Workshop on Mining Scientific Publications",
        "abstract": "We present a system to mine and visualize collections of scientific documents by semantically browsing information extracted from single publications or aggregated throughout corpora of articles. The text mining tool performs deep analysis of document collections allowing the extraction and interpretation of research paper's contents. In addition to the extraction and enrichment of documents with metadata (titles, authors, affiliations, etc), the deep analysis performed comprises semantic interpretation, rhetorical analysis of sentences, triple-based information extraction, and text summarization. The visualization components allow geographical-based exploration of collections, topic-evolution interpretation, and collaborative network analysis among others. The paper presents a case study of a bi-lingual collection in the field of Natural Language Processing (NLP).",
        "link": "https://dl.acm.org/doi/10.1145/3127526.3127529"
    },
    {
        "title": "Hierarchical Topic Mining via Joint Spherical Tree and Text Embedding",
        "authors": [
            "Yu Meng",
            "Yunyi Zhang",
            "Jiaxin Huang",
            "Yu Zhang",
            "Chao Zhang",
            "Jiawei Han"
        ],
        "date": "August 2020",
        "source": "KDD '20: Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Mining a set of meaningful topics organized into a hierarchy is intuitively appealing since topic correlations are ubiquitous in massive text corpora. To account for potential hierarchical topic structures, hierarchical topic models generalize flat topic models by incorporating latent topic hierarchies into their generative modeling process. However, due to their purely unsupervised nature, the learned topic hierarchy often deviates from users' particular needs or interests. To guide the hierarchical topic discovery process with minimal user supervision, we propose a new task, Hierarchical Topic Mining, which takes a category tree described by category names only, and aims to mine a set of representative terms for each category from a text corpus to help a user comprehend his/her interested topics. We develop a novel joint tree and text embedding method along with a principled optimization procedure that allows simultaneous modeling of the category tree structure and the corpus generative process in the spherical space for effective category-representative term discovery. Our comprehensive experiments show that our model, named JoSH, mines a high-quality set of hierarchical topics with high efficiency and benefits weakly-supervised hierarchical text classification tasks.",
        "link": "https://dl.acm.org/doi/10.1145/3394486.3403242"
    },
    {
        "title": "Unsupervised behavioural mining and clustering for malware family identification",
        "authors": [
            "Khanh Huu The Dam",
            "Thomas Given-Wilson",
            "Axel Legay"
        ],
        "date": "March 2021",
        "source": "SAC '21: Proceedings of the 36th Annual ACM Symposium on Applied Computing",
        "abstract": "More accurate and advanced detection and classification of malware requires exploiting program behaviour and not merely syntactic or static features. One approach is to use system call dependency graphs (SCDGs) that represent the program behaviour by interactions with the system, and the relations between these interactions. These SCDGs have been used with supervised learning techniques to very accurately detect and classify malware. This works considers the unsupervised learning challenge of mining for common clusters of behaviour without a priori knowledge. This allows for clustering of similar programs by behaviour, that can then be used for either classification or further analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3412841.3441919"
    },
    {
        "title": "Text Mining for Evaluating Authors' Birth and Death Years",
        "authors": [
            "Dror Moghaz",
            "Yaakov Hacohen-Kerner",
            "Dov Gabbay"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "This article presents a unique method in text and data mining for finding the era, i.e., mining temporal data, in which an anonymous author was living. Finding this era can assist in the examination of a fake document or extracting the time period in which a writer lived. The study and the experiments concern Hebrew, and in some parts, Aramaic and Yiddish rabbinic texts. The rabbinic texts are undated and contain no bibliographic sections, posing an interesting challenge. This work proposes algorithms using key phrases and key words that allow the temporal organization of citations together with linguistic patterns. Based on these key phrases, key words, and the references, we established several types of “Iron-clad,” Heuristic and Greedy rules for estimating the years of birth and death of a writer in an interesting classification task. Experiments were conducted on corpora, including documents authored by 12, 24, and 36 rabbinic writers and demonstrated promising results.",
        "link": "https://dl.acm.org/doi/10.1145/3281631"
    },
    {
        "title": "Transfer Learning to Timed Text Based Video Classification Using CNN",
        "authors": [
            "Zenun Kastrati",
            "Ali Shariq Imran",
            "Arianit Kurti"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "Open educational video resources are gaining popularity with a growing number of massive open online courses (MOOCs). This has created a niche for content providers to adopt effective solutions in automatically organizing and structuring of educational resources for maximum visibility. Recent advances in deep learning techniques are proving useful in managing and classifying resources into appropriate categories. This paper proposes one such convolutional neural network (CNN) model for classifying video lectures in a MOOC setting using a transfer learning approach. The model uses a time-aligned text transcripts corresponding to video lectures from six broader subject categories. Video lectures and their corresponding transcript dataset is gathered from the Coursera MOOC platform. Two different CNN models are proposed: i) CNN based classification using embeddings learned from our MOOC dataset, ii) CNN based classification using transfer learning. Word embeddings generated from two well known state-of-the-art pre-trained models Word2Vec and GloVe, are used in the transfer learning approach for the second case. The proposed CNN models are evaluated using precision, recall, and F1 score and the obtained performance is compared with both conventional and deep learning classifiers. The proposed CNN models have an F1 score improvement of 10-22 percentage points over DNN and conventional classifiers",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326483"
    },
    {
        "title": "Evolutionary role mining in complex networks by ensemble clustering",
        "authors": [
            "Sarvenaz Choobdar",
            "Pedro Ribeiro",
            "Fernando Silva"
        ],
        "date": "April 2017",
        "source": "SAC '17: Proceedings of the Symposium on Applied Computing",
        "abstract": "The structural patterns in the neighborhood of nodes assign unique roles to the nodes. Mining the set of existing roles in a network provides a descriptive profile of the network and draws its general picture. This paper proposes a new method to determine structural roles in a dynamic network based on the current position of nodes and their historic behavior. We develop a temporal ensemble clustering technique to dynamically find groups of nodes, holding similar tempo-structural roles. We compare two weighting functions, based on age and distribution of data, to incorporate temporal behavior of nodes in the role discovery. To evaluate the performance of the proposed method, we assess the results from two points of view: 1) goodness of fit to current structure of the network; 2) consistency with historic data. We conduct the evaluation using different ensemble clustering techniques. The results on real world networks demonstrate that our method can detect tempo-structural roles that simultaneously depict the topology of a network and reflect its dynamics with high accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3019612.3019815"
    },
    {
        "title": "Analyzing used-car web listings via text mining",
        "authors": [
            "Ayhan Demiriz",
            "Fatma Cantaş"
        ],
        "date": "October 2017",
        "source": "IML '17: Proceedings of the 1st International Conference on Internet of Things and Machine Learning",
        "abstract": "Used car trade is one of the major components of the world economies. It is not uncommon to sell a car by placing an internet advertisement irrespective of the geography in these days. A typical content of an advertisement is usually composed of two parts namely the structured and the free text data. The structured data may include some information about the asking price, make, model, year, mileage of the car and the contact info. In most cases, seller may give important clues about the car's current conditions in the free text data where the title (head) of the advertisement can be included as free text too. This paper reports preliminary results from a text mining study conducted on 75K used car internet listings collected from two major car listing web sites in Turkey. As expected, the words and the phrases related to the description of the car are observed to be frequent. The leading concepts in the free text are found to be regarding how to describe the current condition of a car, for example \"no crash history\".",
        "link": "https://dl.acm.org/doi/10.1145/3109761.3109782"
    },
    {
        "title": "Text classification of Diseases Treated by Traditional Chinese Medicine Prescription based on machine learning",
        "authors": [
            "Hanqing Zhao",
            "Bo Han",
            "Chen Li"
        ],
        "date": "September 2020",
        "source": "ISAIMS '20: Proceedings of the 1st International Symposium on Artificial Intelligence in Medical Sciences",
        "abstract": "OBJECTIVE: To explore the application of machine learning in th e identification of diseases treated by traditional Chinese medicine prescriptions. METHODS: Based on the composition of the text document of Chinese medicine prescriptions, the prescriptions were divided into cough, headache and diarrhea. THUTCT were introduced to establish and train two machine learning text classification models, LibSVM and LibLinear, and the prescriptions to be deter mined were put into the model for classification and prediction. R ESULTS: The Precision rate of LibSVM model and Liblinear mod el were 0.7283 and 0.6690. Seven prescriptions were classified an d predicted, and the results were in line with expectations. CONCLUSION: THUCTC has good universality for the content of TCM prescriptions, high classification accuracy and fast testing speed, which is suitable for the text classification and discrimination research of TCM prescriptions for diseases.",
        "link": "https://dl.acm.org/doi/10.1145/3429889.3429895"
    },
    {
        "title": "Evaluation of retweet clustering method classification method using retweets on Twitter without text data",
        "authors": [
            "K. Uchida",
            "F. Toriumi",
            "T. Sakaki"
        ],
        "date": "August 2017",
        "source": "WI '17: Proceedings of the International Conference on Web Intelligence",
        "abstract": "Burst phenomena, which frequently occur on social media, are caused by such social events as flaming on the internet, elections, and natural disasters. To understand people's thoughts and feelings, we must classify their opinions from burst phenomena. Therefore, classification methods that categorize tweets are critical. However, since most classification methods focus on text mining, they cannot group tweets by topics because each tweet has poor linguistic similarities. We used a non-text-based classification method proposed by Baba et al. that groups tweets by topics, even if they have poor linguistic similarities, and verified its validity by comparing it with a text-based classification method in two different evaluations: qualitative and quantitative. In the qualitative evaluation part, we did a questionnaire survey and validated the suitability of the topic clusters created using both the non-and text-based methods. Since evaluating the similarity of every pair of tweets in each topic is difficult, we evaluated the similarity between sampled pairs in the survey and acquired more appropriate topic clustering results using the non-text-based method than the text-based method. In the quantitative evaluation part, we focused on the robustness of each method against data reduction. Many approaches analyze social media data, especially because collecting data from social media is comparatively easy. However, since collecting the whole data of burst phenomena is very costly due to the vast amounts of available social media data, robustness against data reduction is an important index to evaluate classification methods. With the non-text-based method, over 55% of the pairs of tweets in the same cluster were also included in the same cluster even when the data were reduced to 10% in all three of our example cases. In this paper, as a source we focus on Twitter, one of the most popular microblogging services. Using clustering to conduct detailed case analyses, we scrutinized three burst cases that include natural disasters and flaming on the internet and found that a non-text-based method more effectively classified tweets in burst phenomena than a text-based method.",
        "link": "https://dl.acm.org/doi/10.1145/3106426.3106451"
    },
    {
        "title": "Using Spark for Text Mining on Large Scale Liver Cancer Literature",
        "authors": [
            "Ming-Yen Lin",
            "Yu-Ju Lin",
            "Sue-Chen Hsueh"
        ],
        "date": "January 2021",
        "source": "BDET 2021: 2021 the 3rd International Conference on Big Data Engineering and Technology (BDET)",
        "abstract": "Cancer is one of the main causes of death. The number of known cancer to-date is more than one hundred. Liver cancer, ranked after trachea and lung cancer, has long been high in the cancer leading cause in Taiwan and even ranked as second in 2016. The number of scientific articles related to cancer proliferates every year. The number reaches as high as 20 million in PubMed so that discovering useful information from the massive collection is very difficult. In addition, using a single machine to sift through these articles is very time-consuming. Therefore, we present a big data analytic framework using the distributed Apache Spark platform for text mining in PubMed literature. We establish a prediction model for liver cancer articles so that researchers may effectively validate whether an article is related to liver cancer or not. Classification models in Spark MLlib including Linear Support Vector Machines (SVM), Logistic Regression, are used in our experiments. Relevancy to liver cancer is further confirmed by using MeSH (Medical Subject Headings) terms. Logistic regression is about 3 times faster than SVMs and the accuracy of both methods is close to 95% in the experiments using hold-out validation. When max_features is 500 and min_df ≤ 0.1 (or min_df = 1), the accuracy may reach 96%. In the experiments with K-fold cross-validation, the accuracy of SVMs methods is 96%. The experimental results show that that our prediction model may effectively classify liver cancer articles.",
        "link": "https://dl.acm.org/doi/10.1145/3474944.3474958"
    },
    {
        "title": "Automatic topic classification of test cases using text mining at an Android smartphone vendor",
        "authors": [
            "Junji Shimagaki",
            "Yasutaka Kamei",
            "Naoyasu Ubayashi",
            "Abram Hindle"
        ],
        "date": "October 2018",
        "source": "ESEM '18: Proceedings of the 12th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Background: An Android smartphone is an ecosystem of applications, drivers, operating system components, and assets. The volume of the software is large and the number of test cases needed to cover the functionality of an Android system is substantial. Enormous effort has been already taken to properly quantify \"what features and apps were tested and verified?\". This insight is provided by dashboards that summarize test coverage and results per feature. One method to achieve this is to manually tag or label test cases with the topic or function they cover, much like function points. At the studied Android smartphone vendor, tests are labelled with manually defined tags, so-called \"feature labels (FLs)\", and the FLs serve to categorize 100s to 1000s test cases into 10 to 50 groups. Aim: Unfortunately for developers, manual assignment of FLs to 1000s of test cases is a time consuming task, leading to inaccurately labeled test cases, which will render the dashboard useless. We created an automated system that suggests tags/labels to the developers for their test cases rather than manual labeling. Method: We use machine learning models to predict and label the functionality tested by 10,000 test cases developed at the company. Results: Through the quantitative experiments, our models achieved acceptable F-1 performance of 0.3 to 0.88. Also through the qualitative studies with expert teams, we showed that the hierarchy and path of tests was a good predictor of a feature's label. Conclusions: We find that this method can reduce tedious manual effort that software developers spent classifying test cases, while providing more accurate classification results.",
        "link": "https://dl.acm.org/doi/10.1145/3239235.3268927"
    },
    {
        "title": "To apply Data Mining for Classification of Crowd sourced Software Requirements",
        "authors": [
            "Soonh Taj",
            "Qasim Arain",
            "Imran Memon",
            "Asma Zubedi"
        ],
        "date": "April 2019",
        "source": "ICSIE '19: Proceedings of the 8th International Conference on Software and Information Engineering",
        "abstract": "Now a day's main focus of developers is to build quality software that works according to customer needs and for this reason it is necessary to gather right requirements as requirement elicitation is the critical step that impacts on the success of software project as misinterpreted requirements leads to the failure of software project. By keeping this in mind a research is carried out on improving requirements elicitation process and automating the process of classifying requirements. In this research, a model is proposed which will help in this scenario for requirements elicitation and requirement classification. This paper presents a model in which crowd sourcing approach is used so that customers, end users, stakeholders, developers and software engineers can make active participation for requirement elicitation process and requirements gathered using crowdsourcing approach are used by model for classification process i.e. classification of requirements into functional and non-functional requirements. For the proof of proposed model a case study is conducted. Results of case study provided the usefulness and efficiency of proposed model for classification of crowd sourced software requirements.",
        "link": "https://dl.acm.org/doi/10.1145/3328833.3328837"
    },
    {
        "title": "Text classification using Fuzzy TF-IDF and Machine Learning Models",
        "authors": [
            "Mariem Bounabi",
            "Karim El Moutaouakil",
            "Khalid Satori"
        ],
        "date": "October 2019",
        "source": "BDIoT'19: Proceedings of the 4th International Conference on Big Data and Internet of Things",
        "abstract": "The representation of the information has an important impact on the text classification task. Several weighting methods were proposed in the literature, and the term frequency-inverse term frequency (TFIDF), the most know on the text treatment field. The FTF-IDF is a vector representation where the components of the TFIDF are presented as inputs to the Fuzzy Inference System (FIS). In this work, we compare several Machin Learning algorithms such as Naïve Bayes and its derivatives, SVM and Random forest classifiers, using the FTF-IDF representation. To improve the quality of the used classifiers, we call sum attribute selection methods. The recognition rate, for the tested systems, is satisfied, where the system based on naïve Bayes classifier, the FTF-IDF weighting terms, and the info gain select attributes method gives 98.7% as accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3372938.3372956"
    },
    {
        "title": "Text Mining in Cybersecurity: A Systematic Literature Review",
        "authors": [
            "Luciano Ignaczak",
            "Guilherme Goldschmidt",
            "Cristiano André Da Costa",
            "Rodrigo Da Rosa Righi"
        ],
        "date": null,
        "source": "ACM Computing Surveys",
        "abstract": "The growth of data volume has changed cybersecurity activities, demanding a higher level of automation. In this new cybersecurity landscape, text mining emerged as an alternative to improve the efficiency of the activities involving unstructured data. This article proposes a Systematic Literature Review (SLR) to present the application of text mining in the cybersecurity domain. Using a systematic protocol, we identified 2,196 studies, out of which 83 were summarized. As a contribution, we propose a taxonomy to demonstrate the different activities in the cybersecurity domain supported by text mining. We also detail the strategies evaluated in the application of text mining tasks and the use of neural networks to support activities involving unstructured data. The work also discusses text classification performance aiming its application in real-world solutions. The SLR also highlights open gaps for future research, such as the analysis of non-English content and the intensification in the usage of neural networks.",
        "link": "https://dl.acm.org/doi/10.1145/3462477"
    },
    {
        "title": "Using Text Mining to Analyze the Financial Patents",
        "authors": [
            "Yung-Feng Lu",
            "Jia-Lang Xu",
            "Mu-Yen Chen"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "With the advancement and development of science and technology, Nowadays society is the development of information technology, cloud computing, mobile Internet, Internet of Things, block chain and information technology.The combination of finance and technology has made financial technology terms gradually emerging.. This research analyzes the patents of financial technology. However, Taiwan's financial technology patents are mostly G06Q-20 (payment plan, architecture or agreement), G06Q-30 (commercial, such as marketing, shopping, payment, auction or e-commerce). G06Q-40 (financial, investment or tax treatment, insurance).The research use text mining technology to analyze and understand Taiwan's trends and developments in financial technology. The research results show that G06Q-20 patents are biased towards information security and mobile payment. G06Q-30 patents favor shopping websites and advertising. G06Q-40 patents are biased towards diversity, such as securities, insurance, finance, and stocks.",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326478"
    },
    {
        "title": "Identifying Online Profiles of Distance Learning Students Using Data Mining Techniques",
        "authors": [
            "Osama Islam",
            "Muazzam Siddiqui",
            "Naif Radi Aljohani"
        ],
        "date": "October 2019",
        "source": "ICDTE '19: Proceedings of the 3rd International Conference on Digital Technology in Education",
        "abstract": "Educational data has grown over the years with the increased use of technology within educational environments. This has led to a huge amount of data being stored in various data sources representing the student, his/her activities, and other aspects relevant to the learning process. To meet this analytical need, Educational Data Mining (EDM) has emerged to assist educational institutions in identifying key benefits such as students at risk, the level of student engagement or predicting student performance. The aim of this research was to explore the various aspects of student interaction data using data mining techniques to identify relevant patterns of behaviors and possible key attributes that have higher degrees of influence on distance learning students. The main findings identified several patterns of user online profiles based on a set of adopted learning strategies, the research proposed a framework for analyzing such interaction data based on R and Hadoop platforms to correlate online profiles with student performance.",
        "link": "https://dl.acm.org/doi/10.1145/3369199.3369249"
    },
    {
        "title": "Extremely Fast Decision Tree Mining for Evolving Data Streams",
        "authors": [
            "Albert Bifet",
            "Jiajin Zhang",
            "Wei Fan",
            "Cheng He",
            "Jianfeng Zhang",
            "Jianfeng Qian",
            "Geoff Holmes",
            "Bernhard Pfahringer"
        ],
        "date": "August 2017",
        "source": "KDD '17: Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
        "abstract": "Nowadays real-time industrial applications are generating a huge amount of data continuously every day. To process these large data streams, we need fast and efficient methodologies and systems. A useful feature desired for data scientists and analysts is to have easy to visualize and understand machine learning models. Decision trees are preferred in many real-time applications for this reason, and also, because combined in an ensemble, they are one of the most powerful methods in machine learning. In this paper, we present a new system called STREAMDM-C++, that implements decision trees for data streams in C++, and that has been used extensively at Huawei. Streaming decision trees adapt to changes on streams, a huge advantage since standard decision trees are built using a snapshot of data, and can not evolve over time. STREAMDM-C++ is easy to extend, and contains more powerful ensemble methods, and a more efficient and easy to use adaptive decision trees. We compare our new implementation with VFML, the current state of the art implementation in C, and show how our new system outperforms VFML in speed using less resources.",
        "link": "https://dl.acm.org/doi/10.1145/3097983.3098139"
    },
    {
        "title": "Data miners' little helper: data transformation activity cues for cluster analysis on document collections",
        "authors": [
            "Tania Cerquitelli",
            "Evelina Di Corso",
            "Francesco Ventura",
            "Silvia Chiusano"
        ],
        "date": "June 2017",
        "source": "WIMS '17: Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "In this paper we propose a new self-learning engine to streamline the analytics process, as it enables analysts to mine massive data repositories with minimal user intervention. In the context of cluster analysis on a collection of documents this new system, named SELF-DATA (SELF-learning DAta TrAnsformation), suggests to the analyst how to configure the whole mining process for a given dataset. SELF-DATA relies on an engine exploring different data weighting schemas (e.g., normalized term frequencies) and data transformation methods (e.g., PCA) before applying the cluster analysis, evaluating and comparing solutions through different quality indices (e.g., weighted Silhouette), and presenting the k-top solutions to the analyst. SELF-DATA will also include a knowledge base storing results of experiments on previously processed datasets, and a classification algorithm trained on the knowledge base content to forecast the best configuration for the whole mining process for an unexplored dataset. The first development of SELF-DATA running on Apache Spark has been validated on 5 collections of documents. Experimental results highlight that TF-IDF and logarithmic entropy are effective to measure item relevance with sparse datasets, and the LSI method outperforms PCA with a large dictionary.",
        "link": "https://dl.acm.org/doi/10.1145/3102254.3102288"
    },
    {
        "title": "Interactive Visual Graph Mining and Learning",
        "authors": [
            "Ryan A. Rossi",
            "Nesreen K. Ahmed",
            "Rong Zhou",
            "Hoda Eldardiry"
        ],
        "date": null,
        "source": "ACM Transactions on Intelligent Systems and Technology",
        "abstract": "This article presents a platform for interactive graph mining and relational machine learning called GraphVis. The platform combines interactive visual representations with state-of-the-art graph mining and relational machine learning techniques to aid in revealing important insights quickly as well as learning an appropriate and highly predictive model for a particular task (e.g., classification, link prediction, discovering the roles of nodes, and finding influential nodes). Visual representations and interaction techniques and tools are developed for simple, fast, and intuitive real-time interactive exploration, mining, and modeling of graph data. In particular, we propose techniques for interactive relational learning (e.g., node/link classification), interactive link prediction and weighting, role discovery and community detection, higher-order network analysis (via graphlets, network motifs), among others. GraphVis also allows for the refinement and tuning of graph mining and relational learning methods for specific application domains and constraints via an end-to-end interactive visual analytic pipeline that learns, infers, and provides rapid interactive visualization with immediate feedback at each change/prediction in real-time. Other key aspects include interactive filtering, querying, ranking, manipulating, exporting, as well as tools for dynamic network analysis and visualization, interactive graph generators (including new block model approaches), and a variety of multi-level network analysis techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3200764"
    },
    {
        "title": "Data Mining and Opinion Mining: A Tool in Educational Context",
        "authors": [
            "Myriam Peñafiel",
            "Stefanie Vásquez",
            "Diego Vásquez",
            "Juan Zaldumbide",
            "Sergio Luján-Mora"
        ],
        "date": "July 2018",
        "source": "ICoMS '18: Proceedings of the 2018 1st International Conference on Mathematics and Statistics",
        "abstract": "The use of the web as a universal communication platform generates large volumes of data (Big data), which in many cases, need to be processed so that they can become useful knowledge in face of the sceptics who have doubts about the credibility of such information. The use of web data that comes from educational contexts needs to be addressed, since that large amount of unstructured information is not being valued, losing valuable information that can be used. To solve this problem, we propose the use of data mining techniques such as sentiment analysis to validate the information that comes from the educational platforms. The objective of this research is to propose a methodology that allows the user to apply sentiment analysis in a simple way, because although some researchers have done it, very few do with data in the educational context. The results obtained prove that the proposal can be used in similar cases.",
        "link": "https://dl.acm.org/doi/10.1145/3274250.3274263"
    },
    {
        "title": "Research Hotspots Mining and Visualized Analysis Based on Linking Cluster and K-Core Decomposition",
        "authors": [
            "Ying Cai",
            "Fang Huang",
            "Shuai Wang",
            "Hao Zhang",
            "Chunxiu Du"
        ],
        "date": "May 2018",
        "source": "ICDPA 2018: Proceedings of the International Conference on Data Processing and Applications",
        "abstract": "In this paper, we focus on research hotspots mining with knowledge semantic features on discipline topic word networks, which are extracted from scientific and technical documents. For the topic networks, a novel method of combining linking community clustering with k-core decomposition is proposed to mine the hotspot community with knowledge hierarchy structure. And as a post-processing, the hierarchical community identification is implemented by k-core decomposition in order to discover the internal connection with knowledge semantic hierarchy in the research hotspot. Finally, the community density and visualization method is utilized to analyze the core and sub- problem in the research hotspot community. In experiments, the topic networks for a single discipline and interdisciplinary research hotspot are taken as examples of analysis, and the results show the proposed approach can effectively realize the objective of mining and analyzing the discipline research hotspot on the academic topic word network.",
        "link": "https://dl.acm.org/doi/10.1145/3224207.3224216"
    },
    {
        "title": "The Application of Data Mining Algorithms in the Construction of Travel Recommendation System",
        "authors": [
            "Chao Lou",
            "Zhaonan Mu",
            "Mengzhu Liu"
        ],
        "date": "January 2021",
        "source": "CONF-CDS 2021: The 2nd International Conference on Computing and Data Science",
        "abstract": "Photos shared by users of online social network often associated with tags, texts, geographic and time information. These data are ideal research resource which can be used by researchers to observe people's interests and behaviors. They can also be used to analyze people's travel behaviors and interests because it can reflect people's travel activities and experiences directly. This project aimed to consolidate people into groups based on their travel preferences of travel season, travel time span, scenery type of travel location and then analyze popular travel places for different people groups. The travel preferences were analyzed from extracted data applying PHP language and Naive Bayes classification approach. According to the analyzed data, people were divided into groups. Then, DBSCAN clustering was used to get popular locations based on the different groups. As a result, 80 thousand photo data of 5 cities have been collected from Flickr by Phython. It has been showed that people's preference of season in most of the mined cities was spring and summer and the preference of time span was short. Moreover, mined people has been divided into 16 groups according to their preference and based on the groups popular locations have been calculated by DBSCAN clustering. Theses locations have been provided in a recommendation website which was built in this project. For the future work, mining Flickr data associated with other travel social network sites could be a good way to build a more effective recommendation system.",
        "link": "https://dl.acm.org/doi/10.1145/3448734.3450477"
    },
    {
        "title": "Compare Machine Learning Models in Text Classification Using Steam User Reviews",
        "authors": [
            "Youchen Miao",
            "Zeyu Jin",
            "Yumeng Zhang",
            "Yuchen Chen",
            "Junren Lai"
        ],
        "date": "November 2021",
        "source": "ICSED '21: Proceedings of the 2021 3rd International Conference on Software Engineering and Development",
        "abstract": "Text Classification and Sentiment Analysis of game reviews are viewed as important parts in not only academic fields but also in game studies. In this paper, with more than 400 thousand game reviews on Steam platform, we preprocess the data using different libraries (sklearn, nltk, and spaCy) and use them as inputs to build three sentiment classification models based on different algorithms (Naive Bayes, SVM, and Random Forest). In contrast to previous studies that only focus on different sentiment analysis models, our paper also highlights the use of different APIs to preprocess the data and their corresponding model performance. The results show that no matter which API we choose, Random Forest models always perform the best. However, in terms of training time, Naive Bayes is the fastest. This work can be used to apply grid search for researchers to automatically find the optimum API before conducting sentiment analysis in the future.",
        "link": "https://dl.acm.org/doi/10.1145/3507473.3507480"
    },
    {
        "title": "Lost in Transduction: Transductive Transfer Learning in Text Classification",
        "authors": [
            "Alejandro Moreo",
            "Andrea Esuli",
            "Fabrizio Sebastiani"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Obtaining high-quality labelled data for training a classifier in a new application domain is often costly. Transfer Learning\n(a.k.a. “Inductive Transfer”) tries to alleviate these costs by transferring, to the “target” domain of interest, knowledge available from a different “source” domain. In transfer learning the lack of labelled information from the target domain is compensated by the availability at training time of a set of unlabelled examples from the target distribution. Transductive Transfer Learning denotes the transfer learning setting in which the only set of target documents that we are interested in classifying is known and available at training time. Although this definition is indeed in line with Vapnik’s original definition of “transduction”, current terminology in the field is confused. In this article, we discuss how the term “transduction” has been misused in the transfer learning literature, and propose a clarification consistent with the original characterization of this term given by Vapnik. We go on to observe that the above terminology misuse has brought about misleading experimental comparisons, with inductive transfer learning methods that have been incorrectly compared with transductive transfer learning methods. We then, give empirical evidence that the difference in performance between the inductive version and the transductive version of a transfer learning method can indeed be statistically significant (i.e., that knowing at training time the only data one needs to classify indeed gives an advantage). Our clarification allows a reassessment of the field, and of the relative merits of the major, state-of-the-art algorithms for transfer learning in text classification.",
        "link": "https://dl.acm.org/doi/10.1145/3453146"
    },
    {
        "title": "Machine Learning on Mining Potential Adverse Drug Reactions for Pharmacovigilance",
        "authors": [
            "Ke Jia"
        ],
        "date": "July 2021",
        "source": "DSIT 2021: 2021 4th International Conference on Data Science and Information Technology",
        "abstract": "Drugs are related to the lives and health of patients. Mining and discovering potential adverse drug reactions from a large number of unstructured texts play an important role in drug research and pharmacovigilance. It is a hot research issue in the field of biomedicine in recent years. A large number of scholars have discovered a large amount of information on adverse drug reactions from different data sources through effective machine learning methods. This paper reviews the research progress in recent years to provide a reference for future research.",
        "link": "https://dl.acm.org/doi/10.1145/3478905.3478964"
    },
    {
        "title": "TACAM: Topic And Context Aware Argument Mining",
        "authors": [
            "Michael Fromm",
            "Evgeniy Faerman",
            "Thomas Seidl"
        ],
        "date": "October 2019",
        "source": "WI '19: IEEE/WIC/ACM International Conference on Web Intelligence",
        "abstract": "In this work we address the problem of argument search. The purpose of argument search is the distillation of pro and contra arguments for requested topics from large text corpora. In previous works, the usual approach is to use a standard search engine to extract text parts which are relevant to the given topic and subsequently use an argument recognition algorithm to select arguments from them. The main challenge in the argument recognition task, which is also known as argument mining, is that often sentences containing arguments are structurally similar to purely informative sentences without any stance about the topic. In fact, they only differ semantically. Most approaches use topic or search term information only for the first search step and therefore assume that arguments can be classified independently of a topic. We argue that topic information is crucial for argument mining, since the topic defines the semantic context of an argument. Precisely, we propose different models for the classification of arguments, which take information about a topic of an argument into account. Moreover, to enrich the context of a topic and to let models understand the context of the potential argument better, we integrate information from different external sources such as Knowledge Graphs or pre-trained NLP models. Our evaluation shows that considering topic information, especially in connection with external information, provides a significant performance boost for the argument mining task.",
        "link": "https://dl.acm.org/doi/10.1145/3350546.3352506"
    },
    {
        "title": "Text mining for incoming tasks based on the urgency/importance factors and task classification using machine learning tools",
        "authors": [
            "Yasser Ali Alshehri"
        ],
        "date": "March 2020",
        "source": "ICCDA 2020: Proceedings of the 2020 the 4th International Conference on Compute and Data Analysis",
        "abstract": "In workplaces, there is a massive amount of unstructured data from different sources. In this paper, we present a case study that explains how can through communications between employees, we can help to prioritize tasks requests to increase the efficiency of their works for both technical and non-technical workers. This involves managing daily incoming tasks based on their level of urgency and importance.To allow all workers to utilize the urgency-importance matrix as a time-management tool, we need to automate this tool. The textual content of incoming tasks are analyzed, and metrics related to urgency and importance are extracted. A third factor (i.e., the response variable) is defined based on the two input variables (urgency and importance). Then, machine learning applied to the data to predict the class of incoming tasks based on data outcome desired. We used ordinal regression, neural networks, and decision tree algorithms to predict the four levels of task priority. We measure the performance of all using recalls, precisions, and F-scores. All classifiers perform higher than 89% in terms of all measures.",
        "link": "https://dl.acm.org/doi/10.1145/3388142.3388153"
    },
    {
        "title": "Text mining for malware classification using multivariate all repeated patterns detection",
        "authors": [
            "Konstantinos F. Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2019",
        "source": "ASONAM '19: Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Mobile phones have become nowadays a commodity to the majority of people. Using them, people are able to access the world of Internet and connect with their friends, their colleagues at work or even unknown people with common interests. This proliferation of the mobile devices has also been seen as an opportunity for the cyber criminals to deceive smartphone users and steel their money directly or indirectly, respectively, by accessing their bank accounts through the smartphones or by blackmailing them or selling their private data such as photos, credit card data, etc. to third parties. This is usually achieved by installing malware to smartphones masking their malevolent payload as a legitimate application and advertise it to the users with the hope that mobile users will install it in their devices. Thus, any existing application can easily be modified by integrating a malware and then presented it as a legitimate one. In response to this, scientists have proposed a number of malware detection and classification methods using a variety of techniques. Even though, several of them achieve relatively high precision in malware classification, there is still space for improvement. In this paper, we propose a text mining all repeated pattern detection method which uses the decompiled files of an application in order to classify a suspicious application into one of the known malware families. Based on the experimental results using a real malware dataset, the methodology tries to correctly classify (without any misclassification) all randomly selected malware applications of 3 categories with 3 different families each.",
        "link": "https://dl.acm.org/doi/10.1145/3341161.3350841"
    },
    {
        "title": "Comparative Study between Traditional Machine Learning and Deep Learning Approaches for Text Classification",
        "authors": [
            "Cannannore Nidhi Kamath",
            "Syed Saqib Bukhari",
            "Andreas Dengel"
        ],
        "date": "August 2018",
        "source": "DocEng '18: Proceedings of the ACM Symposium on Document Engineering 2018",
        "abstract": "In this contemporaneous world, it is an obligation for any organization working with documents to end up with the insipid task of classifying truckload of documents, which is the nascent stage of venturing into the realm of information retrieval and data mining. But classification of such humongous documents into multiple classes, calls for a lot of time and labor. Hence a system which could classify these documents with acceptable accuracy would be of an unfathomable help in document engineering. We have created multiple classifiers for document classification and compared their accuracy on raw and processed data. We have garnered data used in a corporate organization as well as publicly available data for comparison. Data is processed by removing the stop-words and stemming is implemented to produce root words. Multiple traditional machine learning techniques like Naive Bayes, Logistic Regression, Support Vector Machine, Random forest Classifier and Multi-Layer Perceptron are used for classification of documents. Classifiers are applied on raw and processed data separately and their accuracy is noted. Along with this, Deep learning technique such as Convolution Neural Network is also used to classify the data and its accuracy is compared with that of traditional machine learning techniques. We are also exploring hierarchical classifiers for classification of classes and subclasses. The system classifies the data faster and with better accuracy than if done manually. The results are discussed in the results and evaluation section.",
        "link": "https://dl.acm.org/doi/10.1145/3209280.3209526"
    },
    {
        "title": "Identification of Overpricing in the Purchase of Medication by the Federal Government of Brazil, Using Text Mining and Clustering Based on Ontology",
        "authors": [
            "Marco Aurelio O. S. Correa",
            "Adriano Galindo Leal"
        ],
        "date": "August 2018",
        "source": "ICCBDC '18: Proceedings of the 2018 2nd International Conference on Cloud and Big Data Computing",
        "abstract": "Increasing the transparency level in his actions and spending is one of the primary duties of the Brazilian Federal Government. The creation of laws that oblige full disclose of all its expenditures through transparency portals enables citizens to supervise all government entities. However, only the dissemination of these data, without a definite standard or the availability of data analysis tools, does not guarantee that the citizen is empowered to play his role. Therefore hence, the objective of this work is to identify overprice in the acquisition of products purchased by the federal government of Brazil using the unstructured data available on the Transparency Portal. The last two-years' worth of purchasing data, available in the Transparency Portal, were extracted, processed and stored. Due to his diverse nature and high volume of data, this study focused only on medicines purchased by the Ministry of Health. Ontology-based text mining and clustering techniques were applied for automatic identification and classification of products. The processing of this information was done through text mining and clustering, based on the ontology registered in another database of the Brazilian government. Because of this work, a consolidated price base per medication was created to allow the identification of distortions in prices practised, facilitating the identification of cases that merit further investigation to unravel fraud to the treasury.",
        "link": "https://dl.acm.org/doi/10.1145/3264560.3264569"
    },
    {
        "title": "Computational Estimation by Scientific Data Mining with Classical Methods to Automate Learning Strategies of Scientists",
        "authors": [
            "Aparna S. Varde"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Experimental results are often plotted as 2-dimensional graphical plots (aka graphs) in scientific domains depicting dependent versus independent variables to aid visual analysis of processes. Repeatedly performing laboratory experiments consumes significant time and resources, motivating the need for computational estimation. The goals are to estimate the graph obtained in an experiment given its input conditions, and to estimate the conditions that would lead to a desired graph. Existing estimation approaches often do not meet accuracy and efficiency needs of targeted applications. We develop a computational estimation approach called AutoDomainMine that integrates clustering and classification over complex scientific data in a framework so as to automate classical learning methods of scientists. Knowledge discovered thereby from a database of existing experiments serves as the basis for estimation. Challenges include preserving domain semantics in clustering, finding matching strategies in classification, striking a good balance between elaboration and conciseness while displaying estimation results based on needs of targeted users, and deriving objective measures to capture subjective user interests. These and other challenges are addressed in this work. The AutoDomainMine approach is used to build a computational estimation system, rigorously evaluated with real data in Materials Science. Our evaluation confirms that AutoDomainMine provides desired accuracy and efficiency in computational estimation. It is extendable to other science and engineering domains as proved by adaptation of its sub-processes within fields such as Bioinformatics and Nanotechnology.",
        "link": "https://dl.acm.org/doi/10.1145/3502736"
    },
    {
        "title": "Where is the road for issue reports classification based on text mining?",
        "authors": [
            "Qiang Fan",
            "Yue Yu",
            "Gang Yin",
            "Tao Wang",
            "Huaimin Wang"
        ],
        "date": "November 2017",
        "source": "ESEM '17: Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Currently, open source projects receive various kinds of issues daily, because of the extreme openness of Issue Tracking System (ITS) in GitHub. ITS is a labor-intensive and time-consuming task of issue categorization for project managers. However, a contributor is only required a short textual abstract to report an issue in GitHub. Thus, most traditional classification approaches based on detailed and structured data (e.g., priority, severity, software version and so on) are difficult to adopt. In this paper, issue classification approaches on a large-scale dataset, including 80 popular projects and over 252,000 issue reports collected from GitHub, were investigated. First, four traditional text-based classification methods and their performances were discussed. Semantic perplexity (i.e., an issues description confuses bug-related sentences with nonbug-related sentences) is a crucial factor that affects the classification performances based on quantitative and qualitative study. Finally, A two-stage classifier framework based on the novel metrics of semantic perplexity of issue reports was designed. Results show that our two-stage classification can significantly improve issue classification performances.",
        "link": "https://dl.acm.org/doi/10.1109/ESEM.2017.19"
    },
    {
        "title": "Feature-based Facebook reviews process model for e-management using data mining",
        "authors": [
            "Anish Kumar Varudharajulu",
            "Yongsheng Ma"
        ],
        "date": "January 2019",
        "source": "IC4E '19: Proceedings of the 10th International Conference on E-Education, E-Business, E-Management and E-Learning",
        "abstract": "The data generated from online communication acts as potential gold mines for discovering knowledge for researchers. A large amount of data is also generated in the form of web documents, emails, blogs, and feedback, etc. Text analytics is being significantly employed to mine important information. Opinion mining is the process of extracting human thoughts and perceptions from unstructured texts. The showstopper for designing an opinion mining system for analyzing reviews arise from the fact that customer reviews are often noisy. These reviews are informally written. In addition, they are subjected to spelling mistakes, grammatical errors, improper punctuation and irrational capitalization. This paper focuses on analyzing the different classification and clustering algorithms aimed at extracting and consolidating opinions of customers from social media sites like Facebook, Twitter and through surveys, at multiple levels of granularity to monitor and measure customer satisfaction. Ours is an automated approach, in which the system aids in the process of knowledge assimilation for knowledge-based building and also performs the analytics. Domain experts ratify the knowledge base and also provide training datasets for the system to intuitively gather more instances for ratification. The system identifies opinion expressions as phrases containing opinion words, opinionated features and also opinion modifiers. These expressions are categorized as positive, negative or neutral. Opinion expressions are identified and categorized using localized linguistic techniques. Opinions can be congregated at any desired level of specificity i.e. feature level or product level, user level or service level, etc. We have developed a system based on this approach, which provides the user with a platform to analyze opinion expressions crawled from a set of pre-defined datasets.",
        "link": "https://dl.acm.org/doi/10.1145/3306500.3306514"
    },
    {
        "title": "Discriminative Topic Mining via Category-Name Guided Text Embedding",
        "authors": [
            "Yu Meng",
            "Jiaxin Huang",
            "Guangyuan Wang",
            "Zihan Wang",
            "Chao Zhang",
            "Yu Zhang",
            "Jiawei Han"
        ],
        "date": "April 2020",
        "source": "WWW '20: Proceedings of The Web Conference 2020",
        "abstract": "Mining a set of meaningful and distinctive topics automatically from massive text corpora has broad applications. Existing topic models, however, typically work in a purely unsupervised way, which often generate topics that do not fit users’ particular needs and yield suboptimal performance on downstream tasks. We propose a new task, discriminative topic mining, which leverages a set of user-provided category names to mine discriminative topics from text corpora. This new task not only helps a user understand clearly and distinctively the topics he/she is most interested in, but also benefits directly keyword-driven classification tasks. We develop CatE, a novel category-name guided text embedding method for discriminative topic mining, which effectively leverages minimal user guidance to learn a discriminative embedding space and discover category representative terms in an iterative manner. We conduct a comprehensive set of experiments to show that CatE mines high-quality set of topics guided by category names only, and benefits a variety of downstream applications including weakly-supervised classification and lexical entailment direction identification.",
        "link": "https://dl.acm.org/doi/10.1145/3366423.3380278"
    },
    {
        "title": "Comparative Study of Heart Disease Diagnosis Using Top Ten Data Mining Classification Algorithms",
        "authors": [
            "I. Ketut Agung Enriko"
        ],
        "date": "June 2019",
        "source": "ICFET '19: Proceedings of the 5th International Conference on Frontiers of Educational Technologies",
        "abstract": "Data mining has been used for many purposes, especially for prediction system. In healthcare, data mining algorithms often used in disease diagnosis. Meanwhile, heart disease is known as a primary cause of death over the years. Many studies have been performed in heart disease diagnosis using data mining methods. There are some popular data mining algorithms that can be used in heart disease diagnosis, for example, k-Nearest Neighbor, CART, and AdaBoost. The algorithms are used to analyze a sample of cardiovascular patients data and predict the heart disease type that they suffer. Some parameters are taken from the patient, including EKG morphology, blood pressure, and information about the existence of chest pain, shortness of breath, palpitation, and cold sweat. In this study, medical records data are collected from Harapan Kita Hospital and utilized as a dataset sample in this research. Top ten data mining classification algorithms are used in diagnosing heart disease from Harapan Kita Hospital data and examining their performance by checking the accuracy and speed.",
        "link": "https://dl.acm.org/doi/10.1145/3338188.3338220"
    },
    {
        "title": "Text Mining Approach for Identifying Research Trends",
        "authors": [
            "Snezhana Sulova"
        ],
        "date": "June 2021",
        "source": "CompSysTech '21: Proceedings of the 22nd International Conference on Computer Systems and Technologies",
        "abstract": "With the increase of unstructured data, the issues connected with automatic text processing, the categorization of documents and the discovery of topics have become objects of growing interest. In order to improve the process of grouping and processing research publications, we would like to propose a method based upon natural language processing. It is based on text mining technologies which aim to identify key tendencies in documents. It processes the content of publications by clustering and identifies the topics of each identified group. This analysis helps by identifying key tendencies as well as discovering emerging new areas of research. Publications from the research literature database, Scopus, were used to test the approach. The topic of the publications is “the application of digital technologies in the logistics business”. The experiments were completed using the RapidMiner Studio software.",
        "link": "https://dl.acm.org/doi/10.1145/3472410.3472433"
    },
    {
        "title": "Assessment of Vulnerability Severity using Text Mining",
        "authors": [
            "Georgios Spanos",
            "Lefteris Angelis",
            "Dimitrios Toloudis"
        ],
        "date": "September 2017",
        "source": "PCI '17: Proceedings of the 21st Pan-Hellenic Conference on Informatics",
        "abstract": "Software1 vulnerabilities are closely associated with information systems security, a major and critical field in today's technology. Vulnerabilities constitute a constant and increasing threat for various aspects of everyday life, especially for safety and economy, since the social impact from the problems that they cause is complicated and often unpredictable. Although there is an entire research branch in software engineering that deals with the identification and elimination of vulnerabilities, the growing complexity of software products and the variability of software production procedures are factors contributing to the ongoing occurrence of vulnerabilities, Hence, another area that is being developed in parallel focuses on the study and management of the vulnerabilities that have already been reported and registered in databases. The information contained in such databases includes, a textual description and a number of metrics related to vulnerabilities. The purpose of this paper is to investigate to what extend the assessment of the vulnerability severity can be inferred directly from the corresponding textual description, or in other words, to examine the informative power of the description with respect to the vulnerability severity. For this purpose, text mining techniques, i.e. text analysis and three different classification methods (decision trees, neural networks and support vector machines) were employed. The application of text mining to a sample of 70,678 vulnerabilities from a public data source shows that the description itself is a reliable and highly accurate source of information for vulnerability prioritization.",
        "link": "https://dl.acm.org/doi/10.1145/3139367.3139390"
    },
    {
        "title": "Clustering Algorithms for Spatial Data Mining",
        "authors": [
            "Chetashri Bhadane",
            "Ketan Shah"
        ],
        "date": "April 2020",
        "source": "ICGDA '20: Proceedings of the 2020 3rd International Conference on Geoinformatics and Data Analysis",
        "abstract": "With the advances in mobile and wireless technologies, there has been a rise in applications that track and share the users' geospatial data. People use several social networking sites such as Twitter, Facebook and Flickr, where they share their status updates. With the integration of Global Positioning System (GPS) with mobile phones, it is now possible to share one's locations on these social networks. GPS allows us to record and track a person's movement along with the timestamp. The data set obtained from these GPS logs is vast and is widely used to analyze the users' movement patterns. Specifically, we can find out significant locations based on the number of users present at that location and the time spent by them at such places. Once significant places have been identified, it is also possible to identify the semantic importance of these locations. This paper presents an overview of the clustering techniques used to find important places of interest using large GPS based mobility datasets. Four clustering algorithms, K-Means, DBSCAN, OPTICS and Hierarchical, are implemented, and performance is tested using real-time data of 50 users collected over 2--5 years. Performance summary depicts that K-Means and DBSCAN perform well for spatial data.",
        "link": "https://dl.acm.org/doi/10.1145/3397056.3397068"
    },
    {
        "title": "Feature Selection based Arabic Text Classification using Different Machine Learning Algorithms: Comparative Study",
        "authors": [
            "Sakina Rim Bennabi",
            "Zakaria Elberrichi"
        ],
        "date": "June 2020",
        "source": "ICIST '20: Proceedings of the 10th International Conference on Information Systems and Technologies",
        "abstract": "Feature selection is a method of data pre-processing widely used when mining large data, such as textual classification. Several studies have been conducted to compare the different methods of feature selection applied to corpora in English. Unfortunately, a small number of works concern the Arabic language. This article aims to present a comparative study of different feature selection techniques including: Chi2, the ANOVA method and mutual information, applied on a corpus in Arabic language, while also diversifying the machine learning algorithms (Naive Bayes, SVM and KNN). This experimental study has shown in general that reducing dimensionality with feature selection techniques has slightly affected the performance of textual classification, reducing the size of the corpus by up to 1%.",
        "link": "https://dl.acm.org/doi/10.1145/3447568.3448531"
    },
    {
        "title": "Intelligent Data Analysis using Optimized Support Vector Machine Based Data Mining Approach for Tourism Industry",
        "authors": [
            "Ms Promila Sharma",
            "Uma Meena",
            "Girish Kumar Sharma"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Data analysis involves the deployment of sophisticated approaches from data mining methods, information theory, and artificial intelligence in various fields like tourism, hospitality, and so on for the extraction of knowledge from the gathered and preprocessed data. In tourism, pattern analysis or data analysis using classification is significant for finding the patterns that represent new and potentially useful information or knowledge about the destination and other data. Several data mining techniques are introduced for the classification of data or patterns. However, overfitting, less accuracy, local minima, sensitive to noise are the drawbacks in some existing data mining classification methods. To overcome these challenges, Support vector machine with Red deer optimization (SVM-RDO) based data mining strategy is proposed in this article. Extended Kalman filter (EKF) is utilized in the first phase, i.e., data cleaning to remove the noise and missing values from the input data. Mantaray foraging algorithm (MaFA) is used in the data selection phase, in which the significant data are selected for the further process to reduce the computational complexity. The final phase is the classification, in which SVM-RDO is proposed to access the useful pattern from the selected data. PYTHON is the implementation tool used for the experiment of the proposed model. The experimental analysis is done to show the efficacy of the proposed work. From the experimental results, the proposed SVM-RDO achieved better accuracy, precision, recall, and F1 score than the existing methods for the tourism dataset. Thus, it is showed the effectiveness of the proposed SVM-RDO for pattern analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3494566"
    },
    {
        "title": "Applying text mining to predict learners' cognitive engagement",
        "authors": [
            "Hayati Hind",
            "Mohammed Khalidi Idrissi",
            "Samir Bennani"
        ],
        "date": "October 2017",
        "source": "SCAMS '17: Proceedings of the Mediterranean Symposium on Smart City Application",
        "abstract": "In academic process, engagement represent one of the important success' factor. This paper proposes a predictive system for learners' cognitive engagement based on their online discussion forums participation. In the first level the ontology OWL and the LSA method are used to perform a semantic classification system of the threads according to a specific context chosen by the tutor. Then the Text mining, as a predictive method, is applied to the classified threads and learners' participation in the forums.",
        "link": "https://dl.acm.org/doi/10.1145/3175628.3175655"
    },
    {
        "title": "Investigate Transitions into Drug Addiction through Text Mining of Reddit Data",
        "authors": [
            "John Lu",
            "Sumati Sridhar",
            "Ritika Pandey",
            "Mohammad Al Hasan",
            "Georege Mohler"
        ],
        "date": "July 2019",
        "source": "KDD '19: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Increasing rates of opioid drug abuse and heightened prevalence of online support communities underscore the necessity of employing data mining techniques to better understand drug addiction using these rapidly developing online resources. In this work, we obtained data from Reddit, an online collection of forums, to gather insight into drug use/misuse using text snippets from users narratives. Specifically, using users' posts, we trained a binary classifier which predicts a user's transitions from casual drug discussion forums to drug recovery forums. We also proposed a Cox regression model that outputs likelihoods of such transitions. In doing so, we found that utterances of select drugs and certain linguistic features contained in one's posts can help predict these transitions. Using unfiltered drug-related posts, our research delineates drugs that are associated with higher rates of transitions from recreational drug discussion to support/recovery discussion, offers insight into modern drug culture, and provides tools with potential applications in combating the opioid crisis.",
        "link": "https://dl.acm.org/doi/10.1145/3292500.3330737"
    },
    {
        "title": "Analysing digital banking reviews using text mining",
        "authors": [
            "Li Chen Cheng",
            "Legaspi Rhea Sharmayne"
        ],
        "date": "December 2020",
        "source": "ASONAM '20: Proceedings of the 12th IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Digital banks are new entrants in the banking industry in the Philippines as they only started late 2018. Since then, a handful of players have and are still emerging. With more and more people becoming technologically savvy, it is very critical for financial institutions to develop a digital banking application that will stand out from the competition. This paper aims to use text mining methods to analyse digital banking application reviews. This study will perform topic modelling using LDA to explore customer concerns and will mine association rules between the digital banking features with the review score. The results will reveal which areas the digital banking application can further optimize for customer satisfaction and retention.",
        "link": "https://dl.acm.org/doi/10.1109/ASONAM49781.2020.9381429"
    },
    {
        "title": "Mining Moocs Videos Metadata Using Classification Techniques",
        "authors": [
            "El Harrak Othman",
            "Ghadi Abderrahim",
            "El Bouhdidi Jaber"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "In few years, the internet has become the first source of information for most people, Today MOOCs makes it possible for everyone to access to the education over the world, it's represents an emerging methodology of online teaching and an important development in open education. Due to the rapid development of Moocs and the rapid growth of digital data and video database over the Internet, it is becoming very difficult for learners to browsing and choosing the best training for them. The scientific community has increased the amount of research into new technologies, with a view to improve the digital video utilization: its archiving, indexing, accessibility, acquisition, store and even its process and usability. All these parts of the video utilization entail the necessity of the extraction of all important information of a video, especially in cases of lack of metadata information. Web video mining is retrieving the content using data mining techniques from World Wide Web. There are two approaches for web video mining using traditional image processing (signal processing) and metadata based approach. In this paper, we present the various research makes in this subject and we propose an effective methodology to extract the metadata from Moocs videos and classify them based on the extracted metadata by applying data mining techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090450"
    },
    {
        "title": "Text mining with HathiTrust",
        "authors": [
            "Eleanor Dickson Koehl",
            "Ryan Dubnicek"
        ],
        "date": "June 2019",
        "source": "JCDL '19: Proceedings of the 18th Joint Conference on Digital Libraries",
        "abstract": "This tutorial will introduce attendees to the HathiTrust Research Center and its tools and services for computational text analysis research. HTRC leverages the scope and scale of the HathiTrust Digital Library collection to create opportunities for researchers to perform text data mining on subsets of the corpus. Attendees will develop skills that will allow them to conduct basic text analysis research using HathiTrust data, learn techniques for engaging with scholars on their text data mining research, and explore strategies for computational access to large-scale digital collections.",
        "link": "https://dl.acm.org/doi/10.1109/JCDL.2019.00115"
    },
    {
        "title": "Text mining for plagiarism detection: multivariate pattern detection for recognition of text similarities",
        "authors": [
            "Konstantinos Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2018",
        "source": "ASONAM '18: Proceedings of the 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "The problem of plagiarism the recent years has been intensified by the availability of information in digital form and the accessibility of the electronic libraries through the Internet. As a result, plagiarism detection has been transformed into a big data analytics problem since the number of digital sources is extravagant and a new document needs to be compared with millions of other existing documents. In this paper, a text mining methodology is proposed that can detect all common patterns between a document and the documents in a reference database. The technique is based on a pattern detection algorithm and the corresponding data structure that enables the algorithm to detect all common patterns. The methodology has been applied in a well-defined dataset providing very promising results identifying difficult cases of plagiarism such as technical disguise.",
        "link": "https://dl.acm.org/doi/10.5555/3382225.3382424"
    },
    {
        "title": "Petroleum Engineering Data Text Classification Using Convolutional Neural Network Based Classifier",
        "authors": [
            "Tzuhan Hsu",
            "Yaoqin Zhang"
        ],
        "date": "May 2018",
        "source": "ICMLT '18: Proceedings of the 2018 International Conference on Machine Learning Technologies",
        "abstract": "In this paper, authors introduce a convolutional neural network (CNN) based classification approach for petroleum engineering event. Recently, text classification problem has raised substantial attention in the field of machine learning. The aim of text classification process is to assign correct label to a text on the basis of its content. However, due to the imprecise of feature representation, most of former text classification methods cannot export satisfying result. To solve this problem, authors propose a model based on convolutional neural network. Authors conduct experiments with real world petroleum engineering data text, artificially labelled into 6 categories. Beside purposed CNN-based model, the authors also implement other baseline machine learning approaches: k nearest neighbor, random forest, support vector machines, recurrent neural network, gated recurrent units, long short-term memory. Among all these models, our CNN-based model reach best performance in text classification task. In the last part of this paper, authors compare and analysis their purpose and baseline model.",
        "link": "https://dl.acm.org/doi/10.1145/3231884.3231898"
    },
    {
        "title": "Classifying Indonesian Online Articles as Advertisement Placement Base Using Text Mining",
        "authors": [
            "Nadhira Tasya",
            "Arian Dhini"
        ],
        "date": "July 2017",
        "source": "ICBIM 2017: Proceedings of the International Conference on Business and Information Management",
        "abstract": "Rapid development in technological aspect resulting in growing level of human needs for the latest news, so that emerged a new trend of publishing and accessing news through online media or usually called online journalism. In addition, the number of people who sell and purchase through online sites also continues to increase and this opportunity is utilized by the company and the advertiser by implementing targeted web advertising. However, the high number of articles that have been published and accessed leads to great opportunities for errors in determining where to place the ads. Therefore, it needs a system that can categorize articles accessed by users as the basis of advertisement placement by the company and this classification system can be done by applying the method of Data Mining and Text Mining. This research uses document data in the form of article content that will be categorized into twenty categories of class of advertisement by using Text Mining technique with Support Vector Machine algorithm. The results of this study may be used by companies or advertisers as a basis for placement of ads on selected online media sites.",
        "link": "https://dl.acm.org/doi/10.1145/3134271.3134288"
    },
    {
        "title": "Mining Free-Text Medical Notes for Suicide Risk Assessment",
        "authors": [
            "Marios Adamou",
            "Grigoris Antoniou",
            "Elissavet Greasidou",
            "Vincenzo Lagani",
            "Paulos Charonyktakis",
            "Ioannis Tsamardinos"
        ],
        "date": "July 2018",
        "source": "SETN '18: Proceedings of the 10th Hellenic Conference on Artificial Intelligence",
        "abstract": "Suicide has been considered as an important public health issue for a very long time, and is one of the main causes of death worldwide. Despite suicide prevention strategies being applied, the rate of suicide has not changed substantially over the past decades. Advances in machine learning make it possible to attempt to predict suicide based on the analysis of relevant data to inform clinical practice. This paper reports on findings from the analysis of data of patients who died by suicide in the period 2013-2016 and made use of both structured data and free-text medical notes. We focus on examining various text-mining approaches to support risk assessment. The results show that using advance machine learning and text-mining techniques, it is possible to predict within a specified period which people are most at risk of taking their own life at the time of referral to a mental health service.",
        "link": "https://dl.acm.org/doi/10.1145/3200947.3201020"
    },
    {
        "title": "Integration of data mining techniques in e-learning systems: Clustering Profil of Lerners and Recommender Course System",
        "authors": [
            "A. El Moustamid",
            "E. En-Naimi",
            "J. El Bouhdidi"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "Data mining aims at the extraction of knowledge from large quantities of data by automatic or semi-automatic methods. In the field of education and training e-learning, Data Mining techniques as clustering (The aim of this technique is to identify groups that are similar in some aspect) [21], prediction (is used for developing model where combination of data viewed from different aspects can effect single viewed data. Here, is important to watch how predictors variables are effecting predicted variable)[21], classification are applied in e-learning to improve the level of learners by providing learning according to the learners' requirement) are used to analyze learners profile, to predict and improve student performance. The aim of our project is to develop a system capable of analyzing learners profile and indexing web videos in order to offer learners a rich database with courses that correspond to their levels.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090453"
    },
    {
        "title": "Multi-level mining and visualization of scientific text collections: Exploring a bi-lingual scientific repository",
        "authors": [
            "Pablo Accuosto",
            "Francesco Ronzano",
            "Daniel Ferrés",
            "Horacio Saggion"
        ],
        "date": "December 2017",
        "source": "WOSP 2017: Proceedings of the 6th International Workshop on Mining Scientific Publications",
        "abstract": "We present a system to mine and visualize collections of scientific documents by semantically browsing information extracted from single publications or aggregated throughout corpora of articles. The text mining tool performs deep analysis of document collections allowing the extraction and interpretation of research paper's contents. In addition to the extraction and enrichment of documents with metadata (titles, authors, affiliations, etc), the deep analysis performed comprises semantic interpretation, rhetorical analysis of sentences, triple-based information extraction, and text summarization. The visualization components allow geographical-based exploration of collections, topic-evolution interpretation, and collaborative network analysis among others. The paper presents a case study of a bi-lingual collection in the field of Natural Language Processing (NLP).",
        "link": "https://dl.acm.org/doi/10.1145/3127526.3127529"
    },
    {
        "title": "Hierarchical Topic Mining via Joint Spherical Tree and Text Embedding",
        "authors": [
            "Yu Meng",
            "Yunyi Zhang",
            "Jiaxin Huang",
            "Yu Zhang",
            "Chao Zhang",
            "Jiawei Han"
        ],
        "date": "August 2020",
        "source": "KDD '20: Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Mining a set of meaningful topics organized into a hierarchy is intuitively appealing since topic correlations are ubiquitous in massive text corpora. To account for potential hierarchical topic structures, hierarchical topic models generalize flat topic models by incorporating latent topic hierarchies into their generative modeling process. However, due to their purely unsupervised nature, the learned topic hierarchy often deviates from users' particular needs or interests. To guide the hierarchical topic discovery process with minimal user supervision, we propose a new task, Hierarchical Topic Mining, which takes a category tree described by category names only, and aims to mine a set of representative terms for each category from a text corpus to help a user comprehend his/her interested topics. We develop a novel joint tree and text embedding method along with a principled optimization procedure that allows simultaneous modeling of the category tree structure and the corpus generative process in the spherical space for effective category-representative term discovery. Our comprehensive experiments show that our model, named JoSH, mines a high-quality set of hierarchical topics with high efficiency and benefits weakly-supervised hierarchical text classification tasks.",
        "link": "https://dl.acm.org/doi/10.1145/3394486.3403242"
    },
    {
        "title": "Unsupervised behavioural mining and clustering for malware family identification",
        "authors": [
            "Khanh Huu The Dam",
            "Thomas Given-Wilson",
            "Axel Legay"
        ],
        "date": "March 2021",
        "source": "SAC '21: Proceedings of the 36th Annual ACM Symposium on Applied Computing",
        "abstract": "More accurate and advanced detection and classification of malware requires exploiting program behaviour and not merely syntactic or static features. One approach is to use system call dependency graphs (SCDGs) that represent the program behaviour by interactions with the system, and the relations between these interactions. These SCDGs have been used with supervised learning techniques to very accurately detect and classify malware. This works considers the unsupervised learning challenge of mining for common clusters of behaviour without a priori knowledge. This allows for clustering of similar programs by behaviour, that can then be used for either classification or further analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3412841.3441919"
    },
    {
        "title": "Text Mining for Evaluating Authors' Birth and Death Years",
        "authors": [
            "Dror Moghaz",
            "Yaakov Hacohen-Kerner",
            "Dov Gabbay"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "This article presents a unique method in text and data mining for finding the era, i.e., mining temporal data, in which an anonymous author was living. Finding this era can assist in the examination of a fake document or extracting the time period in which a writer lived. The study and the experiments concern Hebrew, and in some parts, Aramaic and Yiddish rabbinic texts. The rabbinic texts are undated and contain no bibliographic sections, posing an interesting challenge. This work proposes algorithms using key phrases and key words that allow the temporal organization of citations together with linguistic patterns. Based on these key phrases, key words, and the references, we established several types of “Iron-clad,” Heuristic and Greedy rules for estimating the years of birth and death of a writer in an interesting classification task. Experiments were conducted on corpora, including documents authored by 12, 24, and 36 rabbinic writers and demonstrated promising results.",
        "link": "https://dl.acm.org/doi/10.1145/3281631"
    },
    {
        "title": "Transfer Learning to Timed Text Based Video Classification Using CNN",
        "authors": [
            "Zenun Kastrati",
            "Ali Shariq Imran",
            "Arianit Kurti"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "Open educational video resources are gaining popularity with a growing number of massive open online courses (MOOCs). This has created a niche for content providers to adopt effective solutions in automatically organizing and structuring of educational resources for maximum visibility. Recent advances in deep learning techniques are proving useful in managing and classifying resources into appropriate categories. This paper proposes one such convolutional neural network (CNN) model for classifying video lectures in a MOOC setting using a transfer learning approach. The model uses a time-aligned text transcripts corresponding to video lectures from six broader subject categories. Video lectures and their corresponding transcript dataset is gathered from the Coursera MOOC platform. Two different CNN models are proposed: i) CNN based classification using embeddings learned from our MOOC dataset, ii) CNN based classification using transfer learning. Word embeddings generated from two well known state-of-the-art pre-trained models Word2Vec and GloVe, are used in the transfer learning approach for the second case. The proposed CNN models are evaluated using precision, recall, and F1 score and the obtained performance is compared with both conventional and deep learning classifiers. The proposed CNN models have an F1 score improvement of 10-22 percentage points over DNN and conventional classifiers",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326483"
    },
    {
        "title": "Evolutionary role mining in complex networks by ensemble clustering",
        "authors": [
            "Sarvenaz Choobdar",
            "Pedro Ribeiro",
            "Fernando Silva"
        ],
        "date": "April 2017",
        "source": "SAC '17: Proceedings of the Symposium on Applied Computing",
        "abstract": "The structural patterns in the neighborhood of nodes assign unique roles to the nodes. Mining the set of existing roles in a network provides a descriptive profile of the network and draws its general picture. This paper proposes a new method to determine structural roles in a dynamic network based on the current position of nodes and their historic behavior. We develop a temporal ensemble clustering technique to dynamically find groups of nodes, holding similar tempo-structural roles. We compare two weighting functions, based on age and distribution of data, to incorporate temporal behavior of nodes in the role discovery. To evaluate the performance of the proposed method, we assess the results from two points of view: 1) goodness of fit to current structure of the network; 2) consistency with historic data. We conduct the evaluation using different ensemble clustering techniques. The results on real world networks demonstrate that our method can detect tempo-structural roles that simultaneously depict the topology of a network and reflect its dynamics with high accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3019612.3019815"
    },
    {
        "title": "Analyzing used-car web listings via text mining",
        "authors": [
            "Ayhan Demiriz",
            "Fatma Cantaş"
        ],
        "date": "October 2017",
        "source": "IML '17: Proceedings of the 1st International Conference on Internet of Things and Machine Learning",
        "abstract": "Used car trade is one of the major components of the world economies. It is not uncommon to sell a car by placing an internet advertisement irrespective of the geography in these days. A typical content of an advertisement is usually composed of two parts namely the structured and the free text data. The structured data may include some information about the asking price, make, model, year, mileage of the car and the contact info. In most cases, seller may give important clues about the car's current conditions in the free text data where the title (head) of the advertisement can be included as free text too. This paper reports preliminary results from a text mining study conducted on 75K used car internet listings collected from two major car listing web sites in Turkey. As expected, the words and the phrases related to the description of the car are observed to be frequent. The leading concepts in the free text are found to be regarding how to describe the current condition of a car, for example \"no crash history\".",
        "link": "https://dl.acm.org/doi/10.1145/3109761.3109782"
    },
    {
        "title": "Text classification of Diseases Treated by Traditional Chinese Medicine Prescription based on machine learning",
        "authors": [
            "Hanqing Zhao",
            "Bo Han",
            "Chen Li"
        ],
        "date": "September 2020",
        "source": "ISAIMS '20: Proceedings of the 1st International Symposium on Artificial Intelligence in Medical Sciences",
        "abstract": "OBJECTIVE: To explore the application of machine learning in th e identification of diseases treated by traditional Chinese medicine prescriptions. METHODS: Based on the composition of the text document of Chinese medicine prescriptions, the prescriptions were divided into cough, headache and diarrhea. THUTCT were introduced to establish and train two machine learning text classification models, LibSVM and LibLinear, and the prescriptions to be deter mined were put into the model for classification and prediction. R ESULTS: The Precision rate of LibSVM model and Liblinear mod el were 0.7283 and 0.6690. Seven prescriptions were classified an d predicted, and the results were in line with expectations. CONCLUSION: THUCTC has good universality for the content of TCM prescriptions, high classification accuracy and fast testing speed, which is suitable for the text classification and discrimination research of TCM prescriptions for diseases.",
        "link": "https://dl.acm.org/doi/10.1145/3429889.3429895"
    },
    {
        "title": "Evaluation of retweet clustering method classification method using retweets on Twitter without text data",
        "authors": [
            "K. Uchida",
            "F. Toriumi",
            "T. Sakaki"
        ],
        "date": "August 2017",
        "source": "WI '17: Proceedings of the International Conference on Web Intelligence",
        "abstract": "Burst phenomena, which frequently occur on social media, are caused by such social events as flaming on the internet, elections, and natural disasters. To understand people's thoughts and feelings, we must classify their opinions from burst phenomena. Therefore, classification methods that categorize tweets are critical. However, since most classification methods focus on text mining, they cannot group tweets by topics because each tweet has poor linguistic similarities. We used a non-text-based classification method proposed by Baba et al. that groups tweets by topics, even if they have poor linguistic similarities, and verified its validity by comparing it with a text-based classification method in two different evaluations: qualitative and quantitative. In the qualitative evaluation part, we did a questionnaire survey and validated the suitability of the topic clusters created using both the non-and text-based methods. Since evaluating the similarity of every pair of tweets in each topic is difficult, we evaluated the similarity between sampled pairs in the survey and acquired more appropriate topic clustering results using the non-text-based method than the text-based method. In the quantitative evaluation part, we focused on the robustness of each method against data reduction. Many approaches analyze social media data, especially because collecting data from social media is comparatively easy. However, since collecting the whole data of burst phenomena is very costly due to the vast amounts of available social media data, robustness against data reduction is an important index to evaluate classification methods. With the non-text-based method, over 55% of the pairs of tweets in the same cluster were also included in the same cluster even when the data were reduced to 10% in all three of our example cases. In this paper, as a source we focus on Twitter, one of the most popular microblogging services. Using clustering to conduct detailed case analyses, we scrutinized three burst cases that include natural disasters and flaming on the internet and found that a non-text-based method more effectively classified tweets in burst phenomena than a text-based method.",
        "link": "https://dl.acm.org/doi/10.1145/3106426.3106451"
    },
    {
        "title": "Using Spark for Text Mining on Large Scale Liver Cancer Literature",
        "authors": [
            "Ming-Yen Lin",
            "Yu-Ju Lin",
            "Sue-Chen Hsueh"
        ],
        "date": "January 2021",
        "source": "BDET 2021: 2021 the 3rd International Conference on Big Data Engineering and Technology (BDET)",
        "abstract": "Cancer is one of the main causes of death. The number of known cancer to-date is more than one hundred. Liver cancer, ranked after trachea and lung cancer, has long been high in the cancer leading cause in Taiwan and even ranked as second in 2016. The number of scientific articles related to cancer proliferates every year. The number reaches as high as 20 million in PubMed so that discovering useful information from the massive collection is very difficult. In addition, using a single machine to sift through these articles is very time-consuming. Therefore, we present a big data analytic framework using the distributed Apache Spark platform for text mining in PubMed literature. We establish a prediction model for liver cancer articles so that researchers may effectively validate whether an article is related to liver cancer or not. Classification models in Spark MLlib including Linear Support Vector Machines (SVM), Logistic Regression, are used in our experiments. Relevancy to liver cancer is further confirmed by using MeSH (Medical Subject Headings) terms. Logistic regression is about 3 times faster than SVMs and the accuracy of both methods is close to 95% in the experiments using hold-out validation. When max_features is 500 and min_df ≤ 0.1 (or min_df = 1), the accuracy may reach 96%. In the experiments with K-fold cross-validation, the accuracy of SVMs methods is 96%. The experimental results show that that our prediction model may effectively classify liver cancer articles.",
        "link": "https://dl.acm.org/doi/10.1145/3474944.3474958"
    },
    {
        "title": "Automatic topic classification of test cases using text mining at an Android smartphone vendor",
        "authors": [
            "Junji Shimagaki",
            "Yasutaka Kamei",
            "Naoyasu Ubayashi",
            "Abram Hindle"
        ],
        "date": "October 2018",
        "source": "ESEM '18: Proceedings of the 12th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Background: An Android smartphone is an ecosystem of applications, drivers, operating system components, and assets. The volume of the software is large and the number of test cases needed to cover the functionality of an Android system is substantial. Enormous effort has been already taken to properly quantify \"what features and apps were tested and verified?\". This insight is provided by dashboards that summarize test coverage and results per feature. One method to achieve this is to manually tag or label test cases with the topic or function they cover, much like function points. At the studied Android smartphone vendor, tests are labelled with manually defined tags, so-called \"feature labels (FLs)\", and the FLs serve to categorize 100s to 1000s test cases into 10 to 50 groups. Aim: Unfortunately for developers, manual assignment of FLs to 1000s of test cases is a time consuming task, leading to inaccurately labeled test cases, which will render the dashboard useless. We created an automated system that suggests tags/labels to the developers for their test cases rather than manual labeling. Method: We use machine learning models to predict and label the functionality tested by 10,000 test cases developed at the company. Results: Through the quantitative experiments, our models achieved acceptable F-1 performance of 0.3 to 0.88. Also through the qualitative studies with expert teams, we showed that the hierarchy and path of tests was a good predictor of a feature's label. Conclusions: We find that this method can reduce tedious manual effort that software developers spent classifying test cases, while providing more accurate classification results.",
        "link": "https://dl.acm.org/doi/10.1145/3239235.3268927"
    },
    {
        "title": "To apply Data Mining for Classification of Crowd sourced Software Requirements",
        "authors": [
            "Soonh Taj",
            "Qasim Arain",
            "Imran Memon",
            "Asma Zubedi"
        ],
        "date": "April 2019",
        "source": "ICSIE '19: Proceedings of the 8th International Conference on Software and Information Engineering",
        "abstract": "Now a day's main focus of developers is to build quality software that works according to customer needs and for this reason it is necessary to gather right requirements as requirement elicitation is the critical step that impacts on the success of software project as misinterpreted requirements leads to the failure of software project. By keeping this in mind a research is carried out on improving requirements elicitation process and automating the process of classifying requirements. In this research, a model is proposed which will help in this scenario for requirements elicitation and requirement classification. This paper presents a model in which crowd sourcing approach is used so that customers, end users, stakeholders, developers and software engineers can make active participation for requirement elicitation process and requirements gathered using crowdsourcing approach are used by model for classification process i.e. classification of requirements into functional and non-functional requirements. For the proof of proposed model a case study is conducted. Results of case study provided the usefulness and efficiency of proposed model for classification of crowd sourced software requirements.",
        "link": "https://dl.acm.org/doi/10.1145/3328833.3328837"
    },
    {
        "title": "Text classification using Fuzzy TF-IDF and Machine Learning Models",
        "authors": [
            "Mariem Bounabi",
            "Karim El Moutaouakil",
            "Khalid Satori"
        ],
        "date": "October 2019",
        "source": "BDIoT'19: Proceedings of the 4th International Conference on Big Data and Internet of Things",
        "abstract": "The representation of the information has an important impact on the text classification task. Several weighting methods were proposed in the literature, and the term frequency-inverse term frequency (TFIDF), the most know on the text treatment field. The FTF-IDF is a vector representation where the components of the TFIDF are presented as inputs to the Fuzzy Inference System (FIS). In this work, we compare several Machin Learning algorithms such as Naïve Bayes and its derivatives, SVM and Random forest classifiers, using the FTF-IDF representation. To improve the quality of the used classifiers, we call sum attribute selection methods. The recognition rate, for the tested systems, is satisfied, where the system based on naïve Bayes classifier, the FTF-IDF weighting terms, and the info gain select attributes method gives 98.7% as accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3372938.3372956"
    },
    {
        "title": "Text Mining in Cybersecurity: A Systematic Literature Review",
        "authors": [
            "Luciano Ignaczak",
            "Guilherme Goldschmidt",
            "Cristiano André Da Costa",
            "Rodrigo Da Rosa Righi"
        ],
        "date": null,
        "source": "ACM Computing Surveys",
        "abstract": "The growth of data volume has changed cybersecurity activities, demanding a higher level of automation. In this new cybersecurity landscape, text mining emerged as an alternative to improve the efficiency of the activities involving unstructured data. This article proposes a Systematic Literature Review (SLR) to present the application of text mining in the cybersecurity domain. Using a systematic protocol, we identified 2,196 studies, out of which 83 were summarized. As a contribution, we propose a taxonomy to demonstrate the different activities in the cybersecurity domain supported by text mining. We also detail the strategies evaluated in the application of text mining tasks and the use of neural networks to support activities involving unstructured data. The work also discusses text classification performance aiming its application in real-world solutions. The SLR also highlights open gaps for future research, such as the analysis of non-English content and the intensification in the usage of neural networks.",
        "link": "https://dl.acm.org/doi/10.1145/3462477"
    },
    {
        "title": "Using Text Mining to Analyze the Financial Patents",
        "authors": [
            "Yung-Feng Lu",
            "Jia-Lang Xu",
            "Mu-Yen Chen"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "With the advancement and development of science and technology, Nowadays society is the development of information technology, cloud computing, mobile Internet, Internet of Things, block chain and information technology.The combination of finance and technology has made financial technology terms gradually emerging.. This research analyzes the patents of financial technology. However, Taiwan's financial technology patents are mostly G06Q-20 (payment plan, architecture or agreement), G06Q-30 (commercial, such as marketing, shopping, payment, auction or e-commerce). G06Q-40 (financial, investment or tax treatment, insurance).The research use text mining technology to analyze and understand Taiwan's trends and developments in financial technology. The research results show that G06Q-20 patents are biased towards information security and mobile payment. G06Q-30 patents favor shopping websites and advertising. G06Q-40 patents are biased towards diversity, such as securities, insurance, finance, and stocks.",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326478"
    },
    {
        "title": "Identifying Online Profiles of Distance Learning Students Using Data Mining Techniques",
        "authors": [
            "Osama Islam",
            "Muazzam Siddiqui",
            "Naif Radi Aljohani"
        ],
        "date": "October 2019",
        "source": "ICDTE '19: Proceedings of the 3rd International Conference on Digital Technology in Education",
        "abstract": "Educational data has grown over the years with the increased use of technology within educational environments. This has led to a huge amount of data being stored in various data sources representing the student, his/her activities, and other aspects relevant to the learning process. To meet this analytical need, Educational Data Mining (EDM) has emerged to assist educational institutions in identifying key benefits such as students at risk, the level of student engagement or predicting student performance. The aim of this research was to explore the various aspects of student interaction data using data mining techniques to identify relevant patterns of behaviors and possible key attributes that have higher degrees of influence on distance learning students. The main findings identified several patterns of user online profiles based on a set of adopted learning strategies, the research proposed a framework for analyzing such interaction data based on R and Hadoop platforms to correlate online profiles with student performance.",
        "link": "https://dl.acm.org/doi/10.1145/3369199.3369249"
    },
    {
        "title": "Extremely Fast Decision Tree Mining for Evolving Data Streams",
        "authors": [
            "Albert Bifet",
            "Jiajin Zhang",
            "Wei Fan",
            "Cheng He",
            "Jianfeng Zhang",
            "Jianfeng Qian",
            "Geoff Holmes",
            "Bernhard Pfahringer"
        ],
        "date": "August 2017",
        "source": "KDD '17: Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
        "abstract": "Nowadays real-time industrial applications are generating a huge amount of data continuously every day. To process these large data streams, we need fast and efficient methodologies and systems. A useful feature desired for data scientists and analysts is to have easy to visualize and understand machine learning models. Decision trees are preferred in many real-time applications for this reason, and also, because combined in an ensemble, they are one of the most powerful methods in machine learning. In this paper, we present a new system called STREAMDM-C++, that implements decision trees for data streams in C++, and that has been used extensively at Huawei. Streaming decision trees adapt to changes on streams, a huge advantage since standard decision trees are built using a snapshot of data, and can not evolve over time. STREAMDM-C++ is easy to extend, and contains more powerful ensemble methods, and a more efficient and easy to use adaptive decision trees. We compare our new implementation with VFML, the current state of the art implementation in C, and show how our new system outperforms VFML in speed using less resources.",
        "link": "https://dl.acm.org/doi/10.1145/3097983.3098139"
    },
    {
        "title": "Data miners' little helper: data transformation activity cues for cluster analysis on document collections",
        "authors": [
            "Tania Cerquitelli",
            "Evelina Di Corso",
            "Francesco Ventura",
            "Silvia Chiusano"
        ],
        "date": "June 2017",
        "source": "WIMS '17: Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "In this paper we propose a new self-learning engine to streamline the analytics process, as it enables analysts to mine massive data repositories with minimal user intervention. In the context of cluster analysis on a collection of documents this new system, named SELF-DATA (SELF-learning DAta TrAnsformation), suggests to the analyst how to configure the whole mining process for a given dataset. SELF-DATA relies on an engine exploring different data weighting schemas (e.g., normalized term frequencies) and data transformation methods (e.g., PCA) before applying the cluster analysis, evaluating and comparing solutions through different quality indices (e.g., weighted Silhouette), and presenting the k-top solutions to the analyst. SELF-DATA will also include a knowledge base storing results of experiments on previously processed datasets, and a classification algorithm trained on the knowledge base content to forecast the best configuration for the whole mining process for an unexplored dataset. The first development of SELF-DATA running on Apache Spark has been validated on 5 collections of documents. Experimental results highlight that TF-IDF and logarithmic entropy are effective to measure item relevance with sparse datasets, and the LSI method outperforms PCA with a large dictionary.",
        "link": "https://dl.acm.org/doi/10.1145/3102254.3102288"
    },
    {
        "title": "Interactive Visual Graph Mining and Learning",
        "authors": [
            "Ryan A. Rossi",
            "Nesreen K. Ahmed",
            "Rong Zhou",
            "Hoda Eldardiry"
        ],
        "date": null,
        "source": "ACM Transactions on Intelligent Systems and Technology",
        "abstract": "This article presents a platform for interactive graph mining and relational machine learning called GraphVis. The platform combines interactive visual representations with state-of-the-art graph mining and relational machine learning techniques to aid in revealing important insights quickly as well as learning an appropriate and highly predictive model for a particular task (e.g., classification, link prediction, discovering the roles of nodes, and finding influential nodes). Visual representations and interaction techniques and tools are developed for simple, fast, and intuitive real-time interactive exploration, mining, and modeling of graph data. In particular, we propose techniques for interactive relational learning (e.g., node/link classification), interactive link prediction and weighting, role discovery and community detection, higher-order network analysis (via graphlets, network motifs), among others. GraphVis also allows for the refinement and tuning of graph mining and relational learning methods for specific application domains and constraints via an end-to-end interactive visual analytic pipeline that learns, infers, and provides rapid interactive visualization with immediate feedback at each change/prediction in real-time. Other key aspects include interactive filtering, querying, ranking, manipulating, exporting, as well as tools for dynamic network analysis and visualization, interactive graph generators (including new block model approaches), and a variety of multi-level network analysis techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3200764"
    },
    {
        "title": "Data Mining and Opinion Mining: A Tool in Educational Context",
        "authors": [
            "Myriam Peñafiel",
            "Stefanie Vásquez",
            "Diego Vásquez",
            "Juan Zaldumbide",
            "Sergio Luján-Mora"
        ],
        "date": "July 2018",
        "source": "ICoMS '18: Proceedings of the 2018 1st International Conference on Mathematics and Statistics",
        "abstract": "The use of the web as a universal communication platform generates large volumes of data (Big data), which in many cases, need to be processed so that they can become useful knowledge in face of the sceptics who have doubts about the credibility of such information. The use of web data that comes from educational contexts needs to be addressed, since that large amount of unstructured information is not being valued, losing valuable information that can be used. To solve this problem, we propose the use of data mining techniques such as sentiment analysis to validate the information that comes from the educational platforms. The objective of this research is to propose a methodology that allows the user to apply sentiment analysis in a simple way, because although some researchers have done it, very few do with data in the educational context. The results obtained prove that the proposal can be used in similar cases.",
        "link": "https://dl.acm.org/doi/10.1145/3274250.3274263"
    },
    {
        "title": "Research Hotspots Mining and Visualized Analysis Based on Linking Cluster and K-Core Decomposition",
        "authors": [
            "Ying Cai",
            "Fang Huang",
            "Shuai Wang",
            "Hao Zhang",
            "Chunxiu Du"
        ],
        "date": "May 2018",
        "source": "ICDPA 2018: Proceedings of the International Conference on Data Processing and Applications",
        "abstract": "In this paper, we focus on research hotspots mining with knowledge semantic features on discipline topic word networks, which are extracted from scientific and technical documents. For the topic networks, a novel method of combining linking community clustering with k-core decomposition is proposed to mine the hotspot community with knowledge hierarchy structure. And as a post-processing, the hierarchical community identification is implemented by k-core decomposition in order to discover the internal connection with knowledge semantic hierarchy in the research hotspot. Finally, the community density and visualization method is utilized to analyze the core and sub- problem in the research hotspot community. In experiments, the topic networks for a single discipline and interdisciplinary research hotspot are taken as examples of analysis, and the results show the proposed approach can effectively realize the objective of mining and analyzing the discipline research hotspot on the academic topic word network.",
        "link": "https://dl.acm.org/doi/10.1145/3224207.3224216"
    },
    {
        "title": "The Application of Data Mining Algorithms in the Construction of Travel Recommendation System",
        "authors": [
            "Chao Lou",
            "Zhaonan Mu",
            "Mengzhu Liu"
        ],
        "date": "January 2021",
        "source": "CONF-CDS 2021: The 2nd International Conference on Computing and Data Science",
        "abstract": "Photos shared by users of online social network often associated with tags, texts, geographic and time information. These data are ideal research resource which can be used by researchers to observe people's interests and behaviors. They can also be used to analyze people's travel behaviors and interests because it can reflect people's travel activities and experiences directly. This project aimed to consolidate people into groups based on their travel preferences of travel season, travel time span, scenery type of travel location and then analyze popular travel places for different people groups. The travel preferences were analyzed from extracted data applying PHP language and Naive Bayes classification approach. According to the analyzed data, people were divided into groups. Then, DBSCAN clustering was used to get popular locations based on the different groups. As a result, 80 thousand photo data of 5 cities have been collected from Flickr by Phython. It has been showed that people's preference of season in most of the mined cities was spring and summer and the preference of time span was short. Moreover, mined people has been divided into 16 groups according to their preference and based on the groups popular locations have been calculated by DBSCAN clustering. Theses locations have been provided in a recommendation website which was built in this project. For the future work, mining Flickr data associated with other travel social network sites could be a good way to build a more effective recommendation system.",
        "link": "https://dl.acm.org/doi/10.1145/3448734.3450477"
    },
    {
        "title": "Compare Machine Learning Models in Text Classification Using Steam User Reviews",
        "authors": [
            "Youchen Miao",
            "Zeyu Jin",
            "Yumeng Zhang",
            "Yuchen Chen",
            "Junren Lai"
        ],
        "date": "November 2021",
        "source": "ICSED '21: Proceedings of the 2021 3rd International Conference on Software Engineering and Development",
        "abstract": "Text Classification and Sentiment Analysis of game reviews are viewed as important parts in not only academic fields but also in game studies. In this paper, with more than 400 thousand game reviews on Steam platform, we preprocess the data using different libraries (sklearn, nltk, and spaCy) and use them as inputs to build three sentiment classification models based on different algorithms (Naive Bayes, SVM, and Random Forest). In contrast to previous studies that only focus on different sentiment analysis models, our paper also highlights the use of different APIs to preprocess the data and their corresponding model performance. The results show that no matter which API we choose, Random Forest models always perform the best. However, in terms of training time, Naive Bayes is the fastest. This work can be used to apply grid search for researchers to automatically find the optimum API before conducting sentiment analysis in the future.",
        "link": "https://dl.acm.org/doi/10.1145/3507473.3507480"
    },
    {
        "title": "Lost in Transduction: Transductive Transfer Learning in Text Classification",
        "authors": [
            "Alejandro Moreo",
            "Andrea Esuli",
            "Fabrizio Sebastiani"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Obtaining high-quality labelled data for training a classifier in a new application domain is often costly. Transfer Learning\n(a.k.a. “Inductive Transfer”) tries to alleviate these costs by transferring, to the “target” domain of interest, knowledge available from a different “source” domain. In transfer learning the lack of labelled information from the target domain is compensated by the availability at training time of a set of unlabelled examples from the target distribution. Transductive Transfer Learning denotes the transfer learning setting in which the only set of target documents that we are interested in classifying is known and available at training time. Although this definition is indeed in line with Vapnik’s original definition of “transduction”, current terminology in the field is confused. In this article, we discuss how the term “transduction” has been misused in the transfer learning literature, and propose a clarification consistent with the original characterization of this term given by Vapnik. We go on to observe that the above terminology misuse has brought about misleading experimental comparisons, with inductive transfer learning methods that have been incorrectly compared with transductive transfer learning methods. We then, give empirical evidence that the difference in performance between the inductive version and the transductive version of a transfer learning method can indeed be statistically significant (i.e., that knowing at training time the only data one needs to classify indeed gives an advantage). Our clarification allows a reassessment of the field, and of the relative merits of the major, state-of-the-art algorithms for transfer learning in text classification.",
        "link": "https://dl.acm.org/doi/10.1145/3453146"
    },
    {
        "title": "Machine Learning on Mining Potential Adverse Drug Reactions for Pharmacovigilance",
        "authors": [
            "Ke Jia"
        ],
        "date": "July 2021",
        "source": "DSIT 2021: 2021 4th International Conference on Data Science and Information Technology",
        "abstract": "Drugs are related to the lives and health of patients. Mining and discovering potential adverse drug reactions from a large number of unstructured texts play an important role in drug research and pharmacovigilance. It is a hot research issue in the field of biomedicine in recent years. A large number of scholars have discovered a large amount of information on adverse drug reactions from different data sources through effective machine learning methods. This paper reviews the research progress in recent years to provide a reference for future research.",
        "link": "https://dl.acm.org/doi/10.1145/3478905.3478964"
    },
    {
        "title": "TACAM: Topic And Context Aware Argument Mining",
        "authors": [
            "Michael Fromm",
            "Evgeniy Faerman",
            "Thomas Seidl"
        ],
        "date": "October 2019",
        "source": "WI '19: IEEE/WIC/ACM International Conference on Web Intelligence",
        "abstract": "In this work we address the problem of argument search. The purpose of argument search is the distillation of pro and contra arguments for requested topics from large text corpora. In previous works, the usual approach is to use a standard search engine to extract text parts which are relevant to the given topic and subsequently use an argument recognition algorithm to select arguments from them. The main challenge in the argument recognition task, which is also known as argument mining, is that often sentences containing arguments are structurally similar to purely informative sentences without any stance about the topic. In fact, they only differ semantically. Most approaches use topic or search term information only for the first search step and therefore assume that arguments can be classified independently of a topic. We argue that topic information is crucial for argument mining, since the topic defines the semantic context of an argument. Precisely, we propose different models for the classification of arguments, which take information about a topic of an argument into account. Moreover, to enrich the context of a topic and to let models understand the context of the potential argument better, we integrate information from different external sources such as Knowledge Graphs or pre-trained NLP models. Our evaluation shows that considering topic information, especially in connection with external information, provides a significant performance boost for the argument mining task.",
        "link": "https://dl.acm.org/doi/10.1145/3350546.3352506"
    },
    {
        "title": "Text mining for incoming tasks based on the urgency/importance factors and task classification using machine learning tools",
        "authors": [
            "Yasser Ali Alshehri"
        ],
        "date": "March 2020",
        "source": "ICCDA 2020: Proceedings of the 2020 the 4th International Conference on Compute and Data Analysis",
        "abstract": "In workplaces, there is a massive amount of unstructured data from different sources. In this paper, we present a case study that explains how can through communications between employees, we can help to prioritize tasks requests to increase the efficiency of their works for both technical and non-technical workers. This involves managing daily incoming tasks based on their level of urgency and importance.To allow all workers to utilize the urgency-importance matrix as a time-management tool, we need to automate this tool. The textual content of incoming tasks are analyzed, and metrics related to urgency and importance are extracted. A third factor (i.e., the response variable) is defined based on the two input variables (urgency and importance). Then, machine learning applied to the data to predict the class of incoming tasks based on data outcome desired. We used ordinal regression, neural networks, and decision tree algorithms to predict the four levels of task priority. We measure the performance of all using recalls, precisions, and F-scores. All classifiers perform higher than 89% in terms of all measures.",
        "link": "https://dl.acm.org/doi/10.1145/3388142.3388153"
    },
    {
        "title": "Text mining for malware classification using multivariate all repeated patterns detection",
        "authors": [
            "Konstantinos F. Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2019",
        "source": "ASONAM '19: Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Mobile phones have become nowadays a commodity to the majority of people. Using them, people are able to access the world of Internet and connect with their friends, their colleagues at work or even unknown people with common interests. This proliferation of the mobile devices has also been seen as an opportunity for the cyber criminals to deceive smartphone users and steel their money directly or indirectly, respectively, by accessing their bank accounts through the smartphones or by blackmailing them or selling their private data such as photos, credit card data, etc. to third parties. This is usually achieved by installing malware to smartphones masking their malevolent payload as a legitimate application and advertise it to the users with the hope that mobile users will install it in their devices. Thus, any existing application can easily be modified by integrating a malware and then presented it as a legitimate one. In response to this, scientists have proposed a number of malware detection and classification methods using a variety of techniques. Even though, several of them achieve relatively high precision in malware classification, there is still space for improvement. In this paper, we propose a text mining all repeated pattern detection method which uses the decompiled files of an application in order to classify a suspicious application into one of the known malware families. Based on the experimental results using a real malware dataset, the methodology tries to correctly classify (without any misclassification) all randomly selected malware applications of 3 categories with 3 different families each.",
        "link": "https://dl.acm.org/doi/10.1145/3341161.3350841"
    },
    {
        "title": "Comparative Study between Traditional Machine Learning and Deep Learning Approaches for Text Classification",
        "authors": [
            "Cannannore Nidhi Kamath",
            "Syed Saqib Bukhari",
            "Andreas Dengel"
        ],
        "date": "August 2018",
        "source": "DocEng '18: Proceedings of the ACM Symposium on Document Engineering 2018",
        "abstract": "In this contemporaneous world, it is an obligation for any organization working with documents to end up with the insipid task of classifying truckload of documents, which is the nascent stage of venturing into the realm of information retrieval and data mining. But classification of such humongous documents into multiple classes, calls for a lot of time and labor. Hence a system which could classify these documents with acceptable accuracy would be of an unfathomable help in document engineering. We have created multiple classifiers for document classification and compared their accuracy on raw and processed data. We have garnered data used in a corporate organization as well as publicly available data for comparison. Data is processed by removing the stop-words and stemming is implemented to produce root words. Multiple traditional machine learning techniques like Naive Bayes, Logistic Regression, Support Vector Machine, Random forest Classifier and Multi-Layer Perceptron are used for classification of documents. Classifiers are applied on raw and processed data separately and their accuracy is noted. Along with this, Deep learning technique such as Convolution Neural Network is also used to classify the data and its accuracy is compared with that of traditional machine learning techniques. We are also exploring hierarchical classifiers for classification of classes and subclasses. The system classifies the data faster and with better accuracy than if done manually. The results are discussed in the results and evaluation section.",
        "link": "https://dl.acm.org/doi/10.1145/3209280.3209526"
    },
    {
        "title": "Identification of Overpricing in the Purchase of Medication by the Federal Government of Brazil, Using Text Mining and Clustering Based on Ontology",
        "authors": [
            "Marco Aurelio O. S. Correa",
            "Adriano Galindo Leal"
        ],
        "date": "August 2018",
        "source": "ICCBDC '18: Proceedings of the 2018 2nd International Conference on Cloud and Big Data Computing",
        "abstract": "Increasing the transparency level in his actions and spending is one of the primary duties of the Brazilian Federal Government. The creation of laws that oblige full disclose of all its expenditures through transparency portals enables citizens to supervise all government entities. However, only the dissemination of these data, without a definite standard or the availability of data analysis tools, does not guarantee that the citizen is empowered to play his role. Therefore hence, the objective of this work is to identify overprice in the acquisition of products purchased by the federal government of Brazil using the unstructured data available on the Transparency Portal. The last two-years' worth of purchasing data, available in the Transparency Portal, were extracted, processed and stored. Due to his diverse nature and high volume of data, this study focused only on medicines purchased by the Ministry of Health. Ontology-based text mining and clustering techniques were applied for automatic identification and classification of products. The processing of this information was done through text mining and clustering, based on the ontology registered in another database of the Brazilian government. Because of this work, a consolidated price base per medication was created to allow the identification of distortions in prices practised, facilitating the identification of cases that merit further investigation to unravel fraud to the treasury.",
        "link": "https://dl.acm.org/doi/10.1145/3264560.3264569"
    },
    {
        "title": "Computational Estimation by Scientific Data Mining with Classical Methods to Automate Learning Strategies of Scientists",
        "authors": [
            "Aparna S. Varde"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Experimental results are often plotted as 2-dimensional graphical plots (aka graphs) in scientific domains depicting dependent versus independent variables to aid visual analysis of processes. Repeatedly performing laboratory experiments consumes significant time and resources, motivating the need for computational estimation. The goals are to estimate the graph obtained in an experiment given its input conditions, and to estimate the conditions that would lead to a desired graph. Existing estimation approaches often do not meet accuracy and efficiency needs of targeted applications. We develop a computational estimation approach called AutoDomainMine that integrates clustering and classification over complex scientific data in a framework so as to automate classical learning methods of scientists. Knowledge discovered thereby from a database of existing experiments serves as the basis for estimation. Challenges include preserving domain semantics in clustering, finding matching strategies in classification, striking a good balance between elaboration and conciseness while displaying estimation results based on needs of targeted users, and deriving objective measures to capture subjective user interests. These and other challenges are addressed in this work. The AutoDomainMine approach is used to build a computational estimation system, rigorously evaluated with real data in Materials Science. Our evaluation confirms that AutoDomainMine provides desired accuracy and efficiency in computational estimation. It is extendable to other science and engineering domains as proved by adaptation of its sub-processes within fields such as Bioinformatics and Nanotechnology.",
        "link": "https://dl.acm.org/doi/10.1145/3502736"
    },
    {
        "title": "Where is the road for issue reports classification based on text mining?",
        "authors": [
            "Qiang Fan",
            "Yue Yu",
            "Gang Yin",
            "Tao Wang",
            "Huaimin Wang"
        ],
        "date": "November 2017",
        "source": "ESEM '17: Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Currently, open source projects receive various kinds of issues daily, because of the extreme openness of Issue Tracking System (ITS) in GitHub. ITS is a labor-intensive and time-consuming task of issue categorization for project managers. However, a contributor is only required a short textual abstract to report an issue in GitHub. Thus, most traditional classification approaches based on detailed and structured data (e.g., priority, severity, software version and so on) are difficult to adopt. In this paper, issue classification approaches on a large-scale dataset, including 80 popular projects and over 252,000 issue reports collected from GitHub, were investigated. First, four traditional text-based classification methods and their performances were discussed. Semantic perplexity (i.e., an issues description confuses bug-related sentences with nonbug-related sentences) is a crucial factor that affects the classification performances based on quantitative and qualitative study. Finally, A two-stage classifier framework based on the novel metrics of semantic perplexity of issue reports was designed. Results show that our two-stage classification can significantly improve issue classification performances.",
        "link": "https://dl.acm.org/doi/10.1109/ESEM.2017.19"
    },
    {
        "title": "Feature-based Facebook reviews process model for e-management using data mining",
        "authors": [
            "Anish Kumar Varudharajulu",
            "Yongsheng Ma"
        ],
        "date": "January 2019",
        "source": "IC4E '19: Proceedings of the 10th International Conference on E-Education, E-Business, E-Management and E-Learning",
        "abstract": "The data generated from online communication acts as potential gold mines for discovering knowledge for researchers. A large amount of data is also generated in the form of web documents, emails, blogs, and feedback, etc. Text analytics is being significantly employed to mine important information. Opinion mining is the process of extracting human thoughts and perceptions from unstructured texts. The showstopper for designing an opinion mining system for analyzing reviews arise from the fact that customer reviews are often noisy. These reviews are informally written. In addition, they are subjected to spelling mistakes, grammatical errors, improper punctuation and irrational capitalization. This paper focuses on analyzing the different classification and clustering algorithms aimed at extracting and consolidating opinions of customers from social media sites like Facebook, Twitter and through surveys, at multiple levels of granularity to monitor and measure customer satisfaction. Ours is an automated approach, in which the system aids in the process of knowledge assimilation for knowledge-based building and also performs the analytics. Domain experts ratify the knowledge base and also provide training datasets for the system to intuitively gather more instances for ratification. The system identifies opinion expressions as phrases containing opinion words, opinionated features and also opinion modifiers. These expressions are categorized as positive, negative or neutral. Opinion expressions are identified and categorized using localized linguistic techniques. Opinions can be congregated at any desired level of specificity i.e. feature level or product level, user level or service level, etc. We have developed a system based on this approach, which provides the user with a platform to analyze opinion expressions crawled from a set of pre-defined datasets.",
        "link": "https://dl.acm.org/doi/10.1145/3306500.3306514"
    },
    {
        "title": "Discriminative Topic Mining via Category-Name Guided Text Embedding",
        "authors": [
            "Yu Meng",
            "Jiaxin Huang",
            "Guangyuan Wang",
            "Zihan Wang",
            "Chao Zhang",
            "Yu Zhang",
            "Jiawei Han"
        ],
        "date": "April 2020",
        "source": "WWW '20: Proceedings of The Web Conference 2020",
        "abstract": "Mining a set of meaningful and distinctive topics automatically from massive text corpora has broad applications. Existing topic models, however, typically work in a purely unsupervised way, which often generate topics that do not fit users’ particular needs and yield suboptimal performance on downstream tasks. We propose a new task, discriminative topic mining, which leverages a set of user-provided category names to mine discriminative topics from text corpora. This new task not only helps a user understand clearly and distinctively the topics he/she is most interested in, but also benefits directly keyword-driven classification tasks. We develop CatE, a novel category-name guided text embedding method for discriminative topic mining, which effectively leverages minimal user guidance to learn a discriminative embedding space and discover category representative terms in an iterative manner. We conduct a comprehensive set of experiments to show that CatE mines high-quality set of topics guided by category names only, and benefits a variety of downstream applications including weakly-supervised classification and lexical entailment direction identification.",
        "link": "https://dl.acm.org/doi/10.1145/3366423.3380278"
    },
    {
        "title": "Comparative Study of Heart Disease Diagnosis Using Top Ten Data Mining Classification Algorithms",
        "authors": [
            "I. Ketut Agung Enriko"
        ],
        "date": "June 2019",
        "source": "ICFET '19: Proceedings of the 5th International Conference on Frontiers of Educational Technologies",
        "abstract": "Data mining has been used for many purposes, especially for prediction system. In healthcare, data mining algorithms often used in disease diagnosis. Meanwhile, heart disease is known as a primary cause of death over the years. Many studies have been performed in heart disease diagnosis using data mining methods. There are some popular data mining algorithms that can be used in heart disease diagnosis, for example, k-Nearest Neighbor, CART, and AdaBoost. The algorithms are used to analyze a sample of cardiovascular patients data and predict the heart disease type that they suffer. Some parameters are taken from the patient, including EKG morphology, blood pressure, and information about the existence of chest pain, shortness of breath, palpitation, and cold sweat. In this study, medical records data are collected from Harapan Kita Hospital and utilized as a dataset sample in this research. Top ten data mining classification algorithms are used in diagnosing heart disease from Harapan Kita Hospital data and examining their performance by checking the accuracy and speed.",
        "link": "https://dl.acm.org/doi/10.1145/3338188.3338220"
    },
    {
        "title": "Text Mining Approach for Identifying Research Trends",
        "authors": [
            "Snezhana Sulova"
        ],
        "date": "June 2021",
        "source": "CompSysTech '21: Proceedings of the 22nd International Conference on Computer Systems and Technologies",
        "abstract": "With the increase of unstructured data, the issues connected with automatic text processing, the categorization of documents and the discovery of topics have become objects of growing interest. In order to improve the process of grouping and processing research publications, we would like to propose a method based upon natural language processing. It is based on text mining technologies which aim to identify key tendencies in documents. It processes the content of publications by clustering and identifies the topics of each identified group. This analysis helps by identifying key tendencies as well as discovering emerging new areas of research. Publications from the research literature database, Scopus, were used to test the approach. The topic of the publications is “the application of digital technologies in the logistics business”. The experiments were completed using the RapidMiner Studio software.",
        "link": "https://dl.acm.org/doi/10.1145/3472410.3472433"
    },
    {
        "title": "Assessment of Vulnerability Severity using Text Mining",
        "authors": [
            "Georgios Spanos",
            "Lefteris Angelis",
            "Dimitrios Toloudis"
        ],
        "date": "September 2017",
        "source": "PCI '17: Proceedings of the 21st Pan-Hellenic Conference on Informatics",
        "abstract": "Software1 vulnerabilities are closely associated with information systems security, a major and critical field in today's technology. Vulnerabilities constitute a constant and increasing threat for various aspects of everyday life, especially for safety and economy, since the social impact from the problems that they cause is complicated and often unpredictable. Although there is an entire research branch in software engineering that deals with the identification and elimination of vulnerabilities, the growing complexity of software products and the variability of software production procedures are factors contributing to the ongoing occurrence of vulnerabilities, Hence, another area that is being developed in parallel focuses on the study and management of the vulnerabilities that have already been reported and registered in databases. The information contained in such databases includes, a textual description and a number of metrics related to vulnerabilities. The purpose of this paper is to investigate to what extend the assessment of the vulnerability severity can be inferred directly from the corresponding textual description, or in other words, to examine the informative power of the description with respect to the vulnerability severity. For this purpose, text mining techniques, i.e. text analysis and three different classification methods (decision trees, neural networks and support vector machines) were employed. The application of text mining to a sample of 70,678 vulnerabilities from a public data source shows that the description itself is a reliable and highly accurate source of information for vulnerability prioritization.",
        "link": "https://dl.acm.org/doi/10.1145/3139367.3139390"
    },
    {
        "title": "Clustering Algorithms for Spatial Data Mining",
        "authors": [
            "Chetashri Bhadane",
            "Ketan Shah"
        ],
        "date": "April 2020",
        "source": "ICGDA '20: Proceedings of the 2020 3rd International Conference on Geoinformatics and Data Analysis",
        "abstract": "With the advances in mobile and wireless technologies, there has been a rise in applications that track and share the users' geospatial data. People use several social networking sites such as Twitter, Facebook and Flickr, where they share their status updates. With the integration of Global Positioning System (GPS) with mobile phones, it is now possible to share one's locations on these social networks. GPS allows us to record and track a person's movement along with the timestamp. The data set obtained from these GPS logs is vast and is widely used to analyze the users' movement patterns. Specifically, we can find out significant locations based on the number of users present at that location and the time spent by them at such places. Once significant places have been identified, it is also possible to identify the semantic importance of these locations. This paper presents an overview of the clustering techniques used to find important places of interest using large GPS based mobility datasets. Four clustering algorithms, K-Means, DBSCAN, OPTICS and Hierarchical, are implemented, and performance is tested using real-time data of 50 users collected over 2--5 years. Performance summary depicts that K-Means and DBSCAN perform well for spatial data.",
        "link": "https://dl.acm.org/doi/10.1145/3397056.3397068"
    },
    {
        "title": "Feature Selection based Arabic Text Classification using Different Machine Learning Algorithms: Comparative Study",
        "authors": [
            "Sakina Rim Bennabi",
            "Zakaria Elberrichi"
        ],
        "date": "June 2020",
        "source": "ICIST '20: Proceedings of the 10th International Conference on Information Systems and Technologies",
        "abstract": "Feature selection is a method of data pre-processing widely used when mining large data, such as textual classification. Several studies have been conducted to compare the different methods of feature selection applied to corpora in English. Unfortunately, a small number of works concern the Arabic language. This article aims to present a comparative study of different feature selection techniques including: Chi2, the ANOVA method and mutual information, applied on a corpus in Arabic language, while also diversifying the machine learning algorithms (Naive Bayes, SVM and KNN). This experimental study has shown in general that reducing dimensionality with feature selection techniques has slightly affected the performance of textual classification, reducing the size of the corpus by up to 1%.",
        "link": "https://dl.acm.org/doi/10.1145/3447568.3448531"
    },
    {
        "title": "Intelligent Data Analysis using Optimized Support Vector Machine Based Data Mining Approach for Tourism Industry",
        "authors": [
            "Ms Promila Sharma",
            "Uma Meena",
            "Girish Kumar Sharma"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Data analysis involves the deployment of sophisticated approaches from data mining methods, information theory, and artificial intelligence in various fields like tourism, hospitality, and so on for the extraction of knowledge from the gathered and preprocessed data. In tourism, pattern analysis or data analysis using classification is significant for finding the patterns that represent new and potentially useful information or knowledge about the destination and other data. Several data mining techniques are introduced for the classification of data or patterns. However, overfitting, less accuracy, local minima, sensitive to noise are the drawbacks in some existing data mining classification methods. To overcome these challenges, Support vector machine with Red deer optimization (SVM-RDO) based data mining strategy is proposed in this article. Extended Kalman filter (EKF) is utilized in the first phase, i.e., data cleaning to remove the noise and missing values from the input data. Mantaray foraging algorithm (MaFA) is used in the data selection phase, in which the significant data are selected for the further process to reduce the computational complexity. The final phase is the classification, in which SVM-RDO is proposed to access the useful pattern from the selected data. PYTHON is the implementation tool used for the experiment of the proposed model. The experimental analysis is done to show the efficacy of the proposed work. From the experimental results, the proposed SVM-RDO achieved better accuracy, precision, recall, and F1 score than the existing methods for the tourism dataset. Thus, it is showed the effectiveness of the proposed SVM-RDO for pattern analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3494566"
    },
    {
        "title": "Applying text mining to predict learners' cognitive engagement",
        "authors": [
            "Hayati Hind",
            "Mohammed Khalidi Idrissi",
            "Samir Bennani"
        ],
        "date": "October 2017",
        "source": "SCAMS '17: Proceedings of the Mediterranean Symposium on Smart City Application",
        "abstract": "In academic process, engagement represent one of the important success' factor. This paper proposes a predictive system for learners' cognitive engagement based on their online discussion forums participation. In the first level the ontology OWL and the LSA method are used to perform a semantic classification system of the threads according to a specific context chosen by the tutor. Then the Text mining, as a predictive method, is applied to the classified threads and learners' participation in the forums.",
        "link": "https://dl.acm.org/doi/10.1145/3175628.3175655"
    },
    {
        "title": "Investigate Transitions into Drug Addiction through Text Mining of Reddit Data",
        "authors": [
            "John Lu",
            "Sumati Sridhar",
            "Ritika Pandey",
            "Mohammad Al Hasan",
            "Georege Mohler"
        ],
        "date": "July 2019",
        "source": "KDD '19: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Increasing rates of opioid drug abuse and heightened prevalence of online support communities underscore the necessity of employing data mining techniques to better understand drug addiction using these rapidly developing online resources. In this work, we obtained data from Reddit, an online collection of forums, to gather insight into drug use/misuse using text snippets from users narratives. Specifically, using users' posts, we trained a binary classifier which predicts a user's transitions from casual drug discussion forums to drug recovery forums. We also proposed a Cox regression model that outputs likelihoods of such transitions. In doing so, we found that utterances of select drugs and certain linguistic features contained in one's posts can help predict these transitions. Using unfiltered drug-related posts, our research delineates drugs that are associated with higher rates of transitions from recreational drug discussion to support/recovery discussion, offers insight into modern drug culture, and provides tools with potential applications in combating the opioid crisis.",
        "link": "https://dl.acm.org/doi/10.1145/3292500.3330737"
    },
    {
        "title": "Analysing digital banking reviews using text mining",
        "authors": [
            "Li Chen Cheng",
            "Legaspi Rhea Sharmayne"
        ],
        "date": "December 2020",
        "source": "ASONAM '20: Proceedings of the 12th IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Digital banks are new entrants in the banking industry in the Philippines as they only started late 2018. Since then, a handful of players have and are still emerging. With more and more people becoming technologically savvy, it is very critical for financial institutions to develop a digital banking application that will stand out from the competition. This paper aims to use text mining methods to analyse digital banking application reviews. This study will perform topic modelling using LDA to explore customer concerns and will mine association rules between the digital banking features with the review score. The results will reveal which areas the digital banking application can further optimize for customer satisfaction and retention.",
        "link": "https://dl.acm.org/doi/10.1109/ASONAM49781.2020.9381429"
    },
    {
        "title": "Mining Moocs Videos Metadata Using Classification Techniques",
        "authors": [
            "El Harrak Othman",
            "Ghadi Abderrahim",
            "El Bouhdidi Jaber"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "In few years, the internet has become the first source of information for most people, Today MOOCs makes it possible for everyone to access to the education over the world, it's represents an emerging methodology of online teaching and an important development in open education. Due to the rapid development of Moocs and the rapid growth of digital data and video database over the Internet, it is becoming very difficult for learners to browsing and choosing the best training for them. The scientific community has increased the amount of research into new technologies, with a view to improve the digital video utilization: its archiving, indexing, accessibility, acquisition, store and even its process and usability. All these parts of the video utilization entail the necessity of the extraction of all important information of a video, especially in cases of lack of metadata information. Web video mining is retrieving the content using data mining techniques from World Wide Web. There are two approaches for web video mining using traditional image processing (signal processing) and metadata based approach. In this paper, we present the various research makes in this subject and we propose an effective methodology to extract the metadata from Moocs videos and classify them based on the extracted metadata by applying data mining techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090450"
    },
    {
        "title": "Text mining with HathiTrust",
        "authors": [
            "Eleanor Dickson Koehl",
            "Ryan Dubnicek"
        ],
        "date": "June 2019",
        "source": "JCDL '19: Proceedings of the 18th Joint Conference on Digital Libraries",
        "abstract": "This tutorial will introduce attendees to the HathiTrust Research Center and its tools and services for computational text analysis research. HTRC leverages the scope and scale of the HathiTrust Digital Library collection to create opportunities for researchers to perform text data mining on subsets of the corpus. Attendees will develop skills that will allow them to conduct basic text analysis research using HathiTrust data, learn techniques for engaging with scholars on their text data mining research, and explore strategies for computational access to large-scale digital collections.",
        "link": "https://dl.acm.org/doi/10.1109/JCDL.2019.00115"
    },
    {
        "title": "Text mining for plagiarism detection: multivariate pattern detection for recognition of text similarities",
        "authors": [
            "Konstantinos Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2018",
        "source": "ASONAM '18: Proceedings of the 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "The problem of plagiarism the recent years has been intensified by the availability of information in digital form and the accessibility of the electronic libraries through the Internet. As a result, plagiarism detection has been transformed into a big data analytics problem since the number of digital sources is extravagant and a new document needs to be compared with millions of other existing documents. In this paper, a text mining methodology is proposed that can detect all common patterns between a document and the documents in a reference database. The technique is based on a pattern detection algorithm and the corresponding data structure that enables the algorithm to detect all common patterns. The methodology has been applied in a well-defined dataset providing very promising results identifying difficult cases of plagiarism such as technical disguise.",
        "link": "https://dl.acm.org/doi/10.5555/3382225.3382424"
    },
    {
        "title": "Petroleum Engineering Data Text Classification Using Convolutional Neural Network Based Classifier",
        "authors": [
            "Tzuhan Hsu",
            "Yaoqin Zhang"
        ],
        "date": "May 2018",
        "source": "ICMLT '18: Proceedings of the 2018 International Conference on Machine Learning Technologies",
        "abstract": "In this paper, authors introduce a convolutional neural network (CNN) based classification approach for petroleum engineering event. Recently, text classification problem has raised substantial attention in the field of machine learning. The aim of text classification process is to assign correct label to a text on the basis of its content. However, due to the imprecise of feature representation, most of former text classification methods cannot export satisfying result. To solve this problem, authors propose a model based on convolutional neural network. Authors conduct experiments with real world petroleum engineering data text, artificially labelled into 6 categories. Beside purposed CNN-based model, the authors also implement other baseline machine learning approaches: k nearest neighbor, random forest, support vector machines, recurrent neural network, gated recurrent units, long short-term memory. Among all these models, our CNN-based model reach best performance in text classification task. In the last part of this paper, authors compare and analysis their purpose and baseline model.",
        "link": "https://dl.acm.org/doi/10.1145/3231884.3231898"
    },
    {
        "title": "Classifying Indonesian Online Articles as Advertisement Placement Base Using Text Mining",
        "authors": [
            "Nadhira Tasya",
            "Arian Dhini"
        ],
        "date": "July 2017",
        "source": "ICBIM 2017: Proceedings of the International Conference on Business and Information Management",
        "abstract": "Rapid development in technological aspect resulting in growing level of human needs for the latest news, so that emerged a new trend of publishing and accessing news through online media or usually called online journalism. In addition, the number of people who sell and purchase through online sites also continues to increase and this opportunity is utilized by the company and the advertiser by implementing targeted web advertising. However, the high number of articles that have been published and accessed leads to great opportunities for errors in determining where to place the ads. Therefore, it needs a system that can categorize articles accessed by users as the basis of advertisement placement by the company and this classification system can be done by applying the method of Data Mining and Text Mining. This research uses document data in the form of article content that will be categorized into twenty categories of class of advertisement by using Text Mining technique with Support Vector Machine algorithm. The results of this study may be used by companies or advertisers as a basis for placement of ads on selected online media sites.",
        "link": "https://dl.acm.org/doi/10.1145/3134271.3134288"
    },
    {
        "title": "Mining Free-Text Medical Notes for Suicide Risk Assessment",
        "authors": [
            "Marios Adamou",
            "Grigoris Antoniou",
            "Elissavet Greasidou",
            "Vincenzo Lagani",
            "Paulos Charonyktakis",
            "Ioannis Tsamardinos"
        ],
        "date": "July 2018",
        "source": "SETN '18: Proceedings of the 10th Hellenic Conference on Artificial Intelligence",
        "abstract": "Suicide has been considered as an important public health issue for a very long time, and is one of the main causes of death worldwide. Despite suicide prevention strategies being applied, the rate of suicide has not changed substantially over the past decades. Advances in machine learning make it possible to attempt to predict suicide based on the analysis of relevant data to inform clinical practice. This paper reports on findings from the analysis of data of patients who died by suicide in the period 2013-2016 and made use of both structured data and free-text medical notes. We focus on examining various text-mining approaches to support risk assessment. The results show that using advance machine learning and text-mining techniques, it is possible to predict within a specified period which people are most at risk of taking their own life at the time of referral to a mental health service.",
        "link": "https://dl.acm.org/doi/10.1145/3200947.3201020"
    },
    {
        "title": "Integration of data mining techniques in e-learning systems: Clustering Profil of Lerners and Recommender Course System",
        "authors": [
            "A. El Moustamid",
            "E. En-Naimi",
            "J. El Bouhdidi"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "Data mining aims at the extraction of knowledge from large quantities of data by automatic or semi-automatic methods. In the field of education and training e-learning, Data Mining techniques as clustering (The aim of this technique is to identify groups that are similar in some aspect) [21], prediction (is used for developing model where combination of data viewed from different aspects can effect single viewed data. Here, is important to watch how predictors variables are effecting predicted variable)[21], classification are applied in e-learning to improve the level of learners by providing learning according to the learners' requirement) are used to analyze learners profile, to predict and improve student performance. The aim of our project is to develop a system capable of analyzing learners profile and indexing web videos in order to offer learners a rich database with courses that correspond to their levels.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090453"
    },
    {
        "title": "Multi-level mining and visualization of scientific text collections: Exploring a bi-lingual scientific repository",
        "authors": [
            "Pablo Accuosto",
            "Francesco Ronzano",
            "Daniel Ferrés",
            "Horacio Saggion"
        ],
        "date": "December 2017",
        "source": "WOSP 2017: Proceedings of the 6th International Workshop on Mining Scientific Publications",
        "abstract": "We present a system to mine and visualize collections of scientific documents by semantically browsing information extracted from single publications or aggregated throughout corpora of articles. The text mining tool performs deep analysis of document collections allowing the extraction and interpretation of research paper's contents. In addition to the extraction and enrichment of documents with metadata (titles, authors, affiliations, etc), the deep analysis performed comprises semantic interpretation, rhetorical analysis of sentences, triple-based information extraction, and text summarization. The visualization components allow geographical-based exploration of collections, topic-evolution interpretation, and collaborative network analysis among others. The paper presents a case study of a bi-lingual collection in the field of Natural Language Processing (NLP).",
        "link": "https://dl.acm.org/doi/10.1145/3127526.3127529"
    },
    {
        "title": "Hierarchical Topic Mining via Joint Spherical Tree and Text Embedding",
        "authors": [
            "Yu Meng",
            "Yunyi Zhang",
            "Jiaxin Huang",
            "Yu Zhang",
            "Chao Zhang",
            "Jiawei Han"
        ],
        "date": "August 2020",
        "source": "KDD '20: Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Mining a set of meaningful topics organized into a hierarchy is intuitively appealing since topic correlations are ubiquitous in massive text corpora. To account for potential hierarchical topic structures, hierarchical topic models generalize flat topic models by incorporating latent topic hierarchies into their generative modeling process. However, due to their purely unsupervised nature, the learned topic hierarchy often deviates from users' particular needs or interests. To guide the hierarchical topic discovery process with minimal user supervision, we propose a new task, Hierarchical Topic Mining, which takes a category tree described by category names only, and aims to mine a set of representative terms for each category from a text corpus to help a user comprehend his/her interested topics. We develop a novel joint tree and text embedding method along with a principled optimization procedure that allows simultaneous modeling of the category tree structure and the corpus generative process in the spherical space for effective category-representative term discovery. Our comprehensive experiments show that our model, named JoSH, mines a high-quality set of hierarchical topics with high efficiency and benefits weakly-supervised hierarchical text classification tasks.",
        "link": "https://dl.acm.org/doi/10.1145/3394486.3403242"
    },
    {
        "title": "Unsupervised behavioural mining and clustering for malware family identification",
        "authors": [
            "Khanh Huu The Dam",
            "Thomas Given-Wilson",
            "Axel Legay"
        ],
        "date": "March 2021",
        "source": "SAC '21: Proceedings of the 36th Annual ACM Symposium on Applied Computing",
        "abstract": "More accurate and advanced detection and classification of malware requires exploiting program behaviour and not merely syntactic or static features. One approach is to use system call dependency graphs (SCDGs) that represent the program behaviour by interactions with the system, and the relations between these interactions. These SCDGs have been used with supervised learning techniques to very accurately detect and classify malware. This works considers the unsupervised learning challenge of mining for common clusters of behaviour without a priori knowledge. This allows for clustering of similar programs by behaviour, that can then be used for either classification or further analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3412841.3441919"
    },
    {
        "title": "Text Mining for Evaluating Authors' Birth and Death Years",
        "authors": [
            "Dror Moghaz",
            "Yaakov Hacohen-Kerner",
            "Dov Gabbay"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "This article presents a unique method in text and data mining for finding the era, i.e., mining temporal data, in which an anonymous author was living. Finding this era can assist in the examination of a fake document or extracting the time period in which a writer lived. The study and the experiments concern Hebrew, and in some parts, Aramaic and Yiddish rabbinic texts. The rabbinic texts are undated and contain no bibliographic sections, posing an interesting challenge. This work proposes algorithms using key phrases and key words that allow the temporal organization of citations together with linguistic patterns. Based on these key phrases, key words, and the references, we established several types of “Iron-clad,” Heuristic and Greedy rules for estimating the years of birth and death of a writer in an interesting classification task. Experiments were conducted on corpora, including documents authored by 12, 24, and 36 rabbinic writers and demonstrated promising results.",
        "link": "https://dl.acm.org/doi/10.1145/3281631"
    },
    {
        "title": "Transfer Learning to Timed Text Based Video Classification Using CNN",
        "authors": [
            "Zenun Kastrati",
            "Ali Shariq Imran",
            "Arianit Kurti"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "Open educational video resources are gaining popularity with a growing number of massive open online courses (MOOCs). This has created a niche for content providers to adopt effective solutions in automatically organizing and structuring of educational resources for maximum visibility. Recent advances in deep learning techniques are proving useful in managing and classifying resources into appropriate categories. This paper proposes one such convolutional neural network (CNN) model for classifying video lectures in a MOOC setting using a transfer learning approach. The model uses a time-aligned text transcripts corresponding to video lectures from six broader subject categories. Video lectures and their corresponding transcript dataset is gathered from the Coursera MOOC platform. Two different CNN models are proposed: i) CNN based classification using embeddings learned from our MOOC dataset, ii) CNN based classification using transfer learning. Word embeddings generated from two well known state-of-the-art pre-trained models Word2Vec and GloVe, are used in the transfer learning approach for the second case. The proposed CNN models are evaluated using precision, recall, and F1 score and the obtained performance is compared with both conventional and deep learning classifiers. The proposed CNN models have an F1 score improvement of 10-22 percentage points over DNN and conventional classifiers",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326483"
    },
    {
        "title": "Evolutionary role mining in complex networks by ensemble clustering",
        "authors": [
            "Sarvenaz Choobdar",
            "Pedro Ribeiro",
            "Fernando Silva"
        ],
        "date": "April 2017",
        "source": "SAC '17: Proceedings of the Symposium on Applied Computing",
        "abstract": "The structural patterns in the neighborhood of nodes assign unique roles to the nodes. Mining the set of existing roles in a network provides a descriptive profile of the network and draws its general picture. This paper proposes a new method to determine structural roles in a dynamic network based on the current position of nodes and their historic behavior. We develop a temporal ensemble clustering technique to dynamically find groups of nodes, holding similar tempo-structural roles. We compare two weighting functions, based on age and distribution of data, to incorporate temporal behavior of nodes in the role discovery. To evaluate the performance of the proposed method, we assess the results from two points of view: 1) goodness of fit to current structure of the network; 2) consistency with historic data. We conduct the evaluation using different ensemble clustering techniques. The results on real world networks demonstrate that our method can detect tempo-structural roles that simultaneously depict the topology of a network and reflect its dynamics with high accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3019612.3019815"
    },
    {
        "title": "Analyzing used-car web listings via text mining",
        "authors": [
            "Ayhan Demiriz",
            "Fatma Cantaş"
        ],
        "date": "October 2017",
        "source": "IML '17: Proceedings of the 1st International Conference on Internet of Things and Machine Learning",
        "abstract": "Used car trade is one of the major components of the world economies. It is not uncommon to sell a car by placing an internet advertisement irrespective of the geography in these days. A typical content of an advertisement is usually composed of two parts namely the structured and the free text data. The structured data may include some information about the asking price, make, model, year, mileage of the car and the contact info. In most cases, seller may give important clues about the car's current conditions in the free text data where the title (head) of the advertisement can be included as free text too. This paper reports preliminary results from a text mining study conducted on 75K used car internet listings collected from two major car listing web sites in Turkey. As expected, the words and the phrases related to the description of the car are observed to be frequent. The leading concepts in the free text are found to be regarding how to describe the current condition of a car, for example \"no crash history\".",
        "link": "https://dl.acm.org/doi/10.1145/3109761.3109782"
    },
    {
        "title": "Text classification of Diseases Treated by Traditional Chinese Medicine Prescription based on machine learning",
        "authors": [
            "Hanqing Zhao",
            "Bo Han",
            "Chen Li"
        ],
        "date": "September 2020",
        "source": "ISAIMS '20: Proceedings of the 1st International Symposium on Artificial Intelligence in Medical Sciences",
        "abstract": "OBJECTIVE: To explore the application of machine learning in th e identification of diseases treated by traditional Chinese medicine prescriptions. METHODS: Based on the composition of the text document of Chinese medicine prescriptions, the prescriptions were divided into cough, headache and diarrhea. THUTCT were introduced to establish and train two machine learning text classification models, LibSVM and LibLinear, and the prescriptions to be deter mined were put into the model for classification and prediction. R ESULTS: The Precision rate of LibSVM model and Liblinear mod el were 0.7283 and 0.6690. Seven prescriptions were classified an d predicted, and the results were in line with expectations. CONCLUSION: THUCTC has good universality for the content of TCM prescriptions, high classification accuracy and fast testing speed, which is suitable for the text classification and discrimination research of TCM prescriptions for diseases.",
        "link": "https://dl.acm.org/doi/10.1145/3429889.3429895"
    },
    {
        "title": "Evaluation of retweet clustering method classification method using retweets on Twitter without text data",
        "authors": [
            "K. Uchida",
            "F. Toriumi",
            "T. Sakaki"
        ],
        "date": "August 2017",
        "source": "WI '17: Proceedings of the International Conference on Web Intelligence",
        "abstract": "Burst phenomena, which frequently occur on social media, are caused by such social events as flaming on the internet, elections, and natural disasters. To understand people's thoughts and feelings, we must classify their opinions from burst phenomena. Therefore, classification methods that categorize tweets are critical. However, since most classification methods focus on text mining, they cannot group tweets by topics because each tweet has poor linguistic similarities. We used a non-text-based classification method proposed by Baba et al. that groups tweets by topics, even if they have poor linguistic similarities, and verified its validity by comparing it with a text-based classification method in two different evaluations: qualitative and quantitative. In the qualitative evaluation part, we did a questionnaire survey and validated the suitability of the topic clusters created using both the non-and text-based methods. Since evaluating the similarity of every pair of tweets in each topic is difficult, we evaluated the similarity between sampled pairs in the survey and acquired more appropriate topic clustering results using the non-text-based method than the text-based method. In the quantitative evaluation part, we focused on the robustness of each method against data reduction. Many approaches analyze social media data, especially because collecting data from social media is comparatively easy. However, since collecting the whole data of burst phenomena is very costly due to the vast amounts of available social media data, robustness against data reduction is an important index to evaluate classification methods. With the non-text-based method, over 55% of the pairs of tweets in the same cluster were also included in the same cluster even when the data were reduced to 10% in all three of our example cases. In this paper, as a source we focus on Twitter, one of the most popular microblogging services. Using clustering to conduct detailed case analyses, we scrutinized three burst cases that include natural disasters and flaming on the internet and found that a non-text-based method more effectively classified tweets in burst phenomena than a text-based method.",
        "link": "https://dl.acm.org/doi/10.1145/3106426.3106451"
    },
    {
        "title": "Using Spark for Text Mining on Large Scale Liver Cancer Literature",
        "authors": [
            "Ming-Yen Lin",
            "Yu-Ju Lin",
            "Sue-Chen Hsueh"
        ],
        "date": "January 2021",
        "source": "BDET 2021: 2021 the 3rd International Conference on Big Data Engineering and Technology (BDET)",
        "abstract": "Cancer is one of the main causes of death. The number of known cancer to-date is more than one hundred. Liver cancer, ranked after trachea and lung cancer, has long been high in the cancer leading cause in Taiwan and even ranked as second in 2016. The number of scientific articles related to cancer proliferates every year. The number reaches as high as 20 million in PubMed so that discovering useful information from the massive collection is very difficult. In addition, using a single machine to sift through these articles is very time-consuming. Therefore, we present a big data analytic framework using the distributed Apache Spark platform for text mining in PubMed literature. We establish a prediction model for liver cancer articles so that researchers may effectively validate whether an article is related to liver cancer or not. Classification models in Spark MLlib including Linear Support Vector Machines (SVM), Logistic Regression, are used in our experiments. Relevancy to liver cancer is further confirmed by using MeSH (Medical Subject Headings) terms. Logistic regression is about 3 times faster than SVMs and the accuracy of both methods is close to 95% in the experiments using hold-out validation. When max_features is 500 and min_df ≤ 0.1 (or min_df = 1), the accuracy may reach 96%. In the experiments with K-fold cross-validation, the accuracy of SVMs methods is 96%. The experimental results show that that our prediction model may effectively classify liver cancer articles.",
        "link": "https://dl.acm.org/doi/10.1145/3474944.3474958"
    },
    {
        "title": "Automatic topic classification of test cases using text mining at an Android smartphone vendor",
        "authors": [
            "Junji Shimagaki",
            "Yasutaka Kamei",
            "Naoyasu Ubayashi",
            "Abram Hindle"
        ],
        "date": "October 2018",
        "source": "ESEM '18: Proceedings of the 12th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Background: An Android smartphone is an ecosystem of applications, drivers, operating system components, and assets. The volume of the software is large and the number of test cases needed to cover the functionality of an Android system is substantial. Enormous effort has been already taken to properly quantify \"what features and apps were tested and verified?\". This insight is provided by dashboards that summarize test coverage and results per feature. One method to achieve this is to manually tag or label test cases with the topic or function they cover, much like function points. At the studied Android smartphone vendor, tests are labelled with manually defined tags, so-called \"feature labels (FLs)\", and the FLs serve to categorize 100s to 1000s test cases into 10 to 50 groups. Aim: Unfortunately for developers, manual assignment of FLs to 1000s of test cases is a time consuming task, leading to inaccurately labeled test cases, which will render the dashboard useless. We created an automated system that suggests tags/labels to the developers for their test cases rather than manual labeling. Method: We use machine learning models to predict and label the functionality tested by 10,000 test cases developed at the company. Results: Through the quantitative experiments, our models achieved acceptable F-1 performance of 0.3 to 0.88. Also through the qualitative studies with expert teams, we showed that the hierarchy and path of tests was a good predictor of a feature's label. Conclusions: We find that this method can reduce tedious manual effort that software developers spent classifying test cases, while providing more accurate classification results.",
        "link": "https://dl.acm.org/doi/10.1145/3239235.3268927"
    },
    {
        "title": "To apply Data Mining for Classification of Crowd sourced Software Requirements",
        "authors": [
            "Soonh Taj",
            "Qasim Arain",
            "Imran Memon",
            "Asma Zubedi"
        ],
        "date": "April 2019",
        "source": "ICSIE '19: Proceedings of the 8th International Conference on Software and Information Engineering",
        "abstract": "Now a day's main focus of developers is to build quality software that works according to customer needs and for this reason it is necessary to gather right requirements as requirement elicitation is the critical step that impacts on the success of software project as misinterpreted requirements leads to the failure of software project. By keeping this in mind a research is carried out on improving requirements elicitation process and automating the process of classifying requirements. In this research, a model is proposed which will help in this scenario for requirements elicitation and requirement classification. This paper presents a model in which crowd sourcing approach is used so that customers, end users, stakeholders, developers and software engineers can make active participation for requirement elicitation process and requirements gathered using crowdsourcing approach are used by model for classification process i.e. classification of requirements into functional and non-functional requirements. For the proof of proposed model a case study is conducted. Results of case study provided the usefulness and efficiency of proposed model for classification of crowd sourced software requirements.",
        "link": "https://dl.acm.org/doi/10.1145/3328833.3328837"
    },
    {
        "title": "Text classification using Fuzzy TF-IDF and Machine Learning Models",
        "authors": [
            "Mariem Bounabi",
            "Karim El Moutaouakil",
            "Khalid Satori"
        ],
        "date": "October 2019",
        "source": "BDIoT'19: Proceedings of the 4th International Conference on Big Data and Internet of Things",
        "abstract": "The representation of the information has an important impact on the text classification task. Several weighting methods were proposed in the literature, and the term frequency-inverse term frequency (TFIDF), the most know on the text treatment field. The FTF-IDF is a vector representation where the components of the TFIDF are presented as inputs to the Fuzzy Inference System (FIS). In this work, we compare several Machin Learning algorithms such as Naïve Bayes and its derivatives, SVM and Random forest classifiers, using the FTF-IDF representation. To improve the quality of the used classifiers, we call sum attribute selection methods. The recognition rate, for the tested systems, is satisfied, where the system based on naïve Bayes classifier, the FTF-IDF weighting terms, and the info gain select attributes method gives 98.7% as accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3372938.3372956"
    },
    {
        "title": "Text Mining in Cybersecurity: A Systematic Literature Review",
        "authors": [
            "Luciano Ignaczak",
            "Guilherme Goldschmidt",
            "Cristiano André Da Costa",
            "Rodrigo Da Rosa Righi"
        ],
        "date": null,
        "source": "ACM Computing Surveys",
        "abstract": "The growth of data volume has changed cybersecurity activities, demanding a higher level of automation. In this new cybersecurity landscape, text mining emerged as an alternative to improve the efficiency of the activities involving unstructured data. This article proposes a Systematic Literature Review (SLR) to present the application of text mining in the cybersecurity domain. Using a systematic protocol, we identified 2,196 studies, out of which 83 were summarized. As a contribution, we propose a taxonomy to demonstrate the different activities in the cybersecurity domain supported by text mining. We also detail the strategies evaluated in the application of text mining tasks and the use of neural networks to support activities involving unstructured data. The work also discusses text classification performance aiming its application in real-world solutions. The SLR also highlights open gaps for future research, such as the analysis of non-English content and the intensification in the usage of neural networks.",
        "link": "https://dl.acm.org/doi/10.1145/3462477"
    },
    {
        "title": "Using Text Mining to Analyze the Financial Patents",
        "authors": [
            "Yung-Feng Lu",
            "Jia-Lang Xu",
            "Mu-Yen Chen"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "With the advancement and development of science and technology, Nowadays society is the development of information technology, cloud computing, mobile Internet, Internet of Things, block chain and information technology.The combination of finance and technology has made financial technology terms gradually emerging.. This research analyzes the patents of financial technology. However, Taiwan's financial technology patents are mostly G06Q-20 (payment plan, architecture or agreement), G06Q-30 (commercial, such as marketing, shopping, payment, auction or e-commerce). G06Q-40 (financial, investment or tax treatment, insurance).The research use text mining technology to analyze and understand Taiwan's trends and developments in financial technology. The research results show that G06Q-20 patents are biased towards information security and mobile payment. G06Q-30 patents favor shopping websites and advertising. G06Q-40 patents are biased towards diversity, such as securities, insurance, finance, and stocks.",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326478"
    },
    {
        "title": "Identifying Online Profiles of Distance Learning Students Using Data Mining Techniques",
        "authors": [
            "Osama Islam",
            "Muazzam Siddiqui",
            "Naif Radi Aljohani"
        ],
        "date": "October 2019",
        "source": "ICDTE '19: Proceedings of the 3rd International Conference on Digital Technology in Education",
        "abstract": "Educational data has grown over the years with the increased use of technology within educational environments. This has led to a huge amount of data being stored in various data sources representing the student, his/her activities, and other aspects relevant to the learning process. To meet this analytical need, Educational Data Mining (EDM) has emerged to assist educational institutions in identifying key benefits such as students at risk, the level of student engagement or predicting student performance. The aim of this research was to explore the various aspects of student interaction data using data mining techniques to identify relevant patterns of behaviors and possible key attributes that have higher degrees of influence on distance learning students. The main findings identified several patterns of user online profiles based on a set of adopted learning strategies, the research proposed a framework for analyzing such interaction data based on R and Hadoop platforms to correlate online profiles with student performance.",
        "link": "https://dl.acm.org/doi/10.1145/3369199.3369249"
    },
    {
        "title": "Extremely Fast Decision Tree Mining for Evolving Data Streams",
        "authors": [
            "Albert Bifet",
            "Jiajin Zhang",
            "Wei Fan",
            "Cheng He",
            "Jianfeng Zhang",
            "Jianfeng Qian",
            "Geoff Holmes",
            "Bernhard Pfahringer"
        ],
        "date": "August 2017",
        "source": "KDD '17: Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
        "abstract": "Nowadays real-time industrial applications are generating a huge amount of data continuously every day. To process these large data streams, we need fast and efficient methodologies and systems. A useful feature desired for data scientists and analysts is to have easy to visualize and understand machine learning models. Decision trees are preferred in many real-time applications for this reason, and also, because combined in an ensemble, they are one of the most powerful methods in machine learning. In this paper, we present a new system called STREAMDM-C++, that implements decision trees for data streams in C++, and that has been used extensively at Huawei. Streaming decision trees adapt to changes on streams, a huge advantage since standard decision trees are built using a snapshot of data, and can not evolve over time. STREAMDM-C++ is easy to extend, and contains more powerful ensemble methods, and a more efficient and easy to use adaptive decision trees. We compare our new implementation with VFML, the current state of the art implementation in C, and show how our new system outperforms VFML in speed using less resources.",
        "link": "https://dl.acm.org/doi/10.1145/3097983.3098139"
    },
    {
        "title": "Data miners' little helper: data transformation activity cues for cluster analysis on document collections",
        "authors": [
            "Tania Cerquitelli",
            "Evelina Di Corso",
            "Francesco Ventura",
            "Silvia Chiusano"
        ],
        "date": "June 2017",
        "source": "WIMS '17: Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "In this paper we propose a new self-learning engine to streamline the analytics process, as it enables analysts to mine massive data repositories with minimal user intervention. In the context of cluster analysis on a collection of documents this new system, named SELF-DATA (SELF-learning DAta TrAnsformation), suggests to the analyst how to configure the whole mining process for a given dataset. SELF-DATA relies on an engine exploring different data weighting schemas (e.g., normalized term frequencies) and data transformation methods (e.g., PCA) before applying the cluster analysis, evaluating and comparing solutions through different quality indices (e.g., weighted Silhouette), and presenting the k-top solutions to the analyst. SELF-DATA will also include a knowledge base storing results of experiments on previously processed datasets, and a classification algorithm trained on the knowledge base content to forecast the best configuration for the whole mining process for an unexplored dataset. The first development of SELF-DATA running on Apache Spark has been validated on 5 collections of documents. Experimental results highlight that TF-IDF and logarithmic entropy are effective to measure item relevance with sparse datasets, and the LSI method outperforms PCA with a large dictionary.",
        "link": "https://dl.acm.org/doi/10.1145/3102254.3102288"
    },
    {
        "title": "Interactive Visual Graph Mining and Learning",
        "authors": [
            "Ryan A. Rossi",
            "Nesreen K. Ahmed",
            "Rong Zhou",
            "Hoda Eldardiry"
        ],
        "date": null,
        "source": "ACM Transactions on Intelligent Systems and Technology",
        "abstract": "This article presents a platform for interactive graph mining and relational machine learning called GraphVis. The platform combines interactive visual representations with state-of-the-art graph mining and relational machine learning techniques to aid in revealing important insights quickly as well as learning an appropriate and highly predictive model for a particular task (e.g., classification, link prediction, discovering the roles of nodes, and finding influential nodes). Visual representations and interaction techniques and tools are developed for simple, fast, and intuitive real-time interactive exploration, mining, and modeling of graph data. In particular, we propose techniques for interactive relational learning (e.g., node/link classification), interactive link prediction and weighting, role discovery and community detection, higher-order network analysis (via graphlets, network motifs), among others. GraphVis also allows for the refinement and tuning of graph mining and relational learning methods for specific application domains and constraints via an end-to-end interactive visual analytic pipeline that learns, infers, and provides rapid interactive visualization with immediate feedback at each change/prediction in real-time. Other key aspects include interactive filtering, querying, ranking, manipulating, exporting, as well as tools for dynamic network analysis and visualization, interactive graph generators (including new block model approaches), and a variety of multi-level network analysis techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3200764"
    },
    {
        "title": "Data Mining and Opinion Mining: A Tool in Educational Context",
        "authors": [
            "Myriam Peñafiel",
            "Stefanie Vásquez",
            "Diego Vásquez",
            "Juan Zaldumbide",
            "Sergio Luján-Mora"
        ],
        "date": "July 2018",
        "source": "ICoMS '18: Proceedings of the 2018 1st International Conference on Mathematics and Statistics",
        "abstract": "The use of the web as a universal communication platform generates large volumes of data (Big data), which in many cases, need to be processed so that they can become useful knowledge in face of the sceptics who have doubts about the credibility of such information. The use of web data that comes from educational contexts needs to be addressed, since that large amount of unstructured information is not being valued, losing valuable information that can be used. To solve this problem, we propose the use of data mining techniques such as sentiment analysis to validate the information that comes from the educational platforms. The objective of this research is to propose a methodology that allows the user to apply sentiment analysis in a simple way, because although some researchers have done it, very few do with data in the educational context. The results obtained prove that the proposal can be used in similar cases.",
        "link": "https://dl.acm.org/doi/10.1145/3274250.3274263"
    },
    {
        "title": "Research Hotspots Mining and Visualized Analysis Based on Linking Cluster and K-Core Decomposition",
        "authors": [
            "Ying Cai",
            "Fang Huang",
            "Shuai Wang",
            "Hao Zhang",
            "Chunxiu Du"
        ],
        "date": "May 2018",
        "source": "ICDPA 2018: Proceedings of the International Conference on Data Processing and Applications",
        "abstract": "In this paper, we focus on research hotspots mining with knowledge semantic features on discipline topic word networks, which are extracted from scientific and technical documents. For the topic networks, a novel method of combining linking community clustering with k-core decomposition is proposed to mine the hotspot community with knowledge hierarchy structure. And as a post-processing, the hierarchical community identification is implemented by k-core decomposition in order to discover the internal connection with knowledge semantic hierarchy in the research hotspot. Finally, the community density and visualization method is utilized to analyze the core and sub- problem in the research hotspot community. In experiments, the topic networks for a single discipline and interdisciplinary research hotspot are taken as examples of analysis, and the results show the proposed approach can effectively realize the objective of mining and analyzing the discipline research hotspot on the academic topic word network.",
        "link": "https://dl.acm.org/doi/10.1145/3224207.3224216"
    },
    {
        "title": "The Application of Data Mining Algorithms in the Construction of Travel Recommendation System",
        "authors": [
            "Chao Lou",
            "Zhaonan Mu",
            "Mengzhu Liu"
        ],
        "date": "January 2021",
        "source": "CONF-CDS 2021: The 2nd International Conference on Computing and Data Science",
        "abstract": "Photos shared by users of online social network often associated with tags, texts, geographic and time information. These data are ideal research resource which can be used by researchers to observe people's interests and behaviors. They can also be used to analyze people's travel behaviors and interests because it can reflect people's travel activities and experiences directly. This project aimed to consolidate people into groups based on their travel preferences of travel season, travel time span, scenery type of travel location and then analyze popular travel places for different people groups. The travel preferences were analyzed from extracted data applying PHP language and Naive Bayes classification approach. According to the analyzed data, people were divided into groups. Then, DBSCAN clustering was used to get popular locations based on the different groups. As a result, 80 thousand photo data of 5 cities have been collected from Flickr by Phython. It has been showed that people's preference of season in most of the mined cities was spring and summer and the preference of time span was short. Moreover, mined people has been divided into 16 groups according to their preference and based on the groups popular locations have been calculated by DBSCAN clustering. Theses locations have been provided in a recommendation website which was built in this project. For the future work, mining Flickr data associated with other travel social network sites could be a good way to build a more effective recommendation system.",
        "link": "https://dl.acm.org/doi/10.1145/3448734.3450477"
    },
    {
        "title": "Compare Machine Learning Models in Text Classification Using Steam User Reviews",
        "authors": [
            "Youchen Miao",
            "Zeyu Jin",
            "Yumeng Zhang",
            "Yuchen Chen",
            "Junren Lai"
        ],
        "date": "November 2021",
        "source": "ICSED '21: Proceedings of the 2021 3rd International Conference on Software Engineering and Development",
        "abstract": "Text Classification and Sentiment Analysis of game reviews are viewed as important parts in not only academic fields but also in game studies. In this paper, with more than 400 thousand game reviews on Steam platform, we preprocess the data using different libraries (sklearn, nltk, and spaCy) and use them as inputs to build three sentiment classification models based on different algorithms (Naive Bayes, SVM, and Random Forest). In contrast to previous studies that only focus on different sentiment analysis models, our paper also highlights the use of different APIs to preprocess the data and their corresponding model performance. The results show that no matter which API we choose, Random Forest models always perform the best. However, in terms of training time, Naive Bayes is the fastest. This work can be used to apply grid search for researchers to automatically find the optimum API before conducting sentiment analysis in the future.",
        "link": "https://dl.acm.org/doi/10.1145/3507473.3507480"
    },
    {
        "title": "Lost in Transduction: Transductive Transfer Learning in Text Classification",
        "authors": [
            "Alejandro Moreo",
            "Andrea Esuli",
            "Fabrizio Sebastiani"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Obtaining high-quality labelled data for training a classifier in a new application domain is often costly. Transfer Learning\n(a.k.a. “Inductive Transfer”) tries to alleviate these costs by transferring, to the “target” domain of interest, knowledge available from a different “source” domain. In transfer learning the lack of labelled information from the target domain is compensated by the availability at training time of a set of unlabelled examples from the target distribution. Transductive Transfer Learning denotes the transfer learning setting in which the only set of target documents that we are interested in classifying is known and available at training time. Although this definition is indeed in line with Vapnik’s original definition of “transduction”, current terminology in the field is confused. In this article, we discuss how the term “transduction” has been misused in the transfer learning literature, and propose a clarification consistent with the original characterization of this term given by Vapnik. We go on to observe that the above terminology misuse has brought about misleading experimental comparisons, with inductive transfer learning methods that have been incorrectly compared with transductive transfer learning methods. We then, give empirical evidence that the difference in performance between the inductive version and the transductive version of a transfer learning method can indeed be statistically significant (i.e., that knowing at training time the only data one needs to classify indeed gives an advantage). Our clarification allows a reassessment of the field, and of the relative merits of the major, state-of-the-art algorithms for transfer learning in text classification.",
        "link": "https://dl.acm.org/doi/10.1145/3453146"
    },
    {
        "title": "Machine Learning on Mining Potential Adverse Drug Reactions for Pharmacovigilance",
        "authors": [
            "Ke Jia"
        ],
        "date": "July 2021",
        "source": "DSIT 2021: 2021 4th International Conference on Data Science and Information Technology",
        "abstract": "Drugs are related to the lives and health of patients. Mining and discovering potential adverse drug reactions from a large number of unstructured texts play an important role in drug research and pharmacovigilance. It is a hot research issue in the field of biomedicine in recent years. A large number of scholars have discovered a large amount of information on adverse drug reactions from different data sources through effective machine learning methods. This paper reviews the research progress in recent years to provide a reference for future research.",
        "link": "https://dl.acm.org/doi/10.1145/3478905.3478964"
    },
    {
        "title": "TACAM: Topic And Context Aware Argument Mining",
        "authors": [
            "Michael Fromm",
            "Evgeniy Faerman",
            "Thomas Seidl"
        ],
        "date": "October 2019",
        "source": "WI '19: IEEE/WIC/ACM International Conference on Web Intelligence",
        "abstract": "In this work we address the problem of argument search. The purpose of argument search is the distillation of pro and contra arguments for requested topics from large text corpora. In previous works, the usual approach is to use a standard search engine to extract text parts which are relevant to the given topic and subsequently use an argument recognition algorithm to select arguments from them. The main challenge in the argument recognition task, which is also known as argument mining, is that often sentences containing arguments are structurally similar to purely informative sentences without any stance about the topic. In fact, they only differ semantically. Most approaches use topic or search term information only for the first search step and therefore assume that arguments can be classified independently of a topic. We argue that topic information is crucial for argument mining, since the topic defines the semantic context of an argument. Precisely, we propose different models for the classification of arguments, which take information about a topic of an argument into account. Moreover, to enrich the context of a topic and to let models understand the context of the potential argument better, we integrate information from different external sources such as Knowledge Graphs or pre-trained NLP models. Our evaluation shows that considering topic information, especially in connection with external information, provides a significant performance boost for the argument mining task.",
        "link": "https://dl.acm.org/doi/10.1145/3350546.3352506"
    },
    {
        "title": "Text mining for incoming tasks based on the urgency/importance factors and task classification using machine learning tools",
        "authors": [
            "Yasser Ali Alshehri"
        ],
        "date": "March 2020",
        "source": "ICCDA 2020: Proceedings of the 2020 the 4th International Conference on Compute and Data Analysis",
        "abstract": "In workplaces, there is a massive amount of unstructured data from different sources. In this paper, we present a case study that explains how can through communications between employees, we can help to prioritize tasks requests to increase the efficiency of their works for both technical and non-technical workers. This involves managing daily incoming tasks based on their level of urgency and importance.To allow all workers to utilize the urgency-importance matrix as a time-management tool, we need to automate this tool. The textual content of incoming tasks are analyzed, and metrics related to urgency and importance are extracted. A third factor (i.e., the response variable) is defined based on the two input variables (urgency and importance). Then, machine learning applied to the data to predict the class of incoming tasks based on data outcome desired. We used ordinal regression, neural networks, and decision tree algorithms to predict the four levels of task priority. We measure the performance of all using recalls, precisions, and F-scores. All classifiers perform higher than 89% in terms of all measures.",
        "link": "https://dl.acm.org/doi/10.1145/3388142.3388153"
    },
    {
        "title": "Text mining for malware classification using multivariate all repeated patterns detection",
        "authors": [
            "Konstantinos F. Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2019",
        "source": "ASONAM '19: Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Mobile phones have become nowadays a commodity to the majority of people. Using them, people are able to access the world of Internet and connect with their friends, their colleagues at work or even unknown people with common interests. This proliferation of the mobile devices has also been seen as an opportunity for the cyber criminals to deceive smartphone users and steel their money directly or indirectly, respectively, by accessing their bank accounts through the smartphones or by blackmailing them or selling their private data such as photos, credit card data, etc. to third parties. This is usually achieved by installing malware to smartphones masking their malevolent payload as a legitimate application and advertise it to the users with the hope that mobile users will install it in their devices. Thus, any existing application can easily be modified by integrating a malware and then presented it as a legitimate one. In response to this, scientists have proposed a number of malware detection and classification methods using a variety of techniques. Even though, several of them achieve relatively high precision in malware classification, there is still space for improvement. In this paper, we propose a text mining all repeated pattern detection method which uses the decompiled files of an application in order to classify a suspicious application into one of the known malware families. Based on the experimental results using a real malware dataset, the methodology tries to correctly classify (without any misclassification) all randomly selected malware applications of 3 categories with 3 different families each.",
        "link": "https://dl.acm.org/doi/10.1145/3341161.3350841"
    },
    {
        "title": "Comparative Study between Traditional Machine Learning and Deep Learning Approaches for Text Classification",
        "authors": [
            "Cannannore Nidhi Kamath",
            "Syed Saqib Bukhari",
            "Andreas Dengel"
        ],
        "date": "August 2018",
        "source": "DocEng '18: Proceedings of the ACM Symposium on Document Engineering 2018",
        "abstract": "In this contemporaneous world, it is an obligation for any organization working with documents to end up with the insipid task of classifying truckload of documents, which is the nascent stage of venturing into the realm of information retrieval and data mining. But classification of such humongous documents into multiple classes, calls for a lot of time and labor. Hence a system which could classify these documents with acceptable accuracy would be of an unfathomable help in document engineering. We have created multiple classifiers for document classification and compared their accuracy on raw and processed data. We have garnered data used in a corporate organization as well as publicly available data for comparison. Data is processed by removing the stop-words and stemming is implemented to produce root words. Multiple traditional machine learning techniques like Naive Bayes, Logistic Regression, Support Vector Machine, Random forest Classifier and Multi-Layer Perceptron are used for classification of documents. Classifiers are applied on raw and processed data separately and their accuracy is noted. Along with this, Deep learning technique such as Convolution Neural Network is also used to classify the data and its accuracy is compared with that of traditional machine learning techniques. We are also exploring hierarchical classifiers for classification of classes and subclasses. The system classifies the data faster and with better accuracy than if done manually. The results are discussed in the results and evaluation section.",
        "link": "https://dl.acm.org/doi/10.1145/3209280.3209526"
    },
    {
        "title": "Identification of Overpricing in the Purchase of Medication by the Federal Government of Brazil, Using Text Mining and Clustering Based on Ontology",
        "authors": [
            "Marco Aurelio O. S. Correa",
            "Adriano Galindo Leal"
        ],
        "date": "August 2018",
        "source": "ICCBDC '18: Proceedings of the 2018 2nd International Conference on Cloud and Big Data Computing",
        "abstract": "Increasing the transparency level in his actions and spending is one of the primary duties of the Brazilian Federal Government. The creation of laws that oblige full disclose of all its expenditures through transparency portals enables citizens to supervise all government entities. However, only the dissemination of these data, without a definite standard or the availability of data analysis tools, does not guarantee that the citizen is empowered to play his role. Therefore hence, the objective of this work is to identify overprice in the acquisition of products purchased by the federal government of Brazil using the unstructured data available on the Transparency Portal. The last two-years' worth of purchasing data, available in the Transparency Portal, were extracted, processed and stored. Due to his diverse nature and high volume of data, this study focused only on medicines purchased by the Ministry of Health. Ontology-based text mining and clustering techniques were applied for automatic identification and classification of products. The processing of this information was done through text mining and clustering, based on the ontology registered in another database of the Brazilian government. Because of this work, a consolidated price base per medication was created to allow the identification of distortions in prices practised, facilitating the identification of cases that merit further investigation to unravel fraud to the treasury.",
        "link": "https://dl.acm.org/doi/10.1145/3264560.3264569"
    },
    {
        "title": "Computational Estimation by Scientific Data Mining with Classical Methods to Automate Learning Strategies of Scientists",
        "authors": [
            "Aparna S. Varde"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Experimental results are often plotted as 2-dimensional graphical plots (aka graphs) in scientific domains depicting dependent versus independent variables to aid visual analysis of processes. Repeatedly performing laboratory experiments consumes significant time and resources, motivating the need for computational estimation. The goals are to estimate the graph obtained in an experiment given its input conditions, and to estimate the conditions that would lead to a desired graph. Existing estimation approaches often do not meet accuracy and efficiency needs of targeted applications. We develop a computational estimation approach called AutoDomainMine that integrates clustering and classification over complex scientific data in a framework so as to automate classical learning methods of scientists. Knowledge discovered thereby from a database of existing experiments serves as the basis for estimation. Challenges include preserving domain semantics in clustering, finding matching strategies in classification, striking a good balance between elaboration and conciseness while displaying estimation results based on needs of targeted users, and deriving objective measures to capture subjective user interests. These and other challenges are addressed in this work. The AutoDomainMine approach is used to build a computational estimation system, rigorously evaluated with real data in Materials Science. Our evaluation confirms that AutoDomainMine provides desired accuracy and efficiency in computational estimation. It is extendable to other science and engineering domains as proved by adaptation of its sub-processes within fields such as Bioinformatics and Nanotechnology.",
        "link": "https://dl.acm.org/doi/10.1145/3502736"
    },
    {
        "title": "Where is the road for issue reports classification based on text mining?",
        "authors": [
            "Qiang Fan",
            "Yue Yu",
            "Gang Yin",
            "Tao Wang",
            "Huaimin Wang"
        ],
        "date": "November 2017",
        "source": "ESEM '17: Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Currently, open source projects receive various kinds of issues daily, because of the extreme openness of Issue Tracking System (ITS) in GitHub. ITS is a labor-intensive and time-consuming task of issue categorization for project managers. However, a contributor is only required a short textual abstract to report an issue in GitHub. Thus, most traditional classification approaches based on detailed and structured data (e.g., priority, severity, software version and so on) are difficult to adopt. In this paper, issue classification approaches on a large-scale dataset, including 80 popular projects and over 252,000 issue reports collected from GitHub, were investigated. First, four traditional text-based classification methods and their performances were discussed. Semantic perplexity (i.e., an issues description confuses bug-related sentences with nonbug-related sentences) is a crucial factor that affects the classification performances based on quantitative and qualitative study. Finally, A two-stage classifier framework based on the novel metrics of semantic perplexity of issue reports was designed. Results show that our two-stage classification can significantly improve issue classification performances.",
        "link": "https://dl.acm.org/doi/10.1109/ESEM.2017.19"
    },
    {
        "title": "Feature-based Facebook reviews process model for e-management using data mining",
        "authors": [
            "Anish Kumar Varudharajulu",
            "Yongsheng Ma"
        ],
        "date": "January 2019",
        "source": "IC4E '19: Proceedings of the 10th International Conference on E-Education, E-Business, E-Management and E-Learning",
        "abstract": "The data generated from online communication acts as potential gold mines for discovering knowledge for researchers. A large amount of data is also generated in the form of web documents, emails, blogs, and feedback, etc. Text analytics is being significantly employed to mine important information. Opinion mining is the process of extracting human thoughts and perceptions from unstructured texts. The showstopper for designing an opinion mining system for analyzing reviews arise from the fact that customer reviews are often noisy. These reviews are informally written. In addition, they are subjected to spelling mistakes, grammatical errors, improper punctuation and irrational capitalization. This paper focuses on analyzing the different classification and clustering algorithms aimed at extracting and consolidating opinions of customers from social media sites like Facebook, Twitter and through surveys, at multiple levels of granularity to monitor and measure customer satisfaction. Ours is an automated approach, in which the system aids in the process of knowledge assimilation for knowledge-based building and also performs the analytics. Domain experts ratify the knowledge base and also provide training datasets for the system to intuitively gather more instances for ratification. The system identifies opinion expressions as phrases containing opinion words, opinionated features and also opinion modifiers. These expressions are categorized as positive, negative or neutral. Opinion expressions are identified and categorized using localized linguistic techniques. Opinions can be congregated at any desired level of specificity i.e. feature level or product level, user level or service level, etc. We have developed a system based on this approach, which provides the user with a platform to analyze opinion expressions crawled from a set of pre-defined datasets.",
        "link": "https://dl.acm.org/doi/10.1145/3306500.3306514"
    },
    {
        "title": "Discriminative Topic Mining via Category-Name Guided Text Embedding",
        "authors": [
            "Yu Meng",
            "Jiaxin Huang",
            "Guangyuan Wang",
            "Zihan Wang",
            "Chao Zhang",
            "Yu Zhang",
            "Jiawei Han"
        ],
        "date": "April 2020",
        "source": "WWW '20: Proceedings of The Web Conference 2020",
        "abstract": "Mining a set of meaningful and distinctive topics automatically from massive text corpora has broad applications. Existing topic models, however, typically work in a purely unsupervised way, which often generate topics that do not fit users’ particular needs and yield suboptimal performance on downstream tasks. We propose a new task, discriminative topic mining, which leverages a set of user-provided category names to mine discriminative topics from text corpora. This new task not only helps a user understand clearly and distinctively the topics he/she is most interested in, but also benefits directly keyword-driven classification tasks. We develop CatE, a novel category-name guided text embedding method for discriminative topic mining, which effectively leverages minimal user guidance to learn a discriminative embedding space and discover category representative terms in an iterative manner. We conduct a comprehensive set of experiments to show that CatE mines high-quality set of topics guided by category names only, and benefits a variety of downstream applications including weakly-supervised classification and lexical entailment direction identification.",
        "link": "https://dl.acm.org/doi/10.1145/3366423.3380278"
    },
    {
        "title": "Comparative Study of Heart Disease Diagnosis Using Top Ten Data Mining Classification Algorithms",
        "authors": [
            "I. Ketut Agung Enriko"
        ],
        "date": "June 2019",
        "source": "ICFET '19: Proceedings of the 5th International Conference on Frontiers of Educational Technologies",
        "abstract": "Data mining has been used for many purposes, especially for prediction system. In healthcare, data mining algorithms often used in disease diagnosis. Meanwhile, heart disease is known as a primary cause of death over the years. Many studies have been performed in heart disease diagnosis using data mining methods. There are some popular data mining algorithms that can be used in heart disease diagnosis, for example, k-Nearest Neighbor, CART, and AdaBoost. The algorithms are used to analyze a sample of cardiovascular patients data and predict the heart disease type that they suffer. Some parameters are taken from the patient, including EKG morphology, blood pressure, and information about the existence of chest pain, shortness of breath, palpitation, and cold sweat. In this study, medical records data are collected from Harapan Kita Hospital and utilized as a dataset sample in this research. Top ten data mining classification algorithms are used in diagnosing heart disease from Harapan Kita Hospital data and examining their performance by checking the accuracy and speed.",
        "link": "https://dl.acm.org/doi/10.1145/3338188.3338220"
    },
    {
        "title": "Text Mining Approach for Identifying Research Trends",
        "authors": [
            "Snezhana Sulova"
        ],
        "date": "June 2021",
        "source": "CompSysTech '21: Proceedings of the 22nd International Conference on Computer Systems and Technologies",
        "abstract": "With the increase of unstructured data, the issues connected with automatic text processing, the categorization of documents and the discovery of topics have become objects of growing interest. In order to improve the process of grouping and processing research publications, we would like to propose a method based upon natural language processing. It is based on text mining technologies which aim to identify key tendencies in documents. It processes the content of publications by clustering and identifies the topics of each identified group. This analysis helps by identifying key tendencies as well as discovering emerging new areas of research. Publications from the research literature database, Scopus, were used to test the approach. The topic of the publications is “the application of digital technologies in the logistics business”. The experiments were completed using the RapidMiner Studio software.",
        "link": "https://dl.acm.org/doi/10.1145/3472410.3472433"
    },
    {
        "title": "Assessment of Vulnerability Severity using Text Mining",
        "authors": [
            "Georgios Spanos",
            "Lefteris Angelis",
            "Dimitrios Toloudis"
        ],
        "date": "September 2017",
        "source": "PCI '17: Proceedings of the 21st Pan-Hellenic Conference on Informatics",
        "abstract": "Software1 vulnerabilities are closely associated with information systems security, a major and critical field in today's technology. Vulnerabilities constitute a constant and increasing threat for various aspects of everyday life, especially for safety and economy, since the social impact from the problems that they cause is complicated and often unpredictable. Although there is an entire research branch in software engineering that deals with the identification and elimination of vulnerabilities, the growing complexity of software products and the variability of software production procedures are factors contributing to the ongoing occurrence of vulnerabilities, Hence, another area that is being developed in parallel focuses on the study and management of the vulnerabilities that have already been reported and registered in databases. The information contained in such databases includes, a textual description and a number of metrics related to vulnerabilities. The purpose of this paper is to investigate to what extend the assessment of the vulnerability severity can be inferred directly from the corresponding textual description, or in other words, to examine the informative power of the description with respect to the vulnerability severity. For this purpose, text mining techniques, i.e. text analysis and three different classification methods (decision trees, neural networks and support vector machines) were employed. The application of text mining to a sample of 70,678 vulnerabilities from a public data source shows that the description itself is a reliable and highly accurate source of information for vulnerability prioritization.",
        "link": "https://dl.acm.org/doi/10.1145/3139367.3139390"
    },
    {
        "title": "Clustering Algorithms for Spatial Data Mining",
        "authors": [
            "Chetashri Bhadane",
            "Ketan Shah"
        ],
        "date": "April 2020",
        "source": "ICGDA '20: Proceedings of the 2020 3rd International Conference on Geoinformatics and Data Analysis",
        "abstract": "With the advances in mobile and wireless technologies, there has been a rise in applications that track and share the users' geospatial data. People use several social networking sites such as Twitter, Facebook and Flickr, where they share their status updates. With the integration of Global Positioning System (GPS) with mobile phones, it is now possible to share one's locations on these social networks. GPS allows us to record and track a person's movement along with the timestamp. The data set obtained from these GPS logs is vast and is widely used to analyze the users' movement patterns. Specifically, we can find out significant locations based on the number of users present at that location and the time spent by them at such places. Once significant places have been identified, it is also possible to identify the semantic importance of these locations. This paper presents an overview of the clustering techniques used to find important places of interest using large GPS based mobility datasets. Four clustering algorithms, K-Means, DBSCAN, OPTICS and Hierarchical, are implemented, and performance is tested using real-time data of 50 users collected over 2--5 years. Performance summary depicts that K-Means and DBSCAN perform well for spatial data.",
        "link": "https://dl.acm.org/doi/10.1145/3397056.3397068"
    },
    {
        "title": "Feature Selection based Arabic Text Classification using Different Machine Learning Algorithms: Comparative Study",
        "authors": [
            "Sakina Rim Bennabi",
            "Zakaria Elberrichi"
        ],
        "date": "June 2020",
        "source": "ICIST '20: Proceedings of the 10th International Conference on Information Systems and Technologies",
        "abstract": "Feature selection is a method of data pre-processing widely used when mining large data, such as textual classification. Several studies have been conducted to compare the different methods of feature selection applied to corpora in English. Unfortunately, a small number of works concern the Arabic language. This article aims to present a comparative study of different feature selection techniques including: Chi2, the ANOVA method and mutual information, applied on a corpus in Arabic language, while also diversifying the machine learning algorithms (Naive Bayes, SVM and KNN). This experimental study has shown in general that reducing dimensionality with feature selection techniques has slightly affected the performance of textual classification, reducing the size of the corpus by up to 1%.",
        "link": "https://dl.acm.org/doi/10.1145/3447568.3448531"
    },
    {
        "title": "Intelligent Data Analysis using Optimized Support Vector Machine Based Data Mining Approach for Tourism Industry",
        "authors": [
            "Ms Promila Sharma",
            "Uma Meena",
            "Girish Kumar Sharma"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Data analysis involves the deployment of sophisticated approaches from data mining methods, information theory, and artificial intelligence in various fields like tourism, hospitality, and so on for the extraction of knowledge from the gathered and preprocessed data. In tourism, pattern analysis or data analysis using classification is significant for finding the patterns that represent new and potentially useful information or knowledge about the destination and other data. Several data mining techniques are introduced for the classification of data or patterns. However, overfitting, less accuracy, local minima, sensitive to noise are the drawbacks in some existing data mining classification methods. To overcome these challenges, Support vector machine with Red deer optimization (SVM-RDO) based data mining strategy is proposed in this article. Extended Kalman filter (EKF) is utilized in the first phase, i.e., data cleaning to remove the noise and missing values from the input data. Mantaray foraging algorithm (MaFA) is used in the data selection phase, in which the significant data are selected for the further process to reduce the computational complexity. The final phase is the classification, in which SVM-RDO is proposed to access the useful pattern from the selected data. PYTHON is the implementation tool used for the experiment of the proposed model. The experimental analysis is done to show the efficacy of the proposed work. From the experimental results, the proposed SVM-RDO achieved better accuracy, precision, recall, and F1 score than the existing methods for the tourism dataset. Thus, it is showed the effectiveness of the proposed SVM-RDO for pattern analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3494566"
    },
    {
        "title": "Applying text mining to predict learners' cognitive engagement",
        "authors": [
            "Hayati Hind",
            "Mohammed Khalidi Idrissi",
            "Samir Bennani"
        ],
        "date": "October 2017",
        "source": "SCAMS '17: Proceedings of the Mediterranean Symposium on Smart City Application",
        "abstract": "In academic process, engagement represent one of the important success' factor. This paper proposes a predictive system for learners' cognitive engagement based on their online discussion forums participation. In the first level the ontology OWL and the LSA method are used to perform a semantic classification system of the threads according to a specific context chosen by the tutor. Then the Text mining, as a predictive method, is applied to the classified threads and learners' participation in the forums.",
        "link": "https://dl.acm.org/doi/10.1145/3175628.3175655"
    },
    {
        "title": "Investigate Transitions into Drug Addiction through Text Mining of Reddit Data",
        "authors": [
            "John Lu",
            "Sumati Sridhar",
            "Ritika Pandey",
            "Mohammad Al Hasan",
            "Georege Mohler"
        ],
        "date": "July 2019",
        "source": "KDD '19: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Increasing rates of opioid drug abuse and heightened prevalence of online support communities underscore the necessity of employing data mining techniques to better understand drug addiction using these rapidly developing online resources. In this work, we obtained data from Reddit, an online collection of forums, to gather insight into drug use/misuse using text snippets from users narratives. Specifically, using users' posts, we trained a binary classifier which predicts a user's transitions from casual drug discussion forums to drug recovery forums. We also proposed a Cox regression model that outputs likelihoods of such transitions. In doing so, we found that utterances of select drugs and certain linguistic features contained in one's posts can help predict these transitions. Using unfiltered drug-related posts, our research delineates drugs that are associated with higher rates of transitions from recreational drug discussion to support/recovery discussion, offers insight into modern drug culture, and provides tools with potential applications in combating the opioid crisis.",
        "link": "https://dl.acm.org/doi/10.1145/3292500.3330737"
    },
    {
        "title": "Analysing digital banking reviews using text mining",
        "authors": [
            "Li Chen Cheng",
            "Legaspi Rhea Sharmayne"
        ],
        "date": "December 2020",
        "source": "ASONAM '20: Proceedings of the 12th IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Digital banks are new entrants in the banking industry in the Philippines as they only started late 2018. Since then, a handful of players have and are still emerging. With more and more people becoming technologically savvy, it is very critical for financial institutions to develop a digital banking application that will stand out from the competition. This paper aims to use text mining methods to analyse digital banking application reviews. This study will perform topic modelling using LDA to explore customer concerns and will mine association rules between the digital banking features with the review score. The results will reveal which areas the digital banking application can further optimize for customer satisfaction and retention.",
        "link": "https://dl.acm.org/doi/10.1109/ASONAM49781.2020.9381429"
    },
    {
        "title": "Mining Moocs Videos Metadata Using Classification Techniques",
        "authors": [
            "El Harrak Othman",
            "Ghadi Abderrahim",
            "El Bouhdidi Jaber"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "In few years, the internet has become the first source of information for most people, Today MOOCs makes it possible for everyone to access to the education over the world, it's represents an emerging methodology of online teaching and an important development in open education. Due to the rapid development of Moocs and the rapid growth of digital data and video database over the Internet, it is becoming very difficult for learners to browsing and choosing the best training for them. The scientific community has increased the amount of research into new technologies, with a view to improve the digital video utilization: its archiving, indexing, accessibility, acquisition, store and even its process and usability. All these parts of the video utilization entail the necessity of the extraction of all important information of a video, especially in cases of lack of metadata information. Web video mining is retrieving the content using data mining techniques from World Wide Web. There are two approaches for web video mining using traditional image processing (signal processing) and metadata based approach. In this paper, we present the various research makes in this subject and we propose an effective methodology to extract the metadata from Moocs videos and classify them based on the extracted metadata by applying data mining techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090450"
    },
    {
        "title": "Text mining with HathiTrust",
        "authors": [
            "Eleanor Dickson Koehl",
            "Ryan Dubnicek"
        ],
        "date": "June 2019",
        "source": "JCDL '19: Proceedings of the 18th Joint Conference on Digital Libraries",
        "abstract": "This tutorial will introduce attendees to the HathiTrust Research Center and its tools and services for computational text analysis research. HTRC leverages the scope and scale of the HathiTrust Digital Library collection to create opportunities for researchers to perform text data mining on subsets of the corpus. Attendees will develop skills that will allow them to conduct basic text analysis research using HathiTrust data, learn techniques for engaging with scholars on their text data mining research, and explore strategies for computational access to large-scale digital collections.",
        "link": "https://dl.acm.org/doi/10.1109/JCDL.2019.00115"
    },
    {
        "title": "Text mining for plagiarism detection: multivariate pattern detection for recognition of text similarities",
        "authors": [
            "Konstantinos Xylogiannopoulos",
            "Panagiotis Karampelas",
            "Reda Alhajj"
        ],
        "date": "August 2018",
        "source": "ASONAM '18: Proceedings of the 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "The problem of plagiarism the recent years has been intensified by the availability of information in digital form and the accessibility of the electronic libraries through the Internet. As a result, plagiarism detection has been transformed into a big data analytics problem since the number of digital sources is extravagant and a new document needs to be compared with millions of other existing documents. In this paper, a text mining methodology is proposed that can detect all common patterns between a document and the documents in a reference database. The technique is based on a pattern detection algorithm and the corresponding data structure that enables the algorithm to detect all common patterns. The methodology has been applied in a well-defined dataset providing very promising results identifying difficult cases of plagiarism such as technical disguise.",
        "link": "https://dl.acm.org/doi/10.5555/3382225.3382424"
    },
    {
        "title": "Petroleum Engineering Data Text Classification Using Convolutional Neural Network Based Classifier",
        "authors": [
            "Tzuhan Hsu",
            "Yaoqin Zhang"
        ],
        "date": "May 2018",
        "source": "ICMLT '18: Proceedings of the 2018 International Conference on Machine Learning Technologies",
        "abstract": "In this paper, authors introduce a convolutional neural network (CNN) based classification approach for petroleum engineering event. Recently, text classification problem has raised substantial attention in the field of machine learning. The aim of text classification process is to assign correct label to a text on the basis of its content. However, due to the imprecise of feature representation, most of former text classification methods cannot export satisfying result. To solve this problem, authors propose a model based on convolutional neural network. Authors conduct experiments with real world petroleum engineering data text, artificially labelled into 6 categories. Beside purposed CNN-based model, the authors also implement other baseline machine learning approaches: k nearest neighbor, random forest, support vector machines, recurrent neural network, gated recurrent units, long short-term memory. Among all these models, our CNN-based model reach best performance in text classification task. In the last part of this paper, authors compare and analysis their purpose and baseline model.",
        "link": "https://dl.acm.org/doi/10.1145/3231884.3231898"
    },
    {
        "title": "Classifying Indonesian Online Articles as Advertisement Placement Base Using Text Mining",
        "authors": [
            "Nadhira Tasya",
            "Arian Dhini"
        ],
        "date": "July 2017",
        "source": "ICBIM 2017: Proceedings of the International Conference on Business and Information Management",
        "abstract": "Rapid development in technological aspect resulting in growing level of human needs for the latest news, so that emerged a new trend of publishing and accessing news through online media or usually called online journalism. In addition, the number of people who sell and purchase through online sites also continues to increase and this opportunity is utilized by the company and the advertiser by implementing targeted web advertising. However, the high number of articles that have been published and accessed leads to great opportunities for errors in determining where to place the ads. Therefore, it needs a system that can categorize articles accessed by users as the basis of advertisement placement by the company and this classification system can be done by applying the method of Data Mining and Text Mining. This research uses document data in the form of article content that will be categorized into twenty categories of class of advertisement by using Text Mining technique with Support Vector Machine algorithm. The results of this study may be used by companies or advertisers as a basis for placement of ads on selected online media sites.",
        "link": "https://dl.acm.org/doi/10.1145/3134271.3134288"
    },
    {
        "title": "Mining Free-Text Medical Notes for Suicide Risk Assessment",
        "authors": [
            "Marios Adamou",
            "Grigoris Antoniou",
            "Elissavet Greasidou",
            "Vincenzo Lagani",
            "Paulos Charonyktakis",
            "Ioannis Tsamardinos"
        ],
        "date": "July 2018",
        "source": "SETN '18: Proceedings of the 10th Hellenic Conference on Artificial Intelligence",
        "abstract": "Suicide has been considered as an important public health issue for a very long time, and is one of the main causes of death worldwide. Despite suicide prevention strategies being applied, the rate of suicide has not changed substantially over the past decades. Advances in machine learning make it possible to attempt to predict suicide based on the analysis of relevant data to inform clinical practice. This paper reports on findings from the analysis of data of patients who died by suicide in the period 2013-2016 and made use of both structured data and free-text medical notes. We focus on examining various text-mining approaches to support risk assessment. The results show that using advance machine learning and text-mining techniques, it is possible to predict within a specified period which people are most at risk of taking their own life at the time of referral to a mental health service.",
        "link": "https://dl.acm.org/doi/10.1145/3200947.3201020"
    },
    {
        "title": "Integration of data mining techniques in e-learning systems: Clustering Profil of Lerners and Recommender Course System",
        "authors": [
            "A. El Moustamid",
            "E. En-Naimi",
            "J. El Bouhdidi"
        ],
        "date": "March 2017",
        "source": "BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
        "abstract": "Data mining aims at the extraction of knowledge from large quantities of data by automatic or semi-automatic methods. In the field of education and training e-learning, Data Mining techniques as clustering (The aim of this technique is to identify groups that are similar in some aspect) [21], prediction (is used for developing model where combination of data viewed from different aspects can effect single viewed data. Here, is important to watch how predictors variables are effecting predicted variable)[21], classification are applied in e-learning to improve the level of learners by providing learning according to the learners' requirement) are used to analyze learners profile, to predict and improve student performance. The aim of our project is to develop a system capable of analyzing learners profile and indexing web videos in order to offer learners a rich database with courses that correspond to their levels.",
        "link": "https://dl.acm.org/doi/10.1145/3090354.3090453"
    },
    {
        "title": "Multi-level mining and visualization of scientific text collections: Exploring a bi-lingual scientific repository",
        "authors": [
            "Pablo Accuosto",
            "Francesco Ronzano",
            "Daniel Ferrés",
            "Horacio Saggion"
        ],
        "date": "December 2017",
        "source": "WOSP 2017: Proceedings of the 6th International Workshop on Mining Scientific Publications",
        "abstract": "We present a system to mine and visualize collections of scientific documents by semantically browsing information extracted from single publications or aggregated throughout corpora of articles. The text mining tool performs deep analysis of document collections allowing the extraction and interpretation of research paper's contents. In addition to the extraction and enrichment of documents with metadata (titles, authors, affiliations, etc), the deep analysis performed comprises semantic interpretation, rhetorical analysis of sentences, triple-based information extraction, and text summarization. The visualization components allow geographical-based exploration of collections, topic-evolution interpretation, and collaborative network analysis among others. The paper presents a case study of a bi-lingual collection in the field of Natural Language Processing (NLP).",
        "link": "https://dl.acm.org/doi/10.1145/3127526.3127529"
    },
    {
        "title": "Hierarchical Topic Mining via Joint Spherical Tree and Text Embedding",
        "authors": [
            "Yu Meng",
            "Yunyi Zhang",
            "Jiaxin Huang",
            "Yu Zhang",
            "Chao Zhang",
            "Jiawei Han"
        ],
        "date": "August 2020",
        "source": "KDD '20: Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Mining a set of meaningful topics organized into a hierarchy is intuitively appealing since topic correlations are ubiquitous in massive text corpora. To account for potential hierarchical topic structures, hierarchical topic models generalize flat topic models by incorporating latent topic hierarchies into their generative modeling process. However, due to their purely unsupervised nature, the learned topic hierarchy often deviates from users' particular needs or interests. To guide the hierarchical topic discovery process with minimal user supervision, we propose a new task, Hierarchical Topic Mining, which takes a category tree described by category names only, and aims to mine a set of representative terms for each category from a text corpus to help a user comprehend his/her interested topics. We develop a novel joint tree and text embedding method along with a principled optimization procedure that allows simultaneous modeling of the category tree structure and the corpus generative process in the spherical space for effective category-representative term discovery. Our comprehensive experiments show that our model, named JoSH, mines a high-quality set of hierarchical topics with high efficiency and benefits weakly-supervised hierarchical text classification tasks.",
        "link": "https://dl.acm.org/doi/10.1145/3394486.3403242"
    },
    {
        "title": "Unsupervised behavioural mining and clustering for malware family identification",
        "authors": [
            "Khanh Huu The Dam",
            "Thomas Given-Wilson",
            "Axel Legay"
        ],
        "date": "March 2021",
        "source": "SAC '21: Proceedings of the 36th Annual ACM Symposium on Applied Computing",
        "abstract": "More accurate and advanced detection and classification of malware requires exploiting program behaviour and not merely syntactic or static features. One approach is to use system call dependency graphs (SCDGs) that represent the program behaviour by interactions with the system, and the relations between these interactions. These SCDGs have been used with supervised learning techniques to very accurately detect and classify malware. This works considers the unsupervised learning challenge of mining for common clusters of behaviour without a priori knowledge. This allows for clustering of similar programs by behaviour, that can then be used for either classification or further analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3412841.3441919"
    },
    {
        "title": "Text Mining for Evaluating Authors' Birth and Death Years",
        "authors": [
            "Dror Moghaz",
            "Yaakov Hacohen-Kerner",
            "Dov Gabbay"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "This article presents a unique method in text and data mining for finding the era, i.e., mining temporal data, in which an anonymous author was living. Finding this era can assist in the examination of a fake document or extracting the time period in which a writer lived. The study and the experiments concern Hebrew, and in some parts, Aramaic and Yiddish rabbinic texts. The rabbinic texts are undated and contain no bibliographic sections, posing an interesting challenge. This work proposes algorithms using key phrases and key words that allow the temporal organization of citations together with linguistic patterns. Based on these key phrases, key words, and the references, we established several types of “Iron-clad,” Heuristic and Greedy rules for estimating the years of birth and death of a writer in an interesting classification task. Experiments were conducted on corpora, including documents authored by 12, 24, and 36 rabbinic writers and demonstrated promising results.",
        "link": "https://dl.acm.org/doi/10.1145/3281631"
    },
    {
        "title": "Transfer Learning to Timed Text Based Video Classification Using CNN",
        "authors": [
            "Zenun Kastrati",
            "Ali Shariq Imran",
            "Arianit Kurti"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "Open educational video resources are gaining popularity with a growing number of massive open online courses (MOOCs). This has created a niche for content providers to adopt effective solutions in automatically organizing and structuring of educational resources for maximum visibility. Recent advances in deep learning techniques are proving useful in managing and classifying resources into appropriate categories. This paper proposes one such convolutional neural network (CNN) model for classifying video lectures in a MOOC setting using a transfer learning approach. The model uses a time-aligned text transcripts corresponding to video lectures from six broader subject categories. Video lectures and their corresponding transcript dataset is gathered from the Coursera MOOC platform. Two different CNN models are proposed: i) CNN based classification using embeddings learned from our MOOC dataset, ii) CNN based classification using transfer learning. Word embeddings generated from two well known state-of-the-art pre-trained models Word2Vec and GloVe, are used in the transfer learning approach for the second case. The proposed CNN models are evaluated using precision, recall, and F1 score and the obtained performance is compared with both conventional and deep learning classifiers. The proposed CNN models have an F1 score improvement of 10-22 percentage points over DNN and conventional classifiers",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326483"
    },
    {
        "title": "Evolutionary role mining in complex networks by ensemble clustering",
        "authors": [
            "Sarvenaz Choobdar",
            "Pedro Ribeiro",
            "Fernando Silva"
        ],
        "date": "April 2017",
        "source": "SAC '17: Proceedings of the Symposium on Applied Computing",
        "abstract": "The structural patterns in the neighborhood of nodes assign unique roles to the nodes. Mining the set of existing roles in a network provides a descriptive profile of the network and draws its general picture. This paper proposes a new method to determine structural roles in a dynamic network based on the current position of nodes and their historic behavior. We develop a temporal ensemble clustering technique to dynamically find groups of nodes, holding similar tempo-structural roles. We compare two weighting functions, based on age and distribution of data, to incorporate temporal behavior of nodes in the role discovery. To evaluate the performance of the proposed method, we assess the results from two points of view: 1) goodness of fit to current structure of the network; 2) consistency with historic data. We conduct the evaluation using different ensemble clustering techniques. The results on real world networks demonstrate that our method can detect tempo-structural roles that simultaneously depict the topology of a network and reflect its dynamics with high accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3019612.3019815"
    },
    {
        "title": "Analyzing used-car web listings via text mining",
        "authors": [
            "Ayhan Demiriz",
            "Fatma Cantaş"
        ],
        "date": "October 2017",
        "source": "IML '17: Proceedings of the 1st International Conference on Internet of Things and Machine Learning",
        "abstract": "Used car trade is one of the major components of the world economies. It is not uncommon to sell a car by placing an internet advertisement irrespective of the geography in these days. A typical content of an advertisement is usually composed of two parts namely the structured and the free text data. The structured data may include some information about the asking price, make, model, year, mileage of the car and the contact info. In most cases, seller may give important clues about the car's current conditions in the free text data where the title (head) of the advertisement can be included as free text too. This paper reports preliminary results from a text mining study conducted on 75K used car internet listings collected from two major car listing web sites in Turkey. As expected, the words and the phrases related to the description of the car are observed to be frequent. The leading concepts in the free text are found to be regarding how to describe the current condition of a car, for example \"no crash history\".",
        "link": "https://dl.acm.org/doi/10.1145/3109761.3109782"
    },
    {
        "title": "Text classification of Diseases Treated by Traditional Chinese Medicine Prescription based on machine learning",
        "authors": [
            "Hanqing Zhao",
            "Bo Han",
            "Chen Li"
        ],
        "date": "September 2020",
        "source": "ISAIMS '20: Proceedings of the 1st International Symposium on Artificial Intelligence in Medical Sciences",
        "abstract": "OBJECTIVE: To explore the application of machine learning in th e identification of diseases treated by traditional Chinese medicine prescriptions. METHODS: Based on the composition of the text document of Chinese medicine prescriptions, the prescriptions were divided into cough, headache and diarrhea. THUTCT were introduced to establish and train two machine learning text classification models, LibSVM and LibLinear, and the prescriptions to be deter mined were put into the model for classification and prediction. R ESULTS: The Precision rate of LibSVM model and Liblinear mod el were 0.7283 and 0.6690. Seven prescriptions were classified an d predicted, and the results were in line with expectations. CONCLUSION: THUCTC has good universality for the content of TCM prescriptions, high classification accuracy and fast testing speed, which is suitable for the text classification and discrimination research of TCM prescriptions for diseases.",
        "link": "https://dl.acm.org/doi/10.1145/3429889.3429895"
    },
    {
        "title": "Evaluation of retweet clustering method classification method using retweets on Twitter without text data",
        "authors": [
            "K. Uchida",
            "F. Toriumi",
            "T. Sakaki"
        ],
        "date": "August 2017",
        "source": "WI '17: Proceedings of the International Conference on Web Intelligence",
        "abstract": "Burst phenomena, which frequently occur on social media, are caused by such social events as flaming on the internet, elections, and natural disasters. To understand people's thoughts and feelings, we must classify their opinions from burst phenomena. Therefore, classification methods that categorize tweets are critical. However, since most classification methods focus on text mining, they cannot group tweets by topics because each tweet has poor linguistic similarities. We used a non-text-based classification method proposed by Baba et al. that groups tweets by topics, even if they have poor linguistic similarities, and verified its validity by comparing it with a text-based classification method in two different evaluations: qualitative and quantitative. In the qualitative evaluation part, we did a questionnaire survey and validated the suitability of the topic clusters created using both the non-and text-based methods. Since evaluating the similarity of every pair of tweets in each topic is difficult, we evaluated the similarity between sampled pairs in the survey and acquired more appropriate topic clustering results using the non-text-based method than the text-based method. In the quantitative evaluation part, we focused on the robustness of each method against data reduction. Many approaches analyze social media data, especially because collecting data from social media is comparatively easy. However, since collecting the whole data of burst phenomena is very costly due to the vast amounts of available social media data, robustness against data reduction is an important index to evaluate classification methods. With the non-text-based method, over 55% of the pairs of tweets in the same cluster were also included in the same cluster even when the data were reduced to 10% in all three of our example cases. In this paper, as a source we focus on Twitter, one of the most popular microblogging services. Using clustering to conduct detailed case analyses, we scrutinized three burst cases that include natural disasters and flaming on the internet and found that a non-text-based method more effectively classified tweets in burst phenomena than a text-based method.",
        "link": "https://dl.acm.org/doi/10.1145/3106426.3106451"
    },
    {
        "title": "Using Spark for Text Mining on Large Scale Liver Cancer Literature",
        "authors": [
            "Ming-Yen Lin",
            "Yu-Ju Lin",
            "Sue-Chen Hsueh"
        ],
        "date": "January 2021",
        "source": "BDET 2021: 2021 the 3rd International Conference on Big Data Engineering and Technology (BDET)",
        "abstract": "Cancer is one of the main causes of death. The number of known cancer to-date is more than one hundred. Liver cancer, ranked after trachea and lung cancer, has long been high in the cancer leading cause in Taiwan and even ranked as second in 2016. The number of scientific articles related to cancer proliferates every year. The number reaches as high as 20 million in PubMed so that discovering useful information from the massive collection is very difficult. In addition, using a single machine to sift through these articles is very time-consuming. Therefore, we present a big data analytic framework using the distributed Apache Spark platform for text mining in PubMed literature. We establish a prediction model for liver cancer articles so that researchers may effectively validate whether an article is related to liver cancer or not. Classification models in Spark MLlib including Linear Support Vector Machines (SVM), Logistic Regression, are used in our experiments. Relevancy to liver cancer is further confirmed by using MeSH (Medical Subject Headings) terms. Logistic regression is about 3 times faster than SVMs and the accuracy of both methods is close to 95% in the experiments using hold-out validation. When max_features is 500 and min_df ≤ 0.1 (or min_df = 1), the accuracy may reach 96%. In the experiments with K-fold cross-validation, the accuracy of SVMs methods is 96%. The experimental results show that that our prediction model may effectively classify liver cancer articles.",
        "link": "https://dl.acm.org/doi/10.1145/3474944.3474958"
    },
    {
        "title": "Automatic topic classification of test cases using text mining at an Android smartphone vendor",
        "authors": [
            "Junji Shimagaki",
            "Yasutaka Kamei",
            "Naoyasu Ubayashi",
            "Abram Hindle"
        ],
        "date": "October 2018",
        "source": "ESEM '18: Proceedings of the 12th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "abstract": "Background: An Android smartphone is an ecosystem of applications, drivers, operating system components, and assets. The volume of the software is large and the number of test cases needed to cover the functionality of an Android system is substantial. Enormous effort has been already taken to properly quantify \"what features and apps were tested and verified?\". This insight is provided by dashboards that summarize test coverage and results per feature. One method to achieve this is to manually tag or label test cases with the topic or function they cover, much like function points. At the studied Android smartphone vendor, tests are labelled with manually defined tags, so-called \"feature labels (FLs)\", and the FLs serve to categorize 100s to 1000s test cases into 10 to 50 groups. Aim: Unfortunately for developers, manual assignment of FLs to 1000s of test cases is a time consuming task, leading to inaccurately labeled test cases, which will render the dashboard useless. We created an automated system that suggests tags/labels to the developers for their test cases rather than manual labeling. Method: We use machine learning models to predict and label the functionality tested by 10,000 test cases developed at the company. Results: Through the quantitative experiments, our models achieved acceptable F-1 performance of 0.3 to 0.88. Also through the qualitative studies with expert teams, we showed that the hierarchy and path of tests was a good predictor of a feature's label. Conclusions: We find that this method can reduce tedious manual effort that software developers spent classifying test cases, while providing more accurate classification results.",
        "link": "https://dl.acm.org/doi/10.1145/3239235.3268927"
    },
    {
        "title": "To apply Data Mining for Classification of Crowd sourced Software Requirements",
        "authors": [
            "Soonh Taj",
            "Qasim Arain",
            "Imran Memon",
            "Asma Zubedi"
        ],
        "date": "April 2019",
        "source": "ICSIE '19: Proceedings of the 8th International Conference on Software and Information Engineering",
        "abstract": "Now a day's main focus of developers is to build quality software that works according to customer needs and for this reason it is necessary to gather right requirements as requirement elicitation is the critical step that impacts on the success of software project as misinterpreted requirements leads to the failure of software project. By keeping this in mind a research is carried out on improving requirements elicitation process and automating the process of classifying requirements. In this research, a model is proposed which will help in this scenario for requirements elicitation and requirement classification. This paper presents a model in which crowd sourcing approach is used so that customers, end users, stakeholders, developers and software engineers can make active participation for requirement elicitation process and requirements gathered using crowdsourcing approach are used by model for classification process i.e. classification of requirements into functional and non-functional requirements. For the proof of proposed model a case study is conducted. Results of case study provided the usefulness and efficiency of proposed model for classification of crowd sourced software requirements.",
        "link": "https://dl.acm.org/doi/10.1145/3328833.3328837"
    },
    {
        "title": "Text classification using Fuzzy TF-IDF and Machine Learning Models",
        "authors": [
            "Mariem Bounabi",
            "Karim El Moutaouakil",
            "Khalid Satori"
        ],
        "date": "October 2019",
        "source": "BDIoT'19: Proceedings of the 4th International Conference on Big Data and Internet of Things",
        "abstract": "The representation of the information has an important impact on the text classification task. Several weighting methods were proposed in the literature, and the term frequency-inverse term frequency (TFIDF), the most know on the text treatment field. The FTF-IDF is a vector representation where the components of the TFIDF are presented as inputs to the Fuzzy Inference System (FIS). In this work, we compare several Machin Learning algorithms such as Naïve Bayes and its derivatives, SVM and Random forest classifiers, using the FTF-IDF representation. To improve the quality of the used classifiers, we call sum attribute selection methods. The recognition rate, for the tested systems, is satisfied, where the system based on naïve Bayes classifier, the FTF-IDF weighting terms, and the info gain select attributes method gives 98.7% as accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3372938.3372956"
    },
    {
        "title": "Text Mining in Cybersecurity: A Systematic Literature Review",
        "authors": [
            "Luciano Ignaczak",
            "Guilherme Goldschmidt",
            "Cristiano André Da Costa",
            "Rodrigo Da Rosa Righi"
        ],
        "date": null,
        "source": "ACM Computing Surveys",
        "abstract": "The growth of data volume has changed cybersecurity activities, demanding a higher level of automation. In this new cybersecurity landscape, text mining emerged as an alternative to improve the efficiency of the activities involving unstructured data. This article proposes a Systematic Literature Review (SLR) to present the application of text mining in the cybersecurity domain. Using a systematic protocol, we identified 2,196 studies, out of which 83 were summarized. As a contribution, we propose a taxonomy to demonstrate the different activities in the cybersecurity domain supported by text mining. We also detail the strategies evaluated in the application of text mining tasks and the use of neural networks to support activities involving unstructured data. The work also discusses text classification performance aiming its application in real-world solutions. The SLR also highlights open gaps for future research, such as the analysis of non-English content and the intensification in the usage of neural networks.",
        "link": "https://dl.acm.org/doi/10.1145/3462477"
    },
    {
        "title": "Using Text Mining to Analyze the Financial Patents",
        "authors": [
            "Yung-Feng Lu",
            "Jia-Lang Xu",
            "Mu-Yen Chen"
        ],
        "date": "June 2019",
        "source": "WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "With the advancement and development of science and technology, Nowadays society is the development of information technology, cloud computing, mobile Internet, Internet of Things, block chain and information technology.The combination of finance and technology has made financial technology terms gradually emerging.. This research analyzes the patents of financial technology. However, Taiwan's financial technology patents are mostly G06Q-20 (payment plan, architecture or agreement), G06Q-30 (commercial, such as marketing, shopping, payment, auction or e-commerce). G06Q-40 (financial, investment or tax treatment, insurance).The research use text mining technology to analyze and understand Taiwan's trends and developments in financial technology. The research results show that G06Q-20 patents are biased towards information security and mobile payment. G06Q-30 patents favor shopping websites and advertising. G06Q-40 patents are biased towards diversity, such as securities, insurance, finance, and stocks.",
        "link": "https://dl.acm.org/doi/10.1145/3326467.3326478"
    },
    {
        "title": "Identifying Online Profiles of Distance Learning Students Using Data Mining Techniques",
        "authors": [
            "Osama Islam",
            "Muazzam Siddiqui",
            "Naif Radi Aljohani"
        ],
        "date": "October 2019",
        "source": "ICDTE '19: Proceedings of the 3rd International Conference on Digital Technology in Education",
        "abstract": "Educational data has grown over the years with the increased use of technology within educational environments. This has led to a huge amount of data being stored in various data sources representing the student, his/her activities, and other aspects relevant to the learning process. To meet this analytical need, Educational Data Mining (EDM) has emerged to assist educational institutions in identifying key benefits such as students at risk, the level of student engagement or predicting student performance. The aim of this research was to explore the various aspects of student interaction data using data mining techniques to identify relevant patterns of behaviors and possible key attributes that have higher degrees of influence on distance learning students. The main findings identified several patterns of user online profiles based on a set of adopted learning strategies, the research proposed a framework for analyzing such interaction data based on R and Hadoop platforms to correlate online profiles with student performance.",
        "link": "https://dl.acm.org/doi/10.1145/3369199.3369249"
    },
    {
        "title": "Extremely Fast Decision Tree Mining for Evolving Data Streams",
        "authors": [
            "Albert Bifet",
            "Jiajin Zhang",
            "Wei Fan",
            "Cheng He",
            "Jianfeng Zhang",
            "Jianfeng Qian",
            "Geoff Holmes",
            "Bernhard Pfahringer"
        ],
        "date": "August 2017",
        "source": "KDD '17: Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
        "abstract": "Nowadays real-time industrial applications are generating a huge amount of data continuously every day. To process these large data streams, we need fast and efficient methodologies and systems. A useful feature desired for data scientists and analysts is to have easy to visualize and understand machine learning models. Decision trees are preferred in many real-time applications for this reason, and also, because combined in an ensemble, they are one of the most powerful methods in machine learning. In this paper, we present a new system called STREAMDM-C++, that implements decision trees for data streams in C++, and that has been used extensively at Huawei. Streaming decision trees adapt to changes on streams, a huge advantage since standard decision trees are built using a snapshot of data, and can not evolve over time. STREAMDM-C++ is easy to extend, and contains more powerful ensemble methods, and a more efficient and easy to use adaptive decision trees. We compare our new implementation with VFML, the current state of the art implementation in C, and show how our new system outperforms VFML in speed using less resources.",
        "link": "https://dl.acm.org/doi/10.1145/3097983.3098139"
    },
    {
        "title": "Data miners' little helper: data transformation activity cues for cluster analysis on document collections",
        "authors": [
            "Tania Cerquitelli",
            "Evelina Di Corso",
            "Francesco Ventura",
            "Silvia Chiusano"
        ],
        "date": "June 2017",
        "source": "WIMS '17: Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics",
        "abstract": "In this paper we propose a new self-learning engine to streamline the analytics process, as it enables analysts to mine massive data repositories with minimal user intervention. In the context of cluster analysis on a collection of documents this new system, named SELF-DATA (SELF-learning DAta TrAnsformation), suggests to the analyst how to configure the whole mining process for a given dataset. SELF-DATA relies on an engine exploring different data weighting schemas (e.g., normalized term frequencies) and data transformation methods (e.g., PCA) before applying the cluster analysis, evaluating and comparing solutions through different quality indices (e.g., weighted Silhouette), and presenting the k-top solutions to the analyst. SELF-DATA will also include a knowledge base storing results of experiments on previously processed datasets, and a classification algorithm trained on the knowledge base content to forecast the best configuration for the whole mining process for an unexplored dataset. The first development of SELF-DATA running on Apache Spark has been validated on 5 collections of documents. Experimental results highlight that TF-IDF and logarithmic entropy are effective to measure item relevance with sparse datasets, and the LSI method outperforms PCA with a large dictionary.",
        "link": "https://dl.acm.org/doi/10.1145/3102254.3102288"
    },
    {
        "title": "Interactive Visual Graph Mining and Learning",
        "authors": [
            "Ryan A. Rossi",
            "Nesreen K. Ahmed",
            "Rong Zhou",
            "Hoda Eldardiry"
        ],
        "date": null,
        "source": "ACM Transactions on Intelligent Systems and Technology",
        "abstract": "This article presents a platform for interactive graph mining and relational machine learning called GraphVis. The platform combines interactive visual representations with state-of-the-art graph mining and relational machine learning techniques to aid in revealing important insights quickly as well as learning an appropriate and highly predictive model for a particular task (e.g., classification, link prediction, discovering the roles of nodes, and finding influential nodes). Visual representations and interaction techniques and tools are developed for simple, fast, and intuitive real-time interactive exploration, mining, and modeling of graph data. In particular, we propose techniques for interactive relational learning (e.g., node/link classification), interactive link prediction and weighting, role discovery and community detection, higher-order network analysis (via graphlets, network motifs), among others. GraphVis also allows for the refinement and tuning of graph mining and relational learning methods for specific application domains and constraints via an end-to-end interactive visual analytic pipeline that learns, infers, and provides rapid interactive visualization with immediate feedback at each change/prediction in real-time. Other key aspects include interactive filtering, querying, ranking, manipulating, exporting, as well as tools for dynamic network analysis and visualization, interactive graph generators (including new block model approaches), and a variety of multi-level network analysis techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3200764"
    },
    {
        "title": "Data Mining and Opinion Mining: A Tool in Educational Context",
        "authors": [
            "Myriam Peñafiel",
            "Stefanie Vásquez",
            "Diego Vásquez",
            "Juan Zaldumbide",
            "Sergio Luján-Mora"
        ],
        "date": "July 2018",
        "source": "ICoMS '18: Proceedings of the 2018 1st International Conference on Mathematics and Statistics",
        "abstract": "The use of the web as a universal communication platform generates large volumes of data (Big data), which in many cases, need to be processed so that they can become useful knowledge in face of the sceptics who have doubts about the credibility of such information. The use of web data that comes from educational contexts needs to be addressed, since that large amount of unstructured information is not being valued, losing valuable information that can be used. To solve this problem, we propose the use of data mining techniques such as sentiment analysis to validate the information that comes from the educational platforms. The objective of this research is to propose a methodology that allows the user to apply sentiment analysis in a simple way, because although some researchers have done it, very few do with data in the educational context. The results obtained prove that the proposal can be used in similar cases.",
        "link": "https://dl.acm.org/doi/10.1145/3274250.3274263"
    },
    {
        "title": "Research Hotspots Mining and Visualized Analysis Based on Linking Cluster and K-Core Decomposition",
        "authors": [
            "Ying Cai",
            "Fang Huang",
            "Shuai Wang",
            "Hao Zhang",
            "Chunxiu Du"
        ],
        "date": "May 2018",
        "source": "ICDPA 2018: Proceedings of the International Conference on Data Processing and Applications",
        "abstract": "In this paper, we focus on research hotspots mining with knowledge semantic features on discipline topic word networks, which are extracted from scientific and technical documents. For the topic networks, a novel method of combining linking community clustering with k-core decomposition is proposed to mine the hotspot community with knowledge hierarchy structure. And as a post-processing, the hierarchical community identification is implemented by k-core decomposition in order to discover the internal connection with knowledge semantic hierarchy in the research hotspot. Finally, the community density and visualization method is utilized to analyze the core and sub- problem in the research hotspot community. In experiments, the topic networks for a single discipline and interdisciplinary research hotspot are taken as examples of analysis, and the results show the proposed approach can effectively realize the objective of mining and analyzing the discipline research hotspot on the academic topic word network.",
        "link": "https://dl.acm.org/doi/10.1145/3224207.3224216"
    },
    {
        "title": "The Application of Data Mining Algorithms in the Construction of Travel Recommendation System",
        "authors": [
            "Chao Lou",
            "Zhaonan Mu",
            "Mengzhu Liu"
        ],
        "date": "January 2021",
        "source": "CONF-CDS 2021: The 2nd International Conference on Computing and Data Science",
        "abstract": "Photos shared by users of online social network often associated with tags, texts, geographic and time information. These data are ideal research resource which can be used by researchers to observe people's interests and behaviors. They can also be used to analyze people's travel behaviors and interests because it can reflect people's travel activities and experiences directly. This project aimed to consolidate people into groups based on their travel preferences of travel season, travel time span, scenery type of travel location and then analyze popular travel places for different people groups. The travel preferences were analyzed from extracted data applying PHP language and Naive Bayes classification approach. According to the analyzed data, people were divided into groups. Then, DBSCAN clustering was used to get popular locations based on the different groups. As a result, 80 thousand photo data of 5 cities have been collected from Flickr by Phython. It has been showed that people's preference of season in most of the mined cities was spring and summer and the preference of time span was short. Moreover, mined people has been divided into 16 groups according to their preference and based on the groups popular locations have been calculated by DBSCAN clustering. Theses locations have been provided in a recommendation website which was built in this project. For the future work, mining Flickr data associated with other travel social network sites could be a good way to build a more effective recommendation system.",
        "link": "https://dl.acm.org/doi/10.1145/3448734.3450477"
    },
    {
        "title": "Compare Machine Learning Models in Text Classification Using Steam User Reviews",
        "authors": [
            "Youchen Miao",
            "Zeyu Jin",
            "Yumeng Zhang",
            "Yuchen Chen",
            "Junren Lai"
        ],
        "date": "November 2021",
        "source": "ICSED '21: Proceedings of the 2021 3rd International Conference on Software Engineering and Development",
        "abstract": "Text Classification and Sentiment Analysis of game reviews are viewed as important parts in not only academic fields but also in game studies. In this paper, with more than 400 thousand game reviews on Steam platform, we preprocess the data using different libraries (sklearn, nltk, and spaCy) and use them as inputs to build three sentiment classification models based on different algorithms (Naive Bayes, SVM, and Random Forest). In contrast to previous studies that only focus on different sentiment analysis models, our paper also highlights the use of different APIs to preprocess the data and their corresponding model performance. The results show that no matter which API we choose, Random Forest models always perform the best. However, in terms of training time, Naive Bayes is the fastest. This work can be used to apply grid search for researchers to automatically find the optimum API before conducting sentiment analysis in the future.",
        "link": "https://dl.acm.org/doi/10.1145/3507473.3507480"
    },
    {
        "title": "Lost in Transduction: Transductive Transfer Learning in Text Classification",
        "authors": [
            "Alejandro Moreo",
            "Andrea Esuli",
            "Fabrizio Sebastiani"
        ],
        "date": null,
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Obtaining high-quality labelled data for training a classifier in a new application domain is often costly. Transfer Learning\n(a.k.a. “Inductive Transfer”) tries to alleviate these costs by transferring, to the “target” domain of interest, knowledge available from a different “source” domain. In transfer learning the lack of labelled information from the target domain is compensated by the availability at training time of a set of unlabelled examples from the target distribution. Transductive Transfer Learning denotes the transfer learning setting in which the only set of target documents that we are interested in classifying is known and available at training time. Although this definition is indeed in line with Vapnik’s original definition of “transduction”, current terminology in the field is confused. In this article, we discuss how the term “transduction” has been misused in the transfer learning literature, and propose a clarification consistent with the original characterization of this term given by Vapnik. We go on to observe that the above terminology misuse has brought about misleading experimental comparisons, with inductive transfer learning methods that have been incorrectly compared with transductive transfer learning methods. We then, give empirical evidence that the difference in performance between the inductive version and the transductive version of a transfer learning method can indeed be statistically significant (i.e., that knowing at training time the only data one needs to classify indeed gives an advantage). Our clarification allows a reassessment of the field, and of the relative merits of the major, state-of-the-art algorithms for transfer learning in text classification.",
        "link": "https://dl.acm.org/doi/10.1145/3453146"
    },
    {
        "title": "Machine Learning on Mining Potential Adverse Drug Reactions for Pharmacovigilance",
        "authors": [
            "Ke Jia"
        ],
        "date": "July 2021",
        "source": "DSIT 2021: 2021 4th International Conference on Data Science and Information Technology",
        "abstract": "Drugs are related to the lives and health of patients. Mining and discovering potential adverse drug reactions from a large number of unstructured texts play an important role in drug research and pharmacovigilance. It is a hot research issue in the field of biomedicine in recent years. A large number of scholars have discovered a large amount of information on adverse drug reactions from different data sources through effective machine learning methods. This paper reviews the research progress in recent years to provide a reference for future research.",
        "link": "https://dl.acm.org/doi/10.1145/3478905.3478964"
    },
    {
        "title": "TACAM: Topic And Context Aware Argument Mining",
        "authors": [
            "Michael Fromm",
            "Evgeniy Faerman",
            "Thomas Seidl"
        ],
        "date": "October 2019",
        "source": "WI '19: IEEE/WIC/ACM International Conference on Web Intelligence",
        "abstract": "In this work we address the problem of argument search. The purpose of argument search is the distillation of pro and contra arguments for requested topics from large text corpora. In previous works, the usual approach is to use a standard search engine to extract text parts which are relevant to the given topic and subsequently use an argument recognition algorithm to select arguments from them. The main challenge in the argument recognition task, which is also known as argument mining, is that often sentences containing arguments are structurally similar to purely informative sentences without any stance about the topic. In fact, they only differ semantically. Most approaches use topic or search term information only for the first search step and therefore assume that arguments can be classified independently of a topic. We argue that topic information is crucial for argument mining, since the topic defines the semantic context of an argument. Precisely, we propose different models for the classification of arguments, which take information about a topic of an argument into account. Moreover, to enrich the context of a topic and to let models understand the context of the potential argument better, we integrate information from different external sources such as Knowledge Graphs or pre-trained NLP models. Our evaluation shows that considering topic information, especially in connection with external information, provides a significant performance boost for the argument mining task.",
        "link": "https://dl.acm.org/doi/10.1145/3350546.3352506"
    }
]