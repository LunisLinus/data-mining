{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6951c59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import copy\n",
    "import string\n",
    "import nltk\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from wordcloud import WordCloud\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate, StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from wordcloud import WordCloud\n",
    "random.seed = 456"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5bf4e6bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordcloud in c:\\users\\linag\\anaconda3\\lib\\site-packages (1.9.2)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\linag\\anaconda3\\lib\\site-packages (from wordcloud) (1.24.3)\n",
      "Requirement already satisfied: pillow in c:\\users\\linag\\anaconda3\\lib\\site-packages (from wordcloud) (9.4.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\linag\\anaconda3\\lib\\site-packages (from wordcloud) (3.7.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\linag\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\linag\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\linag\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\linag\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\linag\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (23.1)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\users\\linag\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\linag\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\linag\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6b0d6b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_keywords_list(list_of_keyword_dicts):\n",
    "    normalized_list = []\n",
    "    for keyword_dict in list_of_keyword_dicts:\n",
    "        total_value = sum(keyword_dict.values())\n",
    "        normalized_keywords = keyword_dict.copy()\n",
    "        for keyword in normalized_keywords:\n",
    "            normalized_keywords[keyword] /= total_value\n",
    "        normalized_list.append(normalized_keywords)\n",
    "    return normalized_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "95b3ef20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tfidf(list_of_keyword_dicts):\n",
    "    idf_values = dict.fromkeys(list_of_keyword_dicts[0].keys(), 0.0)\n",
    "\n",
    "    for keyword_dict in list_of_keyword_dicts:\n",
    "        for word, frequency in filter(lambda x: x[1] > 0, keyword_dict.items()):\n",
    "            idf_values[word] += 1.0\n",
    "\n",
    "    for word in idf_values:\n",
    "        if idf_values[word]:\n",
    "            idf_values[word] = math.log(len(list_of_keyword_dicts) / idf_values[word])\n",
    "\n",
    "    tfidf_results = []\n",
    "    for keyword_dict in list_of_keyword_dicts:\n",
    "        keyword_dict = keyword_dict.copy()\n",
    "        total_frequency = sum(keyword_dict.values())\n",
    "        for word, frequency in filter(lambda x: x[1] > 0, keyword_dict.items()):\n",
    "            keyword_dict[word] *= idf_values[word] / total_frequency\n",
    "        tfidf_results.append(keyword_dict)\n",
    "    return tfidf_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "31adebf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_tfidf(list_of_keyword_dicts):\n",
    "    list_of_keyword_dicts = calculate_tfidf(list_of_keyword_dicts)\n",
    "    keywords = list_of_keyword_dicts[0].keys()\n",
    "    normalized_list = []\n",
    "\n",
    "    for index, value_dict in enumerate(list_of_keyword_dicts):\n",
    "        norm = 0\n",
    "\n",
    "        for v in value_dict.values():\n",
    "            norm += v**2\n",
    "\n",
    "        norm = math.sqrt(norm)\n",
    "\n",
    "        if norm == 0:\n",
    "            normalized_list.append(dict(zip(keywords, value_dict.values())))\n",
    "            continue\n",
    "\n",
    "        normalized_values = map(lambda x: x / norm, value_dict.values())\n",
    "        normalized_list.append(dict(zip(keywords, normalized_values)))\n",
    "\n",
    "    return normalized_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "be2cf9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_word_cloud(word_frequency_dict, title, axis=None):\n",
    "\n",
    "    wordcloud = WordCloud(background_color=\"white\", height=500, width=1000, random_state=random.seed)\n",
    "    wordcloud.generate_from_frequencies(word_frequency_dict)\n",
    "\n",
    "    if axis is not None:\n",
    "        axis.imshow(wordcloud, interpolation='bilinear')\n",
    "        axis.set_title(title)\n",
    "        axis.axis(False)\n",
    "    else:\n",
    "        plt.subplots(num=None, figsize=(15, 10), dpi=80)\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.title(title)\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2ab9fa68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\linag\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\linag\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\linag\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9934c450",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(language='english')\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "stopwords_set = set(stopwords.words('english'))\n",
    "punctuation = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "394457e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8913e4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_stem_nltk(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    return [stemmer.stem(word) for word in tokens if word not in stopwords_set and word not in punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c6df4bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_lemmatize_nltk(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    return [lemmatizer.lemmatize(word) for word in tokens if word not in stopwords_set and word not in punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "065e03af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_total_bag(results):\n",
    "    word_frequency_dict = dict()\n",
    "    for result in results:\n",
    "        tokens = preprocess_and_lemmatize_nltk(result['abstract'])\n",
    "        for token in tokens:\n",
    "            if token not in word_frequency_dict:\n",
    "                word_frequency_dict[token] = 1\n",
    "            else:\n",
    "                word_frequency_dict[token] += 1\n",
    "    return word_frequency_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cc6d6f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_word_frequency(abstract, word_frequency_dict):\n",
    "    tokens = preprocess_and_lemmatize_nltk(abstract)\n",
    "    for token in tokens:\n",
    "        if token in word_frequency_dict:\n",
    "            word_frequency_dict[token] += 1\n",
    "    return word_frequency_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cf47d8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_low_frequency_words(word_frequency_dict, threshold=0):\n",
    "    filtered_dict = dict()\n",
    "    for word, frequency in word_frequency_dict.items():\n",
    "        if frequency > threshold:\n",
    "            filtered_dict[word] = frequency\n",
    "    return filtered_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "569ba227",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_list_keywords(word_frequency_dict, articles):\n",
    "    list_keywords = []\n",
    "    temp_bag = dict.fromkeys(word_frequency_dict, 0)\n",
    "    for article in articles:\n",
    "        list_keywords.append(update_word_frequency(article['abstract'], temp_bag.copy()))\n",
    "    del temp_bag\n",
    "    return list_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "38291d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_list_keywords_weights(list_of_keyword_weights):\n",
    "    list_keywords_weight_items = []\n",
    "    for item in list_of_keyword_weights:\n",
    "        list_keywords_weight_items.append(list(item.values()))\n",
    "    return list_keywords_weight_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fc43cb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_precision_positive_class(y_true, y_pred):\n",
    "    return precision_score(y_true, y_pred, pos_label=0)\n",
    "\n",
    "def binary_recall_positive_class(y_true, y_pred):\n",
    "    return recall_score(y_true, y_pred, pos_label=0)\n",
    "\n",
    "def binary_f1_positive_class(y_true, y_pred):\n",
    "    return f1_score(y_true, y_pred, pos_label=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7a4c5d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_binary_positive = make_scorer(binary_precision_positive_class, greater_is_better=True)\n",
    "recall_binary_positive = make_scorer(binary_recall_positive_class, greater_is_better=True)\n",
    "f1_binary_positive = make_scorer(binary_f1_positive_class, greater_is_better=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1dc673c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classifier(model, X_train, y_train, X_test=None, y_test=None):\n",
    "    cross_validator = StratifiedKFold(n_splits=5, random_state=0, shuffle=True)\n",
    "    scoring_metrics = {'accuracy': 'accuracy', 'precision_binary_positive': precision_binary_positive, 'recall_binary_positive': recall_binary_positive, 'f1_binary_positive': f1_binary_positive}\n",
    "    scores = cross_validate(model, X_train, y_train, cv=cross_validator, return_estimator=True, scoring=scoring_metrics)\n",
    "    return cross_validator, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f164c4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_classifier_roc(list_of_cross_validators, list_of_scores, x_train, y_train, x_test=None, y_test=None):\n",
    "\n",
    "    list_num_best_models = []\n",
    "    for i in range(len(list_of_scores)):\n",
    "        list_num_best_models.append(list_of_scores[i]['test_f1_binary_positive'].argmax())\n",
    "\n",
    "    best_models = []\n",
    "    for i in range(len(list_of_scores)):\n",
    "        best_models.append(list_of_scores[i]['estimator'][list_num_best_models[i]])\n",
    "\n",
    "    list_x_test = []\n",
    "    list_y_test = []\n",
    "    for j in range(len(list_num_best_models)):\n",
    "        if x_test is None and y_test is None:\n",
    "            _, test_num = list(list_of_cross_validators[j].split(x_train, y_train))[list_num_best_models[j]]\n",
    "            x_test, y_test = [], []\n",
    "            for i in test_num:\n",
    "                x_test.append(x_train[i])\n",
    "                y_test.append(y_train[i])\n",
    "            list_x_test.append(x_test)\n",
    "            list_y_test.append(y_test)\n",
    "            x_test = None\n",
    "            y_test = None\n",
    "\n",
    "    list_y_proba = []\n",
    "    for i in range(len(best_models)):\n",
    "        list_y_proba.append(best_models[i].predict_proba(list_x_test[i])[:, 1])\n",
    "\n",
    "    list_fpr = [0] * len(list_y_proba)\n",
    "    list_tpr = [0] * len(list_y_proba)\n",
    "    for i in range(len(list_y_proba)):\n",
    "        list_fpr[i], list_tpr[i], _ = roc_curve(list_y_test[i], list_y_proba[i])\n",
    "\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    lw = 2\n",
    "    for i in range(len(list_tpr)):\n",
    "        plt.plot(\n",
    "            list_fpr[i],\n",
    "            list_tpr[i],\n",
    "            lw=lw,\n",
    "            label=f'{str(best_models[i]).split(\"(\")[0]}\\nAUC = {round(roc_auc_score(list_y_test[i], list_y_proba[i]), 5)}'\n",
    "        )\n",
    "    plt.plot([0, 1], [0, 1], color=\"navy\", lw=lw, linestyle=\"--\")\n",
    "    plt.xlim([-0.1, 1.1])\n",
    "    plt.ylim([-0.1, 1.1])\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title('ROC curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "da334ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_json_files(file_paths: list, output_file_path: str):\n",
    "    merged_data = []\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "            merged_data.extend(data if isinstance(data, list) else [data])\n",
    "\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "        json.dump(merged_data, output_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "decd39f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Text mining for incoming tasks based on the urgency/importance factors and task classification using machine learning tools',\n",
       "  'authors': \"['Yasser Ali Alshehri']\",\n",
       "  'date': 'March 2020',\n",
       "  'source': \"ICCDA '20: Proceedings of the 2020 4th International Conference on Compute and Data Analysis\",\n",
       "  'abstract': 'In workplaces, there is a massive amount of unstructured data from different sources. In this paper, we present a case study that explains how can through communications between employees, we can help to prioritize tasks requests to increase the efficiency of their works for both technical and non-technical workers. This involves managing daily incoming tasks based on their level of urgency and importance.To allow all workers to utilize the urgency-importance matrix as a time-management tool, we need to automate this tool. The textual content of incoming tasks are analyzed, and metrics related to urgency and importance are extracted. A third factor (i.e., the response variable) is defined based on the two input variables (urgency and importance). Then, machine learning applied to the data to predict the class of incoming tasks based on data outcome desired. We used ordinal regression, neural networks, and decision tree algorithms to predict the four levels of task priority. We measure the performance of all using recalls, precisions, and F-scores. All classifiers perform higher than 89% in terms of all measures.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3388142.3388153',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Text mining for malware classification using multivariate all repeated patterns detection',\n",
       "  'authors': \"['Konstantinos F. Xylogiannopoulos', 'Panagiotis Karampelas', 'Reda Alhajj']\",\n",
       "  'date': 'August 2019',\n",
       "  'source': \"ASONAM '19: Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining\",\n",
       "  'abstract': 'Mobile phones have become nowadays a commodity to the majority of people. Using them, people are able to access the world of Internet and connect with their friends, their colleagues at work or even unknown people with common interests. This proliferation of the mobile devices has also been seen as an opportunity for the cyber criminals to deceive smartphone users and steel their money directly or indirectly, respectively, by accessing their bank accounts through the smartphones or by blackmailing them or selling their private data such as photos, credit card data, etc. to third parties. This is usually achieved by installing malware to smartphones masking their malevolent payload as a legitimate application and advertise it to the users with the hope that mobile users will install it in their devices. Thus, any existing application can easily be modified by integrating a malware and then presented it as a legitimate one. In response to this, scientists have proposed a number of malware detection and classification methods using a variety of techniques. Even though, several of them achieve relatively high precision in malware classification, there is still space for improvement. In this paper, we propose a text mining all repeated pattern detection method which uses the decompiled files of an application in order to classify a suspicious application into one of the known malware families. Based on the experimental results using a real malware dataset, the methodology tries to correctly classify (without any misclassification) all randomly selected malware applications of 3 categories with 3 different families each.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3341161.3350841',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Comparative Study between Traditional Machine Learning and Deep Learning Approaches for Text Classification',\n",
       "  'authors': \"['Cannannore Nidhi Kamath', 'Syed Saqib Bukhari', 'Andreas Dengel']\",\n",
       "  'date': 'August 2018',\n",
       "  'source': \"DocEng '18: Proceedings of the ACM Symposium on Document Engineering 2018\",\n",
       "  'abstract': 'In this contemporaneous world, it is an obligation for any organization working with documents to end up with the insipid task of classifying truckload of documents, which is the nascent stage of venturing into the realm of information retrieval and data mining. But classification of such humongous documents into multiple classes, calls for a lot of time and labor. Hence a system which could classify these documents with acceptable accuracy would be of an unfathomable help in document engineering. We have created multiple classifiers for document classification and compared their accuracy on raw and processed data. We have garnered data used in a corporate organization as well as publicly available data for comparison. Data is processed by removing the stop-words and stemming is implemented to produce root words. Multiple traditional machine learning techniques like Naive Bayes, Logistic Regression, Support Vector Machine, Random forest Classifier and Multi-Layer Perceptron are used for classification of documents. Classifiers are applied on raw and processed data separately and their accuracy is noted. Along with this, Deep learning technique such as Convolution Neural Network is also used to classify the data and its accuracy is compared with that of traditional machine learning techniques. We are also exploring hierarchical classifiers for classification of classes and subclasses. The system classifies the data faster and with better accuracy than if done manually. The results are discussed in the results and evaluation section.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3209280.3209526',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Where is the road for issue reports classification based on text mining?',\n",
       "  'authors': \"['Qiang Fan', 'Yue Yu', 'Gang Yin', 'Tao Wang', 'Huaimin Wang']\",\n",
       "  'date': 'November 2017',\n",
       "  'source': \"ESEM '17: Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement\",\n",
       "  'abstract': 'Currently, open source projects receive various kinds of issues daily, because of the extreme openness of Issue Tracking System (ITS) in GitHub. ITS is a labor-intensive and time-consuming task of issue categorization for project managers. However, a contributor is only required a short textual abstract to report an issue in GitHub. Thus, most traditional classification approaches based on detailed and structured data (e.g., priority, severity, software version and so on) are difficult to adopt. In this paper, issue classification approaches on a large-scale dataset, including 80 popular projects and over 252,000 issue reports collected from GitHub, were investigated. First, four traditional text-based classification methods and their performances were discussed. Semantic perplexity (i.e., an issues description confuses bug-related sentences with nonbug-related sentences) is a crucial factor that affects the classification performances based on quantitative and qualitative study. Finally, A two-stage classifier framework based on the novel metrics of semantic perplexity of issue reports was designed. Results show that our two-stage classification can significantly improve issue classification performances.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ESEM.2017.19',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Identification of Overpricing in the Purchase of Medication by the Federal Government of Brazil, Using Text Mining and Clustering Based on Ontology',\n",
       "  'authors': \"['Marco Aurelio O. S. Correa', 'Adriano Galindo Leal']\",\n",
       "  'date': 'August 2018',\n",
       "  'source': \"ICCBDC '18: Proceedings of the 2018 2nd International Conference on Cloud and Big Data Computing\",\n",
       "  'abstract': \"Increasing the transparency level in his actions and spending is one of the primary duties of the Brazilian Federal Government. The creation of laws that oblige full disclose of all its expenditures through transparency portals enables citizens to supervise all government entities. However, only the dissemination of these data, without a definite standard or the availability of data analysis tools, does not guarantee that the citizen is empowered to play his role. Therefore hence, the objective of this work is to identify overprice in the acquisition of products purchased by the federal government of Brazil using the unstructured data available on the Transparency Portal. The last two-years' worth of purchasing data, available in the Transparency Portal, were extracted, processed and stored. Due to his diverse nature and high volume of data, this study focused only on medicines purchased by the Ministry of Health. Ontology-based text mining and clustering techniques were applied for automatic identification and classification of products. The processing of this information was done through text mining and clustering, based on the ontology registered in another database of the Brazilian government. Because of this work, a consolidated price base per medication was created to allow the identification of distortions in prices practised, facilitating the identification of cases that merit further investigation to unravel fraud to the treasury.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3264560.3264569',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Computational Estimation by Scientific Data Mining with Classical Methods to Automate Learning Strategies of Scientists',\n",
       "  'authors': \"['Aparna S. Varde']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Knowledge Discovery from Data',\n",
       "  'abstract': 'Experimental results are often plotted as 2-dimensional graphical plots (aka graphs) in scientific domains depicting dependent versus independent variables to aid visual analysis of processes. Repeatedly performing laboratory experiments consumes significant time and resources, motivating the need for computational estimation. The goals are to estimate the graph obtained in an experiment given its input conditions, and to estimate the conditions that would lead to a desired graph. Existing estimation approaches often do not meet accuracy and efficiency needs of targeted applications. We develop a computational estimation approach called AutoDomainMine that integrates clustering and classification over complex scientific data in a framework so as to automate classical learning methods of scientists. Knowledge discovered thereby from a database of existing experiments serves as the basis for estimation. Challenges include preserving domain semantics in clustering, finding matching strategies in classification, striking a good balance between elaboration and conciseness while displaying estimation results based on needs of targeted users, and deriving objective measures to capture subjective user interests. These and other challenges are addressed in this work. The AutoDomainMine approach is used to build a computational estimation system, rigorously evaluated with real data in Materials Science. Our evaluation confirms that AutoDomainMine provides desired accuracy and efficiency in computational estimation. It is extendable to other science and engineering domains as proved by adaptation of its sub-processes within fields such as Bioinformatics and Nanotechnology.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3502736',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Comparative Study of Heart Disease Diagnosis Using Top Ten Data Mining Classification Algorithms',\n",
       "  'authors': \"['I. Ketut Agung Enriko']\",\n",
       "  'date': 'June 2019',\n",
       "  'source': \"ICFET '19: Proceedings of the 5th International Conference on Frontiers of Educational Technologies\",\n",
       "  'abstract': 'Data mining has been used for many purposes, especially for prediction system. In healthcare, data mining algorithms often used in disease diagnosis. Meanwhile, heart disease is known as a primary cause of death over the years. Many studies have been performed in heart disease diagnosis using data mining methods. There are some popular data mining algorithms that can be used in heart disease diagnosis, for example, k-Nearest Neighbor, CART, and AdaBoost. The algorithms are used to analyze a sample of cardiovascular patients data and predict the heart disease type that they suffer. Some parameters are taken from the patient, including EKG morphology, blood pressure, and information about the existence of chest pain, shortness of breath, palpitation, and cold sweat. In this study, medical records data are collected from Harapan Kita Hospital and utilized as a dataset sample in this research. Top ten data mining classification algorithms are used in diagnosing heart disease from Harapan Kita Hospital data and examining their performance by checking the accuracy and speed.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3338188.3338220',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Feature Selection based Arabic Text Classification using Different Machine Learning Algorithms: Comparative Study',\n",
       "  'authors': \"['Sakina Rim Bennabi', 'Zakaria Elberrichi']\",\n",
       "  'date': 'June 2020',\n",
       "  'source': \"ICIST '20: Proceedings of the 10th International Conference on Information Systems and Technologies\",\n",
       "  'abstract': 'Feature selection is a method of data pre-processing widely used when mining large data, such as textual classification. Several studies have been conducted to compare the different methods of feature selection applied to corpora in English. Unfortunately, a small number of works concern the Arabic language. This article aims to present a comparative study of different feature selection techniques including: Chi2, the ANOVA method and mutual information, applied on a corpus in Arabic language, while also diversifying the machine learning algorithms (Naive Bayes, SVM and KNN). This experimental study has shown in general that reducing dimensionality with feature selection techniques has slightly affected the performance of textual classification, reducing the size of the corpus by up to 1%.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3447568.3448531',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Clustering Algorithms for Spatial Data Mining',\n",
       "  'authors': \"['Chetashri Bhadane', 'Ketan Shah']\",\n",
       "  'date': 'April 2020',\n",
       "  'source': \"ICGDA '20: Proceedings of the 2020 3rd International Conference on Geoinformatics and Data Analysis\",\n",
       "  'abstract': \"With the advances in mobile and wireless technologies, there has been a rise in applications that track and share the users' geospatial data. People use several social networking sites such as Twitter, Facebook and Flickr, where they share their status updates. With the integration of Global Positioning System (GPS) with mobile phones, it is now possible to share one's locations on these social networks. GPS allows us to record and track a person's movement along with the timestamp. The data set obtained from these GPS logs is vast and is widely used to analyze the users' movement patterns. Specifically, we can find out significant locations based on the number of users present at that location and the time spent by them at such places. Once significant places have been identified, it is also possible to identify the semantic importance of these locations. This paper presents an overview of the clustering techniques used to find important places of interest using large GPS based mobility datasets. Four clustering algorithms, K-Means, DBSCAN, OPTICS and Hierarchical, are implemented, and performance is tested using real-time data of 50 users collected over 2--5 years. Performance summary depicts that K-Means and DBSCAN perform well for spatial data.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3397056.3397068',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': \"Applying text mining to predict learners' cognitive engagement\",\n",
       "  'authors': \"['Hayati Hind', 'Mohammed Khalidi Idrissi', 'Samir Bennani']\",\n",
       "  'date': 'October 2017',\n",
       "  'source': \"SCAMS '17: Proceedings of the Mediterranean Symposium on Smart City Application\",\n",
       "  'abstract': \"In academic process, engagement represent one of the important success' factor. This paper proposes a predictive system for learners' cognitive engagement based on their online discussion forums participation. In the first level the ontology OWL and the LSA method are used to perform a semantic classification system of the threads according to a specific context chosen by the tutor. Then the Text mining, as a predictive method, is applied to the classified threads and learners' participation in the forums.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3175628.3175655',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Text Mining Approach for Identifying Research Trends',\n",
       "  'authors': \"['Snezhana Sulova']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"CompSysTech '21: Proceedings of the 22nd International Conference on Computer Systems and Technologies\",\n",
       "  'abstract': 'With the increase of unstructured data, the issues connected with automatic text processing, the categorization of documents and the discovery of topics have become objects of growing interest. In order to improve the process of grouping and processing research publications, we would like to propose a method based upon natural language processing. It is based on text mining technologies which aim to identify key tendencies in documents. It processes the content of publications by clustering and identifies the topics of each identified group. This analysis helps by identifying key tendencies as well as discovering emerging new areas of research. Publications from the research literature database, Scopus, were used to test the approach. The topic of the publications is “the application of digital technologies in the logistics business”. The experiments were completed using the RapidMiner Studio software.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3472410.3472433',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Investigate Transitions into Drug Addiction through Text Mining of Reddit Data',\n",
       "  'authors': \"['John Lu', 'Sumati Sridhar', 'Ritika Pandey', 'Mohammad Al Hasan', 'Georege Mohler']\",\n",
       "  'date': 'July 2019',\n",
       "  'source': \"KDD '19: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining\",\n",
       "  'abstract': \"Increasing rates of opioid drug abuse and heightened prevalence of online support communities underscore the necessity of employing data mining techniques to better understand drug addiction using these rapidly developing online resources. In this work, we obtained data from Reddit, an online collection of forums, to gather insight into drug use/misuse using text snippets from users narratives. Specifically, using users' posts, we trained a binary classifier which predicts a user's transitions from casual drug discussion forums to drug recovery forums. We also proposed a Cox regression model that outputs likelihoods of such transitions. In doing so, we found that utterances of select drugs and certain linguistic features contained in one's posts can help predict these transitions. Using unfiltered drug-related posts, our research delineates drugs that are associated with higher rates of transitions from recreational drug discussion to support/recovery discussion, offers insight into modern drug culture, and provides tools with potential applications in combating the opioid crisis.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3292500.3330737',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Assessment of Vulnerability Severity using Text Mining',\n",
       "  'authors': \"['Georgios Spanos', 'Lefteris Angelis', 'Dimitrios Toloudis']\",\n",
       "  'date': 'September 2017',\n",
       "  'source': \"PCI '17: Proceedings of the 21st Pan-Hellenic Conference on Informatics\",\n",
       "  'abstract': \"Software1 vulnerabilities are closely associated with information systems security, a major and critical field in today's technology. Vulnerabilities constitute a constant and increasing threat for various aspects of everyday life, especially for safety and economy, since the social impact from the problems that they cause is complicated and often unpredictable. Although there is an entire research branch in software engineering that deals with the identification and elimination of vulnerabilities, the growing complexity of software products and the variability of software production procedures are factors contributing to the ongoing occurrence of vulnerabilities, Hence, another area that is being developed in parallel focuses on the study and management of the vulnerabilities that have already been reported and registered in databases. The information contained in such databases includes, a textual description and a number of metrics related to vulnerabilities. The purpose of this paper is to investigate to what extend the assessment of the vulnerability severity can be inferred directly from the corresponding textual description, or in other words, to examine the informative power of the description with respect to the vulnerability severity. For this purpose, text mining techniques, i.e. text analysis and three different classification methods (decision trees, neural networks and support vector machines) were employed. The application of text mining to a sample of 70,678 vulnerabilities from a public data source shows that the description itself is a reliable and highly accurate source of information for vulnerability prioritization.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3139367.3139390',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Intelligent Data Analysis using Optimized Support Vector Machine Based Data Mining Approach for Tourism Industry',\n",
       "  'authors': \"['Ms Promila Sharma', 'Uma Meena', 'Girish Kumar Sharma']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Knowledge Discovery from Data',\n",
       "  'abstract': 'Data analysis involves the deployment of sophisticated approaches from data mining methods, information theory, and artificial intelligence in various fields like tourism, hospitality, and so on for the extraction of knowledge from the gathered and preprocessed data. In tourism, pattern analysis or data analysis using classification is significant for finding the patterns that represent new and potentially useful information or knowledge about the destination and other data. Several data mining techniques are introduced for the classification of data or patterns. However, overfitting, less accuracy, local minima, sensitive to noise are the drawbacks in some existing data mining classification methods. To overcome these challenges, Support vector machine with Red deer optimization (SVM-RDO) based data mining strategy is proposed in this article. Extended Kalman filter (EKF) is utilized in the first phase, i.e., data cleaning to remove the noise and missing values from the input data. Mantaray foraging algorithm (MaFA) is used in the data selection phase, in which the significant data are selected for the further process to reduce the computational complexity. The final phase is the classification, in which SVM-RDO is proposed to access the useful pattern from the selected data. PYTHON is the implementation tool used for the experiment of the proposed model. The experimental analysis is done to show the efficacy of the proposed work. From the experimental results, the proposed SVM-RDO achieved better accuracy, precision, recall, and F1 score than the existing methods for the tourism dataset. Thus, it is showed the effectiveness of the proposed SVM-RDO for pattern analysis.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3494566',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Integration of data mining techniques in e-learning systems: Clustering Profil of Lerners and Recommender Course System',\n",
       "  'authors': \"['A. El Moustamid', 'E. En-Naimi', 'J. El Bouhdidi']\",\n",
       "  'date': 'March 2017',\n",
       "  'source': \"BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications\",\n",
       "  'abstract': \"Data mining aims at the extraction of knowledge from large quantities of data by automatic or semi-automatic methods. In the field of education and training e-learning, Data Mining techniques as clustering (The aim of this technique is to identify groups that are similar in some aspect) [21], prediction (is used for developing model where combination of data viewed from different aspects can effect single viewed data. Here, is important to watch how predictors variables are effecting predicted variable)[21], classification are applied in e-learning to improve the level of learners by providing learning according to the learners' requirement) are used to analyze learners profile, to predict and improve student performance. The aim of our project is to develop a system capable of analyzing learners profile and indexing web videos in order to offer learners a rich database with courses that correspond to their levels.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3090354.3090453',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Text Mining in Cybersecurity: A Systematic Literature Review',\n",
       "  'authors': \"['Luciano Ignaczak', 'Guilherme Goldschmidt', 'Cristiano André Da Costa', 'Rodrigo Da Rosa Righi']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Computing Surveys',\n",
       "  'abstract': 'The growth of data volume has changed cybersecurity activities, demanding a higher level of automation. In this new cybersecurity landscape, text mining emerged as an alternative to improve the efficiency of the activities involving unstructured data. This article proposes a Systematic Literature Review (SLR) to present the application of text mining in the cybersecurity domain. Using a systematic protocol, we identified 2,196 studies, out of which 83 were summarized. As a contribution, we propose a taxonomy to demonstrate the different activities in the cybersecurity domain supported by text mining. We also detail the strategies evaluated in the application of text mining tasks and the use of neural networks to support activities involving unstructured data. The work also discusses text classification performance aiming its application in real-world solutions. The SLR also highlights open gaps for future research, such as the analysis of non-English content and the intensification in the usage of neural networks.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3462477',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Feature-based Facebook reviews process model for e-management using data mining',\n",
       "  'authors': \"['Anish Kumar Varudharajulu', 'Yongsheng Ma']\",\n",
       "  'date': 'January 2019',\n",
       "  'source': \"IC4E '19: Proceedings of the 10th International Conference on E-Education, E-Business, E-Management and E-Learning\",\n",
       "  'abstract': 'The data generated from online communication acts as potential gold mines for discovering knowledge for researchers. A large amount of data is also generated in the form of web documents, emails, blogs, and feedback, etc. Text analytics is being significantly employed to mine important information. Opinion mining is the process of extracting human thoughts and perceptions from unstructured texts. The showstopper for designing an opinion mining system for analyzing reviews arise from the fact that customer reviews are often noisy. These reviews are informally written. In addition, they are subjected to spelling mistakes, grammatical errors, improper punctuation and irrational capitalization. This paper focuses on analyzing the different classification and clustering algorithms aimed at extracting and consolidating opinions of customers from social media sites like Facebook, Twitter and through surveys, at multiple levels of granularity to monitor and measure customer satisfaction. Ours is an automated approach, in which the system aids in the process of knowledge assimilation for knowledge-based building and also performs the analytics. Domain experts ratify the knowledge base and also provide training datasets for the system to intuitively gather more instances for ratification. The system identifies opinion expressions as phrases containing opinion words, opinionated features and also opinion modifiers. These expressions are categorized as positive, negative or neutral. Opinion expressions are identified and categorized using localized linguistic techniques. Opinions can be congregated at any desired level of specificity i.e. feature level or product level, user level or service level, etc. We have developed a system based on this approach, which provides the user with a platform to analyze opinion expressions crawled from a set of pre-defined datasets.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3306500.3306514',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Automatic topic classification of test cases using text mining at an Android smartphone vendor',\n",
       "  'authors': \"['Junji Shimagaki', 'Yasutaka Kamei', 'Naoyasu Ubayashi', 'Abram Hindle']\",\n",
       "  'date': 'October 2018',\n",
       "  'source': \"ESEM '18: Proceedings of the 12th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement\",\n",
       "  'abstract': 'Background: An Android smartphone is an ecosystem of applications, drivers, operating system components, and assets. The volume of the software is large and the number of test cases needed to cover the functionality of an Android system is substantial. Enormous effort has been already taken to properly quantify \"what features and apps were tested and verified?\". This insight is provided by dashboards that summarize test coverage and results per feature. One method to achieve this is to manually tag or label test cases with the topic or function they cover, much like function points. At the studied Android smartphone vendor, tests are labelled with manually defined tags, so-called \"feature labels (FLs)\", and the FLs serve to categorize 100s to 1000s test cases into 10 to 50 groups. Aim: Unfortunately for developers, manual assignment of FLs to 1000s of test cases is a time consuming task, leading to inaccurately labeled test cases, which will render the dashboard useless. We created an automated system that suggests tags/labels to the developers for their test cases rather than manual labeling. Method: We use machine learning models to predict and label the functionality tested by 10,000 test cases developed at the company. Results: Through the quantitative experiments, our models achieved acceptable F-1 performance of 0.3 to 0.88. Also through the qualitative studies with expert teams, we showed that the hierarchy and path of tests was a good predictor of a feature\\'s label. Conclusions: We find that this method can reduce tedious manual effort that software developers spent classifying test cases, while providing more accurate classification results.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3239235.3268927',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Discriminative Topic Mining via Category-Name Guided Text Embedding',\n",
       "  'authors': \"['Yu Meng', 'Jiaxin Huang', 'Guangyuan Wang', 'Zihan Wang', 'Chao Zhang', 'Yu Zhang', 'Jiawei Han']\",\n",
       "  'date': 'April 2020',\n",
       "  'source': \"WWW '20: Proceedings of The Web Conference 2020\",\n",
       "  'abstract': 'Mining a set of meaningful and distinctive topics automatically from massive text corpora has broad applications. Existing topic models, however, typically work in a purely unsupervised way, which often generate topics that do not fit users’ particular needs and yield suboptimal performance on downstream tasks. We propose a new task, discriminative topic mining, which leverages a set of user-provided category names to mine discriminative topics from text corpora. This new task not only helps a user understand clearly and distinctively the topics he/she is most interested in, but also benefits directly keyword-driven classification tasks. We develop CatE, a novel category-name guided text embedding method for discriminative topic mining, which effectively leverages minimal user guidance to learn a discriminative embedding space and discover category representative terms in an iterative manner. We conduct a comprehensive set of experiments to show that CatE mines high-quality set of topics guided by category names only, and benefits a variety of downstream applications including weakly-supervised classification and lexical entailment direction identification.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3366423.3380278',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Text classification of Diseases Treated by Traditional Chinese Medicine Prescription based on machine learning',\n",
       "  'authors': \"['Hanqing Zhao', 'Bo Han', 'Chen Li']\",\n",
       "  'date': 'September 2020',\n",
       "  'source': \"ISAIMS '20: Proceedings of the 1st International Symposium on Artificial Intelligence in Medical Sciences\",\n",
       "  'abstract': 'OBJECTIVE: To explore the application of machine learning in th e identification of diseases treated by traditional Chinese medicine prescriptions. METHODS: Based on the composition of the text document of Chinese medicine prescriptions, the prescriptions were divided into cough, headache and diarrhea. THUTCT were introduced to establish and train two machine learning text classification models, LibSVM and LibLinear, and the prescriptions to be deter mined were put into the model for classification and prediction. R ESULTS: The Precision rate of LibSVM model and Liblinear mod el were 0.7283 and 0.6690. Seven prescriptions were classified an d predicted, and the results were in line with expectations. CONCLUSION: THUCTC has good universality for the content of TCM prescriptions, high classification accuracy and fast testing speed, which is suitable for the text classification and discrimination research of TCM prescriptions for diseases.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3429889.3429895',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Petroleum Engineering Data Text Classification Using Convolutional Neural Network Based Classifier',\n",
       "  'authors': \"['Tzuhan Hsu', 'Yaoqin Zhang']\",\n",
       "  'date': 'May 2018',\n",
       "  'source': \"ICMLT '18: Proceedings of the 2018 International Conference on Machine Learning Technologies\",\n",
       "  'abstract': 'In this paper, authors introduce a convolutional neural network (CNN) based classification approach for petroleum engineering event. Recently, text classification problem has raised substantial attention in the field of machine learning. The aim of text classification process is to assign correct label to a text on the basis of its content. However, due to the imprecise of feature representation, most of former text classification methods cannot export satisfying result. To solve this problem, authors propose a model based on convolutional neural network. Authors conduct experiments with real world petroleum engineering data text, artificially labelled into 6 categories. Beside purposed CNN-based model, the authors also implement other baseline machine learning approaches: k nearest neighbor, random forest, support vector machines, recurrent neural network, gated recurrent units, long short-term memory. Among all these models, our CNN-based model reach best performance in text classification task. In the last part of this paper, authors compare and analysis their purpose and baseline model.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3231884.3231898',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Text classification using Fuzzy TF-IDF and Machine Learning Models',\n",
       "  'authors': \"['Mariem Bounabi', 'Karim El Moutaouakil', 'Khalid Satori']\",\n",
       "  'date': 'October 2019',\n",
       "  'source': \"BDIoT '19: Proceedings of the 4th International Conference on Big Data and Internet of Things\",\n",
       "  'abstract': 'The representation of the information has an important impact on the text classification task. Several weighting methods were proposed in the literature, and the term frequency-inverse term frequency (TFIDF), the most know on the text treatment field. The FTF-IDF is a vector representation where the components of the TFIDF are presented as inputs to the Fuzzy Inference System (FIS). In this work, we compare several Machin Learning algorithms such as Naïve Bayes and its derivatives, SVM and Random forest classifiers, using the FTF-IDF representation. To improve the quality of the used classifiers, we call sum attribute selection methods. The recognition rate, for the tested systems, is satisfied, where the system based on naïve Bayes classifier, the FTF-IDF weighting terms, and the info gain select attributes method gives 98.7% as accuracy.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3372938.3372956',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Text mining for plagiarism detection: multivariate pattern detection for recognition of text similarities',\n",
       "  'authors': \"['Konstantinos Xylogiannopoulos', 'Panagiotis Karampelas', 'Reda Alhajj']\",\n",
       "  'date': 'August 2018',\n",
       "  'source': \"ASONAM '18: Proceedings of the 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining\",\n",
       "  'abstract': 'The problem of plagiarism the recent years has been intensified by the availability of information in digital form and the accessibility of the electronic libraries through the Internet. As a result, plagiarism detection has been transformed into a big data analytics problem since the number of digital sources is extravagant and a new document needs to be compared with millions of other existing documents. In this paper, a text mining methodology is proposed that can detect all common patterns between a document and the documents in a reference database. The technique is based on a pattern detection algorithm and the corresponding data structure that enables the algorithm to detect all common patterns. The methodology has been applied in a well-defined dataset providing very promising results identifying difficult cases of plagiarism such as technical disguise.',\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3382225.3382424',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Mining Moocs Videos Metadata Using Classification Techniques',\n",
       "  'authors': \"['El Harrak Othman', 'Ghadi Abderrahim', 'El Bouhdidi Jaber']\",\n",
       "  'date': 'March 2017',\n",
       "  'source': \"BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications\",\n",
       "  'abstract': \"In few years, the internet has become the first source of information for most people, Today MOOCs makes it possible for everyone to access to the education over the world, it's represents an emerging methodology of online teaching and an important development in open education. Due to the rapid development of Moocs and the rapid growth of digital data and video database over the Internet, it is becoming very difficult for learners to browsing and choosing the best training for them. The scientific community has increased the amount of research into new technologies, with a view to improve the digital video utilization: its archiving, indexing, accessibility, acquisition, store and even its process and usability. All these parts of the video utilization entail the necessity of the extraction of all important information of a video, especially in cases of lack of metadata information. Web video mining is retrieving the content using data mining techniques from World Wide Web. There are two approaches for web video mining using traditional image processing (signal processing) and metadata based approach. In this paper, we present the various research makes in this subject and we propose an effective methodology to extract the metadata from Moocs videos and classify them based on the extracted metadata by applying data mining techniques.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3090354.3090450',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Text mining with HathiTrust',\n",
       "  'authors': \"['Eleanor Dickson Koehl', 'Ryan Dubnicek']\",\n",
       "  'date': 'June 2019',\n",
       "  'source': \"JCDL '19: Proceedings of the 18th Joint Conference on Digital Libraries\",\n",
       "  'abstract': 'This tutorial will introduce attendees to the HathiTrust Research Center and its tools and services for computational text analysis research. HTRC leverages the scope and scale of the HathiTrust Digital Library collection to create opportunities for researchers to perform text data mining on subsets of the corpus. Attendees will develop skills that will allow them to conduct basic text analysis research using HathiTrust data, learn techniques for engaging with scholars on their text data mining research, and explore strategies for computational access to large-scale digital collections.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/JCDL.2019.00115',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Mining Free-Text Medical Notes for Suicide Risk Assessment',\n",
       "  'authors': \"['Marios Adamou', 'Grigoris Antoniou', 'Elissavet Greasidou', 'Vincenzo Lagani', 'Paulos Charonyktakis', 'Ioannis Tsamardinos']\",\n",
       "  'date': 'July 2018',\n",
       "  'source': \"SETN '18: Proceedings of the 10th Hellenic Conference on Artificial Intelligence\",\n",
       "  'abstract': 'Suicide has been considered as an important public health issue for a very long time, and is one of the main causes of death worldwide. Despite suicide prevention strategies being applied, the rate of suicide has not changed substantially over the past decades. Advances in machine learning make it possible to attempt to predict suicide based on the analysis of relevant data to inform clinical practice. This paper reports on findings from the analysis of data of patients who died by suicide in the period 2013-2016 and made use of both structured data and free-text medical notes. We focus on examining various text-mining approaches to support risk assessment. The results show that using advance machine learning and text-mining techniques, it is possible to predict within a specified period which people are most at risk of taking their own life at the time of referral to a mental health service.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3200947.3201020',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Classifying Indonesian Online Articles as Advertisement Placement Base Using Text Mining',\n",
       "  'authors': \"['Nadhira Tasya', 'Arian Dhini']\",\n",
       "  'date': 'July 2017',\n",
       "  'source': 'ICBIM 2017: Proceedings of the International Conference on Business and Information Management',\n",
       "  'abstract': 'Rapid development in technological aspect resulting in growing level of human needs for the latest news, so that emerged a new trend of publishing and accessing news through online media or usually called online journalism. In addition, the number of people who sell and purchase through online sites also continues to increase and this opportunity is utilized by the company and the advertiser by implementing targeted web advertising. However, the high number of articles that have been published and accessed leads to great opportunities for errors in determining where to place the ads. Therefore, it needs a system that can categorize articles accessed by users as the basis of advertisement placement by the company and this classification system can be done by applying the method of Data Mining and Text Mining. This research uses document data in the form of article content that will be categorized into twenty categories of class of advertisement by using Text Mining technique with Support Vector Machine algorithm. The results of this study may be used by companies or advertisers as a basis for placement of ads on selected online media sites.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3134271.3134288',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Compare Machine Learning Models in Text Classification Using Steam User Reviews',\n",
       "  'authors': \"['Youchen Miao', 'Zeyu Jin', 'Yumeng Zhang', 'Yuchen Chen', 'Junren Lai']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"ICSED '21: Proceedings of the 2021 3rd International Conference on Software Engineering and Development\",\n",
       "  'abstract': 'Text Classification and Sentiment Analysis of game reviews are viewed as important parts in not only academic fields but also in game studies. In this paper, with more than 400 thousand game reviews on Steam platform, we preprocess the data using different libraries (sklearn, nltk, and spaCy) and use them as inputs to build three sentiment classification models based on different algorithms (Naive Bayes, SVM, and Random Forest). In contrast to previous studies that only focus on different sentiment analysis models, our paper also highlights the use of different APIs to preprocess the data and their corresponding model performance. The results show that no matter which API we choose, Random Forest models always perform the best. However, in terms of training time, Naive Bayes is the fastest. This work can be used to apply grid search for researchers to automatically find the optimum API before conducting sentiment analysis in the future.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3507473.3507480',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'To apply Data Mining for Classification of Crowd sourced Software Requirements',\n",
       "  'authors': \"['Soonh Taj', 'Qasim Arain', 'Imran Memon', 'Asma Zubedi']\",\n",
       "  'date': 'April 2019',\n",
       "  'source': \"ICSIE '19: Proceedings of the 8th International Conference on Software and Information Engineering\",\n",
       "  'abstract': \"Now a day's main focus of developers is to build quality software that works according to customer needs and for this reason it is necessary to gather right requirements as requirement elicitation is the critical step that impacts on the success of software project as misinterpreted requirements leads to the failure of software project. By keeping this in mind a research is carried out on improving requirements elicitation process and automating the process of classifying requirements. In this research, a model is proposed which will help in this scenario for requirements elicitation and requirement classification. This paper presents a model in which crowd sourcing approach is used so that customers, end users, stakeholders, developers and software engineers can make active participation for requirement elicitation process and requirements gathered using crowdsourcing approach are used by model for classification process i.e. classification of requirements into functional and non-functional requirements. For the proof of proposed model a case study is conducted. Results of case study provided the usefulness and efficiency of proposed model for classification of crowd sourced software requirements.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3328833.3328837',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Evaluation of retweet clustering method classification method using retweets on Twitter without text data',\n",
       "  'authors': \"['K. Uchida', 'F. Toriumi', 'T. Sakaki']\",\n",
       "  'date': 'August 2017',\n",
       "  'source': \"WI '17: Proceedings of the International Conference on Web Intelligence\",\n",
       "  'abstract': \"Burst phenomena, which frequently occur on social media, are caused by such social events as flaming on the internet, elections, and natural disasters. To understand people's thoughts and feelings, we must classify their opinions from burst phenomena. Therefore, classification methods that categorize tweets are critical. However, since most classification methods focus on text mining, they cannot group tweets by topics because each tweet has poor linguistic similarities. We used a non-text-based classification method proposed by Baba et al. that groups tweets by topics, even if they have poor linguistic similarities, and verified its validity by comparing it with a text-based classification method in two different evaluations: qualitative and quantitative. In the qualitative evaluation part, we did a questionnaire survey and validated the suitability of the topic clusters created using both the non-and text-based methods. Since evaluating the similarity of every pair of tweets in each topic is difficult, we evaluated the similarity between sampled pairs in the survey and acquired more appropriate topic clustering results using the non-text-based method than the text-based method. In the quantitative evaluation part, we focused on the robustness of each method against data reduction. Many approaches analyze social media data, especially because collecting data from social media is comparatively easy. However, since collecting the whole data of burst phenomena is very costly due to the vast amounts of available social media data, robustness against data reduction is an important index to evaluate classification methods. With the non-text-based method, over 55% of the pairs of tweets in the same cluster were also included in the same cluster even when the data were reduced to 10% in all three of our example cases. In this paper, as a source we focus on Twitter, one of the most popular microblogging services. Using clustering to conduct detailed case analyses, we scrutinized three burst cases that include natural disasters and flaming on the internet and found that a non-text-based method more effectively classified tweets in burst phenomena than a text-based method.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3106426.3106451',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Analysing digital banking reviews using text mining',\n",
       "  'authors': \"['Li Chen Cheng', 'Legaspi Rhea Sharmayne']\",\n",
       "  'date': 'December 2020',\n",
       "  'source': \"ASONAM '20: Proceedings of the 12th IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining\",\n",
       "  'abstract': 'Digital banks are new entrants in the banking industry in the Philippines as they only started late 2018. Since then, a handful of players have and are still emerging. With more and more people becoming technologically savvy, it is very critical for financial institutions to develop a digital banking application that will stand out from the competition. This paper aims to use text mining methods to analyse digital banking application reviews. This study will perform topic modelling using LDA to explore customer concerns and will mine association rules between the digital banking features with the review score. The results will reveal which areas the digital banking application can further optimize for customer satisfaction and retention.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ASONAM49781.2020.9381429',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Transfer Learning to Timed Text Based Video Classification Using CNN',\n",
       "  'authors': \"['Zenun Kastrati', 'Ali Shariq Imran', 'Arianit Kurti']\",\n",
       "  'date': 'June 2019',\n",
       "  'source': 'WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics',\n",
       "  'abstract': 'Open educational video resources are gaining popularity with a growing number of massive open online courses (MOOCs). This has created a niche for content providers to adopt effective solutions in automatically organizing and structuring of educational resources for maximum visibility. Recent advances in deep learning techniques are proving useful in managing and classifying resources into appropriate categories. This paper proposes one such convolutional neural network (CNN) model for classifying video lectures in a MOOC setting using a transfer learning approach. The model uses a time-aligned text transcripts corresponding to video lectures from six broader subject categories. Video lectures and their corresponding transcript dataset is gathered from the Coursera MOOC platform. Two different CNN models are proposed: i) CNN based classification using embeddings learned from our MOOC dataset, ii) CNN based classification using transfer learning. Word embeddings generated from two well known state-of-the-art pre-trained models Word2Vec and GloVe, are used in the transfer learning approach for the second case. The proposed CNN models are evaluated using precision, recall, and F1 score and the obtained performance is compared with both conventional and deep learning classifiers. The proposed CNN models have an F1 score improvement of 10-22 percentage points over DNN and conventional classifiers',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3326467.3326483',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Clustering legal artifacts using text mining',\n",
       "  'authors': \"['Zoi Lachana', 'Michalis Avgerinos Loutsaris', 'Charalampos Alexopoulos', 'Yannis Charalabidis']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"ICEGOV '21: Proceedings of the 14th International Conference on Theory and Practice of Electronic Governance\",\n",
       "  'abstract': \"The globalization of communication networks and the possibilities offered by the information and communication technologies (ICTs) significantly change the public sector's operation and services. Digital Governance is now integrated into administrations' policies and programs at all levels: local, regional, national, European. At the national level, there is a requirement to provide electronic public services according to citizens' needs while, in the sense of globalization, at the European level, there are many programs (e.g., the Europe 2005 and i2010 program) emphasizing the Digital Governance world (or better Digital Governance community) that indicates rapid changes not only in the sense of the change in the public sector's systems but also in the mentality that the public sector operates. On the other hand, Digital Governance's evolution affects societies intensively, emphasizing the importance of cross-border interaction and information sharing between them. [6]. Concerning the legal informatics domain, this can result in changing governments' operations in many ways [2]. By now, the massive amount of each country's legal information currently remains fragmented across multiple national databases and systems or even better legal databases. Most of these legal databases result from the significant advancements in the “legal informatics” research field that observed since governments have started to promote the development of legal information systems [9]. This research contributes to this purpose by developing an open and automated legal system capable of providing any EU country's legal information based on the existing ontologies.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3494193.3494202',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Data Mining and Opinion Mining: A Tool in Educational Context',\n",
       "  'authors': \"['Myriam Peñafiel', 'Stefanie Vásquez', 'Diego Vásquez', 'Juan Zaldumbide', 'Sergio Luján-Mora']\",\n",
       "  'date': 'July 2018',\n",
       "  'source': \"ICoMS '18: Proceedings of the 2018 1st International Conference on Mathematics and Statistics\",\n",
       "  'abstract': 'The use of the web as a universal communication platform generates large volumes of data (Big data), which in many cases, need to be processed so that they can become useful knowledge in face of the sceptics who have doubts about the credibility of such information. The use of web data that comes from educational contexts needs to be addressed, since that large amount of unstructured information is not being valued, losing valuable information that can be used. To solve this problem, we propose the use of data mining techniques such as sentiment analysis to validate the information that comes from the educational platforms. The objective of this research is to propose a methodology that allows the user to apply sentiment analysis in a simple way, because although some researchers have done it, very few do with data in the educational context. The results obtained prove that the proposal can be used in similar cases.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3274250.3274263',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Analyzing used-car web listings via text mining',\n",
       "  'authors': \"['Ayhan Demiriz', 'Fatma Cantaş']\",\n",
       "  'date': 'October 2017',\n",
       "  'source': \"IML '17: Proceedings of the 1st International Conference on Internet of Things and Machine Learning\",\n",
       "  'abstract': 'Used car trade is one of the major components of the world economies. It is not uncommon to sell a car by placing an internet advertisement irrespective of the geography in these days. A typical content of an advertisement is usually composed of two parts namely the structured and the free text data. The structured data may include some information about the asking price, make, model, year, mileage of the car and the contact info. In most cases, seller may give important clues about the car\\'s current conditions in the free text data where the title (head) of the advertisement can be included as free text too. This paper reports preliminary results from a text mining study conducted on 75K used car internet listings collected from two major car listing web sites in Turkey. As expected, the words and the phrases related to the description of the car are observed to be frequent. The leading concepts in the free text are found to be regarding how to describe the current condition of a car, for example \"no crash history\".',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3109761.3109782',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': \"Text Mining for Evaluating Authors' Birth and Death Years\",\n",
       "  'authors': \"['Dror Moghaz', 'Yaakov Hacohen-Kerner', 'Dov Gabbay']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Knowledge Discovery from Data',\n",
       "  'abstract': 'This article presents a unique method in text and data mining for finding the era, i.e., mining temporal data, in which an anonymous author was living. Finding this era can assist in the examination of a fake document or extracting the time period in which a writer lived. The study and the experiments concern Hebrew, and in some parts, Aramaic and Yiddish rabbinic texts. The rabbinic texts are undated and contain no bibliographic sections, posing an interesting challenge. This work proposes algorithms using key phrases and key words that allow the temporal organization of citations together with linguistic patterns. Based on these key phrases, key words, and the references, we established several types of “Iron-clad,” Heuristic and Greedy rules for estimating the years of birth and death of a writer in\\xa0an interesting classification task. Experiments were conducted on corpora, including documents authored by 12, 24, and 36 rabbinic writers and demonstrated promising results.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3281631',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Unsupervised behavioural mining and clustering for malware family identification',\n",
       "  'authors': \"['Khanh Huu The Dam', 'Thomas Given-Wilson', 'Axel Legay']\",\n",
       "  'date': 'March 2021',\n",
       "  'source': \"SAC '21: Proceedings of the 36th Annual ACM Symposium on Applied Computing\",\n",
       "  'abstract': 'More accurate and advanced detection and classification of malware requires exploiting program behaviour and not merely syntactic or static features. One approach is to use system call dependency graphs (SCDGs) that represent the program behaviour by interactions with the system, and the relations between these interactions. These SCDGs have been used with supervised learning techniques to very accurately detect and classify malware. This works considers the unsupervised learning challenge of mining for common clusters of behaviour without a priori knowledge. This allows for clustering of similar programs by behaviour, that can then be used for either classification or further analysis.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3412841.3441919',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Identifying Online Profiles of Distance Learning Students Using Data Mining Techniques',\n",
       "  'authors': \"['Osama Islam', 'Muazzam Siddiqui', 'Naif Radi Aljohani']\",\n",
       "  'date': 'October 2019',\n",
       "  'source': \"ICDTE '19: Proceedings of the 3rd International Conference on Digital Technology in Education\",\n",
       "  'abstract': 'Educational data has grown over the years with the increased use of technology within educational environments. This has led to a huge amount of data being stored in various data sources representing the student, his/her activities, and other aspects relevant to the learning process. To meet this analytical need, Educational Data Mining (EDM) has emerged to assist educational institutions in identifying key benefits such as students at risk, the level of student engagement or predicting student performance. The aim of this research was to explore the various aspects of student interaction data using data mining techniques to identify relevant patterns of behaviors and possible key attributes that have higher degrees of influence on distance learning students. The main findings identified several patterns of user online profiles based on a set of adopted learning strategies, the research proposed a framework for analyzing such interaction data based on R and Hadoop platforms to correlate online profiles with student performance.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3369199.3369249',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Multi-level mining and visualization of scientific text collections: Exploring a bi-lingual scientific repository',\n",
       "  'authors': \"['Pablo Accuosto', 'Francesco Ronzano', 'Daniel Ferrés', 'Horacio Saggion']\",\n",
       "  'date': 'December 2017',\n",
       "  'source': 'WOSP 2017: Proceedings of the 6th International Workshop on Mining Scientific Publications',\n",
       "  'abstract': \"We present a system to mine and visualize collections of scientific documents by semantically browsing information extracted from single publications or aggregated throughout corpora of articles. The text mining tool performs deep analysis of document collections allowing the extraction and interpretation of research paper's contents. In addition to the extraction and enrichment of documents with metadata (titles, authors, affiliations, etc), the deep analysis performed comprises semantic interpretation, rhetorical analysis of sentences, triple-based information extraction, and text summarization. The visualization components allow geographical-based exploration of collections, topic-evolution interpretation, and collaborative network analysis among others. The paper presents a case study of a bi-lingual collection in the field of Natural Language Processing (NLP).\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3127526.3127529',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Hierarchical Topic Mining via Joint Spherical Tree and Text Embedding',\n",
       "  'authors': \"['Yu Meng', 'Yunyi Zhang', 'Jiaxin Huang', 'Yu Zhang', 'Chao Zhang', 'Jiawei Han']\",\n",
       "  'date': 'August 2020',\n",
       "  'source': \"KDD '20: Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining\",\n",
       "  'abstract': \"Mining a set of meaningful topics organized into a hierarchy is intuitively appealing since topic correlations are ubiquitous in massive text corpora. To account for potential hierarchical topic structures, hierarchical topic models generalize flat topic models by incorporating latent topic hierarchies into their generative modeling process. However, due to their purely unsupervised nature, the learned topic hierarchy often deviates from users' particular needs or interests. To guide the hierarchical topic discovery process with minimal user supervision, we propose a new task, Hierarchical Topic Mining, which takes a category tree described by category names only, and aims to mine a set of representative terms for each category from a text corpus to help a user comprehend his/her interested topics. We develop a novel joint tree and text embedding method along with a principled optimization procedure that allows simultaneous modeling of the category tree structure and the corpus generative process in the spherical space for effective category-representative term discovery. Our comprehensive experiments show that our model, named JoSH, mines a high-quality set of hierarchical topics with high efficiency and benefits weakly-supervised hierarchical text classification tasks.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3394486.3403242',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Using Text Mining to Analyze the Financial Patents',\n",
       "  'authors': \"['Yung-Feng Lu', 'Jia-Lang Xu', 'Mu-Yen Chen']\",\n",
       "  'date': 'June 2019',\n",
       "  'source': 'WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics',\n",
       "  'abstract': \"With the advancement and development of science and technology, Nowadays society is the development of information technology, cloud computing, mobile Internet, Internet of Things, block chain and information technology.The combination of finance and technology has made financial technology terms gradually emerging.. This research analyzes the patents of financial technology. However, Taiwan's financial technology patents are mostly G06Q-20 (payment plan, architecture or agreement), G06Q-30 (commercial, such as marketing, shopping, payment, auction or e-commerce). G06Q-40 (financial, investment or tax treatment, insurance).The research use text mining technology to analyze and understand Taiwan's trends and developments in financial technology. The research results show that G06Q-20 patents are biased towards information security and mobile payment. G06Q-30 patents favor shopping websites and advertising. G06Q-40 patents are biased towards diversity, such as securities, insurance, finance, and stocks.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3326467.3326478',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Multi-Media Content Clustering and Computer Intelligent Analysis by Text Mining',\n",
       "  'authors': \"['Shiqi Ren']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': 'AIAM2021: 2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture',\n",
       "  'abstract': 'The epidemic situation of covid-19 spread all over the world, which is not optimistic. In order to extract valuable information for the epidemic from the numerous Internet data. With data mining technology, this paper crawls more than 10000 pieces of data from the microblog platform of overseas anti epidemic diary topic, and preprocesses the obtained text data set with word segmentation, removing stop words and other data, extracts the keywords of each microblog through word vector model, counts word frequency, and clustes text. In addition, the emotional value of the text is analyzed. Finally, the data were grouped into seven categories, and the trend chart of emotion value was drawn, and each result was displayed in the way of graph. By analysing, on the one hand, valuable information can be extracted from the micro blog data generated by overseas Chinese to help the domestic people understand the real situation of the overseas epidemic and adjust the risk response measures; on the other hand, the general situation of social media data during the epidemic can be generally understood from the macro perspective to provide reference for government departments in terms of management of entry-exit and epidemic prevention and control. It is helpful to further improve the governance system and the modernization of governance capacity in response to public health emergencies in China.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3495018.3501088',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Machine Learning on Mining Potential Adverse Drug Reactions for Pharmacovigilance',\n",
       "  'authors': \"['Ke Jia']\",\n",
       "  'date': 'July 2021',\n",
       "  'source': 'DSIT 2021: 2021 4th International Conference on Data Science and Information Technology',\n",
       "  'abstract': 'Drugs are related to the lives and health of patients. Mining and discovering potential adverse drug reactions from a large number of unstructured texts play an important role in drug research and pharmacovigilance. It is a hot research issue in the field of biomedicine in recent years. A large number of scholars have discovered a large amount of information on adverse drug reactions from different data sources through effective machine learning methods. This paper reviews the research progress in recent years to provide a reference for future research.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3478905.3478964',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Classification of Faults in Web Applications using Machine Learning',\n",
       "  'authors': \"['Akshi Kumar', 'Rajat Chugh', 'Rishab Girdhar', 'Simran Aggarwal']\",\n",
       "  'date': 'March 2017',\n",
       "  'source': \"ISMSI '17: Proceedings of the 2017 International Conference on Intelligent Systems, Metaheuristics &amp; Swarm Intelligence\",\n",
       "  'abstract': \"Web is huge, abundant and heterogeneous and so are the challenges that arise due to this versatility. Web Applications as the new task-centric and action-oriented facilities have assumed a distinguished role in today's Web. At the same time, faults in these Web Applications are tedious and hard to test due to the labor and resource intensive nature of testing. Thus, making it essential to use automated strategies like fault based testing that can test the continuously advancing web. Fault based testing uses fault classification as a baseline to make testing cost-effective and efficient, hence making fault classification necessary. Fault classification in Web Applications is a course of action of developing models to segregate the various kinds of faults for real world fault based testing. Manual classification tends to be strenuous and therefore in this paper, we intend to predict an efficient automated model which will classify the faults. Our model uses Text Mining and Machine Learning technique to classify the faults of three open source Web Applications, namely, the qaManager, bitWeaver and WebCalender. We provide a comparative study among four models in which different Machine Learning techniques are used namely Support Vector Machines (SVM), Decision Tree, Bernoulli Naïve Bayes and Multinomial Naïve Bayes. To analyze the performance of these models, Area Under the Curve (AUC) obtained from Receiver Operating Characteristics (ROC) and 10-fold validation are used. Results show that all considered Machine Learning techniques prove to be efficient in the classification of faults. Apart from this, analysis also shows that the performance of Multinomial Naïve Bayes Classifier is better than the other three classifiers.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3059336.3059353',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Using Spark for Text Mining on Large Scale Liver Cancer Literature',\n",
       "  'authors': \"['Ming-Yen Lin', 'Yu-Ju Lin', 'Sue-Chen Hsueh']\",\n",
       "  'date': 'January 2021',\n",
       "  'source': 'BDET 2021: 2021 the 3rd International Conference on Big Data Engineering and Technology (BDET)',\n",
       "  'abstract': 'Cancer is one of the main causes of death. The number of known cancer to-date is more than one hundred. Liver cancer, ranked after trachea and lung cancer, has long been high in the cancer leading cause in Taiwan and even ranked as second in 2016. The number of scientific articles related to cancer proliferates every year. The number reaches as high as 20 million in PubMed so that discovering useful information from the massive collection is very difficult. In addition, using a single machine to sift through these articles is very time-consuming. Therefore, we present a big data analytic framework using the distributed Apache Spark platform for text mining in PubMed literature. We establish a prediction model for liver cancer articles so that researchers may effectively validate whether an article is related to liver cancer or not. Classification models in Spark MLlib including Linear Support Vector Machines (SVM), Logistic Regression, are used in our experiments. Relevancy to liver cancer is further confirmed by using MeSH (Medical Subject Headings) terms. Logistic regression is about 3 times faster than SVMs and the accuracy of both methods is close to 95% in the experiments using hold-out validation. When max_features is 500 and min_df ≤ 0.1 (or min_df = 1), the accuracy may reach 96%. In the experiments with K-fold cross-validation, the accuracy of SVMs methods is 96%. The experimental results show that that our prediction model may effectively classify liver cancer articles.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3474944.3474958',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Learning Adversarial Networks for Semi-Supervised Text Classification via Policy Gradient',\n",
       "  'authors': \"['Yan Li', 'Jieping Ye']\",\n",
       "  'date': 'July 2018',\n",
       "  'source': \"KDD '18: Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining\",\n",
       "  'abstract': 'Semi-supervised learning is a branch of machine learning techniques that aims to make fully use of both labeled and unlabeled instances to improve the prediction performance. The size of modern real world datasets is ever-growing so that acquiring label information for them is extraordinarily difficult and costly. Therefore, deep semi-supervised learning is becoming more and more popular. Most of the existing deep semi-supervised learning methods are built under the generative model based scheme, where the data distribution is approximated via input data reconstruction. However, this scheme does not naturally work on discrete data, e.g., text; in addition, learning a good data representation is sometimes directly opposed to the goal of learning a high performance prediction model. To address the issues of this type of methods, we reformulate the semi-supervised learning as a model-based reinforcement learning problem and propose an adversarial networks based framework. The proposed framework contains two networks: a predictor network for target estimation and a judge network for evaluation. The judge network iteratively generates proper reward to guide the training of predictor network, and the predictor network is trained via policy gradient. Based on the aforementioned framework, we propose a recurrent neural network based model for semi-supervised text classification. We conduct comprehensive experimental analysis on several real world benchmark text datasets, and the results from our evaluations show that our method outperforms other competing state-of-the-art methods.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3219819.3219956',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Evolutionary role mining in complex networks by ensemble clustering',\n",
       "  'authors': \"['Sarvenaz Choobdar', 'Pedro Ribeiro', 'Fernando Silva']\",\n",
       "  'date': 'April 2017',\n",
       "  'source': \"SAC '17: Proceedings of the Symposium on Applied Computing\",\n",
       "  'abstract': 'The structural patterns in the neighborhood of nodes assign unique roles to the nodes. Mining the set of existing roles in a network provides a descriptive profile of the network and draws its general picture. This paper proposes a new method to determine structural roles in a dynamic network based on the current position of nodes and their historic behavior. We develop a temporal ensemble clustering technique to dynamically find groups of nodes, holding similar tempo-structural roles. We compare two weighting functions, based on age and distribution of data, to incorporate temporal behavior of nodes in the role discovery. To evaluate the performance of the proposed method, we assess the results from two points of view: 1) goodness of fit to current structure of the network; 2) consistency with historic data. We conduct the evaluation using different ensemble clustering techniques. The results on real world networks demonstrate that our method can detect tempo-structural roles that simultaneously depict the topology of a network and reflect its dynamics with high accuracy.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3019612.3019815',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Extremely Fast Decision Tree Mining for Evolving Data Streams',\n",
       "  'authors': \"['Albert Bifet', 'Jiajin Zhang', 'Wei Fan', 'Cheng He', 'Jianfeng Zhang', 'Jianfeng Qian', 'Geoff Holmes', 'Bernhard Pfahringer']\",\n",
       "  'date': 'August 2017',\n",
       "  'source': \"KDD '17: Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining\",\n",
       "  'abstract': 'Nowadays real-time industrial applications are generating a huge amount of data continuously every day. To process these large data streams, we need fast and efficient methodologies and systems. A useful feature desired for data scientists and analysts is to have easy to visualize and understand machine learning models. Decision trees are preferred in many real-time applications for this reason, and also, because combined in an ensemble, they are one of the most powerful methods in machine learning. In this paper, we present a new system called STREAMDM-C++, that implements decision trees for data streams in C++, and that has been used extensively at Huawei. Streaming decision trees adapt to changes on streams, a huge advantage since standard decision trees are built using a snapshot of data, and can not evolve over time. STREAMDM-C++ is easy to extend, and contains more powerful ensemble methods, and a more efficient and easy to use adaptive decision trees. We compare our new implementation with VFML, the current state of the art implementation in C, and show how our new system outperforms VFML in speed using less resources.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3097983.3098139',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'COVID-19 Vaccine Discussion: Evidence from Twitter Data Using Text Mining',\n",
       "  'authors': \"['Johannes Schneider', 'Gramoz Sejfijaj', 'Jan vom Brocke']\",\n",
       "  'date': 'December 2021',\n",
       "  'source': \"WI-IAT '21: IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology\",\n",
       "  'abstract': 'COVID-19 vaccination has led to unrest within societies, and intense public debates are often carried out on social media platforms like Twitter. A better understanding of concerns, issues, and communication on COVID-19 vaccines is a first step to reducing tension within society and improving the negative effects of the pandemic. It can also contribute to addressing the concerns of advocates and opponents, which is essential in the battle against this and possible future pandemics. At the same time, many people report pressure to undergo vaccination in order to continue participating in social and professional life. COVID-19 vaccination has triggered a complex discussion among the public. We use text mining algorithms suitable for big datasets to identify relevant categories of discourse and sentiments from about 250,000 tweets. Our findings highlight (and quantify) expressed shortcomings in vaccination programs related to administration, planning, information, and protective measures. It also hints that rare and severe incidents related to vaccination have a more substantial impact than potential fears related to non-familiar technology such as “mRNA” causing uncertainty. We also provide an extensive discussion setting forth suggestions that might help deal with the current and future pandemic.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3498851.3498935',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': \"Physical Role Limitation - It's Classification and Prediction using Machine Learning\",\n",
       "  'authors': \"['Sadaf Azad', 'Muna Al Fanah', 'Ci Lei']\",\n",
       "  'date': 'March 2019',\n",
       "  'source': \"ICBDE '19: Proceedings of the 2019 International Conference on Big Data and Education\",\n",
       "  'abstract': \"The focus of this study is to define the classification of physical role limitation in patients and predict patient physical role limitation based on other health factors, using machine learning. For this purpose, patients' records in National Child Development Study Dataset (NCDS), which started in 1958, were used. This study performs classification of patients' existing records and categorization of new records on the basis of trained model by implementing data mining techniques, including linear discriminant analysis (LDA), support vector machine (SVM), Decision Tree (CART), Random Forest, and k-nearest neighbor (KNN). Findings of this research will help medical practitioners to classify patients on the basis of other valuable health related information and observe the physical role limitations in patients at age of 50 and above. For instance, knowing the general health scales of patients can help practitioners to predict the physical activity scale in patients, and thus help practitioners in e.g. risk assessment and drawing up treatment and care plans. The contribution of this study is to utilize a study covering 55 years' observations of the candidates in order to help practitioners to identify and predict the physical role limitation in people when they turn 50 and over on the basis of other health related information. Of the five classifiers applied in this study, it can be confidently said that Random Forest is the best classifier with 72.48% accuracy to be used in data situations like NCDS.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3322134.3322144',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Interactive Visual Graph Mining and Learning',\n",
       "  'authors': \"['Ryan A. Rossi', 'Nesreen K. Ahmed', 'Rong Zhou', 'Hoda Eldardiry']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Intelligent Systems and Technology',\n",
       "  'abstract': 'This article presents a platform for interactive graph mining and relational machine learning called GraphVis. The platform combines interactive visual representations with state-of-the-art graph mining and relational machine learning techniques to aid in revealing important insights quickly as well as learning an appropriate and highly predictive model for a particular task (e.g., classification, link prediction, discovering the roles of nodes, and finding influential nodes). Visual representations and interaction techniques and tools are developed for simple, fast, and intuitive real-time interactive exploration, mining, and modeling of graph data. In particular, we propose techniques for interactive relational learning (e.g., node/link classification), interactive link prediction and weighting, role discovery and community detection, higher-order network analysis (via graphlets, network motifs), among others. GraphVis also allows for the refinement and tuning of graph mining and relational learning methods for specific application domains and constraints via an end-to-end interactive visual analytic pipeline that learns, infers, and provides rapid interactive visualization with immediate feedback at each change/prediction in real-time. Other key aspects include interactive filtering, querying, ranking, manipulating, exporting, as well as tools for dynamic network analysis and visualization, interactive graph generators (including new block model approaches), and a variety of multi-level network analysis techniques.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3200764',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Deep Active Learning for Text Classification',\n",
       "  'authors': \"['Bang An', 'Wenjun Wu', 'Huimin Han']\",\n",
       "  'date': 'August 2018',\n",
       "  'source': 'ICVISP 2018: Proceedings of the 2nd International Conference on Vision, Image and Signal Processing',\n",
       "  'abstract': \"In recent years, Active Learning (AL) has been applied in the domain of text classification successfully. However, traditional methods need researchers to pay attention to feature extraction of datasets and different features will influence the final accuracy seriously. In this paper, we propose a new method that uses Recurrent Neutral Network (RNN) as the acquisition function in Active Learning called Deep Active Learning (DAL). For DAL, there is no need to consider how to extract features because RNN can use its internal state to process sequences of inputs. We have proved that DAL can achieve the accuracy that cannot be reached by traditional Active Learning methods when dealing with text classification. What's more, DAL can decrease the need of the great number of labeled instances for Deep Learning (DL). At the same time, we design a strategy to distribute label work to different workers. We have proved by using a proper batch size of instance, we can save much time but not decrease the model's accuracy. Based on this, we provide batch of instances for different workers and the size of batch is determined by worker's ability and scale of dataset, meanwhile, it can be updated with the performance of the workers.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3271553.3271578',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Multi-task Fuzzy Clustering–Based Multi-task TSK Fuzzy System for Text Sentiment Classification',\n",
       "  'authors': \"['Xiaoqing Gu', 'Kaijian Xia', 'Yizhang Jiang', 'Alireza Jolfaei']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Asian and Low-Resource Language Information Processing',\n",
       "  'abstract': 'Text sentiment classification is an important technology for natural language processing. A fuzzy system is a strong tool for processing imprecise or ambiguous data, and it can be used for text sentiment analysis. This article proposes a new formulation of a multi-task Takagi-Sugeno-Kang fuzzy system (TSK FS) modeling, which can be used for text sentiment image classification. Using a novel multi-task fuzzy c-means clustering algorithm, the common (public) information among all tasks and the individual (private) information for each task are extracted. The information about clustering, for example, cluster centers, can be used to learn the antecedent parameters of multi-task TSK fuzzy systems. With the common and individual antecedent parameters obtained, a corresponding multi-task learning mechanism for learning consequent parameters is devised. Accordingly, a multi-task fuzzy clustering–based multi-task TSK fuzzy system (MTFCM-MT-TSK-FS) is proposed. When the proposed model is built, the information conveyed by the fuzzy rules formed is two-fold, including (1) common fuzzy rules representing the inter-task correlation information and (2) individual fuzzy rules depicting the independent information of each task. The experimental results on several text sentiment datasets demonstrate the validity of the proposed model.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3476103',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Text Mining on Player Personality for Game Recommendation',\n",
       "  'authors': \"['Hsin-Chang Yang', 'Cathy S. Lin', 'Zi-Rui Huang', 'Tsung-Hsing Tsai']\",\n",
       "  'date': 'July 2017',\n",
       "  'source': \"MISNC '17: Proceedings of the 4th Multidisciplinary International Social Networks Conference\",\n",
       "  'abstract': \"Recommender systems have emergedrecently in many aspects due to enormous amount of information or items existed in e-commerce or social network services. Users find it convenient if the service can recommend interesting items to them automatically. Past systems adopted users' preference or social connections for such recommendation. In this work, we will try to identify a game player's personality traits and use them to recommend games that may be interested by the player. The personality traits of a player is identified by applying a text mining process on textual contents related to the player. The same process is also applied on the comments of the games to identified the personality traits of games. Recommendation is then performed based on the similarity between the traits of the user and the game. We performed experiments on 63 players and 2050 games and obtained promising results which may demonstrate the plausibility of personality-based recommender systems.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3092090.3092132',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Xiaomi Brand Appraisal Research Based on Zhihu by Text Mining Technology',\n",
       "  'authors': \"['Aiting Xu', 'Fangyan Wang', 'Pingting Ying']\",\n",
       "  'date': 'May 2019',\n",
       "  'source': \"ICBDC '19: Proceedings of the 4th International Conference on Big Data and Computing\",\n",
       "  'abstract': 'As the largest knowledge social platform on the Chinese Internet, Zhihu has gradually become an important resource for merchants to improve publicity and optimize products, and the public to understand the brand image. The topic of \"Xiaomi Technology\" remains hot on Zhihu. In this context, this paper takes the essences of the \"Xiaomi Technology\" topic on Zhihu as the research object. First we carry on the data collection and preprocessing. Then by extracting feature based on word segmentation results, we build a corpus and construct an LDA topic model for text mining. Besides, by calculating and comparing the perplexity index, we select 20 as the number of topics. According to the results, the relationship between document-topic and topic-term is analyzed to form a topic description of the text, which shows that Xiaomi products have received great attention from consumers and are often used for comparison with other brands in the same industry; Xiaomi product launches have received much attention and had a direct impact on product sales; Xiaomi is widely recognized as one of the representatives of China\\'s future technology.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3335484.3335515',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Twitter Association Rule Mining using Clustering and Graph Databases',\n",
       "  'authors': \"['Alessandro Campi', 'Corrado Palese']\",\n",
       "  'date': 'May 2021',\n",
       "  'source': \"ICISDM '21: Proceedings of the 2021 5th International Conference on Information System and Data Mining\",\n",
       "  'abstract': \"In this scenario, the need to efficiently analyze this kind of data is increasing because of characteristics of such big data, especially their huge and sometimes unpredictable variety. Twitter alone, with 320 M active users every month and more than 500 M tweets per day, could represent an important source of information. For this research, we are focusing solely on social networks. The reason for this choice is that they are increasingly becoming a platform where people will comfortably update their status and share or retrieve information about the world in real time. Often news is spreading through them faster than in traditional channels because user capillarity worldwide makes it possible. In particular, we will focus on Twitter, because its micro-blogging nature makes it suitable for this kind of purpose. It questions the concept of a small private community of friends in favor of less private, less personal broadcast communications of common interest. Another reason why we chose Twitter is because semantic value of hashtags, their power in summarizing tweet content and the spreading model through the social network that allows us to highlight clusters of topics by focusing on these tags.One of the objectives of this thesis is to show how data mining can provide useful techniques to deal with these huge datasets for retrieving information to detect and analyze trending topics and the corresponding user's interactions with them. We identified in Association Rules identification and evolution in time, a systematic approach to conduct the analysis.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3471287.3471309',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Innovation of Data Mining & Screening System under Big Data: Take a case as NIMBY',\n",
       "  'authors': \"['Minxuan Li']\",\n",
       "  'date': 'April 2019',\n",
       "  'source': 'ICDMML 2019: Proceedings of the 2019 International Conference on Data Mining and Machine Learning',\n",
       "  'abstract': 'With the growing maturity of web crawler technology and the advent of the era of big data, when you want to study some problems, you can directly get all the data related to them through web crawlers and other means, but it is more important to mining and filter the data to get valuable data for the research content. This study which based on the word list of keywords uses CRN network to construct semantic distance table and TOPSIS evaluation system to sort data to make sure researchers can obtain quantitative screening data with research value and to provide researchers with scientific screening methods.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3335656.3335688',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Research on English Text Classification and Recognition Method Based on Machine Learning Technology',\n",
       "  'authors': \"['Yinhua Chen']\",\n",
       "  'date': 'December 2021',\n",
       "  'source': 'ICASIT 2021: 2021 International Conference on Aviation Safety and Information Technology',\n",
       "  'abstract': \"In recent years, with the rapid development of information network technology, especially the substantial increase in network usage, more and more information is expressed in the form of text. In short, the emergence of a large amount of text information on the Internet is undoubtedly an amazing trend. The rapid generation of massive data and information has greatly affected the efficient, rapid and complete utilization of these resources by human beings. The reason is that people can no longer rely solely on physical labor to effectively obtain the basic content of massive amounts of information. If a computer can help people identify and process information content, then it must provide good help and support in people's work and study in order to alleviate people's embarrassment that they cannot make full use of information. Therefore, we have reason to believe in computer-based texts. Sorting can increase the use of text information. The purpose of this paper is to study the English text classification and recognition method of machine learning technology, take this as the research direction and propose a text classification method based on context. The algorithm has two core parts, namely classification training set and environment learning classification. According to the number of occurrences of keywords, the frequency of occurrence is calculated and classified, and then an index is given to each category. Calculate the weights of the attribute words in all documents in each category, and give the attribute word scores through repetition. Firstly, the attribute words are mined through the association word rules, and then the attribute words form the environment attribute word matrix. The algorithm combines two analysis methods, namely traditional statistical analysis and context analysis, which can simultaneously learn all categories in the document. Experiments have shown that both the weight calculation algorithm and the simulation algorithm can efficiently classify texts and have strong practicability. Experimental research shows that the context learning algorithm proposed in this article has achieved obvious advantages in comparison with traditional algorithms. The recall rate is 47%, the accuracy rate is 89%, the error rate is as low as 11%, and all indicators are higher than Other algorithms.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3510858.3510935',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Data Mining for Discovering Cognitive Models of Learning',\n",
       "  'authors': \"['Jinjin Zhao', 'Candace Thille', 'Dawn Zimmaro']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"ICAAI '21: Proceedings of the 5th International Conference on Advances in Artificial Intelligence\",\n",
       "  'abstract': 'A cognitive model is a descriptive account or computational representation of human thinking about a given concept, skill, or domain. A cognitive model of learning, includes both a way of organizing knowledge within a subject area and an account of how humans develop accurate and complete knowledge of that subject area. Learning designers engage in a variety of practices to unpack knowledge from subject matter experts and novices to develop cognitive models of learning and use those models to guide the design of instruction or instructional technologies. Traditional approaches to eliciting and organizing knowledge, such as conducting a cognitive task analysis (CTA) [14] with experts and novices, are labor-intensive and require specific expertise that many learning designers do not have. However, learning data generated from learners’ interaction with courses, can provide insight into how humans think and develop knowledge. As a continued effort, we extend the framework presented in our earlier work [17] to discover and refine cognitive models of learning with learning data. The framework includes 1. a Variational Autoencoder (VAE) and a Gaussian Mixture Model (GMM) that models and clusters cognitive learning patterns; 2. a multidimensional measure that quantifies validity and reliability of the discovered cognitive models of learning; 3. a topic-based solution that interprets the cognitive models from a linguistic perspective; and 4. a simulation-based analysis for both accuracy measures and course refinement insights. We demonstrate the end-to-end solution with two applications and four case studies that are deployed in an openly navigated learning system in a workforce learning environment. We also report the usefulness of the discovered cognitive models of learning with subject matter expert evaluation.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3505711.3505729',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Hyperspectral Image Classification using Machine Learning',\n",
       "  'authors': \"['Ranjana W. Gore', 'Abhilasha D. Mishra', 'Ratnadeep R. Deshmukh']\",\n",
       "  'date': 'August 2021',\n",
       "  'source': \"DSMLAI '21': Proceedings of the International Conference on Data Science, Machine Learning and Artificial Intelligence\",\n",
       "  'abstract': 'Hyperspectral images are of high resolution so helpful in classifying the land surface for variety of applications like land use and land cover, lithological discrimination, differentiating different land surfaces, segmentation, etc. This work focused on the hyperspectral image classification at Lonar Crater situated at Buldhana district, Maharashtra. This methodology consists of sequence of operations viz. bad band removal, destriping, atmospheric correction, minimum noise fraction, n-D visualization and classification. 242 images of Hyperion sensor are used for classification. The land surface at and near Lonar crater is classified with more user accuracy on an average. The Spectral angle mapper technique is used showing the 59.23% accuracy in classifying with respect to ground truths from image. The results are improved with Support Vector Machine.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3484824.3484883',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Report on the 1st ACM SIGIR/SIGKDD Africa School on Machine Learning for Data Mining and Search',\n",
       "  'authors': \"['Ben Carterette', 'Hussein Suleman', 'Douglas W. Oard']\",\n",
       "  'date': 'June 2019',\n",
       "  'source': 'ACM SIGIR Forum',\n",
       "  'abstract': 'We report on the inception, organization, and activities of the 1st ACM SIGIR/SIGKDD Africa School on Machine Learning for Data Mining and Search, which took place at the University of Cape Town in South Africa January 14--18, 2019.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3458537.3458538',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Application of Data Mining Classification in Job-Changing',\n",
       "  'authors': \"['Xu Qian', 'Hayato Ohwada']\",\n",
       "  'date': 'February 2018',\n",
       "  'source': \"ICMLC '18: Proceedings of the 2018 10th International Conference on Machine Learning and Computing\",\n",
       "  'abstract': \"More and more people are using job-changing websites to change jobs. Every enterprise responsible for recruitment wants to attract excellent applicants from among the hundreds of people using the job-changing website. We propose the use of the clustering analysis on the data information that is accumulated from the job-changing website. Once the current job-changing applicants are classified according to the clustering rules, each enterprise could easily see the job-changing applicant's information. As an intermediary for applicants and enterprises, if the job-changing website uses a good classifier to help the enterprise find the right applicants, this website will attract more users.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3195106.3195136',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Identifying social networks of programmers using text mining for code similarity detection',\n",
       "  'authors': \"['Konstantinos F. Xylogiannopoulos', 'Panagiotis Karampelas']\",\n",
       "  'date': 'December 2020',\n",
       "  'source': \"ASONAM '20: Proceedings of the 12th IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining\",\n",
       "  'abstract': \"The availability of code in many online repositories and collaborating platforms has posed new challenges in source code attribution not only for plagiarism detection but also in other settings such as in the use of insecure copied code in commercial application, etc. The sophistication of different type of attacks in the code sequence used especially by the students requires more effective code similarity detection algorithms. In this paper, a novel source code detection method is proposed that can identify programmers' social network based on advanced pattern detection text mining techniques. The proposed methodology has significant advantages against existing methods since ARPaD algorithm can detect all common patterns between all possible code sequences in one run. Therefore, the computational time is massively reduced to O(mn log n). In order to assess the performance of the methodology, a new dataset was created by assigning to 46 students a code project with specific instructions. The assessment results have been visualized, producing the social network graphs of possible collaboration teams.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ASONAM49781.2020.9381381',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'TACAM: Topic And Context Aware Argument Mining',\n",
       "  'authors': \"['Michael Fromm', 'Evgeniy Faerman', 'Thomas Seidl']\",\n",
       "  'date': 'October 2019',\n",
       "  'source': \"WI '19: IEEE/WIC/ACM International Conference on Web Intelligence\",\n",
       "  'abstract': 'In this work we address the problem of argument search. The purpose of argument search is the distillation of pro and contra arguments for requested topics from large text corpora. In previous works, the usual approach is to use a standard search engine to extract text parts which are relevant to the given topic and subsequently use an argument recognition algorithm to select arguments from them. The main challenge in the argument recognition task, which is also known as argument mining, is that often sentences containing arguments are structurally similar to purely informative sentences without any stance about the topic. In fact, they only differ semantically. Most approaches use topic or search term information only for the first search step and therefore assume that arguments can be classified independently of a topic. We argue that topic information is crucial for argument mining, since the topic defines the semantic context of an argument. Precisely, we propose different models for the classification of arguments, which take information about a topic of an argument into account. Moreover, to enrich the context of a topic and to let models understand the context of the potential argument better, we integrate information from different external sources such as Knowledge Graphs or pre-trained NLP models. Our evaluation shows that considering topic information, especially in connection with external information, provides a significant performance boost for the argument mining task.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3350546.3352506',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Semi-Supervised Text Classification via Self-Pretraining',\n",
       "  'authors': \"['Payam Karisani', 'Negin Karisani']\",\n",
       "  'date': 'March 2021',\n",
       "  'source': \"WSDM '21: Proceedings of the 14th ACM International Conference on Web Search and Data Mining\",\n",
       "  'abstract': \"We present a neural semi-supervised learning model termed Self-Pretraining. Our model is inspired by the classic self-training algorithm. However, as opposed to self-training, Self-Pretraining is threshold-free, it can potentially update its belief about previously labeled documents, and can cope with the semantic drift problem. Self-Pretraining is iterative and consists of two classifiers. In each iteration, one classifier draws a random set of unlabeled documents and labels them. This set is used to initialize the second classifier, to be further trained by the set of labeled documents. The algorithm proceeds to the next iteration and the classifiers' roles are reversed. To improve the flow of information across the iterations and also to cope with the semantic drift problem, Self-Pretraining employs an iterative distillation process, transfers hypotheses across the iterations, utilizes a two-stage training model, uses an efficient learning rate schedule, and employs a pseudo-label transformation heuristic. We have evaluated our model in three publicly available social media datasets. Our experiments show that Self-Pretraining outperforms the existing state-of-the-art semi-supervised classifiers across multiple settings. Our code is available at https://github.com/p-karisani/self-pretraining .\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3437963.3441814',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': \"Based on Hadoop's tech big data combination and mining technology framework\",\n",
       "  'authors': \"['Xu Zhichao', 'Zhao Jiandong', 'Huang Huan']\",\n",
       "  'date': 'March 2018',\n",
       "  'source': \"ICIAI '18: Proceedings of the 2nd International Conference on Innovation in Artificial Intelligence\",\n",
       "  'abstract': 'With the advent of the Internet + era in the field of Tech big data, the big data of Tech big data has a large amount of data and various characteristics. It is an important means to carry out research on the big data of Tech big data to realize the combination and mining of efficient multi-source foreign technology data. However, at present, the big data of Tech big data are divided into disciplines and different formats, which are difficult to realize the intersection of effective scientific and technological information and realize data sharing. This paper puts forward a kind of big data combined with Tech big data and mining technology based on the Hadoop framework.It includes a unified collection and preprocessing method of big data of Tech big data and the design of storage and management platform for data sources. It is based on Map/Reduce Tech big data parallelization computing model and system.Its correlation with important scientific data mining services.The framework has good practicability and expansibility.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3194206.3194229',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Failure Cause Extraction of Railway Switches Based on Text Mining',\n",
       "  'authors': \"['Chunni Lin', 'Guang Wang']\",\n",
       "  'date': 'December 2017',\n",
       "  'source': \"CSAI '17: Proceedings of the 2017 International Conference on Computer Science and Artificial Intelligence\",\n",
       "  'abstract': \"Text mining is useful for knowledge extraction. As an effective textual data analysis technology, it has been widely promoted in many fields. In railway domain, millions of repair verbatims are recorded in the form of text. These ample but unexploited textual data can provide abundant knowledge for us to guarantee the normal operation of railway transportation and relieve maintainers' labor intensity. However, railway switches with different number of operation times, types, in different areas and climates will have various failure cause. Hence, in order to diagnose and predict failures of railway switches quickly and precisely, it is necessary to know all failure cause of the batch of equipment in advance. In this paper, we use text mining technology to extract fault cause using a real-world dataset. To our best knowledge, it is the first research which focuses on failure cause extraction of railway switches based on text mining, and CHI method is used for selecingt features. Experiment results turn that out our method can obtains a satisfactory performance and the proposed method is more persuasive than traditional experience-based methods.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3168390.3168402',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Data mining for electroencephalogram signal processing and analysis',\n",
       "  'authors': \"['Rossana Mancuso', 'Marzia Settino', 'Mario Cannataro']\",\n",
       "  'date': 'August 2021',\n",
       "  'source': \"BCB '21: Proceedings of the 12th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics\",\n",
       "  'abstract': 'Electroencephalography (EEG) is a complex signal that requires advanced signal processing and feature extraction methodologies to be interpreted correctly. EEG, is usually utilized to estimate the trace and the electrical brain activity. It is employed in the discovery and forecast of epileptic and non-epileptic seizures and neurodegenerative pathologies. In this article, we give an overview of the various computational techniques used in the past, in the present and the future to preprocess and analyze EEG signals. In particular, this work aims to briefly review the state of research in this field, trying to understand the needs of EEG analysis in the medical field, with special focus on neurodegenerative pathologies, and epileptic and not-epileptic diseases. After presenting the main pre-processing, feature selection and extraction phases, we focus on classification processes and on Data Mining techniques applied to classify EEGs. Then, through the EEG analysis a discussion of the implementation is provided to investigate, predict and diagnose some cognitive diseases and epilepsy.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3459930.3470905',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Visualization and Analysis of Research Field in University Laboratories by Text Mining',\n",
       "  'authors': \"['Tatsukaze Naganawa', 'Hirotaka Itoh', 'Kenji Funahashi']\",\n",
       "  'date': 'July 2019',\n",
       "  'source': \"ICEMT '19: Proceedings of the 3rd International Conference on Education and Multimedia Technology\",\n",
       "  'abstract': \"The research field at a university is one of the factors to greatly influence student's future life. Therefore, when high school students choose a university, it is important to find out what research fields have been performed at the laboratories of the university there. However, research fields are diversified in recent years, and it takes a lot of time to examine them, and is hard work for students. In this research, a text mining method is applied for the title data set of theses of each university laboratory, and each research field is visualized as tables, figures and graphs, so that the students can easily understand them. We also analyze the annual trends of research fields.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3345120.3345141',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Impacts of Big Data on Data Mining Research: An Empirical Study of Chinese Journals',\n",
       "  'authors': \"['Yue Huang']\",\n",
       "  'date': 'July 2018',\n",
       "  'source': \"ICCSE'18: Proceedings of the 3rd International Conference on Crowd Science and Engineering\",\n",
       "  'abstract': \"With the advent of big data, data mining theories and methods face new challenges. This paper tries to find the impacts of big data on data mining research through 23377 data mining-related papers published in Chinese academic journals during 1996--2016. By utilization of various methods of bibliometrics, this study conducts three different levels of analysis to gradually dig deeper into the contents of literature. For the macro-level, paper amount analysis results show that big data-related research began in 2012 and has brought new growth to data mining area. For the meso-level, journal distribution analysis results indicate that many other disciplines, such as arts and agriculture science, began to apply data mining techniques with the wide spread of big data. For the micro-level, co-word-based research topic clustering results imply that new topics emerged due to the easy access of big data, such as 'clouding computing' and 'teaching and learning analysis'.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3265689.3265706',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A Frequency-Category Based Feature Selection in Big Data for Text Classification',\n",
       "  'authors': \"['Houda Amazal', 'Mohammed Ramdani', 'Mohamed Kissi']\",\n",
       "  'date': 'September 2020',\n",
       "  'source': \"SITA'20: Proceedings of the 13th International Conference on Intelligent Systems: Theories and Applications\",\n",
       "  'abstract': 'In big data era, text classification is considered as one of the most important machine learning application domain. However, to build an efficient algorithm for classification, feature selection is a fundamental step to reduce dimensionality, achieve better accuracy and improve time execution. In the literature, most of the feature ranking techniques are document based. The major weakness of this approach is that it favours the terms occurring frequently in the documents and neglects the correlation between the terms and the categories. In this work, unlike the traditional approaches which deal with documents individually, we use mapreduce paradigm to process the documents of each category as a single document. Then, we introduce a parallel frequency-category feature selection method independently of any classifier to select the most relevant features. Experimental results on the 20-Newsgroups dataset showed that our approach improves the classification accuracy to 90.3%. Moreover, the system maintains the simplicity and lower execution time.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3419604.3419620',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Data mining ancient scripts to investigate their relationships and origins',\n",
       "  'authors': \"['Shruti Daggumati', 'Peter Z. Revesz']\",\n",
       "  'date': 'June 2019',\n",
       "  'source': \"IDEAS '19: Proceedings of the 23rd International Database Applications &amp; Engineering Symposium\",\n",
       "  'abstract': 'This paper describes a data mining study of a set of ancient scripts in order to discover their relationships, including their possible common origin from a single root script. The data mining uses convolutional neural networks and support vector machines to find the degree of visual similarity between pairs of symbols in eight different ancient scripts. Among the surprising results of the data mining are the following: (1) the Indus Valley Script is visually closest to Sumerian pictographs, and (2) the Linear B script is visually closest to the Cretan Hieroglyphic script.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3331076.3331116',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'The Application of Data Mining Algorithms in the Construction of Travel Recommendation System',\n",
       "  'authors': \"['Chao Lou', 'Zhaonan Mu', 'Mengzhu Liu']\",\n",
       "  'date': 'January 2021',\n",
       "  'source': 'CONF-CDS 2021: The 2nd International Conference on Computing and Data Science',\n",
       "  'abstract': \"Photos shared by users of online social network often associated with tags, texts, geographic and time information. These data are ideal research resource which can be used by researchers to observe people's interests and behaviors. They can also be used to analyze people's travel behaviors and interests because it can reflect people's travel activities and experiences directly. This project aimed to consolidate people into groups based on their travel preferences of travel season, travel time span, scenery type of travel location and then analyze popular travel places for different people groups. The travel preferences were analyzed from extracted data applying PHP language and Naive Bayes classification approach. According to the analyzed data, people were divided into groups. Then, DBSCAN clustering was used to get popular locations based on the different groups. As a result, 80 thousand photo data of 5 cities have been collected from Flickr by Phython. It has been showed that people's preference of season in most of the mined cities was spring and summer and the preference of time span was short. Moreover, mined people has been divided into 16 groups according to their preference and based on the groups popular locations have been calculated by DBSCAN clustering. Theses locations have been provided in a recommendation website which was built in this project. For the future work, mining Flickr data associated with other travel social network sites could be a good way to build a more effective recommendation system.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3448734.3450477',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Population-based metaheuristics for Association Rule Text Mining',\n",
       "  'authors': \"['Iztok Fister', 'Suash Deb', 'Iztok Fister']\",\n",
       "  'date': 'March 2020',\n",
       "  'source': \"ISMSI '20: Proceedings of the 2020 4th International Conference on Intelligent Systems, Metaheuristics &amp; Swarm Intelligence\",\n",
       "  'abstract': \"Nowadays, the majority of data on the Internet is held in an unstructured format, like websites and e-mails. The importance of analyzing these data has been growing day by day. Similar to data mining on structured data, text mining methods for handling unstructured data have also received increasing attention from the research community. The paper deals with the problem of Association Rule Text Mining. To solve the problem, the PSO-ARTM method was proposed, that consists of three steps: Text preprocessing, Association Rule Text Mining using population-based metaheuristics, and text postprocessing. The method was applied to a transaction database obtained from professional triathlon athletes' blogs and news posted on their websites. The obtained results reveal that the proposed method is suitable for Association Rule Text Mining and, therefore, offers a promising way for further development.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3396474.3396493',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Using a Text Mining Assignment as an Intervention to Promote Student Engagement With DEI Issues',\n",
       "  'authors': \"['Scott T. Leutenegger', 'Christina H. Paguyo']\",\n",
       "  'date': 'March 2021',\n",
       "  'source': \"SIGCSE '21: Proceedings of the 52nd ACM Technical Symposium on Computer Science Education\",\n",
       "  'abstract': 'The goal of the Partnership For Equity project is to build inclusive computing and engineering professional mindsets, which describes attitudes and identities of students who value knowledge in both technical and diversity, equity, and inclusion (DEI) areas of computer science. In this paper we present results from an intervention we piloted in a text mining special topics class. This intervention is directly applicable to any data mining class. Students applied naive Bayes classification to a survey result dataset and classified responses as \"technical\" or \"equity\", where technical meant the survey question response was focused on technical issues, whereas \"equity\" meant the response was focused on DEI issues. The survey data came from another course where students watched Ms. Joy Buolamwini\\'s 2016 \"How I\\'m fighting bias in algorithms\" TedX talk and then answered several survey questions about the talk. In our text mining course students were first asked to watch the same TedX talk and answer several of the same survey questions. Their answers were added to the original data set. Students were then asked to apply naive Bayes classification to the combined survey results for one question. At the end of the course students took an end of class survey and answered more open-ended questions about whether the assignment influenced their thinking about DEI in computing. Results from this intervention indicate that including a DEI focus in technical programming assignments can positively impact students\\' views on the importance of DEI and contribute to the development of computing and engineering professional mindsets.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3408877.3432557',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Real-time Online Drilling Vibration Analysis Using Data Mining',\n",
       "  'authors': \"['Marzieh Zare', 'Ari Visa', 'Sirpa Launis', 'Mikko Huova']\",\n",
       "  'date': 'July 2019',\n",
       "  'source': 'DSIT 2019: Proceedings of the 2019 2nd International Conference on Data Science and Information Technology',\n",
       "  'abstract': 'While the data mining intermediaries play a critical role in the rock drilling industry, they also tend to provide an optimized real-time model for the drilling systems. In addition, proper online tool condition monitoring (OTOM) methods can improve the drilling performance by accessing real-time data. Hence, OTOM methods assist depreciating error and detect unspecified faults at early stages. In this study, we proposed appropriate OTOM algorithms to develop and enhance the quality of real-time systems and provide a solution to detect and categorize various stages of drilling operation with the aid of vibration signals (especially in terms of acceleration or velocity). In particular, the proposed methods in this article perform based on statistical approaches. Therefore, in order to recognize the drilling stages, we measured the Root Mean Square (RMS) values corresponding to the acceleration signals. In the meantime, we also succeeded to distinguish the drilling stages by employing estimated power spectral density (PSD) in the frequency domain. The acquired results in this publication confirm the real-time prediction and classification potential of the proposed methods for the different drilling stages and especially for the rock drilling engineering.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3352411.3352439',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Text Mining of English Picture Books',\n",
       "  'authors': \"['Hiromi Ban', 'Takashi Oyabu']\",\n",
       "  'date': 'August 2018',\n",
       "  'source': 'ICVISP 2018: Proceedings of the 2nd International Conference on Vision, Image and Signal Processing',\n",
       "  'abstract': \"Picture books play an important role as a material that develops children's linguistic competence. Thus, English picture books can be considered to be indispensable in children's English study. In this paper, metrical characteristics of some English picture books were investigated, compared with English textbooks for Japanese junior high schools students. In short, frequency characteristics of character- and word-appearance were investigated. These characteristics were approximated by an exponential function. Furthermore, the percentage of Japanese junior high school required vocabulary and American basic vocabulary was calculated to obtain the difficulty-level. As a result, it was clearly shown that the English picture books have a similar tendency to literary writings in the characteristics of character-appearance, and some books are more difficult than English textbooks.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3271553.3271616',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Ensemble Block Co-clustering: A Unified Framework for Text Data',\n",
       "  'authors': \"['Séverine Affeldt', 'Lazhar Labiod', 'Mohamed Nadif']\",\n",
       "  'date': 'October 2020',\n",
       "  'source': \"CIKM '20: Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management\",\n",
       "  'abstract': 'In this paper, we propose a unified framework for Ensemble Block Co-clustering (EBCO), which aims to fuse multiple basic co-clusterings into a consensus structured affinity matrix. Each co-clustering to be fused is obtained by applying a co-clustering method on the same document-term dataset. This fusion process reinforces the individual quality of the multiple basic data co-clusterings within a single consensus matrix. Besides, the proposed framework enables a completely unsupervised co-clustering where the number of co-clusters is automatically inferred based on the non trivial generalized modularity. We first define an explicit objective function which allows the joint learning of the basic co-clusterings aggregation and the consensus block co-clustering. Then, we show that EBCO generalizes the one side ensemble clustering to an ensemble block co-clustering context. We also establish theoretical equivalence to spectral co-clustering and weighted double spherical k-means clustering for textual data. Experimental results on various real-world document-term datasets demonstrate that EBCO is an efficient competitor to some state-of-the-art ensemble and co-clustering methods.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3340531.3412058',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Differentially Private Feature Selection for Data Mining',\n",
       "  'authors': \"['Balamurugan Anandan', 'Chris Clifton']\",\n",
       "  'date': 'March 2018',\n",
       "  'source': \"IWSPA '18: Proceedings of the Fourth ACM International Workshop on Security and Privacy Analytics\",\n",
       "  'abstract': 'One approach to analysis of private data is ε-differential privacy, a randomization-based approach that protects individual data items by injecting carefully limited noise into results. A challenge in applying this to private data analysis is that the noise added to the feature parameters is directly proportional to the number of parameters learned. While careful feature selection would alleviate this problem, the process of feature selection itself can reveal private information, requiring the application of differential privacy to the feature selection process. In this paper, we analyze the sensitivity of various feature selection techniques used in data mining and show that some of them are not suitable for differentially private analysis due to high sensitivity. We give experimental results showing the value of using low sensitivity feature selection techniques. We also show that the same concepts can be used to improve differentially private decision trees.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3180445.3180452',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Distributed Classification of Text Streams: Limitations, Challenges, and Solutions',\n",
       "  'authors': \"['Artem Trofimov', 'Nikita Sokolov', 'Mikhail Shavkunov', 'Igor Kuralenok', 'Boris Novikov']\",\n",
       "  'date': 'August 2019',\n",
       "  'source': 'BIRTE 2019: Proceedings of Real-Time Business Intelligence and Analytics',\n",
       "  'abstract': 'Text stream classification is an important problem that is difficult to solve at scale. Batch processing systems, widely adopted for text classification tasks, cannot provide for low latency. Distributed stream processing systems can offer low latency, but do not support the same level of fault tolerance and determinism as the batch systems. In this work, we demonstrate how the distributed stream processing features can affect the results of a typical text classification data flow. Our analysis shows emerged trade-offs between fault tolerance and reproducibility on the one side, and performance on the other side. We outline potential ways to solve the revealed issues and to handle streaming features.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3350489.3350491',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'CrowdTC: Crowd-powered Learning for Text Classification',\n",
       "  'authors': \"['Keyu Yang', 'Yunjun Gao', 'Lei Liang', 'Song Bian', 'Lu Chen', 'Baihua Zheng']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Knowledge Discovery from Data',\n",
       "  'abstract': 'Text classification is a fundamental task in content analysis. Nowadays, deep learning has demonstrated promising performance in text classification compared with shallow models. However, almost all the existing models do not take advantage of the wisdom of human beings to help text classification. Human beings are more intelligent and capable than machine learning models in terms of understanding and capturing the implicit semantic information from text. In this article, we try to take guidance from human beings to classify text. We propose Crowd-powered learning for Text Classification (CrowdTC for short). We design and post the questions on a crowdsourcing platform to extract keywords in text. Sampling and clustering techniques are utilized to reduce the cost of crowdsourcing. Also, we present an attention-based neural network and a hybrid neural network to incorporate the extracted keywords as human guidance into deep neural networks. Extensive experiments on public datasets confirm that CrowdTC improves the text classification accuracy of neural networks by using the crowd-powered keyword guidance.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3457216',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Mining Personality Traits from Social Text Messages',\n",
       "  'authors': \"['Hsin-Chang Yang', 'Chung-Hong Lee', 'Chia-Yi Yeh']\",\n",
       "  'date': 'October 2020',\n",
       "  'source': 'MISNC2020&amp;IEMT2020: Proceedings of the 7th Multidisciplinary in International Social Networks Conference and The 3rd International Conference on Economics, Management and Technology',\n",
       "  'abstract': 'Recently, approaches of applying personality on practical applications have attracted attention from multidisciplinary areas beyond psychology. An example is the personality-based recommender systems that have been successfully applied to music, movie, and game recommendation tasks. However, finding the personality traits of a person is not a trivial task. In this work, we will propose an automatic scheme for identifying the Big Five personality traits using text mining techniques. We analyzed the social messages posted by Facebook users to find the correlations among keywords and personality traits. The personality traits of a person can then be identified by examining his social messages according to such correlations. We performed the experiments using the myPersonality dataset and obtained promising results.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3429395.3429412',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Machine Learning and Images for Malware Detection and Classification',\n",
       "  'authors': \"['Konstantinos Kosmidis', 'Christos Kalloniatis']\",\n",
       "  'date': 'September 2017',\n",
       "  'source': \"PCI '17: Proceedings of the 21st Pan-Hellenic Conference on Informatics\",\n",
       "  'abstract': 'Detecting malicious code with exact match on collected datasets is becoming a large-scale identification problem due to the existence of new malware variants. Being able to promptly and accurately identify new attacks enables security experts to respond effectively. My proposal is to develop an automated framework for identification of unknown vulnerabilities by leveraging current neural network techniques. This has a significant and immediate value for the security field, as current anti-virus software is typically able to recognize the malware type only after its infection, and preventive measures are limited. Artificial Intelligence plays a major role in automatic malware classification: numerous machine-learning methods, both supervised and unsupervised, have been researched to try classifying malware into families based on features acquired by static and dynamic analysis. The value of automated identification is clear, as feature engineering is both a time-consuming and time-sensitive task, with new malware studied while being observed in the wild.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3139367.3139400',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': \"Improvement of massive open online courses by text mining of students' emails: a case study\",\n",
       "  'authors': \"['Diego Buenaño-Fernández', 'Sergio Luján-Mora', 'W. Villegas-Ch']\",\n",
       "  'date': 'October 2017',\n",
       "  'source': 'TEEM 2017: Proceedings of the 5th International Conference on Technological Ecosystems for Enhancing Multiculturality',\n",
       "  'abstract': 'In recent years, the constant increase in the number of online courses has led to radical changes in the education sector. These new online learning environments present a series of challenges that are difficult to manage using traditional methods. The challenges relate to the level of commitment and motivation shown by students on this type of course. Several articles have been identified from the analysed literature related to the application of text or opinion mining techniques for the analysis of comments made in social networks. In the educational field, articles related to the topic that focus on the analysis of opinion have been identified based on entries included in discussion forums for online courses. Many publications are geared towards solutions in the English language, and the nature of linguistic analysis of this type of study makes it necessary to adapt them for languages other than English. In this paper, we explore the opinion mining through text mining in emails from Massive Open Online Courses (MOOC). The opinion mining expressed in emails is a complex task due to the thematic disparity of emails, their size and the depth of linguistic analysis required. The purpose of this study is to analyse students opinions about their courses, their instructors, and the main tools used on the course. The research focus on the calculation and analysis of the frequency of terms, the analysis of concordances, groupings and n-grams. The case study used in this paper is a MOOC on the topic of web development with more than 40,000 enrolled students.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3144826.3145393',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Tracking food insecurity from tweets using data mining techniques',\n",
       "  'authors': \"['Andrew Lukyamuzi', 'John Ngubiri', 'Washington Okori']\",\n",
       "  'date': 'May 2018',\n",
       "  'source': \"SEiA '18: Proceedings of the 2018 International Conference on Software Engineering in Africa\",\n",
       "  'abstract': 'Data mining algorithms can be applied to extract useful patterns from social media conversations to monitor disasters such as tsunami, earth quakes and nuclear power accidents. While food insecurity has persistently remained a world concern, its monitoring with this strategy has received limited attention. In attempt to address this concern, UN Global Pulse demonstrated that tweets reporting food prices from Indonesians can aid in predicting actual food price increase. For regions like Kenya and Uganda where use of tweets is considered low, this option can be problematic. Using Uganda as a case study, this study takes an alternative of using tweets from all over the world with mentions of; (1) uganda +food, (2) uganda + hunger, and (3) uganda + famine for years 2014, 2015 and 2016. The study however utilized tweets on food insecurity instead of tweets on food prices. In the first step, five data mining algorithms (D-tree, SVM, KNN, Neural Networks and N-Bayes) were trained to identify tweets conversations on food insecurity. Algorithmic performance were found comparable with human labeled tweet on the same subject. In step two, tweets reporting food insecurity were generated into trends. Comparing with trends from Uganda Bureau of Statistics, promising findings have been obtained with correlation coefficients of 0.56 and 0.37 for years 2015 and 2016 respectively. The study provides a strategy to generate information about food insecurity for stakeholders such as World Food Program in Uganda for mitigation action or further investigation depending on the situation. To improve performance, future work can; (1) aggregate tweets with other datasets, (2) ensemble algorithms, and (3) apply unexplored algorithms.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3195528.3195531',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A new data science framework for analysing and mining geospatial big data',\n",
       "  'authors': \"['Mo Saraee', 'Charith Silva']\",\n",
       "  'date': 'April 2018',\n",
       "  'source': \"ICGDA '18: Proceedings of the International Conference on Geoinformatics and Data Analysis\",\n",
       "  'abstract': 'Geospatial Big Data analytics are changing the way that businesses operate in many industries. Although a good number of research works have reported in the literature on geospatial data analytics and real-time data processing of large spatial data streams, only a few have addressed the full geospatial big data analytics project lifecycle and geospatial data science project lifecycle. Big data analysis differs from traditional data analysis primarily due to the volume, velocity and variety characteristics of the data being processed. One of a motivation of introducing new framework is to address these big data analysis challenges. Geospatial data science projects differ from most traditional data analysis projects because they could be complex and in need of advanced technologies in comparison to the traditional data analysis projects. For this reason, it is essential to have a process to govern the project and ensure that the project participants are competent enough to carry on the process. To this end, this paper presents, new geospatial big data mining and machine learning framework for geospatial data acquisition, data fusion, data storing, managing, processing, analysing, visualising and modelling and evaluation. Having a good process for data analysis and clear guidelines for comprehensive analysis is always a plus point for any data science project. It also helps to predict required time and resources early in the process to get a clear idea of the business problem to be solved.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3220228.3220236',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Automatic recommendation of prognosis measures for mechanical components based on massive text mining',\n",
       "  'authors': \"['Jorge Martinez-Gil', 'Bernhard Freudenthaler', 'Thomas Natschläger']\",\n",
       "  'date': 'December 2017',\n",
       "  'source': \"iiWAS '17: Proceedings of the 19th International Conference on Information Integration and Web-based Applications &amp; Services\",\n",
       "  'abstract': 'Automatically providing suggestions for predicting the likely status of a mechanical component is a key challenge in a wide variety of industrial domains. Existing solutions based on ontological models have proven to be appropriate for fault diagnosis, but they fail when suggesting activities leading to a successful prognosis of mechanical components. The major reason is that fault prognosis is an activity that, unlike fault diagnosis, involves a lot of uncertainty and it is not always possible to envision a model for predicting possible faults. In this work, we propose a solution based on massive text mining for automatically suggesting prognosis activities concerning mechanical components. The great advantage of text mining is that it is possible to automatically analyze vast amounts of unstructured information in order to find strategies that have been successfully exploited, and formally or informally documented, in the past in any part of the world.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3151759.3151774',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Classification of sentiments in short-text: an approach using mSMTP measure',\n",
       "  'authors': \"['H M Keerthi Kumar', 'B S Harish', 'S V Aruna Kumar', 'V N Manjunath Aradhya']\",\n",
       "  'date': 'February 2018',\n",
       "  'source': \"ICMLSC '18: Proceedings of the 2nd International Conference on Machine Learning and Soft Computing\",\n",
       "  'abstract': 'Sentiment analysis or opinion mining is an automated process to recognize opinion, moods, emotions, attitude of individuals or communities through natural language processing, text analysis, and computational linguistics. In recent years, many studies concentrated on numerous blogs, tweets, forums and consumer review websites to identify sentiment of the communities. The information retrieved from social networking site will be in short informal text because of limited characters in blogging site or consumer review websites. Sentiment analysis in short-text is a challenging task, due to limitation of characters, user tends to shorten his/her conversation, which leads to misspellings, slang terms and shortened forms of words. Moreover, short-texts consists of more number of presence and absence of term/feature compared to regular text. In this work, our major goal is to classify sentiments into positive, negative or neutral polarity using new similarity measure. The proposed method embeds modified Similarity Measure for Text Processing (mSMTP) with K-Nearest Neighbor (KNN) classifier. The effectiveness of the proposed method is evaluated by comparing with Euclidean Distance, Cosine Similarity, Jaccard Coefficient and Correlation Coefficient. The proposed method is also compared with other classifiers like Support Vector Machine and Random Forest using benchmark dataset. The classification results are evaluated based on Accuracy, Precision, Recall and F-measure.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3184066.3184074',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': \"Research on the Topic Mining of Learners' Interest Based on the Mongolian MOOC Platform Course Discussion Text\",\n",
       "  'authors': \"['Maowen Tan', 'Yun Song']\",\n",
       "  'date': 'May 2021',\n",
       "  'source': 'CIPAE 2021: 2021 2nd International Conference on Computers, Information Processing and Advanced Education',\n",
       "  'abstract': 'At present, one of the key directions of MOOC research is to meet the individual learning needs of learners, while the focus of personalized learning is to model learners’ interest in learning, and whether the model can accurately reflect learners’ interest and admiration plays a central role in the lesson recommendation mechanism. This research takes \"Introduction to Computer\" course of the Mongolian MOOC platform as the research object, and discovers the topics of interest of the learners by digging the content of the course discussion area. First, after crawling the content of the discussion area, the text needs to be preprocessed, including encoding conversion, text proofreading, removing stop words, and removing affixes; secondly, the discussion text is described by the vector space model, and the keywords and their weights are calculated by the TF-IDF algorithm; finally, the semantic similarity of keywords is calculated through the cosine formula, and after clustering, the topics of interest of the learners are obtained. The experimental results show that the learner\\'s reason for choosing a course is related to three themes, namely the content of the course, the teaching method and the learning experience.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3456887.3459721',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Toward a Progress Indicator for Machine Learning Model Building and Data Mining Algorithm Execution: A Position Paper',\n",
       "  'authors': \"['Gang Luo']\",\n",
       "  'date': 'December 2017',\n",
       "  'source': 'ACM SIGKDD Explorations Newsletter',\n",
       "  'abstract': 'For user-friendliness, many software systems offer progress indicators for long-duration tasks. A typical progress indicator continuously estimates the remaining task execution time as well as the portion of the task that has been finished. Building a machine learning model often takes a long time, but no existing machine learning software supplies a non-trivial progress indicator. Similarly, running a data mining algorithm often takes a long time, but no existing data mining software provides a nontrivial progress indicator. In this article, we consider the problem of offering progress indicators for machine learning model building and data mining algorithm execution. We discuss the goals and challenges intrinsic to this problem. Then we describe an initial framework for implementing such progress indicators and two advanced, potential uses of them, with the goal of inspiring future research on this topic',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3166054.3166057',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Research on the Consumer Reviews of JD Bookstore Based on Text Mining Technology',\n",
       "  'authors': \"['Aiting Xu', 'Pingting Ying', 'Fangyan Wang']\",\n",
       "  'date': 'May 2019',\n",
       "  'source': \"ICBDC '19: Proceedings of the 4th International Conference on Big Data and Computing\",\n",
       "  'abstract': \"With the rapid development of the Internet economy, the major online shopping platform has accumulated a large amount of consumer comment data containing rich information. This article takes the JD Bookstore consumer comments as the research object and uses the web crawler technology to collect consumer reviews. Further, depth mining and sentiment orientation analysis are performed on the comment data and the feature word list based on the dictionary and sentiment analysis algorithms. The results of the study show that the majority of users' comments have positive emotional tendencies, and comments with negative emotional tendencies are less than 10%. Although the majority of users maintain a more positive attitude towards JD Bookstore' logistics services, customer service, product quality, product packaging and prices, but logistics services and product quality still need to be further improved. Finally, this paper puts forward some suggestions on the distribution, purchase and pricing of JD Bookstore.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3335484.3335512',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Combining Machine Learning and Symbolic Representation of Time Series for Classification of Behavioural Patterns',\n",
       "  'authors': \"['Paula Carballo Pérez', 'Felipe Ortega', 'Jorge Navarro García', 'Isaac Martín de Diego']\",\n",
       "  'date': 'January 2019',\n",
       "  'source': \"ICSLT '19: Proceedings of the 5th International Conference on e-Society, e-Learning and e-Technologies\",\n",
       "  'abstract': 'The emergence of affordable wireless sensors has enabled the development of information systems combining sophisticated data processing and machine learning algorithms for pattern recognition. In many cases, these systems deal with time-series data, continuously gathered by sensors that compile detailed activity records. However, these datasets are frequently affected by numerous problems, including noisy data acquisition, missing data and utilization of inefficient techniques for information representation, which lead to deficient performance in machine learning applications. In this paper, we introduce a novel method to combine the efficient symbolic representation of time-series data with machine learning to improve the performance of classification systems tailored to detection of behavioural patterns of interest.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3312714.3312726',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Machine Learning for Encrypted Malware Traffic Classification: Accounting for Noisy Labels and Non-Stationarity',\n",
       "  'authors': \"['Blake Anderson', 'David McGrew']\",\n",
       "  'date': 'August 2017',\n",
       "  'source': \"KDD '17: Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining\",\n",
       "  'abstract': 'The application of machine learning for the detection of malicious network traffic has been well researched over the past several decades; it is particularly appealing when the traffic is encrypted because traditional pattern-matching approaches cannot be used. Unfortunately, the promise of machine learning has been slow to materialize in the network security domain. In this paper, we highlight two primary reasons why this is the case: inaccurate ground truth and a highly non-stationary data distribution. To demonstrate and understand the effect that these pitfalls have on popular machine learning algorithms, we design and carry out experiments that show how six common algorithms perform when confronted with real network data. With our experimental results, we identify the situations in which certain classes of algorithms underperform on the task of encrypted malware traffic classification. We offer concrete recommendations for practitioners given the real-world constraints outlined. From an algorithmic perspective, we find that the random forest ensemble method outperformed competing methods. More importantly, feature engineering was decisive; we found that iterating on the initial feature set, and including features suggested by domain experts, had a much greater impact on the performance of the classification system. For example, linear regression using the more expressive feature set easily outperformed the random forest method using a standard network traffic representation on all criteria considered. Our analysis is based on millions of TLS encrypted sessions collected over 12 months from a commercial malware sandbox and two geographically distinct, large enterprise networks.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3097983.3098163',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A Text Classification Approach using Parallel Naive Bayes in Big Data Context',\n",
       "  'authors': \"['Houda Amazal', 'Mohammed Ramdani', 'Mohamed Kissi']\",\n",
       "  'date': 'October 2018',\n",
       "  'source': \"SITA'18: Proceedings of the 12th International Conference on Intelligent Systems: Theories and Applications\",\n",
       "  'abstract': 'Text classification is a domain that has been inspiring researchers since many years. Indeed, several approaches have been developed in order to find methods that improve the performance of text classification. But in last decades, because of the technological evolution, textual data becomes more and more abundant on the web. So that classical classification methods are unable to process this huge amount of data and consequently cannot produce satisfied results. Thus, new ways have been explored; to overcome the big dimensions of data, it was necessary to reduce the size of the features of documents and use parallel processing. For this, in our work, we developed a Term Frequency- Inverse Document Frequency (TF-IDF) parallel model to save only the most relevant words in documents. Then, we feed the dataset to a parallel Naive Bayes classifier. Both, the TF-IDF parallel model and parallel Naïve Bayes classifier were implemented on Hadoop system using the MapReduce architecture. The experimental results demonstrate the efficiency of the proposed method to improve the classification accuracy.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3289402.3289536',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Modelling Student Performance Using Data Mining Techniques: Inputs for Academic Program Development',\n",
       "  'authors': \"['Mayreen V. Amazona', 'Alexander A. Hernandez']\",\n",
       "  'date': 'May 2019',\n",
       "  'source': \"ICCDE '19: Proceedings of the 2019 5th International Conference on Computing and Data Engineering\",\n",
       "  'abstract': \"This paper presents the outcomes of linking an educational data mining approach to model students' academic performance. Three data mining classification models (Naïve Bayes, Decision Tree and Deep Learning in Neural Network) were defined to analyze data set and to predict students' performance. The forecast presentation of the three classifiers were calculated and matched. Students' academic histories and data from the Registrar's Office were used to train the models. Results show that Deep Learning classifier beats other two classifiers by gaining the overall forecast accuracy of 95%. These results highlight the impact on student success while pointing to several promising directions for future work. These analysis and information about prediction is more helpful for college administration and faculty members to improve education and also make changes if necessary.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3330530.3330544',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Data Mining of Project Management Data: An Analysis of Applied Research Studies',\n",
       "  'authors': \"['Gurdal Ertek', 'Murat Mustafa Tunc', 'Allan Nengsheng Zhang', 'Omer Tanrikulu', 'Sobhan Asian']\",\n",
       "  'date': 'December 2017',\n",
       "  'source': \"ICIT '17: Proceedings of the 2017 International Conference on Information Technology\",\n",
       "  'abstract': 'Data collected and generated through and posterior to projects, such as data residing in project management software and post-project review documents, can be a major source of actionable insights and competitive advantage. This paper presents a rigorous methodological analysis of the applied research published in academic literature, on the application of data mining (DM) for project management (PM). The objective of the paper is to provide a comprehensive analysis and discussion of where and how data mining is applied for project management data and to provide practical insights for future research in the field.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3176653.3176714',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Exploration of a Balanced Reference Corpus with a Wide Variety of Text Mining Tools',\n",
       "  'authors': \"['Nicolas Turenne', 'Bokai Xu', 'Xinyue Li', 'Xindi Xu', 'Hongyu Liu', 'Xiaolin Zhu']\",\n",
       "  'date': 'December 2020',\n",
       "  'source': \"ACAI '20: Proceedings of the 2020 3rd International Conference on Algorithms, Computing and Artificial Intelligence\",\n",
       "  'abstract': 'To compare various techniques, the same platform is generally used into which the user will import a text dataset. Another approach uses an evaluation based on a gold standard for a specific task, but a balanced common language corpus is not often used. We choose the Corpus of Contemporary American English Corpus (COCA) as a balanced reference corpus, and split this corpus into categories, such as topics and genres, to apply families of feature extraction and machine learning algorithms. We found that the Stanford CoreNLP method was faster and more accurate than the NLTK method, and was more reliable and easier to understand. The results of clustering show that a higher modularity influences interpretation. For genre and topic classification, all techniques achieved a relatively high score, though these were below the state-of-the-art scores from challenge text datasets. Naïve Bayes outperformed the other alternatives. We hope that balanced corpora from a variety of different vernacular (or low-resource) languages can be used as references to determine the efficiency of the wide diversity of state-of-the-art text mining tools.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3446132.3446192',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A Cognitive Pragmatic Analysis of Conceptual Metaphor in Political Discourse Based on Text Data Mining',\n",
       "  'authors': \"['Renqing Hu', 'Xue Wang']\",\n",
       "  'date': 'September 2021',\n",
       "  'source': 'ICISCAE 2021: 2021 4th International Conference on Information Systems and Computer Aided Education',\n",
       "  'abstract': \"As a cognitive mechanism and thinking mode of human beings, conceptual metaphor is not only a way to make the world order, but also one of the main basic ways of human existence. Scholars at home and abroad have done a lot of research on conceptual metaphor and ideology in political discourse. But the research results, especially the domestic research results are not very rich. We should expand the scope of the number and types of corpus, and adopt a variety of methods to continue to deepen. From the perspective of cognitive pragmatics, based on Lakoff's conceptual metaphor theory and framework theory, this paper explores the framework of conceptual metaphor in political speeches from the official website of the Ministry of foreign affairs. This paper explores the powerful appeal of metaphor in political discourse and its influence on the audience's cognition and acceptance. This paper summarizes the similarities and differences of conceptual metaphors in Chinese and American political discourses, aiming to deepen the understanding of metaphors in political discourses, so that learners can better understand the characteristics of political language.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3482632.3482681',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Deep Belief Network based Machine Learning for Daily Activities Classification',\n",
       "  'authors': \"['Tejtasin Phiasai', 'Nutchanun Chinpanthana']\",\n",
       "  'date': 'July 2021',\n",
       "  'source': 'AIVR 2021: 2021 5th International Conference on Artificial Intelligence and Virtual Reality (AIVR)',\n",
       "  'abstract': 'Human activity recognition has been a very active topic in pervasive computing for several years for its important applications in assisted living, healthcare, and security surveillance. Many researchers are finding and representing the details of human body gestures to determine human activity. While simple activities can be easily recognized only by acceleration data, our research has focused on the recognition and understanding the various activities in daily living. In this work, we address this problem by proposing approach theory of deep learning with the Deep belief network. Deep belief network comprises a series of Restricted Boltzmann Machines will be formed by superimposed multiple Restricted Boltzmann Machines and training the model parameters for data reconstruction, feature construction and classification. We tested our approach on PASCAL VOC datasets. The experimental results indicate that our proposed approach offers significant performance improvements with the maximum of 79.8%.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3480433.3480444',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Chinese Text Feature Extraction and Classification Based on Deep Learning',\n",
       "  'authors': \"['Ruishuang Wang', 'Zhao Li', 'Jian Cao', 'Tong Chen']\",\n",
       "  'date': 'October 2019',\n",
       "  'source': \"CSAE '19: Proceedings of the 3rd International Conference on Computer Science and Application Engineering\",\n",
       "  'abstract': 'With the rapid development of deep learning, neural networks have been widely used in natural language processing tasks and achieved good results. Since convolutional neural networks can acquire high-level features that can better represent textual semantic information, convolutional neural networks (CNN) and convolutional recurrent neural networks (CRNN) are used to establish feature extraction models to extract text features. At the same time, tf-idf and word2vec methods are used to represent text features, and then feed them into SVM and Random forest classifier to classify Chinese academic papers dataset. Experimental results show that the classification results obtained by using the CNN and CRNN feature extraction model are better than using the TF-IDF and Word2vec feature extraction methods. In addition, the classification results obtained by using SVM and Random forest classifier are better than that of the original neural network.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3331453.3361636',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A comparative survey of machine learning classification algorithms for breast cancer detection',\n",
       "  'authors': \"['Markos Marios Kaklamanis', 'Michael E. Filippakis']\",\n",
       "  'date': 'November 2019',\n",
       "  'source': \"PCI '19: Proceedings of the 23rd Pan-Hellenic Conference on Informatics\",\n",
       "  'abstract': 'In this paper four machine learning algorithms are compared in order to predict if a cell nucleus is benign or malignant using the Breast Cancer Wisconsin (Diagnostic) Data Set. The algorithms are K-Nearest Neighbours, Classification and Regression Trees (CART), Naïve Bayes and Support Vector Machines with Radial Basis Function Kernel. Data visualization and Pre-Processing using PCA will help in the understanding and the preparation of the dataset for the training phase while parameter tuning will determine the optimal parameter for every model using R as programming language. Also, 10-fold Cross Validation is used as a resampling method after comparing it with Bootstrapping, as it is the most efficient out of the two. In the end, our comparison shows that the machine learning model that marked the highest Accuracy is the one that is trained using K Nearest Neighbours. Nowadays, one of the most common forms of cancer among women is breast cancer with more than one million cases and nearly 600,000 deaths occurring worldwide annually [1]. It is the second leading cause of death among women and thus it must be detected at an early stage in order not to become fatal [2]. Thus, the importance of diagnosing if a biopsied cell is benign or malignant is vital. However, this process is quite complicated as it involves several stages of gathering and analysing samples with many variables, making the final diagnosis a demanding and timely procedure. The rapid growth of Artificial Intelligence and Machine learning and their implementation in Medicine give us a new perspective in the way we process and analyse medical data. Medical experts can use Data Mining techniques and improve their decision making by extracting useful information from massive amounts of data.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3368640.3368642',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Use Case Scenarios on Legal Text Mining',\n",
       "  'authors': \"['Yannis Charalabidis', 'Michalis Avgerinos Loutsaris', 'Shefali Virkar', 'Charalampos Alexopoulos', 'Anna-Sophie Novak', 'Zoi Lachana']\",\n",
       "  'date': 'April 2019',\n",
       "  'source': \"ICEGOV '19: Proceedings of the 12th International Conference on Theory and Practice of Electronic Governance\",\n",
       "  'abstract': \"Europe's vision is to establish a well-functioning Digital Single Market, where Europeans are able to move and trade among the EU member states. On the other hand the large amount of information about laws that apply in each EU country has posed significant barriers in this vision. Moreover only legal experts can follow the latest legislation in each country consuming a large amount of business resources in order to follow the current legislation. However, Mass customization tools can help to filter and thereby reduce the flood of legal information and make it easier to be followed from businesses and citizens without legal expertise. The proposed solution is a novel ICT architecture utilising and built upon text mining, advanced processing and semantic analysis of legal information towards the provision of a set of services for citizens, businesses, and administrations of the European Union. In order to provide the most appealing, comprehensive and added value services in the legal domain, this paper presents six use case scenarios based on the opinion of different target groups. Conducting interviews and focus groups, we were able to identify the novel functionalities and services of great importance for the users highlighting and addressing users' daily problems regarding legal information. Generally, interviews with the different target groups reveal that at this point, users prioritise their needs towards more basic services such as search functionalities and correlation with previous laws. Lawyers on the other hand as more competent target group asked for summarisation and reporting services. All target groups where eager on the implementation of this service which as it seems it will directly impact their everyday professional and personal use of legal information.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3326365.3326413',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Graphical Keyword Service for Research Papers with Text-Mining Method',\n",
       "  'authors': \"['Yejin Jo', 'Eun-Gyeong Kim', 'Yongju Shin']\",\n",
       "  'date': 'May 2017',\n",
       "  'source': \"ICCDA '17: Proceedings of the International Conference on Compute and Data Analysis\",\n",
       "  'abstract': 'This paper is for utilization of text mining method to provide visual keywords of the papers and reports. This study presents a visualization approach to secure intuitive understanding rather than abstract, keywords. The statistical examples of few technical papers are shown. The graphical methods in this paper will be helpful tools for researchers, the public who need to access expert literatures. The authors tried to draw graphical methods by using R programming language in this paper. In addition, we expect this work would contribute to the public who want to seek expert papers in easy and intuitive way.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3093241.3093242',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Point cloud capture and segmentation of animal images using classification and clustering',\n",
       "  'authors': \"['Justin Petluk', 'Wendy Osborn']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"HANIMOB '21: Proceedings of the 1st ACM SIGSPATIAL International Workshop on Animal Movement Ecology and Human Mobility\",\n",
       "  'abstract': 'Measuring characteristics of animals in the wild is not always possible, due to their demeanour and lack of human contact. Remote capture and processing methods, including the segmentation of animal data into relevant body parts, are required. Existing solutions are either costly or too cumbersome to use in the wild. This study explores the use of RGB depth (RGB-D) cameras for data capture of a target animal from a distance. In addition, this study explores the extraction and segmentation of the resulting animal data into point clouds, and the creation of machine learning models for the automated segmentation of this data. Results of this study, including an experimental evaluation, demonstrate the feasibility of utilizing RGB-D cameras for animal data capture, and that classification outperformed clustering for automated animal data segmentation.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3486637.3489485',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Semi-Supervised Techniques for Mining Learning Outcomes and Prerequisites',\n",
       "  'authors': \"['Igor Labutov', 'Yun Huang', 'Peter Brusilovsky', 'Daqing He']\",\n",
       "  'date': 'August 2017',\n",
       "  'source': \"KDD '17: Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining\",\n",
       "  'abstract': 'Educational content of today no longer only resides in textbooks and classrooms; more and more learning material is found in a free, accessible form on the Internet. Our long-standing vision is to transform this web of educational content into an adaptive, web-scale \"textbook\", that can guide its readers to most relevant \"pages\" according to their learning goal and current knowledge. In this paper, we address one core, long-standing problem towards this goal: identifying outcome and prerequisite concepts within a piece of educational content (e.g., a tutorial). Specifically, we propose a novel approach that leverages textbooks as a source of distant supervision, but learns a model that can generalize to arbitrary documents (such as those on the web). As such, our model can take advantage of any existing textbook, without requiring expert annotation. At the task of predicting outcome and prerequisite concepts, we demonstrate improvements over a number of baselines on six textbooks, especially in the regime of little to no ground-truth labels available. Finally, we demonstrate the utility of a model learned using our approach at the task of identifying prerequisite documents for adaptive content recommendation --- an important step towards our vision of the \"web as a textbook\".',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3097983.3098187',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Research on Data Mining in Medical Data Visualization',\n",
       "  'authors': \"['Hengliang Shi', 'Lei Zhang', 'Lintao Zheng', 'Gang Liu']\",\n",
       "  'date': 'December 2019',\n",
       "  'source': \"ICNCC '19: Proceedings of the 2019 8th International Conference on Networks, Communication and Computing\",\n",
       "  'abstract': 'Data mining is an important technology in the field of artificial intelligence. Modern medical and health industry is changing from traditional information representation to isomerization and diversification. The organic combination of the two can excavate intelligent diagnosis and treatment technologies and methods with broad application prospects. This paper mainly summarizes the current classification, regression analysis, clustering, Association rules, features, change and deviation analysis of data mining, the classical algorithms of Web page mining, and the development trend of visualization of medical data mining.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3375998.3376041',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Defining Data Literacy Communities by Their Objectives: A Text Mining Analysis',\n",
       "  'authors': \"['Ahmed Mohamed Fahmy Yousef', 'Johanna Catherine Walker', 'Manuel Leon-Urrutia']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"WebSci '21 Companion: Companion Publication of the 13th ACM Web Science Conference 2021\",\n",
       "  'abstract': 'Data literacy is a multidimensional concept that attracts the attention of a variety of communities of practice, from different angles. The authors grouped these communities of practice in three categories: education, fields and professions, and citizenship. The meaning of data literacy varies depending on who uses it, and its concept is often conveyed in terms other than data literacy. This paper addresses the problematization of data literacy as a term by examining academic literature around it. To this end, a desk study was carried out to gather sources where the term is used. After an extensive search in the main academic databases and a subsequent PRISMA selection process, automated content analysis was applied to the gathered sources. The findings suggest that the concept of data literacy has a different treatment in different communities of practice. For example, librarians and citizen scientists have a different understanding of the concept of data literacy.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3462741.3466663',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Student Performance Prediction and Classification Using Machine Learning Algorithms',\n",
       "  'authors': \"['Boran Sekeroglu', 'Kamil Dimililer', 'Kubra Tuncal']\",\n",
       "  'date': 'March 2019',\n",
       "  'source': 'ICEIT 2019: Proceedings of the 2019 8th International Conference on Educational and Information Technology',\n",
       "  'abstract': \"For a productive and a good life, education is a necessity and it improves individuals' life with value and excellence. Also, education is considered a vital need for motivating self-assurance as well as providing the things are needed to partake in today's World. Throughout the years, education faced a number of challenges. Different methods of teaching and learning are suggested to increase the learning quality. In today's world, computers and portable devices are employed in every phase of daily life and many materials are available online anytime, anywhere. Technologies like Artificial Intelligence had a surprising evolution in many fields especially in educational teaching and learning processes. Higher education institutions have started to adopt the use of technology into their traditional teaching mechanisms for enhancing learning and teaching. In this paper, two datasets have been considered for the prediction and classification of student performance respectively using five machine learning algorithms. Eighteen experiments have been performed and preliminary results suggest that performances of students might be predictable and classification of these performances can be increased by applying pre-processing to the raw data before implementing machine learning algorithms.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3318396.3318419',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Document Enrichment using DBPedia Ontology for Short Text Classification',\n",
       "  'authors': \"['Jernej Flisar', 'Vili Podgorelec']\",\n",
       "  'date': 'June 2018',\n",
       "  'source': \"WIMS '18: Proceedings of the 8th International Conference on Web Intelligence, Mining and Semantics\",\n",
       "  'abstract': 'Every day, millions of short-texts are generated for which effective tools for organization and retrieval are required. Because of the short length of these documents and of their extremely sparse representations, the traditional text classification methods are not effective. We propose a new approach that uses DBpedia Spotlight annotation tools, to identify relevant entities in text and enrich short text documents with concepts derived from those entities, represented in DBpedia ontology. Our experiments show that the proposed document enrichment approach is beneficial for classification of short texts, and is robust with respect to concept drifts and input sources. We report experimental results in three challenging collections, using a variety of classification methods. The results show that the use of DBpedia ontology significantly improves the classification performance of classifiers in short-text classification.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3227609.3227649',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Senegalese Road Crash Data Analysis Based On Clustering And Association Rule Mining',\n",
       "  'authors': \"['Yoro DIA', 'Cheikh Tidiane SECK', 'Mouhamadou Saliou DIALLO', 'Ousmane SALL', 'Mamadou Bousso', 'Edouard Ngor SARR', 'Mamadou Lamine MBOUP']\",\n",
       "  'date': 'February 2021',\n",
       "  'source': \"DSDE '21: 2021 4th International Conference on Data Storage and Data Engineering\",\n",
       "  'abstract': \"This paper presents a study on road accident data in Senegal by identifying associations within accident cases. We use unsupervised classification techniques such as clustering and association rule mining. Pre-processing of our data set revealed subgroup structures within the data. To reduce the association rule mining algorithm's search space, we used the k-modes clustering method as the main segmentation task on road accident data. Then, association rule mining helps identify the different circumstances associated with an accident in each group obtained by the k-modes algorithm. The results of the study show that to improve road safety, the authorities in charge of road transportation could orient their prevention policies towards three main aspects: vehicle fleet renewal, respect for the Highway Code, and prohibition of large vehicles circulation, such as trucks used for goods transportation at rush hours.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3456146.3456158',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A SURVEY ON SIMILARITY MEASURES AND MACHINE LEARNING ALGORITHMS FOR CLASSIFICATION AND PREDICTION',\n",
       "  'authors': \"['Sravan kiran Vangipuram', 'Rajesh Appusamy']\",\n",
       "  'date': 'April 2021',\n",
       "  'source': \"DATA'21: International Conference on Data Science, E-learning and Information Systems 2021\",\n",
       "  'abstract': 'An important observation which figures out when we look into several applications which are the result of applying data science, machine learning, and deep learning techniques is that most of these techniques are based on the concept of measuring similarity between any two vectors. These vectors may act as representatives for objects being considered. Similarity measurement thus gains a great importance in the design of machine learning or deep learning algorithms and techniques. In similar lines, when we are required to carry a supervised or unsupervised learning task, an algorithm is required to carry the task efficiently. Thus, in this paper, our objective is to outline various similarity measures that have been considered for carrying supervised or unsupervised learning tasks and also to throw light on different machine learning algorithms employed for supervised and unsupervised learning tasks from disease classification and prediction point of view and also interdisciplinary domains such as time series analysis, temporal data mining, medical data mining, and anomaly or intrusion detection.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3460620.3460755',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Data mining in IoT: data analysis for a new paradigm on the internet',\n",
       "  'authors': \"['Peter Wlodarczak', 'Mustafa Ally', 'Jeffrey Soar']\",\n",
       "  'date': 'August 2017',\n",
       "  'source': \"WI '17: Proceedings of the International Conference on Web Intelligence\",\n",
       "  'abstract': 'This paper provides an overview on Data Mining (DM) technologies for the Internet of Things (IoT). IoT has become an active area of research, since IoT promises among other to improve quality of live and safety in Smart Cities, to make resource supply and waste management more efficient, and optimize traffic. DM is highly domain specific and depends on what is being mined for. For instance, if IoT is used to optimize traffic in a Smart City to reduce traffic jams and to find parking spaces quicker, different types of data needs to be collected and analysed from an eHealth solution, where IoT is used in a Smart Home to monitor the well being of patients or elderly people. IoT connects things that can collect numeric data from smart sensors, streaming data from cameras or route information on maps. Depending on the type of data, different techniques need to be adopted to analyse them. Also, many IoT applications analyse data from different devices and correlate them to make predictions about possible machine failures in production sites or looming emergency situations in Smart Buildings in a home security application. DM techniques need to handle the heterogeneity of IoT data, the large volumes of data and the speed at which they are produced. This paper explores the state of the art DM techniques for IoT.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3106426.3115866',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Data Mining Methods for Solving Classification Problem of Oil Wells',\n",
       "  'authors': \"['Zayar Aung', 'Mikhaylov Ilya Sergeevich', 'Ye Thu Aung']\",\n",
       "  'date': 'January 2020',\n",
       "  'source': 'BDET 2020: Proceedings of the 2020 2nd International Conference on Big Data Engineering and Technology',\n",
       "  'abstract': 'The purpose of this work is to create a learning algorithm which is based on accumulated historical data on previously drilled wells. Wells will forecast an emergency accompanied by drilling. Such a decision support system will help the engineer time to intervene in the drilling process and prevent high drilling costs simple and repair equipment resulting in an accident.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3378904.3378911',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A Combination of Text Mining Techniques for Relevant Literature Search and Extractive Summarization',\n",
       "  'authors': \"['Thiptanawat Phongwattana', 'Jonathan H. Chan']\",\n",
       "  'date': 'September 2018',\n",
       "  'source': \"NLPIR '18: Proceedings of the 2nd International Conference on Natural Language Processing and Information Retrieval\",\n",
       "  'abstract': 'Over the past few years, the amount of research papers published has dramatically increased. Consequently, researchers spend a lot of time reviewing relevant literature in order to better understand their domain of interest and keep up with new developments. After doing literature reviews in the area of text mining, we found many works proposing the means of sentence representation in machine learning for finding sentence similarity. These include average bag of words, weight average word vectors, bag of n-grams, and matrix-vector operations. However, these techniques are limited in word ordering and semantic analysis. This paper proposes a framework that combines two text mining techniques, paragraph vectors and TextRank, for the selection of relevant research paper and extractive summarization, respectively. Our training corpus includes over 20 million research papers. The aim of this work is to build a supplementary research tool that assists researchers in saving time conducting literature reviews. As the result, we can rank all relevant research papers potentially within the corpus, and utilize the outputs in our literature reviews. Moreover, the tool can extract all potential keywords in a single task as well.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3278293.3278300',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Crytojacking Classification based on Machine Learning Algorithm',\n",
       "  'authors': \"['Wan Nur Aaisyah Binti Wan Mansor', 'Azuan Ahmad', 'Wan Shafiuddin Zainudin', 'Madihah Mohd Saudi', 'Mohd Nazri Kama']\",\n",
       "  'date': 'April 2020',\n",
       "  'source': \"ICCBN '20: Proceedings of the 2020 8th International Conference on Communications and Broadband Networking\",\n",
       "  'abstract': 'The rise of cryptocurrency has resulted in a number of concerns. A new threat known as cryptojacking\" has entered the picture where cryptojacking malware is the trend for future cyber criminals, who infect computers, install cryptocurrency miners, and use stolen information from victim databases to set up wallets for illicit funds transfers. Worst by 2020, researchers estimate there will be 30 billion of IoT devices in the world. Majority of the devices are highly vulnerable to simple attacks based on weak passwords and unpatched vulnerabilities and poorly monitored. Thus it is the best projection that IoT become a perfect target for cryptojacking malwares. There are lacks of study that provide in depth analysis on cryptojacking malware especially in the classification model. As IoT devices requires small processing capability, a lightweight model are required for the cryptojacking malware detection algorithm to maintain its accuracy without sacrificing the performance of other process. As a solution, we propose a new lightweight cryptojacking classifier model based on instruction simplification and machine learning technique that can detect the cryptojacking classification algorithm. This research aims to study the features of existing cryptojacking classification algorithm, to enhanced existing algorithm and to evaluate the enhanced algorithm for cryptojacking malware classification. The output of this research will be significant used in detecting cryptojacking malware attacks that benefits multiple industries including cyber security contractors, oil and gas, water, power and energy industries which align with the National Cyber Security Policy (NCSP) which address the risks to the Critical National Information Infrastructure (CNII).',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3390525.3390537',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Vulnerability assessment of machine learning based malware classification models',\n",
       "  'authors': \"['Godwin Raju', 'Pavol Zavarsky', 'Adetokunbo Makanju', 'Yasir Malik']\",\n",
       "  'date': 'July 2019',\n",
       "  'source': \"GECCO '19: Proceedings of the Genetic and Evolutionary Computation Conference Companion\",\n",
       "  'abstract': 'The primary focus of the machine learning model is to train a system to achieve self-reliance. However, due to the absence of the inbuilt security functions the learning phase itself is not secured which allows attacker to exploit the security vulnerabilities in the machine learning model. When a malicious adversary manipulates the input data, it exploits vulnerabilities of machine learning algorithms which can compromise the entire system. In this research study, we are conducting a vulnerability assessment of the malware classification model by injecting the datasets with an adversarial example to degrade the quality of classification obtained currently by a trained model. The objective is to find the security gaps that are exploitable in the model. The vulnerability assessment is done by introducing the malware classification model to an AML environment using the Black-Box attack. The simulation provided an insight into the inputs injected into the classifiers and proves the inherent security vulnerability exists in the classification model.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3319619.3326897',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Application of Data Mining Classification Algorithms for Breast Cancer Diagnosis',\n",
       "  'authors': \"['Hajar Saoud', 'Abderrahim Ghadi', 'Mohamed Ghailani', 'Boudhir Anouar Abdelhakim']\",\n",
       "  'date': 'October 2018',\n",
       "  'source': \"SCA '18: Proceedings of the 3rd International Conference on Smart City Applications\",\n",
       "  'abstract': 'Breast cancer is one of the diseases that represent a large number of incidence and mortality in the world. Data mining classifications techniques will be effective tools for classifying data of cancer to facilitate decision-making. The objective of this paper is to compare the performance of different machine learning algorithms in the diagnosis of breast cancer, to define exactly if this type of cancer is a benign or malignant tumor. Six machine learning algorithms were evaluated in this research Bayes Network (BN), Support Vector Machine (SVM), k-nearest neighbors algorithm (Knn), Artificial Neural Network (ANN), Decision Tree (C4.5) and Logistic Regression. The simulation of the algorithms is done using the WEKA tool (The Waikato Environment for Knowledge Analysis) on the Wisconsin breast cancer dataset available in UCI machine learning repository.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3286606.3286861',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Data mining toolkit for extraction of knowledge from LMS',\n",
       "  'authors': \"['W. Villegas-Ch', 'S. Luján-Mora', 'Diego Buenaño-Fernandez']\",\n",
       "  'date': 'December 2017',\n",
       "  'source': \"ICETC '17: Proceedings of the 9th International Conference on Education Technology and Computers\",\n",
       "  'abstract': 'Today, information technology (IT) is an active part of education. Its main impact is in the administration of learning management systems (LMS). The support provided by IT in LMS has generated greater dexterity in the evaluation of the quality of education. The evaluation process usually includes the use of tools applied to online analytical processing (OLAP). The application of OLAP allows the consultation of large amounts of data. Data mining algorithms can be applied to the data collected to perform a pattern analysis. The potential use of these tools has resulted in their specialization, both in the presentation and in the algorithmic techniques, allowing the possibility of educational data mining (EDM). EDM seeks to enhance or customize education within LMS by classifying groups of students in terms of similar characteristics that require specific resources. The ease of use and extensive information about some of the EDM tools has caused many educational institutions to consider them for their own use. However, these institutions often make errors in data management. Errors in the use of data mean that the improvements in LMS are inadequate. The work described in this paper provides a guide on the use of applied methodology in the process of knowledge extraction (KDD). It also enumerates some of the tools that can be used for each step of the process.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3175536.3175553',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A new approach to text classification based on naïve Bayes and modified TF-IDF algorithms',\n",
       "  'authors': \"['El Barakaz Fatima', 'El Moutaouakkil Abdelmajid']\",\n",
       "  'date': 'October 2017',\n",
       "  'source': \"SCAMS '17: Proceedings of the Mediterranean Symposium on Smart City Application\",\n",
       "  'abstract': 'In text mining, classification is a technique of assigning documents to predefined classes. Naïve Bayes algorithm is the basic of text classification technique; it is the most widely used algorithm for diverse text classification applications. This paper proposes a new approach of unstructured text classification using quantitative variable; this variable has a strong correlation with the text attribute. We classify the text attribute using the bags of words to get the keywords of our corpus, and we define classes to which we wish affecting terms, then we apply the naive Bayes classifier. This classifier is not highly efficient in an unstructured text classification case. So we propose a new classifier combining two algorithms principles: term frequency- inverse document frequency (TF-IDF) and k-nearest neighbor (KNN). This classifier is applied on the quantitative variable. The objective is to achieve better classification of an unstructured text with a high level of efficiency. The result of the proposed classifier method was very satisfactory, especially since it enriches the dictionary content each time we use it.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3175628.3175643',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Classification of Abstract Images using Machine Learning',\n",
       "  'authors': \"['Animesh Karnewar', 'Ameeth Kanawaday', 'Chinmay Sawant', 'Yash Gupta']\",\n",
       "  'date': 'June 2017',\n",
       "  'source': \"ICDLT '17: Proceedings of the 2017 International Conference on Deep Learning Technologies\",\n",
       "  'abstract': \"Abstract painting uses a visual language of form, color, and line to create a composition that may exist with a degree of independence from visual references in the world. Sometimes, it isn't even about giving the impression of real life without all the little details. This makes the task of classification of the paintings into genres altogether more difficult. In this paper, we describe a systematic method for a machine learning based approach to classifying digital images of abstract art into their most apt artistic styles. To increase the effectiveness of classification, we stack the two classifiers namely Convolutional Neural Network and Deep Neural Network. The hybrid model thus formed outperforms the separate singular models. Furthermore, the task of analysis of color emotions of the artistic image helps to gain further insights of the said classes.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3094243.3094259',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Memory-Centric Reconfigurable Accelerator for Classification and Machine Learning Applications',\n",
       "  'authors': \"['Robert Karam', 'Somnath Paul', 'Ruchir Puri', 'Swarup Bhunia']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Journal on Emerging Technologies in Computing Systems',\n",
       "  'abstract': 'Big Data refers to the growing challenge of turning massive, often unstructured datasets into meaningful, organized, and actionable data. As datasets grow from petabytes to exabytes and beyond, it becomes increasingly difficult to run advanced analytics, especially Machine Learning (ML) applications, in a reasonable time and on a practical power budget using traditional architectures. Previous work has focused on accelerating analytics readily implemented as SQL queries on data-parallel platforms, generally using off-the-shelf CPUs and General Purpose Graphics Processing Units (GPGPUs) for computation or acceleration. However, these systems are general-purpose and still require a vast amount of data transfer between the storage devices and computing elements, thus limiting the system efficiency. As an alternative, this article presents a reconfigurable memory-centric advanced analytics accelerator that operates at the last level of memory and dramatically reduces energy required for data transfer. We functionally validate the framework using an FPGA-based hardware emulation platform and three representative applications: Naïve Bayesian Classification, Convolutional Neural Networks, and k-Means Clustering. Results are compared with implementations on a modern CPU and workstation GPGPU. Finally, the use of in-memory dataset decompression to further reduce data transfer volume is investigated. With these techniques, the system achieves an average energy efficiency improvement of 74× and 212× over GPU and single-threaded CPU, respectively, while dataset compression is shown to improve overall efficiency by an additional 1.8× on average.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/2997649',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Use Text Mining for Financial Reports Analysis: Long Text to Image Converter',\n",
       "  'authors': \"['Chia-Hao Chiu', 'Yun-Cheng Tsai']\",\n",
       "  'date': 'November 2020',\n",
       "  'source': \"ICCIP '20: Proceedings of the 6th International Conference on Communication and Information Processing\",\n",
       "  'abstract': \"In this study, we propose a novel article analysis method. This method converts the article classification problem into image classification problem by projecting texts into images and then applying CNN models for classification. We call the method the Long Text to Image Converter (LTIC). The features are extract automatically from the generate images, hence there is no need of any explicit step of embedding the words or characters into numeric vector representations. This method saves the time to experiment pre-process. This study uses the financial domain as an example. In companies financial reports, there will describes the company's financial trends. The content has many financial terms used to infer the company's current and future financial position. The results indicated an 80% accuracy rate. The proposed LTIC produce excellent results during practical application. The return on simulated investment is 46%. In addition to tangible returns, the LTIC method reduces the time required for article analysis and is able to provide article classification references in a short period to facilitate the decisions of the researcher.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3442555.3442557',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Health Consumer Usage Patterns in Management of CVD using Data Mining Techniques',\n",
       "  'authors': \"['Devipriyaa Nagappan', 'Jim Warren', 'Patricia Riddle']\",\n",
       "  'date': 'January 2019',\n",
       "  'source': \"ACSW '19: Proceedings of the Australasian Computer Science Week Multiconference\",\n",
       "  'abstract': \"The Healthcare system is exposed to the increasing impact of chronic diseases including cardiovascular diseases; it is of much importance to analyze and understand the health trajectories for efficient planning and fair allotment of resources. This work proposes an approach based on mining clinical data to support the exploration of health trajectories related to cardiovascular diseases. As the health data are highly confidential, we aimed to conduct our experiments using a large, synthetic, longitudinal dataset, constituted to represent the CVD risk factors distribution and temporal sequence of events related to heart failure hospitalization and readmission. This research work analyses and represents the temporal events or states of the patient's trajectory with the aim of understanding the patient's journey in the management of the chronic condition and its complications by using data mining techniques. This study focuses on developing an efficient algorithm to find cohesive clusters for handling the temporal events. Clustering health trajectories have been carried out by proposing an improved version of the Ant-based clustering algorithm. Insights from this study can potentially result in evidence that these approaches are useful in understanding and analyzing patient's health trajectories for better management of the chronic condition and its progression.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3290688.3290732',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Automatic classification of Aurora-related tweets using machine learning methods',\n",
       "  'authors': \"['Vyron Christodoulou', 'Rosa Filgueira', 'Emma Bee', 'Elizabeth MacDonald', 'Burcu Kosar']\",\n",
       "  'date': 'March 2019',\n",
       "  'source': \"ICGDA '19: Proceedings of the 2019 2nd International Conference on Geoinformatics and Data Analysis\",\n",
       "  'abstract': 'The constant flow of information by social media provides valuable information about all sorts of events at a high temporal and spatial resolution. Over the past few years we have been analyzing in real-time geological hazards/phenomena, such as earthquakes, volcanic eruptions, landslides, floods or the aurora, as part of the GeoSocial project, by geo-locating tweets filtered by keywords in a web-map. However, up to this date only a keyword-based filtering was applied that does not always filter out tweets that are unrelated to hazard-events. Therefore, this work explores five learning-based classification techniques: a Linear SVM and four Deep Neural Networks (DNNs): a Convolutional Neural Network (CNN), a Recurrent Neural Network (RNN), a RNN-Long-short-term memory (RNN-LSTM) and a RNN-Gated Recurrent Unit (GRU) for automatic hazard-event classification based on tweets about Aurora sightings. In addition, for the DNNS we also trained the algorithms using pre-trained word2vec word-embeddings. We finally evaluate the algorithms using two datasets, one from the Aurorasaurus application and one manually labeled in the BGS. We show that DNNs and especially the CNN perform better for both datasets and that there is potential for improvement. Our code is also available online.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3318236.3318242',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Big Data Analytics in Association Rule Mining: A Systematic Literature Review',\n",
       "  'authors': \"['Mahtab Shahin', 'Sijo Arakkal Peious', 'Rahul Sharma', 'Minakshi Kaushik', 'Sadok Ben Yahia', 'Syed Attique Shah', 'Dirk Draheim']\",\n",
       "  'date': 'January 2021',\n",
       "  'source': 'BDET 2021: 2021 the 3rd International Conference on Big Data Engineering and Technology (BDET)',\n",
       "  'abstract': 'Due to the rapid impact of IT technology, data across the globe is growing exponentially as compared to the last decade. Therefore, the efficient analysis and application of big data require special technologies. The present study performs a systematic literature review to synthesize recent research on the applicability of big data analytics in association rule mining (ARM). Our research strategy identified 4797 scientific articles, 27 of which were identified as primary papers relevant to our research. We have extracted data from these papers to identify various technologies and algorithms of using big data in association rule mining and identified their limitations in regards to the big data categories (volume, velocity, variety, and veracity).',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3474944.3474951',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Application of data mining based on machine learning in automobile power prediction',\n",
       "  'authors': \"['Kexin Zhang']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"EITCE '21: Proceedings of the 2021 5th International Conference on Electronic Information Technology and Computer Engineering\",\n",
       "  'abstract': 'Automobile power prediction is the key to the automobile industry. Only by reasonably predicting the automobile power can we produce cars that meet the needs of consumers. Compared with traditional methods, machine learning model improves the accuracy of classification in power. Machine learning models include BP neural network, random forest and KNN algorithm. In order to select the optimal vehicle power prediction model, this paper compares the advantages and disadvantages of these three machine learning models through design experiments.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3501409.3501532',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Vehicle Image Classification Using Data Mining Techniques',\n",
       "  'authors': \"['Agnes S. Suguitan', 'Lucille N. Dacaymat']\",\n",
       "  'date': 'May 2019',\n",
       "  'source': \"CSSE '19: Proceedings of the 2nd International Conference on Computer Science and Software Engineering\",\n",
       "  'abstract': 'This paper focuses on the application of different data mining techniques to classify images of vehicles into three classes using Weka. The dataset used for this study were collected from Google Image search engine and other dataset websites. In preprocessing the images, filters such as Color Layout, Edge Histogram and Pyramid Histogram of Oriented Gradients were explored to extract the image features from the dataset. Classification techniques such as Multilayer Perceptron, Sequential Minimal Optimization, Logistic Model Trees, Simple Logistic and Random Forest were used. Results of the study showed that the edge histogram features provided much information to the classifiers in order to correctly classify the images. The SMO classifier performs best with the highest accuracy of 82.37%.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3339363.3339366',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Application of Data Mining Technology in Financial Fraud Identification',\n",
       "  'authors': \"['Shunyu Yao']\",\n",
       "  'date': 'September 2021',\n",
       "  'source': 'ICISCAE 2021: 2021 4th International Conference on Information Systems and Computer Aided Education',\n",
       "  'abstract': \"In the era of big data, the identification of financial fraud has changed from the traditional case analysis stage to the data mining stage. The application of data mining technology is helpful in identifying the company's fraud in massive data. In the process of fraud identification, a variety of data mining methods can be used, such as statistical methods, classification, clustering and neural network construction. This paper analyzes the application of several kinds of data mining methods in the field of financial fraud, and finds that due to the different model building methods and sample selection in empirical research, the accuracy of each method has not yet been determined. In the future, with the development of accounting information system, the combination of data mining and text mining will become the inevitable trend of financial fraud identification.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3482632.3487540',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': \"Data miners' little helper: data transformation activity cues for cluster analysis on document collections\",\n",
       "  'authors': \"['Tania Cerquitelli', 'Evelina Di Corso', 'Francesco Ventura', 'Silvia Chiusano']\",\n",
       "  'date': 'June 2017',\n",
       "  'source': \"WIMS '17: Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics\",\n",
       "  'abstract': 'In this paper we propose a new self-learning engine to streamline the analytics process, as it enables analysts to mine massive data repositories with minimal user intervention. In the context of cluster analysis on a collection of documents this new system, named SELF-DATA (SELF-learning DAta TrAnsformation), suggests to the analyst how to configure the whole mining process for a given dataset. SELF-DATA relies on an engine exploring different data weighting schemas (e.g., normalized term frequencies) and data transformation methods (e.g., PCA) before applying the cluster analysis, evaluating and comparing solutions through different quality indices (e.g., weighted Silhouette), and presenting the k-top solutions to the analyst. SELF-DATA will also include a knowledge base storing results of experiments on previously processed datasets, and a classification algorithm trained on the knowledge base content to forecast the best configuration for the whole mining process for an unexplored dataset. The first development of SELF-DATA running on Apache Spark has been validated on 5 collections of documents. Experimental results highlight that TF-IDF and logarithmic entropy are effective to measure item relevance with sparse datasets, and the LSI method outperforms PCA with a large dictionary.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3102254.3102288',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Snippext: Semi-supervised Opinion Mining with Augmented\\xa0Data',\n",
       "  'authors': \"['Zhengjie Miao', 'Yuliang Li', 'Xiaolan Wang', 'Wang-Chiew Tan']\",\n",
       "  'date': 'April 2020',\n",
       "  'source': \"WWW '20: Proceedings of The Web Conference 2020\",\n",
       "  'abstract': 'Online services are interested in solutions to opinion mining, which is the problem of extracting aspects, opinions, and sentiments from text. One method to mine opinions is to leverage the recent success of pre-trained language models which can be fine-tuned to obtain high-quality extractions from reviews. However, fine-tuning language models still requires a non-trivial amount of training data.  In this paper, we study the problem of how to significantly reduce the amount of labeled training data required in fine-tuning language models for opinion mining. We describe , an opinion mining system developed over a language model that is fine-tuned through semi-supervised learning with augmented data. A novelty of is its clever use of a two-prong approach to achieve state-of-the-art (SOTA) performance with little labeled training data through: (1) data augmentation to automatically generate more labeled training data from existing ones, and (2) a semi-supervised learning technique to leverage the massive amount of unlabeled data in addition to the (limited amount of) labeled data. We show with extensive experiments that performs comparably and can even exceed previous SOTA results on several opinion mining tasks with only half the training data required. Furthermore, it achieves new SOTA results when all training data are leveraged. By comparison to a baseline pipeline, we found that extracts significantly more fine-grained opinions which enable new opportunities of downstream applications.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3366423.3380144',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Special Issue: Text Mining and Information Analysis; Retrieving and Clustering Keywords in Neurosurgery Operation Reports Using Text Mining Techniques',\n",
       "  'authors': \"['Chun-Chih Liao', 'Furen Xiao', 'Jau-Min Wong', 'I-Jen Chiang', 'Yi-Hsin Tsai', 'Charles Chih-Ho Liu', 'Ke-Chun Huang']\",\n",
       "  'date': 'June 2018',\n",
       "  'source': \"ICMHI '18: Proceedings of the 2nd International Conference on Medical and Health Informatics\",\n",
       "  'abstract': 'Background: To develop a more practical and reasonable classification of surgical procedures, we applied text mining techniques to retrieve and categorize keywords in operation reports. Materials and Methods: Based on neurosurgical operation reports performed in a Taiwan medical center between 2009 and 2012, a corpus containing 3,657 documents was built. A total of 9,906 words were extracted. Initially, we applied term frequency-inverse document frequency (TF-IDF) weighting to automatically select pertinent keywords but the results were unsatisfactory. Then, we manually chose 45 keywords that belong to 3 categories: brain, spine and others. All documents were checked in an automated fashion for the presence of these keywords, producing a binary data matrix, which was used to compute the cosine similarity matrix. Then, we applied 6 variants of agglomerative clustering to build the dendrograms. Results: The document frequencies (DFs) of these 45 keywords ranged from 12 to 1,250, with an average of 444±342. The number of distinctive keywords per document ranged from 0-15, with an average of 5.5±2.5. The similarities between DF vectors are higher between keywords in the same category (brain or spine). The shortest link method and the unweighted pair-group method using the centroid (UPGMC) methods performed best on external and internal evaluation, respectively. Conclusion: The distributions of important keywords in neurosurgery operation reports reveal the localized nature of surgical procedures.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3239438.3239488',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Analysis and Comparison of Deep Learning Networks for Supporting Sentiment Mining in Text Corpora',\n",
       "  'authors': \"['Teresa Alcamo', 'Alfredo Cuzzocrea', 'Giosue Lo Bosco', 'Giovanni Pilato', 'Daniele Schicchi']\",\n",
       "  'date': 'November 2020',\n",
       "  'source': \"iiWAS '20: Proceedings of the 22nd International Conference on Information Integration and Web-based Applications &amp; Services\",\n",
       "  'abstract': 'In this paper, we tackle the problem of the irony and sarcasm detection for the Italian language to contribute to the enrichment of the sentiment analysis field. We analyze and compare five deep-learning systems. Results show the high suitability of such systems to face the problem by achieving 93% of F1-Score in the best case. Furthermore, we briefly analyze the model architectures in order to choose the best compromise between performances and complexity.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3428757.3429144',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Big Data Processing using Machine Learning algorithms: MLlib and Mahout Use Case',\n",
       "  'authors': \"['Khadija Aziz', 'Dounia Zaidouni', 'Mostafa Bellafkih']\",\n",
       "  'date': 'October 2018',\n",
       "  'source': \"SITA'18: Proceedings of the 12th International Conference on Intelligent Systems: Theories and Applications\",\n",
       "  'abstract': 'Machine learning is a field within artificial intelligence that allows machines to learn on their own from existing information to make predictions or/and decisions. There are three main categories of machine learning techniques: Collaborative filtering (for making recommendations), Clustering (for discovering structure in collections of data) and Classification (form of supervised learning). Machine learning helps users to make better decisions, Machine learning algorithms create patterns based on previous information and use them to design predictive models, then, use this models to obtain predictions about future data. A huge amount of data from several sources need methods and techniques to be processed correctly, in order to exploit this data efficiently, machine learning is a great technology for exploiting the needs in big data analysis. This paper describes the implementation of Apache Spark MLlib and Apache Mahout in order to process Big Data using Machine Learning algorithms. Furthermore, we conduct experimental simulations to show the difference between this two Machine Learning frameworks. Subsequently, we discuss the most striking observations that emerge from the comparison of these technologies through several experimental studies.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3289402.3289525',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Design and research of archives management system based on Data Mining Technology',\n",
       "  'authors': \"['Yangyang Dong']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"DSDE '22: 2022 the 5th International Conference on Data Storage and Data Engineering\",\n",
       "  'abstract': \"In order to enrich the functions of the archives management system, data mining technology is selected as the research tool, and the user service function and text type division function are taken as the key design contents. A new research on the design of archives management system is proposed. Among them, the development of user service function takes the user's personal information and access footprint as the basis, and uses data mining technology to extract highly relevant information as recommendation information. Text type division takes preprocessing, word segmentation and de stop words as the processing core to divide text types in detail. The system test results show that the accuracy of text type classification is 100%, the operation efficiency is significantly improved, and it has great advantages in user experience.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3528114.3528135',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Sentiment Polarity Classification using Minimal Feature Vectors and Machine Learning Algorithms',\n",
       "  'authors': \"['Niwan Wattanakitrungroj', 'Nichapat Pinpo', 'Sasiporn Tongman']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"IAIT '21: Proceedings of the 12th International Conference on Advances in Information Technology\",\n",
       "  'abstract': 'Recently, social media users can comment as texts to describe their opinions. These texts can be analyzed to classify them into either positive or negative attitude. Feature vectors for representing the texts must be designed and prepared before building a classifier. Generally, texts are represented by vectors of weights or frequencies of terms that appear in the text. The length of the feature vector is equal to the number of terms in the dictionary derived from the possible words in all texts. The large amount of words in dictionary leads to the high dimensional vector for representing text and bring about the long processing time to training and testing the text classification models. This paper, the low-dimensional vectors, V8D, were proposed for representing the texts. The set of positive and negative words including the words of negation which have the significant meanings were considered as information to create these vectors. Four machine learning algorithms to solve the classification problem, i.e., k-Nearest Neighbors, Naïve Bayes classifier, Artificial Neural Networks and Support Vector Machine, were applied to classify the opinion texts. By experimenting on eight data sets with various domains, the proposed V8D vectors were compared with the traditional TF-IDF vector in term of the predictive correctness. The experimental results show that representing text as our V8D vector for opinion text classification can provide the best efficiency in both of space usage and processing time.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3468784.3469947',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Uncertainty-Aware Reliable Text Classification',\n",
       "  'authors': \"['Yibo Hu', 'Latifur Khan']\",\n",
       "  'date': 'August 2021',\n",
       "  'source': \"KDD '21: Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining\",\n",
       "  'abstract': 'Deep neural networks have significantly contributed to the success in predictive accuracy for classification tasks. However, they tend to make over-confident predictions in real-world settings, where domain shifting and out-of-distribution (OOD) examples exist. Most research on uncertainty estimation focuses on computer vision because it provides visual validation on uncertainty quality. However, few have been presented in the natural language process domain. Unlike Bayesian methods that indirectly infer uncertainty through weight uncertainties, current evidential uncertainty-based methods explicitly model the uncertainty of class probabilities through subjective opinions. They further consider inherent uncertainty in data with different root causes, vacuity (i.e., uncertainty due to a lack of evidence) and dissonance (i.e., uncertainty due to conflicting evidence). In our paper, we firstly apply evidential uncertainty in OOD detection for text classification tasks. We propose an inexpensive framework that adopts both auxiliary outliers and pseudo off-manifold samples to train the model with prior knowledge of a certain class, which has high vacuity for OOD samples. Extensive empirical experiments demonstrate that our model based on evidential uncertainty outperforms other counterparts for detecting OOD examples. Our approach can be easily deployed to traditional recurrent neural networks and fine-tuned pre-trained transformers.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3447548.3467382',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Image-based Candlestick Pattern Classification with Machine Learning',\n",
       "  'authors': \"['Chenghan Xu']\",\n",
       "  'date': 'April 2021',\n",
       "  'source': \"ICMLT '21: Proceedings of the 2021 6th International Conference on Machine Learning Technologies\",\n",
       "  'abstract': 'Financial markets, such as the stock market, bond market and foreign exchange market, are important channels for fund transfer. As a graphical analysis tool, candlestick charts use graphs to display the open, high, low, and close prices in a specific period. In the past, there have been attempts to identify the characteristics of candlesticks based on Gramian Angular Field (GAF) images, but they are not perfect. In this study, we implemented Multilayer Perceptron (MLP), Convolutional Neural Network (CNN), AdaBoost, Random Forest (RF) and XGBoost models, we found that the use of deep learning models is not the best choice for the recognition of candlestick features based on GAF images. Comparing these models, MLP and CNN are better than AdaBoost and RF, but worse than XGBoost. Our results show that for the candlestick pattern classification problem based on GAF images, it is unnecessary to use complex CNNs and traditional machine learning models can also achieve satisfactory results with much less computation resources.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3468891.3468896',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Classification and Prediction Based Data Mining algorithms to Predict Email Marketing Campaigns',\n",
       "  'authors': \"['Redouan Abakouy', 'El Mokhtar En-Naimi', 'Anass El Haddadi']\",\n",
       "  'date': 'November 2017',\n",
       "  'source': \"ICCWCS'17: Proceedings of the 2nd International Conference on Computing and Wireless Communication Systems\",\n",
       "  'abstract': \"Nowadays, digital marketing has become an important tool to increase both activities and revenue of each enterprise. Therefore, many enterprises from different fields have integrated big data and data mining utilities, in order to well classify and target the marketing offers according to member's specific and individual needs taking advantage of member's navigational behavior. Personalized Email Marketing is the process of delivering right offer, at the right time to the right person based on the customer's profile. The main objective of Personalized Email Marketing is to identify the needs of a customer and offer products and services that appeal to that particular customer. In this perspective of research, a comparative study concerning several classification algorithms that can be handle this kind of problems, will be discussed in this paper.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3167486.3167520',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A Study on the Performance Evaluation of Machine Learning Models for Phoneme Classification',\n",
       "  'authors': \"['Ali Shariq Imran', 'Abdolreza Sabzi Shahrebabaki', 'Negar Olfati', 'Torbjørn Svendsen']\",\n",
       "  'date': 'February 2019',\n",
       "  'source': \"ICMLC '19: Proceedings of the 2019 11th International Conference on Machine Learning and Computing\",\n",
       "  'abstract': \"This paper provides a comparative performance analysis of both shallow and deep machine learning classifiers for speech recognition task using frame-level phoneme classification. Phoneme recognition is still a fundamental and equally crucial initial step toward automatic speech recognition (ASR) systems. Often conventional classifiers perform exceptionally well on domain-specific ASR systems having a limited set of vocabulary and training data in contrast to deep learning approaches. It is thus imperative to evaluate performance of a system using deep artificial networks in terms of correctly recognizing atomic speech units, i.e., phonemes in this case with conventional state-of-the-art machine learning classifiers. Two deep learning models - DNN and LSTM with multiple configuration architectures by varying the number of layers and the number of neurons in each layer on the OLLO speech corpora along with six shallow machine learning classifiers for Filterbank acoustic features are thoroughly studied. Additionally, features with three and ten frames temporal context are computed and compared with no-context features for different models. The classifier's performance is evaluated in terms of precision, recall, and F1 score for 14 consonants and 10 vowels classes for 10 speakers with 4 different dialects. High classification accuracy of 93% and 95% F1 score is obtained with DNN and LSTM networks respectively on context-dependent features for 3-hidden layers containing 1024 nodes each. SVM surprisingly obtained even a higher classification score of 96.13% and a misclassification error of less than 5% for consonants and 4% for vowels.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3318299.3318385',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Bayesian text classification and summarization via a class-specified topic model',\n",
       "  'authors': \"['Feifei Wang', 'Junni L. Zhang', 'Yichao Li', 'Ke Deng', 'Jun S. Liu']\",\n",
       "  'date': 'None',\n",
       "  'source': 'The Journal of Machine Learning Research',\n",
       "  'abstract': 'We propose the class-specified topic model (CSTM) to deal with the tasks of text classification and class-specific text summarization. The model assumes that in addition to a set of latent topics that are shared across classes, there is a set of class-specific latent topics for each class. Each document is a probabilistic mixture of the class-specific topics associated with its class and the shared topics. Each class-specific or shared topic has its own probability distribution over a given dictionary. We develop a Bayesian inference of CSTM in the semisupervised scenario, with the supervised scenario as a special case. We analyze in detail the 20 Newsgroups dataset, a benchmark dataset for text classification, and demonstrate that CSTM has better performance than a two-stage approach based on latent Dirichlet allocation (LDA), several existing supervised extensions of LDA, and an L1 penalized logistic regression. The favorable performance of CSTM is also demonstrated through Monte Carlo simulations and an analysis of the Reuters dataset.',\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3546258.3546347',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Spatio-Temporal Routine Mining on Mobile Phone Data',\n",
       "  'authors': \"['Tian Qin', 'Wufan Shangguan', 'Guojie Song', 'Jie Tang']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Knowledge Discovery from Data',\n",
       "  'abstract': 'Mining human behaviors has always been an important subarea of Data Mining. While it provides empirical evidences to psychological/behavioral studies, it also builds the foundation of various big-data systems, which rely heavily on the prediction of human behaviors. In recent years, the ubiquitous spreading of mobile phones and the massive amount of spatio-temporal data collected from them make it possible to keep track of the daily commute behaviors of mobile subscribers and further conduct routine mining on them. In this article, we propose to model mobile subscribers’ daily commute behaviors by three levels: location trajectory, one-day pattern, and routine pattern. We develop the model Spatio-Temporal Routine Mining Model (STRMM) to characterize the generative process between these three levels. From daily trajectories, the STRMM model unsupervisedly extracts spatio-temporal routine patterns that contain two aspects of information: (1) How people’s typical commute patterns are. (2) How much their commute behaviors vary from day to day. Compared to traditional methods, STRMM takes into account the different degrees of behavioral uncertainty in different timespans of a day, yielding more realistic and intuitive results. To learn model parameters, we adopt Stochastic Expectation Maximization algorithm. Experiments are conducted on two real world datasets, and the empirical results show that the STRMM model can effectively discover hidden routine patterns of human commute behaviors and yields higher accuracy results in trajectory prediction task.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3201577',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Few-Sample and Adversarial Representation Learning for Continual Stream Mining',\n",
       "  'authors': \"['Zhuoyi Wang', 'Yigong Wang', 'Yu Lin', 'Evan Delord', 'Khan Latifur']\",\n",
       "  'date': 'April 2020',\n",
       "  'source': \"WWW '20: Proceedings of The Web Conference 2020\",\n",
       "  'abstract': 'Deep Neural Networks (DNNs) have primarily been demonstrated to be useful for closed-world classification problems where the number of categories is fixed. However, DNNs notoriously fail when tasked with label prediction in a non-stationary data stream scenario, which has the continuous emergence of the unknown or novel class (categories not in the training set). For example, new topics continually emerge in social media or e-commerce. To solve this challenge, a DNN should not only be able to detect the novel class effectively but also incrementally learn new concepts from limited samples over time. Literature that addresses both problems simultaneously is limited. In this paper, we focus on improving the generalization of the model on the novel classes, and making the model continually learn from only a few samples from the novel categories. Different from existing approaches that rely on abundant labeled instances to re-train/update the model, we propose a new approach based on Few Sample and Adversarial Representation Learning (FSAR). The key novelty is that we introduce the adversarial confusion term into both the representation learning and few-sample learning process, which reduces the over-confidence of the model on the seen classes, further enhance the generalization of the model to detect and learn new categories with only a few samples. We train the FSAR operated in two stages: first, FSAR learns an intra-class compacted and inter-class separated feature embedding to detect the novel classes; next, we collect a few labeled samples belong to the new categories, utilize episode-training to exploit the intrinsic features for few-sample learning. We evaluated FSAR on different datasets, using extensive experimental results from various simulated stream benchmarks to show that FSAR effectively outperforms current state-of-the-art approaches.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3366423.3380153',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Automating the Assessment of Problem-solving Practices Using Log Data and Data Mining Techniques',\n",
       "  'authors': \"['Karen D. Wang', 'Shima Salehi', 'Max Arseneault', 'Krishnan Nair', 'Carl Wieman']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"L@S '21: Proceedings of the Eighth ACM Conference on Learning @ Scale\",\n",
       "  'abstract': \"Interactive simulations provide an exciting opportunity to assess and teach students the practices used by scientists and engineers to solve real-world problems. This study examines how the logged interaction data from a simulation-based task could be used to automate the assessment of complex problem-solving practices. A total of 73 college students worked on an interactive circuit puzzle embedded in a science simulation in an interview setting. Their problem-solving processes were videotaped and logged in the backend of the simulation. We extracted different sets of features from the log data and evaluated their effectiveness as predictors of students' problem-solving success and evidence for specific problem-solving practices. Our results indicate that the application of data mining techniques guided by knowledge gained from qualitative observation was instrumental in the discovery of semantically meaningful features from the raw log data. These knowledge-grounded features were significant predictors of students' overall problem-solving success and provided evidence on how well they adopted specific problem-solving practices, including decomposition, data collection, and data recording. The results point to promising directions for how scaffolding/feedback could be provided in educational simulations to enhance student learning in problem-solving skills.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3430895.3460127',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Machine learning Android-based malware classification: faculty poster abstract',\n",
       "  'authors': \"['Micheline Al Harrack']\",\n",
       "  'date': 'None',\n",
       "  'source': 'Journal of Computing Sciences in Colleges',\n",
       "  'abstract': 'Analyzing Android malware to improve classification, clustering and therefore detection continues to increasingly evolve as our interconnected society continues to grow and Android mobile market expands. In this research, I explore two publicly available malware collection datasets and apply a combination of Machine Learning algorithms for classifying and clustering each of them. Some classifiers are applied concurrently, while others exclusively then clusters are being tested on the two different datasets separately. Both sets have the same 215 attributes classified and clustered. The experiment results are presented for inference for best classifiers and clusters by comparing results of the proposed methods on the two datasets sharing the same 215 attributes albeit different in size of malware sample and benign software.',\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3512489.3512512',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Data Mining Technique To Get Characteristics Customers of Bendesa Hotel With K-MEANS Algorithm',\n",
       "  'authors': \"['I. G. Karang Komala Putra', 'Gede Indrawan', 'I. Made Candiasa']\",\n",
       "  'date': 'January 2019',\n",
       "  'source': \"ICSIM '19: Proceedings of the 2nd International Conference on Software Engineering and Information Management\",\n",
       "  'abstract': 'This research aims to find customers based on characteristics of hotel customers who stay since there is still no research provides its technological state of the art. Through collaboration between Computer Science and Tourism, this research contributes on the development of K-Means Algorithm using WEKA application that can be elaborated into: 1) Search for best number of clusters used; 2) Identification of hotel customer characteristics; 3) Measurement of accuracy customer characteristics. This research can be used by hotel management to recognize customer characteristics so that they can develop strategies to get as many customers as possible, especially in Bali Province where Bali tourism is considered as one of the largest foreign exchange earners. K-Means algorithm uses CRISP-DM as a data mining life cycle which consists of 6 phases, the entire sequential phase is adaptive. The next phase in sequence depends on the output from the previous phase. In this research, it was tested on 2 clusters of up to 6 clusters. Using the value of sum of squared errors (SSE) is generated 5 clusters are the best from the other. Data on 5 clusters is used as reference to find characteristics of potential customers who stay in hotels. Through experiments, K-Means algorithm has an accuracy of 72% (108 of 150) tests using sample data compared to characteristics produced by K-Means. In the future, this research could be improved by: 1) collaboration between the K-Means algorithm and other clustering algorithms; and 2) add customer characteristics.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3305160.3305184',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Tslearn, a machine learning toolkit for time series data',\n",
       "  'authors': \"['Romain Tavenard', 'Johann Faouzi', 'Gilles Vandewiele', 'Felix Divo', 'Guillaume Androz', 'Chester Holtz', 'Marie Payne', 'Roman Yurchak', 'Marc Rußwurm', 'Kushal Kolar', 'Eli Woods']\",\n",
       "  'date': 'None',\n",
       "  'source': 'The Journal of Machine Learning Research',\n",
       "  'abstract': \"tslearn is a general-purpose Python machine learning library for time series that offers tools for pre-processing and feature extraction as well as dedicated models for clustering, classification and regression. It follows scikit-learn's Application Programming Interface for transformers and estimators, allowing the use of standard pipelines and model selection tools on top of tslearn objects. It is distributed under the BSD-2-Clause license, and its source code is available at https://github.com/tslearn-team/tslearn.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3455716.3455834',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'MI3: Machine-initiated Intelligent Interaction for Interactive Classification and Data Reconstruction',\n",
       "  'authors': \"['Yu Zhang', 'Bob Coecke', 'Min Chen']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Interactive Intelligent Systems',\n",
       "  'abstract': 'In many applications, while machine learning (ML) can be used to derive algorithmic models to aid decision processes, it is often difficult to learn a precise model when the number of similar data points is limited. One example of such applications is data reconstruction from historical visualizations, many of which encode precious data, but their numerical records are lost. On the one hand, there is not enough similar data for training an ML model. On the other hand, manual reconstruction of the data is both tedious and arduous. Hence, a desirable approach is to train an ML model dynamically using interactive classification, and hopefully, after some training, the model can complete the data reconstruction tasks with less human interference. For this approach to be effective, the number of annotated data objects used for training the ML model should be as small as possible, while the number of data objects to be reconstructed automatically should be as large as possible. In this article, we present a novel technique for the machine to initiate intelligent interactions to reduce the user’s interaction cost in interactive classification tasks. The technique of machine-initiated intelligent interaction (MI3) builds on a generic framework featuring active sampling and default labeling. To demonstrate the MI3 approach, we use the well-known cholera map visualization by John Snow as an example, as it features three instances of MI3 pipelines. The experiment has confirmed the merits of the MI3 approach.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3412848',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Using Text Mining to Discover Skills Demanded in Software Development Jobs in Thailand',\n",
       "  'authors': \"['Chamikorn Hiranrat', 'Atichart Harncharnchai']\",\n",
       "  'date': 'July 2018',\n",
       "  'source': \"ICEMT '18: Proceedings of the 2nd International Conference on Education and Multimedia Technology\",\n",
       "  'abstract': 'Comprehension of knowledge and skills expected by industry helps universities design appropriate courses and improve employability for graduates. The objective of this study is to identify current technical knowledge and soft skills required by software industry in Thailand. Text mining techniques are applied to analyze the data collected from online job portal websites. The results are summarized and reported for the design of training courses to prepare students readiness for employment.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3206129.3239426',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Research of Test Questions Classification Based on Hybrid Frame Mixing Semantic Comprehension and Machine Learning',\n",
       "  'authors': \"['Rihong Wang', 'Xingmei Cui', 'Chenglong Wang']\",\n",
       "  'date': 'December 2017',\n",
       "  'source': \"ICRAI '17: Proceedings of the 3rd International Conference on Robotics and Artificial Intelligence\",\n",
       "  'abstract': 'Text classification primarily from learning these two classifications based on semantic understanding and based on supervised machine to consider. Questions also consist of text, so the questions to achieve automatic classification are the classification text, classification questions help to improve the accuracy of automatic test paper to facilitate question bank management. This paper presented a hybrid model which mixing improved Semantic Comprehension and Machine Learning, and introduced as a word frequency correction index, the dispersion degree and positive and negative correlation coefficient to improve mutual information selection algorithm. Finally, it designed a construction testing training system questions classification module based on the framework, and applied to question classification test. The experiments show that the hybrid framework model improves the efficiency of automatic classification of questions with better classification accuracy.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3175603.3175608',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Taming Pretrained Transformers for Extreme Multi-label Text Classification',\n",
       "  'authors': \"['Wei-Cheng Chang', 'Hsiang-Fu Yu', 'Kai Zhong', 'Yiming Yang', 'Inderjit S. Dhillon']\",\n",
       "  'date': 'August 2020',\n",
       "  'source': \"KDD '20: Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining\",\n",
       "  'abstract': 'We consider the extreme multi-label text classification (XMC) problem: given an input text, return the most relevant labels from a large label collection. For example, the input text could be a product description on Amazon.com and the labels could be product categories. XMC is an important yet challenging problem in the NLP community. Recently, deep pretrained transformer models have achieved state-of-the-art performance on many NLP tasks including sentence classification, albeit with small label sets. However, naively applying deep transformer models to the XMC problem leads to sub-optimal performance due to the large output space and the label sparsity issue. In this paper, we propose X-Transformer, the first scalable approach to fine-tuning deep transformer models for the XMC problem. The proposed method achieves new state-of-the-art results on four XMC benchmark datasets. In particular, on a Wiki dataset with around 0.5 million labels, the prec@1 of X-Transformer is 77.28%, a substantial improvement over state-of-the-art XMC approaches Parabel (linear) and AttentionXML (neural), which achieve 68.70% and 76.95% precision@1, respectively. We further apply X-Transformer to a product2query dataset from Amazon and gained 10.7% relative improvement on prec@1 over Parabel.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3394486.3403368',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Research of Test Questions Classification Based on Hybrid Frame Mixing Semantic Comprehension and Machine Learning',\n",
       "  'authors': \"['Rihong Wang', 'Xingmei Cui', 'Chenglong Wang']\",\n",
       "  'date': 'December 2017',\n",
       "  'source': \"ICRAI '17: Proceedings of the 3rd International Conference on Robotics and Artificial Intelligence\",\n",
       "  'abstract': 'Text classification primarily from learning these two classifications based on semantic understanding and based on supervised machine to consider. Questions also consist of text, so the questions to achieve automatic classification are the classification text, classification questions help to improve the accuracy of automatic test paper to facilitate question bank management. This paper presented a hybrid model which mixing improved Semantic Comprehension and Machine Learning, and introduced as a word frequency correction index, the dispersion degree and positive and negative correlation coefficient to improve mutual information selection algorithm. Finally, it designed a construction testing training system questions classification module based on the framework, and applied to question classification test. The experiments show that the hybrid framework model improves the efficiency of automatic classification of questions with better classification accuracy.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3175603.3175608',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Big Data Framework for Scalable and Efficient Biomedical Literature Mining in the Cloud',\n",
       "  'authors': \"['Zhengru Shen', 'Xi Wang', 'Marco Spruit']\",\n",
       "  'date': 'June 2019',\n",
       "  'source': \"NLPIR '19: Proceedings of the 2019 3rd International Conference on Natural Language Processing and Information Retrieval\",\n",
       "  'abstract': 'The massive size of available biomedical literature requires researchers to utilize novel big data technologies in data storage and analysis. Among them is cloud computing which has become the most popular solution for big data applications in industry. However, many bioinformaticians still rely on expensive and inefficient in-house infrastructure to discover knowledge from biomedical literature. Although some cloud-based solutions were constructed recently, they failed to sufficiently address a few key issues including scalability, flexibility, and reusability. Moreover, no study has taken computational cost into consideration. To fill the gap, we proposed a cloud-based big data framework that enables researchers to perform reproducible and scalable large-scale biomedical literature mining in an efficient and cost-effective way. Additionally, a cloud agnostic platform was constructed and then evaluated on two open access corpora with millions of full-text biomedical articles. The results indicate that our framework supports scalable and efficient large-scale biomedical literature mining.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3342827.3342843',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Load Data Mining Based on Deep Learning Method',\n",
       "  'authors': \"['Ping Zhang', 'Hui Cheng', 'Bo Zou', 'Pan Dai', 'Chengjin Ye']\",\n",
       "  'date': 'October 2019',\n",
       "  'source': \"CSAE '19: Proceedings of the 3rd International Conference on Computer Science and Application Engineering\",\n",
       "  'abstract': 'In smart grids and electricity markets, the number of data generated by smart meters increase rapidly. To reduce the communication and storage overhead, data mining algorithms for load must be able to deal with massive and long data with an efficient data compression. On the other hand, load service entities want loads to be clustered in a finer manner for pricing mechanism, demand response or other applications. Thus, some local shapes or high-order features of load profiles should be concerned for a high-accuracy clustering. In this paper, a load data compression and classification method based on deep learning is proposed. It has the following two features: (1) Load data are compressed with Auto-Encoders nonlinearly. It achieves a higher compression ratio and a lower loss than conventional linear methods. (2) Load features are extracted in a layer- wise way and then classified using a probabilistic soft-max regression. Compared with some existing classifiers, the identification accuracy improves a lot. Case studies demonstrates the proposed deep learning method is a feasible tool to handle big data and dig hidden value of load on demand side.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3331453.3361279',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Abstract Mining',\n",
       "  'authors': \"['Ellie Small', 'Javier Cabrera', 'John B. Kostis']\",\n",
       "  'date': 'September 2020',\n",
       "  'source': \"BCB '20: Proceedings of the 11th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics\",\n",
       "  'abstract': 'IMPORTANCE: The marked explosion and fragmentation of bibliographic databases that include large parts pertaining to medical subspecialties has created the opportunity to identify new areas of research using the citations at the interface of subspecialty information. Bibliographic databases such as PubMed are useful to researchers when they wish to identify specific citations of their interest. However, they are not useful because of their size for the purpose of identifying new areas of research. OBJECTIVE: To present a method and two computer applications that identify areas for new research by finding abstracts at the interface between subspecialty parts of PubMed. DESIGN: Here we present a new method and computer applications that aim to ameliorate the problem by examining all abstracts that fulfill the general search terms from PubMed. Using text-mining algorithms of the abstracts to extract all non-trivial words, the researcher can repeatedly cluster the publications by commonality of the words in the abstracts to find unusual or unexpected combinations of words that may lead to new research. When single words are not descriptive enough to identify unique and unexpected ideas for potential new research, we allow the extraction of principal phrases from those abstracts instead. Here we define a principal phrase as a phrase that is common by itself, i.e. not common only as part of another common phrase, does not cross punctuation marks, and is informative (e.g. \"and this disease\" is not an informative phrase). FINDINGS: We present four examples of identifying new research areas by examining PubMed outcomes after searches for \"takotsubo\", \"embolic stroke\" excluding \"atrial fibrillation\", \"impedance mismatch\", and \"aortic and stenosis\". New areas of research were identified including comparisons of the clinical picture and pathophysiology of Takotsubo with scorpion envenomation, and the importance of impedance mismatch in pulmonary and renal circulation. CONCLUSION AND RELEVANCE: In conclusion, we have developed a method and two computer applications to mine words and/or principal phrases from the abstracts retrieved from PubMed or other databases to identify new ideas for research.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3388440.3412476',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Redescription Model Mining',\n",
       "  'authors': \"['Felix I. Stamm', 'Martin Becker', 'Markus Strohmaier', 'Florian Lemmerich']\",\n",
       "  'date': 'August 2021',\n",
       "  'source': \"KDD '21: Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining\",\n",
       "  'abstract': 'This paper introduces Redescription Model Mining, a novel approach to identify interpretable patterns across two datasets that share only a subset of attributes and have no common instances. In particular, Redescription Model Mining aims to find pairs of describable data subsets -- one for each dataset -- that induce similar exceptional models with respect to a prespecified model class. To achieve this, we combine two previously separate research areas: Exceptional Model Mining and Redescription Mining. For this new problem setting, we develop interestingness measures to select promising patterns, propose efficient algorithms, and demonstrate their potential on synthetic and real-world data. Uncovered patterns can hint at common underlying phenomena that manifest themselves across datasets, enabling the discovery of possible associations between (combinations of) attributes that do not appear in the same dataset.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3447548.3467366',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Comparison on Feature Selection Methods for Text Classification',\n",
       "  'authors': \"['Wenkai Liu', 'Jiongen Xiao', 'Ming Hong']\",\n",
       "  'date': 'January 2020',\n",
       "  'source': 'ICMSS 2020: Proceedings of the 2020 4th International Conference on Management Engineering, Software Engineering and Service Sciences',\n",
       "  'abstract': 'The high-dimensional text data always contains a large quantity of noisy terms which bring negative effects on the performance of text classification. Feature selection is the common solution for dimension reduction in text classification. The choices of feature selection methods for text classification have significant impacts on classification accuracy. According to our literature review, few recent studies of feature selection focus on performance comparisons on feature selection methods. To fill this gap, this paper conducts discussions to compare performances of typical feature selection methods which are commonly involved in previous studies for text classification. Firstly, we introduce and discuss a series of typical feature selection methods in previous studies for text classification in details. Secondly, we conduct comparison experiments on four benchmark datasets to compare the effectiveness of twenty typical feature selection methods in text classification. Finally, we give conclusions on performance of the typical feature selection methods. The result of this paper gives a guideline for selecting appropriate feature selection methods for text classification academic analysis or real-world text classification applications.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3380625.3380677',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A Simulation-driven Methodology for IoT Data Mining Based on Edge Computing',\n",
       "  'authors': \"['Claudio Savaglio', 'Giancarlo Fortino']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Internet Technology',\n",
       "  'abstract': 'With the ever-increasing diffusion of smart devices and Internet of Things (IoT) applications, a completely new set of challenges have been added to the Data Mining domain. Edge Mining and Cloud Mining refer to Data Mining tasks aimed at IoT scenarios and performed according to, respectively, Cloud or Edge computing principles. Given the orthogonality and interdependence among the Data Mining task goals (e.g., accuracy, support, precision), the requirements of IoT applications (mainly bandwidth, energy saving, responsiveness, privacy preserving, and security) and the features of Edge/Cloud deployments (de-centralization, reliability, and ease of management), we propose EdgeMiningSim, a simulation-driven methodology inspired by software engineering principles for enabling IoT Data Mining. Such a methodology drives the domain experts in disclosing actionable knowledge, namely descriptive or predictive models for taking effective actions in the constrained and dynamic IoT scenario. A Smart Monitoring application is instantiated as a case study, aiming to exemplify the EdgeMiningSim approach and to show its benefits in effectively facing all those multifaceted aspects that simultaneously impact on IoT Data Mining.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3402444',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Correlation Networks for Extreme Multi-label Text Classification',\n",
       "  'authors': \"['Guangxu Xun', 'Kishlay Jha', 'Jianhui Sun', 'Aidong Zhang']\",\n",
       "  'date': 'August 2020',\n",
       "  'source': \"KDD '20: Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining\",\n",
       "  'abstract': 'This paper develops the Correlation Networks (CorNet) architecture for the extreme multi-label text classification (XMTC) task, where the objective is to tag an input text sequence with the most relevant subset of labels from an extremely large label set. XMTC can be found in many real-world applications, such as document tagging and product annotation. Recently, deep learning models have achieved outstanding performances in XMTC tasks. However, these deep XMTC models ignore the useful correlation information among different labels. CorNet addresses this limitation by adding an extra CorNet module at the prediction layer of a deep model, which is able to learn label correlations, enhance raw label predictions with correlation knowledge and output augmented label predictions. We show that CorNet can be easily integrated with deep XMTC models and generalize effectively across different datasets. We further demonstrate that CorNet can bring significant improvements over the existing deep XMTC models in terms of both performance and convergence rate. The models and datasets are available at: https://github.com/XunGuangxu/CorNet.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3394486.3403151',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Machine learning based Android malware classification',\n",
       "  'authors': \"['Yena Lee', 'Yongmin Kim', 'Seungyeon Lee', 'Junyoung Heo', 'Jiman Hong']\",\n",
       "  'date': 'September 2019',\n",
       "  'source': \"RACS '19: Proceedings of the Conference on Research in Adaptive and Convergent Systems\",\n",
       "  'abstract': 'As growing Android smart-phones, malware threatens smart-phones is also increasing. There are many types of Android malwares. To detect these Android malwares effectively, first, we need to classify Android malwares. In this paper, we build a database storing Android malwares and their types and characteristics. With the database, we propose a machine learning model to classify the malwares. To evaluate the model, we conducted k-fold cross validation. Through the evaluation, our model showed over 85% accuracy in the malware classification.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3338840.3355693',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Analysis and Research of Food Safety Risk Assessment Based on Data Mining',\n",
       "  'authors': \"['LiJuan Li', 'YuanYing Shen', 'Zhiqiong Yuan', 'Jie Li']\",\n",
       "  'date': 'August 2018',\n",
       "  'source': 'BDET 2018: Proceedings of the 2018 International Conference on Big Data Engineering and Technology',\n",
       "  'abstract': 'With the improvement of living standards, people are increasingly demanding food, food safety problems emerge in an endless stream of emerge in an endless stream of food safety, attracted attention, there was an urgent need to establish a risk assessment system of food safety supervision, the scientific and reasonable decision of export food provides effective theoretical support. This paper proposes a mining food risk assessment and early warning method based on the data, establish scientific risk evaluation system, the risk assessment results, and the evaluation result is multilayer multidimensional association rule mining, early warning information of food inspection project, provide a powerful guarantee for food safety..',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3297730.3297748',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Measuring and Mitigating Unintended Bias in Text Classification',\n",
       "  'authors': \"['Lucas Dixon', 'John Li', 'Jeffrey Sorensen', 'Nithum Thain', 'Lucy Vasserman']\",\n",
       "  'date': 'December 2018',\n",
       "  'source': \"AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society\",\n",
       "  'abstract': 'We introduce and illustrate a new approach to measuring and mitigating unintended bias in machine learning models. Our definition of unintended bias is parameterized by a test set and a subset of input features. We illustrate how this can be used to evaluate text classifiers using a synthetic test set and a public corpus of comments annotated for toxicity from Wikipedia Talk pages. We also demonstrate how imbalances in training data can lead to unintended bias in the resulting models, and therefore potentially unfair applications. We use a set of common demographic identity terms as the subset of input features on which we measure bias. This technique permits analysis in the common scenario where demographic information on authors and readers is unavailable, so that bias mitigation must focus on the content of the text itself. The mitigation method we introduce is an unsupervised approach based on balancing the training dataset. We demonstrate that this approach reduces the unintended bias without compromising overall model quality.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3278721.3278729',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Text classification for predicting multi-level product categories',\n",
       "  'authors': \"['Hadi Jahanshahi', 'Ozan Ozyegen', 'Mucahit Cevik', 'Beste Bulut', 'Deniz Yigit', 'Fahrettin F. Gonen', 'Ayşe Başar']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"CASCON '21: Proceedings of the 31st Annual International Conference on Computer Science and Software Engineering\",\n",
       "  'abstract': 'In an online shopping platform, a detailed classification of the products facilitates user navigation. It also helps online retailers keep track of the price fluctuations in a certain industry or special discounts on a specific product category. Moreover, an automated classification system may help to pinpoint incorrect or subjective categories suggested by an operator. In this study, we focus on product title classification of the grocery products. We perform a comprehensive comparison of six different text classification models to establish a strong baseline for this task, which involves testing both traditional and recent machine learning methods. In our experiments, we investigate the generalizability of the trained models to the products of other online retailers, the dynamic masking of infeasible subcategories for pretrained language models, and the benefits of incorporating product titles in multiple languages. Our numerical results indicate that dynamic masking of subcategories is effective in improving prediction accuracy. In addition, we observe that using bilingual product titles is generally beneficial, and neural network-based models perform significantly better than SVM and XGBoost models. Lastly, we investigate the reasons for the misclassified products and propose future research directions to further enhance the prediction models.',\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3507788.3507794',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Automated Machine Learning with Genetic Programming on Real Dataset of Tax Avoidance Classification Problem',\n",
       "  'authors': \"['Suraya Masrom', 'Rahayu Abdul Rahman', 'Norhayati Baharun', 'Abdullah Sani Abd Rahman']\",\n",
       "  'date': 'February 2020',\n",
       "  'source': 'ICEIT 2020: Proceedings of the 2020 9th International Conference on Educational and Information Technology',\n",
       "  'abstract': 'Dealing with real application datasets often derive a stumbling block for machine learning algorithms to produce good results in solving either prediction or classification problems. Imbalance dataset is the major reason for this problem associated with missing values, small dimension of data size and very skewed data distribution. This paper demonstrates an empirical study that used Automated Machine Learning (AML) based on Genetic Programming (GP) named as AML TPOT. This is a very recent AML developed as an open source Python library and reported as a promising model by a few of researchers who have tested the algorithm. Nevertheless, most of the works on the AML TPOT were conducted on a set of common or benchmark datasets for machine learning testing. In this paper, the focus is on real and deviant dataset, which were collected according to the tax avoidance of the Government-Link Company in Malaysia. Comparison of the AML performances that tested on the dataset with different GP parameters setting is provided. Thus, this paper provides a fundamental knowledge on the experimental design and finding that will be useful for the AML based GP future improvement.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3383923.3383942',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Radar Plot Classification Based on Machine Learning',\n",
       "  'authors': \"['Zhengcheng Liu', 'Yongmei Qi', 'Xiao Dai']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"EITCE '21: Proceedings of the 2021 5th International Conference on Electronic Information Technology and Computer Engineering\",\n",
       "  'abstract': 'Clutter is the inherent environment of radar signal detection and processing. On the one hand, too many clutter points will start a false track, on the other hand, clutter plots will be incorrectly associated with the track, resulting in track errors. Therefore, how to further distinguish target plots and clutter plots after target detection is an important and difficult problem in radar data processing. Aiming at the clutter data mixed in the plot data output by signal processing, this paper extracts the multi-dimensional feature parameters of radar plot on the radar measured data set, classifies the radar real target and false target by using the traditional support vector machine, fully connected neural network and convolution neural network respectively, and compares the effects of three different classification methods.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3501409.3501507',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Data Reduction Workflow Patterns Mining and Optimization based on Ontology',\n",
       "  'authors': \"['Wen-ning Hao', 'Jun-yue Chen', 'Suo-juan Zhang', 'Zi-xuan Zhang', 'Gang Chen', 'Rui-zhi Kang']\",\n",
       "  'date': 'December 2018',\n",
       "  'source': \"ACAI '18: Proceedings of the 2018 International Conference on Algorithms, Computing and Artificial Intelligence\",\n",
       "  'abstract': 'Focusing on massive high-dimensional data reduction, this paper established the ontology model of data reduction, to improve the related theoretical methods of data reduction, and to propose an ontology-based data reduction system architecture. Data reduction workflow model, workflow pattern mining, and workflow optimization and data reduction experiment design based on knowledge base are studied to achieve the accumulation, sharing and reuse of data reduction knowledge and enhance the intelligence level of data reduction process as well as the credibility of reduction results. A mechanism of meta-reduction system framework and its technology is put forward so as to improve the availability, flexibility and applicability of data reduction system.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3302425.3302462',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Social Relationship Mining Based on Student Data',\n",
       "  'authors': \"['Wang Xinzheng', 'Guo Bing', 'Shen Yan', 'Huang FeiHu']\",\n",
       "  'date': 'December 2020',\n",
       "  'source': \"EBIMCS '20: Proceedings of the 2020 3rd International Conference on E-Business, Information Management and Computer Science\",\n",
       "  'abstract': \"In recent years, the campus social network mining has been widely concerned by scholars because it is closely related to the physical and mental health development of college students and the university management work. Compared with the typical social network mining tasks, the challenges of campus social network mining mainly lie in the difficulty of data acquisition and unlabeled data. Based on the phenomenon of homogeneity between friends, this paper proposed a social relationship mining method. Firstly, this paper carried out data dimension reduction operation for each student's daily consumption data, and then innovatively carried out the convolution operation for the consumption behavior data after dimension reduction, and finally obtained the characteristics of students' consumption behavior. The Multiple dimensional scaling algorithm is adopted for data dimension reduction, and the convolutional network is an improved Lenet-5 network. Then, the personalized ranking model is developed base on the phenomenon that the distance between friends is less than the distance between non-friends. To solve the problem of unlabeled data, we mined the friends' relationship from the student's library access control system swiping card records, and then taken the mined friends' relationship as the real friends' relationship to train the model. From the experimental results, the social network mining method proposed in this paper performs well.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3453187.3453397',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Multi-label Text Classification with Label Correction under Noise',\n",
       "  'authors': \"['Tingting Yu', 'Tao Li', 'Xiaomeng Wang']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"ICCPR '21: Proceedings of the 2021 10th International Conference on Computing and Pattern Recognition\",\n",
       "  'abstract': 'Multi-label text classification (MLTC) is a fundamental but difficult problem in text mining, the goal of MLTC is to assign a set of most relevant labels for the given document. While existing supervised training of deep learning models for MLTC usually requires a large number of noise-free labeled samples, which is quite expensive and time-consuming or even impractical in the real-world as label annotations are inevitable error-prone. To handle such a situation, we introduce learning multi-label text classification with noisy labels, and propose an end-to-end method called Multi-label Text Classification with Label Correction under Noise (LCN). LCN contains two modules: a label correction module and a classification module. In the label correction module, a group of prototypes for each class is learnt with the help of label semantic and feature information. These prototypes are then used to calculate the similarity between the extracted deep features to correct the labels of each training sample. In the classification module, the classifier combines the original noisy labels and corrected labels of each sample as supervised information to guide the training procedure. The two modules are combined in a unified framework and trained in an alternative manner. Extensive experimental results on two multi-label text benchmark datasets validate the effectiveness of LCN and show its advantages to the state-of-art methods.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3497623.3497650',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Text Mining Based Adaptive Case Management Automation in the Field of Forensic Medicine',\n",
       "  'authors': \"['Borislav Banchev']\",\n",
       "  'date': 'June 2017',\n",
       "  'source': \"CompSysTech '17: Proceedings of the 18th International Conference on Computer Systems and Technologies\",\n",
       "  'abstract': 'In the paper is introduced an algorithm for automated processing of documents, managed in forensic medicine offices. The algorithm is based on series of steps that are executed in a sequence in which many aspects are controlled by the information extracted from the document itself. The resulted software solution is based on adaptive case management. Using text mining techniques and meta modelling, the incoming information is transformed into knowledge that is injected into the adaptive business processes. The proposed mechanism would reduce the total turnaround time, the needed manual work at each step and the confusion for the end user.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3134302.3134344',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Evaluation Method of Equipment Combat Effectiveness Based On Big Data Mining',\n",
       "  'authors': \"['Zhou Liyao', 'Liu Xiaofang', 'Hu Chunyu']\",\n",
       "  'date': 'January 2020',\n",
       "  'source': \"ICCDE '20: Proceedings of 2020 6th International Conference on Computing and Data Engineering\",\n",
       "  'abstract': 'In the evaluation of equipment combat effectiveness, it is necessary to comprehensively analyze the data of outfield test and infield test, including a variety of audio-visual, image and other combat test data. These data can be classified, extracted, stored and managed by building data model through big data mining technology. The evaluation method of equipment combat effectiveness based on big data mining is based on massive data, through machine learning, statistical analysis, neural network, database and other methods to analyze and process the data, mining the correlation between test data, evaluation index and evaluation conclusion, and extracting useful information and finding new knowledge from it to realize the evaluation of the combat effectiveness of the tested system.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3379247.3379282',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Analysis of Student Academic Performance and Social Media Activities by Using Data Mining Approach',\n",
       "  'authors': \"['Enda Esyudha Pratama', 'Eva Faja Ripanti']\",\n",
       "  'date': 'February 2020',\n",
       "  'source': 'ICEBA 2020: Proceedings of the 2020 The 6th International Conference on E-Business and Applications',\n",
       "  'abstract': 'This study aims to analysis the relationship of student academic performance to the activity of using social media. The method used in this research is a data mining approach to analysis its connectedness. Data mining techniques used are association and classification. As for the needs of the data used comes from student academic data sourced fromAcademic Information Systemand social media activity data comes from Instagram using Application Programming Interface (API) for get data automatically. Data requirements for academic achievement, i.e. grade-point averagre (GPA), duration of study, and faculty. As for identifying the data of social media activity, i.e.number of post (feed), number of following & follower, date & time post, and caption.The results of the analysis in this study indicate a relationship between academic achievement and the activity of using social media.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3387263.3387279',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Binary Classification Model Based on Machine Learning Algorithm for the Short-Circuit Detection in Power System',\n",
       "  'authors': \"['Qiwei Lu', 'Jinpei Cheng', 'Dianlin Guo', 'Mengmeng Su', 'Xuewei Wu', 'Tao Ru']\",\n",
       "  'date': 'December 2019',\n",
       "  'source': \"ACAI '19: Proceedings of the 2019 2nd International Conference on Algorithms, Computing and Artificial Intelligence\",\n",
       "  'abstract': 'Short circuit faults usually occur in the damaged insulation lines or line connections, which will cause serious accidents such as fires and explosions. As the power supply distance increases, accuracy of short-circuit fault detection is insufficient and the process is tedious with the traditional analysis method. In order to solve the problems above, the short-circuit fault detection is classified into the two classification problems while the machine learning method is used. The data of the normal state and short circuit fault state are obtained by the short-circuit simulation experiment. Extract four features from time domain, including the average current and so on. By training support vector machine (SVM) using the different combinations of extraction features above, the model is obtained. The accuracy of classification of the test data set by the model is high. The results show that the short-circuit fault detection method based on machine learning is more accurate and robust than traditional analysis methods.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3377713.3377753',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'The Definition, Current Situation and Development Trend of Latent Aspect Rating Analysis in Text Mining',\n",
       "  'authors': \"['Shaohua Sun', 'Kuisheng Wang', 'Tiantian Zhang']\",\n",
       "  'date': 'June 2018',\n",
       "  'source': \"ICCPR '18: Proceedings of the 2018 International Conference on Computing and Pattern Recognition\",\n",
       "  'abstract': \"The field of latent aspect rating analysis has been developed in the last few years. Firstly, we introduce the background and definition of latent aspect rating analysis in text mining. Secondly, we have collected literature on the latent aspect rating analysis of the research in recent years and summarized the development status of this field. Finally, the future development trend and expectation of this field are put forward according to relevant literature. Furthermore, the main contribution of this paper is to describe the field and analyze its development trend according to the author's research work.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3232829.3232833',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Highly Efficient Mining of Overlapping Clusters in Signed Weighted Networks',\n",
       "  'authors': \"['Tuan-Anh Hoang', 'Ee-Peng Lim']\",\n",
       "  'date': 'November 2017',\n",
       "  'source': \"CIKM '17: Proceedings of the 2017 ACM on Conference on Information and Knowledge Management\",\n",
       "  'abstract': 'In many practical contexts, networks are weighted as their links are assigned numerical weights representing relationship strengths or intensities of inter-node interaction. Moreover, the links\\' weight can be positive or negative, depending on the relationship or interaction between the connected nodes. The existing methods for network clustering however are not ideal for handling very large signed weighted networks. In this paper, we present a novel method called LPOCSIN (short for \"Linear Programming based Overlapping Clustering on Signed Weighted Networks\") for efficient mining of overlapping clusters in signed weighted networks. Different from existing methods that rely on computationally expensive cluster cohesiveness measures, LPOCSIN utilizes a simple yet effective one. Using this measure, we transform the cluster assignment problem into a series of alternating linear programs, and further propose a highly efficient procedure for solving those alternating problems. We evaluate LPOCSIN and other state-of-the-art methods by extensive experiments covering a wide range of synthetic and real networks. The experiments show that LPOCSIN significantly outperforms the other methods in recovering ground-truth clusters while being an order of magnitude faster than the most efficient state-of-the-art method.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3132847.3133004',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Machine Learning Predictive Model for the Passive Transparency at the Brazilian Ministry of Mines and Energy',\n",
       "  'authors': \"['Ingrid Palma', 'Marcelo Ladeira', 'Ana Carla Bittencourt Reis']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"DG.O'21: DG.O2021: The 22nd Annual International Conference on Digital Government Research\",\n",
       "  'abstract': 'This paper presents a case study based on the CRISP-DM Model and the use of Text Mining tools and techniques to automate the Passive Transparency process at the Brazilian Ministry of Mines and Energy. Thus, a Machine Learning Model is proposed to predict the class of the technical unit responsible for the data/information requested by citizens. Through the application of the algorithm LDA and TF-IDF it was possible to map the topics of the most relevant subjects for society. The stability of the model was tested from the comparative analysis between 5 known classification algorithms (Random Forest, Multinomial NB, Linear SVC, Logistic Regression, XGBoost and Gradient Boosting). XGBoost presented better performance and precision in multiclass learning outcomes.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3463677.3463715',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'MotifClass: Weakly Supervised Text Classification with Higher-order Metadata Information',\n",
       "  'authors': \"['Yu Zhang', 'Shweta Garg', 'Yu Meng', 'Xiusi Chen', 'Jiawei Han']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"WSDM '22: Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining\",\n",
       "  'abstract': 'We study the problem of weakly supervised text classification, which aims to classify text documents into a set of pre-defined categories with category surface names only and without any annotated training document provided. Most existing classifiers leverage textual information in each document. However, in many domains, documents are accompanied by various types of metadata (e.g., authors, venue, and year of a research paper). These metadata and their combinations may serve as strong category indicators in addition to textual contents. In this paper, we explore the potential of using metadata to help weakly supervised text classification. To be specific, we model the relationships between documents and metadata via a heterogeneous information network. To effectively capture higher-order structures in the network, we use motifs to describe metadata combinations. We propose a novel framework, named MotifClass , which (1) selects category-indicative motif instances, (2) retrieves and generates pseudo-labeled training samples based on category names and indicative motif instances, and (3) trains a text classifier using the pseudo training data. Extensive experiments on real-world datasets demonstrate the superior performance of MotifClass to existing weakly supervised text classification approaches. Further analysis shows the benefit of considering higher-order metadata information in our framework.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3488560.3498384',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Knowledge Distillation In Medical Data Mining: A Survey',\n",
       "  'authors': \"['Hefeng Meng', 'Zhiqiang Lin', 'Fan Yang', 'Yonghui Xu', 'Lizhen Cui']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"ICCSE '21: 5th International Conference on Crowd Science and Engineering\",\n",
       "  'abstract': 'In recent years, there have always been many problems in the medical field, such as a shortage of professionals and a shortage of medical resources. With the application of machine learning in the medical field, these problems have been alleviated to a certain extent, but these machine learning methods also have shortcomings, such as models are often too large to be deployed on lightweight equipment, and medical data sets are difficult to share, Many researchers have put forward many methods, and knowledge distillation is one of them. As a model compression and acceleration technology, knowledge distillation has been widely used in the medical field. The research of many researchers also shows that the use of knowledge distillation can effectively compress huge and complex models and improve the performance of models. Many studies show that the use of knowledge distillation can effectively solve many problems existing in models in the medical field, Aiming at the various applications of knowledge distillation in the medical field, this paper makes a comprehensive review from the perspectives of knowledge distillation, the problems that knowledge distillation can solve in the medical field and the practical application of knowledge distillation.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503181.3503211',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Word embedding and cognitive linguistic models in text classification tasks',\n",
       "  'authors': \"['Anna Surkova', 'Sergey Skorynin', 'Igor Chernobaev']\",\n",
       "  'date': 'October 2019',\n",
       "  'source': \"CSIS'2019: Proceedings of the XI International Scientific Conference Communicative Strategies of the Information Society\",\n",
       "  'abstract': 'The paper considers two linguistic models, analyzed the possibility of their use for the text data classification as well as their associations in the integrated texts presentation. A cognitive approach for the text classification issues is presented. An algorithm to identify the words basic level using WordNet is considered. A model for text classification based on the pre-trained word embeddings is presented. The model consists of three layers: embedding layer Long-Short Term Memory (LSTM) layer, and softmax layer. The model was trained and evaluated on the 20 Newsgroups dataset. The classification quality was assessed by F- measure, precision and recall. The obtained results analysis is carried out. Both described models show good results, low scores for some texts are explained. The advantages and limitations of the linguistic models are shown. In future works the authors are going to combine proposed models and modify them. Thus, for model based on word embedding there are pretty vast opportunities for extension: from experimenting with different word embeddings and various distance metrics to more complicated architecture of layers and even promising state of the art artificial neural network models, activation functions and their modifications. In addition, there is research area of proper ensemble strategy selection.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3373722.3373778',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': \"Combining traditional machine learning and anomaly detection for several imbalanced Android malware dataset's classification\",\n",
       "  'authors': \"['Yiwei Gan', 'Qian Han', 'Yumeng Gao']\",\n",
       "  'date': 'March 2022',\n",
       "  'source': \"ICMLT '22: Proceedings of the 2022 7th International Conference on Machine Learning Technologies\",\n",
       "  'abstract': \"As the number of mobile devices has exploded in recent years, so has the amount of advanced Android malware. Among the popular Android malware on the market today, click fraud malware, adware, banking Trojans, spyware, etc. are usually disguised and hidden in the heap of good Android applications. These advanced malwares lurk in the third-party application market trusted by users, and potentially endanger the security of the user's smart device causing privacy or economic loss. Therefore, this paper leverages and combines traditional machine learning and anomaly detection methods to detect specific classes of Android malware in three highly imbalanced Android datasets (entertainment + social app vs. click fraud malware + adware; financial services app vs. banking trojan; communication app vs. spyware). The experiment results show that our proposed combined methods have great performance on the three sub-datasets, achieving the average f1-score over 0.98 on three imbalanced datasets, which performs better than the traditional machine learning algorithm used alone. In addition, we use the combined methods to analyze the correlation between the top features of the dataset, and provide interpretable insights for other researchers focusing on Android malware classification in the coming future.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3529399.3529412',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'An efficient approach for dimensionality reduction and classification of high dimensional text documents',\n",
       "  'authors': \"['Kotte Vinay Kumar', 'R. Srinivasan', 'E. B. Singh']\",\n",
       "  'date': 'October 2018',\n",
       "  'source': \"DATA '18: Proceedings of the First International Conference on Data Science, E-learning and Information Systems\",\n",
       "  'abstract': 'Feature representation and dimensionality reduction techniques are two important tasks in text clustering and classification. In this paper, an approach for feature representation and dimensionality reduction of text documents is described. The feature representation and dimensionality reduction approaches that are introduced retain the original distribution of features. Output of feature representation is a hard representation matrix. The hard matrix is used to obtain the low dimensionality document matrix. The input for clustering is the low dimensional matrix. The working of proposed approach is explained using a case study that supports the importance of the approach and advantage of dimensionality reduction. Results prove that the proposed approach has better dimensionality reduction achieved and is also better suited for classification.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3279996.3281364',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Research on Flow Classification Model Based on Similarity and Machine Learning Algorithm',\n",
       "  'authors': \"['Meigen Huang', 'Lingling Wu', 'Xue Yuan']\",\n",
       "  'date': 'February 2021',\n",
       "  'source': \"ICMLC '21: Proceedings of the 2021 13th International Conference on Machine Learning and Computing\",\n",
       "  'abstract': 'In recent years, with the rapid development of the Internet, complex and diverse applications and network traffic have been generated. At the same time, network encryption technologies and various new network traffic have emerged, which affects the efficiency of the original traffic classification technology. In order to improve the efficiency of traffic classification and reduce the classification time, this paper proposes a network traffic classification model (Cosine similarity and decision tree classification model, CSDT) based on cosine similarity and decision tree algorithm to identify and classify traffic. First, the cosine similarity algorithm is used to judge the similarity of adjacent network traffic, and the network traffic with higher similarity is labeled with a known classification and forwarded. For network traffic with low similarity, the decision tree algorithm is used to classify the related feature values. This model utilizes the characteristics of high similarity in adjacent data streams, and uses similarity algorithms to preprocess network traffic to reduce classification time. The Moore data set publicly available in the field of network traffic classification is used for training and testing, and the results are compared with various machine learning algorithms on the Weka platform. The experimental results show that the model has a good classification accuracy, which greatly reduces the classification time and improves the classification efficiency of network traffic is improved.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3457682.3457687',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'An automated new approach in fast text classification (fastText): A case study for Turkish text classification without pre-processing',\n",
       "  'authors': \"['Birol Kuyumcu', 'Cuneyt Aksakalli', 'Selman Delil']\",\n",
       "  'date': 'June 2019',\n",
       "  'source': \"NLPIR '19: Proceedings of the 2019 3rd International Conference on Natural Language Processing and Information Retrieval\",\n",
       "  'abstract': 'Any Text Classification (TC) problem need pre-processing steps which may affect the classification accuracy. Especially pre-processing steps need substantial effort particularly in agglutinative languages such as Turkish. In this context, a traditional text categorization problem requires pre-processing steps such as tokenization, stop-word removal, lower-case conversion, stemming and feature dimension reduction. Before classification, one or more of these steps are applied to text and then a classifier is trained to evaluate the corresponding precision. Deep neural network classifiers combined with word embedding is one of the solutions to eliminate the pre-processing prerequisites. Another novel approach is fastText word embedding based classifier which was developed by Facebook. In this study, we evaluate a fastText classifier on TTC-3600 Turkish dataset without using any pre-processing steps and present the performance of the algorithm.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3342827.3342828',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Research on Media Attention to Quality of Imported Consumer Goods Based on Text Mining – Taking Garments as an Example',\n",
       "  'authors': \"['Yuan Chen', 'Jiajie Wei']\",\n",
       "  'date': 'September 2021',\n",
       "  'source': \"ICBDT '21: Proceedings of the 4th International Conference on Big Data Technologies\",\n",
       "  'abstract': \"With the improvement of residents' consumption level, consumers' demand for all kinds of imported consumer goods has gradually increased, but its quality problem can not be ignored. The quality governance of consumer goods is particularly important, which is a part of social co-governance. It emphasizes the joint participation and responsibility to quality of multiple-subject, such as media, government, enterprises, social groups and the public. In the Internet age, social media is playing an increasingly important role in the reporting of various major event relative to commodity quality. Therefore, the research on the quality attention of imported consumer goods based on text mining to online news can be regarded as a valuable exploration. In this paper, using 3 identified standards —— news authority, news crawling volume and website traffic, we select 2 news platforms, i.e., China Quality News Network as well as Global Textile Network which have the most synthesis advantages to be qualified publishing quality news of imported consumer goods. Then, text mining on online news related to imported garments quality has been conducted by Python through the following efforts: (1) Garments’ quality dictionaries including quality attribution and quality scoring, has been developed by ourselves. (2) This paper verifies the rationality of the dictionary by using a Radial Basis Function (RBF) in Support Vector Machine (SVM) algorithm. The average accuracy on the training data set of the quality attribution dictionary is as high as 93.49%, showing that the selective attribution indexes in the quality attribution dictionary is rational. (3) From the above-mentioned news platforms, all the news relative to garments’ quality until September 10, 2020, has been crawled. There are 10780 crawled news totally, in which 8534 are effective news selected by removing duplicate and other data preprocessing methods. (4) On the basis of Maslow's Hierarchy of Needs, the 33 quality attributions in the dictionary have been divided into 4 categories: Physiological need attribution, Safety attribution, Functional attribution, Cognitive & aesthetic attribution. Text analysis has been made to the media attention about quality of imported consumer goods from the 4 categories , and time series analysis has also been made. (5) The media quality score of quality attribution has been calculated and analysed.)6) A correlation analysis of the media attention and score of each attribute category has been conducted. The results come to the following conclusions: (1) Different platforms basically share the same concern on the attribution quality of imported garments. The top five quality attributions with high media attention are harmful substances, typical mechanical & physical properties, logo, aesthetic, and comfort. The above 5 quality attributions mainly belong to the safety attribution and cognitive & aesthetic attribution. (2) The higher quality score, the higher media attention, and the worse the quality of imported garments. The media attention on the quality of imported garments experienced four stages: the sustained level stage (2001-2003), the slow growth stage(2003-2012), the rapid growth stage (2012-2014)and sustained pullback stage(2014-2019). (3) Media attention has high correlation to the quality of quality score of imported garments. (4) From 2014 to the present, the attributions quality of imported garments from low to high are Safety attribution, Cognitive & aesthetic attribution, Functional attribution, Physiological need attribution, which lead to the media attention to the attributions from high to low are Safety attribution, Cognitive & aesthetic attribution, Functional attribution, Physiological need attribution. (5) During the period of the rapid growth stage (2012-2014), the totality quality of imported garments declined significantly, give rise to the media attention also increased significantly. (6) After 2014, the totality quality of imported garments showed a trend of gradual improvement, and then the media attention of imported clothing quality was also declining. (7) The quality of safety attribution changes the most, but it improves the fastest. According to the analysis of this paper, media attention has a very close relationship with the quality score of imported garments. The more serious the quality problems, the more media attention would be drawn to them. The high media attention quality attributes need more attention of government to strengthen quality supervision. Finally, this paper puts forward some suggestions on the quality governance of imported consumer goods.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3490322.3490327',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Research on User Comments of Douban Animation Made in China Based on Text Mining Technology',\n",
       "  'authors': \"['Siyu Sun', 'Yingjie Gai', 'Yingying Zhou', 'Aiting Xu']\",\n",
       "  'date': 'December 2019',\n",
       "  'source': \"ICIT '19: Proceedings of the 2019 7th International Conference on Information Technology: IoT and Smart City\",\n",
       "  'abstract': 'With the advent of the \"Internet plus\" era, the online film criticism has springing up rapidly. The success of \"NeZha\" has brought the domestic animation film to an unprecedented high tide and brought a lot of information about the film reviews. Based on Chinese natural language processing technology, this paper takes the Douban movie review of domestic animation series as the research object, uses Python to crawl the movie review data, on the basis of the preprocessing of data cleaning, data normalization, Chinese word segmentation and removal of stop words, etc., carries out machine learning based emotional tendency analysis, and the visual analysis of word cloud and the analysis of sentiment orientation based on machine learning, At last, uses LDA theme model to mine movie reviews in depth. The research shows that: the audience pays more attention to the plot setting, character shaping, picture presentation, production ability, line performance and other aspects of domestic animation series films; Douban users tend to be positive attitude of domestic animation, but there is still a large proportion of negative emotion;the praise and popularity of domestic animated films continue to rise, but the uneven quality of domestic animation films still needs to be improved.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3377170.3377232',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Data Mining of Agricultural Software and Suggestions',\n",
       "  'authors': \"['Zheng-Ming Gao', 'Juan Zhao', 'Yu-Rong Hu']\",\n",
       "  'date': 'September 2020',\n",
       "  'source': \"MLMI '20: Proceedings of the 2020 3rd International Conference on Machine Learning and Machine Intelligence\",\n",
       "  'abstract': 'Electric business, also called E-business, or digital business, might be a solution to the so-called Chinese “three agricultural problems”. In order to find the absence of specific agricultural software, we carry out the data scratching of agricultural software regarding agriculture in Mandarin from four most popular web sites and hand phone Android Apps detailers. Data cloud and frequency analysis were carried out and suggestions were proposed for a purpose to lead the farmers to our modern, intelligent and information society.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3426826.3426841',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Comparison of Text Mining Feature Extraction Methods Using Moderated vs Non-Moderated Blogs: An Autism Perspective',\n",
       "  'authors': \"['Abu Saleh Md Tayeen', 'Saleem Masadeh', 'Abderrahmen Mtibaa', 'Satyajayant Misra', 'Moumita Choudhury']\",\n",
       "  'date': 'November 2019',\n",
       "  'source': 'DPH2019: Proceedings of the 9th International Conference on Digital Public Health',\n",
       "  'abstract': 'Online social media is being widely used by social scientists to study human behavior. Researchers have explored different feature extraction (FE) and classification techniques to perform sentiment analysis, topic identification, etc. Most studies tend to evaluate FE and classification methods using only one particular class of datasets---well-defined with little/no noise or with well-defined noise. For instance, when the datasets under study have different noise characteristics, various FE and/or classification methods may fail to identify a given topic. In this paper, we fill this gap by quantitatively comparing multiple FE methods and classifiers using three different datasets (two moderator-controlled blogs and one single-authored personal blogs) related to Autism Spectrum Disorder (ASD). Our result shows that no particular combination of FE and classifier is the best overall, but choosing the right ones can improve accuracy by over 30%.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3357729.3357740',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Analysis of Techniques for Data Mining',\n",
       "  'authors': \"['Keyi Zhu']\",\n",
       "  'date': 'April 2021',\n",
       "  'source': \"ESCC '21: The 2nd European Symposium on Computer and Communications\",\n",
       "  'abstract': 'Data mining has been a popular branch of computer science in recent years with great impact in different areas. It is a technique to find the inner connection, the unexpected pattern from a large amount of data and to obtain certain broad conclusions from it. Currently, many data mining techniques are developed and used in order to process data from various aspects and to improve the speed and accuracy of data analysis. Considering that data mining techniques combine knowledge from multiple domains, this article will present an overview of temporal techniques for data mining.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3478301.3478308',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A Data Mining Framework for the Analysis of Patient Arrivals into Healthcare Centers',\n",
       "  'authors': \"['Salam Abdallah', 'Mohsin Malik', 'Gurdal Ertek']\",\n",
       "  'date': 'December 2017',\n",
       "  'source': \"ICIT '17: Proceedings of the 2017 International Conference on Information Technology\",\n",
       "  'abstract': 'We present a data mining framework that can be applied for analyzing patient arrivals into healthcare centers. The sequentially applied methods are association mining, text cloud analysis, Pareto analysis, cross-tabular analysis, and regression analysis. We applied our framework using real-world data from a one of the largest public hospitals in the U.A.E., demonstrating its applicability and possible benefits. The dataset used was eventually 110,608 rows in total for the regression models, covering the most utilized 14 hospital units. The dataset is at least 10-fold larger than datasets used in closely-related research. The developed data mining framework can provide the input for a subsequent optimization model, which can be used to optimally assign appointments for patients, based on their arrival patterns.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3176653.3176740',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Accelerating Innovation Through Analogy Mining',\n",
       "  'authors': \"['Tom Hope', 'Joel Chan', 'Aniket Kittur', 'Dafna Shahaf']\",\n",
       "  'date': 'August 2017',\n",
       "  'source': \"KDD '17: Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining\",\n",
       "  'abstract': 'The availability of large idea repositories (e.g., the U.S. patent database) could significantly accelerate innovation and discovery by providing people with inspiration from solutions to analogous problems. However, finding useful analogies in these large, messy, real-world repositories remains a persistent challenge for either human or automated methods. Previous approaches include costly hand-created databases that have high relational structure (e.g., predicate calculus representations) but are very sparse. Simpler machine-learning/information-retrieval similarity metrics can scale to large, natural-language datasets, but struggle to account for structural similarity, which is central to analogy. In this paper we explore the viability and value of learning simpler structural representations, specifically, \"problem schemas\", which specify the purpose of a product and the mechanisms by which it achieves that purpose. Our approach combines crowdsourcing and recurrent neural networks to extract purpose and mechanism vector representations from product descriptions. We demonstrate that these learned vectors allow us to find analogies with higher precision and recall than traditional information-retrieval methods. In an ideation experiment, analogies retrieved by our models significantly increased people\\'s likelihood of generating creative ideas compared to analogies retrieved by traditional methods. Our results suggest a promising approach to enabling computational analogy at scale is to learn and leverage weaker structural representations.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3097983.3098038',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Investigating the impact of satisfaction and relational capital on repurchase behavior using a text mining method',\n",
       "  'authors': \"['Zhepeng Lv', 'Yue Jin', 'Jinghua Huang']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ICCIR '21: Proceedings of the 2021 1st International Conference on Control and Intelligent Robotics\",\n",
       "  'abstract': 'Nowadays, sellers concern about how to retain consumers in online context. The present study moves beyond satisfaction and extends the literature by investigating the moderating effect of relational capital. The online shopping platform studied contains reviews and communication texts that are full of descriptive expressions. Those texts are valuable resources to achieve measurements of satisfaction and relational capital in an objective way. Therefore, a text mining method, specifically, the lexicon-based method is employed to extract those constructs. By analyzing 10, 609 transactions and mining related communication conversations and reviews, this study concluded that the extracted satisfaction measurement has a positive effect on repurchase intention with odds ratio of 2.51, and relational capital exerts a moderating effect, with each dimension of relational capital performing distinguished roles.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3473714.3473817',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Biogeography-based rule mining for classification',\n",
       "  'authors': \"['Effat Farhana', 'Steffen Heber']\",\n",
       "  'date': 'July 2017',\n",
       "  'source': \"GECCO '17: Proceedings of the Genetic and Evolutionary Computation Conference\",\n",
       "  'abstract': 'Rule-based classification is a popular approach for solving real world classification problems. Once suitable rules have been obtained, rule-based classifiers are easy to deploy and explain. In this paper, we describe an approach that uses biogeography-based optimization (BBO) to compute rule sets that maximize predictive accuracy. BBO is an evolutionary algorithm inspired by the migration patterns of species between the islands of an archipelago. In our implementation, each species corresponds to a classification rule, each island is occupied by multiple species and corresponds to a classifier, and the fitness of an island is computed as the predictive classification accuracy of the corresponding classifier. The archipelago evolves via mutation, selection, and migration of species between islands. Successful islands have a decreased immigration rate and an increased emigration rate. In general, such islands tend to resist invasion and to colonize less successful islands. This results in an evolving set of habitats that corresponds to a population of classifiers. We demonstrate the effectiveness of our approach by comparing it to several traditional and evolutionary based state-of-the-art classifiers.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3071178.3071221',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Predict the type of hearing aid of audiology patients using data mining techniques',\n",
       "  'authors': \"['Sefer Kurnaz', 'Maalim A. H. Aljabery']\",\n",
       "  'date': 'June 2018',\n",
       "  'source': \"ICEMIS '18: Proceedings of the Fourth International Conference on Engineering &amp; MIS 2018\",\n",
       "  'abstract': \"Our research transacts with a great various audiology data from National Health System (NHS) facility, including audiograms, structured data such as age, gender, and diagnosis, and a text of specific information about each patient, i.e., clinical reports. This research examines factors related to audiology patients depends on various data by using the mining and analysis of this data. This paper looks for factors affecting the choice between two prevalent hearing aid kinds: BTE (Behind The Ear) or ITE (In The Ear). This choice often done by audiology technicians working in specific clinics for this purpose, based on audiograms results and patient consultation. In many situations, there is an obvious choice, but sometimes the technicians need for the second opinion via an automatic system includes clarification of how to obtain that second opinion. The research deals with diversified specifics and more significant factors for choosing of confirmed hearing aid related to those specifics. We depend on the earlier study data (Bareiss, E. Ray, & Porter, Bruce (1987)). Protos: An Exemplar-Based Learning Apprentice. In the Proceedings of the 4th International Workshop on Machine Learning, 12-23, Irvine, California, which illustrates the database analysis for 180,000 records, for 23,000 patients, by the hearing aid clinic at James Cook University Hospital in Middlesbrough, UK. This data mined to find which factors contribute to the deduction to fit a BTE hearing aid as opposed to an ITE hearing aid. Here we conduct some enhancements on this database and analyze the data depends on medical information to create a new class then we use some intelligent Data Mining (DM) techniques to guess the most correct illness that could be associated with patient's information. Based on the result (according to the patients' diagnosis details), we can obtain right predictions of which type of Hearing Aid (HA) they should use.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3234698.3234755',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'The Research of Web Text Classification Based on Wechat Article',\n",
       "  'authors': \"['Huan Liu', 'Jun Li', 'Yaqin Fan', 'Zekun Song']\",\n",
       "  'date': 'August 2017',\n",
       "  'source': \"ICIE '17: Proceedings of the 6th International Conference on Information Engineering\",\n",
       "  'abstract': 'With the rapid development of the network, many document data emerges on the Internet. So automatic text categorization technology becomes extremely important. In this paper, the weighting method based on the weight of HTML semantics and feature items is used to weight the feature items.On the basis of this, LDA model is combined with SVM1, and the performance of LDA good text feature extraction and the strong classification of SVM are used to classify.The experimental results show that the combination of the two can make the text classification performance more superior and shorten the training time greatly.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3078564.3078568',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A Survey on Knowledge Enhanced EHR Data Mining',\n",
       "  'authors': \"['Jiancheng Zhang', 'Xiao Yang', 'Hefeng Meng', 'Zhiqiang Lin', 'Yonghui Xu', 'Lizhen Cui']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"ICCSE '21: 5th International Conference on Crowd Science and Engineering\",\n",
       "  'abstract': 'EHR contains detailed information about a large number of patients. In the past ten years, EHR-related research has involved various fields, and research in various fields is inseparable from the acquisition of knowledge in EHR. The rapid development of various fields in recent years has brought new challenges to the acquisition of knowledge in EHR. At the same time, the knowledge enhancement technology that has emerged in recent years has effectively improved this problem. Therefore, more and more researches personnel applied knowledge enhancement technology to EHR. In this article, we summarized the literature in this area. We first summarized the knowledge types of EHR, and then made a sub-statement on knowledge extraction and modeling. Next, the correct representation method of knowledge is given, and finally, the specific application in each field is summarized. We hope this article can continue to promote the development of knowledge enhancement technology in EHR.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503181.3503202',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Predicting the existence of exploitation concepts linked to software vulnerabilities using text mining',\n",
       "  'authors': \"['Konstantinos Charmanas', 'Nikolaos Mittas', 'Lefteris Angelis']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"PCI '21: Proceedings of the 25th Pan-Hellenic Conference on Informatics\",\n",
       "  'abstract': 'Software vulnerabilities are weaknesses of specific products and versions that may lead in benefiting attackers to exploit such malfunctions, having a further goal to gain access to operating systems, devices and users’ data. As not all vulnerabilities constitute potential threats to such information, this research attempts to explore ways to the identify the ones that are possible to be exploited using only textual descriptions. The practical goal of the experiments is to examine future raw descriptions in order to predict whether the linked product is likely to be exploited or not. The ground truth examined is the existence or absence of references that report exploitation concepts of the related weaknesses. To meet our objectives, in this study, we make use of Natural Language Processing (NLP) techniques, feature evaluation filtering mechanisms and an oversampling method in order to adapt the raw texts into inputs to classification models and detect the most important terms. The results are promising as many constructed models provided an overall accepted accuracy based on the information of the collected dataset.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503823.3503888',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Mining Career Paths from Large Resume Databases: Evidence from IT Professionals',\n",
       "  'authors': \"['Theodoros Lappas']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Knowledge Discovery from Data',\n",
       "  'abstract': 'The emergence of online professional platforms, such as LinkedIn and Indeed, has led to unprecedented volumes of rich resume data that have revolutionized the study of careers. One of the most prevalent problems in this space is the extraction of prototype career paths from a workforce. Previous research has consistently relied on a two-step approach to tackle this problem. The first step computes the pairwise distances between all the career sequences in the database. The second step uses the distance matrix to create clusters, with each cluster representing a different prototype path. As we demonstrate in this work, this approach faces two significant challenges when applied on large resume databases. First, the overwhelming diversity of job titles in the modern workforce prevents the accurate evaluation of distance between career sequences. Second, the clustering step of the standard approach leads to highly heterogeneous clusters, due to its inability to handle categorical sequences and sensitivity to outliers. This leads to non-representative centroids and spurious prototype paths that do not accurately represent the actual groups in the workforce. Our work addresses these two challenges and has practical implications for the numerous researchers and practitioners working on the analysis of career data across domains.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3379984',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Metalearning Applied to Multi-label Text Classification',\n",
       "  'authors': \"['Vânia Batista dos Santos', 'Luiz Henrique de Campos Merschmann']\",\n",
       "  'date': 'November 2020',\n",
       "  'source': \"SBSI '20: Proceedings of the XVI Brazilian Symposium on Information Systems\",\n",
       "  'abstract': 'Data Mining and Machine Learning fields have many techniques that can support data analysts in the text classification task. However, finding the most adequate techniques require advanced technical knowledge, exhaustive computational experiments and, consequently, time. To address this issue, researchers have proposed different approaches for selecting such techniques to be employed in classification tasks and the dynamic selection of classifiers is one of them. Therefore, this work proposes an approach that uses metalearning to automate the process of selecting the best classifier for each instance of a given multi-label textual dataset. Experiments were performed with multi-label text datasets and showed that the proposed approach is promising.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3411564.3411646',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Data Mining Using Clustering Algorithm as Tool for Poverty Analysis',\n",
       "  'authors': \"['Janelyn A. Talingdan']\",\n",
       "  'date': 'February 2019',\n",
       "  'source': \"ICSCA '19: Proceedings of the 2019 8th International Conference on Software and Computer Applications\",\n",
       "  'abstract': 'Poverty in one place can be reduced or minimized if proper poverty alleviation programs are given to the community. In this study different clustering algorithms were evaluated using silhouette index to get the best clustering algorithm to group the households and analyze the poverty data. The k-means algorithm where k=3 outperformed DBSCAN and k-medoids with a silhouette of 0.308. The algorithm produced three groups or clusters and labelled as non-poor, near poor and poor. The result can help policy-makers formulate and implement poverty reduction policies and programs that are clear, reasonable, realistic, and enforceable.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3316615.3316672',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A Machine Learning Approach to Blind Multi-Path Classification for Massive MIMO Systems',\n",
       "  'authors': \"['Ning Xie', 'Le Ou-Yang', 'Alex X. Liu']\",\n",
       "  'date': 'None',\n",
       "  'source': 'IEEE/ACM Transactions on Networking',\n",
       "  'abstract': 'This paper concerns the problem of the multi-path classification in a multi-user multi-input multi-output (MIMO) system. We propose a machine learning approach to achieve a blind multi-path classification in the uplink (UL) scheme of a multi-user massive MIMO system. Note that the &#x201C;blind&#x201D; term in our approach represents the achievement of the multi-path classification without different pilot sequences on different users, without prior channel state information (CSI) at each user, and without any exploiting the special properties of the received signal. Specifically, our approach consists of two phases. In the first phase, multiple users transmit communication-requests to the base station (BS) for message transmission. The BS only estimates the scaled large-scale path loss of each user, which is determined by the distance between the transmitter and the receiver and is independent of the number of multi-path. Then, the BS compares the difference of the scaled large-scale path loss between any two users. If the difference is sufficiently large, the BS notifies all users to permit their simultaneous message transmissions. However, if the difference is small, the BS notifies each user to slightly adjust their transmission power and then permits their simultaneous message transmissions as well. In the second phase, all users simultaneously transmit their messages using the same radio resource. Then the BS selects one predetermined constellation point from the received pilot symbols as the input of clustering algorithms. According to the clustering results, the BS classifies each multi-path into a specific user. The key intuition of our approach is that the clustering algorithms can generate multiple cluster centroids and each cluster centroid represents the average reception power of each user. Moreover, we use a weighted ensemble clustering algorithm to further improve the performance of our approach. We implemented our approach and conducted extensive performance comparison. Our experimental results show that, when the received signal-to-noise ratio (SNR) is more than 13 dB, our approach with the weighted ensemble clustering algorithm can correctly classify all multi-path to the corresponding user and the output SNR can be improved by 3.2 dB, where we consider three users in an 8PSK system and each user possess 50 multi-path.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/TNET.2020.3008287',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Mining Frequency of Drug Side Effects over a Large Twitter Dataset Using Apache Spark',\n",
       "  'authors': \"['Dennis Hsu', 'Melody Moh', 'Teng-Sheng Moh']\",\n",
       "  'date': 'July 2017',\n",
       "  'source': \"ASONAM '17: Proceedings of the 2017 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 2017\",\n",
       "  'abstract': 'Despite clinical trials by pharmaceutical companies as well as current FDA reporting systems, there are still drug side effects that have not been caught. To find a larger sample of reports, a possible way is to mine online social media. With its current widespread use, social media such as Twitter has given rise to massive amounts of data, which can be used as reports for drug side effects. To process these large datasets, Apache Spark has become popular for fast, distributed batch processing. In this work, we have improved on previous pipelines in sentimental analysis-based mining, processing, and extracting tweets with drug-caused side effects. We have also added a new ensemble classifier using a combination of sentiment analysis features to increase the accuracy of identifying drug-caused side effects. In addition, the frequency count for the side effects is also provided. Furthermore, we have also implemented the same pipeline in Apache Spark to improve the speed of processing of tweets by 2.5 times, as well as to support the process of large tweet datasets. As the frequency count of drug side effects opens a wide door for further analysis, we present a preliminary study on this issue, including the side effects of simultaneously using two drugs, and the potential danger of using less-common combination of drugs. We believe the pipeline design and the results present in this work would have great implication on studying drug side effects and on big data analysis in general.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3110025.3110110',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'CLEAR: Contrastive-Prototype Learning with Drift Estimation for Resource Constrained Stream Mining',\n",
       "  'authors': \"['Zhuoyi Wang', 'Yuqiao Chen', 'Chen Zhao', 'Yu Lin', 'Xujiang Zhao', 'Hemeng Tao', 'Yigong Wang', 'Latifur Khan']\",\n",
       "  'date': 'April 2021',\n",
       "  'source': \"WWW '21: Proceedings of the Web Conference 2021\",\n",
       "  'abstract': 'Non-stationary data stream mining aims to classify large scale online instances that emerge continuously. The most apparent challenge compared with the offline learning manner is the issue of consecutive emergence of new categories, when tackling non-static categorical distribution. Non-stationary stream settings often appear in real-world applications, e.g., online classification in E-commerce systems that involves the incoming productions, or the summary of news topics on social networks (Twitter). Ideally, a learning model should be able to learn novel concepts from labeled data (in new tasks) and reduce the abrupt degradation of model performance on the old concept (also named catastrophic forgetting problem). In this work, we focus on improving the performance of the stream mining approach under the constrained resources, where both the memory resource of old data and labeled new instances are limited/scarce. We propose a simple yet efficient resource-constrained framework CLEAR to facilitate previous challenges during the one-pass stream mining. Specifically, CLEAR focuses on creating and calibrating the class representation (the prototype) in the embedding space. We first apply the contrastive-prototype learning on large amount of unlabeled data, and generate the discriminative prototype for each class in the embedding space. Next, for updating on new tasks/categories, we propose a drift estimation strategy to calibrate/compensate for the drift of each class representation, which could reduce the knowledge forgetting without storing any previous data. We perform experiments on public datasets (e.g., CUB200, CIFAR100) under stream setting, our approach is consistently and clearly better than many state-of-the-art methods, along with both the memory and annotation restriction.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3442381.3449820',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A machine learning approach for medical device classification',\n",
       "  'authors': \"['Aaron Ceross', 'Jeroen Bergmann']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"ICEGOV '21: Proceedings of the 14th International Conference on Theory and Practice of Electronic Governance\",\n",
       "  'abstract': 'The growth of medical device innovation over the last decades has necessitated the need for strong regulatory control in order to ensure the safety and performance of such devices. Medical devices are categorised according to the risk posed to the public. However, the legislation describing the classification rules are often dense and difficult to read. In order to facilitate device classification, the medical device regulator in Australia, the Therapeutic Goods Authority (TGA), provides online digital support tool for device classification. In this work, we (i) evaluate the online tool and (ii) make a further a proposal for using machine learning as means to provide more effective results. For the first part of this work, we asses whether the tool increases the readability of the legislative rules by evaluating the Flesch reading ease score of the legislation and the tool. While the online tool provides some degree of simplicity and readability over the legislation, we argue that the TGA can make more use of its data in order to provide more effective services. In the second part, we develop a proof-of-concept machine learning model to classify a device based on its stated purpose. The results of the experiment show a 82% weighted accuracy across four class labels, indicating that a more data-driven approach could be adopted by the authority.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3494193.3494232',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Evaluating Citizen Comments in Public Consultations Using Data Mining: Evaluating Citizen Comments in Public Consultations Using Data Mining: Analyzing Legislation Comments for the Greek General Commercial Registry',\n",
       "  'authors': \"['Eirini Manga', 'Nikitas Karanikolas', 'Catherine Marinagi', 'Christos Skourlas']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"PCI '21: Proceedings of the 25th Pan-Hellenic Conference on Informatics\",\n",
       "  'abstract': 'None',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503823.3503902',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Optimization of Big Data Mining Algorithm Based on Spark Framework: Preparation of Camera-Ready Contributions to SCITEPRESS Proceedings',\n",
       "  'authors': \"['Yan Zeng', 'Jun Li']\",\n",
       "  'date': 'January 2022',\n",
       "  'source': 'BIC 2022: 2022 2nd International Conference on Bioinformatics and Intelligent Computing',\n",
       "  'abstract': \"Abstract: Frequent itemsets mining is the core of association rule mining data. However, with the continuous increase of data, the traditional Apriori algorithm cannot meet people's daily needs, and the algorithm efficiency is low. This paper proposes the Eclat algorithm based on the Spark framework. In view of the shortcomings of serial algorithm in processing big data, it is modified. Using the vertical structure to avoid repetitive traversal of large amounts of data, while computing based on memory can greatly reduce I/O load and reduce computing time. Combined with the pruning strategy, the calculation of irrelevant itemsets is reduced, and the parallel computing capability of the algorithm is improved. The experimental results show that the efficiency of the Eclat algorithm based on the Spark framework is far better than that of the Eclat algorithm, and it has high efficiency and good scalability when processing massive data.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3523286.3524685',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': \"A Three-Step Data-Mining Analysis of Top-Ranked Higher Education Institutions' Communication on Facebook\",\n",
       "  'authors': \"['Álvaro Figueira']\",\n",
       "  'date': 'October 2018',\n",
       "  'source': \"TEEM'18: Proceedings of the Sixth International Conference on Technological Ecosystems for Enhancing Multiculturality\",\n",
       "  'abstract': \"Organizations are rushing into social media networks following a worldwide trend to create a social presence in multiple media channels. However, a social media strategy needs to be aligned with and framed in the overall organizational strategic management goals. Higher Educational Institutions (HEI) are not different from other organizations in which concerns these problems. Determining the organizational positioning of an organization current strategy will allow to combine monitoring and benchmarking methods to foster the identification of opportunities and threats, which can serve as inputs for the internal evaluation of social media strategies', for the necessary strategic readjustments and a subsequent efficiency measurement. In order to address these challenges, we propose a three-step automatic data-mining procedure to assess the posting behavior and strategy of HEI, understand the editorial policy behind it, and predict the future HEI engagement. We used a sample of the 5-top ranked educational institutions in 2017. We collected the posts from each HEI official Facebook page during an entire school year. Our method showed high degree of accuracy and is also capable of describing which topics are most common in each university's social media content strategy and relate them to the corresponding response from their publics.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3284179.3284342',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Mining Redescriptions with Siren',\n",
       "  'authors': \"['Esther Galbrun', 'Pauli Miettinen']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Knowledge Discovery from Data',\n",
       "  'abstract': 'In many areas of science, scientists need to find distinct common characterizations of the same objects and, vice versa, to identify sets of objects that admit multiple shared descriptions. For example, in biology, an important task is to identify the bioclimatic constraints that allow some species to survive, that is, to describe geographical regions both in terms of the fauna that inhabits them and of their bioclimatic conditions. In data analysis, the task of automatically generating such alternative characterizations is called redescription mining.If a domain expert wants to use redescription mining in his research, merely being able to find redescriptions is not enough. He must also be able to understand the redescriptions found, adjust them to better match his domain knowledge, test alternative hypotheses with them, and guide the mining process toward results he considers interesting. To facilitate these goals, we introduce Siren, an interactive tool for mining and visualizing redescriptions.Siren allows to obtain redescriptions in an anytime fashion through efficient, distributed mining, to examine the results in various linked visualizations, to interact with the results either directly or via the visualizations, and to guide the mining algorithm toward specific redescriptions. In this article, we explain the features of Siren and why they are useful for redescription mining. We also propose two novel redescription mining algorithms that improve the generalizability of the results compared to the existing ones.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3007212',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'The Classification of Gene Sequencer Based on Machine Learning',\n",
       "  'authors': \"['Jie Yang', 'Yong Cao']\",\n",
       "  'date': 'December 2021',\n",
       "  'source': \"EBIMCS '21: Proceedings of the 2021 4th International Conference on E-Business, Information Management and Computer Science\",\n",
       "  'abstract': 'Abstract: Biological sequencing plays a very important role in life science, especially with the improvement of sequencing technology and the development of sequencing instruments, and a large number of biological sequencing quality data are produced every day. Because of different sequencers, the quality of sequencing is different. In the process of sequencing quality control, the model of sequencer can be deduced according to the quality of gene sequence. Therefore, in this paper, five sequencers of Illumina HiSeq series, Illumina HiSeq 2000, Illumina HiSeq 2500, Illumina HiSeq 3000, Illumina HiSeq 4000 and Illumina HiSeq XTen, are selected as the classification objects. Firstly, the sequencing quality data of the five sequencers are preprocessed. Then, the classification model is trained by three machine learning algorithms: decision tree, logistic regression and support vector machine. The experimental results show that the accuracy rates of the three machine learning algorithms are 96.67%, 97.50% and 97.50% respectively. These algorithms are very good to solve the problem of using biological sequencing data quality to classify sequencer.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3511716.3511730',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Evaluation of Dimensionality Reduction Techniques-Principal Feature Analysis in case of Text Classification Problems',\n",
       "  'authors': \"['Michael Mammo', 'Tony Lindgren']\",\n",
       "  'date': 'January 2020',\n",
       "  'source': \"ICCDE '20: Proceedings of 2020 6th International Conference on Computing and Data Engineering\",\n",
       "  'abstract': 'One of the commonly observed phenomena in text classification problems is sparsity of the generated feature set. So far, different dimensionality reduction techniques have been developed to reduce feature spaces into a convenient size that a learner algorithm can infer. Among these, Principal Component Analysis (PCA) is one of the well-established techniques which is capable of generating an undistorted view of the data. As a result, variants of the algorithm have been developed and applied in several domains, including text mining. However, PCA does not provide backward traceability to the original features once it projected the initial features to a new space. Also, it needs a relatively large computational space since it uses all features when generating the final features. These drawbacks especially pose a problem in text classification problems where high dimensionality and sparsity are common phenomena. This paper presents a modified version PCA, Principal Feature Analysis (PFA), which enables backward traceability by choosing a subset of optimal features in the original space using the same criteria PCA uses, without involving the initial features into final computation. The proposed technique is tested against benchmark corpora and produced a comparable result as PCA while maintaining traceability to the original feature space.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3379247.3379274',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Do Words with Certain Part of Speech Tags Improve the Performance of Arabic Text Classification?',\n",
       "  'authors': \"['Abdulmohsen Al-Thubaity', 'Aali Alqarni', 'Ahmad Alnafessah']\",\n",
       "  'date': 'April 2018',\n",
       "  'source': \"ICISDM '18: Proceedings of the 2nd International Conference on Information System and Data Mining\",\n",
       "  'abstract': 'Feature extraction - the process of choosing feature types that can represent and discriminate between dataset topics - is one of the critical steps in text classification and varies with the language of the texts. Different feature types have been proposed for Arabic text classification, ranging from features based on word orthography (single word and character and word N-grams) to features based on linguistic analysis (roots, stems). To the best of our knowledge, little attention has been paid to investigating the performance of Arabic text classification when Part of Speech (POS) tagging information is used to extract features. In this study, we used a corpus comprising 4900 newspaper texts distributed evenly over seven topics to investigate the effect of using POS tag distribution and words that belong to certain POS tags on Arabic text classification, namely nouns, verbs and adjectives. For feature selection, feature representation and text classification we used Chi-squared, Log-Weighted Term Frequency Inverse Document Frequency with Cosine Normalization (LTC) and support vector machine (SVM) respectively. We used four metrics, namely accuracy, precision, recall and F-measure to measure classification performance. Experiment data suggest that the words achieved the best classification performance when the number of features was low; however, the classification performance can be marginally increased when nouns, verbs and adjectives are used as features, given that the number of features is increased.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3206098.3206109',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Data Mining in Education: Discussing Knowledge Discovery in Database (KDD) with Cluster Associative Study',\n",
       "  'authors': \"['Abdulkadir Abdulahi Hasan', 'Huan Fang']\",\n",
       "  'date': 'May 2021',\n",
       "  'source': 'ICAIIS 2021: 2021 2nd International Conference on Artificial Intelligence and Information Systems',\n",
       "  'abstract': 'Data Mining, being an emerging field has ties with ever-increasing knowledge base, contributing to historical data trends and patterns as well as providing evidence to predict future behaviors. With the adoption of data mining techniques and their thorough implications, the knowledge base could be enhanced and a futuristic strategic management tool could be formulated in the form of modern data mining techniques. The technique and terminology used in the paper involves extracting knowledge from large datasets. In this paper, cluster sampling associated with data mining is used to dig out a beneficial way of tackling data mining steps. With these combined data mining efforts, the aim of highlighting the significance of data mining could be amalgamated with data warehouses and the steps that need to involve selecting, transforming, mining, and output evaluation to get beneficial or desired results. Furthermore, a mild emphasize is put on the cluster sampling data mining technique, steps, and its implication to enhance knowledge, explore new terms, and broaden our output scenarios so that there is a better understanding of how to move along the data mining channel. The objective of this study is to explain the basics of data mining and provide a way to further this research over data mining with cluster sampling and Knowledge discovery is data bases (KDD). For the said purpose, two clusters of the population of China are used in this study. One of which includes the percentage of students enrolled in the primary schools; and the other includes percentage of students enrolled in secondary schools in China. The steps of cluster sampling technique are followed to give a better understanding of the data mining approach.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3469213.3471319',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Apricot Variety Classification Using Image Processing and Machine Learning Approaches',\n",
       "  'authors': \"['Seyed Vahid Mirnezami', 'Ali HamidiSepehr', 'Mahdi Ghaebi', 'Seyed Reza Hassan-Beygi']\",\n",
       "  'date': 'December 2020',\n",
       "  'source': 'ICVISP 2020: Proceedings of the 2020 4th International Conference on Vision, Image and Signal Processing',\n",
       "  'abstract': 'Apricot which is a cultivated type of Zerdali (wild apricot) has an important place in human nutrition and its medical properties are essential for human health. The objective of this research was to obtain a model for apricot mass and separate apricot variety with image processing technology using external features of apricot fruit. In this study, five verities of apricot were used. In order to determine the size of the fruits, three mutually perpendicular axes were defined, length, width, and thickness. Measurements show that the effect of variety on all properties was statistically significant at the 1% probability level. Furthermore, there is no significant difference between the estimated dimensions by image processing approach and the actual dimensions. The developed system consists of a digital camera, a light diffusion chamber, a distance adjustment pedestal, and a personal computer. Images taken by the digital camera were stored as (RGB) for further analysis. The images were taken for a number of 49 samples of each cultivar in three directions. A linear equation is recommended to calculate the apricot mass based on the length and the width with R2 = 0.97. In addition, ANFIS model with C-means was the best model for classifying the apricot varieties based on the physical features including length, width, thickness, mass, and projected area of three perpendicular surfaces. The accuracy of the model was 87.7.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3448823.3448856',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Data mining techniques and nature-inspired algorithms for query expansion',\n",
       "  'authors': \"['Ilyes Khennak', 'Habiba Drias']\",\n",
       "  'date': 'May 2018',\n",
       "  'source': \"LOPAL '18: Proceedings of the International Conference on Learning and Optimization Algorithms: Theory and Applications\",\n",
       "  'abstract': 'Data mining techniques and nature-inspired algorithms are currently among the most frequently used soft computing techniques for knowledge discovery, optimization, and computational intelligence. In this paper, we propose the application of both data mining techniques and nature-inspired algorithms to overcome the problem of generating the optimum expanded query in web information retrieval. We first use data mining techniques to group similar expansion term candidates into clusters. Next, we use nature-inspired algorithms to extract expansion term candidates from clusters and generate the suitable expanded query. We empirically assess the proposed approach using MEDLINE, the large online medical repository. Numerical experiments show that the proposed approach attains higher effectiveness and efficiency compared to conventional and recently published methods.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3230905.3234631',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Linguistic Pattern Mining for Data Analysis in Microblog Texts using Word Embeddings',\n",
       "  'authors': \"['Danielly Sorato', 'Renato Fileto']\",\n",
       "  'date': 'May 2019',\n",
       "  'source': \"SBSI '19: Proceedings of the XV Brazilian Symposium on Information Systems\",\n",
       "  'abstract': 'Microblog posts (e.g. tweets) often contain users opinions and thoughts about events, products, people, organizations, among other possibilities. However, the usage of social media to promote online disinformation and manipulation is not an uncommon occurrence. Analyzing the characteristics of such discourses in social media is essential for understanding and fighting such actions. Extracting recurrent fragments of text, i.e. word sequences, which are semantically similar can lead to the discovery of linguistic patterns used in certain kinds of discourse. Therefore, we aim to use such patterns to encapsulate frequent discourses textually expressed in microblog posts. In this paper, we propose to exploit linguistic patterns in the context of the 2016 United Estates presidential election. Through a technique that we call Short Semantic Pattern (SSP) mining, we were able to extract sequences of words that share a similar meaning in their word embedding representation. In the experiments we investigate the incidence of SSP instances regarding political adversaries and media in tweets posted by Donald Trump, during the presidential election campaign. Experimental results show a high preponderance of some statements of Donald Trump towards their adversaries and expressions that often appeared in such tweets.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3330204.3330228',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A survey of text classification models',\n",
       "  'authors': \"['JinXiong Yang', 'Liang Bai', 'Yanming Guo']\",\n",
       "  'date': 'October 2020',\n",
       "  'source': \"RICAI '20: Proceedings of the 2020 2nd International Conference on Robotics, Intelligent Control and Artificial Intelligence\",\n",
       "  'abstract': 'With the rapid development of artificial intelligence, text classification method based on deep learning model has surpassed traditional machine learning method in various aspects. This paper introduces dozens of deep learning models for text classification according to the different network structures of the models. In addition, this paper briefly introduces the evaluation indicators and application scenarios of text classification, summarizes and forecasts the current challenges and future development trend of text classification.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3438872.3439101',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Differentiable Pattern Set Mining',\n",
       "  'authors': \"['Jonas Fischer', 'Jilles Vreeken']\",\n",
       "  'date': 'August 2021',\n",
       "  'source': \"KDD '21: Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining\",\n",
       "  'abstract': 'Pattern set mining has been successful in discovering small sets of highly informative and useful patterns from data. To find good models, existing methods heuristically explore the twice-exponential search space over all possible pattern sets in a combinatorial way, by which they are limited to data over at most hundreds of features, as well as likely to get stuck in local minima. Here, we propose a gradient based optimization approach that allows us to efficiently discover high-quality pattern sets from data of millions of rows and hundreds of thousands of features. In particular, we propose a novel type of neural autoencoder called BinaPs, using binary activations and binarizing weights in each forward pass, which are directly interpretable as conjunctive patterns. For training, optimizing a data-sparsity aware reconstruction loss, continuous versions of the weights are learned in small, noisy steps. This formulation provides a link between the discrete search space and continuous optimization, thus allowing for a gradient based strategy to discover sets of high-quality and noise-robust patterns. Through extensive experiments on both synthetic and real world data, we show that BinaPs discovers high quality and noise robust patterns, and unique among all competitors, easily scales to data of supermarket transactions or biological variant calls.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3447548.3467348',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Quantitative interactive investment algorithm based on machine learning and data mining',\n",
       "  'authors': \"['Fangyuan Cheng', 'Junmin Jia']\",\n",
       "  'date': 'July 2021',\n",
       "  'source': 'DSIT 2021: 2021 4th International Conference on Data Science and Information Technology',\n",
       "  'abstract': \"Quantitative investment is a mean to predict the development of securities and conduct transactions by using computer algorithms, but it usually ignores the guiding role of investors' personal preferences in deciding investment plans. Thus, we suggest a quantitative interactive investment algorithm to integrate the rational goals of investment with the individual preference indicators of decision-making investors. The interactive multi-objective solution algorithm creatively combines the decision tree algorithm which mine the investor's individual preference index from the investor's decision data with the second-generation non-dominated sorting genetic algorithm (NSGA-II). This method solves the problem of the lack of personalized choices in current quantitative investment methods by adding decision-makers’ preference indicators.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3478905.3478982',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'From Opinion Mining to Improvement Mining : Understanding Product Improvements from User Reviews',\n",
       "  'authors': \"['Roshni Ramnani', 'Shubhashis Sengupta']\",\n",
       "  'date': 'December 2021',\n",
       "  'source': \"FIRE '21: Proceedings of the 13th Annual Meeting of the Forum for Information Retrieval Evaluation\",\n",
       "  'abstract': 'A valuable trove of information exists for product(s) or services online via user opinions like detailed reviews provided by customers on popular e-commerce websites. Users express their individual opinions in the form of overall product/service experiences, which may include explicit positive/negative feedback, preferences, concerns, and suggestions for the future. Such information can be valuable to product/service owners in helping them understand the improvement(s) that must be made to a particular product or service. The primary focus of opinion mining has been on understanding positive and negative aspects within the review effectively. Limited emphasis has been placed on finer topics like user suggestions or conflicting information from users. In this work, we describe a method to extract possible product / service improvements from opinionated text in the form of non-conflicting negative feedback, user tips, recommendations, product usage details, feature suggestions, and specific complaints.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503162.3503166',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Legal Text Processing: Combing two legal ontological approaches through text mining',\n",
       "  'authors': \"['Michalis Avgerinos Loutsaris', 'Zoi Lachana', 'Charalampos Alexopoulos', 'Yannis Charalabidis']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"DG.O'21: DG.O2021: The 22nd Annual International Conference on Digital Government Research\",\n",
       "  'abstract': \"The globalization of communication networks and the possibilities offered by the information and communication technologies (ICTs) significantly change the public sector's operation and services. Digital Governance is now integrated into administrations' policies and programs at all levels: local, regional, national, European. At the national level, there is a requirement to provide electronic public services according to citizens' needs while, in the sense of globalization, at the European level, there are many programs (e.g., the Europe 2005 and i2010 program) emphasizing the Digital Governance world (or better Digital Governance community) that indicates rapid changes not only in the sense of the change in the public sector's systems but also in the mentality that the public sector operates. On the other hand, Digital Governance's evolution affects societies intensively, emphasizing the importance of cross-border interaction and information sharing between them. [6]. Concerning the legal informatics domain, this can result in changing governments' operations in many ways [2]. By now, the massive amount of each country's legal information currently remains fragmented across multiple national databases and systems or even better legal databases. Most of these legal databases result from the significant advancements in the “legal informatics” research field that observed since governments have started to promote the development of legal information systems [9]. This research contributes to this purpose by developing an open and automated legal system capable of providing any EU country's legal information based on the existing ontologies.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3463677.3463730',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Benchmarking Apache Spark and Hadoop MapReduce on Big Data Classification',\n",
       "  'authors': \"['Taha Tekdogan', 'Ali Cakmak']\",\n",
       "  'date': 'August 2021',\n",
       "  'source': \"ICCBDC '21: Proceedings of the 2021 5th International Conference on Cloud and Big Data Computing\",\n",
       "  'abstract': 'Most of the popular Big Data analytics tools evolved to adapt their working environment to extract valuable information from a vast amount of unstructured data. The ability of data mining techniques to filter this helpful information from Big Data led to the term ‘Big Data Mining’. Shifting the scope of data from small-size, structured, and stable data to huge volume, unstructured, and quickly changing data brings many data management challenges. Different tools cope with these challenges in their own way due to their architectural limitations. There are numerous parameters to take into consideration when choosing the right data management framework based on the task at hand. In this paper, we present a comprehensive benchmark for two widely used Big Data analytics tools, namely Apache Spark and Hadoop MapReduce, on a common data mining task, i.e., classification. We employ several evaluation metrics to compare the performance of the benchmarked frameworks, such as execution time, accuracy, and scalability. These metrics are specialized to measure the performance for classification task. To the best of our knowledge, there is no previous study in the literature that employs all these metrics while taking into consideration task-specific concerns. We show that Spark is 5 times faster than MapReduce on training the model. Nevertheless, the performance of Spark degrades when the input workload gets larger. Scaling the environment by additional clusters significantly improves the performance of Spark. However, similar enhancement is not observed in Hadoop. Machine learning utility of MapReduce tend to have better accuracy scores than that of Spark, like around 2%-3%, even in small-size data sets.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3481646.3481649',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Chinese News Text Multi Classification Based on Naive Bayes Algorithm',\n",
       "  'authors': \"['Fei Wang', 'Xin Deng', 'Lunqing Hou']\",\n",
       "  'date': 'September 2018',\n",
       "  'source': \"ISCSIC '18: Proceedings of the 2nd International Symposium on Computer Science and Intelligent Control\",\n",
       "  'abstract': \"With the development of Internet, there are more and more text data appear, the companies face the challenge to organize the content and the users feel confused about what is useful content for them. If the text data can be classified will make a contribution to solve the problem. It has been a long time, text classification work is done by human beings, like editors. So text classification become a hot topic in nature language processing field, especially for Chinese text classification. Sentiment classification just need to classify two classes, but there are more situations where we need to do multi classification. Such as the news editors have to give an article tags manually. There are several ways to solve the text classification problem: (1) Naive Bayes algorithm (2) support vector machine algorithm (3) neural network (4) k nearest neighbors (5) decision tree [1][2][3][4][5]. Naive Bayes applies Bayes' theorem with strong(naive) independence assumptions between the features. This paper proposes to use Naive Bayes to finish a Chinese news text multi classification with nine classes.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3284557.3284704',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Using Machine Learning Clustering To Find Large Coverage Holes',\n",
       "  'authors': \"['Raviv Gal', 'Giora Simchoni', 'Avi Ziv']\",\n",
       "  'date': 'November 2020',\n",
       "  'source': \"MLCAD '20: Proceedings of the 2020 ACM/IEEE Workshop on Machine Learning for CAD\",\n",
       "  'abstract': 'Identifying large and important coverage holes is a time-consuming process that requires expertise in the design and its verification environment. This paper describes a novel machine learning-based technique for finding large coverage holes when the coverage events are individually defined. The technique is based on clustering the events according to their names and mapping the clusters into cross-products. Our proposed technique is being used in the verification of high-end servers. It has already improved the quality of coverage analysis and helped identify several environment problems.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3380446.3430621',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Research on Data Analysis and Mining Technology Based on Computer Visualization',\n",
       "  'authors': \"['Li Guo']\",\n",
       "  'date': 'October 2020',\n",
       "  'source': 'CIPAE 2020: Proceedings of the 2020 International Conference on Computers, Information Processing and Advanced Education',\n",
       "  'abstract': 'In the era of mobile Internet, resource interconnection and sharing, group collaboration and cooperation have become the new driving forces for scientific and social development. Visualization and human-computer interaction technology have played an important role in collaborative knowledge dissemination and scientific discovery. In order to fully explore and utilize the value of information resources, data mining technology came into being; the thesis first elaborated on the concept and classification of visual data mining, then discussed some of the main techniques of visual data mining, and finally through a system developed The classic shopping basket analysis problem was discussed with the realization of visual data mining technology.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3419635.3419687',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A First Look at the Crypto-Mining Malware Ecosystem: A Decade of Unrestricted Wealth',\n",
       "  'authors': \"['Sergio Pastrana', 'Guillermo Suarez-Tangil']\",\n",
       "  'date': 'October 2019',\n",
       "  'source': \"IMC '19: Proceedings of the Internet Measurement Conference\",\n",
       "  'abstract': 'Illicit crypto-mining leverages resources stolen from victims to mine cryptocurrencies on behalf of criminals. While recent works have analyzed one side of this threat, i.e.: web-browser cryptojacking, only commercial reports have partially covered binary-based crypto-mining malware. In this paper, we conduct the largest measurement of crypto-mining malware to date, analyzing approximately 4.5 million malware samples (1.2 million malicious miners), over a period of twelve years from 2007 to 2019. Our analysis pipeline applies both static and dynamic analysis to extract information from the samples, such as wallet identifiers and mining pools. Together with OSINT data, this information is used to group samples into campaigns. We then analyze publicly-available payments sent to the wallets from mining-pools as a reward for mining, and estimate profits for the different campaigns. All this together is is done in a fully automated fashion, which enables us to leverage measurement-based findings of illicit crypto-mining at scale. Our profit analysis reveals campaigns with multi-million earnings, associating over 4.4% of Monero with illicit mining. We analyze the infrastructure related with the different campaigns, showing that a high proportion of this ecosystem is supported by underground economies such as Pay-Per-Install services. We also uncover novel techniques that allow criminals to run successful campaigns.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3355369.3355576',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Machine Learning Techniques for Classification of Spambase Dataset: A Hybrid Approach',\n",
       "  'authors': \"['Shikha Verma', 'Arun Kumar Gautam']\",\n",
       "  'date': 'September 2019',\n",
       "  'source': 'ISCSIC 2019: Proceedings of the 2019 3rd International Symposium on Computer Science and Intelligent Control',\n",
       "  'abstract': 'Email has become a necessity for this new generation for official communication purposes. As the use of Internet is becoming more and more the risk of being caught into its darker side is so common. The major concern is spam, which is growing exponentially, and the users are becoming victim of it on daily basis. This paper proposes a hybrid machine learning classification model for the spam classification on the spambase dataset. This model uses the four classification algorithms namely Ensemble Classification, Decision Tree, Random Forest and Support Vector Machine (SVM). There are two phases; First phase deals with the classification of spambase dataset in two classes i.e. spam and ham with Decision Tree machine learning algorithm and the second phase comprises of classification improvisation of the output produced by phase one with four machine learning algorithms i.e. Decision Tree, Random Forest, Support Vector Machine (SVM) and Ensemble Learning. The experiment shows a very promising result with improvised accuracy in second phase.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3386164.3389089',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Large-Scale Hierarchical Text Classification with Recursively Regularized Deep Graph-CNN',\n",
       "  'authors': \"['Hao Peng', 'Jianxin Li', 'Yu He', 'Yaopeng Liu', 'Mengjiao Bao', 'Lihong Wang', 'Yangqiu Song', 'Qiang Yang']\",\n",
       "  'date': 'April 2018',\n",
       "  'source': \"WWW '18: Proceedings of the 2018 World Wide Web Conference\",\n",
       "  'abstract': 'Text classification to a hierarchical taxonomy of topics is a common and practical problem. Traditional approaches simply use bag-of-words and have achieved good results. However, when there are a lot of labels with different topical granularities, bag-of-words representation may not be enough. Deep learning models have been proven to be effective to automatically learn different levels of representations for image data. It is interesting to study what is the best way to represent texts. In this paper, we propose a graph-CNN based deep learning model to first convert texts to graph-of-words, and then use graph convolution operations to convolve the word graph. Graph-of-words representation of texts has the advantage of capturing non-consecutive and long-distance semantics. CNN models have the advantage of learning different level of semantics. To further leverage the hierarchy of labels, we regularize the deep architecture with the dependency among labels. Our results on both RCV1 and NYTimes datasets show that we can significantly improve large-scale hierarchical text classification over traditional hierarchical text classification and existing deep models.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3178876.3186005',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Text Classification of Network Pyramid Scheme based on Topic Model',\n",
       "  'authors': \"['Pengyu Mu', 'Jingsha He', 'Nafei Zhu']\",\n",
       "  'date': 'June 2019',\n",
       "  'source': \"NLPIR '19: Proceedings of the 2019 3rd International Conference on Natural Language Processing and Information Retrieval\",\n",
       "  'abstract': 'At present, the network pyramid scheme has become a major tumor that hinders social development. In order to curb the propagation of the network pyramid scheme and effectively identify the pyramid scheme text in the network, this study proposes a joint topic model, Paragraph Vector Latent Dirichlet Allocation (PV_LDA), based on the characteristics of high-yield, high rebate, hierarchical salary and text topic diversity described in the text. The model uses the paragraph as the minimum processing unit to generate the topic distribution matrix of \"high-interest rate\" and \"hierarchical salary\" from the network pyramid scheme text. The Gibbs sampling is used to derive the \"pyramid scheme\" topic distribution matrix represented by the two features, which is used for classification processing by the classifier. the classification accuracy rate for the network pyramid scheme text can reach 86.25%. The conclusions show that the topic model proposed in this paper can capture the characteristics of the pyramid scheme more reasonably.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3342827.3342835',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'The application of data mining techniques and feature selection methods in the risk classification of Egyptian liver cancer patients using clinical and genetic data',\n",
       "  'authors': \"['Esraa H. Abdelaziz', 'Sanaa M. Kamal', 'Khaled El-Bhanasy', 'Rasha Ismail']\",\n",
       "  'date': 'April 2019',\n",
       "  'source': \"ICSIE '19: Proceedings of the 8th International Conference on Software and Information Engineering\",\n",
       "  'abstract': \"Data mining techniques has shown great potential in biomedical and health care fields. The objective of this paper is to apply feature selection methods and data mining techniques to Egyptian liver cancer patients' data to predict their prognosis and extract important features that affect the patient's survivability. Genetic and Clinical data from 1541 patients were analyzed. Three feature selection methods and seven data mining techniques were studied and compared. Wrapper Subset method and Random Forest proved to be the best performing feature selection method and data mining technique respectively. Moreover, important genetic features such as p53 gene exon 6 and 9 mutations proved to have a significant impact on patient's overall prognosis.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3328833.3328849',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Generalizing rules by random forest-based learning classifier systems for high-dimensional data mining',\n",
       "  'authors': \"['Fumito Uwano', 'Koji Dobashi', 'Keiki Takadama', 'Tim Kovacs']\",\n",
       "  'date': 'July 2018',\n",
       "  'source': \"GECCO '18: Proceedings of the Genetic and Evolutionary Computation Conference Companion\",\n",
       "  'abstract': 'This paper proposes high-dimensional data mining technique by integrating two data mining methods: Accuracy-based Learning Classifier Systems (XCS) and Random Forests (RF). Concretely the proposed system integrates RF and XCS: RF generates several numbers of decision trees, and XCS generalizes the rules converted from the decision trees. The convert manner is as follows: (1) the branch node of the decision tree becomes the attribute; (2) if the branch node does not exist, the attribute of that becomes # for XCS; and (3) One decision tree becomes one rule at least. Note that # can become any value in the attribute. From the experiments of Multiplexer problems, we derive that: (i) the good performance of the proposed system; and (ii) RF helps XCS to acquire optimal solutions as knowledge by generating appropriately generalized rules.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3205651.3208298',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Mining Online Training Log Data',\n",
       "  'authors': \"['Susan Mehringer', 'Christopher R. Myers', 'Jennifer Houchins', 'Lorna Rivera']\",\n",
       "  'date': 'July 2019',\n",
       "  'source': \"PEARC '19: Proceedings of the Practice and Experience in Advanced Research Computing on Rise of the Machines (learning)\",\n",
       "  'abstract': 'Online training has been growing in popularity, and offers many advantages for both trainers and learners. Assessing the usage and impact of online material can be difficult, especially if content is made available to anyone and is not part of a course requiring formal enrollment. The Cornell Virtual Workshop (CVW) first offered online training on topics in high-performance computing and computational science in 1994, and ten years ago we began logging usage. We are now performing our first in-depth analysis of those log data to identify patterns in usage, so that we can better understand how users access the material, which types of topics and materials result in the greatest impact, how topic usage changes over time, and what types of presentation format might be preferred. While the CVW is built around a cohesive, sequential narrative for each training topic, we find that many users access our content in a more targeted fashion, suggesting that we rethink how we package our material. We anticipate that ongoing analysis using data science and machine learning methods will enable us to produce more useful training materials, and provide the educational community with valuable information about patterns in online material usage.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3332186.3332235',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Predictive Analytics Using Text Classification for Restaurant Inspections',\n",
       "  'authors': \"['Zhu Wang', 'Booma Sowkarthiga Balasubramani', 'Isabel F. Cruz']\",\n",
       "  'date': 'November 2017',\n",
       "  'source': \"UrbanGIS'17: Proceedings of the 3rd ACM SIGSPATIAL Workshop on Smart Cities and Urban Analytics\",\n",
       "  'abstract': 'According to the Center for Disease Control (CDC), there are almost 48 million people affected by foodborne diseases in the U.S. every year, including 3,000 deaths. The most effective way of avoiding food poisoning would be its prevention. However, complete prevention is not possible, therefore Public Health departments perform routine restaurant inspections, combined with the practice of inspecting specific restaurants once a disease outbreak is identified. Following other health applications (e.g., prediction of a flu outbreak using Twitter), we use social media and a predictive analytics approach to identify the need for targeted visits by city inspectors.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3152178.3152192',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Malware classification using deep learning methods',\n",
       "  'authors': \"['Bugra Cakir', 'Erdogan Dogdu']\",\n",
       "  'date': 'March 2018',\n",
       "  'source': \"ACMSE '18: Proceedings of the ACMSE 2018 Conference\",\n",
       "  'abstract': \"Malware, short for Malicious Software, is growing continuously in numbers and sophistication as our digital world continuous to grow. It is a very serious problem and many efforts are devoted to malware detection in today's cybersecurity world. Many machine learning algorithms are used for the automatic detection of malware in recent years. Most recently, deep learning is being used with better performance. Deep learning models are shown to work much better in the analysis of long sequences of system calls. In this paper a shallow deep learning-based feature extraction method (word2vec) is used for representing any given malware based on its opcodes. Gradient Boosting algorithm is used for the classification task. Then, k-fold cross-validation is used to validate the model performance without sacrificing a validation split. Evaluation results show up to 96% accuracy with limited sample data.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3190645.3190692',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Cloud Computing Resource Load Forecasting Model Based on Data Mining',\n",
       "  'authors': \"['Xiangqin Li']\",\n",
       "  'date': 'September 2021',\n",
       "  'source': 'ICISCAE 2021: 2021 4th International Conference on Information Systems and Computer Aided Education',\n",
       "  'abstract': 'The changes of cloud computing resources are dynamic and random. It is difficult to obtain ideal prediction results by traditional algorithms. In order to improve the accuracy of cloud computing resource prediction, a cloud computing resource load forecasting model based on data mining technology is proposed. Firstly, data mining technology is used to collect cloud computing resource data, and normalized processing, then the resource data are trained, the improved ant colony algorithm is used to select the optimal parameters of the load model, and finally the cloud computing resources are completed by data clustering. The effectiveness of the load prediction model is determined by simulation experiments. The results show that the data mining-based load forecasting model not only improves the prediction accuracy of cloud computing resources, but also reduces the complexity of resource prediction, improves the prediction efficiency, and provides the cloud computing resource prediction, a new modeling approach.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3482632.3484105',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A machine learning approach to flood severity classification and alerting',\n",
       "  'authors': \"['Prativa Sharma', 'Bandana Kar', 'Jun Wang', 'Doug Bausch']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"ARIC '21: Proceedings of the 4th ACM SIGSPATIAL International Workshop on Advances in Resilient and Intelligent Cities\",\n",
       "  'abstract': 'With increased big data and computing power, machine learning is predominantly used for classification to object detection and forecasting of phenomena and relationships. Forecasting, mapping and impact assessment of flood events is one such area where machine learning is gaining momentum. While machine learning has been widely used for forecasting of flood extent and depth using rainfall/runoff datasets, impact assessment based on flood severity distribution using machine learning is still a long way from maturity. In this study, we used several machine learning classifiers such as Decision Tree (DT), Random Forest (RF), Gradient Boosting (GB), Support Vector Machine (SVM) and Multinomial Logit (ML) to classify flood severity into four classes: Information, Advisory, Watch and Warning based on the training datasets obtained from the Model of Models. The Model of Models is an ensemble model which integrates flood forecasting models to determine flood severity globally at sub-watershed level based on spatial extent and duration of flooding, risk scores associated with historic flooding events. The severity classes are used to disseminate alerts to stakeholders globally. The initial results reveal that the GB followed by DT and RF classifier performed better for classifying severity based on the performance assessment metrics. While this study has implemented a first version machine learner, future advancements will focus on deploying adaptive learners to increase the forecasting ability of the machine learner with new datasets generated daily.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3486626.3493432',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Fine and Coarse Granular Argument Classification before Clustering',\n",
       "  'authors': \"['Lorik Dumani', 'Tobias Wiesenfeldt', 'Ralf Schenkel']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"CIKM '21: Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management\",\n",
       "  'abstract': 'Computational argumentation and especially argument mining together with retrieval enjoys increasing popularity. In contrast to standard search engines that focus on finding documents relevant to a query, argument retrieval aims at finding the best supporting and attacking premises given a query claim, e.g., from a predefined collection of arguments. Here, a claim is the central part of an argument representing the standpoint of a speaker with the goal to persuade the audience, and a premise serves as evidence to the claim. In addition to the actual retrieval process, existing work has focused on (1) classifying polarities of arguments into supporting or opposing, (2) classifying arguments by their frames (such as economic or environmental), and (3) clustering similar arguments by their meaning to avoid repetitions in the result list. For experiments, either hand-made argument collections or arguments extracted from debate portals were used. In this paper, we extend existing work on argument clustering, making the following contributions: First, we introduce a novel pipeline for clustering arguments. While previous work classified arguments either by polarity, frame, or meaning, our pipeline incorporates these three, allowing a more systematic presentation of arguments. Second, we introduce a new dataset consisting of 365 argument graphs accompanying more than 11,000 high-quality arguments that, contrary to previous datasets, have been generated, displayed, and verified by journalists and were published in newspapers. A thorough evaluation with this dataset provides a first baseline for future work.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3459637.3482431',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': '\"Keep it Simple, Lazy\" -- MetaLazy: A New MetaStrategy for Lazy Text Classification',\n",
       "  'authors': \"['Luiz Felipe Mendes', 'Marcos Gonçalves', 'Washington Cunha', 'Leonardo Rocha', 'Thierson Couto-Rosa', 'Wellington Martins']\",\n",
       "  'date': 'October 2020',\n",
       "  'source': \"CIKM '20: Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management\",\n",
       "  'abstract': 'Recent advances in text-related tasks on the Web, such as text (topic) classification and sentiment analysis, have been made possible by exploiting mostly the \"rule of more\": more data (massive amounts) more computing power, more complex solutions. We propose a shift in the paradigm to do \"more with less\" by focusing, at maximum extent, just on the task at hand (e.g., classify a single test instance). Accordingly, we propose MetaLazy, a new supervised lazy text classification meta-strategy that greatly extends the scope of lazy solutions. Lazy classifiers postpone the creation of a classification model until a given test instance for decision making is given. MetaLazy exploits new ideas and solutions, which have in common their lazy nature, producing altogether a solution for text classification, which is simpler, more efficient, and less data demanding than new alternatives. It extends and evolves the lazy creation of the model for the test instance by allowing: (i) to dynamically choose the best classifier for the task; (ii) the exploration of distances in the neighborhood of the test document when learning a classification model, thus diminishing the importance of irrelevant training instances; and (iii) a better representational space for training and test documents by augmenting them, in a lazy fashion, with new co-occurrence based features considering just those observed in the specific test instance. In a sizeable experimental evaluation, considering topics and sentiment analysis datasets and nine baselines, we show that our MetaLazy instantiations are among the top performers in most situations, even when compared to state-of-the-art deep learning classifiers such as Deep Network Transformer Architectures.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3340531.3412180',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Mining Hypernyms Semantic Relations from Stack Overflow',\n",
       "  'authors': \"['László Tóth', 'Balázs Nagy', 'Tibor Gyimóthy', 'László Vidács']\",\n",
       "  'date': 'June 2020',\n",
       "  'source': \"ICSEW'20: Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops\",\n",
       "  'abstract': 'Communication between a software development team and business partners is often a challenging task due to the different context of terms used in the information exchange. The various contexts in which the concepts are defined or used create slightly different semantic fields that can evolve into information and communication silos. Due to the silo effect, the necessary information is often inadequately forwarded to developers resulting in poorly specified software requirements or misinterpreted user feedback. Communication difficulties can be reduced by introducing a mapping between the semantic fields of the parties involved in the communication based on the commonly used terminologies. Our research aims to obtain a suitable semantic database in the form of a semantic network built from the Stack Overflow corpus, which can be considered to encompass the common tacit knowledge of the software development community. Terminologies used in the business world can be assigned to our semantic network, so software developers do not miss features that are not specific to their world but relevant to their clients. We present an initial experiment of mining semantic network from Stack Overflow and provide insights of the newly captured relations compared to WordNet.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3387940.3392160',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Mining Abstract XML Data-Types',\n",
       "  'authors': \"['Dionysis Athanasopoulos', 'Apostolos Zarras']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on the Web',\n",
       "  'abstract': 'Schema integration has been a long-standing challenge for the data-engineering community that has received steady attention over the past three decades. General-purpose integration approaches construct unified schemas that encompass all schema elements. Schema integration has been revisited in the past decade in service-oriented computing since the input/output data-types of service interfaces are heterogeneous XML schemas. However, service integration differs from the traditional integration problem, since it should generalize schemas (mining abstract data-types) instead of unifying all schema elements. To mine well-formed abstract data-types, the fundamental Liskov Substitution Principle (LSP), which generally holds between abstract data-types and their subtypes, should be followed. However, due to the heterogeneity of service data-types, the strict employment of LSP is not usually feasible. On top of that, XML offers a rich type system, based on which data-types are defined via combining type patterns (e.g., composition, aggregation). The existing integration approaches have not dealt with the challenges of a defining subtyping relation between XML type patterns. To address these challenges, we propose a relaxed version of LSP between XML type patterns and an automated generalization process for mining abstract XML data-types. We evaluate the effectiveness and the efficiency of the process on the schemas of two datasets against two representative state-of-the-art approaches.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3267467',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Classification of Multiple Affective Attributes of Customer Reviews: Using Classical Machine Learning and Deep Learning',\n",
       "  'authors': \"['Jiawen Wang', 'Wai Ming Wang', 'Zonggui Tian', 'Zhi Li']\",\n",
       "  'date': 'October 2018',\n",
       "  'source': \"CSAE '18: Proceedings of the 2nd International Conference on Computer Science and Application Engineering\",\n",
       "  'abstract': 'Affective1 engineering is a methodology of designing products by collecting customer affective needs and translating them into product designs. It usually begins with questionnaire surveys to collect customer affective demands and responses. However, this process is expensive, which can only be conducted periodically in a small scale. With the rapid development of e-commerce, a larger number of customer product reviews are available on the Internet. Many studies have been done using opinion mining and sentiment analysis. However, the existing studies focus on the polarity classification from a single perspective (such as positive and negative). The classification of multiple affective attributes receives less attention. In this paper, 3-class classifications of four different affective attributes (i.e. Soft-Hard, Appealing-Unappealing, Handy-Bulky, and Reliable-Shoddy) are performed by using two classical machine learning algorithms (i.e. Softmax regression and Support Vector Machine) and two deep learning methods (i.e. Restricted Boltzmann machines and Deep Belief Network) on an Amazon dataset. The results show that the accuracy of deep learning methods is above 90%, while the accuracy of classical machine learning methods is about 64%. This indicates that deep learning methods are significantly better than classical machine learning methods.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3207677.3277953',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Identifying skill sets for bioinformatics graduate students -a text mining approach',\n",
       "  'authors': \"['Richard Shang', 'Mohammed Ghriga']\",\n",
       "  'date': 'None',\n",
       "  'source': 'Journal of Computing Sciences in Colleges',\n",
       "  'abstract': 'This project proposes a set of skills to serve as a guideline for bioinformatics curriculum design at the graduate level. Bioinformatics, also known as computational biology, is a burgeoning inter-disciplinary field with a demonstrated market need for highly trained experts who can analyze biological data with skills in computation and informatics. Current trends in bioinformatics incorporate machine learning, large data set analytics and artificial intelligence in the diagnosis, treatment and prevention of illness. Students in graduate programs of Bioinformatics typically expect that courses will prepare them for future job markets with employable skills. In an effort to identify the skill sets sought after by employers in Bioinformatics filed, we apply a text mining approach to analyze required qualifications of Bioinformatics jobs posted online. Using the keyword \"bioinformatics\", we searched on Google Jobs and collected required qualifications of 38 Bioinformatics jobs. All the jobs indicate that a master\\'s degree is required or preferred. Among the job posts, 14 are under the title \"Analyst\", 13 under \"Scientist\", and 11 under \"Software Engineer\".',\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3344051.3344078',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Statistic Solution for Machine Learning to Analyze Heart Disease Data',\n",
       "  'authors': \"['Abdur Rasool', 'Ran Tao', 'Kaleem Kashif', 'Waqas Khan', 'Promise Agbedanu', 'Neeta Choudhry']\",\n",
       "  'date': 'February 2020',\n",
       "  'source': \"ICMLC '20: Proceedings of the 2020 12th International Conference on Machine Learning and Computing\",\n",
       "  'abstract': 'Data crawling, collection and analysis have become a popular pillar for the business intelligence of big data analysis which is the latest hot-topic among the research association. Numerous tools and techniques to solve and analyze the structured and unstructured datasets are developing very quickly. The previous studies show the different approaches in the identification of the strengths and weaknesses of multiple machine learning algorithms. But, most of the approaches demand more expert knowledge base information to understand the concepts of given data. In this paper, we modernize the machine learning methods for the effective prediction of heart disease. This work deliberates the detailed process of implementation of our proposed system. The goal of this work is to find a strong and effective machine learning algorithm for disease prediction for the problem; how can doctors get fast and better results for their diagnosis of heart disease. We design a new system for disease prediction using machine learning prediction algorithms (LR, ANN and SVC) by utilizing an effective approach of ETL, OLAP and data mining. The results showed that the best machine learning algorithm is SVC with 92% accuracy for the risk prediction model. We found that subjects at 56-64 years old have a high risk of heart disease, as well as men, have more heart disease rate than women. This proposed study can be favorable for the medical practitioners in the field of healthcare, supportive practice and precautions to the heart disease patients.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3383972.3384061',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Extended PrefixSpan for Efficient Sequential Pattern Mining in a Game-based Learning Environment',\n",
       "  'authors': \"['Raymond S. Bermudez', 'Ariel M. Sison', 'Ruji P. Medina']\",\n",
       "  'date': 'January 2020',\n",
       "  'source': \"APIT '20: Proceedings of the 2020 2nd Asia Pacific Information Technology Conference\",\n",
       "  'abstract': 'This paper proposed an extended version of PrefixSpan as a better sequential pattern mining for a game-based learning environment (GBLE). The extended version of PrefixSpan evolved on integrating time interval constraints, clustering valued actions and extracting the closed sequences. These three concepts were derived after a previous work showed limitations of PrefixSpan in generating sequence patterns that can be used in tutoring services of a GBLE. The extended PrefixSpan underwent two phases of evaluation, performance evaluation and analyzing the quality of generated sequence patterns. The evaluation results showed that the extended versions provided a significant improvement in terms of execution time and the number of generated sequence patterns. Lastly, it shows significant improvement in the quality of sequence patterns generated as shown in better tutoring service it provided after integrating it to the GBLE.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3379310.3381044',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Evolutionary mining of relaxed dependencies from big data collections',\n",
       "  'authors': \"['Loredana Caruccio', 'Vincenzo Deufemia', 'Giuseppe Polese']\",\n",
       "  'date': 'June 2017',\n",
       "  'source': \"WIMS '17: Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics\",\n",
       "  'abstract': 'Many modern application contexts, especially those related to the semantic Web, advocate for automatic techniques capable of extracting relationships between semi-structured data, for several purposes, such as the identification of inconsistencies or patterns of semantically related data, query rewriting, and so forth. One way to represent such relationships is to use relaxed functional dependencies (rfds), since they can embed approximate matching paradigms to compare unstructured data, and admit the possibility of exceptions for them. To this end, thresholds might need to be specified in order to limit the similarity degree in approximate comparisons or the occurrence of exceptions. Thanks to the availability of huge amount of data, including unstructured data available on the Web, nowadays it is possible to automatically discover rfds from data. However, due to the many different combinations of similarity and exception thresholds, the discovery process has an exponential complexity. Thus, it is vital devising proper optimization strategies, in order to make the discovery process feasible. To this end, in this paper, we propose a genetic algorithm to discover rfds from data, also providing an empirical evaluation demonstrating its effectiveness.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3102254.3102259',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Evaluating Extreme Learning Machine Models in the Presence of Concept Drift in Streaming Data',\n",
       "  'authors': \"['Tagrid Alshalali', 'Darsana Josyula']\",\n",
       "  'date': 'May 2020',\n",
       "  'source': \"ICISDM '20: Proceedings of the 2020 the 4th International Conference on Information System and Data Mining\",\n",
       "  'abstract': 'This paper discusses concept drift in online streaming data and evaluates the performance of different Extreme Learning Machine (ELM) based techniques on classifying online streaming data in the presence of concept drift. It also compares the performance of a hybrid model called Online Recurrent ELM (OR-ELM) with traditional recurrent neural networks, in terms of training speed and accuracy, on streaming data that has concept drift. The results of our experiments show that OR-ELM has better accuracy and faster training time.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3404663.3404682',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Research Hotspots Mining and Visualized Analysis Based on Linking Cluster and K-Core Decomposition',\n",
       "  'authors': \"['Ying Cai', 'Fang Huang', 'Shuai Wang', 'Hao Zhang', 'Chunxiu Du']\",\n",
       "  'date': 'May 2018',\n",
       "  'source': 'ICDPA 2018: Proceedings of the International Conference on Data Processing and Applications',\n",
       "  'abstract': 'In this paper, we focus on research hotspots mining with knowledge semantic features on discipline topic word networks, which are extracted from scientific and technical documents. For the topic networks, a novel method of combining linking community clustering with k-core decomposition is proposed to mine the hotspot community with knowledge hierarchy structure. And as a post-processing, the hierarchical community identification is implemented by k-core decomposition in order to discover the internal connection with knowledge semantic hierarchy in the research hotspot. Finally, the community density and visualization method is utilized to analyze the core and sub- problem in the research hotspot community. In experiments, the topic networks for a single discipline and interdisciplinary research hotspot are taken as examples of analysis, and the results show the proposed approach can effectively realize the objective of mining and analyzing the discipline research hotspot on the academic topic word network.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3224207.3224216',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Sentimental text processing tool for Russian language based on machine learning algorithms',\n",
       "  'authors': \"['Mohamed A. Hamada', 'Kamila Sultanbek', 'Bauyrzhan Alzhanov', 'Bauyrzhan Tokbanov']\",\n",
       "  'date': 'June 2019',\n",
       "  'source': \"ICEMIS '19: Proceedings of the 5th International Conference on Engineering and MIS\",\n",
       "  'abstract': 'Several studies have been published to analyze different approaches to traditional text classification methods. Most of these studies cover the application of certain methods of semantic terminology to textual classification to a certain extent. However, they are not specifically aimed at the classification algorithms for semantic text and their advantages over the traditional text classification. As it is known, Kazakhstan is a multinational country and Russian is a transnational language. Therefore, it was decided to develop a tool for processing the incoming text in different languages. Actually, there are some existing tools that process text in English, but they do not support some foreign languages, especially that are in use in Kazakhstan. The main goal is to analyze some Machine Learning (ML) algorithms and develop a sentimental text processing tool using those algorithms to make a web page for text processing through categorization and sentimental analysis. It was investigated that the best way to implement Machine Learning and Natural Language Processing (NLP) algorithms on Python is to create a web-page using Django framework. The principle of work is to get the text as input, process it with ML algorithms and send result as a «.json» file.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3330431.3335204',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Mining Relationships among Online Review Texts and Ratings in Indonesian E-commerce Websites',\n",
       "  'authors': \"['Cindy Hosea', 'Rofikoh Rokhim']\",\n",
       "  'date': 'August 2020',\n",
       "  'source': \"BDIOT '20: Proceedings of the 2020 4th International Conference on Big Data and Internet of Things\",\n",
       "  'abstract': \"As the most growing sector for Indonesia's internet economy during the last five years, e-commerce generates online customer reviews that can be a source for information and giving hints for potential improvements for various stakeholders. Online reviews consist of review text and rating, each of which describes customer's concern and satisfaction in purchasing items online. However, online review text is unstructured, and its relation with rating is hardly observed. This study examines 132,085 online reviews about Xiaomi mobile phones on three major e-commerce websites in Indonesia: Shopee, Bukalapak, and Blibli by text mining and quantitative modeling to correlate reviews with ratings. Online reviews are classified into eight distinct topics, and the relationships between each topic and review rating are analyzed. Multilinear regression is implemented to examine the valence and strength in the relationship between each topic-rating. The result shows that there are more topics with a negative relationship with rating, with several topic differences between the three websites. Further improvements should be focused on the most impactful topics, which are referred to the mobile phone features, such as CPU & hardware, system, and physical appearance. After-sales service is also concerned at Bukalapak and Blibli. These relationships are explained further by the valence expressed by customers in review texts. The implications of this study can be applied for academic purposes, e-commerce companies, customers, sellers, and mobile phone companies.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3421537.3421543',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Feature-based Restaurant Customer Reviews Process Model using Data Mining',\n",
       "  'authors': \"['Anish Kumar Varudharajulu', 'Yongsheng Ma']\",\n",
       "  'date': 'September 2018',\n",
       "  'source': \"ICCBD '18: Proceedings of the 2018 International Conference on Computing and Big Data\",\n",
       "  'abstract': 'Mining social media is a popular strategy to revitalize any business. The social media lodges colossal amount of user spawn data which can be used for data mining. The purpose of this research paper is to develop a feature-based software model to analyze customer reviews of an organization using their Facebook page and provide valuable insights for decision making, product quality development, and process improvements. Thus enabling concurrent engineering activities and enhancing collaboration between various departments within the organization. As a sample case study, we have analyzed the customer reviews of a restaurant using the J48 classification algorithm and K-means clustering algorithm to identify areas which need improvement. Results show that customers are giving more importance to features such as the taste, variety of drinks, price, and service, In addition, customers are least bothered about the location, offers, and ambiance of the South Indian restaurant under study.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3277104.3277113',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Research on Cold Chain Logistics Risk in E-commerce Using Text Mining Technology',\n",
       "  'authors': \"['Jingyu Bo']\",\n",
       "  'date': 'January 2020',\n",
       "  'source': \"ICCMB '20: Proceedings of the 2020 the 3rd International Conference on Computers in Management and Business\",\n",
       "  'abstract': 'In recent years, with the rapid development of e-commerce and Internet, more and more products transfer to the information-based transformation. Numerous of goods such as fresh products are turning to online sales model. Selling fresh agricultural products on e-commerce platform can not only broaden the sales channels and increase the profits of peasants but also enable people to buy high-quality fresh products. However, at the same time we cannot ignore that due to the relatively slow development of cold supply chain, there are still a lot of problems. For example, untimely and unreasonable transportation will cause fresh products are corrupted and deteriorated in the transportation process every year, which damages the interests of supply chain members. Accurate identification and quantitative analysis of supply chain hazards have great significance in the improvement of transport efficiency of supply chain and reducing the transport cost of supply chain. This paper indicates the methods of literature analysis to collect and summarize the relevant keywords in the theory of supply chain risk and uses text mining technology to collect and analyze supply chain risk in CNKI literatures. It catches those literatures containing keywords about supply chain risk in database, stores the related information in the Excel. Then it calculates the frequency of each keyword appears in the final statistics. Then it removes the supply chain risk factors that have less attention. At last, using the analytic hierarchy process (AHP) to quantitatively analyze and evaluate the risks in the cold supply chain of fresh agricultural products. Finally, the weight of risks in the cold supply chain of fresh agricultural products is calculated.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3383845.3383853',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Application of Data Mining in English Linguistics Teaching and Appraisal System',\n",
       "  'authors': \"['Wei Zhang', 'Xue Wang']\",\n",
       "  'date': 'October 2020',\n",
       "  'source': 'AIAM2020: Proceedings of the 2nd International Conference on Artificial Intelligence and Advanced Manufacture',\n",
       "  'abstract': 'The data mining algorithm based on rough set plays a very important role in dealing with various application-oriented problems. The suitable algorithm can quickly and accurately mine the core of time attribute and simplify the problem. Based on the characteristics of the teaching and appraise system of English linguistics, this paper optimizes the teaching design of English Linguistics in terms of teaching. Under the framework of systemic functional linguistics, this paper makes a follow-up analysis of the appraise resources in English texts, and verifies the feasibility and effectiveness of this method through an example. Some key problems of English linguistics teaching and price system are solved by data mining algorithm.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3421766.3421824',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Model-based Clustering of Short Text Streams',\n",
       "  'authors': \"['Jianhua Yin', 'Daren Chao', 'Zhongkun Liu', 'Wei Zhang', 'Xiaohui Yu', 'Jianyong Wang']\",\n",
       "  'date': 'July 2018',\n",
       "  'source': \"KDD '18: Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining\",\n",
       "  'abstract': 'Short text stream clustering has become an increasingly important problem due to the explosive growth of short text in diverse social medias. In this paper, we propose a model-based short text stream clustering algorithm (MStream) which can deal with the concept drift problem and sparsity problem naturally. The MStream algorithm can achieve state-of-the-art performance with only one pass of the stream, and can have even better performance when we allow multiple iterations of each batch. We further propose an improved algorithm of MStream with forgetting rules called MStreamF, which can efficiently delete outdated documents by deleting clusters of outdated batches. Our extensive experimental study shows that MStream and MStreamF can achieve better performance than three baselines on several real datasets.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3219819.3220094',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Bayesian Attribute Bagging-Based Extreme Learning Machine for High-Dimensional Classification and Regression',\n",
       "  'authors': \"['Yulin He', 'Xuan Ye', 'Joshua Zhexue Huang', 'Philippe Fournier-Viger']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Intelligent Systems and Technology',\n",
       "  'abstract': 'This article presents a Bayesian attribute bagging-based extreme learning machine (BAB-ELM) to handle high-dimensional classification and regression problems. First, the decision-making degree (DMD) of a condition attribute is calculated based on the Bayesian decision theory, i.e., the conditional probability of the condition attribute given the decision attribute. Second, the condition attribute with the highest DMD is put into the condition attribute group (CAG) corresponding to the specific decision attribute. Third, the bagging attribute groups (BAGs) are used to train an ensemble learning model of extreme learning machines (ELMs). Each base ELM is trained on a BAG which is composed of condition attributes that are randomly selected from the CAGs. Fourth, the information amount ratios of bagging condition attributes to all condition attributes is used as the weights to fuse the predictions of base ELMs in BAB-ELM. Exhaustive experiments have been conducted to compare the feasibility and effectiveness of BAB-ELM with seven other ELM models, i.e., ELM, ensemble-based ELM (EN-ELM), voting-based ELM (V-ELM), ensemble ELM (E-ELM), ensemble ELM based on multi-activation functions (MAF-EELM), bagging ELM, and simple ensemble ELM. Experimental results show that BAB-ELM is convergent with the increase of base ELMs and also can yield higher classification accuracy and lower regression error for high-dimensional classification and regression problems.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3495164',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Aggregating Filter Feature Selection Methods to Enhance Multiclass Text Classification',\n",
       "  'authors': \"['Rhodessa J. Cascaro', 'Bobby D. Gerardo', 'Ruji P. Medina']\",\n",
       "  'date': 'December 2019',\n",
       "  'source': \"ICIT '19: Proceedings of the 2019 7th International Conference on Information Technology: IoT and Smart City\",\n",
       "  'abstract': 'Text data usage has increased rapidly and simultaneously resulted in setbacks, such as high dimensionality of text data becoming a prominent problem. Hence, this study aimed to assess the application of filter feature selection techniques on text data. In this study, features were ranked from highest to lowest by the selected filter feature selection methods in each generated feature subset. Thereafter, a new feature subset was obtained using the proposed method. This study yielded that the accuracy of Information Gain is 1.12 percentage points higher in comparison to the accuracy of Chi-square. Moreover, classification accuracy obtained from aggregation exhibits a rise of 0.93 percentage points compared to the accuracy of Information Gain and 2.05 percentage points against Chi-square. Classification accuracy improved when the features are aggregated. On Precision, in comparison to that of the aggregation, results show the differences in percentage points of 1.41 and significant 11.64 for Information Gain and Chi-square respectively. About Recall, there is a 5.54 percentage points improvement on Information Gain and 3.03 percentage points improvement on Chi-square. Then, in F1, the score for aggregation is quite low. It may mean that the classifier has problems with false positives or false negatives. Thus, the classifier needs to be checked using a confusion matrix or check on the dataset, which was not done in the experiment. Dataset imbalance was also not addressed in this study. For future work, the imbalanced class-dataset issue should be addressed. Also, the performance of other filter methods could be compared as well as utilize other classifiers that support multiclass tasks to determine which is suitable for multiclass text classification.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3377170.3377209',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': \"Classification of public elementary students' game play patterns in a digital game-based learning system with pedagogical agent\",\n",
       "  'authors': \"['Prometheus Peter L. Lazo', 'Chris Lionel Q. Anareta', 'Jule Brianne T. Duremdes', 'Ellenita R. Red']\",\n",
       "  'date': 'January 2018',\n",
       "  'source': \"ICIET '18: Proceedings of the 6th International Conference on Information and Education Technology\",\n",
       "  'abstract': \"This study investigates gameplay attributes that were used to classify student performance in a digital game-based learning system to determine if it will contribute to achieving learning gain. The study was conducted in selected public elementary schools which comprised of 10% of all grade four students in each school visited. Word Infection Version 4, a local-area-network digital game-based learning (DGBL) system with a pedagogical agent, and a pretest and posttest module which served as the tool to collect gameplay logs of students were developed. Also, a dashboard tool was developed to manage, facilitate and administer the game in a distributed network. Usability test results showed strong agreement on its usability, aesthetics and usefulness. Log attributes, gameplay patterns, and performance of elementary students' vocabulary learning were recorded then described using K-means algorithm to determine the different clusters of students' gameplay patterns and performance while using the system. Four clusters were produced to represent the different gameplay styles of the students: gaming, proficient, productive and idle. A model that classified game play patterns of student's performance using Naïve Bayes and J48 algorithms was produced. The accuracy and kappa statistic of the produced models were determined. Higher ratings in accuracy and kappa statistic were yielded by the decision tree algorithm; 52.34% and 0.216 respectively in comparison 42.88% and 0.062 respectively from Naïve Bayes.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3178158.3178160',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Information-theoretic classification accuracy: a criterion that guides data-driven combination of ambiguous outcome labels in multi-class classification',\n",
       "  'authors': \"['Chihao Zhang', 'Yiling Elaine Chen', 'Shihua Zhang', 'Jingyi Jessica Li']\",\n",
       "  'date': 'None',\n",
       "  'source': 'The Journal of Machine Learning Research',\n",
       "  'abstract': 'Outcome labeling ambiguity and subjectivity are ubiquitous in real-world datasets. While practitioners commonly combine ambiguous outcome labels for all data points (instances) in an ad hoc way to improve the accuracy of multi-class classification, there lacks a principled approach to guide the label combination for all data points by any optimality criterion. To address this problem, we propose the information-theoretic classification accuracy (ITCA), a criterion that balances the trade-off between prediction accuracy (how well do predicted labels agree with actual labels) and classification resolution (how many labels are predictable), to guide practitioners on how to combine ambiguous outcome labels. To find the optimal label combination indicated by ITCA, we propose two search strategies: greedy search and breadth-first search. Notably, ITCA and the two search strategies are adaptive to all machine-learning classification algorithms. Coupled with a classification algorithm and a search strategy, ITCA has two uses: improving prediction accuracy and identifying ambiguous labels. We first verify that ITCA achieves high accuracy with both search strategies in finding the correct label combinations on synthetic and real data. Then we demonstrate the effectiveness of ITCA in diverse applications, including medical prognosis, cancer survival prediction, user demographics prediction, and cell type classification. We also provide theoretical insights into ITCA by studying the oracle and the linear discriminant analysis classification algorithms. Python package itca (available at https://github.com/JSB-UCLA/ITCA) implements ITCA and the search strategies.',\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3586589.3586930',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'SynTF: Synthetic and Differentially Private Term Frequency Vectors for Privacy-Preserving Text Mining',\n",
       "  'authors': \"['Benjamin Weggenmann', 'Florian Kerschbaum']\",\n",
       "  'date': 'June 2018',\n",
       "  'source': \"SIGIR '18: The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval\",\n",
       "  'abstract': \"Text mining and information retrieval techniques have been developed to assist us with analyzing, organizing and retrieving documents with the help of computers. In many cases, it is desirable that the authors of such documents remain anonymous: Search logs can reveal sensitive details about a user, critical articles or messages about a company or government might have severe or fatal consequences for a critic, and negative feedback in customer surveys might negatively impact business relations if they are identified. Simply removing personally identifying information from a document is, however, insufficient to protect the writer's identity: Given some reference texts of suspect authors, so-called authorship attribution methods can reidentfy the author from the text itself. One of the most prominent models to represent documents in many common text mining and information retrieval tasks is the vector space model where each document is represented as a vector, typically containing its term frequencies or related quantities. We therefore propose an automated text anonymization approach that produces synthetic term frequency vectors for the input documents that can be used in lieu of the original vectors. We evaluate our method on an exemplary text classification task and demonstrate that it only has a low impact on its accuracy. In contrast, we show that our method strongly affects authorship attribution techniques to the level that they become infeasible with a much stronger decline in accuracy. Other than previous authorship obfuscation methods, our approach is the first that fulfills differential privacy and hence comes with a provable plausible deniability guarantee.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3209978.3210008',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Maximally informative k-itemset mining from massively distributed data streams',\n",
       "  'authors': \"['Mehdi Zitouni', 'Reza Akbarinia', 'Sadok Ben Yahia', 'Florent Masseglia']\",\n",
       "  'date': 'April 2018',\n",
       "  'source': \"SAC '18: Proceedings of the 33rd Annual ACM Symposium on Applied Computing\",\n",
       "  'abstract': 'We address the problem of mining maximally informative k-itemsets (miki) in data streams based on joint entropy. We propose PentroS, a highly scalable parallel miki mining algorithm. PentroS renders the mining process of large volumes of incoming data very efficient. It is designed to take into account the continuous aspect of data streams, particularly by reducing the computations of need for updating the miki results after arrival/departure of transactions to/from the sliding window. PentroS has been extensively evaluated using massive real-world data streams. Our experimental results confirm the effectiveness of our proposal which allows excellent throughput with high itemset length.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3167132.3167187',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Application analysis of computer web data mining technology in E-commerce',\n",
       "  'authors': \"['Huiting Ju', 'Hui Wang']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"EITCE '21: Proceedings of the 2021 5th International Conference on Electronic Information Technology and Computer Engineering\",\n",
       "  'abstract': \"With the development of the Internet, the proportion of e-commerce in people's life is becoming larger and larger. Online shopping has become a major way of shopping and helped people improve their life efficiency. Web data mining technology is a very important technology in e-commerce. The web can deeply mine and analyze the access methods, interest preferences, group characteristics and so on of e-commerce users. These information can help e-commerce push different user groups, and greatly improve the overall promotion effect and benefit of e-commerce. Starting from the web data mining technology, this paper expounds the application process of Web data mining technology used in e-commerce in detail, so as to provide a certain reference for the development of e-commerce.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3501409.3501626',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'InFoRM: Individual Fairness on Graph Mining',\n",
       "  'authors': \"['Jian Kang', 'Jingrui He', 'Ross Maciejewski', 'Hanghang Tong']\",\n",
       "  'date': 'August 2020',\n",
       "  'source': \"KDD '20: Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining\",\n",
       "  'abstract': 'Algorithmic bias and fairness in the context of graph mining have largely remained nascent. The sparse literature on fair graph mining has almost exclusively focused on group-based fairness notation. However, the notion of individual fairness, which promises the fairness notion at a much finer granularity, has not been well studied. This paper presents the first principled study of Individual Fairness on gRaph Mining (InFoRM). First, we present a generic definition of individual fairness for graph mining which naturally leads to a quantitative measure of the potential bias in graph mining results. Second, we propose three mutually complementary algorithmic frameworks to mitigate the proposed individual bias measure, namely debiasing the input graph, debiasing the mining model and debiasing the mining results. Each algorithmic framework is formulated from the optimization perspective, using effective and efficient solvers, which are applicable to multiple graph mining tasks. Third, accommodating individual fairness is likely to change the original graph mining results without the fairness consideration. We conduct a thorough analysis to develop an upper bound to characterize the cost (i.e., the difference between the graph mining results with and without the fairness consideration). We perform extensive experimental evaluations on real-world datasets to demonstrate the efficacy and generality of the proposed methods.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3394486.3403080',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Sequential Cross-Modal Hashing Learning via Multi-scale Correlation Mining',\n",
       "  'authors': \"['Zhaoda Ye', 'Yuxin Peng']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Multimedia Computing, Communications, and Applications',\n",
       "  'abstract': 'Cross-modal hashing aims to map heterogeneous multimedia data into a common Hamming space through hash function, and achieves fast and flexible cross-modal retrieval. Most existing cross-modal hashing methods learn hash function by mining the correlation among multimedia data, but ignore the important property of multimedia data: Each modality of multimedia data has features of different scales, such as texture, object, and scene features in the image, which can provide complementary information for boosting retrieval task. The correlations among the multi-scale features are more abundant than the correlations between single features of multimedia data, which reveal finer underlying structures of the multimedia data and can be used for effective hashing function learning. Therefore, we propose the Multi-scale Correlation Sequential Cross-modal Hashing (MCSCH) approach, and its main contributions can be summarized as follows: (1) Multi-scale feature guided sequential hashing learning method is proposed to share the information from features of different scales through an RNN-based network and generate the hash codes sequentially. The features of different scales are used to guide the hash codes generation, which can enhance the diversity of the hash codes and weaken the influence of errors in specific features, such as false object features caused by occlusion. (2) Multi-scale correlation mining strategy is proposed to align the features of different scales in different modalities and mine the correlations among aligned features. These correlations reveal the finer underlying structure of multimedia data and can help to boost the hash function learning. (3) Correlation evaluation network evaluates the importance of the correlations to select the worthwhile correlations, and increases the impact of these correlations for hash function learning. Experiments on two widely-used 2-media datasets and a 5-media dataset demonstrate the effectiveness of our proposed MCSCH approach.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3356338',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A New System For Audio Mining: Using of MDT+ for automatic prediction of new singers success',\n",
       "  'authors': \"['Faiz Maazouzi', 'Hafed Zarzour', 'Halima Bahi']\",\n",
       "  'date': 'December 2018',\n",
       "  'source': 'ICSENT 2018: Proceedings of the 7th International Conference on Software Engineering and New Technologies',\n",
       "  'abstract': 'Online music diffusion and distribution is becoming increasingly important and commercial and personal databases are increasing in a considerable way. Nowadays, it\\'s necessary to have tools that allow classifying and reaching these bases by carrying out analyzes through musical contents. The work included in this paper consists of automatic prediction of new singers success. This work lies within the scope of audio mining including of the heterogeneous data. Work can be divided into two parts: in the first part, we treat the audio data to make a classification of the Algerian singers\\' voices. There exist two categories of the singing voices classification (type and quality), each category has several classes. Within this framework, we proposed to use a vector of characteristics which contains the descriptors of MPEG-7 + the descriptors Not-MPEG-7 and Standard T2 FGMMs \"Type 2 Fuzzy Gaussian Mixture Models\" for modeling and classification of singing voice. By using the results of the singing voices classification with the statistics of every singer, a new database has been created. The second part of work consists of the data mining to answer our starting question \"are our young people\\'s musical tastes poor\". In this part, we proposed a new method of data excavation, based on the decision trees called: Multi Decision Tree (MDT+). Finally, we used the MDT+ method to excavate our database.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3330089.3330102',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Overlapping Clustering for Textual Data',\n",
       "  'authors': \"['Atefeh Khazaei', 'Mohammad Ghasemzadeh', 'Dieter Gollmann']\",\n",
       "  'date': 'February 2018',\n",
       "  'source': \"ICSCA '18: Proceedings of the 2018 7th International Conference on Software and Computer Applications\",\n",
       "  'abstract': 'Texts have inherent overlapping, therefore for clustering textual data, the overlapping clustering algorithms are more appropriate. In this regard, a major challenge is that they are very slow in clustering big volumes of textual data. Among others, OKM and OSOM are two important overlapping clustering algorithms. In this study, we have implemented and compared the performance of these two algorithms. The experimental results of our study show that OKM clusters have better overlap sizes when these algorithms are used for clustering textual data. Since both of them require much time to complete, none of these two algorithms is suitable for clustering textual data. Therefore we mastermind a fast overlapping version of SOM which is suitable for this purpose.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3185089.3185113',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Semi-Unsupervised Lifelong Learning for Sentiment Classification: Less Manual Data Annotation and More Self-Studying',\n",
       "  'authors': \"['Xianbin Hong', 'Gautam Pal', 'Sheng-Uei Guan', 'Prudence Wong', 'Dawei Liu', 'Ka Lok Man', 'Xin Huang']\",\n",
       "  'date': 'June 2019',\n",
       "  'source': \"HPCCT '19: Proceedings of the 2019 3rd High Performance Computing and Cluster Technologies Conference\",\n",
       "  'abstract': 'Lifelong machine learning is a novel machine learning paradigm which can continually accumulate knowledge during learning. The knowledge extracting and reusing abilities enable the lifelong machine learning to solve the related problems. The traditional approaches like Naïve Bayes and some neural network based approaches only aim to achieve the best performance upon a single task. Unlike them, the lifelong machine learning in this paper focus on how to accumulate knowledge during learning and leverage them for the further tasks. Meanwhile, the demand for labeled data for training also be significantly decreased with the knowledge reusing. This paper suggests that the aim of the lifelong learning is to use less labeled data and computational cost to achieve the performance as well as or even better than the supervised learning.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3341069.3342992',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Multi-Modal Knowledge Representation Learning via Webly-Supervised Relationships Mining',\n",
       "  'authors': \"['Fudong Nian', 'Bing-Kun Bao', 'Teng Li', 'Changsheng Xu']\",\n",
       "  'date': 'October 2017',\n",
       "  'source': \"MM '17: Proceedings of the 25th ACM international conference on Multimedia\",\n",
       "  'abstract': 'Knowledge representation learning (KRL) encodes enormous structured information with entities and relations into a continuous low-dimensional semantic space. Most conventional methods solely focus on learning knowledge representation from single modality, yet neglect the complementary information from others. The more and more rich available multi-modal data on Internet also drive us to explore a novel approach for KRL in multi-modal way, and overcome the limitations of previous single-modal based methods. This paper proposes a novel multi-modal knowledge representation learning (MM-KRL) framework which attempts to handle knowledge from both textual and visual modal web data. It consists of two stages, i.e., webly-supervised multi-modal relationship mining, and bi-enhanced cross-modal knowledge representation learning. Compared with existing knowledge representation methods, our framework has several advantages: (1) It can effectively mine multi-modal knowledge with structured textual and visual relationships from web automatically. (2) It is able to learn a common knowledge space which is independent to both task and modality by the proposed Bi-enhanced Cross-modal Deep Neural Network (BC-DNN). (3) It has the ability to represent unseen multi-modal relationships by transferring the learned knowledge with isolated seen entities and relations into unseen relationships. We build a large-scale multi-modal relationship dataset (MMR-D) and the experimental results show that our framework achieves excellent performance in zero-shot multi-modal retrieval and visual relationship recognition.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3123266.3123443',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Fishing Techniques Classification Based on Beidou Trajectories and Machine Learning',\n",
       "  'authors': \"['Yao Li', 'Nanyu Chen', 'Luo Chen']\",\n",
       "  'date': 'April 2020',\n",
       "  'source': \"ICGDA '20: Proceedings of the 2020 3rd International Conference on Geoinformatics and Data Analysis\",\n",
       "  'abstract': 'Beidou navigation information is widely used in Chinese fisheries. It helps fishermen to locate themselves and hedge. Compared with the data ashore, analysis of satellite data on the ocean is still not enough. Inspired by vehicle trajectory analysis, we want to do some analysis on boat trajectories. Fishing techniques are various, so their trajectories. In this paper, we choose 3 fishing techniques (Trawling, seining and gillnetting). We implement classification with two different algorithms. One of them is ResNet which belongs to image classification with deep learning. The other one is LightGBM which is a kind of decision tree algorithm. The result shows that although deep learning has made great success in daily life images classification, it adds too much redundant pixels and ignore the speed and direction parameters in this task. This leads to a lower precision and more calculations. In contrast, LightGBM can use information effectively and has a higher score with a higher speed. This work shows traditional machine learning algorithm can achieve better result than deep learning algorithm in some circumstance. It will also contribute to establish a smart ocean system.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3397056.3397072',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Text mining as a transparency enabler to support decision making in a people management process',\n",
       "  'authors': \"['José Barroso Júnior', 'Claudia Cappelli', 'Kate Revoredo', 'Vanessa Nunes']\",\n",
       "  'date': 'May 2018',\n",
       "  'source': \"dg.o '18: Proceedings of the 19th Annual International Conference on Digital Government Research: Governance in the Data Age\",\n",
       "  'abstract': 'This paper discusses a case study that was performed using mining techniques to analyze data pertinent to a people management process of a Federal University. This process consists in observing documents containing data concerning the organizational environment, the duties required by the position, the course program carried out by the employee, and whether they have direct or indirect correlation. Currently, this correlation evaluation is performed subjectively and there are no instruments that can indicate the degree of similarity between the information. We use text mining techniques to automatically identify correlation through textual representation approaches and syntactic and semantic modeling, which retrieve terms and dimension their respective meanings. To obtain the degree of similarity between the respective documents, the measure of the cosines similarity was used. The results showed that the documents evaluated as correlated by the domain expert presented a degree of similarity consistent with the automatic evaluation. For the uncorrelated cases, it was perceived that the degree of high similarity was influenced by the comprehensiveness of the organizational environment common to all documents. After investigation and identification of the appropriate environment specification, the grades obtained represented the evaluation correctly. The proposed approach contributes to the speed of process judgment, as well as to promote formulations of criticism about the content of political qualifications. In addition, it enhanced processes and information transparency by tracking and publicizing all steps. Lastly, we present a simulation for a course recommendation task, considering position profiles and organizational environment.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3209281.3209367',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Minimal learning machine: theoretical results and clustering-based reference point selection',\n",
       "  'authors': \"['Joonas Hämäläin', 'Alisson S. C. Alencar', 'Tommi Kärkkäinen', 'César L. C. Mattos', 'Amauri H. Souza Júnior', 'João P. P. Gomes']\",\n",
       "  'date': 'None',\n",
       "  'source': 'The Journal of Machine Learning Research',\n",
       "  'abstract': \"The Minimal Learning Machine (MLM) is a nonlinear, supervised approach based on learning linear mapping between distance matrices computed in input and output data spaces, where distances are calculated using a subset of points called reference points. Its simple formulation has attracted several recent works on extensions and applications. In this paper, we aim to address some open questions related to the MLM. First, we detail the theoretical aspects that assure the MLM's interpolation and universal approximation capabilities, which had previously only been empirically verified. Second, we identify the major importance of the task of selecting reference points for the MLM's generalization capability. Several clustering-based methods for reference point selection in regression scenarios are then proposed and analyzed. Based on an extensive empirical evaluation, we conclude that the evaluated methods are both scalable and useful. Specifically, for a small number of reference points, the clustering-based methods outperform the standard random selection of the original MLM formulation.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3455716.3455955',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Twitter Data Classification Using Big Data Technologies',\n",
       "  'authors': \"['Madani Youness', 'Erritali Mohammed', 'Bengourram Jamaa']\",\n",
       "  'date': 'April 2018',\n",
       "  'source': \"ICIEB '18: Proceedings of the 2018 1st International Conference on Internet and e-Business\",\n",
       "  'abstract': \"Tweets classification or in general the classification of the social network's data is a recent field of scientific research, where researchers look for new methods to classify users data (tweets, Facebook's post...) into classes (positive, negative, neutral).This type of scientific research called sentiment analysis (SA) or opinion mining and it allows to extract the feelings, opinions or attitudes expressed in a tweet or a facebook post ... In this article, we describe how we can collect and store a large volume of data, which is in the form of tweets, in Hadoop Distributed File System (HDFS), and how we can classify these tweets using different classification methods, making a comparison between the well-known machine learning algorithms and a dictionary based-approach using the AFINN dictionary. The experimental results show that the AFINN dictionary outperforms the well-known machine learning algorithms.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3230348.3230368',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A study of leaf classification based on multi machine learning models',\n",
       "  'authors': \"['Yixuan Zhao']\",\n",
       "  'date': 'October 2020',\n",
       "  'source': 'ICASIT 2020: Proceedings of the 2020 International Conference on Aviation Safety and Information Technology',\n",
       "  'abstract': 'Leaf classification identification is important for identifying new or scarce tree species. In this paper, a traditional machine learning approach is utilized to classify tree leaves exploring the data set of leaf characteristics provided by Kaggle. First, the data is analyzed and pre-processed. Second, different classifiers are chosen to classify the data set and compare the classification results. Finally, the classification results are validated using the test set data, i.e., leaf species prediction is performed for each image. The experimental comparison revealed that the selected Linear Discriminant Analysis model achieved good results on this data set. It provides a reference for leaf species identification and classification or other related studies.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3434581.3434636',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Iteratively Divide-and-Conquer Learning for Nonlinear Classification and Ranking',\n",
       "  'authors': \"['Ou Wu', 'Xue Mao', 'Weiming Hu']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Intelligent Systems and Technology',\n",
       "  'abstract': 'Nonlinear classifiers (i.e., kernel support vector machines (SVMs)) are effective for nonlinear data classification. However, nonlinear classifiers are usually prohibitively expensive when dealing with large nonlinear data. Ensembles of linear classifiers have been proposed to address this inefficiency, which is called the ensemble linear classifiers for nonlinear data problem. In this article, a new iterative learning approach is introduced that involves two steps at each iteration: partitioning the data into clusters according to Gaussian mixture models with local consistency and then training basic classifiers (i.e., linear SVMs) for each cluster. The two divide-and-conquer steps are combined into a graphical model. Meanwhile, with training, each classifier is regarded as a task; clustered multitask learning is employed to capture the relatedness among different tasks and avoid overfitting in each task. In addition, two novel extensions are introduced based on the proposed approach. First, the approach is extended for quality-aware web data classification. In this problem, the types of web data vary in terms of information quality. The ignorance of the variations of information quality of web data leads to poor classification models. The proposed approach can effectively integrate quality-aware factors into web data classification. Second, the approach is extended for listwise learning to rank to construct an ensemble of linear ranking models, whereas most existing listwise ranking methods construct a solely linear ranking model. Experimental results on benchmark datasets show that our approach outperforms state-of-the-art algorithms. During prediction for nonlinear classification, it also obtains comparable classification performance to kernel SVMs, with much higher efficiency.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3122802',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Machine Learning Approach for Malware Detection Using Random Forest Classifier on Process List Data Structure',\n",
       "  'authors': \"['Santosh Joshi', 'Himanshu Upadhyay', 'Leonel Lagos', 'Naga Suryamitra Akkipeddi', 'Valerie Guerra']\",\n",
       "  'date': 'April 2018',\n",
       "  'source': \"ICISDM '18: Proceedings of the 2nd International Conference on Information System and Data Mining\",\n",
       "  'abstract': 'As computer systems have become an integral part of every organization, it is a big challenge to safeguard the computer systems from malicious activities which compromise not only the systems but also the data stored within. Traditional malware and rootkit detection using antivirus systems are not dynamic enough to capture the complex behavior of malware and its isolated activities. There are many signature-based malware detection techniques have been introduced, but enterprises as well as general users are still facing problems to get protection for their cyber systems against malware. Thus, it emphasizes the necessity of developing an efficient malware detection technique. In this research paper, we design a machine learning approach for malware detection using Random Forest classifier for the process list data extracted from Linux based virtual machine environment.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3206098.3206113',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Exploiting Translation Model for Parallel Corpus Mining',\n",
       "  'authors': \"['Chongman Leong', 'Xuebo Liu', 'Derek F. Wong', 'Lidia S. Chao']\",\n",
       "  'date': 'None',\n",
       "  'source': 'IEEE/ACM Transactions on Audio, Speech and Language Processing',\n",
       "  'abstract': 'Parallel corpus mining (PCM) is beneficial for many corpus-based natural language processing tasks, e.g., machine translation and bilingual dictionary induction, especially in low-resource languages and domains. It relies heavily on cross-lingual representations to model the interdependencies between different languages and determine whether sentences are parallel or not. In this paper, we take the first step towards exploiting the multilingual Transformer translation model to produce expressive sentence representations for PCM. Since the traditional Transformer lacks an immediate sentence representation, we pool the output representation of the encoder as the sentence representation, which is further optimized as a part of the training flow of the translation model. Experiments conducted on the BUCC PCM task show that the proposed method improves mining performance over the existing methods with the assistance of the pre-trained multilingual BERT. To further test the usability of the proposed method, we mine parallel sentences from public resources and find that the mined sentences can indeed enhance low-resource machine translation.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/TASLP.2021.3105798',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Scholarly data mining: making sense of scientific literature',\n",
       "  'authors': \"['Horacio Saggion', 'Francesco Ronzano']\",\n",
       "  'date': 'June 2017',\n",
       "  'source': \"JCDL '17: Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries\",\n",
       "  'abstract': 'During the last decade the amount of scientific information available on-line increased at an unprecedented rate and this situation is unlikely to change. As a consequence, nowadays researchers are overwhelmed by an enormous and continuously growing number of publications to consider when they perform research activities like the exploration of advances in specific topics, peer reviewing, writing and evaluation of proposals. Natural Language Processing technology plays a key role in enabling intelligent access to the content of scientific publications. By mining the contents of scientific papers, for example, rich scientific knowledge bases can be built, thus supporting more effective information discovery and question answering approaches. Moreover, text summarization technology can help condense long papers to their essential contents so as to speed up the selection of scientific articles of interest or to assist in the manual or automatic generation of state of the art reports. Paraphrase and textual entailment techniques can contribute to the identification of relations across different scientific textual sources, thus, for instance, identifying implicit links between publications. This tutorial provides an overview of approaches to the extraction of knowledge from scientific literature, including the in-depth analysis of the structure of the scientific articles, their semantic interpretation, content extraction, summarization, and visualization.',\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3200334.3200406',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Social Media Mining: Prediction of Box Office Revenue',\n",
       "  'authors': \"['Deepankar Choudhery', 'Carson K. Leung']\",\n",
       "  'date': 'July 2017',\n",
       "  'source': \"IDEAS '17: Proceedings of the 21st International Database Engineering &amp; Applications Symposium\",\n",
       "  'abstract': 'In recent years, social media has played a huge role in how we share and communicate our thoughts and opinions. This information can very valuable for companies and governments as it can be used to analyze public mood and opinion which is a very powerful tool. In this paper, we present a system that mines social media content from a platform such as Twitter for predicting future outcomes. Specifically, it uses chatter from Twitter to predict box office revenue of movies by extracting features such as tweets and their sentiments. Then, by using these features, our system constructs a polynomial regression model for predicting box office revenue. Experimental results show the effectiveness of our system in mining social media and predicting box office revenue.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3105831.3105854',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Mining Moodle Logs for Grade Prediction: A methodology walk-through',\n",
       "  'authors': \"['Álvaro Figueira']\",\n",
       "  'date': 'October 2017',\n",
       "  'source': 'TEEM 2017: Proceedings of the 5th International Conference on Technological Ecosystems for Enhancing Multiculturality',\n",
       "  'abstract': 'Research concerning mining data from learning management systems have been consistently been appearing in the literature. However, in many situations there is not a clear path on the data mining procedures that lead to solid conclusions. Therefore, many studies result in ad-hoc conclusions with insufficient generalization capabilities. In this article, we describe a methodology and report our findings in an experiment which one online course which involved more than 150 students. We used the Moodle LMS during the period of one academic semester, collecting all the interactions between the students and the system. These data scales up to more than 33K records of interactions where we applied data mining tools following the procedure for data extraction, cleaning, feature identification and preparation. We then proceeded to the creation of automatic learning models based on decision trees, we assessed the models and validate the results by assessing the accuracy of the predictions using traditional metrics and draw our conclusions on the validity of the process and possible alternatives1.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3144826.3145394',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'An Effective Bridge Cracks Classification Method Based on Machine Learning',\n",
       "  'authors': \"['Xiaoyan Zhang', 'Xiaodong Wang']\",\n",
       "  'date': 'November 2020',\n",
       "  'source': \"EITCE '20: Proceedings of the 2020 4th International Conference on Electronic Information Technology and Computer Engineering\",\n",
       "  'abstract': 'Crack is the most common threat to the safety of bridges. Historical data show that the safety accidents caused by cracks account for more than 90% of the total bridge disasters. After a long period of engineering practice and rigorous theoretical analysis, it was found that 0.3 mm is the maximum allowable for bridge cracks. If the width exceeds the limit, the integrity of the bridge will be destroyed, and even a collapse accident will occur. Therefore, it is important to identify cracks in bridge structure effectively and provide information for structural disaster reduction projects in time. With the development of machine learning, bridge crack detection and classification based on deep learning has been paid more attention. This paper designs a bridge crack classification algorithm based on convolution neural network and support vector machine. Firstly, the captured image data are divided into training set and test set. Secondly, they are preprocessing and extracted features by convolution neural network. Lastly, they are classified by SVM. The proposed algorithm can solve the problems of insufficient samples and low classification accuracy, and realizes the effective and accurate classification.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3443467.3443855',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Mapping Ordinances and Tweets using Smart City Characteristics to Aid Opinion Mining',\n",
       "  'authors': \"['Manish Puri', 'Xu Du', 'Aparna S. Varde', 'Gerard de Melo']\",\n",
       "  'date': 'April 2018',\n",
       "  'source': \"WWW '18: Companion Proceedings of the The Web Conference 2018\",\n",
       "  'abstract': 'This research focuses on mining ordinances (local laws) and public reactions to them expressed on social media. We place particular emphasis on ordinances and tweets relating to Smart City Characteristics (SCCs), since an important aim of our work is to assess how well a given region heads towards a Smart City. We rely on SCCs as a nexus between a seemingly infinite number of ordinances and tweets to be able to map them, and also to facilitate SCC-based opinion mining later for providing feedback to urban agencies based on public reactions. Common sense knowledge is harnessed in our approach to reflect human judgment in mapping. This paper presents our research in ordinance and tweet mapping with SCCs, including the proposed mapping approach, our initial experiments, related discussion, and future work emerging therein. To the best of our knowledge, ours is among the first works to conduct mining on ordinances and tweets for Smart Cities. This work has a broader impact with a vision to enhance Smart City growth.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3184558.3191632',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': \"Philosophy and methodology of clustering in pattern mining: Japanese anthropologist Jiro Kawakita's KJ method\",\n",
       "  'authors': \"['Takashi Iba', 'Ayaka Yoshikawa', 'Konomi Munakata']\",\n",
       "  'date': 'October 2017',\n",
       "  'source': \"PLoP '17: Proceedings of the 24th Conference on Pattern Languages of Programs\",\n",
       "  'abstract': 'This paper describes the methodology and philosophy behind a pattern mining process known as \"clustering.\" This holistic approach to pattern language creation is referred to as the \"KJ method\" by Jiro Kawakita (Kawakita, 1967). Pattern mining is a process used to extract the knowledge of practice (rules of thumb and tips) from individual cases and experiences, with the aim of creating a pattern language. Clustering is carried out to discover and organize the common points from the extracted knowledge. We have used the KJ method as the foundation of our pattern language clustering method. Invented during the 1950s and 60s, the KJ method has been widely applied in Japan, particularly in the areas of industry and education. We seek to deepen the understanding of this method and its underlying intentions by quoting Kawakita\\'s explanations in an English translation. In particular, this paper elaborates on the following factual statements: the method was developed via field science; the method uses a bottom-up approach to generate order from chaos; the method requires that the data be viewed outside of any existing concept or framework; the method prioritizes feelings over reason; and at the conceptual level, the method is consistent with the essence of creativity. We expect this paper to provide readers with a clear understanding of the KJ method with a view to using it effectively in the practice of clustering in pattern mining.',\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3290281.3290296',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A pipeline for mining association rules from large datasets of retailers invoices',\n",
       "  'authors': \"['Giuseppe Agapito', 'Barbara Calabrese', 'Pietro H. Guzzi', 'Mario Cannataro']\",\n",
       "  'date': 'January 2019',\n",
       "  'source': \"APPIS '19: Proceedings of the 2nd International Conference on Applications of Intelligent Systems\",\n",
       "  'abstract': \"The concept of massive data generation nowadays affects several domains such as marketing including electronic invoices (e-invoices) of large retailers, web access log files, healthcare, life sciences and so on. Datasets dimensions grow up, due to the availability of several cheap connected devices, such as mobile devices, RFID and wireless sensors networks, from which to collect data. Often, the collected data need to be gathered into a consistent, integrated and comprehensive form, to be used for knowledge discovery. Without adequately cleaning, transforming and structuring the data before the analysis, it is hard to mine useful knowledge. Thus, users by using data mining can extract knowledge from large invoices documents. In this paper, a pipeline for preprocessing and mining association rules from large retailers commercial documents has been proposed. The preprocessing provides merging, cleaning, formatting and summarization. The methodology can improve the quality of large retailers data by reducing the quantity of irrelevant data, making the remaining data suitable to mine association rules (ARM). Analyzing a real invoices dataset (provided by an Italian retailer) by using the proposed methodology, it was possible to extract 36 significant association rules, highlighting the customers' behavior in the purchase of goods.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3309772.3309778',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A Mining Software Repository Extended Cookbook: Lessons learned from a literature review',\n",
       "  'authors': \"['Daniel Barros', 'Flavio Horita', 'Igor Wiese', 'Kanan Silva']\",\n",
       "  'date': 'September 2021',\n",
       "  'source': \"SBES '21: Proceedings of the XXXV Brazilian Symposium on Software Engineering\",\n",
       "  'abstract': 'The main purpose of Mining Software Repositories (MSR) is to discover the latest enhancements and provide an insight into how to make improvements in a software project. In light of it, this paper updates the MSR findings of the original MSR Cookbook, by first conducting a systematic mapping study to elicit and analyze the state-of-the-art, and then proposing an extended version of the Cookbook. This extended Cookbook was built on four high-level themes, which were derived from the analysis of a list of 112 selected studies. Hence, it was used to consolidate the extended Cookbook as a contribution to practice and research in the following areas by: 1) including studies published in all available and relevant publication venues; 2) including and updating recommendations in all four high-level themes, with an increase of 84% in comments in this study when compared with the original MSR Cookbook; 3) summarizing the tools employed for each high-level theme; and 4) providing lessons learned for future studies. Thus, the extended Cookbook examined in this work can support new research projects, as upgraded recommendations and the lessons learned are available with the aid of samples and tools.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3474624.3474627',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'NodeAug: Semi-Supervised Node Classification with Data Augmentation',\n",
       "  'authors': \"['Yiwei Wang', 'Wei Wang', 'Yuxuan Liang', 'Yujun Cai', 'Juncheng Liu', 'Bryan Hooi']\",\n",
       "  'date': 'August 2020',\n",
       "  'source': \"KDD '20: Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining\",\n",
       "  'abstract': \"By using Data Augmentation (DA), we present a new method to enhance Graph Convolutional Networks (GCNs), that are the state-of-the-art models for semi-supervised node classification. DA for graph data remains under-explored. Due to the connections built by edges, DA for different nodes influence each other and lead to undesired results, such as uncontrollable DA magnitudes and changes of ground-truth labels. To address this issue, we present the NodeAug (Node-Parallel Augmentation) scheme, that creates a 'parallel universe' for each node to conduct DA, to block the undesired effects from other nodes. NodeAug regularizes the model prediction of every node (including unlabeled) to be invariant with respect to changes induced by Data Augmentation (DA), so as to improve the effectiveness. To augment the input features from different aspects, we propose three DA strategies by modifying both node attributes and the graph structure. In addition, we introduce the subgraph mini-batch training for the efficient implementation of NodeAug. The approach takes the subgraph corresponding to the receptive fields of a batch of nodes as the input per iteration, rather than the whole graph that the prior full-batch training takes. Empirically, NodeAug yields significant gains for strong GCN models on the Cora, Citeseer, Pubmed, and two co-authorship networks, with a more efficient training process thanks to the proposed subgraph mini-batch training approach.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3394486.3403063',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Using process mining to analyse self-regulated learning: a systematic analysis of four algorithms',\n",
       "  'authors': \"['John Saint', 'Yizhou Fan', 'Shaveen Singh', 'Dragan Gasevic', 'Abelardo Pardo']\",\n",
       "  'date': 'April 2021',\n",
       "  'source': 'LAK21: LAK21: 11th International Learning Analytics and Knowledge Conference',\n",
       "  'abstract': 'The conceptualisation of self-regulated learning (SRL) as a process that unfolds over time has influenced the way in which researchers approach analysis. This gave rise to the use of process mining in contemporary SRL research to analyse data about temporal and sequential relations of processes that occur in SRL. However, little attention has been paid to the choice and combinations of process mining algorithms to achieve the nuanced needs of SRL research. We present a study that 1) analysed four process mining algorithms that are most commonly used in the SRL literature – Inductive Miner, Heuristics Miner, Fuzzy Miner, and pMineR; and 2) examined how the metrics produced by the four algorithms complement each. The study looked at micro-level processes that were extracted from trace data collected in an undergraduate course (N=726). The study found that Fuzzy Miner and pMineR offered better insights into SRL than the other two algorithms. The study also found that a combination of metrics produced by several algorithms improved interpretation of temporal and sequential relations between SRL processes. Thus, it is recommended that future studies of SRL combine the use of process mining algorithms and work on new tools and algorithms specifically created for SRL research.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3448139.3448171',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Interactive Machine Learning and Explainability in Mobile Classification of Forest-Aesthetics',\n",
       "  'authors': \"['Simon Flutura', 'Andreas Seiderer', 'Tobias Huber', 'Katharina Weitz', 'Ilhan Aslan', 'Ruben Schlagowski', 'Elisabeth André', 'Joachim Rathmann']\",\n",
       "  'date': 'September 2020',\n",
       "  'source': \"GoodTechs '20: Proceedings of the 6th EAI International Conference on Smart Objects and Technologies for Social Good\",\n",
       "  'abstract': \"This paper presents an application that classifies forest's aesthetics using interactive machine learning on mobile devices. Transfer learning is used to be able to build upon deep ANNs (MobileNet) using the limited resources available on smart-phones. We trained and evaluated a model using our application based on a data-set that is plausible to be created by a single user. In order to increase the comprehensibility of our model we explore the potential of incorporating explainable Artificial Intelligence (XAI) into our mobile application. To this end we use deep Taylor decomposition to generate saliency maps that highlight areas of the input that were relevant for the decision of the ANN and conducted a user study to evaluate the usefulness of this approach for end-users.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3411170.3411225',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'K-means, HAC and FCM Which Clustering Approach for Arabic Text?',\n",
       "  'authors': \"['Lahbib Ajallouda', 'Fatima Zahra Fagroud', 'Ahmed Zellou', 'El Habib Benlahmar']\",\n",
       "  'date': 'September 2020',\n",
       "  'source': \"SITA'20: Proceedings of the 13th International Conference on Intelligent Systems: Theories and Applications\",\n",
       "  'abstract': 'Today, we are witnessing rapid growth in Web resources that allow Internet users to express and share their ideas, opinions, and judgments on a variety of issues. Several classification approaches have been proposed to classify textual data. But all these approaches require us to label the clusters we want to obtain. Which, in reality, is not available because we do not know in advance the information that can be proposed through these opinions. To overcome this constraint, clustering approaches such as K-mean, HAC or FCM can be exploited. In this paper, we present and compare these approaches. And to show the importance of exploiting clustering algorithms, to classify and analyze textual data in Arabic. By applying them to a real case that has created a great debate in Morocco, which is the case of teachers contracting with academies.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3419604.3419779',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Classification and Feature Extraction for Text-based Drug Incident Report',\n",
       "  'authors': \"['Takanori Yamashita', 'Naoki Nakashima', 'Sachio Hirokawa']\",\n",
       "  'date': 'March 2018',\n",
       "  'source': 'ICBCB 2018: Proceedings of the 2018 6th International Conference on Bioinformatics and Computational Biology',\n",
       "  'abstract': 'Medical institutions have been constructed incident report system, then accumulating incident data. Incident data compose text-based data and some structured attributes. We considered based on the analysis result with clustering for drug incident report. Firstly, we generated a network of documents and words from the text-based data. Secondly, Louvain method was applied to the network and 11 clusters were generated. We confirmed the contents of each cluster from feature words extracted by TF-IDF. Then, we compare clusters of text-based data with structured attributes and grasp the trend of the incident. This proposed method showed the possibility of clinical support toward reduction incident from text-based data.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3194480.3194499',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A framework for the analysis of information propagation in social networks combining complex networks and text mining techniques',\n",
       "  'authors': \"['Carlos Magno G. Barbosa', 'Lucas Gabriel da S. Felix', 'Carolina R. Xavier', 'Vinícius da F. Vieira']\",\n",
       "  'date': 'October 2019',\n",
       "  'source': \"WebMedia '19: Proceedings of the 25th Brazillian Symposium on Multimedia and the Web\",\n",
       "  'abstract': 'Online social networks like Twitter, Facebook and WhatsApp are among the greatest innovations of the modern internet. Through these applications, users can consume and be major news broadcasters. These networks are sensitive to real-time events and generate a large amount of data at all times. The ability to extract information from this large amount of data is essential for the survival of companies and the modernization of public policies. With this purpose, this work presents the construction of a framework that combines complex networks and data mining to analyze the content and the propagation of information in social networks, especially in Twitter. As a practical case, the methodology is applied to the analysis of messages posted on twitter related to pension reform in Brazil. As a result, the framework was able to identify the main topics of Internet discussion and the positioning within certain communities regarding the subject. The main feeling surrounding the discussion turned out to be negative and pro-retirement users were more involved in supportive and anti-reform communities.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3323503.3360289',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'MATCH: Metadata-Aware Text Classification in A Large Hierarchy',\n",
       "  'authors': \"['Yu Zhang', 'Zhihong Shen', 'Yuxiao Dong', 'Kuansan Wang', 'Jiawei Han']\",\n",
       "  'date': 'April 2021',\n",
       "  'source': \"WWW '21: Proceedings of the Web Conference 2021\",\n",
       "  'abstract': 'Multi-label text classification refers to the problem of assigning each given document its most relevant labels from a label set. Commonly, the metadata of the given documents and the hierarchy of the labels are available in real-world applications. However, most existing studies focus on only modeling the text information, with a few attempts to utilize either metadata or hierarchy signals, but not both of them. In this paper, we bridge the gap by formalizing the problem of metadata-aware text classification in a large label hierarchy (e.g., with tens of thousands of labels). To address this problem, we present the MATCH1 solution—an end-to-end framework that leverages both metadata and hierarchy information. To incorporate metadata, we pre-train the embeddings of text and metadata in the same space and also leverage the fully-connected attentions to capture the interrelations between them. To leverage the label hierarchy, we propose different ways to regularize the parameters and output probability of each child label by its parents. Extensive experiments on two massive text datasets with large-scale label hierarchies demonstrate the effectiveness of MATCH over the state-of-the-art deep learning baselines.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3442381.3449979',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Epileptic seizures classification in EEG using PCA based genetic algorithm through machine learning',\n",
       "  'authors': \"['Md Khurram Monir Rabby', 'A. K. M. Kamrul Islam', 'Saeid Belkasim', 'Marwan U Bikdash']\",\n",
       "  'date': 'April 2021',\n",
       "  'source': \"ACM SE '21: Proceedings of the 2021 ACM Southeast Conference\",\n",
       "  'abstract': 'In this research, a Principal Component Analysis (PCA) with Genetic Algorithm based Machine Learning (ML) approach is developed for the binary classification of epileptic seizures from the EEG dataset. The proposed approach utilizes PCA to reduce the number of features for binary classification of epileptic seizures and is applied to the existing machine learning models to evaluate the model performance in comparison to the higher number of features. Here, Genetic Algorithm (GA) is employed to tune the hyperparameters of the machine learning models for identifying the best ML model. The proposed approach is applied to the UCI epileptic seizure recognition dataset, which is originated from the EEG dataset of Bonn University. As a preliminary analysis of the proposed approach, the data analysis result shows a significant reduction in the number of features but has minimal impact on the ML performance parameters in comparison to the existing ML method.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3409334.3452065',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Detection and prevention of malicious cryptocurrency mining on internet-connected devices',\n",
       "  'authors': \"['AbedAlqader Swedan', 'Ahmad N. Khuffash', 'Othman Othman', 'Ahmed Awad']\",\n",
       "  'date': 'June 2018',\n",
       "  'source': \"ICFNDS '18: Proceedings of the 2nd International Conference on Future Networks and Distributed Systems\",\n",
       "  'abstract': 'As technology evolves, more and more devices are connected to the Internet. The popularity and increasing significance of cryptocurriences are drawing attention, and crybercriminals are trying to utilize the resources and steal the processing power of these devices. It is highly likely that there are billions of devices that are maliciously mining cryptocurrency for the benefit of a cybercriminal without noticing the damage they may be causing. This paper is proposed because there is a huge need to professionally defend and protect against the misuse of assets in order to avoid losses, both financially and operationally, and how it is possible to mitigate with this rising trend.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3231053.3231076',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Machine learning application for malwares classification using visualization technique',\n",
       "  'authors': \"['Ben Abdel Ouahab Ikram', 'Bouhorma Mohammed', 'Boudhir Anouar Abdelhakim', 'El Aachak Lotfi', 'Bassam Zafar']\",\n",
       "  'date': 'October 2019',\n",
       "  'source': \"SCA '19: Proceedings of the 4th International Conference on Smart City Applications\",\n",
       "  'abstract': 'Nowadays attackers work hard to develop efficient cyberthreats and exploit new techniques. So defenders need to use advanced methodologies to combat the latest threats and safely remove them from computers, mobiles and connected devices. Without the intelligent techniques, these devices would be at increased risk of damage from malicious programs. Recently a novel approach of processing malwares was appeared; it passes from malware binaries into malware images. Researchers found similarities in malwares images by extracting specific features. This paper presents malwares classifier using KNN and malware visualization technique. We used a database of 9339 samples of malwares from 25 families. We calculated the GIST descriptor for grayscale malware images. Then a KNN model was trained and evaluated many times to reach a score of 97%, which is very close to results found on literature.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3368756.3369098',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Automated classification of text sentiment',\n",
       "  'authors': \"['Emmanuel Dufourq', 'Bruce A. Bassett']\",\n",
       "  'date': 'September 2017',\n",
       "  'source': \"SAICSIT '17: Proceedings of the South African Institute of Computer Scientists and Information Technologists\",\n",
       "  'abstract': \"The ability to identify sentiment in text, referred to as sentiment analysis, is one which is natural to adult humans. This task is, however, not one which a computer can perform by default. Identifying sentiments in an automated, algorithmic manner will be a useful capability for business and research in their search to understand what consumers think about their products or services and to understand human sociology. Here we propose two new Genetic Algorithms (GAs) for the task of automated text sentiment analysis. The GAs learn whether words occurring in a text corpus are either sentiment or amplifier words, and their corresponding magnitude. Sentiment words, such as 'horrible', add linearly to the final sentiment. Amplifier words in contrast, which are typically adjectives/adverbs like 'very', multiply the sentiment of the following word. This increases, decreases or negates the sentiment of the following word. The sentiment of the full text is then the sum of these terms. This approach grows both a sentiment and amplifier dictionary which can be reused for other purposes and fed into other machine learning algorithms. We report the results of multiple experiments conducted on large Amazon data sets. The results reveal that our proposed approach was able to outperform several public and/or commercial sentiment analysis algorithms.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3129416.3129420',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Gear Fault Diagnosis and Classification Using Machine Learning Classifier',\n",
       "  'authors': \"['Sudarsan Sahoo', 'R. A. Laskar', 'J. K. Das', 'S. H. Laskar']\",\n",
       "  'date': 'March 2019',\n",
       "  'source': \"ISMSI '19: Proceedings of the 2019 3rd International Conference on Intelligent Systems, Metaheuristics &amp; Swarm Intelligence\",\n",
       "  'abstract': 'In industry the condition monitoring of rotating machinery gear is very important. The defect in gear mesh may cause the failure in machinery and that causes a severe loss in industry. The failure in gear mesh reduces the efficiency and hence decreases the productivity in industrial operation. Therefore the health monitoring of gear mesh is very important. Proper health monitoring of gears can avoid the failure in machinery and can save money in industrial applications. The acoustic emission and vibration are the two widely used measuring parameters which is used for the condition monitoring of gear mesh. In this work the gear fault detection by using the acoustic emission monitoring technique is used. This experimentation is done by using an efficient instrumentation system. The experimental set-up is designed which consists of a gear mesh driving system and a hand-held sound analyzer. To carry out the experiment the measuring signals from the defective and healthy gears are captured and compared. In this work the measuring signal is the acoustic emission from the tested gears. Then for the fault detection, two signal processing techniques are followed. These are statistical analysis and adaptive wavelet transform (AWT) analysis. The comparison in statistical as well as in AWT analysis used to detect the fault present in gears. In AWT analysis the adaptive noise cancellation is used to enhance the signal to noise ratio (SNR). Finally faults in gears are classified using the machine learning classifier. The statistical parameter data are used as the input data for the classifiers to train the system to classify the fault.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3325773.3325782',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Improvement of transparency through mining techniques for reclassification of texts: the case of brazilian transparency portal',\n",
       "  'authors': \"['Gustavo Almeida', 'Kate Revoredo', 'Claudia Cappelli', 'Cristiano Maciel']\",\n",
       "  'date': 'May 2018',\n",
       "  'source': \"dg.o '18: Proceedings of the 19th Annual International Conference on Digital Government Research: Governance in the Data Age\",\n",
       "  'abstract': 'Several countries passed transparency laws requiring that governments make data about its fiscal and financial expenditures publicly available in Internet portals. Nevertheless, available data is not always synonymous with transparent data. This is the case of the Transparency Portal of Brazilian Federal Government, since key data is presented as unstructured text hindering the control of purchased items. This article describes the application of text mining techniques with the objective of reclassifying descriptive texts of unit of measurement related to the products and services procured by Federal Government in Brazil. The results of the efficacy of this model are presented, including the production of analysis based on the transformed dataset, identifying probable input errors, suspicious companies and purchasers and factors affecting procurement prices as well as presenting suggestions for future research and improvements for the way the data is inputted and made available',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3209281.3209332',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Distance Metric Learning Approach for Weather Data Mining',\n",
       "  'authors': \"['Zhongfeng Niu', 'Yian Zhu', 'Linxiu Jiang']\",\n",
       "  'date': 'October 2018',\n",
       "  'source': \"BDIOT '18: Proceedings of the 2018 2nd International Conference on Big Data and Internet of Things\",\n",
       "  'abstract': 'High throughput weather data, which is acquired by remote sensing technology, collected by local weather stations, or gathered by autonomous sensors, is the foundation for modern weather forecast and climate change prediction. Such data set often contains multi-dimensional information on aspects such as temperature, humidity, wind speed/direction, atmospheric pressure, etc., which can be extremely large-scale and convoluted. Therefore, effective and efficient methods for weather data analysis is important and urgently needed. Data mining technologies, which is the computer-aided process that digs useful patterns out of large-scale data sets, is widely acknowledged as a very promising direction in weather data analysis. In this paper, a novel methodology is described, which applies a distance metric learning approach for weather data mining. Such a method is applied to weather data set collected at JFK, MCO and SFO airport in 2016, and shows a very promising advantage in classification accuracy compared with other conventional methods.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3289430.3289455',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Data mining classifiers comparison for seismic hazard prediction',\n",
       "  'authors': \"['Sneha', 'Abdolreza Abhari', 'Chen Ding']\",\n",
       "  'date': 'April 2018',\n",
       "  'source': \"CNS '18: Proceedings of the Communications and Networking Symposium\",\n",
       "  'abstract': 'Earthquake and seismic hazards are natural disasters which are very difficult to predict. Researchers are working hard to predict these disasters for minimizing loss of life and property. Proposed research used data mining algorithms on seismic bumps dataset which was obtained from coal mines for the seismic hazard prediction. Data mining is a powerful technique used to discover patterns of data. In this research, performance of five data mining classifiers was compared for better prediction of seismic hazard. For preprocessing of this dataset, discretization and resampling techniques were used. For modelling, five data mining classifiers were implemented and compared by using feature selection technique on the basis of confusion matrix measures like success rate, mean absolute error, kappa statistics, precision, recall and f-measure. This analysis showed that Random Forest algorithm achieved highest success rate by using feature selection technique and provided promising results for seismic hazard prediction.',\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3213200.3213207',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A Machine Learning Based Ensemble Method for Automatic Multiclass Classification of Decisions',\n",
       "  'authors': \"['Liming Fu', 'Peng Liang', 'Xueying Li', 'Chen Yang']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"EASE '21: Proceedings of the 25th International Conference on Evaluation and Assessment in Software Engineering\",\n",
       "  'abstract': 'Stakeholders make various types of decisions with respect to requirements, design, management, and so on during the software development life cycle. Nevertheless, these decisions are typically not well documented and classified due to limited human resources, time, and budget. To this end, automatic approaches provide a promising way. In this paper, we aimed at automatically classifying decisions into five types to help stakeholders better document and understand decisions. First, we collected a dataset from the Hibernate developer mailing list. We then experimented and evaluated 270 configurations regarding feature selection, feature extraction techniques, and machine learning classifiers to seek the best configuration for classifying decisions. Especially, we applied an ensemble learning method and constructed ensemble classifiers to compare the performance between ensemble classifiers and base classifiers. Our experiment results show that (1) feature selection can decently improve the classification results; (2) ensemble classifiers can outperform base classifiers provided that ensemble classifiers are well constructed; (3) BoW + 50% features selected by feature selection with an ensemble classifier that combines Naïve Bayes (NB), Logistic Regression (LR), and Support Vector Machine (SVM) achieves the best classification result (with a weighted precision of 0.750, a weighted recall of 0.739, and a weighted F1-score of 0.727) among all the configurations. Our work can benefit various types of stakeholders in software development through providing an automatic approach for effectively classifying decisions into specific types that are relevant to their interests.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3463274.3463325',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Research on the design of large data storage structure of database based on Data Mining',\n",
       "  'authors': \"['Lihua Wang']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': 'AIAM2021: 2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture',\n",
       "  'abstract': \"With the continuous development of China's social economy, the informationization function of power grid companies is becoming increasingly powerful, and its effectiveness is very remarkable. However, in the related business of power system, the performance of database cannot be guaranteed. In the process of power marketing, a large amount of data will be generated. In the process of processing a large amount of data, it is necessary to constantly optimize the database to improve the structure of the database. Data mining is a promising subject frontier of database system and new database application. In this paper, a large data storage structure of database based on data mining is designed. The ETL tool in signaling data warehouse is used to realize the integrated design of signaling data storage and application.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3495018.3501223',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Classification of short-texts generated during disasters: a deep neural network based approach',\n",
       "  'authors': \"['Shamik Kundu', 'Srijith P. K', 'Maunendra Sankar Desarkar']\",\n",
       "  'date': 'August 2018',\n",
       "  'source': \"ASONAM '18: Proceedings of the 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining\",\n",
       "  'abstract': 'Micro-blogging sites provide a wealth of resources during disaster events in the form of short texts. Correct classification of these text data into various actionable classes can be of great help in shaping the means to rescue people in disaster-affected places. The process of classification of these text data poses a challenging problem because the texts are usually short and very noisy and finding good features that can distinguish these texts into different classes is time consuming, tedious and often requires a lot of domain knowledge. We propose a deep learning based model to classify tweets into different actionable classes such as resource need and availability, activities of various NGO etc. Our model requires no domain knowledge and can be used in any disaster scenario with little to no modification.',\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3382225.3382396',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'geoGAT: Graph Model Based on Attention Mechanism for Geographic Text Classification',\n",
       "  'authors': \"['Weipeng Jing', 'Xianyang Song', 'Donglin Di', 'Houbing Song']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Asian and Low-Resource Language Information Processing',\n",
       "  'abstract': 'In the area of geographic information processing, there are few researches on geographic text classification. However, the application of this task in Chinese is relatively rare. In our work, we intend to implement a method to extract text containing geographical entities from a large number of network texts. The geographic information in these texts is of great practical significance to transportation, urban and rural planning, disaster relief, and other fields. We use the method of graph convolutional neural network with attention mechanism to achieve this function. Graph attention networks (GAT) is an improvement of graph convolutional neural networks (GCN). Compared with GCN, the advantage of GAT is that the attention mechanism is proposed to weight the sum of the characteristics of adjacent vertices. In addition, We construct a Chinese dataset containing geographical classification from multiple datasets of Chinese text classification. The Macro-F Score of the geoGAT we used reached 95% on the new Chinese dataset.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3434239',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Text classification on software requirements specifications using transformer models',\n",
       "  'authors': \"['Derya Kici', 'Aysun Bozanta', 'Mucahit Cevik', 'Devang Parikh', 'Ayşe Başar']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"CASCON '21: Proceedings of the 31st Annual International Conference on Computer Science and Software Engineering\",\n",
       "  'abstract': 'Text classification in Software Requirements Specifications (SRS) documents is an essential task for various purposes including automatically extracting requirements and their types as well as identification of duplicate or conflicting information, which all contribute to avoiding potential issues in the later stages of the software development life cycle. While a variety of machine learning approaches have been considered for text classification over SRS documents, many of these fail to provide adequate performance as they often ignore the meaning of software artifacts or integrate domain knowledge for the classification task. Recent advances in deep learning methodology have significantly contributed to Natural Language Processing (NLP) and text classification. One of the main challenges in using deep learning models for various NLP tasks in the software engineering domain is the scarcity of labeled textual data. In addition, even with sufficient data, training from the scratch still requires significant training time and computational resources. Transfer learning is a novel approach that proposes a solution to such reservations by providing pre-trained models that enable fine-tuning with the customized data. In this research, we conduct an empirical analysis on multi-class text classification over SRS documents using different pre-trained transformer models including BERT, DistilBERT, Roberta, AlBERT, and XLNet, and compare their performance. We test the performance of these models using three SRS datasets: DOORS, NFR-PROMISE, and PURE. Our numerical study shows that the transformer models are able to generate highly accurate results to classify all categories except Priority of the requirements. While all models provide a 80% or higher accuracy for other classification tasks, the accuracy of the models to classify the Priority does not exceed 60%.',\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3507788.3507811',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A Survey of Distance Metrics in Clustering Data Mining Techniques',\n",
       "  'authors': \"['Marina Adriana Mercioni', 'Stefan Holban']\",\n",
       "  'date': 'June 2019',\n",
       "  'source': \"ICGSP '19: Proceedings of the 3rd International Conference on Graphics and Signal Processing\",\n",
       "  'abstract': 'Lately, due to the increasing size of databases, several aspects have been studied in detail, such as grouping, searching for the closest neighbor and other identification methods. It has been found that in the multidimensional space, the concept of distance does not offer high performance. In this paper, we study the effect of different types of distances on the group to see the similarities between objects. Among these distances we mention two distances: the Euclidean distance and Manhattan distance, implemented in a system developed to identify the architectural styles of the buildings. The aim of this paper is using cluster analysis to identify distance metrics impact in detection of architectural styles using Data Mining techniques.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3338472.3338490',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Object Detection and Classification Using Machine Learning Techniques: A Comparison of Haar Cascades and Neural Networks',\n",
       "  'authors': \"['Chisulo Mukabe', 'Nalina Suresh', 'Valerians Hashiyana', 'Titus Haiduwa', 'William Sverdlik']\",\n",
       "  'date': 'August 2021',\n",
       "  'source': \"DSMLAI '21': Proceedings of the International Conference on Data Science, Machine Learning and Artificial Intelligence\",\n",
       "  'abstract': \"Object recognition and object detection are sub fields of computer vision, the task of giving computers the ability to perceive and respond to the world around them. This is a very useful technology and has many different applications in many different disciplines. Examples of applications include but are not limited to: Use in security by using facial recognition; use in the medical field for classification of cancer types (malignant or benign) or detecting sickle cells in the blood; or it can be used as a research tool to automatically record data (e.g. count the number of trucks that use a particular highway); or used in agriculture or industrialization for quality control; or for entertainment purposes in games that can detect and track a users' movement to control a character in a game. And there are many more examples, in which it can be used. However, just as they are many applications, there are also many methods of implementing these systems, such as Convolutional neural networks, Haar cascades, Scale-Invariant Feature Transformations (SIFT), Histogram of Oriented Gradients (HOG) and several others including combinations of these. Each technique may have different setup procedures and different applications where one may work better than the other. This paper aims to measure and give a comparison of some of these techniques for object detection and tracking, by looking at their training and testing time as well as accuracy. The methods used in this paper was Haar Cascades and Neural Networks. The purpose of this paper is to learn more about these machine learning techniques by comparing the way they work and the way they were implemented, by doing so one can also understand contexts in which to use these systems. The artifacts used in this paper included a robot that can track an object and an application that could classify an image based on a set of objects it was trained on, that is the application can determine what the picture is of based on a set of images it was trained on.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3484824.3484895',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Probabilistic Dynamic Non-negative Group Factor Model for Multi-source Text Mining',\n",
       "  'authors': \"['Chien Lu', 'Jaakko Peltonen', 'Jyrki Nummenmaa', 'Kalervo Järvelin']\",\n",
       "  'date': 'October 2020',\n",
       "  'source': \"CIKM '20: Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management\",\n",
       "  'abstract': 'Nonnegative matrix factorization (NMF) is a popular approach to model data, however, most models are unable to flexibly take into account multiple matrices across sources and time or apply only to integer-valued data. We introduce a probabilistic, Gaussian Process-based, more inclusive NMF-based model which jointly analyzes nonnegative data such as text data word content from multiple sources in a temporal dynamic manner. The model collectively models observed matrix data, source-wise latent variables, and their dependencies and temporal evolution with a full-fledged hierarchical approach including flexible nonparametric temporal dynamics. Experiments on simulated data and real data show the model out-performs, comparable models. A case study on social media and news demonstrates the model discovers semantically meaningful topical factors and their evolution',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3340531.3411956',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Empirical Comparison of Area under ROC curve (AUC) and Mathew Correlation Coefficient (MCC) for Evaluating Machine Learning Algorithms on Imbalanced Datasets for Binary Classification',\n",
       "  'authors': \"['Chongomweru Halimu', 'Asem Kasem', 'S. H. Shah Newaz']\",\n",
       "  'date': 'January 2019',\n",
       "  'source': \"ICMLSC '19: Proceedings of the 3rd International Conference on Machine Learning and Soft Computing\",\n",
       "  'abstract': 'A common challenge encountered when trying to perform classifications and comparing classifiers is selecting a suitable performance metric. This is particularly important when the data has class-imbalance problems. Area under the Receiver Operating Characteristic Curve (AUC) has been commonly used by the machine learning community in such situations, and recently researchers are starting to use Matthew Correlation Coefficient (MCC), especially in biomedical research. However, there is no empirical study that has been conducted to compare the suitability of the two metrics. In this paper, the aim of this study is to provide insights about how AUC and MCC are compared to each other when used with classical machine learning algorithms over a range of imbalanced datasets. In our study, we utilize an earlier-proposed criteria for comparing metrics based on the degree of consistency and degree of Discriminancy to compare AUC against MCC. We carry out experiments using four machine learning algorithms on 54 imbalanced datasets, with imbalance ratios ranging from 1% to 10%. The results demonstrate that both AUC and MCC are statistically consistent with each other; however AUC is more discriminating than MCC. The same observation is noticed when evaluated on 23 balanced datasets. This suggests AUC to be a better measure than MCC in evaluating and comparing classification algorithms.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3310986.3311023',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Research of Classification Impact Factors Mining Based on the Contrast Pattern Equivalence Classes',\n",
       "  'authors': \"['Ji-Cheng Shan', 'Hang Zhang', 'Wei-Ke Liu', 'Qing-Bao Liu']\",\n",
       "  'date': 'October 2017',\n",
       "  'source': 'WCNA 2017: Proceedings of the 2017 International Conference on Wireless Communications, Networking and Applications',\n",
       "  'abstract': 'In the domain of multi-label data and big data, it is hard for most classification methods to generate explainable classify rules, which makes it harder to get impact factors for classification. In this paper, contrast pattern equivalence class was discussed and mined to determine main impact factors for different classes, considering combination of frequent pattern mining and data classification. The rationality and feasibility to determine classification impact factors through contrast pattern equivalence classes mining was theoretically analyzed. Also the mine process and result analysis was shown through an experiment on the real dataset. Experimental results show the validity of the method proposed.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3180496.3180621',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Improving support vector machine classification accuracy based on kernel parameters optimization',\n",
       "  'authors': \"['Lubna B. Mohammed', 'Kaamran Raahemifar']\",\n",
       "  'date': 'April 2018',\n",
       "  'source': \"CNS '18: Proceedings of the Communications and Networking Symposium\",\n",
       "  'abstract': 'Support Vector Machine (SVM) learning algorithm is considered as the most popular classification algorithm. It is a supervised learning technique that is mainly based on the conception of decision planes. These decision planes define decision boundaries which are used to separate a set of objects. It is important to extract the main features of the training datasets. These features can be used to define the separation boundaries. The separation boundaries can also be improved by tuning the parameters of the separation hyperplane. In literature, there are different techniques for feature selection and SVM parameters optimization that can be used to improve classification accuracy. There are a wide variety of applications that use SVM classification algorithm, such as text classification, disease diagnosis, gene analysis, and many others. The aim of this paper is to investigate the techniques that can be used to improve the classification accuracy of SVM based on kernel parameters optimization. The datasets are collected from different applications; having different number of classes and different number of features. The analysis and comparison among different kernel parameters were implemented on different datasets to study the effect of the number of features, the number of classes, and kernel parameters on the performance of the classification process.',\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3213200.3213210',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Traffic Sign and Vehicle Classification based on Machine Learning',\n",
       "  'authors': \"['Malichenko Viktor', 'Han Honggui']\",\n",
       "  'date': 'December 2020',\n",
       "  'source': \"VSIP '20: Proceedings of the 2020 2nd International Conference on Video, Signal and Image Processing\",\n",
       "  'abstract': 'In this paper, we offer a machine learning classifier model, later considered as MLCM, for classifying objects such as road signs and vehicles. Showing the influence of vocabulary size on accuracy of SVM using SURF. Based on SURF method used bag-of-words model as feature extractor. Due to its simplifying representation, it accelerates the first stage of our MLCM. We tested and analyzed accuracy of Support Vector Machines, including Linear, Quadratic and Medium Gaussian SVM as flowed step model and automatically use best result for further estimation. Furthermore, we provide a brief introduction of applied methods and experimental results analysis. MLCM introduces combination of SURF method and several SVMs as well as optimized SVM. This technique shows good performance with minimum failures. Thereafter, it will be implemented for real-time video sequences. The achieved goal can be implemented in the use of self-driving of industrial machines with a safe speed.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3442705.3442711',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Evaluation of student collaboration on canvas LMS using educational data mining techniques',\n",
       "  'authors': \"['Urvashi Desai', 'Vijayalakshmi Ramasamy', 'James Kiper']\",\n",
       "  'date': 'April 2021',\n",
       "  'source': \"ACM SE '21: Proceedings of the 2021 ACM Southeast Conference\",\n",
       "  'abstract': \"Online discussion forums provide valuable information about students' learning and engagement in course activities. The hidden knowledge in the contents of these discussion posts can be examined by analyzing the social interactions between the participants. This research investigates students' learning and collaborative problem-solving aspects by applying social network analysis (SNA) metrics and sophisticated computational techniques. The data is collected from online course discussion forums on Canvas, a Learning Management System (LMS), in a CS1 course at a medium-sized US University. The research demonstrates that efficient tools are needed to model and evaluate goal-oriented discussion forums constructed from active student collaborations. This research aims to develop a systematic data collection and analysis instrument incorporated into LMSs that enables grading the discussions to improve instructional outcomes, gain insights into and explain educational phenomena. The study also emphasizes important SNA metrics that analyze students' social behavior since a positive correlation was seen between the number of posts made by students and their academic performance in terms of the final grade. The prototype developed (CODA - Canvas Online Discussion Analyzer) helps evaluate students' performance based on the useful knowledge they share while participating in course discussions. The experimental results provided evidence that analysis of structured discussion data offers potential insights about changes in student collaboration patterns over time and students' sense of belongingness for pedagogical benefits. As future work, further analysis will be done by extracting additional students' data, such as their demographic data, majors, and performance in other courses to study cognitive and behavioral aspects from the collaboration networks.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3409334.3452042',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Manufacturing Audit Quality Analysis Model Based on Data Mining Technology',\n",
       "  'authors': \"['Fangyu Sun']\",\n",
       "  'date': 'December 2021',\n",
       "  'source': 'ICASIT 2021: 2021 International Conference on Aviation Safety and Information Technology',\n",
       "  'abstract': \"As our country's economic development enters a new normal, the original manufacturing development model can no longer meet the needs of current economic development, and it is urgent to accelerate the transformation of the manufacturing industry. At present, the country's supply-side structural reforms are deepening, and listed manufacturing companies are the most important backbone in terms of scale and innovation opportunities. Data mining technology is used to study the impact of quality control on corporate performance. Listed companies have a positive impact on the further realization of transformation and upgrading and the improvement of corporate performance. This article aims to study the manufacturing audit quality analysis model based on data mining technology, and adopts the analysis method of the combination of supervisory research and empirical analysis, from the perspective of supervisory research, summarizes the theory of internal audit quality and company performance in the research process, and summarizes predecessors' research results and research ideas. The experimental data in this article shows that the average quality of internal audit information disclosure is 3.1156, indicating that the audit disclosure status of listed companies selected by the Shenzhen Stock Exchange is good, and to a certain extent reflects the quality level of some internal controls.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3510858.3510863',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Designing Shapelets for Interpretable Data-Agnostic Classification',\n",
       "  'authors': \"['Riccardo Guidotti', 'Anna Monreale']\",\n",
       "  'date': 'July 2021',\n",
       "  'source': \"AIES '21: Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society\",\n",
       "  'abstract': 'Time series shapelets are discriminatory subsequences which are representative of a class, and their similarity to a time series can be used for successfully tackling the time series classification problem. The literature shows that Artificial Intelligence (AI) systems adopting classification models based on time series shapelets can be interpretable, more accurate, and significantly fast. Thus, in order to design a data-agnostic and interpretable classification approach, in this paper we first extend the notion of shapelets to different types of data, i.e., images, tabular and textual data. Then, based on this extended notion of shapelets we propose an interpretable data-agnostic classification method. Since the shapelets discovery can be time consuming, especially for data types more complex than time series, we exploit a notion of prototypes for finding candidate shapelets, and reducing both the time required to find a solution and the variance of shapelets. A wide experimentation on datasets of different types shows that the data-agnostic prototype-based shapelets returned by the proposed method empower an interpretable classification which is also fast, accurate, and stable. In addition, we show and we prove that shapelets can be at the basis of explainable AI methods.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3461702.3462553',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Research on E-commerce Big Data Classification and Mining Algorithm Based on BP Neural Network Technology',\n",
       "  'authors': \"['Ye Wang']\",\n",
       "  'date': 'December 2021',\n",
       "  'source': 'ICASIT 2021: 2021 International Conference on Aviation Safety and Information Technology',\n",
       "  'abstract': \"With the development of the Internet, with the gradual increase of people's needs, more and more people are engaged in various activities on the Internet, such as online transactions. This article mainly studies the classification and mining algorithm of e-commerce big data based on BP neural network technology. Take user movie rating data as the object of analysis and mining, and use platform functions to complete the whole process of data from preprocessing to data mining to result data storage. According to the requirements of the platform design modules and functions, the construction process from the installation of the cluster dependent tool software, node communication, cluster configuration to the final operation monitoring was completed, and the experimental environment of the e-commerce big data platform was established. According to the real open source movie rating data, according to the BP neural network algorithm, relying on the function of the module, the whole recommendation task from data processing, algorithm mining to the final result generation is completed. Through the comparison of speedup, accuracy, recall and coverage indicators, it can be seen that when the experimental data set is 100K, the recommendation efficiency in the cluster environment is not improved. The results show that the BP neural network improves the efficiency of the e-commerce big data platform and the accuracy of task execution results.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3510858.3510892',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Product-review Classification Combining Multiple Clustering Algorithms',\n",
       "  'authors': \"['Yilun Wei', 'Yingying Lao', 'Yudai Sato', 'Dongli Han']\",\n",
       "  'date': 'June 2019',\n",
       "  'source': \"ICFET '19: Proceedings of the 5th International Conference on Frontiers of Educational Technologies\",\n",
       "  'abstract': 'As product reviews accumulate more and more at online shopping sites, customers begin to have an increasing demand for analyzing reviews automatically. In some previous studies, clustering algorithms have been proved to be effective in grouping reviews. However, most of the existing systems are built based on a single clustering algorithm which might make the system fragile. In this paper, we have proposed a method to combine multiple clustering algorithms. Evaluation experiments have shown the effectiveness of our approach.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3338188.3338211',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'An Ideology for the Prediction of Critical Haplotype Blocks of Variants in Genes (Cyp2c9 And Vkorc1) for Warfarin (Anticoagulant) Drug Dosage to Treat Heart Patients Efficiently by Using Ml (Machine Learning) and Data Stream Mining Techniques',\n",
       "  'authors': \"['Hina Saeeda', 'Muhammad Adil Abid']\",\n",
       "  'date': 'November 2019',\n",
       "  'source': \"ICAIP '19: Proceedings of the 2019 3rd International Conference on Advances in Image Processing\",\n",
       "  'abstract': \"Now a day's on time treatment of heart diseases is a very critical part of medical diagnoses. So far there are total 50 SNP (Single Nucleotide Polymorphism) diagnosed that are responsible for the heart problems. But it is very hard to study all of the SNP together because of their different base pairs' locations or changes in base pairs positions (variations in genetic code A C G T). These all 50 SNP are present in all individuals with different variations, it is a tough job to calculate all the changes in this SNP set as there are total of (50^50) positions to calculate which is making it a huge data set. For acquiring a data set of all these positions, we will need some good Data Stream Mining (data mining techniques) to find out all the possible locations of all the variants responsible for the heart problems. In this research paper, we are giving a short analysis and introduction to the problem of heart patients drug dosage associated with anticoagulant (Warfarin) and its risks, solution for the challenge of calculating all variants of two genes (CYP2C9 and VKORC1) and advantages of the proposed solution in the future.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3373419.3373424',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A Self-Training Subspace Clustering Algorithm under Low-Rank Representation for Cancer Classification on Gene Expression Data',\n",
       "  'authors': \"['Chun-Qiu Xia', 'Ke Han', 'Yong Qi', 'Yang Zhang', 'Dong-Jun Yu']\",\n",
       "  'date': 'None',\n",
       "  'source': 'IEEE/ACM Transactions on Computational Biology and Bioinformatics',\n",
       "  'abstract': 'Accurate identification of the cancer types is essential to cancer diagnoses and treatments. Since cancer tissue and normal tissue have different gene expression, gene expression data can be used as an efficient feature source for cancer classification. However, accurate cancer classification directly using original gene expression profiles remains challenging due to the intrinsic high-dimension feature and the small size of the data samples. We proposed a new self-training subspace clustering algorithm under low-rank representation, called SSC-LRR, for cancer classification on gene expression data. Low-rank representation LRR is first applied to extract discriminative features from the high-dimensional gene expression data; the self-training subspace clustering SSC method is then used to generate the cancer classification predictions. The SSC-LRR was tested on two separate benchmark datasets in control with four state-of-the-art classification methods. It generated cancer classification predictions with an overall accuracy 89.7 percent and a general correlation 0.920, which are 18.9 and 24.4 percent higher than that of the best control method respectively. In addition, several genes RNF114, HLA-DRB5, USP9Y, and PTPN20 were identified by SSC-LRR as new cancer identifiers that deserve further clinical investigation. Overall, the study demonstrated a new sensitive avenue to recognize cancer classifications from large-scale gene expression data.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/TCBB.2017.2712607',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'OWSP-Miner: Self-adaptive One-off Weak-gap Strong Pattern Mining',\n",
       "  'authors': \"['Youxi Wu', 'Xiaohui Wang', 'Yan Li', 'Lei Guo', 'Zhao Li', 'Ji Zhang', 'Xindong Wu']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Management Information Systems',\n",
       "  'abstract': 'Gap constraint sequential pattern mining (SPM), as a kind of repetitive SPM, can avoid mining too many useless patterns. However, this method is difficult for users to set a suitable gap without prior knowledge and each character is considered to have the same effects. To tackle these issues, this article addresses a self-adaptive One-off Weak-gap Strong Pattern (OWSP) mining, which has three characteristics. First, it determines the gap constraint adaptively according to the sequence. Second, all characters are divided into two groups: strong and weak characters, and the pattern is composed of strong characters, while weak characters are allowed in the gaps. Third, each character can be used at most once in the process of support (the frequency of pattern) calculation. To handle this problem, this article presents OWSP-Miner, which equips with two key steps: support calculation and candidate pattern generation. A reverse-order filling strategy is employed to calculate the support of a candidate pattern, which reduces the time complexity. OWSP-Miner generates candidate patterns using pattern join strategy, which effectively reduces the candidate patterns. For clarification, time series is employed in the experiments and the results show that OWSP-Miner is not only more efficient but also is easier to mine valuable patterns. In the experiment of stock application, we also employ OWSP-Miner to mine OWSPs and the results show that OWSPs mining is more meaningful in real life. The algorithms and data can be downloaded at https://github.com/wuc567/Pattern-Mining/tree/master/OWSP-Miner.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3476247',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'An Extensive Study on Cross-Dataset Bias and Evaluation Metrics Interpretation for Machine Learning Applied to Gastrointestinal Tract Abnormality Classification',\n",
       "  'authors': \"['Vajira Thambawita', 'Debesh Jha', 'Hugo Lewi Hammer', 'Håvard D. Johansen', 'Dag Johansen', 'Pål Halvorsen', 'Michael A. Riegler']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Computing for Healthcare',\n",
       "  'abstract': 'Precise and efficient automated identification of gastrointestinal (GI) tract diseases can help doctors treat more patients and improve the rate of disease detection and identification. Currently, automatic analysis of diseases in the GI tract is a hot topic in both computer science and medical-related journals. Nevertheless, the evaluation of such an automatic analysis is often incomplete or simply wrong. Algorithms are often only tested on small and biased datasets, and cross-dataset evaluations are rarely performed. A clear understanding of evaluation metrics and machine learning models with cross datasets is crucial to bring research in the field to a new quality level. Toward this goal, we present comprehensive evaluations of five distinct machine learning models using global features and deep neural networks that can classify 16 different key types of GI tract conditions, including pathological findings, anatomical landmarks, polyp removal conditions, and normal findings from images captured by common GI tract examination instruments. In our evaluation, we introduce performance hexagons using six performance metrics, such as recall, precision, specificity, accuracy, F1-score, and the Matthews correlation coefficient to demonstrate how to determine the real capabilities of models rather than evaluating them shallowly. Furthermore, we perform cross-dataset evaluations using different datasets for training and testing. With these cross-dataset evaluations, we demonstrate the challenge of actually building a generalizable model that could be used across different hospitals. Our experiments clearly show that more sophisticated performance metrics and evaluation methods need to be applied to get reliable models rather than depending on evaluations of the splits of the same dataset—that is, the performance metrics should always be interpreted together rather than relying on a single metric.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3386295',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A Semantic Graph-Based Approach for Mining Common Topics from Multiple Asynchronous Text Streams',\n",
       "  'authors': \"['Long Chen', 'Joemon M. Jose', 'Haitao Yu', 'Fajie Yuan']\",\n",
       "  'date': 'April 2017',\n",
       "  'source': \"WWW '17: Proceedings of the 26th International Conference on World Wide Web\",\n",
       "  'abstract': 'In the age of Web 2.0, a substantial amount of unstructured content are distributed through multiple text streams in an asynchronous fashion, which makes it increasingly difficult to glean and distill useful information. An effective way to explore the information in text streams is topic modelling, which can further facilitate other applications such as search, information browsing, and pattern mining. In this paper, we propose a semantic graph based topic modelling approach for structuring asynchronous text streams. Our model integrates topic mining and time synchronization, two core modules for addressing the problem, into a unified model. Specifically, for handling the lexical gap issues, we use global semantic graphs of each timestamp for capturing the hidden interaction among entities from all the text streams. For dealing with the sources asynchronism problem, local semantic graphs are employed to discover similar topics of different entities that can be potentially separated by time gaps. Our experiment on two real-world datasets shows that the proposed model significantly outperforms the existing ones.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3038912.3052630',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Deep Learning Method for Spectral Classification',\n",
       "  'authors': \"['Tianxu Zhang', 'Yao Ma', 'Hang Lin']\",\n",
       "  'date': 'November 2020',\n",
       "  'source': \"ICAIP '20: Proceedings of the 4th International Conference on Advances in Image Processing\",\n",
       "  'abstract': 'Spectral images contain both spatial information and spectral information. The resolution of spectral information is very high, generally reaching nanometer level, but the spatial resolution is relatively low. Spectral image classification is a pixel level classification problem [1]. Specifically, it is to classify each pixel in the image and confirm the category of the pixel.Spectral image classification can be divided into unsupervised classification and supervised classification (including semi supervised classification) [2]. Unsupervised classification in deep learning refers to the classification (clustering) of spectral images without data labels in advance. The main idea is to classify similar pixels into one group according to the characteristic information (spatial information, spectral information and characteristics, etc.) that can represent the characteristics of pixels [3]. Supervised classification refers to the classification of spectral images when there are pre-labeled data as supervisory signals. The main idea is to use the labeled data to learn the intrinsic relationship between pixel feature information and pixel categories, and then use this relationship to classify the unlabeled data to determine the pixel category [4].',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3441250.3441268',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Using opinion mining techniques on Twitter streaming data regards drug safety issues',\n",
       "  'authors': \"['Abeer Nafel Alharbi', 'Hessah Alnamlah', 'Liyakathunsia Syed']\",\n",
       "  'date': 'March 2017',\n",
       "  'source': \"ICC '17: Proceedings of the Second International Conference on Internet of things, Data and Cloud Computing\",\n",
       "  'abstract': \"In this paper, we propose a four-step guideline to perform twitter mining based on consumer's opinion and adverse drug reaction of certain drugs. Due to advances in technology and increased use of social networks, there has been a tremendous amount of public data which grows from terabytes to petabytes. The accessibility of this enormous amount of data offers vast research opportunities for extracting meaningful opinion data for many applications. Drug consumption is one that could be benefited, from methodical sentiment analysis techniques. In this paper we have focused on social media mining for drug related information. To clean the Twitter streaming data and to increase the accuracy of the results, a spam filter and a preprocessing procedure have been developed, to retrieve relevant information about certain drug. Processing and analysis of data were done on 1579 tweets using R-programming. The results show that Twitter mining with formed word cloud is a very useful technique to get the majority of consumer's opinion about the consumed drug. The obtained results shows that, real time streaming of social networking data could help in early detection and prediction of side effects of the drug for patient safety.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3018896.3036386',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': \"Spatial data mining and O-D hotspots discovery in cities based on an O-D hotspots clustering model using vehicles' GPS data: a case study in the morning rush hours in Beijing, China\",\n",
       "  'authors': \"['Xiaoyong Ni', 'Hong Huang', 'Shiwei Zhou', 'Boni Su', 'Yangyang Meng', 'Zhongliang Huang']\",\n",
       "  'date': 'November 2018',\n",
       "  'source': \"Safety and Resilience'18: Proceedings of the 4th ACM SIGSPATIAL International Workshop on Safety and Resilience\",\n",
       "  'abstract': 'With the rapid development of cities in recent years, the size of the cities is becoming bigger and bigger and the structure of the cities is becoming more and more complex. The first step to study the urban resilience is hotspots mining and POI analysis. This paper established an O-D hotspots clustering model based on Iterative Self Organizing Data Analysis Techniques Algorithm (hereinafter referred to as ISODATA) to mine the Origin-Destination (hereinafter referred to as O-D) hotspots in the rush hours in cities and study the distribution characteristics of Point of Interests (hereinafter referred to as POIs) in the hotspots area. It is found that the pick-up hotspots tend to be gathered in the residential zones and the drop-off hotspots tend to be gathered in the working zones. Besides, the distribution characteristics of POIs in both pick-up and drop-off hotspots areas and huge railway stations (special drop-off hotspots) areas are quite special. This study provides an in-depth understanding of the structure of the cities and provides an effective guidance in urban zones planning. This study also provides fundamental knowledge for urban resilience design.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3284103.3284108',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Three-level binary tree structure for sentiment classification in Arabic text',\n",
       "  'authors': \"['Hajar Ait Addi', 'Redouane Ezzahir', 'Abdelhak Mahmoudi']\",\n",
       "  'date': 'March 2020',\n",
       "  'source': \"NISS '20: Proceedings of the 3rd International Conference on Networking, Information Systems &amp; Security\",\n",
       "  'abstract': 'The advent of web 2.0 platforms allowed users to generate and share textual content. This results in an explosive increase of online personal opinion. Sentiment Analysis, which is a recent field of Natural Language Processing, aims to predict the orientation of sentiment present on this massive textual data. This plays a vital role in many applications, such as recommender systems, customer intelligence, information retrieval and psychological study of crowd. Most existing approaches in sentiment analysis trait only positive, negative and neutral classes, ignoring the class strength (weak or strong positive/negative). In this paper, we propose an innovative approach for multi-class hierarchical sentiment classification in Arabic text based on a three-level binary tree structure. Experimental results show that our approach gives significant improvements over other classification methods.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3386723.3387844',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Text Classification Based on Keywords with Different Thresholds',\n",
       "  'authors': \"['Tu Cam Thi Tran', 'Hiep Xuan Huynh', 'Phuc Quang Tran', 'Dinh Quoc Truong']\",\n",
       "  'date': 'February 2019',\n",
       "  'source': \"ICIIT '19: Proceedings of the 2019 4th International Conference on Intelligent Information Technology\",\n",
       "  'abstract': 'Text classification is a supervised learning task for assigning text document to one or more predefined classes/topics. These topics are determined by a set of training documents. In order to construct a classification model, a machine learning algorithm was used. The training model is used to predict a class for new coming document. In this paper, we propose a text classification approach based on automatic keywords extraction with different thresholes. We use 3000 Vietnamese text documents, which belong to ten topics, downloaded from two electronic magazines vnexpress.net and vietnamnet.vn to create ten sets of the keywords. These keywords are used to predict the topic of new text document. The experimental results confirm the feasibility of proposed model.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3321454.3321473',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'End-to-end Learning for Short Text Expansion',\n",
       "  'authors': \"['Jian Tang', 'Yue Wang', 'Kai Zheng', 'Qiaozhu Mei']\",\n",
       "  'date': 'August 2017',\n",
       "  'source': \"KDD '17: Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining\",\n",
       "  'abstract': 'Effectively making sense of short texts is a critical task for many real world applications such as search engines, social media services, and recommender systems. The task is particularly challenging as a short text contains very sparse information, often too sparse for a machine learning algorithm to pick up useful signals. A common practice for analyzing short text is to first expand it with external information, which is usually harvested from a large collection of longer texts. In literature, short text expansion has been done with all kinds of heuristics. We propose an end-to-end solution that automatically learns how to expand short text to optimize a given learning task. A novel deep memory network is proposed to automatically find relevant information from a collection of longer documents and reformulate the short text through a gating mechanism. Using short text classification as a demonstrating task, we show that the deep memory network significantly outperforms classical text expansion methods with comprehensive experiments on real world data sets.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3097983.3098166',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Deep specification mining',\n",
       "  'authors': \"['Tien-Duy B. Le', 'David Lo']\",\n",
       "  'date': 'July 2018',\n",
       "  'source': 'ISSTA 2018: Proceedings of the 27th ACM SIGSOFT International Symposium on Software Testing and Analysis',\n",
       "  'abstract': 'Formal specifcations are essential but usually unavailable in software systems. Furthermore, writing these specifcations is costly and requires skills from developers. Recently, many automated techniques have been proposed to mine specifcations in various formats including fnite-state automaton (FSA). However, more works in specifcation mining are needed to further improve the accuracy of the inferred specifcations. In this work, we propose Deep Specifcation Miner (DSM), a new approach that performs deep learning for mining FSA-based specifcations. Our proposed approach uses test case generation to generate a richer set of execution traces for training a Recurrent Neural Network Based Language Model (RNNLM). From these execution traces, we construct a Prefx Tree Acceptor (PTA) and use the learned RNNLM to extract many features. These features are subsequently utilized by clustering algorithms to merge similar automata states in the PTA for constructing a number of FSAs. Then, our approach performs a model selection heuristic to estimate F-measure of FSAs and returns the one with the highest estimated Fmeasure. We execute DSM to mine specifcations of 11 target library classes. Our empirical analysis shows that DSM achieves an average F-measure of 71.97%, outperforming the best performing baseline by 28.22%. We also demonstrate the value of DSM in sandboxing Android apps.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3213846.3213876',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Opinion mining using an LVQ neural network',\n",
       "  'authors': \"['Matthaios Stylianidis', 'Eleni Galiotou', 'Cleo Sgouropoulou', 'Christos Skourlas']\",\n",
       "  'date': 'September 2017',\n",
       "  'source': \"PCI '17: Proceedings of the 21st Pan-Hellenic Conference on Informatics\",\n",
       "  'abstract': \"Due to the increased use of social media in the past few years, a large volume of data has been accumulated which contains human sentiments and opinions. The field that deals with the automated extraction of opinions is named opinion mining. In this paper, we evaluate the performance of an LVQ neural network on document level analysis using a benchmark movie review dataset. Document-level opinion mining aims at classifying a text, usually as positive or negative based on its overall sentiment. In order to reduce the dimensions of the reviews' vector representations, we use the feature selection method Information Gain. We use an exhaustive grid search for hyperparameter tuning and two methods for performance evaluation: a nested cross validation and a non-nested 10-fold cross validation. We study the performance of our model for different numbers of selected features by Information-Gain.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3139367.3139416',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A new text representation method for clustering based on higher order Markov model',\n",
       "  'authors': \"['Weifeng Yang', 'Guosheng Han', 'Xiaoqiang Xie']\",\n",
       "  'date': 'April 2018',\n",
       "  'source': \"ICISDM '18: Proceedings of the 2nd International Conference on Information System and Data Mining\",\n",
       "  'abstract': 'The ordinal relations in word sequence and character sequence can reflect the latent information about writing style, genre features and topic. Thus, the ordinal relations are important information and should be considered for text clustering. However, the ordinal relations were often neglected in the traditional methods of text clustering. In view of that the ordinal relations can be statistically characterized by the transition probabilities of the higher order Markov model, in this paper, a new method based on higher order Markov model was proposed for text representation. In the new method, all transition probabilities of a higher order Markov model are used as features of text, and the order is identified by maximizing the average Markov-Shannon entropy (MME). The experimental results imply the new text representation method performs better than traditional method.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3206098.3206099',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Analyzing the Color Image of Taiwan Town by Using Data Mining',\n",
       "  'authors': \"['Yu-Wei Su', 'Tzren Ru Chou']\",\n",
       "  'date': 'February 2019',\n",
       "  'source': \"ICSCA '19: Proceedings of the 2019 8th International Conference on Software and Computer Applications\",\n",
       "  'abstract': 'Researches have pointed out that the colors have the function of conveying messages and is even easier than words to be memorized. The color image of a city, which means people connect with color through knowledge acquired and their life experiences. It includes landscapes, buildings, food, the city culture, local specialties and so on. Thus, building up the distinctive style of a city is an important part of the city image and its related applications. Nowadays, people mostly use three different applications in order to set up and correct the city color tickets. First, field research; second, residents participate in the comprehensive community development; third, particular projects. In this paper, we use data mining to analyze the connecting between people and the city color image. At the beginning, we select ten cities of Taiwan, which were elected by Taiwan Tourism Bureau as our research object. Secondly, we use Word to vector and Google searching engine to find the relevance between the city and adjectives. For the third step, the highest connection of adjective will be the keyword of Google searching engine to collect the pictures by doing cross-comparison of the results. Last but not least, we capture ten colors from city pictures and result in city color combinations. According to the result of this paper, although it needs to adjust in accordance with local culture, we still can find the regional pictures that correspond with the characteristics and also can obtain the city color combinations, which can be used as a reference of harmonious colors.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3316615.3316624',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Mining Expressive Rules in Knowledge Graphs',\n",
       "  'authors': \"['Naser Ahmadi', 'Viet-Phi Huynh', 'Vamsi Meduri', 'Stefano Ortona', 'Paolo Papotti']\",\n",
       "  'date': 'None',\n",
       "  'source': 'Journal of Data and Information Quality',\n",
       "  'abstract': 'We describe RuDiK, an algorithm and a system for mining declarative rules over RDF knowledge graphs (KGs). RuDiK can discover rules expressing both positive relationships between KG elements, e.g., “if two persons share at least one parent, they are likely to be siblings,” and negative patterns identifying data contradictions, e.g., “if two persons are married, one cannot be the child of the other” or “the birth year for a person cannot be bigger than her graduation year.” While the first kind of rules identify new facts in the KG, the second kind enables the detection of incorrect triples and the generation of (training) negative examples for learning algorithms. High-quality rules are also critical for any reasoning task involving the KGs.Our approach increases the expressive power of the supported rule language w.r.t. the existing systems. RuDiK discovers rules containing (i) comparisons among literal values and (ii) selection conditions with constants. Richer rules increase the accuracy and the coverage over the facts in the KG for the task at hand. This is achieved with aggressive pruning of the search space and with disk-based algorithms, which enable the execution of the system in commodity machines. Also, RuDiK is robust to errors and missing data in the input graph. It discovers approximate rules with a measure of support that is aware of the quality issues. Our experimental evaluation with real-world KGs shows that RuDiK does better than existing solutions in terms of scalability and that it can identify effective rules for different target applications.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3371315',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'TLS Encrypted Application Classification Using Machine Learning with Flow Feature Engineering',\n",
       "  'authors': \"['Onur Barut', 'Rebecca Zhu', 'Yan Luo', 'Tong Zhang']\",\n",
       "  'date': 'November 2020',\n",
       "  'source': \"ICCNS '20: Proceedings of the 2020 10th International Conference on Communication and Network Security\",\n",
       "  'abstract': 'Network traffic classification has become increasingly important as the number of devices connected to the Internet is rapidly growing. Proportionally, the amount of encrypted traffic is also increasing, making payload based classification methods obsolete. Consequently, machine learning approaches have become crucial when user privacy is concerned. For this purpose, we propose an accurate, fast, and privacy preserved encrypted traffic classification approach with engineered flow feature extraction and appropriate feature selection. The proposed scheme achieves a 0.92899 macro-average F1 score and a 0.88313 macro-averaged mAP score for the encrypted traffic classification of Audio, Email, Chat, and Video classes derived from the non-vpn2016 dataset. Further experiments on the mixed non-encrypted and encrypted flow dataset with a data augmentation method called Synthetic Minority Over-Sampling Technique are conducted and the results are discussed for TLS-encrypted and mixed flows.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3442520.3442529',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Discussion on Information Overlap in Hyperspectral Image Classification Based on Machine Learning',\n",
       "  'authors': \"['Qingjie Yang', 'Rong Zhang', 'Yi Fang']\",\n",
       "  'date': 'January 2021',\n",
       "  'source': \"ICIGP '21: Proceedings of the 2021 4th International Conference on Image and Graphics Processing\",\n",
       "  'abstract': 'Principal component analysis(PCA) and spectral-spatial methods are usually used in hyperspectral images(HSI) classification based on machine learning.\\xa0These methods have achieved high classification accuracy for several publicly hyperspectral datasets. However, we believe that high accuracy is mainly due to information overlap between the training and test set. In this paper, we have discussed the problem of information overlap caused by PCA and neighborhood region extraction.\\xa0First, the PCA transformation matrix is fitted on the whole image, which leads to the spectral information overlap between the training and test samples.\\xa0Second, there are pixels for testing in the neighborhood regions extracted by the training pixels, which cause spatial information overlap. In order to explore the influence of information overlap on classification results, we compare the classification results with or without information overlap using support vector machine (SVM) and convolutional neural network (CNN). Experiments indicate that classification results in above two information overlap cases are less reliable.\\xa0In fact, in the method based on machine learning, the test set should be completely independent of the training set to ensure the accuracy, reliability and universality of the classification results.\\xa0Therefore, the discussion on information overlap in HSIs classification method creates a new approach in future research and design for more reliable hyperspectral classification methods.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3447587.3447610',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'AutoSmart: An Efficient and Automatic Machine Learning Framework for Temporal Relational Data',\n",
       "  'authors': \"['Zhipeng Luo', 'Zhixing He', 'Jin Wang', 'Manqing Dong', 'Jianqiang Huang', 'Mingjian Chen', 'Bohang Zheng']\",\n",
       "  'date': 'August 2021',\n",
       "  'source': \"KDD '21: Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining\",\n",
       "  'abstract': \"Temporal relational data, perhaps the most commonly used data type in industrial machine learning applications, needs labor-intensive feature engineering and data analyzing for giving precise model predictions. An automatic machine learning framework is needed to ease the manual efforts in fine-tuning the models so that the experts can focus more on other problems that really need humans' engagement such as problem definition, deployment, and business services. However, there are three main challenges for building automatic solutions for temporal relational data: 1) how to effectively and automatically mining useful information from the multiple tables and the relations from them? 2) how to be self-adjustable to control the time and memory consumption within a certain budget? and 3) how to give generic solutions to a wide range of tasks? In this work, we propose our solution that successfully addresses the above issues in an end-to-end automatic way. The proposed framework, AutoSmart, is the winning solution to the KDD Cup 2019 of the AutoML Track, which is one of the largest AutoML competition to date (860 teams with around 4,955 submissions). The framework includes automatic data processing, table merging, feature engineering, and model tuning, with a time and memory controller for efficiently and automatically formulating the models. The proposed framework outperforms the baseline solution significantly on several datasets in various domains.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3447548.3467088',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Similarity Association Pattern Mining in Transaction Databases',\n",
       "  'authors': \"['Phridviraj M.S.B', 'Guru Rao C.V', 'Radhakrishna Vangipuram', 'Aravind Cheruvu']\",\n",
       "  'date': 'April 2021',\n",
       "  'source': \"DATA'21: International Conference on Data Science, E-learning and Information Systems 2021\",\n",
       "  'abstract': 'Association pattern mining is a method of finding interesting relationships or patterns between item sets present in each of the transactions of the transactional databases. Current researchers in this area are focusing on the data mining task of finding frequent patterns among the item sets based on the interestingness measures like the support and confidence which is called as Frequent pattern mining. Till date, in existing frequent pattern mining algorithms, an itemset is said to be frequent if the support of the itemset satisfies the minimum support input. In this paper, the objective of our algorithm is to find interesting patterns among the item sets based on a Gaussian similarity for an input reference threshold which is first of its kind in the research literature. This study is limited to outlining naïve approach of mining frequent itemsets which requires validating every itemset to verify if the itemset is frequent or not.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3460620.3460752',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Augmented SVM with ordinal partitioning for text classification',\n",
       "  'authors': \"['Yong Shi', 'Peijia Li', 'Lingfeng Niu']\",\n",
       "  'date': 'August 2017',\n",
       "  'source': \"WI '17: Proceedings of the International Conference on Web Intelligence\",\n",
       "  'abstract': 'Ordinal regression has received increasing interest in the past years. It aims to classify patterns by an ordinal scale. With the the explosive growth of data, the method of SVM with ordinal partitioning called SVMOP highlights its advantages due to its convenience of dealing with large scale data. However, the method of SVMOP for ordinal regression has not been exploited much. As we know, the costs should be different when dealing with mislabeled samples and how to use them plays a dominant role in model building. However, L2-loss which could enlarge the cost sensitivity has not been applied into SVM ordinal partition yet. In this paper, we propose the method of SVMOP with L2-loss for ordinal regression. Numerical results show that our approach outperforms the method of SVMOP with L1-loss and other ordianl regression models.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3106426.3109428',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Data Integration and Machine Learning: A Natural Synergy',\n",
       "  'authors': \"['Xin Luna Dong', 'Theodoros Rekatsinas']\",\n",
       "  'date': 'May 2018',\n",
       "  'source': \"SIGMOD '18: Proceedings of the 2018 International Conference on Management of Data\",\n",
       "  'abstract': 'There is now more data to analyze than ever before. As data volume and variety have increased, so have the ties between machine learning and data integration become stronger. For machine learning to be effective, one must utilize data from the greatest possible variety of sources; and this is why data integration plays a key role. At the same time machine learning is driving automation in data integration, resulting in overall reduction of integration costs and improved accuracy. This tutorial focuses on three aspects of the synergistic relationship between data integration and machine learning: (1) we survey how state-of-the-art data integration solutions rely on machine learning-based approaches for accurate results and effective human-in-the-loop pipelines, (2) we review how end-to-end machine learning applications rely on data integration to identify accurate, clean, and relevant data for their analytics exercises, and (3) we discuss open research challenges and opportunities that span across data integration and machine learning.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3183713.3197387',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Classification and Clustering of arXiv Documents, Sections, and Abstracts, Comparing Encodings of Natural and Mathematical Language',\n",
       "  'authors': \"['Philipp Scharpf', 'Moritz Schubotz', 'Abdou Youssef', 'Felix Hamborg', 'Norman Meuschke', 'Bela Gipp']\",\n",
       "  'date': 'August 2020',\n",
       "  'source': \"JCDL '20: Proceedings of the ACM/IEEE Joint Conference on Digital Libraries in 2020\",\n",
       "  'abstract': 'In this paper, we show how selecting and combining encodings of natural and mathematical language affect classification and clustering of documents with mathematical content. We demonstrate this by using sets of documents, sections, and abstracts from the arXiv preprint server that are labeled by their subject class (mathematics, computer science, physics, etc.) to compare different encodings of text and formulae and evaluate the performance and runtimes of selected classification and clustering algorithms. Our encodings achieve classification accuracies up to 82.8% and cluster purities up to 69.4% (number of clusters equals number of classes), and 99.9% (unspecified number of clusters) respectively. We observe a relatively low correlation between text and math similarity, which indicates the independence of text and formulae and motivates treating them as separate features of a document. The classification and clustering can be employed, e.g., for document search and recommendation. Furthermore, we show that the computer outperforms a human expert when classifying documents. Finally, we evaluate and discuss multi-label classification and formula semantification.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3383583.3398529',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Hybrid machine learning methods for demand forecasting: consecutive application of classification and regression methods for the forecasting of periodic and non-continuous demand',\n",
       "  'authors': \"['Víctor Álvarez-López', 'B. Rosario Campomanes-Álvarez', 'Pelayo Quirós']\",\n",
       "  'date': 'January 2019',\n",
       "  'source': \"APPIS '19: Proceedings of the 2nd International Conference on Applications of Intelligent Systems\",\n",
       "  'abstract': 'This paper is focused on demand forecasting, where the orders from each customer are generated periodically but in a non-continuous way, so most of the values for each client and temporal instant are zeros. The application of regression models is compared to hybrid methods, where an initial classification is considered in order to identify the temporal instants in which an order has been predicted, and afterwards, a regression is generated to obtain the predicted amount of such order. This procedure is complemented by an application to real demand data, for selecting the proper methods for both phases, as well as comparing to simple regression models.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3309772.3309773',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Compatibility-Aware Web API Recommendation for Mashup Creation via Textual Description Mining',\n",
       "  'authors': \"['Lianyong Qi', 'Houbing Song', 'Xuyun Zhang', 'Gautam Srivastava', 'Xiaolong Xu', 'Shui Yu']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Multimedia Computing, Communications, and Applications',\n",
       "  'abstract': \"With the ever-increasing prosperity of web Application Programming Interface (API) sharing platforms, it is becoming an economic and efficient way for software developers to design their interested mashups through web API re-use. Generally, a software developer can browse, evaluate, and select his or her preferred web APIs from the API's sharing platforms to create various mashups with rich functionality. The big volume of candidate APIs places a heavy burden on software developers’ API selection decisions. This, in turn, calls for the support of intelligent API recommender systems. However, existing API recommender systems often face two challenges. First, they focus more on the functional accuracy of APIs while neglecting the APIs’ actual compatibility. This then creates incompatible mashups. Second, they often require software developers to input a set of keywords that can accurately describe the expected functions of the mashup to be developed. This second challenge tests partial developers who have little background knowledge in the fields. To tackle the above-mentioned challenges, in this article we propose a compatibility-aware and text description-driven web API recommendation approach (named WARtext). WARtext guarantees the compatibility among the recommended APIs by utilizing the APIs’ composition records produced by historical mashup creations. Besides, WARtext entitles a software developer to type a simple text document that describes the expected mashup functions as input. Then through textual description mining, WARtext can precisely capture the developers’ functional requirements and then return a set of APIs with the highest compatibility. Finally, through a real-world mashup dataset ProgrammableWeb, we validate the feasibility of our novel approach.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3417293',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Website Defacements Detection Based on Support Vector Machine Classification Method',\n",
       "  'authors': \"['Siyan Wu', 'Xiaojun Tong', 'Wei Wang', 'Guodong Xin', 'Bailing Wang', 'Qi Zhou']\",\n",
       "  'date': 'May 2018',\n",
       "  'source': \"ICCDE '18: Proceedings of the 2018 International Conference on Computing and Data Engineering\",\n",
       "  'abstract': 'Website defacements can inflict significant harm on the website owner through the loss of reputation, the loss of money, or the leakage of information. Due to the complexity and diversity of all kinds of web application systems, especially a lack of necessary security maintenance, website defacements increased year by year. In this paper, we focus on detecting whether the website has been defaced by extracting website features and website embedded trojan features. We use three kinds of classification learning algorithms which include Gradient Boosting Decision Tree (GBDT), Random Forest (RF) and Support Vector Machine (SVM) to do the classification experiments, and experimental results show that Support Vector Machine classifier performed better than two other classifiers. It can achieve an overall accuracy of 95%-96% in detecting website defacements.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3219788.3219804',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Towards rapid interactive machine learning: evaluating tradeoffs of classification without representation',\n",
       "  'authors': \"['Dustin Arendt', 'Emily Saldanha', 'Ryan Wesslen', 'Svitlana Volkova', 'Wenwen Dou']\",\n",
       "  'date': 'March 2019',\n",
       "  'source': \"IUI '19: Proceedings of the 24th International Conference on Intelligent User Interfaces\",\n",
       "  'abstract': 'Our contribution is the design and evaluation of an interactive machine learning interface that rapidly provides the user with model feedback after every interaction. To address visual scalability, this interface communicates with the user via a \"tip of the iceberg\" approach, where the user interacts with a small set of recommended instances for each class. To address computational scalability, we developed an O(n) classification algorithm that incorporates user feedback incrementally, and without consulting the data\\'s underlying representation matrix. Our computational evaluation showed that this algorithm has similar accuracy to several off-the-shelf classification algorithms with small amounts of labeled data. Empirical evaluation revealed that users performed better using our design compared to an equivalent active learning setup.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3301275.3302280',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Compressive Sensing Based on Homomorphic Encryption and Attack Classification using Machine Learning Algorithm in WSN Security',\n",
       "  'authors': \"['Samir Ifzarne', 'Imad Hafidi', 'Nadia Idrissi']\",\n",
       "  'date': 'March 2020',\n",
       "  'source': \"NISS '20: Proceedings of the 3rd International Conference on Networking, Information Systems &amp; Security\",\n",
       "  'abstract': \"Data protection is essential for sensitive applications using Wireless Sensor Networks like health monitoring or video surveillance. WSN are deployed generally in harsh environment making them vulnerable for attacks thus it's important to secure data while being transferred from the sensor until the base station. This article is a proposal for a methodology which enable attack detection and classification on WSN. Compressive sensing is used to optimize the size of data exchange and hence optimize energy consumption. Homomorphic encryption allows to reduce encryption complexity by applying arithmetic operations on cypher text. Machine learning is applied to classify the attacks quickly.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3386723.3387859',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Distance Based Pattern Driven Mining for Outlier Detection in High Dimensional Big Dataset',\n",
       "  'authors': \"['Ankit Kumar', 'Abhishek Kumar', 'Ali Kashif Bashir', 'Mamoon Rashid', 'V. D. Ambeth Kumar', 'Rupak Kharel']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Management Information Systems',\n",
       "  'abstract': 'Detection of outliers or anomalies is one of the vital issues in pattern-driven data mining. Outlier detection detects the inconsistent behavior of individual objects. It is an important sector in the data mining field with several different applications such as detecting credit card fraud, hacking discovery and discovering criminal activities. It is necessary to develop tools used to uncover the critical information established in the extensive data. This paper investigated a novel method for detecting cluster outliers in a multidimensional dataset, capable of identifying the clusters and outliers for datasets containing noise. The proposed method can detect the groups and outliers left by the clustering process, like instant irregular sets of clusters (C) and outliers (O), to boost the results. The results obtained after applying the algorithm to the dataset improved in terms of several parameters. For the comparative analysis, the accurate average value and the recall value parameters are computed. The accurate average value is 74.05% of the existing COID algorithm, and our proposed algorithm has 77.21%. The average recall value is 81.19% and 89.51% of the existing and proposed algorithm, which shows that the proposed work efficiency is better than the existing COID algorithm.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3469891',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Explanation Mining: Post Hoc Interpretability of Latent Factor Models for Recommendation Systems',\n",
       "  'authors': \"['Georgina Peake', 'Jun Wang']\",\n",
       "  'date': 'July 2018',\n",
       "  'source': \"KDD '18: Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining\",\n",
       "  'abstract': 'The widescale use of machine learning algorithms to drive decision-making has highlighted the critical importance of ensuring the interpretability of such models in order to engender trust in their output. The state-of-the-art recommendation systems use black-box latent factor models that provide no explanation of why a recommendation has been made, as they abstract their decision processes to a high-dimensional latent space which is beyond the direct comprehension of humans. We propose a novel approach for extracting explanations from latent factor recommendation systems by training association rules on the output of a matrix factorisation black-box model. By taking advantage of the interpretable structure of association rules, we demonstrate that predictive accuracy of the recommendation model can be maintained whilst yielding explanations with high fidelity to the black-box model on a unique industry dataset. Our approach mitigates the accuracy-interpretability trade-off whilst avoiding the need to sacrifice flexibility or use external data sources. We also contribute to the ill-defined problem of evaluating interpretability.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3219819.3220072',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Application of Support Vector Machine Methods in Classification of Customer Communications Data',\n",
       "  'authors': \"['Huynh Tan Hoi', 'Le Vu Truong']\",\n",
       "  'date': 'August 2020',\n",
       "  'source': 'CCCIS 2020: Proceedings of the 2020 International Conference on Computer Communication and Information Systems',\n",
       "  'abstract': 'The paper used automatic information extraction technique, text classification by SVM (Support vector machine) method, combined with Vietnamese word separation technique and natural language processing. Application results of the research used in extracting information, collecting user feedback from e-commerce websites, social networking sites providing businesses with useful information from contributing users in order to build an effective business strategy. The main contributions of this paper are the following: • Apply the BiGAN, the ability of the model to learn mutually coherent inference and generation networks through the inspections of model samples and reconstructions and confirm the usefulness of the learned representations by obtaining a performance competitive with state-of-the-art on Motorbike Generator task. • Improves learning for BiGAN outperforming several state-of-the-art GAN methods training on motorbike dataset by a few techniques such as preprocessing data, data augmentation and hyperparameter tuning.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3418994.3419002',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Lifelong Machine Learning: Outlook and Direction',\n",
       "  'authors': \"['Xianbin Hong', 'Prudence Wong', 'Dawei Liu', 'Sheng-Uei Guan', 'Ka Lok Man', 'Xin Huang']\",\n",
       "  'date': 'October 2018',\n",
       "  'source': \"ICBDR '18: Proceedings of the 2nd International Conference on Big Data Research\",\n",
       "  'abstract': 'The Lifelong machine learning is an advanced machine learning paradigm and also is the key to the stronger AI. In this paper, we review the development history of the lifelong machine learning and evaluate the current stage. The aim, definition and main components of it is introduced. In addition, the bottleneck and possible solution also is discussed and the further development waypoint is proposed.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3291801.3291829',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Discriminant Projection Shared Dictionary Learning for Classification of Tumors Using Gene Expression Data',\n",
       "  'authors': \"['Shaoliang Peng', 'Yaning Yang', 'Wei Liu', 'Fei Li', 'Xiangke Liao']\",\n",
       "  'date': 'None',\n",
       "  'source': 'IEEE/ACM Transactions on Computational Biology and Bioinformatics',\n",
       "  'abstract': 'With a variety of tumor subtypes, personalized treatments need to identify the subtype of a tumor as accurately as possible. The development of DNA microarrays provides an opportunity to predict tumor classification. One strategy is to use gene expression profiling to extend current biological insights into the disease. However, overfitting problems exist in most machine learning methods when classifying tumor gene expression profile data characterized by high dimensional, small samples and nonlinearities. As a new machine learning methods, dictionary learning has become a more effective algorithm for gene expression profile classification. Here, a new method called discriminant projection shared dictionary learning (DPSDL) is proposed for classifying tumor subtypes using LINCS gene expression profile data. The method trains a shared dictionary, embeds Fisher discriminant criteria to obtain a class-specific sub-dictionary and coding coefficients. At the same time, a projection matrix is trained to widen the distance between different classes of samples. Experimental results show that our method performs better classification based on gene expression profile than the other dictionary learning methods and machine learning methods.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/TCBB.2019.2950209',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Implicit data recommendation based on refined classification and ranking learning',\n",
       "  'authors': \"['Yuwei Liu']\",\n",
       "  'date': 'November 2020',\n",
       "  'source': \"ICBDR '20: Proceedings of the 4th International Conference on Big Data Research\",\n",
       "  'abstract': \"With the exponential growth of data, it becomes more and more difficult to quickly obtain valuable information from massive amounts of data. Clicking and browsing of such non-scoring implicit data has also attracted more and more attention from scholars. Recommendations for implicit data generally use bayesian personalized ranking algorithm (BPR). The algorithm focuses on the difference in preferences between item pairs, and believes that users prefer items that have interacted with items that have never interacted. However, this assumption is still not specific enough for analyzing the relationship between users' items. Therefore, this paper expands the single pairwise sorting algorithm into a more detailed pair-level parallel sorting. First, the non-interactive item set is refined into two categories through the concept of frequent item sets: uncertain feedback and negative feedback. Secondly, the algorithm of fusion of BPR and list sorting is used to relax the assumptions of independence, and two sets are analyzed separately. This also alleviates the sparsity problem of implicit data due to natural imbalance. Finally, a simulation experiment was performed on the public data set. The experimental results show that the refined BPR algorithm has a better improvement than the baseline algorithm.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3445945.3445959',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Multi-View Fusion with Extreme Learning Machine for Clustering',\n",
       "  'authors': \"['Yongshan Zhang', 'Jia Wu', 'Chuan Zhou', 'Zhihua Cai', 'Jian Yang', 'Philip S. Yu']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Intelligent Systems and Technology',\n",
       "  'abstract': 'Unlabeled, multi-view data presents a considerable challenge in many real-world data analysis tasks. These data are worth exploring because they often contain complementary information that improves the quality of the analysis results. Clustering with multi-view data is a particularly challenging problem as revealing the complex data structures between many feature spaces demands discriminative features that are specific to the task and, when too few of these features are present, performance suffers. Extreme learning machines (ELMs) are an emerging form of learning model that have shown an outstanding representation ability and superior performance in a range of different learning tasks. Motivated by the promise of this advancement, we have developed a novel multi-view fusion clustering framework based on an ELM, called MVEC. MVEC learns the embeddings from each view of the data via the ELM network, then constructs a single unified embedding according to the correlations and dependencies between each embedding and automatically weighting the contribution of each. This process exposes the underlying clustering structures embedded within multi-view data with a high degree of accuracy. A simple yet efficient solution is also provided to solve the optimization problem within MVEC. Experiments and comparisons on eight different benchmarks from different domains confirm MVEC’s clustering accuracy.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3340268',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'NTP-Miner: Nonoverlapping Three-Way Sequential Pattern Mining',\n",
       "  'authors': \"['Youxi Wu', 'Lanfang Luo', 'Yan Li', 'Lei Guo', 'Philippe Fournier-Viger', 'Xingquan Zhu', 'Xindong Wu']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Knowledge Discovery from Data',\n",
       "  'abstract': 'Nonoverlapping sequential pattern mining is an important type of sequential pattern mining (SPM) with gap constraints, which not only can reveal interesting patterns to users but also can effectively reduce the search space using the Apriori (anti-monotonicity) property. However, the existing algorithms do not focus on attributes of interest to users, meaning that existing methods may discover many frequent patterns that are redundant. To solve this problem, this article proposes a task called nonoverlapping three-way sequential pattern (NTP) mining, where attributes are categorized according to three levels of interest: strong, medium, and weak interest. NTP mining can effectively avoid mining redundant patterns since the NTPs are composed of strong and medium interest items. Moreover, NTPs can avoid serious deviations (the occurrence is significantly different from its pattern) since gap constraints cannot match with strong interest patterns. To mine NTPs, an effective algorithm is put forward, called NTP-Miner, which applies two main steps: support (frequency occurrence) calculation and candidate pattern generation. To calculate the support of an NTP, depth-first and backtracking strategies are adopted, which do not require creating a whole Nettree structure, meaning that many redundant nodes and parent–child relationships do not need to be created. Hence, time and space efficiency is improved. To generate candidate patterns while reducing their number, NTP-Miner employs a pattern join strategy and only mines patterns of strong and medium interest. Experimental results on stock market and protein datasets show that NTP-Miner not only is more efficient than other competitive approaches but can also help users find more valuable patterns. More importantly, NTP mining has achieved better performance than other competitive methods in clustering tasks. Algorithms and data are available at:  https://github.com/wuc567/Pattern-Mining/tree/master/NTP-Miner.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3480245',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Holy Basil Curl Leaf Disease Classification using Edge Detection and Machine Learning',\n",
       "  'authors': \"['Pikulkaew Tangtisanon', 'Suttipong Kornrapat']\",\n",
       "  'date': 'February 2020',\n",
       "  'source': 'ICCAE 2020: Proceedings of the 2020 12th International Conference on Computer and Automation Engineering',\n",
       "  'abstract': 'Holy basil (Ocimum basilicum L.) is one of the most vital economic crops that has a significant impact on export earnings. However, the holy basil prices could be dropped due to a curl leaf disease caused by pests. Several previous studies focused on plant leaf disease detection based on the leaf color. Unfortunately, the leaf curl disease sometimes changes a shape of the leaf not the color so it cannot be detected with those schemes. We proposed a novel approach aims to automatically detect a curling leaf on holy basil. This paper presents a Neural Network (NN) model and Logistic Regression (LR) model to automatically detect a curling leaf on holy basil. To be able to detect the infected one not by colors but by its shape, we have applied edge detection algorithms which are Canny and Sobel model. To speed up processing time, images were resized and converted to grayscale before passing them to machine learning models. Moreover, NN and LR were modified with mini-batch technique in order to increase the speed of the processing time. The dataset contains 600 images of holy basil leaves with 300 images of healthy leaves and 300 images of infected leaves. The experimental results indicate that the proposed method effectively detects the curling leaves on holy basil.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3384613.3384634',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Classification of News Texts Based on Bayes Algorithm',\n",
       "  'authors': \"['Qian Wang', 'HongLi Xu', 'YanLing Li']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"EITCE '21: Proceedings of the 2021 5th International Conference on Electronic Information Technology and Computer Engineering\",\n",
       "  'abstract': 'At present, in order to obtain valuable information from the mixed information in a short time, the text classification in data mining emerges with the time going. we used the naive Bayes algorithm to solve the problem of text classification in data mining. Firstly, we need further preprocessing to reflect this feature of the text, which is TF-ID. Secondly, through the integrated environment anaconda platform of Python, the principle of naive Bayesian classification model is mastered and the whole process of news classification is displayed. Finally, the characteristic extraction with TF-IDF value, the accuracy of Bayesian classifier was significantly improved. The emphasis is on the working principle of naive Bayes model with multinomial as a priori and the improvement on the methodof text feature extraction.: the extraction method of TF-IDF is more reasonable.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3501409.3501636',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Web Intelligence Data Clustering by Bare Bone Fireworks Algorithm Combined with K-Means',\n",
       "  'authors': \"['Eva Tuba', 'Raka Jovanovic', 'Romana Capor Hrosik', 'Adis Alihodzic', 'Milan Tuba']\",\n",
       "  'date': 'June 2018',\n",
       "  'source': \"WIMS '18: Proceedings of the 8th International Conference on Web Intelligence, Mining and Semantics\",\n",
       "  'abstract': 'Data mining and clustering are important elements of various applications in different fields. One of the areas were clustering is rather frequently used is web intelligence, which nowadays represents an important research area. Data collected from the web are usually very complex, dynamic, without structure and rather large. Traditional clustering techniques are not efficient enough and need to be improved. In this paper, we propose combination of recent swarm intelligence algorithm, bare bones fireworks algorithm, and k-means for clustering web intelligence data. The proposed method was compared with other approaches from literature. Based on the experimental results, it can be concluded that the proposed method has very promising characteristics in terms of the quality of clustering, as well as the execution time.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3227609.3227650',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'NTP-Miner: Nonoverlapping Three-Way Sequential Pattern Mining',\n",
       "  'authors': \"['Youxi Wu', 'Lanfang Luo', 'Yan Li', 'Lei Guo', 'Philippe Fournier-Viger', 'Xingquan Zhu', 'Xindong Wu']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Knowledge Discovery from Data',\n",
       "  'abstract': 'Nonoverlapping sequential pattern mining is an important type of sequential pattern mining (SPM) with gap constraints, which not only can reveal interesting patterns to users but also can effectively reduce the search space using the Apriori (anti-monotonicity) property. However, the existing algorithms do not focus on attributes of interest to users, meaning that existing methods may discover many frequent patterns that are redundant. To solve this problem, this article proposes a task called nonoverlapping three-way sequential pattern (NTP) mining, where attributes are categorized according to three levels of interest: strong, medium, and weak interest. NTP mining can effectively avoid mining redundant patterns since the NTPs are composed of strong and medium interest items. Moreover, NTPs can avoid serious deviations (the occurrence is significantly different from its pattern) since gap constraints cannot match with strong interest patterns. To mine NTPs, an effective algorithm is put forward, called NTP-Miner, which applies two main steps: support (frequency occurrence) calculation and candidate pattern generation. To calculate the support of an NTP, depth-first and backtracking strategies are adopted, which do not require creating a whole Nettree structure, meaning that many redundant nodes and parent–child relationships do not need to be created. Hence, time and space efficiency is improved. To generate candidate patterns while reducing their number, NTP-Miner employs a pattern join strategy and only mines patterns of strong and medium interest. Experimental results on stock market and protein datasets show that NTP-Miner not only is more efficient than other competitive approaches but can also help users find more valuable patterns. More importantly, NTP mining has achieved better performance than other competitive methods in clustering tasks. Algorithms and data are available at:  https://github.com/wuc567/Pattern-Mining/tree/master/NTP-Miner.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3480245',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Ecg Classification using Machine Learning Techniques and Smote Oversampling Technique',\n",
       "  'authors': \"['Zhang Xing Zhong', 'Akotonou J. Michael', 'Zhao Jie Lun', 'Dong Hong Yue']\",\n",
       "  'date': 'August 2020',\n",
       "  'source': \"IPMV '20: Proceedings of the 2020 2nd International Conference on Image Processing and Machine Vision\",\n",
       "  'abstract': 'In this paper, automatic classification of Atrial Fibrillation (AF) based on single lead ECG signal was proposed using three different classification algorithm AdaBoost, K-Nearest Neighbors (KNN) and Support Vector Machine (SVM). SMOTE technique was applied as data oversampling techniques. Many features were extracted and Minimum Redundancy Maximum Relevance (MRMR) algorithm was used to select relevant features. 5834 records were selected from the Physionet Challenge 2017 dataset for this experiment. Classification using oversampling technique yields best results for all classifiers involved. AdaBoost on oversampling data yields the best accuracy of 98.8%.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3421558.3421560',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Probabilistic Topic Models for Text Data Retrieval and Analysis',\n",
       "  'authors': \"['ChengXiang Zhai']\",\n",
       "  'date': 'August 2017',\n",
       "  'source': \"SIGIR '17: Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval\",\n",
       "  'abstract': 'Text data include all kinds of natural language text such as web pages, news articles, scientific literature, emails, enterprise documents, and social media posts. As text data continues to grow quickly, it is increasingly important to develop intelligent systems to help people manage and make use of vast amounts of text data (\"big text data\"). As a new family of effective general approaches to text data retrieval and analysis, probabilistic topic models, notably Probabilistic Latent Semantic Analysis (PLSA), Latent Dirichlet Allocations (LDA), and many extensions of them, have been studied actively in the past decade with widespread applications. These topic models are powerful tools for extracting and analyzing latent topics contained in text data; they also provide a general and robust latent semantic representation of text data, thus improving many applications in information retrieval and text mining. Since they are general and robust, they can be applied to text data in any natural language and about any topics. This tutorial will systematically review the major research progress in probabilistic topic models and discuss their applications in text retrieval and text mining. The tutorial will provide (1) an in-depth explanation of the basic concepts, underlying principles, and the two basic topic models (i.e., PLSA and LDA) that have widespread applications, (2) a broad overview of all the major representative topic models (that are usually extensions of PLSA or LDA), and (3) a discussion of major challenges and future research directions. The tutorial should be appealing to anyone who would like to learn about topic models, how and why they work, their widespread applications, and the remaining research challenges to be solved, including especially graduate students, researchers who want to develop new topic models, and practitioners who want to apply topic models to solve many application problems. The attendants are expected to have basic knowledge of probability and statistics.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3077136.3082067',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Speeding up the data extraction of machine learning approaches: a distributed framework',\n",
       "  'authors': \"['Martin Steinhauer', 'Fabio Palomba']\",\n",
       "  'date': 'November 2020',\n",
       "  'source': 'MaLTeSQuE 2020: Proceedings of the 4th ACM SIGSOFT International Workshop on Machine-Learning Techniques for Software-Quality Evaluation',\n",
       "  'abstract': 'In the last decade, mining software repositories (MSR) has become one of the most important sources to feed machine learning models. Especially open-source projects on platforms like GitHub are providing a tremendous amount of data and make them easily accessible. Nevertheless, there is still a lack of standardized pipelines to extract data in an automated and fast way. Even though several frameworks and tools exist which can fulfill specific tasks or parts of the data extraction process, none of them allow neither building an automated mining pipeline nor the possibility for full parallelization. As a consequence, researchers interested in using mining software repositories to feed machine learning models are often forced to re-implement commonly used tasks leading to additional development time and libraries may not be integrated optimally.   This preliminary study aims to demonstrate current limitations of existing tools and Git itself which are threatening the prospects of standardization and parallelization. We also introduce the multi-dimensionality aspects of a Git repository and how they affect the computation time. Finally, as a proof of concept, we define an exemplary pipeline for predicting refactoring operations, assessing its performance. Finally, we discuss the limitations of the pipeline and further optimizations to be done.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3416505.3423562',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Recognizing Common Skin Diseases in the Philippines Using Image Processing and Machine Learning Classification',\n",
       "  'authors': \"['JOEL CASUAYAN DE GOMA', 'MADHAVI DEVARAJ']\",\n",
       "  'date': 'August 2020',\n",
       "  'source': \"ICCBD '20: 2020 the 3rd International Conference on Computing and Big Data\",\n",
       "  'abstract': 'Skin disease is prevalent in tropical climates, developing countries, people with poor hygiene, and polluted areas. It is a kind of disease that is visible to human eyes which makes carriers susceptible to shame and may cause people to keep distance from them. Even though it is visible to human eyes, people are unaware of what kind of skin disease they have, and with this, people would go to dermatological clinics to have their skin checked, and to have a diagnosis. In line with this, the proponents created a system that detects and classifies skin diseases, particularly acne vulgaris, atopic dermatitis, keratosis pilaris (Chicken Skin), psoriasis, leprosy, and warts. This research has used different pre-processing and segmentation algorithms to successfully extract features (texture, edge, and color). The extracted features were contained as a feature vector and were used to train the Support Vector Machine classifier and the Artificial Neural Network classifier. For the Support Vector Machine (SVM), the model peaked an average of 93.55% and 93.33% for precision and recall, respectively, and for the Artificial Neural Network (ANN) classifier, it peaked 96.55% for precision and 100% for recall.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3418688.3418700',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Identifying Social Network Delusion to Investigate Addiction Ratio using Data Mining',\n",
       "  'authors': \"['K. S. Thakre', 'Deepali Dawande', 'Vaidehi S. Thakre']\",\n",
       "  'date': 'January 2020',\n",
       "  'source': \"APIT '20: Proceedings of the 2020 2nd Asia Pacific Information Technology Conference\",\n",
       "  'abstract': 'Mining social media is the process of defining, analyzing, and extracting applicative patterns and trends from row social media data. Social media are very popular way of expressing opinions and interacting with many individual in the online world. However growing number of social network delusion among various age categories are recently noted. Mental sickness can have a deep influence on person, families, and society as well. Hence, we propose a framework that analyzes Social Network Delusion (SND) and investigates the addiction ratio. This work first defines the framework for analyzing the social network delusion based on mining online social behavior that provides an early stage opportunity to identify SNDs (Social Network Delusion). The proposed system mainly works in three phases. Feature extraction and analysis of the various posts posted by the users on Facebook, Instagram and Twitter is performed by using mining algorithm in the first step. The SND prediction using the extracted features is done in the second phase; Third phase uses the predicted results as an input for investigating the addiction ratio. We investigate the addiction ratio among different genders and age groups for analyzing the prevention strategies against growing number of SND.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3379310.3379321',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A Hybrid Machine Learning Approach for Improving Mortality Risk Prediction on Imbalanced Data',\n",
       "  'authors': \"['Araek Tashkandi', 'Lena Wiese']\",\n",
       "  'date': 'December 2019',\n",
       "  'source': 'iiWAS2019: Proceedings of the 21st International Conference on Information Integration and Web-based Applications &amp; Services',\n",
       "  'abstract': 'The efficiency of Machine Learning (ML) models has widely been acknowledged in the healthcare area. However, the quality of the underlying medical data is a major challenge when applying ML in medical decision making. In particular, the imbalanced class distribution problem causes the ML model to be biased towards the majority class. Furthermore, the accuracy will be biased, too, which produces the Accuracy Paradox. In this paper, we identify an optimal ML model for predicting mortality risk for Intensive Care Units (ICU) patients. We comprehensively assess an approach that leverages the efficiency of ML ensemble learning (in particular, Gradient Boosting Decision Tree) and clustering-based data sampling to handle the imbalanced data problem that this model faces. We comprehensively compare different competitors (in terms of ML models as well as clustering methods) on a big real-world ICU dataset achieving a maximum area under the curve value of 0.956.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3366030.3366040',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Breast Tissue Density Classification in Mammograms Based on Supervised Machine Learning Technique',\n",
       "  'authors': \"['Kanchan Lata Kashyap', 'Manish Kumar Bajpai', 'Pritee Khanna']\",\n",
       "  'date': 'November 2017',\n",
       "  'source': \"Compute '17: Proceedings of the 10th Annual ACM India Compute Conference\",\n",
       "  'abstract': 'Breast tissue density is one of the symptoms for breast cancer detection. Fully automatic breast tissue density classification is presented in this work. Present work consists of four steps which include breast region extraction and enhancement of mammograms, segmentation, feature extraction, and breast tissue density classification. Enhancement of mammogram is done by applying fractional order differential based filter. Segmentation of breast tissue segmentation has been done by using clustering based fast fuzzy c-means technique. Further, texture based local binary pattern (LBP) and dominant rotated local binary pattern (DRLBP) features have been computed from the extracted breast tissues to characterize its texture property. Support vector machine with linear kernel functions are used to classify the breast tissue density. Proposed algorithm is validated on the publicly available 322 mammograms of Mini-Mammographic Image Analysis Society (MIAS).',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3140107.3140131',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Classification of Heart Disease Using Machine Learning Techniques',\n",
       "  'authors': \"['Perivitta Rajendran', 'Su-Cheng Haw', 'Palaichamy Naveen']\",\n",
       "  'date': 'September 2021',\n",
       "  'source': \"ICDTE '21: Proceedings of the 5th International Conference on Digital Technology in Education\",\n",
       "  'abstract': 'The most crucial task in the medical field is diagnosing an illness. If a disease is determined at the early stage then many lives can be saved. The purpose of this paper is to use the medical data to predict cardiovascular heart disease using both supervised and unsupervised learning techniques and to show the effects of feature correlation on the classification model with over four different algorithms namely, Logistic Regression, Naive Bayes, Random Forest and Artificial Neural Networks. For the performance assessment, it incorporates F1-score, precision, Area under curve and recall. Overall, Logistic Regression algorithm tends to perform well for both Hungary and Statlog dataset whereas for Cleveland dataset, Artificial Neural Networks performs better than Logistic Regression in terms of accuracy. In terms of area under curve score, Logistic Regression performance is higher in all the dataset compared to Naive Bayes, Random Forest and Artificial Neural Networks. The results tabulated evidently prove that the designed diagnostic system is capable of predicting the risk level of heart disease effectively when compared to other approaches.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3488466.3488482',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Detection and Classification of Embung Land Cover using Support Vector Machine',\n",
       "  'authors': \"['Ahmad Syarif Hidayat', 'Fatwa Ramdani', 'Fitra Bachtiar']\",\n",
       "  'date': 'September 2021',\n",
       "  'source': \"SIET '21: Proceedings of the 6th International Conference on Sustainable Information Engineering and Technology\",\n",
       "  'abstract': \"The agricultural sector is the mainstay sector in the economy of Malang Regency. However, Malang Regency has experienced a decrease in rice harvested area caused by drought. One of the Government's efforts to overcome this is by carrying out embung for agriculture. The use of remote sensing technology is one of the practical tools to monitor the phenomenon of change that occurs continuously and in a large area, in this case, the reservoir. This study aims to determine and analyze the use of SVM classification in satellite imagery to detect embung in Malang Regency. This research uses PlanetScope satellite imagery and Support Vector Machine (SVM) to classify land cover types. This research consists of three main tasks: satellite image preprocessing, satellite image classification, and land cover detection. The results showed that the increase in the number of sample areas in the SVM algorithm impacted the computational time and accuracy of the embung classification. The number of sample areas was small, the computation time was 16 seconds, and the accuracy was 0.5641. While the number of sample areas is large, the computation time is 307 seconds, and the accuracy is 0.7093.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3479645.3479673',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Stacked Sparse Autoencoder and Machine Learning Based Anxiety Classification Using EEG Signals',\n",
       "  'authors': \"['Shikha', 'Manan Agrawal', 'Mohd Ayaan Anwar', 'Divyashikha Sethia']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"AIMLSystems '21: Proceedings of the First International Conference on AI-ML Systems\",\n",
       "  'abstract': 'Anxiety is an emotion characterized by trepidation, stress, or uneasiness that involves extreme worry or fear over future unwanted events or an actual situation. Careful analysis for anxiety is critical since approximately 2 to 4% of the general population have experienced adequate symptoms indicating an anxiety disorder. This paper aims to classify anxiety levels based on machine learning and deep learning algorithms with improved performance. This work uses the publically available DASPS Database (Database for Anxious States based on a Psychological Stimulation). The dataset consists of EEG recordings from 23 participants during anxiety elicitation through face-to-face psychological stimuli. This work uses RFECV with the classifiers to reduce redundancy between features and improve results. We achieve the highest classification accuracy of 83.93% and 70.25% using Stacked Sparse Autoencoder and Decision Tree for two-class anxiety classification.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3486001.3486227',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'RMDL: Random Multimodel Deep Learning for Classification',\n",
       "  'authors': \"['Kamran Kowsari', 'Mojtaba Heidarysafa', 'Donald E. Brown', 'Kiana Jafari Meimandi', 'Laura E. Barnes']\",\n",
       "  'date': 'April 2018',\n",
       "  'source': \"ICISDM '18: Proceedings of the 2nd International Conference on Information System and Data Mining\",\n",
       "  'abstract': 'The continually increasing number of complex datasets each year necessitates ever improving machine learning methods for robust and accurate categorization of these data. This paper introduces Random Multimodel Deep Learning (RMDL): a new ensemble, deep learning approach for classification. Deep learning models have achieved state-of-the-art results across many domains. RMDL solves the problem of finding the best deep learning structure and architecture while simultaneously improving robustness and accuracy through ensembles of deep learning architectures. RDML can accept as input a variety data to include text, video, images, and symbolic. This paper describes RMDL and shows test results for image and text data including MNIST, CIFAR-10, WOS, Reuters, IMDB, and 20newsgroup. These test results show that RDML produces consistently better performance than standard methods over a broad range of data types and classification problems.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3206098.3206111',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Subconcept Based One Class Classification Method with Cluster Updating',\n",
       "  'authors': \"['Zhen Liu', 'Nathalie Japkowicz', 'Ruoyu Wang']\",\n",
       "  'date': 'February 2020',\n",
       "  'source': \"ICMLC '20: Proceedings of the 2020 12th International Conference on Machine Learning and Computing\",\n",
       "  'abstract': 'One class classification is an effective way to perform anomaly detection or outlier detection. Previous work has empirically shown that complexity in the background (also known as target) class degrades the performance of one-class classifiers, and one source of data complexity is the presence of subconcepts in that class. Learning over subconcepts individually can mitigate the effects of domain complexity and improve one class classification performance. Unless a clustering could be derived from domain knowledge, the approach used to search for subconcepts is unsupervised clustering. However, in some cases, the examples belonging to some of the clusters may be too scattered, while, in others, clusters may be affected by the small disjunct problem where some clusters comprise only of a few examples that cannot be used to train a robust classifier. To handle these problems, this paper presents a method to update the clusters obtained by the clustering process prior to learning over them. In addition, it introduces the c-Nearest-Cluster (c-NC) method for combining the individual classifiers derived from each subconcept. Our experiments on classical outlier detection datasets and on cyber security datasets show that our method can improve upon the classification performance obtained by the recently proposed subconcept based one class classification method.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3383972.3384016',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Data Management in Machine Learning: Challenges, Techniques, and Systems',\n",
       "  'authors': \"['Arun Kumar', 'Matthias Boehm', 'Jun Yang']\",\n",
       "  'date': 'May 2017',\n",
       "  'source': \"SIGMOD '17: Proceedings of the 2017 ACM International Conference on Management of Data\",\n",
       "  'abstract': 'Large-scale data analytics using statistical machine learning (ML), popularly called advanced analytics, underpins many modern data-driven applications. The data management community has been working for over a decade on tackling data management-related challenges that arise in ML workloads, and has built several systems for advanced analytics. This tutorial provides a comprehensive review of such systems and analyzes key data management challenges and techniques. We focus on three complementary lines of work: (1) integrating ML algorithms and languages with existing data systems such as RDBMSs, (2) adapting data management-inspired techniques such as query optimization, partitioning, and compression to new systems that target ML workloads, and (3) combining data management and ML ideas to build systems that improve ML lifecycle-related tasks. Finally, we identify key open data management challenges for future research in this important area.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3035918.3054775',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'MFPMiner: Mining Meaningful Frequent Patterns from Spatio-textual Trajectories',\n",
       "  'authors': \"['Fabio Valdes']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Spatial Algorithms and Systems',\n",
       "  'abstract': 'In the second decade of this century, technical progress has led to a worldwide proliferation of devices for tracking the movement behavior of a person, a vehicle, or another kind of entity. One of the consequences of this development is a massive and still growing amount of movement and movement-related data recorded by cellphones, automobiles, vessels, aircraft, and further GPS-enabled entities. As a result, the requirements for managing and analyzing movement records also increase, serving commercial, administrative, or private purposes. Since the development of hardware components cannot keep pace with the data growth, exploring methods of analyzing such trajectory datasets has become a very active and influential research field.For many application scenarios, besides the spatial trajectory of an entity, it is desirable to take additional semantic information into consideration. These descriptions also change with time and may represent, e.g., the course of streets passed by a bus, the sequence of region names traversed by an aircraft, or the points of interest in proximity of the positions of a taxi. Such data may be directly recorded by a sensor (such as the altitude of an aircraft) or computed from the spatial trajectory combined with some underlying information (for example, street names). It is often helpful or even necessary to focus on such semantic information for efficient analyses, as changes usually occur less frequently than it is the case for the spatial trajectory, where data points usually arrive in very close temporal distances. However, any kind of querying requires a deep semantic knowledge of the dataset at hand, particularly for retrieving the set of trajectories that match a certain mobility pattern, that is, a sequence of temporal, spatial, and semantic specifications.In this article, we introduce a framework named MFPMiner1 for retrieving all mobility patterns fulfilling a user-specified frequency threshold from a spatio-textual trajectory dataset. The resulting patterns and their relative frequency can be regarded as a knowledge base of the considered data. They may be directly visualized or applied for a pattern matching query yielding the set of matching trajectories. We demonstrate the functionality of our approach in an application scenario and provide an experimental evaluation of its performance on real and synthetic datasets by comparing it to three competitive methods. The framework has been fully implemented in a DBMS environment and is freely available open source software.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3498728',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Text mining to Discover Design Features for Cybersecurity tools: The case of Password Management Systems',\n",
       "  'authors': \"['Yazan Alshboul', 'Wael Odat']\",\n",
       "  'date': 'December 2021',\n",
       "  'source': \"ICSEB '21: Proceedings of the 2021 5th International Conference on Software and e-Business\",\n",
       "  'abstract': 'Username and password are the most common authentication approach used today in cybersecurity. Most people have more than one online account protected by passwords. Managing passwords is one of the daunting tasks for most people. Therefore, users may rely on password management systems PMS to help creating, managing, retrieving, and storing passwords. However, there is still a lack of relying on and using such systems to manage users’ passwords. This study aims to use a text mining approach to investigate the factors that influence users’ behavior to use PMSs which leads to improve their security. This study adopts a text mining approach to uncover the design principles (features) of PMSs extracted from online users’ reviews and feedbacks to improve the adoption of PMSs. Specifically, our study used a topic modeling algorithm to analyze users’ feedback by collecting and analyzing the reviews of nine password management systems. The results indicate the need to address principles and design features related to compatibility, security, usefulness, ease of use, and customer care services.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3507485.3507507',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Effects of Image Augmentation and Dual-layer Transfer Machine Learning Architecture on Tumor Classification',\n",
       "  'authors': \"['Cheng Chen', 'Christine Chen', 'Xuesong Mei', 'Chaoyang Chen', 'Guoxin Ni', 'Stephen Lemos']\",\n",
       "  'date': 'October 2019',\n",
       "  'source': \"ICCPR '19: Proceedings of the 2019 8th International Conference on Computing and Pattern Recognition\",\n",
       "  'abstract': 'Breast tumor (BT) is the second most common health problem for women. Traditional diagnosis methods can be very labor-intensive and time-consuming with the risk of making a wrong diagnosis. Computer vision and imaging processing techniques using machine learning (ML) methods are emerging to aide in clinical diagnosis. Some machine learning methods have yielded an accuracy of 85% using a single-layer classifier. In this study Inception-V3, a two-layer classifier of transfer machine learning tool was used for image processing with enhancement technologies and for the classification of breast tumor histopathological types. Results showed that image augmentation with dual-layer transfer machine learning algorithms yielded an accuracy of 95.6% in identification of breast tumor pathologic types, which was higher than previously reported methods in the literature. Different image preprocessing methods, dataset preparing methods, and classifier architectures were also studied to identify the optimal algorithm. Results showed that multiple-layer processing algorithms using color images, instead of black and white images, yielded a better accuracy in histopathological type classification.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3373509.3373584',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Mining Latent Semantic Correlation inspired by Quantum Entanglement',\n",
       "  'authors': \"['Zan Li', 'Yuexian Hou', 'Tingsan Pan', 'Tian Tian', 'Yingjie Gao']\",\n",
       "  'date': 'December 2021',\n",
       "  'source': \"CSAI '21: Proceedings of the 2021 5th International Conference on Computer Science and Artificial Intelligence\",\n",
       "  'abstract': 'Text representation learning is the cornerstone of solving downstream problems in Natural Language Processing (NLP). However, mining the potential explanatory factors or semantic associations behind data, rather than simply representing the superficial co-occurrence of words, remains a non-trivial challenge. To this end, we seek inspiration from the Quantum Entanglement (QE) which can effectively provide a complete description for the nature of realities and a globally-determined intrinsic correlation of considered objects, thus proposing a novel representation learning hypothesis called the Latent Semantic Correlation (LSC), namely the implicit internal coherence between the semantic space and its corresponding category space. To construct a multi-granularity representation from sememes to words, phrases, sentences, and higher-level LSC, we implement a QE-inspired Network (QEN) under the constraints of quantum formalism and propose the Local Semantic Measurement (LSM) and Extraction (LSE) for effectively capturing probability distribution information from the entangled state of a bipartite quantum system, which has a clear geometrical motivation but also supports a well-founded probabilistic interpretation. Experimental results conducted on several benchmarking classification tasks prove the validity of the LSC hypothesis and the superiority of QEN.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3507548.3507598',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A User-Centered Concept Mining System for Query and Document Understanding at Tencent',\n",
       "  'authors': \"['Bang Liu', 'Weidong Guo', 'Di Niu', 'Chaoyue Wang', 'Shunnan Xu', 'Jinghong Lin', 'Kunfeng Lai', 'Yu Xu']\",\n",
       "  'date': 'July 2019',\n",
       "  'source': \"KDD '19: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining\",\n",
       "  'abstract': 'Concepts embody the knowledge of the world and facilitate the cognitive processes of human beings. Mining concepts from web documents and constructing the corresponding taxonomy are core research problems in text understanding and support many downstream tasks such as query analysis, knowledge base construction, recommendation, and search. However, we argue that most prior studies extract formal and overly general concepts from Wikipedia or static web pages, which are not representing the user perspective. In this paper, we describe our experience of implementing and deploying ConcepT in Tencent QQ Browser. It discovers user-centered concepts at the right granularity conforming to user interests, by mining a large amount of user queries and interactive search click logs. The extracted concepts have the proper granularity, are consistent with user language styles and are dynamically updated. We further present our techniques to tag documents with user-centered concepts and to construct a topic-concept-instance taxonomy, which has helped to improve search as well as news feeds recommendation in Tencent QQ Browser. We performed extensive offline evaluation to demonstrate that our approach could extract concepts of higher quality compared to several other existing methods. Our system has been deployed in Tencent QQ Browser. Results from online A/B testing involving a large number of real users suggest that the Impression Efficiency of feeds users increased by 6.01% after incorporating the user-centered concepts into the recommendation framework of Tencent QQ Browser.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3292500.3330727',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Towards investigation of iterative strategy for data mining of short-term traffic flow with Recurrent Neural Networks',\n",
       "  'authors': \"['Armando Fandango', 'R. Paul Wiegand']\",\n",
       "  'date': 'April 2018',\n",
       "  'source': \"ICISDM '18: Proceedings of the 2nd International Conference on Information System and Data Mining\",\n",
       "  'abstract': 'The smart cities of modern nations rely on the smooth flow of transportation that depends on the predictions of the traffic flow patterns. Since last few years, deep learning based methods have emerged to show better results for short-term traffic ow prediction. For multi-step-ahead prediction, researchers applying statistical methods have used the iterative strategies for preparing input data and building forecast models. In studies applying recurrent neural networks (RNN), the iterative strategies are not used. Hence, we investigate the usage of an iterative strategy for building the RNN models for short-term traffic flow forecasting.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3206098.3206112',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Effects of Part-of-Speech on Thai Sentence Classification to Wh-Question Categories using Machine Learning Approach',\n",
       "  'authors': \"['Saranlita Chotirat', 'Phayung Meesad']\",\n",
       "  'date': 'July 2020',\n",
       "  'source': \"IAIT '20: Proceedings of the 11th International Conference on Advances in Information Technology\",\n",
       "  'abstract': 'In the last decade, question classification is a strong signal for answer selection and help to find the structure of question sentences from sentences. For this paper, we evaluated the proposed pre-processing method for classifying the simple sentence to wh-question categories (\"What\", \"When\", \"Who\", \"Where\", and \"How\") on Thai texts by considering Part-of-Speech tagging (POS). The performances are evaluated using classification accuracy obtained from traditional classification models including Naïve Bayes, Logistic Regression, Support Vector Machine, K-Nearest Neighbors, and neural networks which employs Long Short-Term Memory (LSTM) and Convolutional Neural Network (CNN). We compared traditional models and neural networks; the experimental results showed that the result of the neural networks models better than the traditional model. The accuracy of the proposed model using the LSTM model with pre-trained word embedding is improved with an average F1 of 79.60%.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3406601.3406648',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A parallel island model for biogeography-based classification rule mining in julia',\n",
       "  'authors': \"['Samuel Ebert', 'Effat Farhana', 'Steffen Heber']\",\n",
       "  'date': 'July 2018',\n",
       "  'source': \"GECCO '18: Proceedings of the Genetic and Evolutionary Computation Conference Companion\",\n",
       "  'abstract': 'In this paper, we present a distributed island model implementation of biogeography-based optimization for classification rule mining (island BBO-RM). Island BBO-RM is an evolutionary algorithm for rule mining that uses Pittsburgh style classification rule encoding, which represents an entire ruleset (classifier) as a single chromosome. Our algorithm relies on biogeography-based optimization (BBO), an optimization technique that is inspired by species migration pattern between habitats. Biogeography-based optimization has been reported to perform well in various applications ranging from function optimization to image classification. A major limitation of evolutionary rule mining algorithms is their high computational cost and running time. To address this challenge, we have applied a distributed island model to parallelize the rule extraction phase via BBO. We have explored several different migration topologies and data windowing techniques. Our algorithm is implemented in Julia, a dynamic programming language designed for high-performance and parallel computation. Our results show that our distributed implementation is able to achieve considerable speedups when compared to a serial implementation. Without data windowing, we obtain speedups up to a factor of nine without a loss of classification accuracy. With data windowing, we obtain speedups up to a factor of 30 with a small loss of accuracy in some cases.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3205651.3208262',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'ClassiNet -- Predicting Missing Features for Short-Text Classification',\n",
       "  'authors': \"['Danushka Bollegala', 'Vincent Atanasov', 'Takanori Maehara', 'Ken-Ichi Kawarabayashi']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Knowledge Discovery from Data',\n",
       "  'abstract': 'Short and sparse texts such as tweets, search engine snippets, product reviews, and chat messages are abundant on the Web. Classifying such short-texts into a pre-defined set of categories is a common problem that arises in various contexts, such as sentiment classification, spam detection, and information recommendation. The fundamental problem in short-text classification is feature sparseness -- the lack of feature overlap between a trained model and a test instance to be classified. We propose ClassiNet -- a network of classifiers trained for predicting missing features in a given instance, to overcome the feature sparseness problem. Using a set of unlabeled training instances, we first learn binary classifiers as feature predictors for predicting whether a particular feature occurs in a given instance. Next, each feature predictor is represented as a vertex vi in the ClassiNet, where a one-to-one correspondence exists between feature predictors and vertices. The weight of the directed edge eij connecting a vertex vi to a vertex vj represents the conditional probability that given vi exists in an instance, vj also exists in the same instance.We show that ClassiNets generalize word co-occurrence graphs by considering implicit co-occurrences between features. We extract numerous features from the trained ClassiNet to overcome feature sparseness. In particular, for a given instance x, we find similar features from ClassiNet that did not appear in x, and append those features in the representation of x. Moreover, we propose a method based on graph propagation to find features that are indirectly related to a given short-text. We evaluate ClassiNets on several benchmark datasets for short-text classification. Our experimental results show that by using ClassiNet, we can statistically significantly improve the accuracy in short-text classification tasks, without having to use any external resources such as thesauri for finding related features.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3201578',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A Machine Learning Approach to Violin Bow Technique Classification: a Comparison Between IMU and MOCAP systems',\n",
       "  'authors': \"['David Dalmazzo', 'Simone Tassani', 'Rafael Ramírez']\",\n",
       "  'date': 'September 2018',\n",
       "  'source': \"iWOAR '18: Proceedings of the 5th International Workshop on Sensor-based Activity Recognition and Interaction\",\n",
       "  'abstract': 'Motion Capture (MOCAP) Systems have been used to analyze body motion and postures in biomedicine, sports, rehabilitation, and music. With the aim to compare the precision of low-cost devices for motion tracking (e.g. Myo) with the precision of MOCAP systems in the context of music performance, we recorded MOCAP and Myo data of a top professional violinist executing four fundamental bowing techniques (i.e. Détaché, Martelé, Spiccato and Ricochet). Using the recorded data we applied machine learning techniques to train models to classify the four bowing techniques. Despite intrinsic differences between the MOCAP and low-cost data, the Myo-based classifier resulted in slightly higher accuracy than the MOCAP-based classifier. This result shows that it is possible to develop music-gesture learning applications based on low-cost technology which can be used in home environments for self-learning practitioners.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3266157.3266216',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Mining personal media thresholds for opinion dynamics and social influence',\n",
       "  'authors': \"['Casey Doyle', 'Alex Meandzija', 'Gyorgy Korniss', 'Boleslaw Szymanski', 'Derrik Asher', 'Elizabeth Bowman']\",\n",
       "  'date': 'August 2018',\n",
       "  'source': \"ASONAM '18: Proceedings of the 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining\",\n",
       "  'abstract': 'To study the detailed effects of social media consumption on personal opinion dynamics, we gather self reported survey data on the volume of different media types an individual must consume before forming or changing their opinion on a subject. We then use frequent pattern mining to analyze the data for common groupings of responses with respect to various media types, sources, and contexts. We show that in general individuals tend to perceive their behavior to be consistent across many variations in these parameters, while further detail shows various common parameter groupings that indicate response changes as well as small groups of individuals that tend to be consistently more easily swayed than the average participant.',\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3382225.3382478',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Joint Mobility Pattern Mining with Urban Region Partitions',\n",
       "  'authors': \"['Jing Lian', 'Yang Li', 'Weixi Gu', 'Shao-Lun Huang', 'Lin Zhang']\",\n",
       "  'date': 'November 2018',\n",
       "  'source': \"MobiQuitous '18: Proceedings of the 15th EAI International Conference on Mobile and Ubiquitous Systems: Computing, Networking and Services\",\n",
       "  'abstract': \"Mobility pattern mining answers the fundamental question of where people are likely to go from a given location. It plays an important role in city planning, public transport management and location-based mobile applications. Among these applications, many concern the mobility pattern over contiguous spatial regions as a whole. Traditional ways of mobility pattern mining either result in trip clusters with overlapped origin and destination regions, or require an extra step to partition the city into discrete regions, which may not be optimal for mobility pattern extraction. In this paper, we present a region-aware mobility pattern mining framework to jointly extract trip clusters while maintaining non-overlapping partitions of trip origins and destinations. We developed kernelized ACE, a novel extension to a classic algorithm in statistics to compute the optimal mobility clusters under spatial constraints. Experimental results using Beijing taxi trip data show that our approach outperforms other methods with only ~ 0.3% spatial overlap and 86.43% origin-destination correlation. Our case studies on New York City's and Beijing's taxi datasets also yield insightful findings that reveal city-scale mobility patterns and propose potential improvement for public transportation.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3286978.3287004',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'An ensemble outlier detection method for multiclass classification problem in data mining',\n",
       "  'authors': \"['Dalton Ndirangu', 'Waweru Mwangi', 'Lawrence Nderu']\",\n",
       "  'date': 'July 2018',\n",
       "  'source': \"DSIT '18: Proceedings of the 2018 International Conference on Data Science and Information Technology\",\n",
       "  'abstract': 'We proposed to develop a heterogeneous ensemble method that boost the performance of random forest classifier. The proposed method utilized the boosting capability of adaboost algorithm and the feature selection and bagging capability of random subspace algorithm. Both algorithms used random forest as the base classifier and were combined using voting methodology. We preprocessed the dataset by removing both redundant features and the detected outliers associated with dataset features. We addressed the multiclass problem of the dataset by decomposing the dataset into binary classes using the technique of 1 against 1 enhanced by pairwise coupling. Since supervised algorithms are designed to be biased with majority class, we overcome that challenge by generating synthetic instances using synthetic minority over sampling technique. The proposed SMOTE_Voted outlier ensemble method outperformed Random Forest, KNN, NaiveBayes, C4.5 and Support Vector machine outlier detection methods. We conclude that ensemble technique improves performance of outlier detection for multiclass problem.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3239283.3239303',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Automated Classification of Class Role-Stereotypes via Machine Learning',\n",
       "  'authors': \"['Arif Nurwidyantoro', 'Truong Ho-Quang', 'Michel R. V. Chaudron']\",\n",
       "  'date': 'April 2019',\n",
       "  'source': \"EASE '19: Proceedings of the 23rd International Conference on Evaluation and Assessment in Software Engineering\",\n",
       "  'abstract': 'Role stereotypes indicate generic roles that classes play in the design of software systems (e.g. controller, information holder, or interfacer). Knowledge about the role-stereotypes can help in various tasks in software development and maintenance, such as program understanding, program summarization, and quality assurance. This paper presents an automated machine learning-based approach for classifying the role-stereotype of classes in Java. We analyse the performance of this approach against a manually labelled ground truth for a sizable open source project (of 770+ Java classes) for the Android platform. Moreover, we compare our approach to an existing rule-based classification approach. The contributions of this paper include an analysis of which machine learning algorithms and which features provide the best classification performance. This analysis shows that the Random Forest algorithm yields the best classification performance. We find however, that the performance of the ML-classifier varies a lot for classifying different role-stereotypes. In particular its performs degrades for rare role-types. Our ML-classifier improves over the existing rule-based classification method in that the ML-approach classifies all classes, while rule-based approaches leave a significant number of classes unclassified.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3319008.3319016',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Machine learning classification methods in hyperspectral data processing for agricultural applications',\n",
       "  'authors': \"['Jonáš Hruška', 'Telmo Adão', 'Luís Pádua', 'Pedro Marques', 'António Cunha', 'Emanuel Peres', 'António Sousa', 'Raul Morais', 'Joaquim J. Sousa']\",\n",
       "  'date': 'April 2018',\n",
       "  'source': \"ICGDA '18: Proceedings of the International Conference on Geoinformatics and Data Analysis\",\n",
       "  'abstract': 'In agricultural applications hyperspectral imaging is used in cases where differences in spectral reflectance of the examined objects are small. However, the large amount of data generated by hyperspectral sensors requires advance processing methods. Machine learning approaches may play an important role in this task. They are known for decades, but they need high volume of data to compute accurate results. Until recently, the availability of hyperspectral data was a big drawback. It was first used in satellites, later in manned aircrafts and data availability from those platforms was limited because of logistics complexity and high price. Nowadays, hyperspectral sensors are available for unmanned aerial vehicles, which enabled to reach a high volume of data, thus overcoming these issues. This way, the aim of this paper is to present the status of the usage of machine learning approaches in the hyperspectral data processing, with a focus on agriculture applications. Nevertheless, there are not many studies available applying machine learning approach to hyperspectral data for agricultural applications. This apparent limitation was in fact the inspiration for making this survey. Preliminary results using UAV-based data are presented, showing the suitability of machine learning techniques in remote sensed data.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3220228.3220242',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Learning One-class Support Vector Machine by Using Artificial Bee Colony Algorithm and Its Application for Disease Classification',\n",
       "  'authors': \"['Ming-Huwi Horng', 'Yu-Lun Hong', 'Yung-Nien Sun', 'Zhe-Yuan Zhan', 'Chen-Yu Hong']\",\n",
       "  'date': 'July 2019',\n",
       "  'source': \"IECC '19: Proceedings of the 1st International Electronics Communication Conference\",\n",
       "  'abstract': 'The one-classification support vector (OCSVM) is a variant of SVM which only uses the positive class sample set in training stage. It has been widely used in the applications of disease diagnose, handwritten signature verification, remote sensing and document classification. However, there are many parameters needed to regulate. The mistake of parameter setting makes OCSVM it to be not effectiveness. Therefore, in this paper we proposed a learning algorithm based on the artificial bee colony algorithm to select the parameters. The construction algorithm of OSCVM is called the artificial bee colony based OSCVM (ABC-OCSVM) algorithm. Experimental results of two medical datasets of UCI data repository showed that our proposed ABC-OCSVM method outperforms the conventional LIBSVM package.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3343147.3343152',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Automating developer chat mining',\n",
       "  'authors': \"['Shengyi Pan', 'Lingfeng Bao', 'Xiaoxue Ren', 'Xin Xia', 'David Lo', 'Shanping Li']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"ASE '21: Proceedings of the 36th IEEE/ACM International Conference on Automated Software Engineering\",\n",
       "  'abstract': 'Online chatrooms are gaining popularity as a communication channel between widely distributed developers of Open Source Software (OSS) projects. Most discussion threads in chatrooms follow a Q&A format, with some developers (askers) raising an initial question and others (respondents) joining in to provide answers. These discussion threads are embedded with rich information that can satisfy the diverse needs of various OSS stakeholders. However, retrieving information from threads is challenging as it requires a thread-level analysis to understand the context. Moreover, the chat data is transient and unstructured, consisting of entangled informal conversations. In this paper, we address this challenge by identifying the information types available in developer chats and further introducing an automated mining technique. Through manual examination of chat data from three chatrooms on Gitter, using card sorting, we build a thread-level taxonomy with nine information categories and create a labeled dataset with 2,959 threads. We propose a classification approach (named F2Chat) to structure the vast amount of threads based on the information type automatically, helping stakeholders quickly acquire their desired information. F2Chat effectively combines handcrafted non-textual features with deep textual features extracted by neural models. Specifically, it has two stages with the first one leveraging the siamese architecture to pretrain the textual feature encoder, and the second one facilitating an in-depth fusion of two types of features. Evaluation results suggest that our approach achieves an average F1-score of 0.628, which improves the baseline by 57%. Experiments also verify the effectiveness of our identified non-textual features under both intra-project and cross-project validations.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ASE51524.2021.9678923',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Scalable and efficient data analytics and mining with lemonade',\n",
       "  'authors': \"['Walter dos Santos', 'Gustavo P. Avelar', 'Manoel Horta Ribeiro', 'Dorgival Guedes', 'Wagner Meira']\",\n",
       "  'date': 'None',\n",
       "  'source': 'Proceedings of the VLDB Endowment',\n",
       "  'abstract': 'Professionals outside of the area of Computer Science have an increasing need to analyze large bodies of data. This analysis often demands high level of security and has to be done in the cloud. However, current data analysis tools that demand little proficiency in systems programming struggle to deliver solutions which are scalable and safe. In this context we present Lemonade, a platform which focuses on creating data analysis and mining flows in the cloud, with authentication, authorization and accounting (AAA) guarantees. Lemonade provides an interface for the visual construction of flows, and encapsulates storage and data processing environment details, providing higher-level abstractions for data source access and algorithms. We illustrate its usage through a demo, where a data processing flow builds a classification model for detecting fake-news, also extracting some insights along the way.',\n",
       "  'link': 'https://dl.acm.org/doi/10.14778/3229863.3236262',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Mining ordinal data under human response uncertainty',\n",
       "  'authors': \"['Sergej Sizov']\",\n",
       "  'date': 'August 2017',\n",
       "  'source': \"WI '17: Proceedings of the International Conference on Web Intelligence\",\n",
       "  'abstract': 'Analysis and interpretation of collective feedback on ordinal scales is an important issue for several disciplines, including social sciences, recommender systems research, marketing, political science, and many others. A \"reasonable\" model is expected to provide an \"explanation\" of collective user behaviour. Many existing data mining approaches employ for this purpose probabilistic models, based on distributions and mixtures from a certain parametric family. In real life, users meet their decisions with considerable uncertainty. Its assessment and use in probabilistic models for better interpretation of collective feedback is the key concern of this paper. In doing so, we introduce approaches for gathering individual uncertainty, and discuss their viability and limitations. Consequently, we enrich state of the art response mining models (especially focused on discovery of latent user groups) with uncertainty knowledge, and demonstrate resulting advantages in systematic experiments with real users.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3106426.3106448',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'The Research of Semantic Kernel in SVM for Chinese Text Classification',\n",
       "  'authors': \"['Mai Fanjin', 'Huang Ling', 'Tan Jing', 'Wang Xinzheng']\",\n",
       "  'date': 'July 2017',\n",
       "  'source': \"ICIIP '17: Proceedings of the 2nd International Conference on Intelligent Information Processing\",\n",
       "  'abstract': 'The study of semantic kernel function is an important branch of kernel function research in recent years. However, there are few studies on the use of semantic kernel function in support vector machine for Chinese text classification. This paper constructs a domain-related weight matrix based on statistical features and a semantic kernel function based on the combination of \"HowNet\" and \"Synonyms\", and how to make full use of semantic relations in Chinese text to improve classification performance. The semantic kernel function is embedded in the support vector machine classifier for Chinese text classification experiment. The test results show that the accuracy, recall rate and F1 value of the support vector machine (SVM) of the semantic kernel function are higher than that of a single ontology or statistic-based semantic kernel function.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3144789.3144801',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Academic Diagnosis in College English Teaching Supported By Data Mining',\n",
       "  'authors': \"['Qiaoe Ni', 'Weiwei Zeng']\",\n",
       "  'date': 'September 2021',\n",
       "  'source': 'ICISCAE 2021: 2021 4th International Conference on Information Systems and Computer Aided Education',\n",
       "  'abstract': \"With the rapid development of big data era, there are more and more research and application of data-based mining and analysis technology in education. This paper discusses how to use data mining technology to integrate learning data into college English classroom, and to track and analyze students’ learning situation dynamically through data mining in student-centered college English teaching. According to the constructed academic diagnosis model, accurately diagnose learning problems, accurately locate all kinds of learning groups in the classroom, correctly locate learning needs, realize the push of personalized learning resources, and promote the improvement of learning effect. Through classified and hierarchical teaching intervention, we can guide learners to cultivate their interest in learning and improve teachers' grasp of the classroom and the efficiency of teaching. Create a positive and effective learning community.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3482632.3483075',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Performance Evaluation and Application of Computation Based Low-cost Homogeneous Machine Learning Model Algorithm for Image Classification',\n",
       "  'authors': \"['W. H. Huang']\",\n",
       "  'date': 'May 2020',\n",
       "  'source': 'ICBDM 2020: Proceedings of the 2020 International Conference on Big Data in Management',\n",
       "  'abstract': \"The image classification machine learning model was trained with the intention to predict the category of the input image. While multiple state-of-the-art ensemble model method- ologies are openly available, this paper evaluates the performance of a low-cost, simple algorithm that would integrate seamlessly into modern production-grade cloud-based applications. The homogeneous models, trained with the full instead of subsets of data, contains varying hyper-parameters and neural layers from one another. These models' inferences will be processed by the new algorithm, which is loosely based on conditional probability theories. The final output will be evaluated.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3437075.3437095',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Classification Performance Analysis in Medical Science: Using Kidney Disease Data',\n",
       "  'authors': \"['R. A. Jeewantha', 'Malka N. Halgamuge', 'Azeem Mohammad', 'Gullu Ekici']\",\n",
       "  'date': 'October 2017',\n",
       "  'source': \"ICBDR '17: Proceedings of the 1st International Conference on Big Data Research\",\n",
       "  'abstract': 'Health-care practices face data storage problems in the growing world. Huge data storage demands have caused undeniable data storage problems leaving health practitioners exclaimed. Without delay, accumulated data becomes too difficult to analyzed and handled by traditional approaches. A solution to this problem is urgently needed. One possible answer to this problem is Data mining that delivers the technology and procedure to convert these embankments of ordinary data into meaningful evidences for futuristic planning and decision-making. Data mining is a tool that not only solves the problem of piled up data; nonetheless it similarly turns it into meaningful data themes based on reoccurrences of trends in the data. The healthcare trade is mostly an \"information and document rich industry,\" and manual handling is not feasible in practical life. These huge volumes of data have been key to the arena of data-mining to generate associations among the attributes and extract expedient information. Recent research shows that combating Kidney diseases is a complex assignment that involves considerable knowledge and experience for annual testing and screening. In developed nations, Kidney diseases have become a silent killer, that makes key factors of disease burden in third world nations. Various data mining procedures are available for forecasting diseases such as clustering, classification, association rules, regression, and summarizations. The key objective of this study is to analyze datasets collected from 400 patients grounded on 25 different attributes attended for treatment for Chronic Kidney Disease (CKD), after using classification methods to forecast class precisely. Our analysis illustrates that Multilayer Perceptron is the most suitable classification method that outperforms the highest classification accuracy by 99.75% (0.0085 error) with only 5% of fluctuation among algorithm measures. Introspectively, the computational time, Multilayer Perception can be time-consuming comparatively, when it comes to deal with billions of data. Nonetheless, for the field of bioinformatics and medical science accuracy, the key objective is to deal with sensitive data because, a single error can lead to a disastrous confidentiality breech. Hence, our results show that Multilayer Perception classification method is the most accurate and suitable classification algorithm that could be used in the field of bioinformatics and medical science, for further data analysis and predictions. This paper will be useful for many medical institutions and work-related bioinformatics in pursuance to understand the prediction accuracies of data patterns in related work.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3152723.3152724',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Mining Moodle Data to Detect the Inactive and Low-performance Students during the Moodle Course',\n",
       "  'authors': \"['Mushtaq Hussain', 'Sadiq Hussain', 'Wu Zhang', 'Wenhao Zhu', 'Paraskevi Theodorou', 'Syed Muhammad Raza Abidi']\",\n",
       "  'date': 'October 2018',\n",
       "  'source': \"ICBDR '18: Proceedings of the 2nd International Conference on Big Data Research\",\n",
       "  'abstract': \"In web-based learning systems such as massive open online course (MOOC) and modular object-oriented developmental learning environment (Moodle), monitoring the student's activities as well as predict the low-performance students is an important task because it enables the instructors to award the students when their activities level drops from normal activities levels as well as having lower grades. We used several machine learning (ML) classification and clustering techniques to extract the pattern from student data during completing the Moodle course; which enables the instructor to detect the low-performance student in advance before the examination. The experimental result shows that the fuzzy unordered rule induction algorithm (FURIA) classification technique achieves high accuracy in detecting inactive students as well as predicts the different categories of the student during the Moodle course. The K-means clustering is also able to group the inactive and active users and poorly performed users. The result demonstrates that our proposed system will be easily integrated to Moodle system to send alert to inactive and low- performance students while completing the course and build efficient education environment for the students.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3291801.3291828',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A Novel Classification Model SA-MPCNN for Power Equipment Defect Text',\n",
       "  'authors': \"['Xiuxia Tian', 'Can Li', 'Bo Zhao']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Asian and Low-Resource Language Information Processing',\n",
       "  'abstract': 'The text classification of power equipment defect is of great significance to equipment health condition evaluation and power equipment maintenance decisions. Most of the existing classification methods do not sufficiently consider the semantic relation between words in the same sentence and cannot extract deep semantic features. To tackle those problems, this article proposes a novel classification method by combining the self-attention mechanism and multi-channel pyramid convolution neural networks. We utilize the bidirectional gated recurrent unit to model the text sequence and, on this basis, improve self-attention layer to dot multiplication on the forward and backward features to obtain the global attention score. Thereby, effective features are enhanced, invalid features are weakened, and important text representation vectors are obtained. To solve the problem that the shallow network structure cannot extract deep semantic features, we design a multi-channel pyramid convolution network, which first extracts deep text features from the channels of different windows and then fuses the text features of each channel. By comparing with the state-of-the-art methods, the model in this article has better performance in text classification of power equipment defects.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3464380',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Application and Research of Deep Mining of Health Medical Big Data Based on Internet of Things',\n",
       "  'authors': \"['Youshen Chi']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': 'AIAM2021: 2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture',\n",
       "  'abstract': 'Traditional data mining algorithms are mostly difficult to handle large data sets, but need to manage big data and discover hidden knowledge. Therefore, the combination of data mining algorithms and IoT technology is the development trend of data processing in the future, but because of the difficulty, not all algorithms can be implemented on the cloud platform, so the research results are not enough. The core idea of cloud computing is architecture. All data mining algorithm improvement strategies need to be designed according to the characteristics of the architecture, and each part must implement specific functions. It is difficult or impossible to implement an algorithm that cannot be decomposed into the above form in the architecture. With the widespread application of IoT technology, a large number of unstructured, distributed and even data mining requirements in mobile devices pose serious challenges to existing data mining technologies. How to use cloud computing technology to achieve large-scale distributed data collection, transmission and mining has become an important research direction. This paper investigates some health care big data, combines the Internet of Things cloud data platform, and introduces the traditional association rules algorithm and its interest level in mining technology. On this basis, the improved algorithm is improved and compared with other traditional algorithms. Establish a simulation platform to implement algorithms and mine training data. It proves that the improved algorithm has lower spatial complexity and faster processing rate in data mining, and analyzes the data mining technology in health medical data.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3495018.3495462',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Automatic detection of emotions in Twitter data: a scalable decision tree classification method',\n",
       "  'authors': \"['Jaishree Ranganathan', 'Nikhil Hedge', 'Allen S. Irudayaraj', 'Angelina A. Tzacheva']\",\n",
       "  'date': 'July 2018',\n",
       "  'source': \"RevOpiD '18: Proceedings of the Workshop on Opinion Mining, Summarization and Diversification\",\n",
       "  'abstract': 'Social media data is one of the promising datasets to mine meaningful insights with applications in business and social science. Emotion mining has significant importance in the field of psychology, cognitive science, and linguistics etc. Recently, textual emotion mining has gained attraction in modern science applications. In this paper, we propose an approach which builds a corpus of tweets and related fields where each tweet is classified with respective emotion based on lexicon, and emoticons. Also, we have developed decision tree classifier, decision forest, and rule-based classifier for automatic classification of emotion based on the labeled corpus. The method is implemented in Apache Spark for scalability and BigData accommodation. Results show higher classification accuracy than previous works.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3301020.3303751',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'RHPTree—Risk Hierarchical Pattern Tree for Scalable Long Pattern Mining',\n",
       "  'authors': \"['Danlu Liu', 'Yu Li', 'William Baskett', 'Dan Lin', 'Chi-Ren Shyu']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Knowledge Discovery from Data',\n",
       "  'abstract': 'Risk patterns are crucial in biomedical research and have served as an important factor in precision health and disease prevention. Despite recent development in parallel and high-performance computing, existing risk pattern mining methods still struggle with problems caused by large-scale datasets, such as redundant candidate generation, inability to discover long significant patterns, and prolonged post pattern filtering. In this article, we propose a novel dynamic tree structure, Risk Hierarchical Pattern Tree (RHPTree), and a top-down search method, RHPSearch, which are capable of efficiently analyzing a large volume of data and overcoming the limitations of previous works. The dynamic nature of the RHPTree avoids costly tree reconstruction for the iterative search process and dataset updates. We also introduce two specialized search methods, the extended target search (RHPSearch-TS) and the parallel search approach (RHPSearch-SD), to further speed up the retrieval of certain items of interest. Experiments on both UCI machine learning datasets and sampled datasets of the Simons Foundation Autism Research Initiative (SFARI)—Simon’s Simplex Collection (SSC) datasets demonstrate that our method is not only faster but also more effective in identifying comprehensive long risk patterns than existing works. Moreover, the proposed new tree structure is generic and applicable to other pattern mining problems.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3488380',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Exploiting Deep Neural Networks for Intention Mining',\n",
       "  'authors': \"['Anam Habib', 'Nosheen Jelani', 'Asad Masood Khattak', 'Saima Akbar', 'Muhammad Zubair Asghar']\",\n",
       "  'date': 'February 2020',\n",
       "  'source': \"ICSCA '20: Proceedings of the 2020 9th International Conference on Software and Computer Applications\",\n",
       "  'abstract': \"In the current era of digital media, people are greatly interested to express themselves on online interaction which produces a huge amount of data. The user generated content may contain user's emotions, opinions, daily events and specially their intent or motive behind their communication. Intention identification/mining of user's reviews, that is whether a user review contains intent or not, from social media network, is an emerging area and is in great demand in various fields like online advertising, improving customer services and decision making. Until now, a lot of work has been performed by researchers on user intention identification using machine learning approaches. However, it is demanded to focus on deep neural network methods. In this research work, we have conducted experimentation on intention dataset using a deep learning method namely CNN+BILSTM. The results exhibit that the proposed model efficiently performed identification of intention sentences in user generated text with a 90% accuracy.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3384544.3384607',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Improving classification performance of support vector machines via guided custom kernel search',\n",
       "  'authors': \"['Kumar Ayush', 'Abhishek Sinha']\",\n",
       "  'date': 'July 2019',\n",
       "  'source': \"GECCO '19: Proceedings of the Genetic and Evolutionary Computation Conference Companion\",\n",
       "  'abstract': 'Support Vector Machines (SVMs) deliver state-of-the-art performance in real-world applications and are established as one of the standard tools for machine learning and data mining. A key problem of these methods is how to choose an optimal kernel function. The real-world applications have also emphasized the need to adapt the kernel to the characteristics of heterogeneous data in order to boost the classification accuracy. Therefore, our goal is to automatically search a task specific kernel function. We use reinforcement learning based search mechanisms to discover custom kernel functions and verify the effectiveness of our approach by conducting an empirical evaluation with the discovered kernel function on MNIST classification. Our experiments show that the discovered kernel function shows significantly better classification performance than well-known classic kernels. Our solution will be very effective for resource constrained systems with low memory footprint which rely on traditional machine learning algorithms like SVMs for classification tasks.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3319619.3321923',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Automation of Android applications functional testing using machine learning activities classification',\n",
       "  'authors': \"['Ariel Rosenfeld', 'Odaya Kardashov', 'Orel Zang']\",\n",
       "  'date': 'May 2018',\n",
       "  'source': \"MOBILESoft '18: Proceedings of the 5th International Conference on Mobile Software Engineering and Systems\",\n",
       "  'abstract': \"Following the ever-growing demand for mobile applications, researchers are constantly developing new test automation solutions for mobile developers. However, researchers have yet to produce an automated functional testing approach, resulting in many developers relying on a resource consuming manual testing. In this paper, we present a novel approach for the automation of functional testing in mobile software by leveraging machine learning techniques and reusing generic test scenarios. Our approach aims at relieving some of the manual functional testing burden by automatically classifying each of the application's screens to a set of common screen behaviors for which generic test scripts can be instantiated and reused. We empirically demonstrate the potential benefits of our approach in two experiments: First, using 26 randomly selected Android applications, we show that our approach can successfully instantiate and reuse generic functional tests and discover functional bugs. Second, in a human study with two experienced human mobile testers, we show that our approach can automatically cover a large portion of the human testers' work suggesting a significant potential relief in the manual testing efforts.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3197231.3197241',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A Unified Framework for Frequent Sequence Mining with Subsequence Constraints',\n",
       "  'authors': \"['Kaustubh Beedkar', 'Rainer Gemulla', 'Wim Martens']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Database Systems',\n",
       "  'abstract': 'Frequent sequence mining methods often make use of constraints to control which subsequences should be mined. A variety of such subsequence constraints has been studied in the literature, including length, gap, span, regular-expression, and hierarchy constraints. In this article, we show that many subsequence constraints—including and beyond those considered in the literature—can be unified in a single framework. A unified treatment allows researchers to study jointly many types of subsequence constraints (instead of each one individually) and helps to improve usability of pattern mining systems for practitioners. In more detail, we propose a set of simple and intuitive “pattern expressions” to describe subsequence constraints and explore algorithms for efficiently mining frequent subsequences under such general constraints. Our algorithms translate pattern expressions to succinct finite-state transducers, which we use as computational model, and simulate these transducers in a way suitable for frequent sequence mining. Our experimental study on real-world datasets indicates that our algorithms—although more general—are efficient and, when used for sequence mining with prior constraints studied in literature, competitive to (and in some cases superior to) state-of-the-art specialized methods.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3321486',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A novel malware analysis for malware detection and classification using machine learning algorithms',\n",
       "  'authors': \"['Kamalakanta Sethi', 'Shankar Kumar Chaudhary', 'Bata Krishan Tripathy', 'Padmalochan Bera']\",\n",
       "  'date': 'October 2017',\n",
       "  'source': \"SIN '17: Proceedings of the 10th International Conference on Security of Information and Networks\",\n",
       "  'abstract': 'Nowadays, Malware has become a serious threat to the digitization of the world due to the emergence of various new and complex malware every day. Due to this, the traditional signature-based methods for detection of malware effectively becomes an obsolete method. The efficiency of the machine learning model in context to the detection of malware files has been proved by different researches and studies. In this paper, a framework has been developed to detect and classify different files (e.g exe, pdf, php, etc.) as benign and malicious using two level classifier namely, Macro (for detection of malware) and Micro (for classification of malware files as a Trojan, Spyware, Adware, etc.). Cuckoo Sandbox is used for generating static and dynamic analysis report by executing files in the virtual environment. In addition, a novel model is developed for extracting features based on static, behavioral and network analysis using analysis report generated by the Cuckoo Sandbox. Weka Framework is used to develop machine learning models by using training datasets. The experimental results using proposed framework shows high detection rate with an accuracy of 100% using J48 Decision tree model, 99% using SMO (Sequential Minimal Optimization) and 97% using Random Forest tree. It also shows effective classification rate with accuracy 100% using J48 Decision tree, 91% using SMO and 66% using Random Forest tree. These results are used for detecting and classifying unknown files as benign or malicious.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3136825.3136883',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Imbalanced Data Classification Based on Hybrid Methods',\n",
       "  'authors': \"['Nai-Nan Zhang', 'Shao-Zhen Ye', 'Ting-Ying Chien']\",\n",
       "  'date': 'October 2018',\n",
       "  'source': \"ICBDR '18: Proceedings of the 2nd International Conference on Big Data Research\",\n",
       "  'abstract': 'Imbalanced data are ubiquitous in real-world datasets. This study investigate imbalanced data distribution for binary classification, i.e., where the number of majority class instances is significantly greater than the number of minority class instances. It is assumed that traditional machine learning algorithms attempt to minimize empirical risk factors, and, as a result, the classification accuracy of the minority is often sacrificed. However, people are often interested in the minority. Various data-level methods, such as over- and under-sampling, and algorithm-level methods, such as ensemble, cost-sensitive, and one-class learning, have been proposed to improve classifier performance with an imbalanced data distribution. Based on such methods, this study proposed a hybrid approach to deal with imbalanced data problem that comprises data preprocessing, clustering, data balancing, model building, and ensemble.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3291801.3291812',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Ontology-based workflow pattern mining: application to bioinformatics expertise acquisition',\n",
       "  'authors': \"['Ahmed Halioui', 'Tomas Martin', 'Petko Valtchev', 'Abdoulaye Baniré Diallo']\",\n",
       "  'date': 'April 2017',\n",
       "  'source': \"SAC '17: Proceedings of the Symposium on Applied Computing\",\n",
       "  'abstract': 'Workflow platforms enable the construction of solutions to complex problems as step-wise processes made of components including methods, tools, data formats, parameters, etc. Successful workflow solutions require a mastering of the different components paving the way to automated acquisition of problem solving expertise. Thus, process mining could be applied to discover workflow patterns. Due to the combinatorics of component instances in rich domains such as bioinformatics, generalized patterns could be a relevant way of abstraction. Here, we propose an approach for mining workflow patterns, defined on the top of a domain ontology which categorizes workflow elements and their interactions. While original workflows are doubly-labelled DAGs, the underlying problem is transformed into a mining of generalized sequential patterns with links between their items. The proposed mining method traverses the ensuing pattern space using five refinement primitives that exploit the is-a links from the ontology. To assess the prediction power of the approach, we applied the generated patterns as templates in a recommendation platform to complete partial workflows under construction. The analyses of recommendations vs. actual content of a real-world dataset reveals that non trivial patterns can be found and further used to provide plausible recommendations with high accuracies (fMeasure >75+).',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3019612.3019866',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Indoor Seat Occupancy Classification with Wi-Fi Channel State Information and Machine Learning Methods',\n",
       "  'authors': \"['Yichuan Zhang', 'Jiefeng Li', 'Hang Wang']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"SSIP '21: Proceedings of the 2021 4th International Conference on Sensors, Signal and Image Processing\",\n",
       "  'abstract': 'Keeping a distance by monitoring the seat occupancy is an essential way to prevent the spread of virus inside a room. However, most current human sensing methods need customized devices, so a cheaper way of indoor seat occupancy classification is in need. Recent researches indicate that Wi-Fi channel state information (CSI) can be utilized for indoor human sensing without wearable sensors. This paper proposes a multi-person seat occupancy classification method based on machine learning and Wi-Fi CSI received by commercial network interface card. We designed an experimental scenario of 5 seats and 2 individuals, and use commercial Wi-Fi devices to build a multi-input multi-output (MIMO) system indoors to acquire an adequate dataset. Then a pipeline consists of phase calibration, linear interpolation, outlier removal and threshold de-noising was applied to preprocess the raw CSI amplitude and phase data. After sliding window feature extraction, convolutional neural network (CNN) and some conventional machine learning methods, such as naive Bayes (NB), decision tree (DT), support vector machine (SVM) and K-nearest neighbors (KNN), are used to classify seat occupancy, among which CNN performs the best, with a classification accuracy of 95%.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3502814.3502826',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'MSURU: Large Scale E-commerce Image Classification with Weakly Supervised Search Data',\n",
       "  'authors': \"['Yina Tang', 'Fedor Borisyuk', 'Siddarth Malreddy', 'Yixuan Li', 'Yiqun Liu', 'Sergey Kirshner']\",\n",
       "  'date': 'July 2019',\n",
       "  'source': \"KDD '19: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining\",\n",
       "  'abstract': 'In this paper we present a deployed image recognition system used in a large scale commerce search engine, which we call MSURU. It is designed to process product images uploaded daily to Facebook Marketplace. Social commerce is a growing area within Facebook and understanding visual representations of product content is important for search and recommendation applications on Marketplace. In this paper, we present techniques we used to develop efficient large-scale image classifiers using weakly supervised search log data. We perform extensive evaluation of presented techniques, explain practical experience of developing large-scale classification systems and discuss challenges we faced. Our system, MSURU out-performed current state of the art system developed at Facebook [23] by 16% in e-commerce domain. MSURU is deployed to production with significant improvements in search success rate and active interactions on Facebook Marketplace.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3292500.3330696',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Mobile App Classification Method Using Machine Learning Based User Emotion Recognition',\n",
       "  'authors': \"['Taewon Kwak', 'Moonhyun Kim']\",\n",
       "  'date': 'August 2018',\n",
       "  'source': \"ICICM '18: Proceedings of the 8th International Conference on Information Communication and Management\",\n",
       "  'abstract': \"In this paper, we propose a convolutional neural network based application method which shows superior performance in image classification. Recently, various requirements such as emotional UI, rather than the existing Touch UI method, have been presented in the mobile UI field, and a methodology for this is presented. First, it recognizes human facial expressions through Convolutional Neural Networks (CNN).Based on the second recognized facial expression, a multi-layer perceptron (MLP) Learning. This enables the application to be executed only by the user's face when the mobile application is restarted. In order to implement and experiment on this, we implemented and experimented with the Google inception model structure to enhance the performance of face recognition in the first CNN - based facial recognition step. In the second application classification step, We implemented a method using multidimensional data for recognition. As a result, CNN - based facial expression recognition achieved about 98% accuracy, and based on this, the application classification to be studied in this paper was able to obtain a maximum of 97.9% accuracy\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3268891.3268909',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Relative evolutionary hierarchical analysis for gene expression data classification',\n",
       "  'authors': \"['Marcin Czajkowski', 'Marek Kretowski']\",\n",
       "  'date': 'July 2019',\n",
       "  'source': \"GECCO '19: Proceedings of the Genetic and Evolutionary Computation Conference\",\n",
       "  'abstract': 'Relative Expression Analysis (RXA) focuses on finding interactions among a small group of genes and studies the relative ordering of their expression rather than their raw values. Algorithms based on that idea play an important role in biomarker discovery and gene expression data classification. We propose a new evolutionary approach and a paradigm shift for RXA applications in data mining as we redefine the inter-gene relations using the concept of a cluster of co-expressed genes. The global hierarchical classification allows finding various sub-groups of genes, unifies the main variants of RXA algorithms and explores a much larger solution space compared to current solutions based on exhaustive search. Finally, the multi-objective fitness function, which includes accuracy, discriminative power of genes and clusters consistency, as well as specialized variants of genetic operators improve evolutionary convergence and reduce model underfitting. Importantly, patterns in predictive structures are kept comprehensible and may have direct applicability. Experiments carried out on 8 cancer-related gene expression datasets show that the proposed approach allows finding interesting patterns and significantly improves the accuracy of predictions.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3321707.3321862',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Mining Algorithm Roadmap in Scientific Publications',\n",
       "  'authors': \"['Hanwen Zha', 'Wenhu Chen', 'Keqian Li', 'Xifeng Yan']\",\n",
       "  'date': 'July 2019',\n",
       "  'source': \"KDD '19: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining\",\n",
       "  'abstract': 'The number of scientific publications is ever increasing. The long time to digest a scientific paper posts great challenges on the number of papers people can read, which impedes a quick grasp of major activities in new research areas especially for intelligence analysts and novice researchers. To accelerate such a process, we first define a new problem called mining algorithm roadmap in scientific publications, and then propose a new weakly supervised method to build the roadmap. The algorithm roadmap describes evolutionary relation between different algorithms, and sketches the undergoing research and the dynamics of the area. It is a tool for analysts and researchers to locate the successors and families of algorithms when analyzing and surveying a research field. We first propose abbreviated words as candidates for algorithms and then use tables as weak supervision to extract these candidates and labels. Next we propose a new method called Cross-sentence Attention NeTwork for cOmparative Relation (CANTOR) to extract comparative algorithms from text. Finally, we derive order for individual algorithm pairs with time and frequency to construct the algorithm roadmap. Through comprehensive experiments, our proposed algorithm shows its superiority over the baseline methods on the proposed task.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3292500.3330913',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Hybrid model of correlation based filter feature selection and machine learning classifiers applied on smart meter data set',\n",
       "  'authors': \"['Sinayobye Janvier Omar', 'Kiwanuka N. Fred', 'Kaawaase Kyanda Swaib', 'Musabe Richard']\",\n",
       "  'date': 'May 2019',\n",
       "  'source': \"SEiA '19: Proceedings of the 2nd Symposium on Software Engineering in Africa\",\n",
       "  'abstract': 'Feature selection is referred to the process of obtaining a subset from an original feature set according to certain feature selection criterion, which selects the relevant features of the dataset. It plays a role in compressing the data processing scale, where the redundant and irrelevant features are removed. Feature selection techniques show that more information is not always good in machine learning applications. Apply different algorithms for the data at hand and with baseline classification performance values we can select a final feature selection algorithm. In this paper, we propose a hybrid classification model, which has correlation based filter feature selection algorithm and Machine learning as classifiers. The objective of this study is to select relevant features and analyze the outperform machine learning algorithms in order to train our model, predict and compare their classification performance. In this method, features are ordered according to their Absolute correlation value with respect to the class attribute. Then top K Features are selected from ordered list of features to form a reduced dataset. This proposed classifier model is applied to our smart meter datasets. To measure the performance of these selected features; seven benchmark classifier are used; Random Forest (RF), Logistic Regression (LR), k-Nearest Neighbor (kNN), Naïve Bayes (NB), Decision Tree (DT), Linear Discriminant Analysis (LDA) and Support Vector Machine (SVM). This paper then analyzes the performance of all classifiers with feature selection in term of accuracy, sensitivity, F-Measure, Specificity, Precision, and MCC. From our experiment, we found that Random Forest classifier performed higher than other used classifiers.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/SEiA.2019.00009',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Process Mining to Unleash Variability Management: Discovering Configuration Workflows Using Logs',\n",
       "  'authors': \"['Ángel Jesús Varela-Vaca', 'José A. Galindo', 'Belén Ramos-Gutiérrez', 'María Teresa Gómez-López', 'David Benavides']\",\n",
       "  'date': 'September 2019',\n",
       "  'source': \"SPLC '19: Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A\",\n",
       "  'abstract': 'Variability models are used to build configurators. Configurators are programs that guide users through the configuration process to reach a desired configuration that fulfils user requirements. The same variability model can be used to design different configurators employing different techniques. One of the elements that can change in a configurator is the configuration workflow, i.e., the order and sequence in which the different configuration elements are presented to the configuration stakeholders. When developing a configurator, a challenge is to decide the configuration workflow that better suites stakeholders according to previous configurations. For example, when configuring a Linux distribution, the configuration process start by choosing the network or the graphic card, and then other packages with respect to a given sequence. In this paper, we present COnfiguration workfLOw proceSS mIning (COLOSSI), an automated technique that given a set of logs of previous configurations and a variability model can automatically assist to determine the configuration workflow that better fits the configuration logs generated by user activities. The technique is based on process discovery, commonly used in the process mining area, with an adaptation to configuration contexts. Our proposal is validated using existing data from an ERP configuration environment showing its feasibility. Furthermore, we open the door to new applications of process mining techniques in different areas of software product line engineering.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3336294.3336303',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Comparative Analysis of Machine Learning Techniques using Customer Feedback Reviews of Oil and Gas Companies',\n",
       "  'authors': \"['Layth Nabeel AlRawi', 'Osama Ibraheem Ashour Ashour']\",\n",
       "  'date': 'November 2020',\n",
       "  'source': \"ICSIE '20: Proceedings of the 9th International Conference on Software and Information Engineering\",\n",
       "  'abstract': \"Sentiment analysis is the process of computationally identifying and categorizing opinions from a piece of text to determine whether the writer's attitude towards a practical topic, products or services is positive, negative or neutral. In this study, Machine Learning techniques are used to perform sentiment analysis on Oil and Gas customer feedback data. We present a comparison of different classification algorithms used for opinion mining, including Support Vector Machine (SVM), Naïve Bayes (NB), Instance Based Learning (IB3), Random Forest (RF), Partial Decision trees (PART), and Logit Boost (LB). Many studies have been performed on sentiment analysis in different sectors, but research into Oil and Gas customer feedback has been limited. Therefore, we have targeted a pathless sector, namely the Petroleum sector, where companies express their opinions towards specific products or services. Waikato Environment for Knowledge Analysis (WEKA) is used for experimental results. The WEKA environment is open source software entailing a collection of machine learning algorithms to solve data mining problems. The main aim of this study is to evaluate the efficiency of the above mentioned classifiers in terms of Precision, Recall, F-Measure and Accuracy. The findings of the comparison analysis indicate that the Naïve-Bayes classifier gives the best Accuracy of all classifiers. A small dataset could be considered as a limitation to our study due to the difficulty of gaining more datasets at the time of the research. However, this research will play a vital role for researchers in making decisions about the algorithm that they are going to use to solve their data mining problems.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3436829.3436871',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Mining World Indicators for Analyzing and Modeling the Development of Countries',\n",
       "  'authors': \"['Hong Huang', 'Mingyuan Chi', 'Yu Song', 'Hai Jin']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM/IMS Transactions on Data Science',\n",
       "  'abstract': 'The world indicators released by the World Bank or other organizations usually give the basic public knowledge about the world. However, separate and static index lacks the complex interplay among different indicators and thus cannot help us have an overall understanding of the world. To this end, we study the world indicators from a different angle. Firstly, we discover that there exist correlations between indicators either from a static view or from a dynamic view. Moreover, taking the trade and diplomatic relationships into consideration, we construct a multi-relational network to depict the interactions between different countries, and propose a\\xa0Multiple Relations to Vector (MR2vec) model to study world indicators from a network perspective. The experimental results show the changes of world indicators are predictable with the proposed model, and our proposed MR2vec has wide adaptability in predicting multi-relation networks.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3488059',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Sentiment Analysis of Twitter Data Using Machine Learning Techniques and Scikit-learn',\n",
       "  'authors': \"['Shihab Elbagir', 'Jing Yang']\",\n",
       "  'date': 'December 2018',\n",
       "  'source': \"ACAI '18: Proceedings of the 2018 International Conference on Algorithms, Computing and Artificial Intelligence\",\n",
       "  'abstract': 'Sentiment analysis of Twitter data is an area that has experienced significant growth in recent years. The ability to identify sentiment from tweets using machine learning techniques has attracted researchers because of the simple efficiency of machine learning techniques. This paper tackles the use of machine learning algorithms and Scikit-learn in sentiment analysis of Twitter data. To do this, we perform analyses on Twitter datasets made publicly available by NLTK Corpora and create an efficient feature by using a feature extraction technique. We train and test various machine learning classifiers such as MultinomialNB, BernoulliNB, LogisticRegression, SGD classifier, SVC, LinearSVC, and NuSVC. Experimental results demonstrate that BernoulliNB, LogisticRegression, and SGD classifier reached accuracy as high as 75%.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3302425.3302492',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Mining Software Entities in Scientific Literature: Document-level NER for an Extremely Imbalance and Large-scale Task',\n",
       "  'authors': \"['Patrice Lopez', 'Caifan Du', 'Johanna Cohoon', 'Karthik Ram', 'James Howison']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"CIKM '21: Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management\",\n",
       "  'abstract': 'We present a comprehensive information extraction system dedicated to software entities in scientific literature. This task combines the complexity of automatic reading of scientific documents (PDF processing, document structuring, styled/rich text, scaling) with challenges specific to mining software entities: high heterogeneity and extreme sparsity of mentions, document-level cross-references, disambiguation of noisy software mentions and poor portability of Machine Learning approaches between highly specialized domains. While NER is a key component to recognize new and unseen software, considering this task as a simple NER application fails to address most of these issues. In this paper, we propose a multi-model Machine Learning approach where raw documents are ingested by a cascade of document structuring processes applied not to text, but to layout token elements. The cascading process further enriches the relevant structures of the document with a Deep Learning software mention recognizer adapted to the high sparsity of mentions. The Machine Learning cascade culminates with entity disambiguation to alleviate false positives and to provide software entity linking. A bibliographical reference resolution is integrated to the process for attaching references cited alongside the software mentions. Based on the first gold-standard annotated dataset developed for software mentions, this work establishes a new reference end-to-end performance for this task. Experiments with the CORD-19 publications have further demonstrated that our system provides practically usable performance and is scalable to the whole scientific corpus, enabling novel applications for crediting research software and for better understanding the impact of software in science.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3459637.3481936',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Text Classification Based on Enriched Vector Space Model',\n",
       "  'authors': \"['Tsvetanka Georgieva-Trifonova']\",\n",
       "  'date': 'June 2017',\n",
       "  'source': \"CompSysTech '17: Proceedings of the 18th International Conference on Computer Systems and Technologies\",\n",
       "  'abstract': 'As one of the challenges to text classification can be indicated applying a model with the following characteristics: acceptable computational complexity of the model construction; dimension reduction of the vector space without significant decreasing of the classification performance. The present research aims to find a possible solution to mentioned problems. This paper proposes a model obtained by enrichment of the vector space model with the association relationships between words extracted from their co-occurrence in the text documents. For this purpose, the lift measure of association rules between word pairs is calculated. Experiments are conducted on Reuters-21578 dataset by using SVM classifier. The results confirm that applying the model improves the binominal and polynomial classification performance in comparison to the vector space model with respect to the F-measure even after word filtering, leading to a significant dimension reduction.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3134302.3134343',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'COVID-19 classification using thermal images: thermal images capability for identifying COVID-19 using traditional machine learning classifiers',\n",
       "  'authors': \"['Martha Rebeca Canales-Fiscal', 'Rocío Ortiz López', 'Regina Barzilay', 'Víctor Treviño', 'Servando Cardona-Huerta', 'Luis Javier Ramírez-Treviño', 'Adam Yala', 'José Tamez-Peña']\",\n",
       "  'date': 'August 2021',\n",
       "  'source': \"BCB '21: Proceedings of the 12th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics\",\n",
       "  'abstract': 'Medical images have been proposed as a diagnostic tool for SARS-COV-2. The image modality more investigated on this subject is computed tomography (CT), however it has some disadvantages: it uses ionizing radiation, requires unique installations along with a complicated process limiting the number of possible tests per equipment, and the economic costs can be prohibitively high for screening a large population. For these reasons, the aim of this study is to investigate thermal images as an alternative modality for diagnosis of COVID-19. The methodology used in this study consisted of using radiomics and moment features extracted from six images obtained from thermal video clips in which optical flow and super resolution were used, these features were classified using traditional machine learning methods. Accuracies were in the range of 0.433 - 0.524. These first results conducted on thermal images suggest that the use of this type of image modality is unlikely to be favorable for COVID-19 detection.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3459930.3469558',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Report on the 2nd ACM SIGIR/SIGKDD Africa school on machine learning for data mining and search',\n",
       "  'authors': \"['Tanya Berger-Wolf', 'Ben Carterette', 'Tamer Elsayed', 'Maria Keet', 'Fabrizio Sebastiani', 'Hussein Suleman']\",\n",
       "  'date': 'June 2020',\n",
       "  'source': 'ACM SIGIR Forum',\n",
       "  'abstract': 'We report on the organization and activities of the 2nd ACM SIGIR/SIGKDD Africa School on Machine Learning for Data Mining and Search, which took place at the University of Cape Town in South Africa January 27--31, 2020.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3451964.3451968',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A unified framework of density-based clustering for semi-supervised classification',\n",
       "  'authors': \"['Jadson Castro Gertrudes', 'Arthur Zimek', 'Jörg Sander', 'Ricardo J. G. B. Campello']\",\n",
       "  'date': 'July 2018',\n",
       "  'source': \"SSDBM '18: Proceedings of the 30th International Conference on Scientific and Statistical Database Management\",\n",
       "  'abstract': 'Semi-supervised classification is drawing increasing attention in the era of big data, as the gap between the abundance of cheap, automatically collected unlabeled data and the scarcity of labeled data that are laborious and expensive to obtain is dramatically increasing. In this paper, we introduce a unified framework for semi-supervised classification based on building-blocks from density-based clustering. This framework is not only efficient and effective, but it is also statistically sound. Experimental results on a large collection of datasets show the advantages of the proposed framework.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3221269.3223037',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Decision Support Method for Product Replacement in Coin-Less Vending Machine Using Data Mining Techniques and Product Segmentation',\n",
       "  'authors': \"['Peeraya Wasutanachai', 'Kun-Ming Yu', 'Yi-Hui Chiang', 'Wen Ouyang', 'Yen-Chiu Chen']\",\n",
       "  'date': 'June 2019',\n",
       "  'source': 'DSDE 2019: Proceedings of the 2019 2nd International Conference on Data Storage and Data Engineering',\n",
       "  'abstract': 'In this research paper, we introduce a novel model for product segmentation based on data collected from coin-less vending machines called SFM (Steadiness, Frequency and Monetary) in order to track customer behavior through each segments of products and provide vending machine owner the more reasonably way to change the items on the vending machine. We adopted two well-known algorithms from data mining techniques which are association rules mining and K-means clustering. Apriori together with k-means were used in our proposed methodology. After performing the method, we tried our theory on the machines. The results from the real-word experiment showed that there were only 25% of new products that were in low value segment after applied our method while there were 67% before the usage.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3354153.3354161',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Auditing Data Provenance in Text-Generation Models',\n",
       "  'authors': \"['Congzheng Song', 'Vitaly Shmatikov']\",\n",
       "  'date': 'July 2019',\n",
       "  'source': \"KDD '19: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining\",\n",
       "  'abstract': \"To help enforce data-protection regulations such as GDPR and detect unauthorized uses of personal data, we develop a new model auditing technique that helps users check if their data was used to train a machine learning model. We focus on auditing deep-learning models that generate natural-language text, including word prediction and dialog generation. These models are at the core of popular online services and are often trained on personal data such as users' messages, searches, chats, and comments. We design and evaluate a black-box auditing method that can detect, with very few queries to a model, if a particular user's texts were used to train it (among thousands of other users). We empirically show that our method can successfully audit well-generalized models that are not overfitted to the training data. We also analyze how text-generation models memorize word sequences and explain why this memorization makes them amenable to auditing.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3292500.3330885',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'An Unsupervised Approach for Sentiment Analysis on Social Media Short Text Classification in Roman Urdu',\n",
       "  'authors': \"['Toqir A. Rana', 'Kiran Shahzadi', 'Tauseef Rana', 'Ahsan Arshad', 'Mohammad Tubishat']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Asian and Low-Resource Language Information Processing',\n",
       "  'abstract': 'During the last two decades, sentiment analysis, also known as opinion mining, has become one of the most explored research areas in Natural Language Processing (NLP) and data mining. Sentiment analysis focuses on the sentiments or opinions of consumers expressed over social media or different web sites. Due to exposure on the Internet, sentiment analysis has attracted vast numbers of researchers over the globe. A large amount of research has been conducted in English, Chinese, and other languages used worldwide. However, Roman Urdu has been neglected despite being the third most used language for communication in the world, covering millions of users around the globe. Although some techniques have been proposed for sentiment analysis in Roman Urdu, these techniques are limited to a specific domain or developed incorrectly due to the unavailability of language resources available for Roman Urdu. Therefore, in this article, we are proposing an unsupervised approach for sentiment analysis in Roman Urdu. First, the proposed model normalizes the text to overcome spelling variations of different words. After normalizing text, we have used Roman Urdu and English opinion lexicons to correctly identify users’ opinions from the text. We have also incorporated negation terms and stemming to assign polarities to each extracted opinion. Furthermore, our model assigns a score to each sentence on the basis of the polarities of extracted opinions and classifies each sentence as positive, negative, or neutral. In order to verify our approach, we have conducted experiments on two publicly available datasets for Roman Urdu and compared our approach with the existing model. Results have demonstrated that our approach outperforms existing models for sentiment analysis tasks in Roman Urdu. Furthermore, our approach does not suffer from domain dependency.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3474119',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Radio Frequency-based Techniques of Drone Detection and Classification using Machine Learning',\n",
       "  'authors': \"['Mariam M. Alaboudi', 'Manar Abu Talib', 'Qassim Nasir']\",\n",
       "  'date': 'November 2020',\n",
       "  'source': \"ICRAI '20: Proceedings of the 6th International Conference on Robotics and Artificial Intelligence\",\n",
       "  'abstract': 'This research paper provides a comprehensive survey review on drone detection using Radio Frequency (RF)-based techniques along with machine learning and localization algorithms. RF signals proved its effectiveness in detecting drones, however, due to the lack of a published survey, this research paper reviews the newly emerged RF-based techniques by addressing the implemented methods and discussing the results obtained in terms of the testing environment, range of detection and accuracy of the system. In this survey review, thirty conference and journal papers have been collected, however only selected papers have been discussed depending on the contribution and limited space of the paper. Finally, this survey also discusses the challenges encountered in drone detection using RF due to its great impact on the efficiency of the system.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3449301.3449348',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A data mining approach for predicting main-engine rotational speed from vessel-data measurements',\n",
       "  'authors': \"['Dimitrios Kaklis', 'George Giannakopoulos', 'Iraklis Varlamis', 'Constantine D. Spyropoulos', 'Takis J. Varelas']\",\n",
       "  'date': 'June 2019',\n",
       "  'source': \"IDEAS '19: Proceedings of the 23rd International Database Applications &amp; Engineering Symposium\",\n",
       "  'abstract': \"In this work we face the challenge of estimating a ship's main-engine rotational speed from vessel data series, in the context of sea vessel route optimization. To this end, we study the value of different vessel data types as predictors of the engine rotational speed. As a result, we utilize speed data under a time-series view and examine how extracting locally-aware prediction models affects the learning performance. We apply two different approaches: the first utilizes clustering as a pre-processing step to the creation of many local models; the second builds upon splines to predict the target value. Given the above, we show that clustering can improve performance and demonstrate how the number of clusters affects the outcome. We also show that splines perform in a promising manner, but do not clearly outperform other methods. On the other hand, we show that spline regression combined with a Delaunay partitioning offers most competitive results.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3331076.3331123',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Improving Multiclass Classification of Cybersecurity Breaches in Railway Infrastructure using Imbalanced Learning',\n",
       "  'authors': \"['Aleksandr N. Nebaba', 'Ilias K. Savvas', 'Maria A. Butakova', 'Andrey V. Chernov', 'Petr S. Shevchuk']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"ESSE '21: Proceedings of the 2021 European Symposium on Software Engineering\",\n",
       "  'abstract': \"Machine learning approaches and algorithms are spreading in wide areas in research and technology. Cybersecurity breaches are the common anomalies for networked and distributed infrastructures which are monitored, registered, and described carefully. However, the description of each security breaches episode and its classification is still a difficult problem, especially in highly complex telecommunication infrastructure. Railway information infrastructure usually has a large scale and large diversity of possible security breaches. Today's situation shows the registering of the security breaches has a mature and stable character, but the problem of their automated classification is not solved completely. Many studies on security breaches multiclass classification show inadequate accuracy of classification. We investigated the origins of this problem and suggested the possible roots consist in disbalance the datasets used for machine learning multiclass classification. Thus, we proposed an approach to improve the accuracy of the classification and verified our approach on the really collected datasets with cybersecurity breaches in railway telecommunication infrastructure. We analyzed the results of applying three imbalanced learning methodologies, namely random oversampling, synthetic minority oversampling technique, and the last one with Tomek links. We have implemented three machine learning algorithms, namely Naïve Bayes, K-means, and support vector machine, on disbalances and balanced data to estimate imbalance learning methodologies with comparing results. The proposed approach demonstrated the increase of the accuracy for multiclass classification in the range from 30 to 41%, depending on the imbalanced learning technique.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3501774.3501789',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Evaluating User Satisfaction with Typography Designs via Mining Touch Interaction Data in Mobile Reading',\n",
       "  'authors': \"['Junxiang Wang', 'Jianwei Yin', 'Shuiguang Deng', 'Ying Li', 'Calton Pu', 'Yan Tang', 'Zhiling Luo']\",\n",
       "  'date': 'April 2018',\n",
       "  'source': \"CHI '18: Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems\",\n",
       "  'abstract': \"Previous work has demonstrated that typography design has a great influence on users' reading experience. However, current typography design guidelines are mainly for general purpose, while the individual needs are nearly ignored. To achieve personalized typography designs, an important and necessary step is accurately evaluating user satisfaction with the typography designs. Current evaluation approaches, e.g., asking for users' opinions directly, however, interrupt the reading and affect users' judgments. In this paper, we propose a novel method to address this challenge by mining users' implicit feedbacks, e.g., touch interaction data. We conduct two mobile reading studies in Chinese to collect the touch interaction data from 91 participants. We propose various features based on our three hypotheses to capture meaningful patterns in the touch behaviors. The experiment results show the effectiveness of our evaluation models with higher accuracy on comparing with the baseline under three text difficulty levels, respectively.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3173574.3173687',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Predicting the Survivors of the Titanic Kaggle, Machine Learning From Disaster',\n",
       "  'authors': \"['Nadine Farag', 'Ghada Hassan']\",\n",
       "  'date': 'May 2018',\n",
       "  'source': \"ICSIE '18: Proceedings of the 7th International Conference on Software and Information Engineering\",\n",
       "  'abstract': 'April 14th, 1912 was very unfortunate for the most powerful ship ever built at that time, the Titanic. Grievously, 1503 out of 2203 passengers perished the sinking, but the rationale behind survival still remains a question mark. In efforts to study the Titanic passengers; Kaggle, a popular data science website, assembled information about each passenger back in the days of the Titanic into a dataset, and made it available for a competition titled: \"Titanic: Machine Learning from Disaster.\" This research aims to use machine learning techniques on the Titanic data to analyze the data for classification and to predict the survival of the Titanic passengers by using data-mining algorithms; specifically Decision Trees and Naïve Bayes. The prediction and efficiency of these algorithms depend greatly on data analysis and the model. The paper presents an implementation which combines the benefits of feature selection and machine learning to accurately select and distinguish characteristics of passengers\\' age, class, cabin, and port of embarkation then consequently infer an authentic model for an accurate prediction. The data-set is described and the implementation details and prediction results are presented then compared to other results. The Decision Tree algorithm has accurately predicted 90.01% of the survival of passengers, while the Gaussian Naïve Bayes witnessed 92.52% accuracy in prediction.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3220267.3220282',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Using Contextual Text Mining Algorithm to Analysis Yearly Trend for Population Ageing and Declining Fertility with Government Science and Technology Projects in Taiwan',\n",
       "  'authors': \"['C. Y. Chuang', 'M. C. Huang', 'Y. Y. Lin', 'Y. H. Hsiao', 'T. C. Wang']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"CSSE '21: Proceedings of the 4th International Conference on Computer Science and Software Engineering\",\n",
       "  'abstract': 'None',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3494885.3494919',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A Comparative Analysis with the Hybrid Algorithm Approach for Sentimental Analysis through Machine Learning',\n",
       "  'authors': \"['Ravleen Singh', 'Ganpat Joshi', 'Paras Kothari']\",\n",
       "  'date': 'August 2021',\n",
       "  'source': \"DSMLAI '21': Proceedings of the International Conference on Data Science, Machine Learning and Artificial Intelligence\",\n",
       "  'abstract': 'This research work focuses on the latest studies that have used Machine learning to find a solution to sentiment analysis problems related to sentiment polarisation. In preprocessing steps, the Models applied to stop words and Bag of words to collect datasets. Even with the widespread usage and acceptance of some approaches, a superior technique for categorising the polarisation of text documents is tough to make out. Machine learning has lately evoked attention as a method for sentiment investigation. The present work proposes a machine learning-based hybrid algorithm that incorporates N-gram technique as feature extraction. It combines a Decision tree classifier and Random forest Classifier techniques as a classification for sentiment analysis. Naïve bayse, linear classifier and support vector machine approaches are perform in the perspective of sentiment classification. Finally, a comparative study with the different supervised algorithms implemented on the product reviews dataset. The performance of the model evaluated on the confusion matrix. In the comparative analysis of classification techniques, the combined technique has shown better results than previously used supervised techniques of naïve bayse, linear classifier and support vector machine.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3484824.3484904',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Simulation Analysis of Standardized Management Measures for Private College Students Based on Data Mining',\n",
       "  'authors': \"['Lijun Fan']\",\n",
       "  'date': 'September 2021',\n",
       "  'source': 'ICISCAE 2021: 2021 4th International Conference on Information Systems and Computer Aided Education',\n",
       "  'abstract': 'With the continuous expansion of college enrollment and the flexible diversification of educational methods, most universities are facing the contradiction between the sharp increase in the number of students and the increasing shortage of teaching resources, which brings unprecedented challenges to the management of universities. With the networking of computers, the rapid development of database technology and the wide application of database management system, DM (data mining) technology has been widely used in university information management. In the era of rapid development of Internet and Internet of Things, the pace of education informatization construction is accelerating, and the construction of \"Internet+Education\" and \"Smart Campus\" is deepening. Students\\' studies, life and network behaviors are recorded as electronic data by information systems and electronic devices. This article aims to apply DM technology to the standardized management of private college students, establish a database that collects useful information, analyze the status of students in school, and provide management personnel with a basis for decision-making.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3482632.3482658',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'On the behavior of the infinite restricted boltzmann machine for clustering',\n",
       "  'authors': \"['Nikolas A. Huhnstock', 'Alexander Karlsson', 'Maria Riveiro', 'H. Joe Steinhauer']\",\n",
       "  'date': 'April 2018',\n",
       "  'source': \"SAC '18: Proceedings of the 33rd Annual ACM Symposium on Applied Computing\",\n",
       "  'abstract': 'Clustering is a core problem within a wide range of research disciplines ranging from machine learning and data mining to classical statistics. A group of clustering approaches so-called nonparametric methods, aims to cluster a set of entities into a beforehand unspecified and unknown number of clusters, making potentially expensive pre-analysis of data obsolete. In this paper, the recently, by Cote and Larochelle introduced infinite Restricted Boltzmann Machine that has the ability to self-regulate its number of hidden parameters is adapted to the problem of clustering by the introduction of two basic cluster membership assumptions. A descriptive study of the influence of several regularization and sparsity settings on the clustering behavior is presented and results are discussed. The results show that sparsity is a key adaption when using the iRBM for clustering that improves both the clustering performances as well as the number of identified clusters.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3167132.3167183',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'DL-Learner Structured Machine Learning on Semantic Web Data',\n",
       "  'authors': \"['Lorenz Bühmann', 'Jens Lehmann', 'Patrick Westphal', 'Simon Bin']\",\n",
       "  'date': 'April 2018',\n",
       "  'source': \"WWW '18: Companion Proceedings of the The Web Conference 2018\",\n",
       "  'abstract': 'The following paper is an extended summary of the journal paper \"DL-Learner A framework for inductive learning on the Semantic Web\". In this system paper, we describe the DL-Learner framework. It is beneficial in various data and schema analytic tasks with applications in different standard machine learning scenarios, e.g. life sciences, as well as Semantic Web specific applications such as ontology learning and enrichment. Since its creation in 2007, it has become the main OWL and RDF-based software framework for supervised structured machine learning and includes several algorithm implementations, usage examples and has applications building on top of the framework.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3184558.3186235',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Genetic programming based feature construction for classification with incomplete data',\n",
       "  'authors': \"['Cao Truong Tran', 'Mengjie Zhang', 'Peter Andreae', 'Bing Xue']\",\n",
       "  'date': 'July 2017',\n",
       "  'source': \"GECCO '17: Proceedings of the Genetic and Evolutionary Computation Conference\",\n",
       "  'abstract': 'Missing values are an unavoidable problem in many real-world datasets. Dealing with incomplete data is an crucial requirement for classification because inadequate treatment of missing values often causes large classification error. Feature construction has been successfully applied to improve classification with complete data, but it has been seldom applied to incomplete data. Genetic programming-based multiple feature construction (GPMFC) is a current encouraging feature construction method which uses genetic programming to evolve new multiple features from original features for classification tasks. GPMFC can improve the accuracy and reduce the complexity of many decision trees and rule-based classifiers; however, it cannot directly work with incomplete data. This paper proposes IGPMFC which is extended from GPMFC to tackle with incomplete data. IGPMFC uses genetic programming with interval functions to directly evolve multiple features for classification with incomplete data. Experimental results reveal that not only IGPMFC can substantially improve the accuracy, but also can reduce the complexity of learnt classifiers facing with incomplete data.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3071178.3071183',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Taking Advantage of Multitask Learning for Fair Classification',\n",
       "  'authors': \"['Luca Oneto', 'Michele Doninini', 'Amon Elders', 'Massimiliano Pontil']\",\n",
       "  'date': 'January 2019',\n",
       "  'source': \"AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society\",\n",
       "  'abstract': 'A central goal of algorithmic fairness is to reduce bias in automated decision making. An unavoidable tension exists between accuracy gains obtained by using sensitive information as part of a statistical model, and any commitment to protect these characteristics. Often, due to biases present in the data, using the sensitive information in the functional form of a classifier improves classification accuracy. In this paper we show how it is possible to get the best of both worlds: optimize model accuracy and fairness without explicitly using the sensitive feature in the functional form of the model, thereby treating different individuals equally. Our method is based on two key ideas. On the one hand, we propose to use Multitask Learning (MTL), enhanced with fairness constraints, to jointly learn group specific classifiers that leverage information between sensitive groups. On the other hand, since learning group specific models might not be permitted, we propose to first predict the sensitive features by any learning method and then to use the predicted sensitive feature to train MTL with fairness constraints. This enables us to tackle fairness with a three-pronged approach, that is, by increasing accuracy on each group, enforcing measures of fairness during training, and protecting sensitive information during testing. Experimental results on two real datasets support our proposal, showing substantial improvements in both accuracy and fairness.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3306618.3314255',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'On accelerating ultra-large-scale mining',\n",
       "  'authors': \"['Ganesha Upadhyaya', 'Hridesh Rajan']\",\n",
       "  'date': 'May 2017',\n",
       "  'source': \"ICSE-NIER '17: Proceedings of the 39th International Conference on Software Engineering: New Ideas and Emerging Results Track\",\n",
       "  'abstract': 'Ultra-large-scale mining has been shown to be useful for a number of software engineering tasks e.g. mining specifications, defect prediction. We propose a new research direction for accelerating ultra-large-scale mining that goes beyond parallelization. Our key idea is to analyze the interaction pattern between the mining task and the artifact to cluster artifacts such that running the mining task on one candidate artifact from each cluster is sufficient to produce results for other artifacts in the same cluster. Our artifact clustering criteria go beyond syntactic, semantic, and functional similarities to mining-task-specific similarity, where the interaction pattern between the mining task and the artifact is used for clustering. Our preliminary evaluation demonstrates that our technique significantly reduces the overall mining time.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ICSE-NIER.2017.11',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Unsupervised context switch for classification tasks on data streams with recurrent concepts',\n",
       "  'authors': \"['Denis M. dos Reis', 'André G. Maletzke', 'Gustavo E. A. P. A. Batista']\",\n",
       "  'date': 'April 2018',\n",
       "  'source': \"SAC '18: Proceedings of the 33rd Annual ACM Symposium on Applied Computing\",\n",
       "  'abstract': 'In this paper, we propose a novel approach to deal with concept drifts in data streams. We assume we can collect labeled data for different concepts in the training phase; however, in the test phase, no labels are available. Our approach consists of the storage of a limited number of classification models and the unsupervised identification of the most suitable one depending on the current concept. Several real-world classification problems with extreme label latency can use this setting. One example is the identification of insects species using wing-beat data gathered by sensors in field conditions. Flying insects have their wing-beat frequency indirectly affected by temperature, among other factors. In this work, we show that we can dynamically identify which is the most appropriate classification model, among other models from data with different temperature conditions, without any temperature information. We then expand the use of the method to other data sets and obtain accurate results.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3167132.3167189',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'An FPGA-Based Hardware Accelerator for K-Nearest Neighbor Classification for Machine Learning on Mobile Devices',\n",
       "  'authors': \"['Mokhles A. Mohsin', 'Darshika G. Perera']\",\n",
       "  'date': 'June 2018',\n",
       "  'source': \"HEART '18: Proceedings of the 9th International Symposium on Highly-Efficient Accelerators and Reconfigurable Technologies\",\n",
       "  'abstract': 'Machine learning has become one of the cornerstones of information technology. Many machine learning algorithms have found their way into mobile devices, which have stringent requirements. Also, machine learning algorithms, such as classification and clustering, are becoming complex, requiring high processing power, thus affecting the speedup. In this paper, we introduce unique, novel, and efficient hardware architecture to accelerate the K-nearest neighbor classifier on mobile devices, considering constraints associated with these devices. We evaluate the efficiency of our hardware architecture, in terms of speedup, space, and accuracy. Our design is generic, parameterized, and scalable. Our hardware design achieves 127 times speedup compared to its software counterpart, and can also achieve 100% classification accuracy.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3241793.3241810',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Data mining Contribution to Intrusion Detection Systems Improvement',\n",
       "  'authors': \"['Abdelkader khobzaoui', 'Mohamed Benhamouda', 'Mahmoud Fahsi']\",\n",
       "  'date': 'June 2020',\n",
       "  'source': \"ICIST '20: Proceedings of the 10th International Conference on Information Systems and Technologies\",\n",
       "  'abstract': 'Intrusion detection has become one of the most prominent components in computer security field. In order to improve intrusion detection systems performances, Data mining techniques have been massively used. Actually, the massive integration of data mining technique in intrusion detection has quickly emerged due to the fact that intrusion detection task is a classification problem by nature and Data mining provides tools to discover consistent and useful patterns of system features that describe program and user behavior, and use the set of relevant system features to build classifiers able to recognize abnormal or suspicious activities. In this paper, we will stress the role of the data mining in intrusion detection systems development and promotion.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3447568.3448514',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Histological classification of non-small cell lung cancer with RNA-seq data using machine learning models',\n",
       "  'authors': \"['Robert B. Eshun', 'Md Khurram Monir Rabby', 'A. K. M. Kamrul Islam', 'Marwan U. Bikdash']\",\n",
       "  'date': 'August 2021',\n",
       "  'source': \"BCB '21: Proceedings of the 12th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics\",\n",
       "  'abstract': 'This study develops an automated model using the supervised learning framework(s) for the classification of the histological subtypes of non-small cell lung cancer (NSCLC). The machine learning (ML) approach is performed on gene expression profiles for the diagnosis of lung cancer that is the primary cause of cancer deaths worldwide. The performance of five classical Machine Learning (ML) estimators and four ensemble ML classifiers are evaluated on an RNA-Sequence dataset of 127 cases of NSCLC. The Decision Tree (DT) and Bagging models show promising classification accuracy up to 100% and area under curves (AUCs) is more than 0.97. The implemented ensemble methods collectively exhibit good performance in terms of AUCs (0.68 -- 1.00). The findings are comparable to the high precision ML models and the results provide an insight into the supervised models that can achieve higher diagnosis accuracy on RNA-Seq-based gene expression profiles of NSCLC subtypes.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3459930.3471168',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Facilitating and Managing Machine Learning and Data Analysis Tasks in Big Data Environments using Web and Microservice Technologies',\n",
       "  'authors': \"['Shadi Shahoud', 'Sonja Gunnarsdottir', 'Hatem Khalloof', 'Clemens Duepmeier', 'Veit Hagenmeyer']\",\n",
       "  'date': 'November 2019',\n",
       "  'source': \"MEDES '19: Proceedings of the 11th International Conference on Management of Digital EcoSystems\",\n",
       "  'abstract': 'Driven by the great advance of machine learning in a wide range of application areas, the need for developing machine learning frameworks effectively as well as easily usable by novices increased dramatically. Furthermore, building machine learning models in the context of big data environments still represents a great challenge. In the present paper, we tackle these challenges by introducing a new generic framework for efficiently facilitating the training, testing, managing, storing, and retrieving of machine learning models in the context of big data. The framework makes use of a powerful big data software stack and a microservice architecture for a fully manageable and highly scalable solution. A highly configurable user interface is introduced giving the user the ability to easily train, test, and manage machine learning models. Moreover, it automatically indexes models and allows flexible exploration of them in the visual interface. The performance of the new framework is evaluated on state-of-the-arts machine learning algorithms: it is shown that storing and retrieving machine learning models as well as a respective acceptable low overhead demonstrate an efficient approach to facilitate machine learning in big data environments.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3297662.3365807',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Novel Machine Learning for Big Data Analytics in Intelligent Support Information Management Systems',\n",
       "  'authors': \"['Zhihan Lv', 'Ranran Lou', 'Hailin Feng', 'Dongliang Chen', 'Haibin Lv']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Management Information Systems',\n",
       "  'abstract': \"Two-dimensional1 arrays of bi-component structures made of cobalt and permalloy elliptical dots with thickness of 25 nm, length 1 mm and width of 225 nm, have been prepared by a self-aligned shadow deposition technique. Brillouin light scattering has been exploited to study the frequency dependence of thermally excited magnetic eigenmodes on the intensity of the external magnetic field, applied along the easy axis of the elements.Scientific information technology has been developed rapidly. Here, the purposes are to make people's lives more convenient and ensure information management and classification. The machine learning algorithm is improved to obtain the optimized Light Gradient Boosting Machine (LightGBM) algorithm. Then, an Android-based intelligent support information management system is designed based on LightGBM for the big data analysis and classification management of information in the intelligent support information management system. The system is designed with modules of employee registration and login, company announcement notice, attendance and attendance management, self-service, and daily tools with the company as the subject. Furthermore, the performance of the constructed information management system is analyzed through simulations. Results demonstrate that the training time of the optimized LightGBM algorithm can stabilize at about 100s, and the test time can stabilize at 0.68s. Besides, its accuracy rate can reach 89.24%, which is at least 3.6% higher than other machine learning algorithms. Moreover, the acceleration efficiency analysis of each algorithm suggests that the optimized LightGBM algorithm is suitable for processing large amounts of data; its acceleration effect is more apparent, and its acceleration ratio is higher than other algorithms. Hence, the constructed intelligent support information management system can reach a high accuracy while ensuring the error, with apparent acceleration effect. Therefore, this model can provide an experimental reference for information classification and management in various fields.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3469890',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A Case Study of HealthCare Platform using Big Data Analytics and Machine Learning',\n",
       "  'authors': \"['M. D. Samiul Islam', 'Daizong Liu', 'Kewei Wang', 'Pan Zhou', 'Li Yu', 'Dapeng Wu']\",\n",
       "  'date': 'June 2019',\n",
       "  'source': \"HPCCT '19: Proceedings of the 2019 3rd High Performance Computing and Cluster Technologies Conference\",\n",
       "  'abstract': \"The medical services in Bangladesh are shortage nowadays; people are suffering from getting the correct treatment from the hospital. With the low proportion of the doctors and the low per capita salary in Bangladesh, patients need to spend more money to get the appropriate treatments. Therefore, it is necessary to apply modern information technologies by which the scaffold between the patients and specialists can be reduced, and the patients can take proper treatment at a lower cost. Fortunately, we can solve this critical problem by utilizing interaction among electrical devices. With the big data collected from these devices, machine learning is a powerful tool for the data analytics because of its high accuracy, lower computational costs, and lower power consumption. This research is based on a case of study by the incorporation of the database, mobile application, web application and develops a novel platform through which the patients and the doctors can interact. In addition, the platform helps to store the patients' health data to make the final prediction using machine learning methods to get the proper healthcare treatment with the help of the machines and the doctors. The experiment result shows the high accuracy over 95% of the disease detection using machine learning methods, with the cost 90% lower than the local hospital in Bangladesh, which provides the strong support to implement of our platform in the remote area of the country.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3341069.3342980',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Event modeling and mining: a long journey toward explainable events',\n",
       "  'authors': \"['Xinhong Chen', 'Qing Li']\",\n",
       "  'date': 'None',\n",
       "  'source': 'The VLDB Journal — The International Journal on Very Large Data Bases',\n",
       "  'abstract': 'AbstractRecently, research on event management has redrawn much attention and made great progress. As the core tasks of event management, event modeling and mining are essential for accessing and utilizing events effectively. In this survey, we provide a detailed review of event modeling and event mining. Based on a general definition, different characteristics of events are described, along with the associated challenges. Then, we define four forms of events in order to better classify currently available but somewhat confusing event types; we also compare different event representation and relationship analysis techniques used for different forms of events. Finally, we discuss several pending issues and application-specific challenges which also shed light on future research directions.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1007/s00778-019-00545-0',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': '500+ times faster than deep learning: a case study exploring faster methods for text mining stackoverflow',\n",
       "  'authors': \"['Suvodeep Majumder', 'Nikhila Balaji', 'Katie Brey', 'Wei Fu', 'Tim Menzies']\",\n",
       "  'date': 'May 2018',\n",
       "  'source': \"MSR '18: Proceedings of the 15th International Conference on Mining Software Repositories\",\n",
       "  'abstract': 'Deep learning methods are useful for high-dimensional data and are becoming widely used in many areas of software engineering. Deep learners utilizes extensive computational power and can take a long time to train- making it difficult to widely validate and repeat and improve their results. Further, they are not the best solution in all domains. For example, recent results show that for finding related Stack Overflow posts, a tuned SVM performs similarly to a deep learner, but is significantly faster to train. This paper extends that recent result by clustering the dataset, then tuning every learners within each cluster. This approach is over 500 times faster than deep learning (and over 900 times faster if we use all the cores on a standard laptop computer). Significantly, this faster approach generates classifiers nearly as good (within 2% F1 Score) as the much slower deep learning method. Hence we recommend this faster methods since it is much easier to reproduce and utilizes far fewer CPU resources. More generally, we recommend that before researchers release research results, that they compare their supposedly sophisticated methods against simpler alternatives (e.g applying simpler learners to build local models).',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3196398.3196424',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Deep learning benchmarks and datasets for social media image classification for disaster response',\n",
       "  'authors': \"['Firoj Alam', 'Ferda Ofli', 'Muhammad Imran', 'Tanvirul Alam', 'Umair Qazi']\",\n",
       "  'date': 'December 2020',\n",
       "  'source': \"ASONAM '20: Proceedings of the 12th IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining\",\n",
       "  'abstract': 'During a disaster event, images shared on social media helps crisis managers gain situational awareness and assess incurred damages, among other response tasks. Recent advances in computer vision and deep neural networks have enabled the development of models for real-time image classification for a number of tasks, including detecting crisis incidents, filtering irrelevant images, classifying images into specific humanitarian categories, and assessing the severity of damage. Despite several efforts, past works mainly suffer from limited resources (i.e., labeled images) available to train more robust deep learning models. In this study, we propose new datasets for disaster type detection, and informativeness classification, and damage severity assessment. Moreover, we relabel existing publicly available datasets for new tasks. We identify exact- and near-duplicates to form non-overlapping data splits, and finally consolidate them to create larger datasets. In our extensive experiments, we benchmark several state-of-the-art deep learning models and achieve promising results. We release our datasets and models publicly, aiming to provide proper baselines as well as to spur further research in the crisis informatics community.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ASONAM49781.2020.9381294',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'DeepXML: A Deep Extreme Multi-Label Learning Framework Applied to Short Text Documents',\n",
       "  'authors': \"['Kunal Dahiya', 'Deepak Saini', 'Anshul Mittal', 'Ankush Shaw', 'Kushal Dave', 'Akshay Soni', 'Himanshu Jain', 'Sumeet Agarwal', 'Manik Varma']\",\n",
       "  'date': 'March 2021',\n",
       "  'source': \"WSDM '21: Proceedings of the 14th ACM International Conference on Web Search and Data Mining\",\n",
       "  'abstract': \"Scalability and accuracy are well recognized challenges in deep extreme multi-label learning where the objective is to train architectures for automatically annotating a data point with the most relevant subset of labels from an extremely large label set. This paper develops the DeepXML framework that addresses these challenges by decomposing the deep extreme multi-label task into four simpler sub-tasks each of which can be trained accurately and efficiently. Choosing different components for the four sub-tasks allows DeepXML to generate a family of algorithms with varying trade-offs between accuracy and scalability. In particular, DeepXML yields the Astec algorithm that could be 2-12% more accurate and 5-30x faster to train than leading deep extreme classifiers on publically available short text datasets. Astec could also efficiently train on Bing short text datasets containing up to 62 million labels while making predictions for billions of users and data points per day on commodity hardware. This allowed Astec to be deployed on the Bing search engine for a number of short text applications ranging from matching user queries to advertiser bid phrases to showing personalized ads where it yielded significant gains in click-through-rates, coverage, revenue and other online metrics over state-of-the-art techniques currently in production. DeepXML's code is available at https://github.com/Extreme-classification/deepxml.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3437963.3441810',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Mining Frequent Itemsets Using Improved Apriori on Spark',\n",
       "  'authors': \"['Fei Gao', 'Ashutosh Khandelwal', 'Jiangjiang Liu']\",\n",
       "  'date': 'April 2019',\n",
       "  'source': \"ICISDM '19: Proceedings of the 2019 3rd International Conference on Information System and Data Mining\",\n",
       "  'abstract': 'Finding the frequent itemset is one of the most investigated extents of data mining. The Apriori algorithm is the most established algorithm for frequent itemset mining, but it has issues regarding scanning frequent databases and generating a large amount of candidate sets. To solve these issues, an Improved Apriori algorithm was proposed. We examined the data structure, implementation, and algorithmic features that mainly focus on frequent itemset mining. We are representing an Improved Apriori algorithm on Spark in which simple and scalable implementation is done to achieve a faster process with lower support thresholds. We examined the improved Apriori algorithm on Extended Bakery Dataset and Retail Dataset. The results show execution time was reduced by 40% and 57% compared with the original Apriori algorithm.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3325917.3325925',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Data Preprocessing for Learning, Analyzing and Detecting Scene Text Video based on Rotational Gradient',\n",
       "  'authors': \"['Manasa Devi Devi Mortha', 'Seetha Maddala', 'Vishwanadha Raju']\",\n",
       "  'date': 'April 2021',\n",
       "  'source': \"DATA'21: International Conference on Data Science, E-learning and Information Systems 2021\",\n",
       "  'abstract': 'Challenging annotated video datasets are in huge demand for the researchers and embedded industrials to learn and build an artificial intelligence for detecting, localizing and classifying the objects of interest aimed at various applications under pattern recognition and computer vision domain. It is very significant to produce those annotated sets to the respective communal. This paper focuses on text as annotated data in video for detection, localization, tracking and classification to solve several optical character recognition (OCR) based problems. Text is very essential in understanding the nature of the video because of diverse applications which are in renowned today like video retrieval and searching, driverless cars, industrial goods automation, geocoding and many more. Hence, it is important to understand how to create, prepare and load datasets to make ready for the machine to learn and understand. First, we have applied bilateral filter to preserve the edge information. Then, rotational gradient approach is proposed to detect the text in variable viewpoints. Later, the combination of morphology and contours has applied to generate blobs with bounding box around the detected regions by eradicating quasi text areas. The simulation results have shown better performance than traditional techniques with better detection rate on ICDAR Robust Reading Competition on Text in Video 2013-15 datasets.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3460620.3460621',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Data Mining to Characterize Seasonal Patterns of Apis mellifera Honey Bee Colonies',\n",
       "  'authors': \"['Felipe Anderson O. Maciel', 'Antonio Rafael Braga', 'Rhaniel M. Xavier', 'Ticiana L. Coelho da Silva', 'Breno M. Freitas', 'Danielo G. Gomes']\",\n",
       "  'date': 'June 2018',\n",
       "  'source': \"SBSI '18: Proceedings of the XIV Brazilian Symposium on Information Systems\",\n",
       "  'abstract': 'Among the agricultural crops used for human consumption, 75% depends on pollination. As the principal pollinating agent, bees are essential for the food production for humans and the ecosystems sustainability. However, a combination of habitat destruction, climate change and exposure to pesticides and pathogens has led to a significant decrease in bee population. Here we propose a method to recognize status patterns of Apis mellifera colonies through the application of data mining techniques. Using a real dataset from the HiveTool.net containing Apis mellifera temperature, humidity and weight data, we identified 3 status patterns in the observed hive. Our results suggest that the recognized patterns are consistent with a honey bee colony life cycle. Based on the found patterns, we propose a high accuracy classification model capable of automatically identifying colony status for new samples.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3229345.3229386',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Discovering software vulnerabilities using data-flow analysis and machine learning',\n",
       "  'authors': \"['Jorrit Kronjee', 'Arjen Hommersom', 'Harald Vranken']\",\n",
       "  'date': 'August 2018',\n",
       "  'source': \"ARES '18: Proceedings of the 13th International Conference on Availability, Reliability and Security\",\n",
       "  'abstract': 'We present a novel method for static analysis in which we combine data-flow analysis with machine learning to detect SQL injection (SQLi) and Cross-Site Scripting (XSS) vulnerabilities in PHP applications. We assembled a dataset from the National Vulnerability Database and the SAMATE project, containing vulnerable PHP code samples and their patched versions in which the vulnerability is solved. We extracted features from the code samples by applying data-flow analysis techniques, including reaching definitions analysis, taint analysis, and reaching constants analysis. We used these features in machine learning to train various probabilistic classifiers. To demonstrate the effectiveness of our approach, we built a tool called WIRECAML, and compared our tool to other tools for vulnerability detection in PHP code. Our tool performed best for detecting both SQLi and XSS vulnerabilities. We also tried our approach on a number of open-source software applications, and found a previously unknown vulnerability in a photo-sharing web application.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3230833.3230856',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Analysis of Various Diabetic Prediction Methods of Machine Learning',\n",
       "  'authors': \"['Ankur Goyal', 'Kailash Kumar']\",\n",
       "  'date': 'August 2021',\n",
       "  'source': \"DSMLAI '21': Proceedings of the International Conference on Data Science, Machine Learning and Artificial Intelligence\",\n",
       "  'abstract': 'The method that can derive valuable information from rough data is data mining. Prediction analysis is a data mining technique that predicts future possibilities from current knowledge. A range of statistical techniques (including machine learning, predictive modelling and data mining) are included in predictive analytics and statistics (both historical and current) to estimate, predict, and predict future results. There are different steps in the prediction analysis that include pre-processing, extraction of features and classification. This paper is focused on the use of machine learning techniques for diabetic prediction. In this paper, different techniques for diabetic prediction are reviewed based on machine learning.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3484824.3484898',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'HGAT: Heterogeneous Graph Attention Networks for Semi-supervised Short Text Classification',\n",
       "  'authors': \"['Tianchi Yang', 'Linmei Hu', 'Chuan Shi', 'Houye Ji', 'Xiaoli Li', 'Liqiang Nie']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Information Systems',\n",
       "  'abstract': 'Short text classification has been widely explored in news tagging to provide more efficient search strategies and more effective search results for information retrieval. However, most existing studies, concentrating on long text classification, deliver unsatisfactory performance on short texts due to the sparsity issue and the insufficiency of labeled data. In this article, we propose a novel heterogeneous graph neural network-based method for semi-supervised short text classification, leveraging full advantage of limited labeled data and large unlabeled data through information propagation along the graph. Specifically, we first present a flexible heterogeneous information network (HIN) framework for modeling short texts, which can integrate any type of additional information and meanwhile capture their relations to address the semantic sparsity. Then, we propose Heterogeneous Graph Attention networks (HGAT) to embed the HIN for short text classification based on a dual-level attention mechanism, including node-level and type-level attentions. To efficiently classify new coming texts that do not previously exist in the HIN, we extend our model HGAT for inductive learning, avoiding re-training the model on the evolving HIN. Extensive experiments on single-/multi-label classification demonstrates that our proposed model HGAT significantly outperforms state-of-the-art methods across the benchmark datasets under both transductive and inductive learning.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3450352',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Multiple imputation and genetic programming for classification with incomplete data',\n",
       "  'authors': \"['Cao Truong Tran', 'Mengjie Zhang', 'Peter Andreae', 'Bing Xue']\",\n",
       "  'date': 'July 2017',\n",
       "  'source': \"GECCO '17: Proceedings of the Genetic and Evolutionary Computation Conference\",\n",
       "  'abstract': 'Many industrial and research datasets suffer from an unavoidable issue of missing values. One of the most common approaches to solving classification with incomplete data is to use an imputation method to fill missing values with plausible values before applying classification algorithms. Multiple imputation is a powerful approach to estimating missing values, but it is very expensive to use multiple imputation to estimate missing values for a single instance that needs to be classified. Genetic programming (GP) has been widely used to construct classifiers for complete data, but it seldom has been used for incomplete data. This paper proposes an approach to combining multiple imputation and GP to evolve classifiers for incomplete data. The proposed method uses multiple imputation to provide a high quality training data. It also searches for common patterns of missing values, and uses GP to build a classifier for each pattern of missing values. Therefore, the proposed method generates a set of classifiers that can be used to directly classify any new incomplete instance without requiring imputation. Experimental results show that the proposed method not only can be faster than other common methods for classification with incomplete data but also can achieve better classification accuracy.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3071178.3071181',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Employing Auto-Annotated Data for Government Document Classification',\n",
       "  'authors': \"['Yajun Song', 'Zeyuan Li', 'Jie He', 'Zesong Li', 'Xin Fang', 'Dagang Chen']\",\n",
       "  'date': 'March 2019',\n",
       "  'source': \"ICIAI '19: Proceedings of the 2019 3rd International Conference on Innovation in Artificial Intelligence\",\n",
       "  'abstract': 'In China, the government documents are documents with legal effect and of standard forms formulated in the process of government administration. With the continuous development of e-government in China, government database size increases hugely. To fully utilize the potential of the database, many applications based on natural language processing (NLP) are developed. Classification is a fundamental task for many NLP applications such as automatic document archive, intelligent search, and personalized recommendation. Presently, in China, the government document classification method which based on issuing departments has very low accuracy. Traditional text classifiers based on machine learning or deep learning models rely heavily on human-labeled training data. While there are no open data sets on the government documents, we propose a method to automatically constructing large-scale annotated data set for government document classification based on the information retrieval method. Experiment results show that the supervised classification model trained on our automatically constructed data set outperforms the baseline method 15% on F1-score.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3319921.3319970',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': \"Supervised machine learning for service providers' classification using multiple criteria in a network architecture environment\",\n",
       "  'authors': \"['Imane Haddar', 'Brahim Raouyane', 'Mostafa Bellafkih']\",\n",
       "  'date': 'October 2018',\n",
       "  'source': \"SITA'18: Proceedings of the 12th International Conference on Intelligent Systems: Theories and Applications\",\n",
       "  'abstract': 'The service selection in a Next Generation Network field remains a challenging problem for service providers, as they have to satisfy customers and keep their earnings. Given the growing number of telecom service providers, the customer is in a dilemma to choose the right service with a fair price. To do so, we propose in this paper a supervised learning algorithm since it is a classification problem. Based on requirements specified in the contract called Service Level Agreement (SLA) in IP Multimedia Service (IMS) network, we ended up choosing the decision trees algorithm for several reasons that we will explore later in this work. This method will assist users in selecting the right service for a better management of contracts between the involved entities.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3289402.3289532',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Role of Discourse Information in Urdu Sentiment Classification: A Rule-based Method and Machine-learning Technique',\n",
       "  'authors': \"['Dr. Muhammad Awais', 'Dr. Muhammad Shoaib']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Asian and Low-Resource Language Information Processing',\n",
       "  'abstract': 'In computational linguistics, sentiment analysis refers to the classification of opinions in a positive class or a negative class. There exist a lot of different methods for sentiment analysis of the English language, but the literature lacks the availability of methods and techniques for Urdu, which is the largely spoken language in the South Asian sub-continent and the national language of Pakistan. The currently available techniques, such as adjective count method known as Bag of Words (BoW), is not sufficient for classification of complex sentiment written in the Urdu language. Also, the performance of available machine-learning techniques (with legacy features), for classification of Urdu sentiments, are not comparable with the achieved accuracy of other languages. In the case of the English language, the discourse information (sub-sentence-level information) boosts the performance of both the BoW method and machine-learning techniques, but there are very few works available that have tested the context-level information for the sentiment analysis of the Urdu language. This research aims to extract the discourse information from the Urdu sentiments and utilise the discourse information to improve the performance and reduce the error rate of existing techniques for Urdu Sentiment classification. The proposed solution extracts the discourse information, suggests a new set of features for machine-learning techniques, and introduces a set of rules to extend the capabilities of the BoW model. The results show that the task has been enhanced significantly and the performance metrics such as recall, precision, and accuracy are increased by 31.25%, 8.46%, and 21.6%, respectively. In future, the proposed technique can be extended to sentiments with more than two sub-opinions, such as for blogs, reviews, and TV talk shows.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3300050',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Employing Auto-Annotated Data for Government Document Classification',\n",
       "  'authors': \"['Yajun Song', 'Zeyuan Li', 'Jie He', 'Zesong Li', 'Xin Fang', 'Dagang Chen']\",\n",
       "  'date': 'March 2019',\n",
       "  'source': \"ICIAI '19: Proceedings of the 2019 3rd International Conference on Innovation in Artificial Intelligence\",\n",
       "  'abstract': 'In China, the government documents are documents with legal effect and of standard forms formulated in the process of government administration. With the continuous development of e-government in China, government database size increases hugely. To fully utilize the potential of the database, many applications based on natural language processing (NLP) are developed. Classification is a fundamental task for many NLP applications such as automatic document archive, intelligent search, and personalized recommendation. Presently, in China, the government document classification method which based on issuing departments has very low accuracy. Traditional text classifiers based on machine learning or deep learning models rely heavily on human-labeled training data. While there are no open data sets on the government documents, we propose a method to automatically constructing large-scale annotated data set for government document classification based on the information retrieval method. Experiment results show that the supervised classification model trained on our automatically constructed data set outperforms the baseline method 15% on F1-score.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3319921.3319970',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Linear support vector machine to classify the vibrational modes for complex chemical systems',\n",
       "  'authors': \"['Triet Huynh Minh Le', 'Tung Thanh Tran', 'Lam Kim Huynh']\",\n",
       "  'date': 'February 2018',\n",
       "  'source': \"ICMLSC '18: Proceedings of the 2nd International Conference on Machine Learning and Soft Computing\",\n",
       "  'abstract': 'Classification of vibrational modes into hindered internal rotation (HIR) and harmonic oscillation modes is important to obtain correct thermodynamic data for a chemical species for a wide range of temperatures. In this study, we propose a multivariate linear support vector machine (SVM) model to solve this challenging binary classification problem. The results of the proposed model were found to be similar to those of logistic regression and 2-5% better than those of the rule-based method. Moreover, the number of features found by linear SVM was also fewer than that of logistic regression (five versus six), which makes it easier to be interpreted by chemists. The detailed explanation of such differences is also presented. The three models were implemented in the GUI of the Multi-Species Multi-Channel Software Suite (Duong et al., Int. J. Chem. Kinet, 2015, 564) to facilitate the determination of HIR modes as well as the calculation of thermodynamic properties for a chemical species of interest.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3184066.3184087',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Discovering knowledge from student interactions: clustering vs classification',\n",
       "  'authors': \"['Sheila Lucero Sánchez López', 'Rebeca P. Díaz Redondo', 'Ana Fernández Vilas']\",\n",
       "  'date': 'October 2017',\n",
       "  'source': 'TEEM 2017: Proceedings of the 5th International Conference on Technological Ecosystems for Enhancing Multiculturality',\n",
       "  'abstract': 'Currently, we live in a technological environment that gives us the possibility to carry out many actions with our own style, times and preferences. We are free to interact with technology and the education is no exception, the use of educational platforms has increased considerably in recent years. The main objective of this work is to know how students interact. We analyse the interaction of university students in a blended course comparing two methods: clustering and classification.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3144826.3145390',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Mining assumptions for software components using machine learning',\n",
       "  'authors': \"['Khouloud Gaaloul', 'Claudio Menghi', 'Shiva Nejati', 'Lionel C. Briand', 'David Wolfe']\",\n",
       "  'date': 'November 2020',\n",
       "  'source': 'ESEC/FSE 2020: Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering',\n",
       "  'abstract': 'Software verification approaches aim to check a software component under analysis for all possible environments. In reality, however, components are expected to operate within a larger system and are required to satisfy their requirements only when their inputs are constrained by environment assumptions. In this paper, we propose EPIcuRus, an approach to automatically synthesize environment assumptions for a component under analysis (i.e., conditions on the component inputs under which the component is guaranteed to satisfy its requirements). EPIcuRus combines search-based testing, machine learning and model checking. The core of EPIcuRus is a decision tree algorithm that infers environment assumptions from a set of test results including test cases and their verdicts. The test cases are generated using search-based testing, and the assumptions inferred by decision trees are validated through model checking. In order to improve the efficiency and effectiveness of the assumption generation process, we propose a novel test case generation technique, namely Important Features Boundary Test (IFBT), that guides the test generation based on the feedback produced by machine learning. We evaluated EPIcuRus by assessing its effectiveness in computing assumptions on a set of study subjects that include 18 requirements of four industrial models. We show that, for each of the 18 requirements, EPIcuRus was able to compute an assumption to ensure the satisfaction of that requirement, and further, ≈78% of these assumptions were computed in one hour.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3368089.3409737',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Early-Stage Diabetes Prediction using Data Mining Algorithms',\n",
       "  'authors': \"['Md Moniruzzaman', 'A. G. M. Zaman', 'Rifah Tasnia', 'Sutopa Biswas', 'Mehnur Khanam']\",\n",
       "  'date': 'March 2022',\n",
       "  'source': \"ICCA '22: Proceedings of the 2nd International Conference on Computing Advancements\",\n",
       "  'abstract': 'Diabetes is a very common disease nowadays. If not treated early diabetes can pose a profoundly serious health threat. Much research has been conducted to find out the optimal solution for diabetes detection by applying different data mining algorithms, where the dataset consists of different medicinal attributes. In this study, our aim is to examine whether diabetes can be detected at early-stage by applying different data mining algorithms to the non-medicinal dataset; as well as to investigate whether data normalization techniques can improve the classifiers accuracy. Naive Bayes, K-Nearest Neighbor (KNN), Support Vector Machines (SVM), Decision Tree, Random Forest, and Gradient Boosting Classifier (GBC) algorithms are applied to the Early Stage Diabetes Risk Prediction Dataset in conjunction with Decimal Point Scaling, Z-Score Normalization, Pareto Scaling, Variable Stability Scaling, Min-Max normalization, Max normalization, Maximum Absolute Scaling, Mean Centered Scaling, Soft-max normalization, Power Transformer, Median and Median Absolute Deviation Normalization, Robust Scaling and Log Scaling normalization methods. In this experiment, we discovered that early-stage diabetes detection is possible without any medical diagnosis data. The result shows that GBC performs better compared to other classification algorithms in combination with data normalization and achieved an impressive 99.038% prediction accuracy.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3542954.3542990',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': \"Supervised machine learning for service providers' classification using multiple criteria in a network architecture environment\",\n",
       "  'authors': \"['Imane Haddar', 'Brahim Raouyane', 'Mostafa Bellafkih']\",\n",
       "  'date': 'October 2018',\n",
       "  'source': \"SITA'18: Proceedings of the 12th International Conference on Intelligent Systems: Theories and Applications\",\n",
       "  'abstract': 'The service selection in a Next Generation Network field remains a challenging problem for service providers, as they have to satisfy customers and keep their earnings. Given the growing number of telecom service providers, the customer is in a dilemma to choose the right service with a fair price. To do so, we propose in this paper a supervised learning algorithm since it is a classification problem. Based on requirements specified in the contract called Service Level Agreement (SLA) in IP Multimedia Service (IMS) network, we ended up choosing the decision trees algorithm for several reasons that we will explore later in this work. This method will assist users in selecting the right service for a better management of contracts between the involved entities.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3289402.3289532',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Mining cross product line rules with multi-objective search and machine learning',\n",
       "  'authors': \"['Safdar Aqeel Safdar', 'Hong Lu', 'Tao Yue', 'Shaukat Ali']\",\n",
       "  'date': 'July 2017',\n",
       "  'source': \"GECCO '17: Proceedings of the Genetic and Evolutionary Computation Conference\",\n",
       "  'abstract': \"Nowadays, an increasing number of systems are being developed by integrating products (belonging to different product lines) that communicate with each other through information networks. Cost-effectively supporting Product Line Engineering (PLE) and in particular enabling automation of configuration in PLE is a challenge. Capturing rules is the key for enabling automation of configuration. Product configuration has a direct impact on runtime interactions of communicating products. Such products might be within or across product lines and there usually don't exist explicitly specified rules constraining configurable parameter values of such products. Manually specifying such rules is tedious, time-consuming, and requires expert's knowledge of the domain and the product lines. To address this challenge, we propose an approach named as SBRM that combines multi-objective search with machine learning to mine rules. To evaluate the proposed approach, we performed a real case study of two communicating Video Conferencing Systems belonging to two different product lines. Results show that SBRM performed significantly better than Random Search in terms of fitness values, Hyper-Volume, and machine learning quality measurements. When comparing with rules mined with real data, SBRM performed significantly better in terms of Failed Precision (18%), Failed Recall (72%), and Failed F-measure (59%).\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3071178.3071261',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Design and Implementation of Tax Collection and Management Index Early Warning System Based on Data Mining',\n",
       "  'authors': \"['Jie Mao']\",\n",
       "  'date': 'December 2021',\n",
       "  'source': 'ICASIT 2021: 2021 International Conference on Aviation Safety and Information Technology',\n",
       "  'abstract': \"With the continuous improvement of tax information level of tax authorities, tax authorities have formed a large number of tax business management data in their daily business. However, at present, only a simple historical query is provided for these historical data, which is far from the expected advanced applications such as analysis and prediction and decision support. This paper designs and implements a set of early warning system of tax collection and management indicators by using database technology, constructs indicators according to taxpayers' declaration data, and uses data warehouse technology, attribute-oriented summary method and association rule algorithm for data mining to provide data support for tax early warning. Setting up the corresponding early warning value system can reduce the risk of tax evasion, improve the quality of tax monitoring, and then improve the efficiency of law enforcement and management level.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3510858.3511348',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'MeLL: Large-scale Extensible User Intent Classification for Dialogue Systems with Meta Lifelong Learning',\n",
       "  'authors': \"['Chengyu Wang', 'Haojie Pan', 'Yuan Liu', 'Kehan Chen', 'Minghui Qiu', 'Wei Zhou', 'Jun Huang', 'Haiqing Chen', 'Wei Lin', 'Deng Cai']\",\n",
       "  'date': 'August 2021',\n",
       "  'source': \"KDD '21: Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining\",\n",
       "  'abstract': 'User intent detection is vital for understanding their demands in dialogue systems. Although the User Intent Classification (UIC) task has been widely studied, for large-scale industrial applications, the task is still challenging. This is because user inputs in distinct domains may have different text distributions and target intent sets. When the underlying application evolves, new UIC tasks continuously emerge in a large quantity. Hence, it is crucial to develop a framework for large-scale extensible UIC that continuously fits new tasks and avoids catastrophic forgetting with an acceptable parameter growth rate. In this paper, we introduce the Meta Lifelong Learning (MeLL) framework to address this task. In MeLL, a BERT-based text encoder is employed to learn robust text representations across tasks, which is slowly updated for lifelong learning. We design global and local memory networks to capture the cross-task prototype representations of different classes, treated as the meta-learner quickly adapted to different tasks. Additionally, the Least Recently Used replacement policy is applied to manage the global memory such that the model size does not explode through time. Finally, each UIC task has its own task-specific output layer, with the attentive summarization of various features. We have conducted extensive experiments on both open-source and real industry datasets. Results show that MeLL improves the performance compared with strong baselines and also reduces the number of total parameters. We have also deployed MeLL on a real-world e-commerce dialogue system AliMe and observed significant improvements in terms of both F1 and the resources usage.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3447548.3467107',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Quantum Fair Machine Learning',\n",
       "  'authors': \"['Elija Perrier']\",\n",
       "  'date': 'July 2021',\n",
       "  'source': \"AIES '21: Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society\",\n",
       "  'abstract': \"In this paper, we inaugurate the field of quantum fair machine learning. We undertake a comparative analysis of differences and similarities between classical and quantum fair machine learning algorithms, specifying how the unique features of quantum computation alter measures, metrics and remediation strategies when quantum algorithms are subject to fairness constraints. We present the first results in quantum fair machine learning by demonstrating the use of Grover's search algorithm to satisfy statistical parity constraints imposed on quantum algorithms. We provide lower-bounds on iterations needed to achieve such statistical parity within ε-tolerance. We extend canonical Lipschitz-conditioned individual fairness criteria to the quantum setting using quantum metrics. We examine the consequences for typical measures of fairness in machine learning context when quantum information processing and quantum data are involved. Finally, we propose open questions and research programmes for this new field of interest to researchers in computer science, ethics and quantum computation.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3461702.3462611',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Wikipedia-Based Relatedness Measurements for Multilingual Short Text Clustering',\n",
       "  'authors': \"['Tatsuya Nakamura', 'Masumi Shirakawa', 'Takahiro Hara', 'Shojiro Nishio']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Asian and Low-Resource Language Information Processing',\n",
       "  'abstract': 'Throughout the world, people can post information about their local area in their own languages using social networking services. Multilingual short text clustering is an important task to organize such information, and it can be applied to various applications, such as event detection and summarization. However, measuring the relatedness between short texts written in various languages is a challenging problem. In addition to handling multiple languages, the semantic gaps among all languages must be considered. In this article, we propose two Wikipedia-based semantic relatedness measurement methods for multilingual short text clustering. The proposed methods solve the semantic gap problem by incorporating the inter-language links of Wikipedia into Extended Naive Bayes (ENB), a probabilistic method that can be applied to measure semantic relatedness among monolingual short texts. The proposed methods represent a multilingual short text as a vector of the English version of Wikipedia articles (entities). By transferring texts to a unified vector space, the relatedness between texts in different languages with similar meanings can be increased. We also propose an approach that can improve clustering performance and reduce the processing time by eliminating language-specific entities in the unified vector space. Experimental results on multilingual Twitter message clustering revealed that the proposed methods outperformed cross-lingual explicit semantic analysis, a previously proposed method to measure relatedness between texts in different languages. Moreover, the proposed methods were comparable to ENB applied to texts translated into English using a proprietary translation service. The proposed methods enabled relatedness measurements for multilingual short text clustering without requiring machine translation processes.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3276473',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Context-aware Outstanding Fact Mining from Knowledge Graphs',\n",
       "  'authors': \"['Yueji Yang', 'Yuchen Li', 'Panagiotis Karras', 'Anthony K. H. Tung']\",\n",
       "  'date': 'August 2021',\n",
       "  'source': \"KDD '21: Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining\",\n",
       "  'abstract': 'An Outstanding Fact (OF) is an attribute that makes a target entity stand out from its peers. The mining of OFs has important applications, especially in Computational Journalism, such as news promotion, fact-checking, and news story finding. However, existing approaches to OF mining: (i) disregard the context in which the target entity appears, hence may report facts irrelevant to that context; and (ii) require relational data, which are often unavailable or incomplete in many application domains. In this paper, we introduce the novel problem of mining Context-aware Outstanding Facts (COFs) for a target entity under a given context specified by a context entity. We propose FMiner, a context-aware mining framework that leverages knowledge graphs (KGs) for COF mining. FMiner generates COFs in two steps. First, it discovers top-k relevant relationships between the target and the context entity from a KG. We propose novel optimizations and pruning techniques to expedite this operation, as this process is very expensive on large KGs due to its exponential complexity. Second, for each derived relationship, we find the attributes of the target entity that distinguish it from peer entities that have the same relationship with the context entity, yielding the top-l COFs. As such, the mining process is modeled as a top-(k,l) search problem. Context-awareness is ensured by relying on the relevant relationships with the context entity to derive peer entities for COF extraction. Consequently, FMiner can effectively navigate the search to obtain context-aware OFs by incorporating a context entity. We conduct extensive experiments, including a user study, to validate the efficiency and the effectiveness of FMiner.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3447548.3467272',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Classification of APIs by hierarchical clustering',\n",
       "  'authors': \"['Johannes Härtel', 'Hakan Aksu', 'Ralf Lämmel']\",\n",
       "  'date': 'May 2018',\n",
       "  'source': \"ICPC '18: Proceedings of the 26th Conference on Program Comprehension\",\n",
       "  'abstract': 'APIs can be classified according to the programming domains (e.g., GUIs, databases, collections, or security) that they address. Such classification is vital in searching repositories (e.g., the Maven Central Repository for Java) and for understanding the technology stack used in software projects. We apply hierarchical clustering to a curated suite of Java APIs to compare the computed API clusters with preexisting API classifications. Clustering entails various parameters (e.g., the choice of IDF versus LSI versus LDA). We describe the corresponding variability in terms of a feature model. We exercise all possible configurations to determine the maximum correlation with respect to two baselines: i) a smaller suite of APIs manually classified in previous research; ii) a larger suite of APIs from the Maven Central Repository, thereby taking advantage of crowd-sourced classification while relying on a threshold-based approach for identifying important APIs and versions thereof, subject to an API dependency analysis on GitHub. We discuss the configurations found in this way and we examine the influence of particular features on the correlation between computed clusters and baselines. To this end, we also leverage interactive exploration of the parameter space and the resulting dendrograms. In this manner, we can also identify issues with the use of classifiers (e.g., missing classifiers) in the baselines and limitations of the clustering approach.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3196321.3196344',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Modeling and Mining Domain Shared Knowledge for Sentiment Analysis',\n",
       "  'authors': \"['Guang-You Zhou', 'Jimmy Xiangji Huang']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Information Systems',\n",
       "  'abstract': 'Sentiment classification aims to automatically predict sentiment polarity (e.g., positive or negative) of user generated sentiment data (e.g., reviews, blogs). In real applications, these user-generated sentiment data can span so many different domains that it is difficult to label the training data for all of them. Therefore, we study the problem of sentiment classification adaptation task in this article. That is, a system is trained to label reviews from one source domain but is meant to be used on the target domain. One of the biggest challenges for sentiment classification adaptation task is how to deal with the problem when two data distributions between the source domain and target domain are significantly different from one another. However, our observation is that there might exist some domain shared knowledge among certain input dimensions of different domains. In this article, we present a novel method for modeling and mining the domain shared knowledge from different sentiment review domains via a joint non-negative matrix factorization–based framework. In this proposed framework, we attempt to learn the domain shared knowledge and the domain-specific information from different sentiment review domains with several various regularization constraints. The advantage of the proposed method can promote the correspondence under the topic space between the source domain and the target domain, which can significantly reduce the data distribution gap across two domains. We conduct extensive experiments on two real-world balanced data sets from Amazon product reviews for sentence-level and document-level binary sentiment classification. Experimental results show that our proposed approach significantly outperforms several strong baselines and achieves an accuracy that is competitive with the most well-known methods for sentiment classification adaptation.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3091995',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Mining Patterns of Drug-Disease Association from Biomedical Texts',\n",
       "  'authors': \"['Wen-Juan Hou', 'Bo-Syun Lee', 'Hung-Chi Chen']\",\n",
       "  'date': 'January 2018',\n",
       "  'source': \"ICBBB '18: Proceedings of the 2018 8th International Conference on Bioscience, Biochemistry and Bioinformatics\",\n",
       "  'abstract': 'Drug repurposing aims to identify new indications for approved drugs, and it can promisingly reduce time and drug development costs. The goal of the paper, drug-disease relation extraction automatically from biomedical texts, is fundamental to the study of drug repurposing since lots of clinical case studies published in an unstructured textual form. To analyze the number of verbs and nouns pertinent to diseases and medications in the training data, two models with different drug-disease orders are established, and some rules are proposed at this phase. The first model is for the sentences with the order that the disease name precedes the drug name. The second model is for the reverse order to the first model. These verbs and nouns are then classified into categories of \"pure association,\" \"pure no association\" and \"neutrals.\" Among them, some neutrals are further verified by the Chi-square test method. As a result, the associations between diseases and medications are identified, which are called patterns later. Finally, the patterns are used in the test data to extract the disease and drug pairs. The best experimental results show the precision value of 100%, recall value of 89.0%, and F-score value of 94.2%.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3180382.3180401',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Mining Features Associated with Effective Tweets',\n",
       "  'authors': \"['Jian Xu', 'Nitesh V. Chawla']\",\n",
       "  'date': 'July 2017',\n",
       "  'source': \"ASONAM '17: Proceedings of the 2017 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 2017\",\n",
       "  'abstract': 'What tweet features are associated with higher effectiveness in tweets? Through the mining of 122 million engagements of 2.5 million original tweets, we present a systematic review of tweet time, entities, composition, and user account features. We show that the relationship between various features and tweeting effectiveness is non-linear; for example, tweets that use a few hashtags have higher effectiveness than using no or too many hashtags. This research closely relates to various industrial applications that are based on tweet features, including the analysis of advertising campaigns, the prediction of user engagement, the extraction of signals for automated trading, etc.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3110025.3110126',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Performance Evaluation of Extreme Learning Machines Classification Algorithm for Medical Datasets',\n",
       "  'authors': \"['Oyekale Abel Alade', 'Roselina Sallehuddin', 'Nor Haizan Mohamed Radzi']\",\n",
       "  'date': 'August 2021',\n",
       "  'source': 'BECB 2021: 2021 International Symposium on Biomedical Engineering and Computational Biology',\n",
       "  'abstract': 'The choice of efficient algorithms is a critical issue in the classification of medical datasets. This requires the consideration of a number of measures to ensure reliable results. In this study, the robustness of Extreme Learning Machine (ELM) and some state-of-arts classifiers were investigated on six (6) different (complete and incomplete) medical datasets. Multiple imputation technique with 5-fold-iteration was used to address the issue of missing data points in datasets with holes. The technique regenerated the missing values 100% in all the datasets. The performance of ELM was compared with Support Vector Machine (SVM), k-Nearest Neighbour (KNN) and Classification and Regression Tree (CART) on the complete and imputed datasets. The evaluations were based on classification accuracy, computational time and stability of the algorithms. ELM has 83.33% overall best accuracy, and 100% best computational time of the simulations. However, the stability of ELM is subject to further improvement, which is an area of further research.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3502060.3502156',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Imputation Techniques and Recursive Feature Elimination in Machine Learning Applied to Type II Diabetes Classification',\n",
       "  'authors': \"['Vincent Peter Catimbang Magboo', 'Ma. Sheila Abad Magboo']\",\n",
       "  'date': 'December 2021',\n",
       "  'source': \"AICCC '21: Proceedings of the 2021 4th Artificial Intelligence and Cloud Computing Conference\",\n",
       "  'abstract': 'Type II diabetes is a chronic metabolic disease secondary to elevated blood glucose levels. Complications of this disease include heart attack, stroke, blindness, renal failure, lower limb amputation and mortality. Due to its rising prevalence and consequent mortality, it is important to identify at an early stage those patients at high risk of developing diabetes. We applied 8 machine learning techniques namely: support vector machine, logistic regression, k-nearest neighbor, naïve Bayes, decision tree, random forest, AdaBoost and XGBoost in predicting diabetes using a publicly available diabetes dataset. In our study, Naïve Bayes with median imputation and recursive feature elimination obtained the highest performance with an accuracy rate of 81.0%. Although the results are very promising, one major limitation in this study is the small number of samples in the dataset. Early accurate detection can help patients to proactively monitor their lifestyle habits mitigating the risks of complications of uncontrolled diabetes.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3508259.3508288',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Web Mining in e-Procurement: A Case Study in Indonesia',\n",
       "  'authors': \"['Julius Dimas Trisaktyo Nugroho', 'Rahmad Mahendra', 'Indra Budi']\",\n",
       "  'date': 'January 2021',\n",
       "  'source': \"APIT '21: Proceedings of the 2021 3rd Asia Pacific Information Technology Conference\",\n",
       "  'abstract': 'E-procurement is an electronic procurement system that became a key factor required to manage financial aspect of a country with appropriate controls and protected by legal policies. The Presidential Regulation in Indonesia expect all government institutions to run the e-procurement process following the procurement principles, namely effective, efficient, transparent, open, competitive, fair and accountable. However in the implementation of e-tendering, which is part of e-procurement, found there are practices that not in accordance with the procurement principles. In this study, an in-depth analysis conducted to evaluate the tender activities of the ministry in Indonesia. We apply data mining process towards the national procurement portal to analyze tender data and find the hidden pattern that would be useful to support the decision-making process. This study combines several techniques, e.g. web mining and statistical analysis approach. Our finding includes correlation patterns among a number of values existing in e-procurement portal.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3449365.3449382',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Automated Intention Mining with Comparatively Fine-tuning BERT',\n",
       "  'authors': \"['Xuan Sun', 'Luqun Li', 'Francesco Mercaldo', 'Yichen Yang', 'Antonella Santone', 'Fabio Martinelli']\",\n",
       "  'date': 'December 2021',\n",
       "  'source': \"NLPIR '21: Proceedings of the 2021 5th International Conference on Natural Language Processing and Information Retrieval\",\n",
       "  'abstract': 'In the field of software engineering, intention mining is an interesting but challenging task, where the goal is to have a good understanding of user generated texts so as to capture their requirements that are useful for software maintenance and evolution. Recently, BERT and its variants have achieved state-of-the-art performance among various natural language processing tasks such as machine translation, machine reading comprehension and natural language inference. However, few studies try to investigate the efficacy of pre-trained language models in the task. In this paper, we present a new baseline with fine-tuned BERT model. Our method achieves state-of-the-art results on three benchmark data sets, outscoring baselines by a substantial margin. We also further investigate the efficacy of the pre-trained BERT model with shallower network depths through a simple strategy for layer selection.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3508230.3508254',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Cross-Modality Transfer Learning for Image-Text Information Management',\n",
       "  'authors': \"['Shuteng Niu', 'Yushan Jiang', 'Bowen Chen', 'Jian Wang', 'Yongxin Liu', 'Houbing Song']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Management Information Systems',\n",
       "  'abstract': 'In the past decades, information from all kinds of data has been on a rapid increase. With state-of-the-art performance, machine learning algorithms have been beneficial for information management. However, insufficient supervised training data is still an adversity in many real-world applications. Therefore, transfer learning (TF) was proposed to address this issue. This article studies a not well investigated but important TL problem termed cross-modality transfer learning (CMTL). This topic is closely related to distant domain transfer learning (DDTL) and negative transfer. In general, conventional TL disciplines assume that the source domain and the target domain are in the same modality. DDTL aims to make efficient transfers even when the domains or the tasks are entirely different. As an extension of DDTL, CMTL aims to make efficient transfers between two different data modalities, such as from image to text. As the main focus of this study, we aim to improve the performance of image classification by transferring knowledge from text data. Previously, a few CMTL algorithms were proposed to deal with image classification problems. However, most existing algorithms are very task specific, and they are unstable on convergence. There are four main contributions in this study. First, we propose a novel heterogeneous CMTL algorithm, which requires only a tiny set of unlabeled target data and labeled source data with associate text tags. Second, we introduce a latent semantic information extraction method to connect the information learned from the image data and the text data. Third, the proposed method can effectively handle the information transfer across different modalities (text-image). Fourth, we examined our algorithm on a public dataset, Office-31. It has achieved up to 5% higher classification accuracy than “non-transfer” algorithms and up to 9% higher than existing CMTL algorithms.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3464324',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Efficient Big Data Clustering',\n",
       "  'authors': \"['Michele Ianni', 'Elio Masciari', 'Giuseppe M. Mazzeo', 'Carlo Zaniolo']\",\n",
       "  'date': 'June 2018',\n",
       "  'source': \"IDEAS '18: Proceedings of the 22nd International Database Engineering &amp; Applications Symposium\",\n",
       "  'abstract': \"The need to support advanced analytics on Big Data is driving data scientist' interest toward massively parallel distributed systems and software platforms, such as Map-Reduce and Spark, that make possible their scalable utilization. However, when complex data mining algorithms are required, their fully scalable deployment on such platforms faces a number of technical challenges that grow with the complexity of the algorithms involved. Thus algorithms, that were originally designed for a sequential nature, must often be redesigned in order to effectively use the distributed computational resources. In this paper, we explore these problems, and then propose a solution which has proven to be very effective on the complex hierarchical clustering algorithm CLUBS+. By using four stages of successive refinements, CLUBS+ delivers high-quality clusters of data grouped around their centroids, working in a totally unsupervised fashion. Experimental results confirm the accuracy and scalability of CLUBS+.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3216122.3216154',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Multi-Feature Sparse Representation Classification Method Based on Clustering',\n",
       "  'authors': \"['Zeli Wang', 'Weizhen Sun', 'Jielong Guo', 'Xiaoliang Tang', 'Chao Li', 'Xian Wei']\",\n",
       "  'date': 'July 2019',\n",
       "  'source': 'AICS 2019: Proceedings of the 2019 International Conference on Artificial Intelligence and Computer Science',\n",
       "  'abstract': 'In complex environments, the point cloud data obtained by LiDAR Often have shadows and occlusion, which greatly reduces the accuracy and the robustness of target classification. To solve this problem, this paper proposes a robust LiDAR point cloud recognition method, called Multi-Feature Sparse Representation Classification based on Clustering (MFSRCC). Firstly, all training data are used to generate a 3D-SIFT multi-feature dictionary. Secondly, the data are reconstructed on the basis of a complete dictionary. Finally, the sparse coefficients are clustered by K-means, and hence the classifier is constructed according to the principle of minimum cluster center value. The experimental results performed on Large-Scale Point Cloud Classification benchmark show that the proposed method can significantly improve the recognition rate of LiDAR point cloud objects, and it has strong robustness to interference information.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3349341.3349506',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Mining social web service repositories for social relationships to aid service discovery',\n",
       "  'authors': \"['Alejandro Corbellini', 'Daniela Godoy', 'Cristian Mateos', 'Alejandro Zunino', 'Ignacio Lizarralde']\",\n",
       "  'date': 'May 2017',\n",
       "  'source': \"MSR '17: Proceedings of the 14th International Conference on Mining Software Repositories\",\n",
       "  'abstract': 'The Service Oriented Computing (SOC) paradigm promotes building new applications by discovering and then invoking services, i.e., software components accessible through the Internet. Discovering services means inspecting registries where textual descriptions of services functional capabilities are stored. To automate this, existing approaches index descriptions and associate users\\' queries to relevant services. However, the massive adoption of Web-exposed API development practices, specially in large service ecosystems such as the IoT, is leading to ever-growing registries which challenge the accuracy and speed of such approaches. The recent notion of Social Web Services (SWS), where registries not only store service information but also social-like relationships between users and services opens the door to new discovery schemes. We investigate an approach to discover SWSs that operates on graphs with user-service relationships and employs lightweight topological metrics to assess service similarity. Then, \"socially\" similar services, which are determined exploiting explicit relationships and mining implicit relationships in the graph, are clustered via exemplar-based clustering to ultimately aid discovery. Experiments performed with the ProgrammableWeb.com registry, which is at present the largest SWS repository with over 15k services and 140k user-service relationships, show that pure topology-based clustering may represent a promising complement to content-based approaches, which in fact are more time-consuming due to text processing operations.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/MSR.2017.16',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Research on Higher Education Intelligent Decision System Based on Data Mining',\n",
       "  'authors': \"['Shufeng Zhang', 'Lantao You']\",\n",
       "  'date': 'September 2021',\n",
       "  'source': 'ICISCAE 2021: 2021 4th International Conference on Information Systems and Computer Aided Education',\n",
       "  'abstract': 'The paper elaborated on the application process of data mining in college management decision support system: establishment of data mining goals, selection of data sources, data preparation, processing and evaluation models, verification models, implementation and maintenance models, especially for the key data preparations were analysed in depth. According to the specific characteristics of college management data, the data cleaning method has been usefully explored and tried. Finally, combined with the data conversion service (DTS) in SQL Server 2010, the establishment of the SQL Server data warehouse is explained, and data mining is carried out on this basis. According to this, the design and implementation process of establishing a university management decision support system is introduced. Practice shows that the current system is well adapted to the pace of development of college management informationization, and improves college management decision-making ability and level.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3482632.3487530',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Evolution of Decision Tree Classifiers in Open Ended Educational Data Mining',\n",
       "  'authors': \"['Tapani Toivonen', 'Ilkka Jormanainen']\",\n",
       "  'date': 'October 2019',\n",
       "  'source': \"TEEM'19: Proceedings of the Seventh International Conference on Technological Ecosystems for Enhancing Multiculturality\",\n",
       "  'abstract': 'Educational Data Mining (EDM) aims to produce new knowledge from educational settings to support educators, learners and other stakeholders. EDM aims to facilitate the understanding of the educational context by utilizing different methods of statistics and machine learning. Like wise to the current trends in data mining, also EDM approaches have shifted from black box tools and algorithms to more open-ended tools and algorithms where the EDM end-users can adjust multiple parameters, view visualizations, and even adjust the predictive models. Multiple studies have shown that the EDM end-users benefit from the white box approaches and tools. We introduce the concept of Augmented Intelligence (AUI) method in EDM. AUI method is applied in an iterative process where a white box machine learning algorithm generates a predictive model, which is adjustable by the EDM end-user. The adjustable predictive model affects to the perception of the end-user and the adjusting affects to the output of the predictive model. When applied in cycles, the AUI method generates new knowledge from the educational context. To study AUI method, a potential EDM end-user generated multiple adjustable decision tree models and we observed the evolution of the models. The study indicates that, over time, the models generalize better and the AUI method helps to avoid the issue of overfitting. Moreover, the study indicates that the cyclic nature of the AUI method facilitates deeper knowledge generation from the dataset, if the context is known by the end-user.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3362789.3362880',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Transfer learning for malware multi-classification',\n",
       "  'authors': \"['Mohamad Al Kadri', 'Mohamed Nassar', 'Haidar Safa']\",\n",
       "  'date': 'June 2019',\n",
       "  'source': \"IDEAS '19: Proceedings of the 23rd International Database Applications &amp; Engineering Symposium\",\n",
       "  'abstract': 'In this paper, we build on top of the MalConv neural networks learning architecture which was initially designed for malware/benign classification. We evaluate the transfer learning of MalConv for malware multi-class classification by extending its contribution in several directions: (1) We assess MalConv performance on a multi-classification problem using a new dataset composed of solely malware samples belonging to different malware families, (2) we evaluate MalConv on the raw bytes data as well as on the opcodes extracted from the reversed assembly samples and compare the results, (3) we validate the MalConv findings about regularization, and (4) we study MalConv performance when using a medium size dataset and limited computational resources and GPU. The obtained results show that MalConv performs equally well for multi-classification and its performance on raw byte sequences is comparable to opcodes sequences. DeCov regularization is shown to improve the accuracy results better than other regularization techniques.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3331076.3331111',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Self-tuning techniques for large scale cluster analysis on textual data collections',\n",
       "  'authors': \"['Evelina Di Corso', 'Tania Cerquitelli', 'Francesco Ventura']\",\n",
       "  'date': 'April 2017',\n",
       "  'source': \"SAC '17: Proceedings of the Symposium on Applied Computing\",\n",
       "  'abstract': \"This paper proposes PASTA (PArameter-free Solutions for Textual Analysis), a large scale engine providing strategies to automatically tune the algorithm parameters for the whole text clustering process. A data weighting strategy (e.g., TF-IDF) and a transformation method of input data (e.g., LSI) is explored before performing the cluster analysis to reduce sparseness, and make the overall analysis problem more effectively tractable. PASTA includes auto-selection strategies to off-load the end-user from parameter tuning and achieve a good quality of the clustering results. PASTA's current implementation runs on Apache Spark, a state-of-the-art distributed computing framework. As a case study, PASTA has been validated on three collections of Wikipedia documents. The experimental results show the effectiveness and the efficiency of the proposed solution in analyzing collections of documents without tuning algorithm parameters and in discovering cohesive and well-separated groups of documents.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3019612.3019661',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Exploring Active Learning for Student Behavior Classification',\n",
       "  'authors': \"['Cristina E. Dumdumaya', 'Yance Vance M. Paredes', 'Ma. Mercedes T. Rodrigo']\",\n",
       "  'date': 'March 2019',\n",
       "  'source': 'ICIET 2019: Proceedings of the 2019 7th International Conference on Information and Education Technology',\n",
       "  'abstract': \"Selection of high-quality ground truth data is a critical step for machine learning. Conventionally, a human-centered strategy is utilized to label the data. While this technique provides accurate annotations of task-specific behaviors, it is difficult, costly and error-prone. One method explored to solve these problems is active learning, a model-centered approach that minimizes human involvement. In this work, we conduct an experiment to compare the performance of active learning and passive learning strategies in selecting ground truth data for a classification task to detect the incidence of task persistent behavior from students' interaction logs. Our findings suggest that active learning tends to be more effective and efficient than passive learning in achieving a certain level of performance. However, the overall performance comparison shows that passive selection for ground truth data is as effective as the active learning approach for applications with relatively small sample size.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3323771.3323807',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Improve performance of extreme learning machine in classification of patchouli varieties with imbalanced class',\n",
       "  'authors': \"['Candra Dewi', 'Wayan Firdaus Mahmudy', 'Rio Arifando', 'Yoke Kusuma Arbawa', 'Beryl Labique Ahmadie']\",\n",
       "  'date': 'November 2020',\n",
       "  'source': \"SIET '20: Proceedings of the 5th International Conference on Sustainable Information Engineering and Technology\",\n",
       "  'abstract': 'Patchouli has various varieties with almost the same physical characteristics. This often makes it difficult to recognize varieties with a high PA (Patchouli Alcohol) content. In this study an improvisation was introduced in the identification of patchouli varieties using leaf images using Extreme Learning Machine (ELM). However, problems occur in ELM if the data used is not balanced where the training process can not able to recognize data in the minority class well. Therefore, this study conducted a process to balance the composition of the data using the Synthetic Minority Over-sampling Technique (SMOTE) method. The test results of 93 data on the imbalanced composition with a comparison of 70% of training data and 30% of test data obtained an average accuracy of 93.57%. After implementing SMOTE in the Tetraploid, Patchoulina and Sidikalang classes where the amount of data in each class becomes 58 data, an average accuracy of 96.00% is achieved. This shows the existence of an increase in the process of identification with ELM when new data generation with SMOTE is carried out to balance the composition of the data.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3427423.3427424',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Mining Implicit Relevance Feedback from User Behavior for Web Question Answering',\n",
       "  'authors': \"['Linjun Shou', 'Shining Bo', 'Feixiang Cheng', 'Ming Gong', 'Jian Pei', 'Daxin Jiang']\",\n",
       "  'date': 'August 2020',\n",
       "  'source': \"KDD '20: Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining\",\n",
       "  'abstract': 'Training and refreshing a web-scale Question Answering (QA) system for a multi-lingual commercial search engine often requires a huge amount of training examples. One principled idea is to mine implicit relevance feedback from user behavior recorded in search engine logs. All previous works on mining implicit relevance feedback target at relevance of web documents rather than passages. Due to several unique characteristics of QA tasks, the existing user behavior models for web documents cannot be applied to infer passage relevance. In this paper, we make the first study to explore the correlation between user behavior and passage relevance, and propose a novel approach for mining training data for Web QA. We conduct extensive experiments on four test datasets and the results show our approach significantly improves the accuracy of passage ranking without extra human labeled data. In practice, this work has proved effective to substantially reduce the human labeling cost for the QA service in a global commercial search engine, especially for languages with low resources. Our techniques have been deployed in multi-language services.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3394486.3403343',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Mining of DSLs and generator templates from reference applications',\n",
       "  'authors': \"['Wolf Rost']\",\n",
       "  'date': 'October 2020',\n",
       "  'source': \"MODELS '20: Proceedings of the 23rd ACM/IEEE International Conference on Model Driven Engineering Languages and Systems: Companion Proceedings\",\n",
       "  'abstract': 'Domain-Specific Languages (DSLs) found application in different domains. The development of Model-Driven Development (MDD) components is facilitated by a wealth of frameworks like EMF, Xtext, and Xtend. However, the development of the necessary IDE components still can take up to several weeks or even months until it can be used in a production environment. The first step during the development of such an MDD infrastructure is to analyse a set of reference applications to deduce the DSL used by the domain experts and the templates used in the generator. The analysis requires technical expertise and is usually performed by MDD infrastructure developers, who have to adhere to a close communication with domain experts and are exposed to high cognitive load and time-consuming tasks. The objective of this PhD project is to reduce the initial effort during the creation of new MDD infrastructure facilities for either a new domain or newly discovered platforms within a known domain. This should be made possible by the (semi-)automatic analysis of multiple codebases using Code Clone Detection (CCD) tools in a defined process flow. Code clones represent schematically redundant and generic code fragments which were found in the provided codebase. In the process, the key steps include (i) choosing appropriate reference applications (ii) distinguishing the codebase by clustering the files, (iii) reviewing the quality of the clusters, (iv) analysing the cluster by tailored CCD, and (v) transforming of the code clones, depending on the code clone type, to extract a DSL and the corresponding generator templates.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3417990.3419492',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Discovering, selecting and exploiting feature sequence records of study participants for the classification of epidemiological data on hepatic steatosis',\n",
       "  'authors': \"['Tommy Hielscher', 'Henry Völzke', 'Panagiotis Papapetrou', 'Myra Spiliopoulou']\",\n",
       "  'date': 'April 2018',\n",
       "  'source': \"SAC '18: Proceedings of the 33rd Annual ACM Symposium on Applied Computing\",\n",
       "  'abstract': 'In longitudinal epidemiological studies, participants undergo repeated medical examinations and are thus represented by a potentially large number of short examination outcome sequences. Some of those sequences may contain important information in various forms, such as patterns, with respect to the disease under study, while others may be on features of little relevance to the outcome. In this work, we propose a framework for Discovery, Selection and Exploitation (DiSelEx) of longitudinal epidemiological data, aiming to identify informative patterns among these sequences. DiSelEx combines sequence clustering with supervised learning to identify sequence groups that contribute to class separation. Newly derived and old features are evaluated and selected according to their redundancy and informativeness regarding the target variable. The selected feature set is then used to learn a classification model on the study data. We evaluate DiSelEx on cohort participants for the disorder \"hepatic steatosis\" and report on the impact on predictive performance when using sequential data in comparison to utilizing only the basic classifier.1',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3167132.3167162',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Mining and Leveraging Background Knowledge for Improving Named Entity Linking',\n",
       "  'authors': \"['Albert Weichselbraun', 'Philipp Kuntschik', 'Adrian M.P. Braşoveanu']\",\n",
       "  'date': 'June 2018',\n",
       "  'source': \"WIMS '18: Proceedings of the 8th International Conference on Web Intelligence, Mining and Semantics\",\n",
       "  'abstract': 'Knowledge-rich Information Extraction (IE) methods aspire towards combining classical IE with background knowledge obtained from third-party resources. Linked Open Data repositories that encode billions of machine readable facts from sources such as Wikipedia play a pivotal role in this development. The recent growth of Linked Data adoption for Information Extraction tasks has shed light on many data quality issues in these data sources that seriously challenge their usefulness such as completeness, timeliness and semantic correctness. Information Extraction methods are, therefore, faced with problems such as name variance and type confusability. If multiple linked data sources are used in parallel, additional concerns regarding link stability and entity mappings emerge. This paper develops methods for integrating Linked Data into Named Entity Linking methods and addresses challenges in regard to mining knowledge from Linked Data, mitigating data quality issues, and adapting algorithms to leverage this knowledge. Finally, we apply these methods to Recognyze, a graph-based Named Entity Linking (NEL) system, and provide a comprehensive evaluation which compares its performance to other well-known NEL systems, demonstrating the impact of the suggested methods on its own entity linking performance.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3227609.3227670',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Swarm Search Methods in Weka for Data Mining',\n",
       "  'authors': \"['Simon Fong', 'Robert P. Biuk-Aghai', 'Richard C. Millham']\",\n",
       "  'date': 'February 2018',\n",
       "  'source': \"ICMLC '18: Proceedings of the 2018 10th International Conference on Machine Learning and Computing\",\n",
       "  'abstract': 'Building a good prediction from high-dimensional data model in data mining is a challenging endeavor. One key step in data pre-processing is feature selection (FS) which is about finding the right feature subset for effective supervised learning. FS has two parts: feature evaluators and search methods to find the appropriate features in the search space. In this paper we introduce a collection of search methods that implement metaheuristics search which is also known as swarm search (SS). SS has the advantage over conventional search such as local search, that SS has the facility to explore global optima by a group of autonomous search agents. We have recently added nine new methods to the Weka machine learning workbench. The objective of these nine swarm search methods is to supplement the existing search methods in Weka for providing efficient and effective FS in data mining. We have carried out two experiments using synthetic data and medical data. The results show that in general SS has certain advantages over the conventional search methods. The SS methods can be found in the Weka Package Manager as open source code. Researchers and Weka users are encouraged to enhance data mining performance using these free swarm search programs.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3195106.3195167',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Research on the Effect of Blended Learning Mode Based on Text Data Analysis',\n",
       "  'authors': \"['Lin Tan', 'Yali Chen', 'Li Lai', 'Runhan Yang']\",\n",
       "  'date': 'November 2019',\n",
       "  'source': 'WAIE 2019: Proceedings of the International Workshop on Artificial Intelligence and Education',\n",
       "  'abstract': \"Blended Learning mode is developing towards more and more scientific direction under the promotion of information technology. Aiming at freshmen's learning situation, this study designs a Blended Learning mode with four communication patterns and effective teaching cycle as the main ideas. It mainly uses data mining technology to analyze the communication records of online group, and it is concluded that the students in the pilot class in the Blended Learning mode have a better learning initiative. It also proved the feasibility of text data analysis method in teaching research. Through the comparison of homework grade and final examination results with the control classes, the advantages of Blended Learning mode are further demonstrated.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3397453.3397460',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'From data points to data curves: a new approach on big data curves clustering',\n",
       "  'authors': \"['Konstantinos F. Xylogiannopoulos']\",\n",
       "  'date': 'August 2018',\n",
       "  'source': \"ASONAM '18: Proceedings of the 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining\",\n",
       "  'abstract': 'In the new era of IoT, enormous real-values datasets are produced daily. Time series created by smart devices, financial data, weather analysis, medical applications, traffic control etc. become more and more important in human day life. Analyzing and clustering these time series or in general any kind of curve could be critical. In the current paper, a new methodology (BD2C) is presented, which applies text mining and pattern detection techniques in order to cluster curves according to their shape. Several experiments have been conducted on artificial and real datasets in order to present the accuracy, efficiency and rapid discovery of the best possible clustering that the proposed methodology can achieve.',\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3382225.3382412',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Machine Learning Techniques for Automated Software Fault Detection via Dynamic Execution Data: Empirical Evaluation Study',\n",
       "  'authors': \"['Rafig Almaghairbe', 'Marc Roper', 'Tahani Almabruk']\",\n",
       "  'date': 'September 2020',\n",
       "  'source': \"ICEMIS'20: Proceedings of the 6th International Conference on Engineering &amp; MIS 2020\",\n",
       "  'abstract': 'The biggest obstacle of automated software testing is the construction of test oracles. Today, it is possible to generate enormous amount of test cases for an arbitrary system that reach a remarkably high level of coverage, but the effectiveness of test cases is limited by the availability of test oracles that can distinguish failing executions. Previous work by the authors has explored the use of unsupervised and semi-supervised learning techniques to develop test oracles so that the correctness of software outputs and behaviours on new test cases can be predicated [1], [2], [10], and experimental results demonstrate the promise of this approach. In this paper, we present an evaluation study for test oracles based on machine-learning approaches via dynamic execution data (firstly, input/output pairs and secondly, amalgamations of input/output pairs and execution traces) by comparing their effectiveness with existing techniques from the specification mining domain (the data invariant detector Daikon [5]). The two approaches are evaluated on a range of mid-sized systems and compared in terms of their fault detection ability and false positive rate. The empirical study also discuss the major limitations and the most important properties related to the application of machine learning techniques as test oracles in practice. The study also gives a road map for further research direction in order to tackle some of discussed limitations such as accuracy and scalability. The results show that in most cases semi-supervised learning techniques performed far better as an automated test classifier than Daikon (especially in the case that input/output pairs were augmented with their execution traces). However, there is one system for which our strategy struggles and Daikon performed far better. Furthermore, unsupervised learning techniques performed on a par when compared with Daikon in several cases particularly when input/output pairs were used together with execution traces.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3410352.3410747',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Active Learning for Spam Email Classification',\n",
       "  'authors': \"['Zheng Chen', 'Ruiwen Tao', 'Xiaoyang Wu', 'Zhimin Wei', 'Xiao Luo']\",\n",
       "  'date': 'December 2019',\n",
       "  'source': \"ACAI '19: Proceedings of the 2019 2nd International Conference on Algorithms, Computing and Artificial Intelligence\",\n",
       "  'abstract': 'Deep learning has yielded state-of-the-art performance on text classification tasks. In this paper, a new neural network based on Long-Short-Term-Memory model is applied to classify spam emails. Using deep learning method to classify spam emails requires large amounts of labeled data. To solve this problem, active learning method is used to reduce labeling cost and increase model adaptability. In this paper, it is found that the new model performs better than standard CNNs and RNNs on email classification task, and active learning methods can match state-of-the-art performance with just 10% of the labeled data.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3377713.3377789',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Mining online learner profile through learning behavior analysis',\n",
       "  'authors': \"['Bing Wu', 'Jun Xiao']\",\n",
       "  'date': 'October 2018',\n",
       "  'source': \"ICETC '18: Proceedings of the 10th International Conference on Education Technology and Computers\",\n",
       "  'abstract': \"User profile is an effective model to describe the user's interests and preferences. In the learning field, learner profile should meet the demand of learning and teaching such as learning patterns recognition or performance prediction. Analysis of user's behaviors is the common way to build user profile. Statistics show that online learning activities are incontinuous and diverse. By taking a closer look at the learning activity data, we found back accessing behavior is a frequent activity and reveals the truth of learners' intention. In this study, we make use of Shanghai Open University's learning platform as the data source for our research, adopt machine learning method to find the hidden patterns of learning activities and build the online learner profile. Statistics show that 15.68% of the accessing activities are back accessing. We found three learning patterns with different amount of back accessing behaviors and learning paths. Meanwhile, they relate to many factors including demographics, major type and area where learners join in learning. Through learner profile, we can predict learner's learning pattern which we found in this study. In the conclusion of our study, we suggest that learning path should be taken into consideration of learning engagement.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3290511.3290560',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Pile Foundation Detection Data Analysis and Classification Method',\n",
       "  'authors': \"['Luo Zhong', 'Bingqing Wu', 'Ruiqi Luo', 'Shujun Zhang', 'Zhaoyu Dong', 'Ye Lu']\",\n",
       "  'date': 'April 2019',\n",
       "  'source': 'ICDMML 2019: Proceedings of the 2019 International Conference on Data Mining and Machine Learning',\n",
       "  'abstract': 'In this paper, by combing the collected testing data of pile bearing capacity from 78 reinforced concrete cast-in-place bored piles. The distribution characteristics of the pile bearing capacity are analyzed in detail. Based on this, the n-σ criteria are introduced and a more practical data processing method for bearing capacity of foundation piles is proposed. Using this method, the data of pile bearing capacity detected was analyzed and processed. Then the data was divided into \"strong data\", \"good data\" and \"weak data\". In addition, we verified the validity of this method to determine the detected data quality of pile bearing capacity through engineering examples. The verification shows that the data quality has a significant influence on the calculation results of the reliability index and the resistance coefficient.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3335656.3335678',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Big data execution time based on Spark Machine Learning Libraries',\n",
       "  'authors': \"['Anna Karen Gárate-Escamilla', 'Amir Hajjam El Hassani', 'Emmanuel Andres']\",\n",
       "  'date': 'August 2019',\n",
       "  'source': \"ICCBDC '19: Proceedings of the 2019 3rd International Conference on Cloud and Big Data Computing\",\n",
       "  'abstract': 'The paper focuses on exploring the time consumption of supervised and unsupervised models of Apache Spark framework in massive datasets. Big Data analytics has been relevant in the industry due to the need to convert information into knowledge. Among the challenge of big data is the creation of strategies to improve the execution costs of running machine learning models to make a prediction. Apache Spark is a powerful in-memory platform that offers an extensive machine learning library for regression, classification, clustering, and rule extraction. This investigation, from a computation cost perspective, performs different experiments using real datasets. The main contribution of the paper is to compare the execution time of different machine learning models, such as random forests, decision tree, logistic regression, linear support vector machine, and kNN. The present work expects to combine the areas of big data and machine learning, comparing the results with different configurations and the use of the optimization methods, cache and persist. The evaluation experiments show that logistic regression performed the shortest execution time of the Spark MLlib models.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3358505.3358519',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'An automated ensemble learning framework using genetic programming for image classification',\n",
       "  'authors': \"['Ying Bi', 'Bing Xue', 'Mengjie Zhang']\",\n",
       "  'date': 'July 2019',\n",
       "  'source': \"GECCO '19: Proceedings of the Genetic and Evolutionary Computation Conference\",\n",
       "  'abstract': 'An ensemble consists of multiple learners and can achieve a better generalisation performance than a single learner. Genetic programming (GP) has been applied to construct ensembles using different strategies such as bagging and boosting. However, no GP-based ensemble methods focus on dealing with image classification, which is a challenging task in computer vision and machine learning. This paper proposes an automated ensemble learning framework using GP (EGP) for image classification. The new method integrates feature learning, classification function selection, classifier training, and combination into a single program tree. To achieve this, a novel program structure, a new function set and a new terminal set are developed in EGP. The performance of EGP is examined on nine different image classification data sets of varying difficulty and compared with a large number of commonly used methods including recently published methods. The results demonstrate that EGP achieves better performance than most competitive methods. Further analysis reveals that EGP evolves good ensembles simultaneously balancing diversity and accuracy. To the best of our knowledge, this study is the first work using GP to automatically generate ensembles for image classification.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3321707.3321750',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': '(1+ε)-class classification: an anomaly detection method for highly imbalanced or incomplete data sets',\n",
       "  'authors': \"['Maxim Borisyak', 'Artem Ryzhikov', 'Andrey Ustyuzhanin', 'Denis Derkach', 'Fedor Ratnikov', 'Olga Mineeva']\",\n",
       "  'date': 'None',\n",
       "  'source': 'The Journal of Machine Learning Research',\n",
       "  'abstract': 'Anomaly detection is not an easy problem since distribution of anomalous samples is unknown a priori. We explore a novel method that gives a trade-off possibility between oneclass and two-class approaches, and leads to a better performance on anomaly detection problems with small or non-representative anomalous samples. The method is evaluated using several data sets and compared to a set of conventional one-class and two-class approaches.',\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3455716.3455788',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Graph-based Semi-supervised Learning for Text Classification',\n",
       "  'authors': \"['Natalie Widmann', 'Suzan Verberne']\",\n",
       "  'date': 'October 2017',\n",
       "  'source': \"ICTIR '17: Proceedings of the ACM SIGIR International Conference on Theory of Information Retrieval\",\n",
       "  'abstract': 'In this paper, we propose a graph-based representation of document collections in which both documents and features are represented by nodes. The nodes are connected with weights based on word order, context similarity and word frequency. Graph-based representations can overcome the limitations of bag-of-words based representations that suffer from sparseness for collections with short documents. In a series of experiments, we evaluate multiple types of graph-based text features in the context of semi-supervised text classification, and investigate the effect of the number of labeled documents in the collection. We find that graph-based semi-supervised learning outperforms bag-of-words semi-supervised learning but not bag-of-words supervised learning in 20-class text categorization. A large asset of graph-based representations is that they are flexible in the types of nodes and relations that are included.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3121050.3121055',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'The Efficient Mining of Skyline Patterns from a Volunteer Computing Network',\n",
       "  'authors': \"['Jimmy Ming-Tai Wu', 'Qian Teng', 'Gautam Srivastava', 'Matin Pirouz', 'Jerry Chun-Wei Lin']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Internet Technology',\n",
       "  'abstract': 'In the ever-growing world, the concepts of High-utility Itemset Mining (HUIM) as well as Frequent Itemset Mining (FIM) are fundamental works in knowledge discovery. Several algorithms have been designed successfully. However, these algorithms only used one factor to estimate an itemset. In the past, skyline pattern mining by considering both aspects of frequency and utility has been extensively discussed. In most cases, however, people tend to focus on purchase quantities of itemsets rather than frequencies. In this article, we propose a new knowledge called skyline quantity-utility pattern (SQUP) to provide better estimations in the decision-making process by considering quantity and utility together. Two algorithms, respectively, called SQU-Miner and SKYQUP are presented to efficiently mine the set of SQUPs. Moreover, the usage of volunteer computing is proposed to show the potential in real supermarket applications. Two new efficient utility-max structures are also mentioned for the reduction of the candidate itemsets, respectively, utilized in SQU-Miner and SKYQUP. These two new utility-max structures are used to store the upper-bound of utility for itemsets under the quantity constraint instead of frequency constraint, and the second proposed utility-max structure moreover applies a recursive updated process to further obtain strict upper-bound of utility. Our in-depth experimental results prove that SKYQUP has stronger performance when a comparison is made to SQU-Miner in terms of memory usage, runtime, and the number of candidates.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3423557',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Multi-Relational Classification via Bayesian Ranked Non-Linear Embeddings',\n",
       "  'authors': \"['Ahmed Rashed', 'Josif Grabocka', 'Lars Schmidt-Thieme']\",\n",
       "  'date': 'July 2019',\n",
       "  'source': \"KDD '19: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining\",\n",
       "  'abstract': \"The task of classifying multi-relational data spans a wide range of domains such as document classification in citation networks, classification of emails, and protein labeling in proteins interaction graphs. Current state-of-the-art classification models rely on learning per-entity latent representations by mining the whole structure of the relations' graph, however, they still face two major problems. Firstly, it is very challenging to generate expressive latent representations in sparse multi-relational settings with implicit feedback relations as there is very little information per-entity. Secondly, for entities with structured properties such as titles and abstracts (text) in documents, models have to be modified ad-hoc. In this paper, we aim to overcome these two main drawbacks by proposing a flexible nonlinear latent embedding model (BRNLE) for the classification of multi-relational data. The proposed model can be applied to entities with structured properties such as text by utilizing the numerical vector representations of those properties. To address the sparsity problem of implicit feedback relations, the model is optimized via a sparsely-regularized multi-relational pair-wise Bayesian personalized ranking loss (BPR). Experiments on four different real-world datasets show that the proposed model significantly outperforms state-of-the-art models for multi-relational classification.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3292500.3330863',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Automatic Feature Learning for MOOC Forum Thread Classification',\n",
       "  'authors': \"['Lin Feng', 'Huimin Lu', 'Shenglan Liu', 'Guochao Liu', 'Sen Luo']\",\n",
       "  'date': 'April 2018',\n",
       "  'source': \"ICBDC '18: Proceedings of the 3rd International Conference on Big Data and Computing\",\n",
       "  'abstract': 'Discussion thread classification plays an important role for Massive Open Online Courses (MOOCs) forum. Most existing methods in this filed focus on extracting text features (e.g. key words) from the content of discussions using NLP methods. However, diversity of languages used in MOOC forums results in poor expansibility of these methods. To tackle this problem, in this paper, we artificially design 23 language independent features related to structure, popularity and underlying social network of thread. Furthermore, a hybrid model which combine Gradient Boosting Decision Tree (GBDT) with Linear Regression (LR) (GBDT + LR) is employed to reduce the traditional cost of feature learning for discussion threads classification manually. Experiments are carried out on the datasets contributed by Coursera with nearly 100, 000 discussion threads of 60 courses taught in 4 different languages. Results demonstrate that our method can significantly improve the performance of discussion threads classification. It is worth drawing that the average AUC of our model is 0.832, outperforming baseline by 15%.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3220199.3220201',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'MiningBreastCancer: Selection of Candidate Gene Associated with Breast Cancer via Comparison between Data Mining of TCGA and Text Mining of PubMed',\n",
       "  'authors': \"['Chou-Cheng Chen', 'Yao-Lung Kuo', 'Chi-Hui Chiang']\",\n",
       "  'date': 'December 2020',\n",
       "  'source': \"ACM ICEA '20: Proceedings of the 2020 ACM International Conference on Intelligent Computing and its Emerging Applications\",\n",
       "  'abstract': 'In 2016, 12,676 new cases of breast cancer were diagnosed among Taiwan women. In 2018 the standardized death rate of breast cancer was 12.5 per 100,000 persons. Previous studies have integrated data and text mining to yield fusion genes, identify genetic factors for breast cancer and select single-gene feature sets for colon cancer discrimination. However, our study is the first to select significantly different expression between breast normal tissue and cancer using TCGA data and biostatistics, excluding know genes using abstracts from PubMed and natural language processing. The top twenty genes for research potential from the selection of Mining-BreastCancer are EML3, ABCB9, GRASP, KANK3, GPR146, ZNF623, CCDC9, ADCY4, DLL1, ADAM33, GRRP1, LRRN4CL, C14orf180, ABCD4, ABCC6P1, PEAR1, FAM43A, C20orf160, KIF21A and PP-FIA3. Few studies for these genes exist, but they hold significantly different expressions between breast cancer and normal tissue, each pathologic tumor and lymph node, or between each pathologic metastasis. These results show that MiningBreastCancer can help scientists select genes for research potential. MiningBreastCancer is available through http://bio.yungyun.com.tw/MiningBreastCancer.aspx.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3440943.3444718',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'MiningBreastCancer: Selection of Candidate Gene Associated with Breast Cancer via Comparison between Data Mining of TCGA and Text Mining of PubMed',\n",
       "  'authors': \"['Chou-Cheng Chen', 'Yao-Lung Kuo', 'Chi-Hui Chiang']\",\n",
       "  'date': 'December 2020',\n",
       "  'source': \"ACM ICEA '20: Proceedings of the 2020 ACM International Conference on Intelligent Computing and its Emerging Applications\",\n",
       "  'abstract': 'In 2016, 12,676 new cases of breast cancer were diagnosed among Taiwan women. In 2018 the standardized death rate of breast cancer was 12.5 per 100,000 persons. Previous studies have integrated data and text mining to yield fusion genes, identify genetic factors for breast cancer and select single-gene feature sets for colon cancer discrimination. However, our study is the first to select significantly different expression between breast normal tissue and cancer using TCGA data and biostatistics, excluding know genes using abstracts from PubMed and natural language processing. The top twenty genes for research potential from the selection of Mining-BreastCancer are EML3, ABCB9, GRASP, KANK3, GPR146, ZNF623, CCDC9, ADCY4, DLL1, ADAM33, GRRP1, LRRN4CL, C14orf180, ABCD4, ABCC6P1, PEAR1, FAM43A, C20orf160, KIF21A and PP-FIA3. Few studies for these genes exist, but they hold significantly different expressions between breast cancer and normal tissue, each pathologic tumor and lymph node, or between each pathologic metastasis. These results show that MiningBreastCancer can help scientists select genes for research potential. MiningBreastCancer is available through http://bio.yungyun.com.tw/MiningBreastCancer.aspx.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3440943.3444718',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Actor-based incremental tree data processing for large-scale machine learning applications',\n",
       "  'authors': \"['Kouhei Sakurai', 'Taiki Shimizu']\",\n",
       "  'date': 'October 2019',\n",
       "  'source': 'AGERE 2019: Proceedings of the 9th ACM SIGPLAN International Workshop on Programming Based on Actors, Agents, and Decentralized Control',\n",
       "  'abstract': \"A number of online machine learning techniques based on tree model have been studied in order to cope with today's requirements of quickly processing large scale data-sets. We present a design pattern for incremental tree data processing as gradually constructing on-demand tree-model on memory. Our approach adopts the actor model as making use of multi-cores and distributed computers without largely rewriting code for algorithms. The pattern basically defines a node in the tree as an actor which is the unit of asynchronous processes and each data instance flows between actor nodes as a message. We study concrete two machine learning algorithms, VFDT for decision tree's top-down growth and BIRCH for hierarchical clustering's bottom up growth. For supporting VFDT, we propose an extension mechanism of replicating root nodes so that it can address bottleneck as starting of inputs. For supporting BIRCH, we split processes of recursive construction into asynchronous steps with correcting target node by traversing extra horizontal links between sibling nodes. We carried out machine learning tasks with our implementation on top of Akka Java, and we confirmed reasonable performance for the tasks with large scale data-sets.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3358499.3361220',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Dynamically Adjusting Diversity in Ensembles for the Classification of Data Streams with Concept Drift',\n",
       "  'authors': \"['Juan I. G. Hidalgo', 'Silas G. T. C. Santos', 'Roberto S. M. Barros']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Knowledge Discovery from Data',\n",
       "  'abstract': 'A data stream can be defined as a system that continually generates a lot of data over time. Today, processing data streams requires new demands and challenging tasks in the data mining and machine learning areas. Concept Drift is a problem commonly characterized as changes in the distribution of the data within a data stream. The implementation of new methods for dealing with data streams where concept drifts occur requires algorithms that can adapt to several scenarios to improve its performance in the different experimental situations where they are tested. This research proposes a strategy for dynamic parameter adjustment in the presence of concept drifts. Parameter Estimation Procedure (PEP) is a general method proposed for dynamically adjusting parameters which is applied to the diversity parameter (λ) of several classification ensembles commonly used in the area. To this end, the proposed estimation method (PEP) was used to create Boosting-like Online Learning Ensemble with Parameter Estimation (BOLE-PE), Online AdaBoost-based M1 with Parameter Estimation (OABM1-PE), and Oza and Russell’s Online Bagging with Parameter Estimation (OzaBag-PE), based on the existing ensembles BOLE, OABM1, and OzaBag, respectively. To validate them, experiments were performed with artificial and real-world datasets using Hoeffding Tree (HT) as base classifier. The accuracy results were statistically evaluated using a variation of the Friedman test and the Nemenyi post-hoc test. The experimental results showed that the application of the dynamic estimation in the diversity parameter (λ) produced good results in most scenarios, i.e.,\\xa0the modified methods have improved accuracy in the experiments with both artificial and real-world datasets.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3466616',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Frontmatter: mining Android user interfaces at scale',\n",
       "  'authors': \"['Konstantin Kuznetsov', 'Chen Fu', 'Song Gao', 'David N. Jansen', 'Lijun Zhang', 'Andreas Zeller']\",\n",
       "  'date': 'August 2021',\n",
       "  'source': 'ESEC/FSE 2021: Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering',\n",
       "  'abstract': 'We introduce Frontmatter: the largest open-access dataset containing user interface models of about 160,000 Android apps. Frontmatter opens the door for comprehensive mining of mobile user interfaces, jumpstarting empirical research at a large scale, addressing questions such as \"How many travel apps require registration?\", \"Which apps do not follow accessibility guidelines?\", \"Does the user interface correspond to the description?\", and many more. The Frontmatter UI analysis tool and the Frontmatter dataset are available under an open-source license.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3468264.3473125',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Adaptive-Step Graph Meta-Learner for Few-Shot Graph Classification',\n",
       "  'authors': \"['Ning Ma', 'Jiajun Bu', 'Jieyu Yang', 'Zhen Zhang', 'Chengwei Yao', 'Zhi Yu', 'Sheng Zhou', 'Xifeng Yan']\",\n",
       "  'date': 'October 2020',\n",
       "  'source': \"CIKM '20: Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management\",\n",
       "  'abstract': \"Graph classification aims to extract accurate information from graph-structured data for classification and is becoming more and more important in the graph learning community. Although Graph Neural Networks (GNNs) have been successfully applied to graph classification tasks, most of them overlook the scarcity of labeled graph data in many applications. For example, in bioinformatics, obtaining protein graph labels usually needs laborious experiments. Recently, few-shot learning has been explored to alleviate this problem with only a few labeled graph samples of test classes. The shared sub-structures between training classes and test classes are essential in the few-shot graph classification. Existing methods assume that the test classes belong to the same set of super-classes clustered from training classes. However, according to our observations, the label spaces of training classes and test classes usually do not overlap in a real-world scenario. As a result, the existing methods don't well capture the local structures of unseen test classes. To overcome the limitation, in this paper, we propose a direct method to capture the sub-structures with a well initialized meta-learner within a few adaptation steps. More specifically, (1) we propose a novel framework consisting of a graph meta-learner, which uses GNNs based modules for fast adaptation on graph data, and a step controller for the robustness and generalization of meta-learner; (2) we provide quantitative analysis for the framework and give a graph-dependent upper bound of the generalization error based on our framework; (3) the extensive experiments on real-world datasets demonstrate that our framework gets state-of-the-art results on several few-shot graph classification tasks compared to baselines.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3340531.3411951',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Probabilistic Feature Selection and Classification Vector Machine',\n",
       "  'authors': \"['Bingbing Jiang', 'Chang Li', 'Maarten De Rijke', 'Xin Yao', 'Huanhuan Chen']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Knowledge Discovery from Data',\n",
       "  'abstract': 'Sparse Bayesian learning is a state-of-the-art supervised learning algorithm that can choose a subset of relevant samples from the input data and make reliable probabilistic predictions. However, in the presence of high-dimensional data with irrelevant features, traditional sparse Bayesian classifiers suffer from performance degradation and low efficiency due to the incapability of eliminating irrelevant features. To tackle this problem, we propose a novel sparse Bayesian embedded feature selection algorithm that adopts truncated Gaussian distributions as both sample and feature priors. The proposed algorithm, called probabilistic feature selection and classification vector machine (PFCVMLP) is able to simultaneously select relevant features and samples for classification tasks. In order to derive the analytical solutions, Laplace approximation is applied to compute approximate posteriors and marginal likelihoods. Finally, parameters and hyperparameters are optimized by the type-II maximum likelihood method. Experiments on three datasets validate the performance of PFCVMLP along two dimensions: classification performance and effectiveness for feature selection. Finally, we analyze the generalization performance and derive a generalization error bound for PFCVMLP. By tightening the bound, the importance of feature selection is demonstrated.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3309541',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Scalable Machine Learning on High-Dimensional Vectors: From Data Series to Deep Network Embeddings',\n",
       "  'authors': \"['Karima Echihabi', 'Kostas Zoumpatianos', 'Themis Palpanas']\",\n",
       "  'date': 'June 2020',\n",
       "  'source': 'WIMS 2020: Proceedings of the 10th International Conference on Web Intelligence, Mining and Semantics',\n",
       "  'abstract': 'There is an increasingly pressing need, by several applications in diverse domains, for developing techniques able to analyze very large collections of static and streaming sequences (a.k.a. data series), predominantly in real-time. Examples of such applications come from Internet of Things installations, neuroscience, astrophysics, and a multitude of other scientific and application domains that need to apply machine learning techniques for knowledge extraction. It is not unusual for these applications, for which similarity search is a core operation, to involve numbers of data series in the order of hundreds of millions to billions, which are seldom analyzed in their full detail due to their sheer size. Such application requirements have driven the development of novel similarity search methods that can facilitate scalable analytics in this context. At the same time, a host of other methods have been developed for similarity search of high-dimensional vectors in general. All these methods are now becoming increasingly important, because of the growing popularity and size of sequence collections, as well as the growing use of high-dimensional vector representations of a large variety of objects (such as text, multimedia, images, audio and video recordings, graphs, database tables, and others) thanks to deep network embeddings. In this work, we review recent efforts in designing techniques for indexing and analyzing massive collections of data series, and argue that they are the methods of choice even for general high-dimensional vectors. Finally, we discuss the challenges and open research problems in this area.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3405962.3405989',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A Lightweight Deep Learning Approach to Mosquito Classification from Wingbeat Sounds',\n",
       "  'authors': \"['Myat Su Yin', 'Peter Haddawy', 'Borvorntat Nirandmongkol', 'Tup Kongthaworn', 'Chanaporn Chaisumritchoke', 'Akara Supratak', 'Chaitawat Sa-ngamuang', 'Patchara Sriwichai']\",\n",
       "  'date': 'September 2021',\n",
       "  'source': \"GoodIT '21: Proceedings of the Conference on Information Technology for Social Good\",\n",
       "  'abstract': 'Diseases transmitted by mosquito vectors such as malaria, dengue, and Zika virus are amongst the largest healthcare concerns across the globe today. To tackle such life-threatening diseases, it is vital to evaluate the risk of transmission. Of critical importance in this task is the estimation of vector species populations in an area of interest. Traditional approaches to estimating vector populations involve physically collecting vector samples in traps and manually classifying species, which is highly labor intensive. A promising alternative approach is to classify mosquito species based on the audio signal from their wingbeats. Various traditional machine learning and deep learning models have been developed for such automated acoustic mosquito species classification. But they require data preprocessing and significant computation, limiting their suitability to be deployed on low-cost sensor devices. This paper presents two lightweight deep learning models for mosquito species and sex classification from wingbeat audio signals which are suitable to be deployed on small IoT sensor devices. One model is a 1D CNN and the other combines the 1D CNN with an LSTM model. The models operate directly on a low-sample-rate raw audio signal and thus require no signal preprocessing. Both models achieve a classification accuracy of over 93% on a dataset of recordings of males and females of five species. In addition, we explore the relation between model size and classification accuracy. Through model tuning, we are able to reduce the sizes of both models by approx. 60% while losing only 3% in classification accuracy.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3462203.3475908',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Efficient Document Filtering Using Vector Space Topic Expansion and Pattern-Mining: The Case of Event Detection in Microposts',\n",
       "  'authors': \"['Julia Proskurnia', 'Ruslan Mavlyutov', 'Carlos Castillo', 'Karl Aberer', 'Philippe Cudré-Mauroux']\",\n",
       "  'date': 'November 2017',\n",
       "  'source': \"CIKM '17: Proceedings of the 2017 ACM on Conference on Information and Knowledge Management\",\n",
       "  'abstract': \"Automatically extracting information from social media is challenging given that social content is often noisy, ambiguous, and inconsistent. However, as many stories break on social channels first before being picked up by mainstream media, developing methods to better handle social content is of utmost importance. In this paper, we propose a robust and effective approach to automatically identify microposts related to a specific topic defined by a small sample of reference documents. Our framework extracts clusters of semantically similar microposts that overlap with the reference documents, by extracting combinations of key features that define those clusters through frequent pattern mining. This allows us to construct compact and interpretable representations of the topic, dramatically decreasing the computational burden compared to classical clustering and k-NN-based machine learning techniques and producing highly-competitive results even with small training sets (less than 1'000 training objects). Our method is efficient and scales gracefully with large sets of incoming microposts. We experimentally validate our approach on a large corpus of over 60M microposts, showing that it significantly outperforms state-of-the-art techniques.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3132847.3133016',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Mining software defects: should we consider affected releases?',\n",
       "  'authors': \"['Suraj Yatish', 'Jirayus Jiarpakdee', 'Patanamon Thongtanunam', 'Chakkrit Tantithamthavorn']\",\n",
       "  'date': 'May 2019',\n",
       "  'source': \"ICSE '19: Proceedings of the 41st International Conference on Software Engineering\",\n",
       "  'abstract': 'With the rise of the Mining Software Repositories (MSR) field, defect datasets extracted from software repositories play a foundational role in many empirical studies related to software quality. At the core of defect data preparation is the identification of post-release defects. Prior studies leverage many heuristics (e.g., keywords and issue IDs) to identify post-release defects. However, such the heuristic approach is based on several assumptions, which pose common threats to the validity of many studies. In this paper, we set out to investigate the nature of the difference of defect datasets generated by the heuristic approach and the realistic approach that leverages the earliest affected release that is realistically estimated by a software development team for a given defect. In addition, we investigate the impact of defect identification approaches on the predictive accuracy and the ranking of defective modules that are produced by defect models. Through a case study of defect datasets of 32 releases, we find that that the heuristic approach has a large impact on both defect count datasets and binary defect datasets. Surprisingly, we find that the heuristic approach has a minimal impact on defect count models, suggesting that future work should not be too concerned about defect count models that are constructed using heuristic defect datasets. On the other hand, using defect datasets generated by the realistic approach lead to an improvement in the predictive accuracy of defect classification models.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ICSE.2019.00075',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Laconic Image Classification: Human vs. Machine Performance',\n",
       "  'authors': \"['Javier Carrasco', 'Aidan Hogan', 'Jorge Pérez']\",\n",
       "  'date': 'October 2020',\n",
       "  'source': \"CIKM '20: Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management\",\n",
       "  'abstract': 'We propose laconic classification as a novel way to understand and compare the performance of diverse image classifiers. The goal in this setting is to minimise the amount of information (aka. entropy) required in individual test images to maintain correct classification. Given a classifier and a test image, we compute an approximate minimal-entropy positive image for which the classifier provides a correct classification, becoming incorrect upon any further reduction. The notion of entropy offers a unifying metric that allows to combine and compare the effects of various types of reductions (e.g., crop, colour reduction, resolution reduction) on classification performance, in turn generalising similar methods explored in previous works. Proposing two complementary frameworks for computing the minimal-entropy positive images of both human and machine classifiers, in experiments over the ILSVRC test-set, we find that machine classifiers are more sensitive entropy-wise to reduced resolution (versus cropping or reduced colour for machines, as well as reduced resolution for humans), supporting recent results suggesting a texture bias in the ILSVRC-trained models used. We also find, in the evaluated setting, that humans classify the minimal-entropy positive images of machine models with higher precision than machines classify those of humans.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3340531.3411984',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Clustering of Functionally Related Genes Using Machine Learning Techniques',\n",
       "  'authors': \"['Yujing Xue', 'Lang Cao']\",\n",
       "  'date': 'February 2021',\n",
       "  'source': \"ICCDA '21: Proceedings of the 2021 5th International Conference on Compute and Data Analysis\",\n",
       "  'abstract': 'The clustering of functionally related genes has been an important task for biologists. With the recent progress of machine learning technology, researchers now have more powerful weapons to identify the structures within a large amount of DNA sequencing data. That allows the research on genes to be conducted in an efficient and scalable way. This paper studies the clustering of functionally related genes and their impact on the development and prognosis of lung cancer with machine learning technologies. The patient data derived from 218 patients are analyzed. We focus on two extreme cases, one case includes patients who survived less than 1 year, and the other case includes patients who survived longer than 5 years. We will investigate how different clustering methods can assist in the visualization of the DNA sequence data of such patients, and how such methods can help us identify the underlying patterns of the DNA sequence data.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3456529.3456538',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': \"Aspect-Based Opinion Mining of Students' Reviews on Online Courses\",\n",
       "  'authors': \"['Zenun Kastrati', 'Blend Arifaj', 'Arianit Lubishtani', 'Fitim Gashi', 'Engjëll Nishliu']\",\n",
       "  'date': 'April 2020',\n",
       "  'source': \"ICCAI '20: Proceedings of the 2020 6th International Conference on Computing and Artificial Intelligence\",\n",
       "  'abstract': \"It is critical for higher education institutions to work on improvement of their teaching and learning strategy by examining feedback of students. Analyzing these feedbacks typically requires manual interventions which are not only labor intensive but prone to errors as well. Therefore, automatic models and techniques are needed to handle textual feedback efficiently. To this end, we propose a model for aspect-based opinion mining of comments of students that are posted in online learning platforms. The model aims to predict some of the key aspects related to an online course from students' reviews and then assess the attitude of students toward these commented aspects. The proposed model is tested on a large-scale real-world dataset which is collected for this purpose. The dataset consists of more than 21 thousand manually annotated students' reviews that are collected from Coursera. Conventional machine learning algorithms and deep learning techniques are used for prediction of the aspect categories and the aspect sentiment classification as well. The obtained results with respect to precision, recall, and F1 score are very promising.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3404555.3404633',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Mining Bad Credit Card Accounts from OLAP and OLTP',\n",
       "  'authors': \"['Sheikh Rabiul Islam', 'William Eberle', 'Sheikh Khaled Ghafoor']\",\n",
       "  'date': 'May 2017',\n",
       "  'source': \"ICCDA '17: Proceedings of the International Conference on Compute and Data Analysis\",\n",
       "  'abstract': 'Credit card companies classify accounts as a good or bad based on historical data where a bad account may default on payments in the near future. If an account is classified as a bad account, then further action can be taken to investigate the actual nature of the account and take preventive actions. In addition, marking an account as \"good\" when it is actually bad, could lead to loss of revenue - and marking an account as \"bad\" when it is actually good, could lead to loss of business. However, detecting bad credit card accounts in real time from Online Transaction Processing (OLTP) data is challenging due to the volume of data needed to be processed to compute the risk factor. We propose an approach which precomputes and maintains the risk probability of an account based on historical transactions data from offline data or data from a data warehouse. Furthermore, using the most recent OLTP transactional data, risk probability is calculated for the latest transaction and combined with the previously computed risk probability from the data warehouse. If accumulated risk probability crosses a predefined threshold, then the account is treated as a bad account and is flagged for manual verification.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3093241.3093279',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Graph Embeddings for Linked Data Clustering',\n",
       "  'authors': \"['Siham Eddamiri', 'Elmoukhtar Zemmouri', 'Asmaa Benghabrit']\",\n",
       "  'date': 'November 2018',\n",
       "  'source': 'iiWAS2018: Proceedings of the 20th International Conference on Information Integration and Web-based Applications &amp; Services',\n",
       "  'abstract': 'The availability and accessibility of large RDF data in the Linked Open Data cloud encourage the machine learning community to develop approaches and techniques to extract useful knowledge from such type of data. Moreover, Data Clustering is identified as a crucial task for many web-based applications. In this paper, we present an approach that uses neural language models for RDF data clustering. We, first, generate sequences of entities extracted from several graph substructures using Doc2vec and Word2vec combined with TF-IDF. Then we apply K-Means to cluster generated vectors. Our experiments on real datasets show good results when applying TF-IDF with Doc2vec for vector representation of RDF data.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3282373.3282401',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'The Classification of Spoiler Comments in Thai Discussion Forums',\n",
       "  'authors': \"['Rangsipan Marukatat']\",\n",
       "  'date': 'December 2020',\n",
       "  'source': \"NLPIR '20: Proceedings of the 4th International Conference on Natural Language Processing and Information Retrieval\",\n",
       "  'abstract': 'Classifying comments about movies into spoiler or non-spoiler is a challenging application of text mining. This research proposes using intrinsic and extrinsic attributes for the classification task. Intrinsic attributes consist of words in a bag-of-words model and spoiler cues extracted from the comments, whereas extrinsic ones are genre information gathered from external sources. First, a few methods to select predictive words from the bag of words were compared. Ensemble attribute selector was found to achieve the best results. Then, the classification was done by support vector machine (SVM) and Naïve Bayes. The accuracies on unseen data were around 85--89% when using only bags of selected words, and close to 91% when using all the proposed attributes.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3443279.3443298',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Review highlights: opinion mining on reviews: a hybrid model for rule selection in aspect extraction',\n",
       "  'authors': \"['Amit Kushwaha', 'Shubham Chaudhary']\",\n",
       "  'date': 'October 2017',\n",
       "  'source': \"IML '17: Proceedings of the 1st International Conference on Internet of Things and Machine Learning\",\n",
       "  'abstract': 'This paper proposes a methodology to extract key insights from user generated reviews. This work is based on Aspect Based Sentiment Analysis (ABSA) which predicts the sentiment of aspects mentioned in the text documents. The extracted aspects are fine-grained for the presentation form known as Review Highlights. The syntactic approach for extraction process suffers from the overlapping chunking rules which result in noise extraction. We introduce a hybrid technique which combines machine learning and rule based model. A multi-label classifier identifies the effective rules which efficiently parse aspects and opinions from texts. This selection of rules reduce the amount of noise in extraction tasks. This is a novel attempt to learn syntactic rule fitness from a corpus using machine learning for accurate aspect extraction. As the model learns the syntactic rule prediction from the corpus, it makes the extraction method domain independent. It also allows studying the quality of syntactic rules in a different corpus.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3109761.3158385',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Mining Twitter messages for software evolution',\n",
       "  'authors': \"['Emitza Guzman', 'Mohamed Ibrahim', 'Martin Glinz']\",\n",
       "  'date': 'May 2017',\n",
       "  'source': \"ICSE-C '17: Proceedings of the 39th International Conference on Software Engineering Companion\",\n",
       "  'abstract': \"Twitter is a widely used social network. Previous research showed that users engage in Twitter to communicate about software applications via short messages, referred to as tweets, and that some of these tweets are relevant for software evolution. However, a manual analysis is impractical due to the large number of tweets - in the range of thousands per day for popular apps. In this work we present ALERTme, an approach to automatically classify, group and rank tweets about software applications. We apply machine learning techniques for automatically classifying tweets requesting improvements, topic modeling for grouping semantically related tweets and a weighted function for ranking tweets according to their relevance for software evolution. We ran our approach on 68,108 tweets from three different software applications and compared the results against practitioners' assessments. Our results are promising and could help incorporate short, informal user feedback with social components into the software evolution process.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ICSE-C.2017.65',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Detection of Cyber-Aggressive Comments on Social Media Networks: A Machine Learning and Text mining approach',\n",
       "  'authors': \"['Risul Islam Rasel', 'Nasrin Sultana', 'Sharna Akhter', 'Phayung Meesad']\",\n",
       "  'date': 'September 2018',\n",
       "  'source': \"NLPIR '18: Proceedings of the 2nd International Conference on Natural Language Processing and Information Retrieval\",\n",
       "  'abstract': 'The spread of aggressive tweets, status and comments on social network are increasing gradually. People are using social media networks as a virtual platform to troll, objurgate, blaspheme and revile one another. These activities are spreading animosity in race-to-race, religion to religion etc. So, these comments should be identified and blocked on social networks. This work focuses on extracting comments from social networks and analyzes those comments whether they convey any blaspheme or revile in meaning. Comments are classified into three distinct classes; offensive, hate speech and neither. Document similarity analyses are done to identify the correlations among the documents. A well defined text pre-processing analysis is done to create an optimized word vector to train the classification model. Finally, the proposed model categorizes the comments into their respective classes with more than 93% accuracy.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3278293.3278303',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Enhancing classification performance of convolutional neural networks for prostate cancer detection on magnetic resonance images: a study with the semantic learning machine',\n",
       "  'authors': \"['Paulo Lapa', 'Ivo Gonçalves', 'Leonardo Rundo', 'Mauro Castelli']\",\n",
       "  'date': 'July 2019',\n",
       "  'source': \"GECCO '19: Proceedings of the Genetic and Evolutionary Computation Conference Companion\",\n",
       "  'abstract': 'Prostate cancer (PCa) is the most common oncological disease in Western men. Even though a significant effort has been carried out by the scientific community, accurate and reliable automated PCa detection methods are still a compelling issue. In this clinical scenario, high-resolution multiparametric Magnetic Resonance Imaging (MRI) is becoming the most used modality, also enabling quantitative studies. Recently, deep learning techniques have achieved outstanding results in prostate MRI analysis tasks, in particular with regard to image classification. This paper studies the feasibility of using the Semantic Learning Machine (SLM) neuroevolution algorithm to replace the fully-connected architecture commonly used in the last layers of Convolutional Neural Networks (CNNs). The experimental phase considered the PROSTATEx dataset composed of multispectral MRI sequences. The achieved results show that, on the same non-contrast-enhanced MRI series, SLM outperforms with statistical significance a state-of-the-art CNN trained with backpropagation. The SLM performance is achieved without pre-training the underlying CNN with backpropagation. Furthermore, on average the SLM training time is approximately 14 times faster than the backpropagation-based approach.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3319619.3322035',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Improving Multimodal Data Labeling with Deep Active Learning for Post Classification in Social Networks',\n",
       "  'authors': \"['Dmitry Krylov', 'Semen Poliakov', 'Natalia Khanzhina', 'Alexey Zabashta', 'Andrey Filchenkov', 'Aleksandr Farseev']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"MULL'21: Multimedia Understanding with Less Labeling on Multimedia Understanding with Less Labeling\",\n",
       "  'abstract': 'Automatic user post classification is an important task in the field of social network analysis. Being effectively solved, post classification could be used for thematic user feed composition or inappropriate content identification. Commonly addressed by applying various Machine Learning approaches, the task often involves manual processes related to ground truth sourcing, which is known to be a hardly-scalable and increasingly expensive procedure. At the same time, Active Learning for automatic user post classification is a promising way to bridge such a gap, as it does not require massive ground truth availability aligning our research with the real world settings. In this work, we put our focus on leveraging textual and visual data modalities for the application of user post classification and investigate how batch size and batch normalization disabling techniques could affect active deep neural network learning process. We solve the problem of automatic user post classification by employing our novel multimodal neural network architecture with multi-head tunable loss function components. We show that the proposed approach, coupled with Active Learning, allows for the achievement of a significant classification performance boost in terms of crowd assessing resources as compared to the passive learning approaches.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3476098.3485055',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Active Learning and Visual Analytics for Stance Classification with ALVA',\n",
       "  'authors': \"['Kostiantyn Kucher', 'Carita Paradis', 'Magnus Sahlgren', 'Andreas Kerren']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Interactive Intelligent Systems',\n",
       "  'abstract': 'The automatic detection and classification of stance (e.g., certainty or agreement) in text data using natural language processing and machine-learning methods creates an opportunity to gain insight into the speakers’ attitudes toward their own and other people’s utterances. However, identifying stance in text presents many challenges related to training data collection and classifier training. To facilitate the entire process of training a stance classifier, we propose a visual analytics approach, called ALVA, for text data annotation and visualization. ALVA’s interplay with the stance classifier follows an active learning strategy to select suitable candidate utterances for manual annotaion. Our approach supports annotation process management and provides the annotators with a clean user interface for labeling utterances with multiple stance categories. ALVA also contains a visualization method to help analysts of the annotation and training process gain a better understanding of the categories used by the annotators. The visualization uses a novel visual representation, called CatCombos, which groups individual annotation items by the combination of stance categories. Additionally, our system makes a visualization of a vector space model available that is itself based on utterances. ALVA is already being used by our domain experts in linguistics and computational linguistics to improve the understanding of stance phenomena and to build a stance classifier for applications such as social media monitoring.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3132169',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Opinion Mining for Software Development: A Systematic Literature Review',\n",
       "  'authors': \"['Bin Lin', 'Nathan Cassee', 'Alexander Serebrenik', 'Gabriele Bavota', 'Nicole Novielli', 'Michele Lanza']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Software Engineering and Methodology',\n",
       "  'abstract': 'Opinion mining, sometimes referred to as sentiment analysis, has gained increasing attention in software engineering (SE) studies. SE researchers have applied opinion mining techniques in various contexts, such as identifying developers’ emotions expressed in code comments and extracting users’ critics toward mobile apps. Given the large amount of relevant studies available, it can take considerable time for researchers and developers to figure out which approaches they can adopt in their own studies and what perils these approaches entail.We conducted a systematic literature review involving 185 papers. More specifically, we present (1) well-defined categories of opinion mining-related software development activities, (2) available opinion mining approaches, whether they are evaluated when adopted in other studies, and how their performance is compared, (3) available datasets for performance evaluation and tool customization, and (4) concerns or limitations SE researchers might need to take into account when applying/customizing these opinion mining techniques. The results of our study serve as references to choose suitable opinion mining tools for software development activities and provide critical insights for the further development of opinion mining techniques in the SE domain.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3490388',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A Comparative Study of Unsupervised Classification Algorithms in Multi-Sized Data Sets',\n",
       "  'authors': \"['Syed Quddus', 'Adil Bagirov']\",\n",
       "  'date': 'December 2019',\n",
       "  'source': \"AICCC '19: Proceedings of the 2019 2nd Artificial Intelligence and Cloud Computing Conference\",\n",
       "  'abstract': 'The ability to mine and extract useful information automatically, from large data sets, is a common concern for organizations, for the last few decades. Over the internet, data is vastly increasing gradually and consequently the capacity to collect and store very large data is significantly increasing. Existing clustering algorithms are not always efficient and accurate in solving clustering problems for large data sets. However, the development of accurate and fast data classification algorithms for very large scale data sets is still a challenge. In this paper, we present an overview of various algorithms and approaches which are recently being used for Clustering of large data and E-document. In this paper, a comparative study of the performance of various algorithms: the global kmeans algorithm (GKM), the multi-start modified global kmeans algorithm (MS-MGKM), the multi-start kmeans algorithm (MS-KM), the difference of convex clustering algorithm (DCA), the clustering algorithm based on the difference of convex representation of the cluster function and non-smooth optimization (DC-L2), is carried out using C++.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3375959.3375979',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Strong agile metrics: mining log data to determine predictive power of software metrics for continuous delivery teams',\n",
       "  'authors': \"['Hennie Huijgens', 'Robert Lamping', 'Dick Stevens', 'Hartger Rothengatter', 'Georgios Gousios', 'Daniele Romano']\",\n",
       "  'date': 'August 2017',\n",
       "  'source': 'ESEC/FSE 2017: Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering',\n",
       "  'abstract': 'ING Bank, a large Netherlands-based internationally operating bank, implemented a fully automated continuous delivery pipe-line for its software engineering activities in more than 300 teams, that perform more than 2500 deployments to production each month on more than 750 different applications. Our objective is to examine how strong metrics for agile (Scrum) DevOps teams can be set in an iterative fashion. We perform an exploratory case study that focuses on the classification based on predictive power of software metrics, in which we analyze log data derived from two initial sources within this pipeline. We analyzed a subset of 16 metrics from 59 squads. We identified two lagging metrics and assessed four leading metrics to be strong.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3106237.3117779',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Incident prediction through logging management and machine learning',\n",
       "  'authors': \"['J. El Abdelkhalki', 'M. Ben Ahmed', 'A. Slimani']\",\n",
       "  'date': 'October 2019',\n",
       "  'source': \"SCA '19: Proceedings of the 4th International Conference on Smart City Applications\",\n",
       "  'abstract': 'Analyzing the log file for software or device provides a focal point for making incremental improvements; it is the performed step to start the incident analysis. Although, log messages format or contents may not always be fully documented, and described in many different formats. It makes the log analysis task more difficult, affects the correction deadline of incidents and therefore involves a high financial risk. In this paper, we survey the log file analysis and the existing systems elaborated to resolve current issue. Then, we propose a methodology to support the log analysis in the complex environment. The KN-K-Nearest-Neighbor (KNN) classification method was chosed to be used online by weka to predict the error. Therefore, a program was developed in python to extract, clean and format the log file before comparing the different algorithms of the classifiation method KNN, J48 and Bayes - NaiveBayes in the context of dataset.API was used in order to process Weka. Finally, we illustrate our proposal in the Tivoli Storage Manager (TSM) file log and provide a description of the results obtained.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3368756.3369069',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Social Networks and Railway Passenger Capacity: An Empirical Study Based on Text Mining and Deep Learning',\n",
       "  'authors': \"['Chao Wang', 'Xuyan Pan', 'Yibo Wang']\",\n",
       "  'date': 'November 2018',\n",
       "  'source': \"Safety and Resilience'18: Proceedings of the 4th ACM SIGSPATIAL International Workshop on Safety and Resilience\",\n",
       "  'abstract': 'Railway passenger transport is essential to modern transportation in China. The prediction of railway passenger capacity is of vital importance for ensuring the safety of railway transportation. This paper introduces social network text data into the prediction of railway passenger capacity. In the process of analyzing social network text data, text mining methods are used to analyze the text data, and the information related to railway passenger flow is extracted from the text and added to the prediction model. Meanwhile, in order to obtain better prediction results, this paper applies deep learning method on the data. The combination of text mining and deep learning method has greatly improved the accuracy of our prediction model. Experimental results show that a good accuracy rate has been achieved.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3284103.3284125',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Debugging crashes using continuous contrast set mining',\n",
       "  'authors': \"['Rebecca Qian', 'Yang Yu', 'Wonhee Park', 'Vijayaraghavan Murali', 'Stephen Fink', 'Satish Chandra']\",\n",
       "  'date': 'June 2020',\n",
       "  'source': \"ICSE-SEIP '20: Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Software Engineering in Practice\",\n",
       "  'abstract': 'Facebook operates a family of services used by over two billion people daily on a huge variety of mobile devices. Many devices are configured to upload crash reports should the app crash for any reason. Engineers monitor and triage millions of crash reports logged each day to check for bugs, regressions, and any other quality problems. Debugging groups of crashes is a manually intensive process that requires deep domain expertise and close inspection of traces and code, often under time constraints. We use contrast set mining, a form of discriminative pattern mining, to learn what distinguishes one group of crashes from another. Prior works focus on discretization to apply contrast mining to continuous data. We propose the first direct application of contrast learning to continuous data, without the need for discretization. We also define a weighted anomaly score that unifies continuous and categorical contrast sets while mitigating bias, as well as uncertainty measures that communicate confidence to developers. We demonstrate the value of our novel statistical improvements by applying it on a challenging dataset from Facebook production logs, where we achieve 40x speedup over baseline approaches using discretization.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3377813.3381369',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Spatial data mining of public transport incidents reported in social media',\n",
       "  'authors': \"['Kamil Raczycki', 'Marcin Szymański', 'Yahor Yeliseyenka', 'Piotr Szymański', 'Tomasz Kajdanowicz']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"IWCTS '21: Proceedings of the 14th ACM SIGSPATIAL International Workshop on Computational Transportation Science\",\n",
       "  'abstract': 'Public transport agencies use social media as an essential tool for communicating mobility incidents to passengers. However, while the short term, day-to-day information about transport phenomena is usually posted in social media with low latency, its availability is short term as the content is rarely made an aggregated form. Social media communication of transport phenomena usually lacks GIS annotations as most social media platforms do not allow attaching non-POI GPS coordinates to posts. As a result, the analysis of transport phenomena information is minimal. We collected three years of social media posts of a polish public transport company with user comments. Through exploration, we infer a six-class transport information typology. We successfully build an information type classifier for social media posts, detect stop names in posts, and relate them to GPS coordinates, obtaining a spatial understanding of long-term aggregated phenomena. We show that our approach enables citizen science and use it to analyze the impact of three years of infrastructure incidents on passenger mobility, and the sentiment and reaction scale towards each of the events. All these results are achieved for Polish, an under-resourced language when it comes to spatial language understanding, especially in social media contexts. To improve the situation, we released two of our annotated data sets: social media posts with incident type labels and matched stop names and social media comments with the annotated sentiment. We also opensource the experimental codebase.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3486629.3490696',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Scalable Mining of High-Utility Sequential Patterns With Three-Tier MapReduce Model',\n",
       "  'authors': \"['Jerry Chun-Wei Lin', 'Youcef Djenouri', 'Gautam Srivastava', 'Yuanfa Li', 'Philip S. Yu']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Knowledge Discovery from Data',\n",
       "  'abstract': 'High-utility sequential pattern mining (HUSPM) is a hot research topic in recent decades since it combines both sequential and utility properties to reveal more information and knowledge rather than the traditional frequent itemset mining or sequential pattern mining. Several works of HUSPM have been presented but most of them are based on main memory to speed up mining performance. However, this assumption is not realistic and not suitable in large-scale environments since in real industry, the size of the collected data is very huge and it is impossible to fit the data into the main memory of a single machine. In this article, we first develop a parallel and distributed three-stage MapReduce model for mining high-utility sequential patterns based on large-scale databases. Two properties are then developed to hold the correctness and completeness of the discovered patterns in the developed framework. In addition, two data structures called sidset and utility-linked list are utilized in the developed framework to accelerate the computation for mining the required patterns. From the results, we can observe that the designed model has good performance in large-scale datasets in terms of runtime, memory, efficiency of the number of distributed nodes, and scalability compared to the serial HUSP-Span approach.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3487046',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'New Multi-View Classification Method with Uncertain Data',\n",
       "  'authors': \"['Bo Liu', 'Haowen Zhong', 'Yanshan Xiao']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Knowledge Discovery from Data',\n",
       "  'abstract': 'Multi-view classification aims at designing a multi-view learning strategy to train a classifier from multi-view data, which are easily collected in practice. Most of the existing works focus on multi-view classification by assuming the multi-view data are collected with precise information. However, we always collect the uncertain multi-view data due to the collection process is corrupted with noise in real-life application. In this case, this article proposes a novel approach, called uncertain multi-view learning with support vector machine (UMV-SVM) to cope with the problem of multi-view learning with uncertain data. The method first enforces the agreement among all the views to seek complementary information of multi-view data and takes the uncertainty of the multi-view data into consideration by modeling reachability area of the noise. Then it proposes an iterative framework to solve the proposed UMV-SVM model such that we can obtain the multi-view classifier for prediction. Extensive experiments on real-life datasets have shown that the proposed UMV-SVM can achieve a better performance for uncertain multi-view classification in comparison to the state-of-the-art multi-view classification methods.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3458282',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Identifying Cognitive Attributes Using Deep Learning Classification Techniques',\n",
       "  'authors': \"['Shuai Zhao', 'Xiaoting Huang']\",\n",
       "  'date': 'November 2019',\n",
       "  'source': 'WAIE 2019: Proceedings of the International Workshop on Artificial Intelligence and Education',\n",
       "  'abstract': 'Cognitive diagnosis is very useful to teachers and students, but its application is limited at present. This is largely because identifying the cognitive attributes of items currently is labor intensive and time-consuming. In this study, we used text classification techniques to automatically identify cognitive attributes. Specifically, two popular deep learning classification models, long-short term memory and bi-directional long-short term memory, were employed in conjunction with word embeddings. As the baseline, support vector machine with feature selection using information gain was also adopted. Experiments based on a sample of 805 third grade math items showed that both the deep learning models performed better than support vector machine, and bi-directional long-short term memory achieved the best performance, yielding the accuracy of 82% and the F1 measure of 80%. Our result indicated that text classification methods, especially deep learning models, have great potential in identifying cognitive attributes efficiently, and in turn, make cognitive diagnostic more feasible to practitioners.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3397453.3397458',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Multi-task Learning for Animal Species and Group Category Classification',\n",
       "  'authors': \"['Donghyeon Kim', 'Younglo Lee', 'Hanseok Ko']\",\n",
       "  'date': 'December 2019',\n",
       "  'source': \"ICIT '19: Proceedings of the 2019 7th International Conference on Information Technology: IoT and Smart City\",\n",
       "  'abstract': 'Accurate animal sound classification is an important task in automated animal monitoring system. Such monitoring system is essential for preventing epidemics caused by animal disease. Based on such needs, there has been a variety of efforts to develop an accurate system performing animal sound classification in deep learning framework. Although many research issues and methods to address the issues were introduced, no one has yet to address overcoming the machine learning barriers induced by a single objective function. As learnable parameters only consider a single penalty at the output prediction for training, they cannot capture other characteristics contained in the dataset to extract more generalized prediction. This paper proposes a deep learning based multi-task learning framework for animal sound classification. Both animal species and group classification are performed in an end-to-end learning process. Experimental results show that the proposed multi-task method outperforms single-task method in our recorded animal sound dataset.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3377170.3377259',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Junction density based clustering algorithm for data with arbitrary shapes',\n",
       "  'authors': \"['Ruijia Li', 'Zhiling Cai', 'Hong Wu']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"ICMLC '22: Proceedings of the 2022 14th International Conference on Machine Learning and Computing\",\n",
       "  'abstract': 'Density-based clustering algorithms can deal with arbitrary shaped clusters in data. However, most of these algorithms face difficulties in handling large scale data, since they usually need to compute the distance between each pair of data points for density estimation. To alleviate this problem, we define a new type of density called junction density to measure the density of the junction region of two groups generated by K-means. Since the junction density is only computed for neighboring groups, the computation burden is small. Based on the junction density, we propose a new clustering method to merge the groups instead of directly clustering the data points. Specifically, it mines initial clusters in the groups then assigns the remaining groups to corresponding initial clusters. The experiments on several arbitrary shaped datasets demonstrate the efficiency and effectiveness of the proposed method.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3529836.3529860',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A Survey of Opinion Mining in Arabic: A Comprehensive System Perspective Covering Challenges and Advances in Tools, Resources, Models, Applications, and Visualizations',\n",
       "  'authors': \"['Gilbert Badaro', 'Ramy Baly', 'Hazem Hajj', 'Wassim El-Hajj', 'Khaled Bashir Shaban', 'Nizar Habash', 'Ahmad Al-Sallab', 'Ali Hamdi']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Asian and Low-Resource Language Information Processing',\n",
       "  'abstract': 'Opinion-mining or sentiment analysis continues to gain interest in industry and academics. While there has been significant progress in developing models for sentiment analysis, the field remains an active area of research for many languages across the world, and in particular for the Arabic language, which is the fifth most-spoken language and has become the fourth most-used language on the Internet. With the flurry of research activity in Arabic opinion mining, several researchers have provided surveys to capture advances in the field. While these surveys capture a wealth of important progress in the field, the fast pace of advances in machine learning and natural language processing (NLP) necessitates a continuous need for a more up-to-date literature survey. The aim of this article is to provide a comprehensive literature survey for state-of-the-art advances in Arabic opinion mining. The survey goes beyond surveying previous works that were primarily focused on classification models. Instead, this article provides a comprehensive system perspective by covering advances in different aspects of an opinion-mining system, including advances in NLP software tools, lexical sentiment and corpora resources, classification models, and applications of opinion mining. It also presents future directions for opinion mining in Arabic. The survey also covers latest advances in the field, including deep learning advances in Arabic Opinion Mining. The article provides state-of-the-art information to help new or established researchers in the field as well as industry developers who aim to deploy an operational complete opinion-mining system. Key insights are captured at the end of each section for particular aspects of the opinion-mining system giving the reader a choice of focusing on particular aspects of interest.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3295662',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Combining clustering and classification algorithms for automatic bot detection: a case study on posts about COVID-19',\n",
       "  'authors': \"['Diego Bezerra Lira', 'Fernando Xavier', 'Luciano Antonio Digiampietri']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"SBSI '21: Proceedings of the XVII Brazilian Symposium on Information Systems\",\n",
       "  'abstract': 'In the last decade, there has been a great insertion of bots in several social media. Among the potentially harmful effects of these software agents, there are: the spread of computer viruses and different internet scams, and the spread of fake news, with emphasis on political-electoral and public health-related news. This work presents a new approach for bots’ detection on Twitter, combining the use of feature selection, clustering, and classification algorithms. The proposed approach was compared with more conventional ones (for example, without the use of clustering) and the premise used in this work proved to be true: the use of clustering, together with the features selection, allowed the production of better classification models in order to identify not only the bots who have an activity profile considered non-human (extremely active on Twitter) but also other bots whose profiles are more similar to humans’ ones. The best results of automatic detection of bots reached an overall accuracy of 96.8% and F1 score equal to 0.622. As an additional advantage, these values were achieved by decision-tree models, which can be considered explainable artificial intelligence models.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3466933.3466970',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Mining Approximate Acyclic Schemes from Relations',\n",
       "  'authors': \"['Batya Kenig', 'Pranay Mundra', 'Guna Prasaad', 'Babak Salimi', 'Dan Suciu']\",\n",
       "  'date': 'June 2020',\n",
       "  'source': \"SIGMOD '20: Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data\",\n",
       "  'abstract': 'Acyclic schemes have numerous applications in databases and in machine learning, such as improved design, more efficient storage, and increased performance for queries and machine learning algorithms. Multivalued dependencies (MVDs) are the building blocks of acyclic schemes. The discovery from data of both MVDs and acyclic schemes is more challenging than other forms of data dependencies, such as Functional Dependencies, because these dependencies do not hold on subsets of data, and because they are very sensitive to noise in the data; for example a single wrong or missing tuple may invalidate the schema. In this paper we present Maimon, a system for discovering approximate acyclic schemes and MVDs from data. We give a principled definition of approximation, by using notions from information theory, then describe the two components of Maimon: mining for approximate MVDs, then reconstructing acyclic schemes from approximate MVDs. We conduct an experimental evaluation of Maimon on 20 real-world datasets, and show that it can scale up to 1M rows, and up to 30 columns.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3318464.3380573',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Secure Naïve Bayes Classification Protocol over Encrypted Data Using Fully Homomorphic Encryption',\n",
       "  'authors': \"['Yoshiko Yasumura', 'Yu Ishimaki', 'Hayato Yamana']\",\n",
       "  'date': 'December 2019',\n",
       "  'source': 'iiWAS2019: Proceedings of the 21st International Conference on Information Integration and Web-based Applications &amp; Services',\n",
       "  'abstract': \"Machine learning classification has a wide range of applications. In the big data era, a client may want to outsource classification tasks to reduce the computational burden at the client. Meanwhile, an entity may want to provide a classification model and classification services to such clients. However, applications such as medical diagnosis require sensitive data that both parties may not want to reveal. Fully homomorphic encryption (FHE) enables secure computation over encrypted data without decryption. By applying FHE, classification can be outsourced to a cloud without revealing any data. However, existing studies on classification over FHE do not achieve the scenario of outsourcing classification to a cloud while preserving the privacy of the classification model, client's data and result. In this work, we apply FHE to a naïve Bayes classifier and, to the best of our knowledge, propose the first concrete secure classification protocol that satisfies the above scenario.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3366030.3366056',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Concept-based classification of software defect reports',\n",
       "  'authors': \"['Sangameshwar Patil']\",\n",
       "  'date': 'May 2017',\n",
       "  'source': \"MSR '17: Proceedings of the 14th International Conference on Mining Software Repositories\",\n",
       "  'abstract': 'Automatic identification of the defect type from the textual description of a software defect can significantly speedup as well as improve the software defect management life-cycle. This has been recognized in the research community and multiple solutions based on supervised learning approach have been proposed in the recent literature. However, these approaches need significant amount of labeled training data for use in real-life projects. In this paper, we propose to use Explicit Semantic Analysis (ESA) to carry out concept-based classification of software defect reports. We compute the \"semantic similarity\" between the defect type labels and the defect report in a concept space spanned by Wikipedia articles and then, assign the defect type which has the highest similarity with the defect report. This approach helps us to circumvent the problem of dependence on labeled training data. Experimental results show that using concept-based classification is a promising approach for software defect classification to avoid the expensive process of creating labeled training data and yet get accuracy comparable to the traditional supervised learning approaches. To the best of our knowledge, this is the first use of Wikipedia and ESA for software defect classification problem.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/MSR.2017.20',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Deep Learning for Extreme Multi-label Text Classification',\n",
       "  'authors': \"['Jingzhou Liu', 'Wei-Cheng Chang', 'Yuexin Wu', 'Yiming Yang']\",\n",
       "  'date': 'August 2017',\n",
       "  'source': \"SIGIR '17: Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval\",\n",
       "  'abstract': 'Extreme multi-label text classification (XMTC) refers to the problem of assigning to each document its most relevant subset of class labels from an extremely large label collection, where the number of labels could reach hundreds of thousands or millions. The huge label space raises research challenges such as data sparsity and scalability. Significant progress has been made in recent years by the development of new machine learning methods, such as tree induction with large-margin partitions of the instance spaces and label-vector embedding in the target space. However, deep learning has not been explored for XMTC, despite its big successes in other related areas. This paper presents the first attempt at applying deep learning to XMTC, with a family of new Convolutional Neural Network (CNN) models which are tailored for multi-label classification in particular. With a comparative evaluation of 7 state-of-the-art methods on 6 benchmark datasets where the number of labels is up to 670,000, we show that the proposed CNN approach successfully scaled to the largest datasets, and consistently produced the best or the second best results on all the datasets. On the Wikipedia dataset with over 2 million documents and 500,000 labels in particular, it outperformed the second best method by 11.7%~15.3% in precision@K and by 11.5%~11.7% in NDCG@K for K = 1,3,5.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3077136.3080834',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Emerging applications of machine learning in modern data management',\n",
       "  'authors': \"['Amin Kamali', 'Calisto Zuzarte', 'Verena Kantere']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"CASCON '21: Proceedings of the 31st Annual International Conference on Computer Science and Software Engineering\",\n",
       "  'abstract': \"In recent years, the applications of machine learning (ML) have proliferated in most aspects of traditional computer science. Data management discipline is no exception in this regard. Rule-based modules are being replaced by ML-based counterparts that effectively 'mine the rules' from experience. Approaches that rely on crude statistics are rapidly being outdated by the ones that 'learn' the functional dependencies, correlations, and skewness from the underlying data. These learning-based methods have an upper hand on many different fronts. On one hand, they promise to reduce the cost of development and maintenance of the highly complex classical modules. On the other hand, they avoid the 'one solution fits all' approach by effectively tailoring the behavior to fit the requirements of individual system instances. This workshop brought together leaders of cutting-edge research projects in the area and audience from academia and industry, to discuss some examples of using machine learning for modernizing different aspects of data management. The discussed examples covered four different areas: Query Optimization, Data Partitioning, Database Knobs Tuning, and Data Caching.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3507788.3507841',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Machine Learning at Microsoft with ML.NET',\n",
       "  'authors': \"['Zeeshan Ahmed', 'Saeed Amizadeh', 'Mikhail Bilenko', 'Rogan Carr', 'Wei-Sheng Chin', 'Yael Dekel', 'Xavier Dupre', 'Vadim Eksarevskiy', 'Senja Filipi', 'Tom Finley', 'Abhishek Goswami', 'Monte Hoover', 'Scott Inglis', 'Matteo Interlandi', 'Najeeb Kazmi', 'Gleb Krivosheev', 'Pete Luferenko', 'Ivan Matantsev', 'Sergiy Matusevych', 'Shahab Moradi', 'Gani Nazirov', 'Justin Ormont', 'Gal Oshri', 'Artidoro Pagnoni', 'Jignesh Parmar', 'Prabhat Roy', 'Mohammad Zeeshan Siddiqui', 'Markus Weimer', 'Shauheen Zahirazami', 'Yiwen Zhu']\",\n",
       "  'date': 'July 2019',\n",
       "  'source': \"KDD '19: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining\",\n",
       "  'abstract': 'Machine Learning is transitioning from an art and science into a technology available to every developer. In the near future, every application on every platform will incorporate trained models to encode data-based decisions that would be impossible for developers to author. This presents a significant engineering challenge, since currently data science and modeling are largely decoupled from standard software development processes. This separation makes incorporating machine learning capabilities inside applications unnecessarily costly and difficult, and furthermore discourage developers from embracing ML in first place. In this paper we present ML.NET, a framework developed at Microsoft over the last decade in response to the challenge of making it easy to ship machine learning models in large software applications. We present its architecture, and illuminate the application demands that shaped it. Specifically, we introduce DataView, the core data abstraction of ML.NET which allows it to capture full predictive pipelines efficiently and consistently across training and inference lifecycles. We close the paper with a surprisingly favorable performance study of ML.NET compared to more recent entrants, and a discussion of some lessons learned.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3292500.3330667',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Towards Model-based Pricing for Machine Learning in a Data Marketplace',\n",
       "  'authors': \"['Lingjiao Chen', 'Paraschos Koutris', 'Arun Kumar']\",\n",
       "  'date': 'June 2019',\n",
       "  'source': \"SIGMOD '19: Proceedings of the 2019 International Conference on Management of Data\",\n",
       "  'abstract': \"Data analytics using machine learning (ML) has become ubiquitous in science, business intelligence, journalism and many other domains. While a lot of work focuses on reducing the training cost, inference runtime and storage cost of ML models, little work studies how to reduce the cost of data acquisition, which potentially leads to a loss of sellers' revenue and buyers' affordability and efficiency. In this paper, we propose a model-based pricing (MBP) framework, which instead of pricing the data, directly prices ML model instances. We first formally describe the desired properties of the MBP framework, with a focus on avoiding arbitrage. Next, we show a concrete realization of the MBP framework via a noise injection approach, which provably satisfies the desired formal properties. Based on the proposed framework, we then provide algorithmic solutions on how the seller can assign prices to models under different market scenarios (such as to maximize revenue). Finally, we conduct extensive experiments, which validate that the MBP framework can provide high revenue to the seller, high affordability to the buyer, and also operate on low runtime cost.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3299869.3300078',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A Sentiment Analysis Model for Faculty Comment Evaluation Using Ensemble Machine Learning Algorithms',\n",
       "  'authors': \"['Jay-ar P. Lalata', 'Bobby Gerardo', 'Ruji Medina']\",\n",
       "  'date': 'June 2019',\n",
       "  'source': 'BDE 2019: Proceedings of the 2019 International Conference on Big Data Engineering',\n",
       "  'abstract': \"Teacher evaluation is the systematic procedure done in educational institutions to review the performance of the teachers in a classroom. It aims to provide constructive feedback for teacher's professional growth which benefits students in their education. Students' feedback in the evaluation typically include textual comments which are unstructured but are rich with adequate information and insight about teacher's mastery of the course, teaching style, course content and learning experiences of the students. In this study, sentiment analysis or opinion mining was used to analyze the students' comments. An ensemble approach integrating five individual machine algorithms namely Naive Bayes, Logistic Regression, Support Vector Machine, Decision Tree and Random Forest algorithms were applied to classify the comments based on Majority Voting Principle. The experimental result shows that the ensemble classification system outperforms these individual classifiers with 90.32% accuracy. It helps to improve machine learning results producing better predictions compared to a single model.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3341620.3341638',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Opinion Mining in Facebook Regional Discussion Groups: A Case Study to Identify Health, Education and Security Posts in Discussion Groups',\n",
       "  'authors': \"['Leonardo Augusto Sápiras', 'Rodrigo Antônio Weber']\",\n",
       "  'date': 'May 2019',\n",
       "  'source': \"SBSI '19: Proceedings of the XV Brazilian Symposium on Information Systems\",\n",
       "  'abstract': \"This paper presents the results a case study that apply opinion mining about health, security and education, using as source discussions in Facebook regional groups. The method used is quite different from other researches because it propose an approach to identify regional posts. Five different supervisioned learning algorithms was applied during the classification step. The results show that region's posts can be identified with this new approach.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3330204.3330221',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': \"Let's shine together!: a comparative study between learning analytics and educational data mining\",\n",
       "  'authors': \"['Guanliang Chen', 'Vitor Rolim', 'Rafael Ferreira Mello', 'Dragan Gašević']\",\n",
       "  'date': 'March 2020',\n",
       "  'source': \"LAK '20: Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge\",\n",
       "  'abstract': 'Learning Analytics and Knowledge (LAK) and Educational Data Mining (EDM) are two of the most popular venues for researchers and practitioners to report and disseminate discoveries in data-intensive research on technology-enhanced education. After the development of about a decade, it is time to scrutinize and compare these two venues. By doing this, we expected to inform relevant stakeholders of a better understanding of the past development of LAK and EDM and provide suggestions for their future development. Specifically, we conducted an extensive comparison analysis between LAK and EDM from four perspectives, including (i) the topics investigated; (ii) community development; (iii) community diversity; and (iv) research impact. Furthermore, we applied one of the most widely-used language modeling techniques (Word2Vec) to capture words used frequently by researchers to describe future works that can be pursued by building upon suggestions made in the published papers to shed light on potential directions for future research.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3375462.3375500',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Understanding and Visualizing Data Iteration in Machine Learning',\n",
       "  'authors': \"['Fred Hohman', 'Kanit Wongsuphasawat', 'Mary Beth Kery', 'Kayur Patel']\",\n",
       "  'date': 'April 2020',\n",
       "  'source': \"CHI '20: Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems\",\n",
       "  'abstract': 'Successful machine learning (ML) applications require iterations on both modeling and the underlying data. While prior visualization tools for ML primarily focus on modeling, our interviews with 23 ML practitioners reveal that they improve model performance frequently by iterating on their data (e.g., collecting new data, adding labels) rather than their models. We also identify common types of data iterations and associated analysis tasks and challenges. To help attribute data iterations to model performance, we design a collection of interactive visualizations and integrate them into a prototype, Chameleon, that lets users compare data features, training/testing splits, and performance across data versions. We present two case studies where developers apply \\\\system to their own evolving datasets on production ML projects. Our interface helps them verify data collection efforts, find failure cases stretching across data versions, capture data processing changes that impacted performance, and identify opportunities for future data iterations.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3313831.3376177',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Machine Learning for Detecting Data Exfiltration: A Review',\n",
       "  'authors': \"['Bushra Sabir', 'Faheem Ullah', 'M. Ali Babar', 'Raj Gaire']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Computing Surveys',\n",
       "  'abstract': 'Context: Research at the intersection of cybersecurity, Machine Learning (ML), and Software Engineering (SE) has recently taken significant steps in proposing countermeasures for detecting sophisticated data exfiltration attacks. It is important to systematically review and synthesize the ML-based data exfiltration countermeasures for building a body of knowledge on this important topic. Objective: This article aims at systematically reviewing ML-based data exfiltration countermeasures to identify and classify ML approaches, feature engineering techniques, evaluation datasets, and performance metrics used for these countermeasures. This review also aims at identifying gaps in research on ML-based data exfiltration countermeasures. Method: We used Systematic Literature Review (SLR) method to select and review 92 papers. Results: The review has enabled us to: (a) classify the ML approaches used in the countermeasures into data-driven, and behavior-driven approaches; (b) categorize features into six types: behavioral, content-based, statistical, syntactical, spatial, and temporal; (c) classify the evaluation datasets into simulated, synthesized, and real datasets; and (d) identify 11 performance measures used by these studies. Conclusion: We conclude that: (i) The integration of data-driven and behavior-driven approaches should be explored; (ii) There is a need of developing high quality and large size evaluation datasets; (iii) Incremental ML model training should be incorporated in countermeasures; (iv) Resilience to adversarial learning should be considered and explored during the development of countermeasures to avoid poisoning attacks; and (v) The use of automated feature engineering should be encouraged for efficiently detecting data exfiltration attacks.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3442181',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Pattern Classification of Instantaneous Cognitive Task-load Through GMM Clustering, Laplacian Eigenmap, and Ensemble SVMs',\n",
       "  'authors': \"['Jianhua Zhang', 'Zhong Yin', 'Rubin Wang']\",\n",
       "  'date': 'None',\n",
       "  'source': 'IEEE/ACM Transactions on Computational Biology and Bioinformatics',\n",
       "  'abstract': \"The identification of the temporal variations in human operator cognitive task-load CTL is crucial for preventing possible accidents in human-machine collaborative systems. Recent literature has shown that the change of discrete CTL level during human-machine system operations can be objectively recognized using neurophysiological data and supervised learning technique. The objective of this work is to design subject-specific multi-class CTL classifier to reveal the complex unknown relationship between the operator's task performance and neurophysiological features by combining target class labeling, physiological feature reduction and selection, and ensemble classification techniques. The psychophysiological data acquisition experiments were performed under multiple human-machine process control tasks. Four or five target classes of CTL were determined by using a Gaussian mixture model and three human performance variables. By using Laplacian eigenmap, a few salient EEG features were extracted, and heart rates were used as the input features of the CTL classifier. Then, multiple support vector machines were aggregated via majority voting to create an ensemble classifier for recognizing the CTL classes. Finally, the obtained CTL classification results were compared with those of several existing methods. The results showed that the proposed methods are capable of deriving a reasonable number of target classes and low-dimensional optimal EEG features for individual human operator subjects.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/TCBB.2016.2561927',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Rotom: A Meta-Learned Data Augmentation Framework for Entity Matching, Data Cleaning, Text Classification, and Beyond',\n",
       "  'authors': \"['Zhengjie Miao', 'Yuliang Li', 'Xiaolan Wang']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data\",\n",
       "  'abstract': \"Deep Learning revolutionizes almost all fields of computer science including data management. However, the demand for high-quality training data is slowing down deep neural nets' wider adoption. To this end, data augmentation (DA), which generates more labeled examples from existing ones, becomes a common technique. Meanwhile, the risk of creating noisy examples and the large space of hyper-parameters make DA less attractive in practice. We introduce Rotom, a multi-purpose data augmentation framework for a range of data management and mining tasks including entity matching, data cleaning, and text classification. Rotom features InvDA, a new DA operator that generates natural yet diverse augmented examples by formulating DA as a seq2seq task. The key technical novelty of Rotom is a meta-learning framework that automatically learns a policy for combining examples from different DA operators, whereby combinatorially reduces the hyper-parameters space. Our experimental results show that Rotom effectively improves a model's performance by combining multiple DA operators, even when applying them individually does not yield performance improvement. With this strength, Rotom outperforms the state-of-the-art entity matching and data cleaning systems in the low-resource settings as well as two recently proposed DA techniques for text classification.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3448016.3457258',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Machine learning predictive analytics for player movement prediction in NBA: applications, opportunities, and challenges',\n",
       "  'authors': \"['Dembe Koi Stephanos', 'Ghaith Husari', 'Brian T. Bennett', 'Emma Stephanos']\",\n",
       "  'date': 'April 2021',\n",
       "  'source': \"ACM SE '21: Proceedings of the 2021 ACM Southeast Conference\",\n",
       "  'abstract': 'Recently, strategies of National Basketball Association (NBA) teams have evolved with the skillsets of players and the emergence of advanced analytics. This has led to a more free-flowing game in which traditional positions and play calls have been replaced with player archetypes and read-and-react offensives that operate off a variety of isolated actions. The introduction of position tracking technology by SportVU has aided the analysis of these patterns by offering a vast dataset of on-court behavior. There have been numerous attempts to identify and classify patterns by evaluating the outcomes of offensive and defensive strategies associated with actions within this dataset, a job currently done manually by reviewing game tape. Some of these classification attempts have used supervised techniques that begin with labeled sets of plays and feature sets to automate the detection of future cases. Increasingly, however, deep learning approaches such as convolutional neural networks have been used in conjunction with player trajectory images generated from positional data. This enables classification to occur in a bottom-up manner, potentially discerning unexpected patterns. Others have shifted focus from classification, instead using this positional data to evaluate the success of a given possession based on spatial factors such as defender proximity and player factors such as role or skillset. While play/action detection, classification and analysis have each been addressed in literature, a comprehensive approach that accounts for modern trends is still lacking. In this paper, we discuss various approaches to action detection and analysis and ultimately propose an outline for a deep learning approach of identification and analysis resulting in a queryable dataset complete with shot evaluations, thus combining multiple contributions into a serviceable tool capable of assisting and automating much of the work currently done by NBA professionals.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3409334.3452064',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Process-mining based dynamic software architecture reconstruction',\n",
       "  'authors': \"['Tijmen de Jong', 'Jan Martijn E. M. van der Werf']\",\n",
       "  'date': 'September 2019',\n",
       "  'source': \"ECSA '19: Proceedings of the 13th European Conference on Software Architecture - Volume 2\",\n",
       "  'abstract': 'Dynamic architecture reconstruction approaches aim to reconstruct the run-time architecture of a software system. Process mining is an emerging field combining process analytics and data science techniques. In this paper, we present an approach that creates interactive architecture visualizations without requiring any knowledge of the source code of the system under study. The approach is implemented in the tool AJPOLog. with two case studies, we show that this approach creates reliable results with relatively little effort. Though the studies also show that more research is needed to apply process mining techniques in the field of architecture reconstruction.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3344948.3344985',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Large-Scale Frequent Episode Mining from Complex Event Sequences with Hierarchies',\n",
       "  'authors': \"['Xiang Ao', 'Haoran Shi', 'Jin Wang', 'Luo Zuo', 'Hongwei Li', 'Qing He']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Intelligent Systems and Technology',\n",
       "  'abstract': 'Frequent Episode Mining (FEM), which aims at mining frequent sub-sequences from a single long event sequence, is one of the essential building blocks for the sequence mining research field. Existing studies about FEM suffer from unsatisfied scalability when faced with complex sequences as it is an NP-complete problem for testing whether an episode occurs in a sequence. In this article, we propose a scalable, distributed framework to support FEM on “big” event sequences. As a rule of thumb, “big” illustrates an event sequence is either very long or with masses of simultaneous events. Meanwhile, the events in this article are arranged in a predefined hierarchy. It derives some abstractive events that can form episodes that may not directly appear in the input sequence. Specifically, we devise an event-centered and hierarchy-aware partitioning strategy to allocate events from different levels of the hierarchy into local processes. We then present an efficient special-purpose algorithm to improve the local mining performance. We also extend our framework to support maximal and closed episode mining in the context of event hierarchy, and to the best of our knowledge, we are the first attempt to define and discover hierarchy-aware maximal and closed episodes. We implement the proposed framework on Apache Spark and conduct experiments on both synthetic and real-world datasets. Experimental results demonstrate the efficiency and scalability of the proposed approach and show that we can find practical patterns when taking event hierarchies into account.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3326163',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Pattern Classification of Instantaneous Cognitive Task-load Through GMM Clustering, Laplacian Eigenmap, and Ensemble SVMs',\n",
       "  'authors': \"['Jianhua Zhang', 'Zhong Yin', 'Rubin Wang']\",\n",
       "  'date': 'None',\n",
       "  'source': 'IEEE/ACM Transactions on Computational Biology and Bioinformatics',\n",
       "  'abstract': \"The identification of the temporal variations in human operator cognitive task-load CTL is crucial for preventing possible accidents in human-machine collaborative systems. Recent literature has shown that the change of discrete CTL level during human-machine system operations can be objectively recognized using neurophysiological data and supervised learning technique. The objective of this work is to design subject-specific multi-class CTL classifier to reveal the complex unknown relationship between the operator's task performance and neurophysiological features by combining target class labeling, physiological feature reduction and selection, and ensemble classification techniques. The psychophysiological data acquisition experiments were performed under multiple human-machine process control tasks. Four or five target classes of CTL were determined by using a Gaussian mixture model and three human performance variables. By using Laplacian eigenmap, a few salient EEG features were extracted, and heart rates were used as the input features of the CTL classifier. Then, multiple support vector machines were aggregated via majority voting to create an ensemble classifier for recognizing the CTL classes. Finally, the obtained CTL classification results were compared with those of several existing methods. The results showed that the proposed methods are capable of deriving a reasonable number of target classes and low-dimensional optimal EEG features for individual human operator subjects.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/TCBB.2016.2561927',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A Label Extension Schema for Improved Text Emotion Classification',\n",
       "  'authors': \"['Zongxi Li', 'Xianming Li', 'Haoran Xie', 'Qing Li', 'Xiaohui Tao']\",\n",
       "  'date': 'December 2021',\n",
       "  'source': \"WI-IAT '21: IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology\",\n",
       "  'abstract': 'Due to the subjectiveness and fuzziness of emotions in texts, researchers have been aware that it is ubiquitous to observe multiple emotions in a sentence, and the one-hot label approach is not informative enough in emotion-relevant text classification tasks. Therefore, to facilitate the classification task, recent works focus on generating and employing a coarse-grained emotion distribution, which is based on coarse-grained labels provided by the underlying dataset. Although such methods can alleviate the problem of overfitting and improve robustness, they may cause inter-class confusion between similar emotion categories and introduce undesirable noise during training. Meanwhile, current studies neglect the fine-grained emotions associated with these coarse-grained labels. To address the issue caused by utilizing a coarse-grained distribution, we propose in this paper a general and novel emotion label extension method based on fine-grained emotions. Specifically, we first identify a mapping function between coarse-grained emotions and fine-grained emotion concepts, and extend the original label space with specific fine-grained emotions. Then, we generate a fine-grained emotion distribution by employing a rule-based method, and utilize it as a model constraint to incorporate the dependencies among fine-grained emotions to predict the original coarse-grained emotion labels. We conduct extensive experiments to demonstrate the effectiveness of our proposed label extension method. The results indicate that our proposed method can produce notable improvements over baseline models on the applied datasets.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3486622.3493935',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Prediction of Injuries and Fatalities in Aviation Accidents through Machine Learning',\n",
       "  'authors': \"['R. Alan Burnett', 'Dong Si']\",\n",
       "  'date': 'May 2017',\n",
       "  'source': \"ICCDA '17: Proceedings of the International Conference on Compute and Data Analysis\",\n",
       "  'abstract': 'This paper concerns application of various machine learning techniques to derive classification models for predicting conditions that increase the likelihood of aviation accidents involving fatalities and serious injuries. Machine learning classification techniques, including Decision Trees, K-Nearest Neighbors, Support Vector Machines (SVMs), and Artificial Neural Networks (ANNs) are applied to datasets derived from original data obtained from Federal Aviation Administration (FAA) Aviation Accident and Incident Records from 1975-2002. The accident data are filtered to focus on FAA Part 91 (General Aviation) accidents involving powered, fixed-wing, manufactured aircraft. The results demonstrate ANNs to yield the most accurate prediction levels for both fatal accidents and accidents involving severe injuries. The results also demonstrate that machine learning approaches may yield insightful information beyond what is available through traditional statistical analysis methodologies.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3093241.3093288',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Evaluation of large scale RoI mining applications in edge computing environments',\n",
       "  'authors': \"['Loris Belcastro', 'Alberto Falcone', 'Alfredo Garro', 'Fabrizio Marozzo']\",\n",
       "  'date': 'September 2021',\n",
       "  'source': \"DS-RT '21: Proceedings of the 2021 IEEE/ACM 25th International Symposium on Distributed Simulation and Real Time Applications\",\n",
       "  'abstract': \"Researchers and leading IT companies are increasingly proposing hybrid cloud/edge solutions, which allow to move part of the workload from the cloud to the edge nodes, by reducing the network traffic and energy consumption, but also getting low latency responses near to real time. This paper proposes a novel hybrid cloud/edge architecture for efficiently extracting Regions-of-Interest (RoI) in a large scale urban computing environment, where a huge amount of geotagged data are generated and collected through users's mobile devices. The proposal is organized in two parts: (i) a modeling part that defines the hybrid cloud/edge architecture capable of managing a large number of devices; (ii) a simulation part in which different design choices are evaluated to improve the performance of RoI mining algorithms in terms of processing time, network delay, task failure and computing resource utilization. Several experiments have been carried out to evaluate the performance of the proposed architecture starting from different configurations and orchestration policies. The achieved results showed that the proposed hybrid cloud/edge architecture, with the use of two novel orchestration policies (network- and utilization-based), permits to improve the exploitation of resources, also granting low network latency and task failure rate in comparison with other standard scenarios (only-edge or only-cloud).\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/DS-RT52167.2021.9576131',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Anomaly detection for machinery by using Big Data Real-Time processing and clustering technique',\n",
       "  'authors': \"['Zhuo Wang', 'Yanghui Zhou', 'Gangmin Li']\",\n",
       "  'date': 'November 2019',\n",
       "  'source': \"ICBDR '19: Proceedings of the 3rd International Conference on Big Data Research\",\n",
       "  'abstract': 'This paper aims to apply techniques of Big Data Analytics including K-Means Clustering to diagnose potential problems for offshore rotating machinery. The innovative methods are attempted in both Batch K-Means and Streaming K-Means. Their performances are compared with the conventional signal analysis method. Both K-Means models have a better performance on detecting significant mechanical faults as anomalies for offshore rotating machinery which can be considered as appropriate method for machine operational maintenance.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3372454.3372480',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Text Classification of Micro-blog\\'s \"Tree Hole\" Based on Convolutional Neural Network',\n",
       "  'authors': \"['Xiaoli Zhao', 'Shaofu Lin', 'Zhisheng Huang']\",\n",
       "  'date': 'December 2018',\n",
       "  'source': \"ACAI '18: Proceedings of the 2018 International Conference on Algorithms, Computing and Artificial Intelligence\",\n",
       "  'abstract': 'Rapid recognition of depression is an important step in the research of depression. With the development of social networking platform, more and more depressive patients regard micro-blog as one of the ways of self-expression. And this information provides support of data for the recognition of depression. In this study, the data crawled from micro-blog\\'s \"tree hole\"[1] is used as experimental corpus. Combined with the features of micro-blog text with depression, a double-input convolutional neural network structure (D-CNN) is proposed. This method takes both the external features and the semantic features of text as input. By comparing the accuracy of classification with Support Vector Machine (SVM) and convolutional neural network (CNN) algorithm, it is finally shown that the D-CNN can further improve the accuracy of text classify.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3302425.3302501',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Multi-Label Emotion Mining From Student Comments',\n",
       "  'authors': \"['Angelina Tzacheva', 'Jaishree Ranganathan', 'Rajendra Jadi']\",\n",
       "  'date': 'July 2019',\n",
       "  'source': \"ICIEI '19: Proceedings of the 4th International Conference on Information and Education Innovations\",\n",
       "  'abstract': \"Science, Technology, Engineering, and Mathematics (STEM) education is gaining more attention not today but has been under research, and discussion for the past few decades. Factors that are considered for research include but not limited to the following, culture on campus, teaching and learning models, and student experience in classroom, gender bias, and stereotypes. One of the major factors is the teaching model adopted which have impact on the student learning styles and their experience in the classroom. Teaching models include traditional models, modern flipped class-room models, and active learning approaches. This study focuses on active learning approaches and their impact on students learning and experience. Light-weight team is an active learning approach, in which team members have little direct impact on each other's final grades, with significant long-term socialization. In this work we used data from end of course student evaluation. We propose extend our previous method for assessing the effectiveness of the Light-weight team teaching model, through automatic detection of emotions in student feedback in computer science course by creating multi-label for each text comment. The students are surveyed about their feelings and thoughts about teaching and learning models adopted and student experience in the classroom. Results show that implementation of these methods result in increased positivity in student emotions.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3345094.3345112',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Online Analysis of Simulation Data with Stream-based Data Mining',\n",
       "  'authors': \"['Niclas Feldkamp', 'Soeren Bergmann', 'Steffen Strassburger']\",\n",
       "  'date': 'May 2017',\n",
       "  'source': \"SIGSIM-PADS '17: Proceedings of the 2017 ACM SIGSIM Conference on Principles of Advanced Discrete Simulation\",\n",
       "  'abstract': 'Discrete event simulation is an accepted instrument for investigating the dynamic behavior of complex systems and evaluating processes. Usually simulation experts conduct simulation experiments for a predetermined system specification by manually varying parameters through educated assumptions and according to a prior defined goal. As an alternative, data farming and knowledge discovery in simulation data are ongoing and popular methods in order to uncover unknown relationships and effects in the model to gain useful information about the underlying system. Those methods usually demand broad scale and data intensive experimental design, so computing time can quickly become large. As a solution to that, we extend an existing concept of knowledge discovery in simulation data with an online stream mining component to get data mining results even while experiments are still running. For this purpose, we introduce a method for using decision tree classification in combination with clustering algorithms for analyzing simulation output data that considers the flow of experiments as a data stream. A prototypical implementation proves the basic applicability of the concept and yields large possibilities for future research.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3064911.3064915',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'E-Commerce Merchant Classification using Website Information',\n",
       "  'authors': \"['Galuh Tunggadewi Sahid', 'Rahmad Mahendra', 'Indra Budi']\",\n",
       "  'date': 'June 2019',\n",
       "  'source': 'WIMS2019: Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics',\n",
       "  'abstract': 'With the rapid growth of the e-commerce landscape, classifying e-commerce merchants has become an important task as it is an integral part of various processes in e-commerce. One of the examples is merchant on boarding, where the category of an e-commerce merchant has proven to be a good indicator of the risk of the merchant. However, since most of e-commerce businesses do not have brick-and-mortar stores from which we can assess it directly, the only source of information regarding the merchant itself is its website. Thus, we can view this problem as a web classification problem, where we classify e-commerce websites into a category. In this research, we aim to build an end-to-end classification system for e-commerce websites. There are a few challenges such as the number of pages to be processed, imbalanced dataset, and the language of e-commerce websites that can be mixed language. We built a website classification system and experimented with case study of Indonesian and English e-commerce webs, that are classified into 37 different categories. Our best result achieved an F-score of 0.83.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3326467.3326486',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Effective Media Traffic Classification Using Deep Learning',\n",
       "  'authors': \"['Qing Lyu', 'Xingjian Lu']\",\n",
       "  'date': 'March 2019',\n",
       "  'source': \"ICCDA '19: Proceedings of the 2019 3rd International Conference on Compute and Data Analysis\",\n",
       "  'abstract': 'Traffic classification (TC) is very important as it can provide useful information which can be used in the flexible management of the network. However, TC has become more and more complicated because of the emergence of various network applications and techniques. In this paper, we apply deep learning based method to the classification of four different kinds of media traffic, i.e., audio, picture, text and video. We collect traffic data from the real network environment. Multilayer Perceptron (MLP) and Convolutional Neural Network (CNN) based traffic classification methods are designed to accurately classify the target traffic into different categories. We found that MLP has very good performance in most scenarios. Moreover, specific architecture can reduce the training time of the neural network in the classification.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3314545.3316278',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A Deep Learning Approach to the Malware Classification Problem using Autoencoders',\n",
       "  'authors': '[\\'Dhiego Ramos Pinto\\', \\'Julio Cesar Duarte\\', \"Ricardo Sant\\'Ana\"]',\n",
       "  'date': 'May 2019',\n",
       "  'source': \"SBSI '19: Proceedings of the XV Brazilian Symposium on Information Systems\",\n",
       "  'abstract': \"Detecting malicious code or categorizing it among families has become an increasingly difficult task. Malware1 exploits vulnerabilities and employ sophisticated techniques to avoid their detection and further classification, challenging cybersecurity teams, governments, enterprises, and the ordinary user, causing uncountable losses annually. Traditional machine learning algorithms have been used to attack the problem, although, these methods are heavily relying on domain expertise to be successful. Deep Learning methods requires less dependency on feature engineering, discovering the important features straightly from the raw data, recognizing patterns that humans usually can't. This work presents a deep learning approach for malware multi-class classification based on an unsupervised pre-trained classifier, using opcodes and its operands frequencies as raw data, ignoring knowledge that could be acquired from any known features from the malware families. The results confirmed that the approach is well succeeded and our best model achieved a MacroF1 of 93.14% a competitive result comparing to best-known classifier, since it uses less information about the malware.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3330204.3330229',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Functional Classification of Websites',\n",
       "  'authors': \"['Najlah Gali', 'Radu Mariescu Istodor', 'Pasi Fränti']\",\n",
       "  'date': 'December 2017',\n",
       "  'source': \"SoICT '17: Proceedings of the 8th International Symposium on Information and Communication Technology\",\n",
       "  'abstract': 'We propose a novel method to classify websites based on their functional purpose. A website is classified either as single service, brand or service directory. We utilize a number of features that are derived from the link of the website, the postal addresses found in the website, the size of the website, and the text of the anchor element in the Document Object Model tree. We utilize two models to perform the classification task: decision tree and clustering-based models. Our method is fully automated and does not require extensive training data or user interaction. The proposed website classifier improves the baseline by 2 percentage points in case of single service, 33 percentage points in case of brand and 18 percentage points in case of service directory.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3155133.3155178',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Prediction of Coronary Heart Disease using Machine Learning: An Experimental Analysis',\n",
       "  'authors': \"['Amanda H. Gonsalves', 'Fadi Thabtah', 'Rami Mustafa A. Mohammad', 'Gurpreet Singh']\",\n",
       "  'date': 'July 2019',\n",
       "  'source': \"ICDLT '19: Proceedings of the 2019 3rd International Conference on Deep Learning Technologies\",\n",
       "  'abstract': 'The field of medical analysis is often referred to be a valuable source of rich information. Coronary Heart Disease (CHD) is one of the major causes of death all around the world therefore early detection of CHD can help reduce these rates. The challenge lies in the complexity of the data and correlations when it comes to prediction using conventional techniques. The aim of this research is to use the historical medical data to predict CHD using Machine Learning (ML) technology. The scope of this research is limited to using three supervised learning techniques namely Naïve Bayes (NB), Support Vector Machine (SVM) and Decision Tree (DT), to discover correlations in CHD data that might help improving the prediction rate. Using the South African Heart Disease dataset of 462 instances, intelligent models are derived by the considered ML techniques using 10-fold cross validation. Empirical results using different performance evaluation measures report that probabilistic models derived by NB are promising in detecting CHD.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3342999.3343015',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Collective classification in social networks',\n",
       "  'authors': \"['Omar Jaafor', 'Babiga Birregah']\",\n",
       "  'date': 'July 2017',\n",
       "  'source': \"ASONAM '17: Proceedings of the 2017 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 2017\",\n",
       "  'abstract': 'Classification is one of the most studied subjects in machine learning. Most classification methods that were developed this last decade either account for structure (interactions, relationships) or attributes (text, numerical, etc). This leads to ignoring significant patterns in a dataset that could only be captured by analyzing the features of an item and its interactions. Collective classification methods use both structure and attributes, often by aggregating data from neighbors of a node and learning a model on the aggregated data. In social networks, the degree distribution of nodes follows a power law where few nodes have many neighbors. High degree nodes have incoming links from low degree nodes of different classes and many nodes have very few edges. Hence, using only local structure may lead to poor predictions. Also, many social networks allow for different types of interactions (retweet, reply, like, etc.) that affect classification differently. This article proposes a collective classification method that makes use of the structure of a network to determine its neighbors. It then presents experiments aimed at detecting jihadi propagandists and malware distributors on social networks.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3110025.3110128',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'CAST: A Correlation-based Adaptive Spectral Clustering Algorithm on Multi-scale Data',\n",
       "  'authors': \"['Xiang Li', 'Ben Kao', 'Caihua Shan', 'Dawei Yin', 'Martin Ester']\",\n",
       "  'date': 'August 2020',\n",
       "  'source': \"KDD '20: Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining\",\n",
       "  'abstract': 'We study the problem of applying spectral clustering to cluster multi-scale data, which is data whose clusters are of various sizes and densities. Traditional spectral clustering techniques discover clusters by processing a similarity matrix that reflects the proximity of objects. For multi-scale data, distance-based similarity is not effective because objects of a sparse cluster could be far apart while those of a dense cluster have to be sufficiently close. Following [16], we solve the problem of spectral clustering on multi-scale data by integrating the concept of objects\\' \"reachability similarity\" with a given distance-based similarity to derive an objects\\' coefficient matrix. We propose the algorithm CAST that applies trace Lasso to regularize the coefficient matrix. We prove that the resulting coefficient matrix has the \"grouping effect\" and that it exhibits \"sparsity\". We show that these two characteristics imply very effective spectral clustering. We evaluate CAST and 10 other clustering methods on a wide range of datasets w.r.t. various measures. Experimental results show that CAST provides excellent performance and is highly robust across test cases of multi-scale data.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3394486.3403086',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A Deep Learning approach to Hyperspectral Image Classification using an improved Hybrid 3D-2D Convolutional Neural Network',\n",
       "  'authors': \"['Dimitra Koumoutsou', 'Eleni Charou']\",\n",
       "  'date': 'September 2020',\n",
       "  'source': 'SETN 2020: 11th Hellenic Conference on Artificial Intelligence',\n",
       "  'abstract': 'In recent years, the task of Hyperspectral Image (HSI) classification has appeared in various fields, including Remote Sensing. Meanwhile, the evolution of Deep Learning, and the prevalence of the Convolutional Neural Network (CNN) has revolutionized the way unstructured, especially visual, data are processed. 2D CNN have proved highly efficient in exploiting the spatial information of images, but in HSI classification, data contain both spectral and spatial features. To make use of these characteristics, many variations of a 3D CNN have been proposed, but a 3D Convolution comes at a high computational cost. A fusion of 3D and 2D convolutions decreases processing time by distributing spectral-spatial feature extraction across a lighter, less complex model. An enhanced Hybrid network architecture is proposed alongside a data preprocessing plan, with the aim of achieving a significant improvement in classification results. Four benchmark datasets (Indian Pines, Pavia University, Salinas and Data Fusion 2013 Contest) are used to compare the model to other hand-crafted or deep learning architectures. It is demonstrated that the proposed network outperforms state-of-the-art approaches in terms of classification accuracy, while avoiding some commonly used, computationally expensive design choices.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3411408.3411462',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Seeds Classification and Quality Testing Using Deep Learning and YOLO v5',\n",
       "  'authors': \"['Nidhi Kundu', 'Geeta Rani', 'Vijaypal Singh Dhaka']\",\n",
       "  'date': 'August 2021',\n",
       "  'source': \"DSMLAI '21': Proceedings of the International Conference on Data Science, Machine Learning and Artificial Intelligence\",\n",
       "  'abstract': \"Segregation of seeds of different crops grown in the mixed cropping is a major cause of concern for the farmers as well as the food industry. Also, the classification and packaging of seeds based on their quality is a challenging task for farmers and agro-industries. Moreover, Post-thrashing separation of seeds by the traditional techniques such as sieving, hand-picking, etc. is a time-consuming and tedious task. Thus, there is a need to automate seed segregation. The potential of deep learning and machine learning techniques in object detection, classification, and pattern recognition motivated the researchers to employ these techniques for the automatic segregation of seeds at the harvesting site. The techniques proposed so far focus on the classification of seeds of different crops. Limited research work is observed that focuses on the classification of seeds of crops grown as a part of mixed cropping as well as seeds of different quality standards. Also, there is a huge scope to improve the classification performance of the proposed models. The purpose of this research to develop the deep learning-based system 'Mixed Cropping Seed Classifier and Quality Tester (MCSCQT)' for accurate classification and quality testing of seeds based on their shape, color, and texture. The system is trained on the dataset comprising labelled images of healthy and diseased seeds of pearl millet and maize. It reports the highest precision and recall of 99%. The efficacy of the system in discriminating the seeds of pearl millet and maize may prove a game-changer for the food industry. Also, its capability in recognition of diseased and healthy seeds of maize enhances its utility in the food industry.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3484824.3484913',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Horse Breed Classification Based on Transfer Learning',\n",
       "  'authors': \"['Yang Fu', 'Xiangnian Huang', 'Yunfeng Li']\",\n",
       "  'date': 'November 2020',\n",
       "  'source': \"ICAIP '20: Proceedings of the 4th International Conference on Advances in Image Processing\",\n",
       "  'abstract': 'Expert identification of horse breeds is an age-old task that can now be identified using genetic techniques. However, neither approach is cheap nor efficient. The automatic classification of horse breeds by computer vision is an effective solution. In this paper, we solve this task by proposing a novel method using transfer learning of pre-trained deep convolution neural networks architectures. The pre-trained convolutional neural networks include MobilenetV2, Mobilenet, Xception, VGG16, and VGG19. We use the keras deep learning framework, and train these deep convolution neural networks for transfer learning, which overcomes the problem of small amount of data in the early stage. An extensive experimental study on various horse breeds datasets shows that our method obtains an average accuracy rate of automatic classification of horse breeds to 89.34%, which has obvious advantage over other deep convolutional neural network models such as xception, vgg16, vgg19 and self-made convolutional neural network.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3441250.3441264',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Argument Mining: A Survey',\n",
       "  'authors': \"['John Lawrence', 'Chris Reed']\",\n",
       "  'date': 'None',\n",
       "  'source': 'Computational Linguistics',\n",
       "  'abstract': 'Argument mining is the automatic identification and extraction of the structure of inference and reasoning expressed as arguments presented in natural language. Understanding argumentative structure makes it possible to determine not only what positions people are adopting, but also why they hold the opinions they do, providing valuable insights in domains as diverse as financial market prediction and public relations. This survey explores the techniques that establish the foundations for argument mining, provides a review of recent advances in argument mining techniques, and discusses the challenges faced in automatically extracting a deeper understanding of reasoning expressed in language in general.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1162/coli_a_00364',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Big data scalability based on Spark Machine Learning Libraries',\n",
       "  'authors': \"['Anna Karen Garate-Escamilla', 'Amir Hajjam El Hassani', 'Emmanuel Andres']\",\n",
       "  'date': 'November 2019',\n",
       "  'source': \"ICBDR '19: Proceedings of the 3rd International Conference on Big Data Research\",\n",
       "  'abstract': 'The paper introduces the challenge of scalability in machine learning algorithms suitable for massive datasets. Today, big data has relevant applications in the industry due to improvements in the system performance and by turning information into knowledge. Big data challenges include the lack of strategies to process computational cost and the large amount of data when computing machine learning predictions. To overcome these scalability issues, it is convenient to work with distributed and parallelized architecture across multiple nodes. The approach is based on Apache Spark, an in-memory distributed application that offers extensive machine learning libraries. The main contribution of the study is to measure the scalability by calculating the execution time that a classifier achieves with larger workloads. We validate our classifier models with experiments on logistic regression and random forest by studying their adaptability to the Apache Spark framework. The present work expects to combine the areas of big data and machine learning on scalability, and the use of optimization methods, cache and persist. In addition, a comparison between the classifiers is provided. The evaluation experiments show that logistic regression performed the shortest execution time and best scalability.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3372454.3372469',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Keeping the Data Lake in Form: Proximity Mining for Pre-Filtering Schema Matching',\n",
       "  'authors': \"['Ayman Alserafi', 'Alberto Abelló', 'Oscar Romero', 'Toon Calders']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Information Systems',\n",
       "  'abstract': 'Data lakes (DLs) are large repositories of raw datasets from disparate sources. As more datasets are ingested into a DL, there is an increasing need for efficient techniques to profile them and to detect the relationships among their schemata, commonly known as holistic schema matching. Schema matching detects similarity between the information stored in the datasets to support information discovery and retrieval. Currently, this is computationally expensive with the volume of state-of-the-art DLs. To handle this challenge, we propose a novel early-pruning approach to improve efficiency, where we collect different types of content metadata and schema metadata about the datasets, and then use this metadata in early-pruning steps to pre-filter the schema matching comparisons. This involves computing proximities between datasets based on their metadata, discovering their relationships based on overall proximities and proposing similar dataset pairs for schema matching. We improve the effectiveness of this task by introducing a supervised mining approach for effectively detecting similar datasets that are proposed for further schema matching. We conduct extensive experiments on a real-world DL that proves the success of our approach in effectively detecting similar datasets for schema matching, with recall rates of more than 85% and efficiency improvements above 70%. We empirically show the computational cost saving in space and time by applying our approach in comparison to instance-based schema matching techniques.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3388870',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Analysis of Hotspot Data for Drought Clustering Using K-Means Algorithm',\n",
       "  'authors': \"['Ekki Rizki Ramadhan', 'Edi Sutoyo', 'Ahmad Musnansyah', 'Halda Aditya Belgaman']\",\n",
       "  'date': 'September 2020',\n",
       "  'source': \"ICONETSI '20: Proceedings of the 2020 International Conference on Engineering and Information Technology for Sustainable Industry\",\n",
       "  'abstract': \"Drought is a disaster that is often experienced in Indonesia. This disaster occurred because Indonesia's geographical location is on the equator. Drought has had a major impact on the community such as crop failure, forest fires, soil damage, the emergence of disease outbreaks, and the extinction of animals and plants. Based on data from the Ministry of Environment of the Republic of Indonesia, the distribution of Riau's hotspots is quite unique. It is said so, because in this distribution, Riau has increased in every February and March as many as 277 and 248 hotspots in the last two years, namely between 2018 and 2019. To anticipate the drought that occurred in Riau, the clustering of drought-prone areas was conducted based on the analysis of hotspots data. This clustering of vulnerable areas is done by the K-Means algorithm. In determining the number of clusters of vulnerable areas, the elbow method is used as a determinant and produces as many as 4 cluster. The results of these method were analyzed by the silhouette coefficient. The result of analyzed is 0.388632163 and were classified as well-clustered. From these results, Rokan Hilir, Bengkalis, Kota Dumai are the dangerous district with 3106, 2361, and 117 point of dangerous distribution, respectively.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3429789.3429824',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Mining Rank Data',\n",
       "  'authors': \"['Sascha Henzgen', 'Eyke Hüllermeier']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Knowledge Discovery from Data',\n",
       "  'abstract': 'The problem of frequent pattern mining has been studied quite extensively for various types of data, including sets, sequences, and graphs. Somewhat surprisingly, another important type of data, namely rank data, has received very little attention in data mining so far. In this article, we therefore address the problem of mining rank data, that is, data in the form of rankings (total orders) of an underlying set of items. More specifically, two types of patterns are considered, namely frequent rankings and dependencies between such rankings in the form of association rules. Algorithms for mining frequent rankings and frequent closed rankings are proposed and tested experimentally, using both synthetic and real data.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3363572',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Using Classification Technique for Customer Relationship Management based on Thai Social Media Data',\n",
       "  'authors': \"['Todsanai Chumwatana', 'Karnsiree Wongkolkitsilp']\",\n",
       "  'date': 'February 2019',\n",
       "  'source': 'ICCAE 2019: Proceedings of the 2019 11th International Conference on Computer and Automation Engineering',\n",
       "  'abstract': 'Many businesses nowadays use social media as a main channel to connect with their customers. To utilize the data gained from social media, businesses can manage the relationship with customers effectively. This paper proposed the technique classifying the customer intentions into 2 groups; purchase intention and quit intention, by analyzing the text-based data collected from social media platforms. The analyzing process consists of 4 steps including source identification, data extraction, data preparation, and data classification. A thousand of Thai social media texts were used as an input dataset for training and testing steps, which apply to two classification models: Naïve Bayes and Support Vector Machine (SVM). The accuracy performance of SVM model is 78.1% while Naïve Bayes provided 63.4% accuracy. As a result, from the large number of texts posted and commented on social media, the businesses can identify their customers intention in order to manage the relationship with them; by approaching the customers who intend to purchase with a sales offer as well as solving the problems for those who are quitting to reduce the customer churn rate.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3313991.3314010',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Coupled Clustering Ensemble by Exploring Data Interdependence',\n",
       "  'authors': \"['Can Wang', 'Chi-Hung Chi', 'Zhong She', 'Longbing Cao', 'Bela Stantic']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Knowledge Discovery from Data',\n",
       "  'abstract': 'Clustering ensembles combine multiple partitions of data into a single clustering solution. It is an effective technique for improving the quality of clustering results. Current clustering ensemble algorithms are usually built on the pairwise agreements between clusterings that focus on the similarity via consensus functions, between data objects that induce similarity measures from partitions and re-cluster objects, and between clusters that collapse groups of clusters into meta-clusters. In most of those models, there is a strong assumption on IIDness (i.e., independent and identical distribution), which states that base clusterings perform independently of one another and all objects are also independent. In the real world, however, objects are generally likely related to each other through features that are either explicit or even implicit. There is also latent but definite relationship among intermediate base clusterings because they are derived from the same set of data. All these demand a further investigation of clustering ensembles that explores the interdependence characteristics of data. To solve this problem, a new coupled clustering ensemble (CCE) framework that works on the interdependence nature of objects and intermediate base clusterings is proposed in this article. The main idea is to model the coupling relationship between objects by aggregating the similarity of base clusterings, and the interactive relationship among objects by addressing their neighborhood domains. Once these interdependence relationships are discovered, they will act as critical supplements to clustering ensembles. We verified our proposed framework by using three types of consensus function: clustering-based, object-based, and cluster-based. Substantial experiments on multiple synthetic and real-life benchmark datasets indicate that CCE can effectively capture the implicit interdependence relationships among base clusterings and among objects with higher clustering accuracy, stability, and robustness compared to 14 state-of-the-art techniques, supported by statistical analysis. In addition, we show that the final clustering quality is dependent on the data characteristics (e.g., quality and consistency) of base clusterings in terms of sensitivity analysis. Finally, the applications in document clustering, as well as on the datasets with much larger size and dimensionality, further demonstrate the effectiveness, efficiency, and scalability of our proposed models.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3230967',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A clustering-based rule-mining approach for monitoring long-term energy use and understanding system behavior',\n",
       "  'authors': \"['Seyed Hamid Mirebrahim', 'Mohammad Shokoohi-Yekta', 'Unmesh Kurup', 'Torsten Welfonder', 'Mohak Shah']\",\n",
       "  'date': 'November 2017',\n",
       "  'source': \"BuildSys '17: Proceedings of the 4th ACM International Conference on Systems for Energy-Efficient Built Environments\",\n",
       "  'abstract': 'We describe a data mining approach to discover possible explanations for long-term energy consumption patterns in commercial and residential buildings. Our approach uses clustering to identify interesting patterns in energy data and correlates these patterns to other sensor information. These correlations, written in the form of rules, provide potential explanations for the patterns. Our approach is different from existing approaches in a number of ways: First, we apply these techniques to producing explanatory rules in long-term energy usage for large datasets. Second, we use clustering to find interesting patterns and provide explanatory rules about these patterns by applying rule mining on a dataset made up of secondary information (including temporal ranges and other building sensors) that include these cluster ids. Finally, we include in our analysis the list of rules that are exclusive to each cluster. We show that our approach for finding the rules is capable of finding useful explanatory rules for a real dataset.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3137133.3137144',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': \"Mining Domain Terminologies Using Search Engine's Query Log\",\n",
       "  'authors': \"['Weijian Ni', 'Tong Liu', 'Qingtian Zeng', 'Nengfu Xie']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Asian and Low-Resource Language Information Processing',\n",
       "  'abstract': \"Domain terminologies are a basic resource for various natural language processing tasks. To automatically discover terminologies for a domain of interest, most traditional approaches mostly rely on a domain-specific corpus given in advance; thus, the performance of traditional approaches can only be guaranteed when collecting a high-quality domain-specific corpus, which requires extensive human involvement and domain expertise. In this article, we propose a novel approach that is capable of automatically mining domain terminologies using search engine's query log—a type of domain-independent corpus of higher availability, coverage, and timeliness than a manually collected domain-specific corpus. In particular, we represent query log as a heterogeneous network and formulate the task of mining domain terminology as transductive learning on the heterogeneous network. In the proposed approach, the manifold structure of domain-specificity inherent in query log is captured by using a novel network embedding algorithm and further exploited to reduce the need for the manual annotation efforts for domain terminology classification. We select Agriculture and Healthcare as the target domains and experiment using a real query log from a commercial search engine. Experimental results show that the proposed approach outperforms several state-of-the-art approaches.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3462327',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'An evidence-based approach to mining patterns',\n",
       "  'authors': \"['Michael Weiss']\",\n",
       "  'date': 'March 2017',\n",
       "  'source': \"VikingPLoP '17: Proceedings of the VikingPLoP 2017 Conference on Pattern Languages of Program\",\n",
       "  'abstract': 'In this paper, we experiment with an evidence-based approach to mining patterns. The goal of the approach is to support pattern discovery from design documentation. The approach is semi-automated: semantic word clouds are generated from the design documentation and then examined by a domain expert for interesting configurations of design elements. These configurations are expected to indicate elements of pattern candidates like the solution, problem, or context. Unlike regular word clouds, which are purely visual, semantic word clouds preserve semantic relationships in the underlying text. Hence, pattern elements found in close proximity in the same word cloud can be expected to be related. Clusters of pattern elements can be interpreted as the core of a pattern to be mined. The approach will be tested using design documentation for several projects related to the design of online communities. As a text-based approach, the approach is expected to be useful for pattern discovery in software architecture, high-level designs, requirements, as well as business models.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3158491.3158492',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Learning from Multi-annotator Data: A Noise-aware Classification Framework',\n",
       "  'authors': \"['Xueying Zhan', 'Yaowei Wang', 'Yanghui Rao', 'Qing Li']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Information Systems',\n",
       "  'abstract': 'In the field of sentiment analysis and emotion detection in social media, or other tasks such as text classification involving supervised learning, researchers rely more heavily on large and accurate labelled training datasets. However, obtaining large-scale labelled datasets is time-consuming and high-quality labelled datasets are expensive and scarce. To deal with these problems, online crowdsourcing systems provide us an efficient way to accelerate the process of collecting training data via distributing the enormous tasks to various annotators to help create large amounts of labelled data at an affordable cost. Nowadays, these crowdsourcing platforms are heavily needed in dealing with social media text, since the social network platforms (e.g., Twitter) generate huge amounts of data in textual form everyday. However, people from different social and knowledge backgrounds have different views on various texts, which may lead to noisy labels. The existing noisy label aggregation/refinement algorithms mostly focus on aggregating labels from noisy annotations, which would not guarantee their effectiveness on the subsequent classification/ranking tasks. In this article, we propose a noise-aware classification framework that integrates the steps of noisy label aggregation and classification. The aggregated noisy crowd labels are fed into a classifier for training, while the predicted labels are employed as feedback for adjusting the parameters at the label aggregating stage. The classification framework is suitable for directly running on crowdsourcing datasets and applies to various kinds of classification algorithms. The feedback strategy makes it possible for us to find optimal parameters instead of using known data for parameter selection. Simulation experiments demonstrate that our method provide significant label aggregation performance for both binary and multiple classification tasks under various noisy environments. Experimenting on real-world data validates the feasibility of our framework in real noise data and helps us verify the reasonableness of the simulated experiment settings.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3309543',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A Dictionary-Based Method for Classification with Universum Data',\n",
       "  'authors': \"['Zhiyong Che', 'Bo Liu', 'Yanshan Xiao']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"CSAE '21: Proceedings of the 5th International Conference on Computer Science and Application Engineering\",\n",
       "  'abstract': 'In fact, the collected examples included the third-class examples, they do not belong to positive samples or negative samples, which are referred as the Universum data. And Universum data can make better performance for the classifier. In this paper, a dictionary-based method for classification with Universum data is proposed to construct a unified model. In the proposed method, we embed the dictionary and Universum data to construct a unified framework, and the Universum data is introduced into the framework by the ɛ-insensitive loss. For the optimization, the SVD algorithm and gradient-based optimization methods are utilized to alternately optimize and update the dictionary, and the Lagrangian function is used to iteratively optimize the unified framework to obtain the classifier. Finally, extensive experiments are conducted on the benchmark datasets to evaluate the performance of the proposed U-DL method and baselines. The results have shown that the proposed U-DL method makes better performance than previous methods.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3487075.3487115',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Statistical guarantees for local spectral clustering on random neighborhood graphs',\n",
       "  'authors': \"['Alden Green', 'Sivaraman Balakrishnan', 'Ryan J. Tibshirani']\",\n",
       "  'date': 'None',\n",
       "  'source': 'The Journal of Machine Learning Research',\n",
       "  'abstract': 'We study the Personalized PageRank (PPR) algorithm, a local spectral method for clustering, which extracts clusters using locally-biased random walks around a given seed node. In contrast to previous work, we adopt a classical statistical learning setup, where we obtain samples from an unknown nonparametric distribution, and aim to identify sufficiently salient clusters. We introduce a trio of population-level functionals--the normalized cut, conductance, and local spread, analogous to graph-based functionals of the same name--and prove that PPR, run on a neighborhood graph, recovers clusters with small population normalized cut and large conductance and local spread. We apply our general theory to establish that PPR identifies connected regions of high density (density clusters) that satisfy a set of natural geometric conditions. We also show a converse result, that PPR can fail to recover geometrically poorly-conditioned density clusters, even asymptotically. Finally, we provide empirical support for our theory.',\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3546258.3546505',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Abnormal Data Classification Using Time-Frequency Temporal Logic',\n",
       "  'authors': \"['Luan Viet Nguyen', 'James Kapinski', 'Xiaoqing Jin', 'Jyotirmoy V. Deshmukh', 'Ken Butts', 'Taylor T. Johnson']\",\n",
       "  'date': 'April 2017',\n",
       "  'source': \"HSCC '17: Proceedings of the 20th International Conference on Hybrid Systems: Computation and Control\",\n",
       "  'abstract': 'We present a technique to investigate abnormal behaviors of signals in both time and frequency domains using an extension of time-frequency logic that uses the continuous wavelet transform. Abnormal signal behaviors such as unexpected oscillations, called hunting behavior, can be challenging to capture in the time domain; however, these behaviors can be naturally captured in the time-frequency domain. We introduce the concept of parametric time-frequency logic and propose a parameter synthesis approach that can be used to classify hunting behavior. We perform a comparative analysis between the proposed algorithm, an approach based on support vector machines using linear classification, and a method that infers a signal temporal logic formula as a data classifier. We present experimental results based on data from a hydrogen fuel cell vehicle application and electrocardiogram data extracted from the MIT-BIH Arrhythmia Database.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3049797.3049809',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': '150 Successful Machine Learning Models: 6 Lessons Learned at Booking.com',\n",
       "  'authors': \"['Lucas Bernardi', 'Themistoklis Mavridis', 'Pablo Estevez']\",\n",
       "  'date': 'July 2019',\n",
       "  'source': \"KDD '19: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining\",\n",
       "  'abstract': \"Booking.com is the world's largest online travel agent where millions of guests find their accommodation and millions of accommodation providers list their properties including hotels, apartments, bed and breakfasts, guest houses, and more. During the last years we have applied Machine Learning to improve the experience of our customers and our business. While most of the Machine Learning literature focuses on the algorithmic or mathematical aspects of the field, not much has been published about how Machine Learning can deliver meaningful impact in an industrial environment where commercial gains are paramount. We conducted an analysis on about 150 successful customer facing applications of Machine Learning, developed by dozens of teams in Booking.com, exposed to hundreds of millions of users worldwide and validated through rigorous Randomized Controlled Trials. Following the phases of a Machine Learning project we describe our approach, the many challenges we found, and the lessons we learned while scaling up such a complex technology across our organization. Our main conclusion is that an iterative, hypothesis driven process, integrated with other disciplines was fundamental to build 150 successful products enabled by Machine Learning.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3292500.3330744',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Melanoma Segmentation and Classification in Clinical Images Using Deep Learning',\n",
       "  'authors': \"['Yunhao Ge', 'Bin Li', 'Yanzheng Zhao', 'Enguang Guan', 'Weixin Yan']\",\n",
       "  'date': 'February 2018',\n",
       "  'source': \"ICMLC '18: Proceedings of the 2018 10th International Conference on Machine Learning and Computing\",\n",
       "  'abstract': 'In this paper, a deep learning computer aided diagnosis system (CADs) is proposed for automatic segmentation and classification of melanoma lesions, containing a fully convolutional neural network (FCN) and a specific convolutional neural network (CNN). FCN, which consists of a 28-layer neural structure, is designed for segmentation and with a mask for region of interest (ROI) as its output. Later, the CNN only uses the segmented ROI of raw image to extract features, while the DLCM features, statistical and contrast location features extracted from same ROI are merged into CNN features. Finally, the combined features are utilized by the fully connected layers in CNN to obtain the final classification of melanoma, malignant or benign. The training of FCN and CNN are separated with different loss functions. Publicly available database ISBI 2016 is used for evaluating the effectiveness, efficiency, and generalization capability with evaluating indicator, such as accuracy, precision, and recall. Preprocessing methods, such as data argumentation and balancing are utilized to make further improvements to performance. Experiments on a batch size of 100 images yielded an accuracy of 92%, a specificity of 93% and a sensitivity of 94%, revealing that the proposed system is superior in terms of diagnostic accuracy in comparison with the state-of-the-art methods.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3195106.3195164',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Deep learning-based defective product classification system for smart factory',\n",
       "  'authors': \"['Huy Toan Nguyen', 'Nu-ri Shin', 'Gwang-Hyun Yu', 'Gyeong-Ju Kwon', 'Woo-Young Kwak', 'Jin-Young Kim']\",\n",
       "  'date': 'September 2020',\n",
       "  'source': 'SMA 2020: The 9th International Conference on Smart Media and Applications',\n",
       "  'abstract': 'In this paper, the defective product classification based on deep learning for a smart factory is introduced. The proposed system contains PLC (Programmable Logic Controller), Artificial Intelligence (AI) embedded board and cloud service. The AI embedded board is connected and communicated to receive and send commands to PLC via SPI (Serial Peripheral Interface) protocol. The pre-trained defective product classification model is uploaded, saved on a cloud server and downloaded to AI Embedded board for each particular product. The core technique of the system is the AI-based embedded board. Due to the limitation of label data, we use transfer learning method to retrain deep neural networks (DNN). We implement and compare the classification results on different deep neural network including ResNet, DenseNet, and GoogLeNet. We trained these networks by GPU server on casting product classification data. After that, the pre-trained models are optimized and applied on practical embedded board. The experimental results show that our system is able to classify defective products with high accuracy and fast speed.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3426020.3426039',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': \"Mining Activity Log Data to Predict Student's Outcome in a Course\",\n",
       "  'authors': \"['Rahila Umer', 'Anuradha Mathrani', 'Teo Susnjak', 'Suriadi Lim']\",\n",
       "  'date': 'March 2019',\n",
       "  'source': \"ICBDE '19: Proceedings of the 2019 International Conference on Big Data and Education\",\n",
       "  'abstract': \"Use of learning management system (LMS) is very common, which provide support to teaching staff for communication, delivery of resources and in design of learning activities. The wide spread use of technologies like LMS, provide large amount of data. Research shows that higher education institutes can make use of this data to extract data-driven insights to understand the learning process and benefit the students by supporting them in their academics. In this study we used several machine learning algorithms to predict student's outcome in a course using LMS trace data and assessment scores. Selection of the courses is based on the extent the LMS is used and is divided into two categories; distance and internal. This study confirms the importance of LMS data and assessment scores in the prediction of academic performance. However, frequent use of LMS may increase the trace data but it is not necessary improve the predictive accuracy. Predictive models developed for courses, without considering the context of use of LMS data, may not generalize the effects of LMS trace data on student's outcome in the course.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3322134.3322140',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Mining Twitter data for a more responsive software engineering process',\n",
       "  'authors': \"['Grant Williams', 'Anas Mahmoud']\",\n",
       "  'date': 'May 2017',\n",
       "  'source': \"ICSE-C '17: Proceedings of the 39th International Conference on Software Engineering Companion\",\n",
       "  'abstract': 'Twitter has created an unprecedented opportunity for software developers to monitor the opinions of large populations of end-users of their software. However, automatically classifying useful tweets is not a trivial task. Challenges stem from the scale of the data available, its unique format, diverse nature, and high percentage of spam. To overcome these challenges, this extended abstract introduces a three-fold procedure that is aimed at leveraging Twitter as a main source of technical feedback that software developers can benefit from. The main objective is to enable a more responsive, interactive, and adaptive software engineering process. Our analysis is conducted using a dataset of tweets collected from the Twitter feeds of three software systems. Our results provide an initial proof of the technical value of software-relevant tweets and uncover several challenges to be pursued in our future work.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ICSE-C.2017.53',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Research on Comprehensive Potato Classification Based on Support Vector Machine',\n",
       "  'authors': \"['Wenhu Nan', 'Yanjun Liu', 'Shuzhen Zhang']\",\n",
       "  'date': 'January 2021',\n",
       "  'source': 'CONF-CDS 2021: The 2nd International Conference on Computing and Data Science',\n",
       "  'abstract': 'In order to identify the potato accurately and quickly, this paper uses saturation and support vector machine to classify potatoes. Firstly, we collect 90 potato image samples, and then use the method of extracting the S component to filter out the unusable potato targets due to budding. Secondly, the R, G and B components are extracted and to perform R+G which is gray-scaled. The point noise is removed through corrosion expansion, then we use the boundary function for finding the image boundary. Finally, according to the rectangle degree and other parameters as the shape feature, the support vector machine is trained to classify the potatoes. The experimental results show that this method can classify potatoes effectively in potato shape and budding, it meets the demand for potato commercial production.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3448734.3450929',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'An Efficient Angle-based Universum Least Squares Twin Support Vector Machine for Classification',\n",
       "  'authors': \"['B. Richhariya', 'M. Tanveer', 'Alzheimer’s Disease Neuroimaging InitiativeDiscipline of Mathematics, Indian Institute of Technology Indore, Simrol, Indore, IndiaProgram']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Internet Technology',\n",
       "  'abstract': 'Universum-based support vector machine incorporates prior information about the distribution of data in training of the classifier. This leads to better generalization performance but with increased computation cost. Various twin hyperplane-based models are proposed to reduce the computation cost of universum-based algorithms. In this work, we present an efficient angle-based universum least squares twin support vector machine (AULSTSVM) for classification. This is a novel approach of incorporating universum in the formulation of least squares-based twin SVM model. First, the proposed AULSTSVM constructs a universum hyperplane, which is proximal to universum data points. Then, the classifying hyperplane is constructed by minimizing the angle with the universum hyperplane. This gives prior information about data distribution to the classifier. In addition to the quadratic loss, we introduce linear loss in the optimization problem of the proposed AULSTSVM, which leads to lesser computation cost of the model. Numerical experiments are performed on several benchmark synthetic, real-world, and large-scale datasets. The results show that proposed AULSTSVM performs better than existing algorithms w.r.t. generalization performance as well as computation time. Moreover, an application to Alzheimer’s disease is presented, where AULSTSVM obtains accuracy of 95% for classification of healthy and Alzheimers subjects. The results imply that the proposed AULSTSVM is a better alternative for classification of large-scale datasets and biomedical applications.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3387131',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Design of Intelligent Recognition English Translation Model based on Association Rule Mining',\n",
       "  'authors': \"['Kang Sun']\",\n",
       "  'date': 'December 2021',\n",
       "  'source': 'ICASIT 2021: 2021 International Conference on Aviation Safety and Information Technology',\n",
       "  'abstract': 'Due to the rapid development of globalization, the information flow between different countries shows high speed, and English has become the main language of international communication. At present, the application value of intelligent recognition technology in different fields is increasing. The English machine translation model based on modern intelligent recognition technology can improve the efficiency and accuracy of English machine translation and realize barrier free communication. However, the traditional English machine translation method based on syntactic analysis can not solve the problem of partial structural ambiguity in the massive English language in intelligent recognition technology, which has the problem of low accuracy of machine translation. With the development of modern intelligent recognition technology, there are many intelligent machine translation tools. The current machine translation results of online machine translation still have some defects, especially after the server is used to carry out comparative learning on data in different languages in the full text range, it can obtain the grammar and text correlation laws between languages, which has the disadvantages of low efficiency and low accuracy of machine translation. Therefore, the recognizable technology of association rule mining should be used to realize accurate machine translation of English.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3510858.3511426',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Transfer Learning for Multi-language Twitter Election Classification',\n",
       "  'authors': \"['Xiao Yang', 'Richard McCreadie', 'Craig Macdonald', 'Iadh Ounis']\",\n",
       "  'date': 'July 2017',\n",
       "  'source': \"ASONAM '17: Proceedings of the 2017 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 2017\",\n",
       "  'abstract': 'Both politicians and citizens are increasingly embracing social media as a means to disseminate information and comment on various topics, particularly during significant political events, such as elections. Such commentary during elections is also of interest to social scientists and pollsters. To facilitate the study of social media during elections, there is a need to automatically identify posts that are topically related to those elections. However, current studies have focused on elections within English-speaking regions, and hence the resultant election content classifiers are only applicable for elections in countries where the predominant language is English. On the other hand, as social media is becoming more prevalent worldwide, there is an increasing need for election classifiers that can be generalised across different languages, without building a training dataset for each election. In this paper, based upon transfer learning, we study the development of effective and reusable election classifiers for use on social media across multiple languages. We combine transfer learning with different classifiers such as Support Vector Machines (SVM) and state-of-the-art Convolutional Neural Networks (CNN), which make use of word embedding representations for each social media post. We generalise the learned classifier models for cross-language classification by using a linear translation approach to map the word embedding vectors from one language into another. Experiments conducted over two election datasets in different languages show that without using any training data from the target language, linear translations outperform a classical transfer learning approach, namely Transfer Component Analysis (TCA), by 80% in recall and 25% in F1 measure.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3110025.3110059',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Hybrid Classification and Clustering Algorithm on Recent Android Malware Detection',\n",
       "  'authors': \"['jiezhong xiao', 'qian han', 'yumeng gao']\",\n",
       "  'date': 'December 2021',\n",
       "  'source': \"CSAI '21: Proceedings of the 2021 5th International Conference on Computer Science and Artificial Intelligence\",\n",
       "  'abstract': 'With the explosion in the popularity of smartphones over the previous decade, mobile malware appears to be unavoidable. Because Android is an open platform that is fast dominating other rival platforms (e.g. iOS) in the mobile smart device industry, Android malware has been much more widespread. Recent Android malware developers have more advanced capabilities when building their malicious apps, which make the apps themselves much more difficult to detect using conventional methods. In our paper, we proposed a hybrid machine learning classification and clustering algorithm to detect recent Android malware. The proposed algorithm performs better than the state-of-art algorithms with both F1-score and recall of 0.9944. More importantly, the top features returned by our algorithm clearly explain the important factors in the detection task. They can not only be used for enhanced Android malware detection but also quicker white-box analysis by means of more interpretable results.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3507548.3507586',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Vertica-ML: Distributed Machine Learning in Vertica Database',\n",
       "  'authors': \"['Arash Fard', 'Anh Le', 'George Larionov', 'Waqas Dhillon', 'Chuck Bear']\",\n",
       "  'date': 'June 2020',\n",
       "  'source': \"SIGMOD '20: Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data\",\n",
       "  'abstract': 'A growing number of companies rely on machine learning as a key element for gaining a competitive edge from their collected Big Data. An in-database machine learning system can provide many advantages in this scenario, e.g., eliminating the overhead of data transfer, avoiding the maintenance costs of a separate analytical system, and addressing data security and provenance concerns. In this paper, we present our distributed machine learning subsystem within the Vertica database. This subsystem, Vertica-ML, includes machine learning functionalities with SQL API which cover a complete data science workflow as well as model management. We treat machine learning models in Vertica as first-class database objects like tables and views; therefore, they enjoy a similar mechanism for archiving and managing. We explain the architecture of the subsystem, and present a set of experiments to evaluate the performance of the machine learning algorithms implemented on top of it.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3318464.3386137',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Unbalanced data sentiment classification method based on ensemble learning',\n",
       "  'authors': \"['Jidong Duan', 'Kun Ma', 'Runyuan Sun']\",\n",
       "  'date': 'August 2019',\n",
       "  'source': \"ICBDT '19: Proceedings of the 2nd International Conference on Big Data Technologies\",\n",
       "  'abstract': 'Sentiment classification is a hot research direction at present, but most research is based on balanced data sets. In real life, the sample is impossible to balance. For sentiment analysis of unbalanced data, we not only need to pay attention to the overall classification performance, but also need to care about the classification performance of a few classes. How to improve the recognition rate of a few types of samples while improving the overall recognition rate has become a research hotspot. Aiming at this problem, this paper proposes a model based on ensemble learning, extracts features by TF-IDF+SVD, and integrates five base classifiers by stacking to sentiment classification. The experimental results show that it can be more effective in emotional classification on unbalanced data sets than other methods.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3358528.3358597',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Differentially private sequential pattern mining considering time interval for electronic medical record systems',\n",
       "  'authors': \"['Hieu Hanh Le', 'Muneo Kushima', 'Kenji Araki', 'Haruo Yokota']\",\n",
       "  'date': 'June 2019',\n",
       "  'source': \"IDEAS '19: Proceedings of the 23rd International Database Applications &amp; Engineering Symposium\",\n",
       "  'abstract': 'Electronic medical record (EMR) systems have now been widely adopted to support medical workers. There also has been much interest in the machine-based generation of clinical pathways that can utilize sequential pattern mining (SPM) to extract them from historical EMR systems. However, the existing methods do not protect individual privacy, even though they involve sensitive medical data. To ensure the privacy of individual data, this paper describes two algorithms that deploy differential privacy by adding noise during calculations in the SPM considering time interval for guaranteeing privacy. The proposals can limit the amount of added noise by adding noise to the frequency calculations of only a part of candidate closed sequences. Experiments on real medical datasets show that our proposal can ensure the robust and high utility of mining process even with minimum privacy budget and amount of added noise.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3331076.3331098',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Research on outlier detection of data based on machine learning',\n",
       "  'authors': \"['Chunyang Wang']\",\n",
       "  'date': 'July 2021',\n",
       "  'source': \"ACM TURC '21: Proceedings of the ACM Turing Award Celebration Conference - China\",\n",
       "  'abstract': 'With the increasing magnitude of data, the accuracy of single data cannot be guaranteed. In order to improve the accuracy of model prediction data, and further for the subsequent data processing, this paper focuses on the detection of data on outliers. This paper introduces the density clustering and outlier detection methods of Isolation Forest in outlier recognition, and mainly describes the principle and process of outlier recognition using density clustering and Isolation Forest. Based on this, according to the features of data, an improved algorithm combining density clustering and Isolation Forest is proposed. Finally, through the existing common outlier detection data set, the statistical outlier recognition method, the existing machine learning algorithm and the improved algorithm proposed in this paper are compared to identify outliers, which verifies the stability of the proposed method and the effective improvement compared with the existing outlier detection.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3472634.3474072',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Evolutionary Classifier and Cluster Selection Approach for Ensemble Classification',\n",
       "  'authors': \"['Zohaib Md. Jan', 'Brijesh Verma']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Knowledge Discovery from Data',\n",
       "  'abstract': 'Ensemble classifiers improve the classification performance by combining several classifiers using a suitable fusion methodology. Many ensemble classifier generation methods have been developed that allowed the training of multiple classifiers on a single dataset. As such random subspace is a common methodology utilized by many state-of-the-art ensemble classifiers that generate random subsamples from the input data and train classifiers on different subsamples. Real-world datasets have randomness and noise in them, therefore not all randomly generated samples are suitable for training. In this article, we propose a novel particle swarm optimization-based approach to optimize the random subspace to generate an ensemble classifier. We first generate a random subspace by incrementally clustering input data and then optimize all generated data clusters. On all optimized data clusters, a set of classifiers is trained and added to the pool. The pool of classifiers is then optimized and an optimized ensemble classifier is generated. The proposed approach is tested on 12 benchmark datasets from the UCI repository and results are compared with current state-of-the-art ensemble classifier approaches. A statistical significance test is also conducted and an analysis is presented.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3366633',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Differentially private sequential pattern mining considering time interval for electronic medical record systems',\n",
       "  'authors': \"['Hieu Hanh Le', 'Muneo Kushima', 'Kenji Araki', 'Haruo Yokota']\",\n",
       "  'date': 'June 2019',\n",
       "  'source': \"IDEAS '19: Proceedings of the 23rd International Database Applications &amp; Engineering Symposium\",\n",
       "  'abstract': 'Electronic medical record (EMR) systems have now been widely adopted to support medical workers. There also has been much interest in the machine-based generation of clinical pathways that can utilize sequential pattern mining (SPM) to extract them from historical EMR systems. However, the existing methods do not protect individual privacy, even though they involve sensitive medical data. To ensure the privacy of individual data, this paper describes two algorithms that deploy differential privacy by adding noise during calculations in the SPM considering time interval for guaranteeing privacy. The proposals can limit the amount of added noise by adding noise to the frequency calculations of only a part of candidate closed sequences. Experiments on real medical datasets show that our proposal can ensure the robust and high utility of mining process even with minimum privacy budget and amount of added noise.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3331076.3331098',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Mining Implicit Entity Preference from User-Item Interaction Data for Knowledge Graph Completion via Adversarial Learning',\n",
       "  'authors': \"['Gaole He', 'Junyi Li', 'Wayne Xin Zhao', 'Peiju Liu', 'Ji-Rong Wen']\",\n",
       "  'date': 'April 2020',\n",
       "  'source': \"WWW '20: Proceedings of The Web Conference 2020\",\n",
       "  'abstract': 'The task of Knowledge Graph Completion\\xa0(KGC) aims to automatically infer the missing fact information in Knowledge Graph (KG). In this paper, we take a new perspective that aims to leverage rich user-item interaction data (user interaction data for short) for improving the KGC task. Our work is inspired by the observation that many KG entities correspond to online items in application systems. However, the two kinds of data sources have very different intrinsic characteristics, and it is likely to hurt the original performance using simple fusion strategy.  To address this challenge, we propose a novel adversarial learning approach by leveraging user interaction data for the KGC task. Our generator is isolated from user interaction data, and serves to improve the performance of the discriminator. The discriminator takes the learned useful information from user interaction data as input, and gradually enhances the evaluation capacity in order to identify the fake samples generated by the generator. To discover implicit entity preference of users, we design an elaborate collaborative learning algorithms based on graph neural networks, which will be jointly optimized with the discriminator. Such an approach is effective to alleviate the issues about data heterogeneity and semantic complexity for the KGC task. Extensive experiments on three real-world datasets have demonstrated the effectiveness of our approach on the KGC task.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3366423.3380155',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Coupling topic modelling in opinion mining for social media analysis',\n",
       "  'authors': \"['Xujuan Zhou', 'Xiaohui Tao', 'Md Mostafijur Rahman', 'Ji Zhang']\",\n",
       "  'date': 'August 2017',\n",
       "  'source': \"WI '17: Proceedings of the International Conference on Web Intelligence\",\n",
       "  'abstract': 'Many of social media platforms such as Facebook and Twitter make it easy for everyone to share their thoughts on literally anything. Topic and opinion detection in social media facilitates the identification of emerging societal trends, analysis of public reactions to policies and business products. In this paper, we proposed a new method that combines the opining mining and context-based topic modelling to analyse public opinions on social media data. Context based topic modelling is used to categorise data in groups and discover hidden communities in data group. The unwanted data group discovered by the topic model then will be discarded. A lexicon based opinion mining method will be applied to the remaining data groups to spot out the public sentiment about the entities. A set of Tweets data on Australian Federal Election 2010 was used in our experiments. Our experimental results demonstrate that, with the help of topic modelling, our social media analysis model is accurate and effective.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3106426.3106459',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Utilizing the buckshot algorithm for efficient big data clustering in the MapReduce model',\n",
       "  'authors': \"['Sergios Gerakidis', 'Basilis Mamalis']\",\n",
       "  'date': 'November 2019',\n",
       "  'source': \"PCI '19: Proceedings of the 23rd Pan-Hellenic Conference on Informatics\",\n",
       "  'abstract': 'Clustering is an efficient data mining as well as machine-learning method when we need to get an insight of the objects of a dataset that could be grouped together. The K-Means algorithm and the Hierarchical Agglomerative Clustering (HAC) algorithm are two of the most known and commonly used methods of clustering; the former due to its low time cost and the latter due to its accuracy. However, even the use of K-Means in document clustering over large-scale collections can lead to unpredictable time costs. In this paper, towards the direction of the efficient handling of big text data, we present a hybrid clustering approach based on a customized version of the Buckshot algorithm, which first applies a hierarchical clustering procedure on a sample of the input dataset and then uses the results as the initial centers for a K-Means based assignment of the remaining documents, with very few iterations. We also give a highly efficient adaptation of the proposed Buckshot-based approach in the MapReduce model which is then experimentally tested using Apache Hadoop over a real cluster environment. As it comes out of the experiments, it leads to acceptable clustering quality as well as to significant execution time improvements. Preliminary results drawn from relevant experiments using the Spark framework are also presented.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3368640.3368658',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': \"Utilizing cost-sensitive machine learning classifiers to identify compounds that inhibit Alzheimer's APP translation\",\n",
       "  'authors': \"['Hany Alashwal', 'Juwayni Lucman']\",\n",
       "  'date': 'August 2020',\n",
       "  'source': \"ICCBDC '20: Proceedings of the 2020 4th International Conference on Cloud and Big Data Computing\",\n",
       "  'abstract': 'Virtual screening of bioassay data can be of immense benefit to identify compounds which can assist in restricting the production of amyloid beta peptides (Aβ), observed in Alzheimer patients, by inhibiting the translation of amyloid precursor protein (APP). Machine learning classifiers can be adopted on the dataset to investigate those compounds. The ratio of the active molecules that achieve the goal of inhibiting APP, nonetheless, is minimal compared to their inactive counterparts. The imbalance between the two classes is handled by introducing cost-sensitivity to reweight the training instances depending on the misclassification cost allotted to each class. The paper shows the performance of cost-sensitive classifiers (Random Forest, Naive Bayes, and Logistic Regression classifier) to spot the minority (active) molecules from the majority (inactive) classes and shows their evaluation metrics. Sensitivity, specificity, False Negative rate, ROC area, and accuracy are evaluated while keeping the False Positive rate at 20.6%. The aim of the study is to investigate the most reliable classifier for the bioassay data and to explore the ideal misclassification cost. Random Forest classifier was the most robust model compared to Naive Bayes and Logistic Regression Classifiers. Moreover, each classifier had a different optimal misclassification cost.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3416921.3416931',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Enhanced Data Mining Technique to Measure Satisfaction Degree of Social Media Users of Xeljanz Drug',\n",
       "  'authors': \"['M. M. Abd-Elaziz', 'Hazem M. El-Bakry', 'Ahmed Abou Elfetouh', 'Amira Elzeiny']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Knowledge Discovery from Data',\n",
       "  'abstract': 'In the recent times, social media has become important in the field of health care as a major resource of valuable health information. Social media can provide massive amounts of data in real-time through user interaction, and this data can be analysed to reflect the harms and benefits of treatment by using the personal health experiences of users to improve health outcomes. In this study, we propose an enhanced data mining framework for analysing user opinions on Twitter and on a health-care forum. The proposed framework measures the degree of satisfaction of consumers regarding the drug Xeljanz, which is used to treat rheumatoid arthritis. The proposed framework is based on seven steps distributed in two phases. The first phase involves aggregating data related to the drug Xeljanz. This data is pre-processed to produce a list of words with a term frequency-inverse document frequency score. The word list is then classified into the following three categories: positive, negative and neutral. The second phase involves modelling social media posts using network analysis, identifying sub-graphs, calculating average opinions and detecting influential users. The results showed 77.3% user satisfaction with Xeljanz. Positive opinions were especially pronounced among users who switched to Xeljanz based on advice from a physician. Negative opinions of Xeljanz typically pertained to the high cost of the drug.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3389433',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Resource-centric process mining: clustering using local process models',\n",
       "  'authors': \"['Landelin Delcoucq', 'Fabian Lecron', 'Philippe Fortemps', 'Wil. M. P. van der Aalst']\",\n",
       "  'date': 'March 2020',\n",
       "  'source': \"SAC '20: Proceedings of the 35th Annual ACM Symposium on Applied Computing\",\n",
       "  'abstract': 'In this paper, we focus on the resource perspective in the context of process mining. Most process mining techniques focus on the control-flow to uncover problems related to performance or compliance. However, the behavior of resources (e.g., employees) influences the effectiveness and efficiency of processes and should not be considered as secondary. We aim to identify resources exhibiting similar behavioral patterns that go beyond just looking at the mix of activities performed. We want to be able to identify subgroups of resources that perform similar activities but in a different order. We also provide a comparison between existing ways of grouping resources into roles and our resource-centered approach that takes into account the order in which work is performed. We will compare the results of clustering based only on the activities performed and clustering based on local process models that identify work patterns. Experiments are considered on synthetic and real data.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3341105.3373864',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Efficient mining of regional movement patterns in semantic trajectories',\n",
       "  'authors': \"['Dong-Wan Choi', 'Jian Pei', 'Thomas Heinis']\",\n",
       "  'date': 'None',\n",
       "  'source': 'Proceedings of the VLDB Endowment',\n",
       "  'abstract': 'Semantic trajectory pattern mining is becoming more and more important with the rapidly growing volumes of semantically rich trajectory data. Extracting sequential patterns in semantic trajectories plays a key role in understanding semantic behaviour of human movement, which can widely be used in many applications such as location-based advertising, road capacity optimisation, and urban planning. However, most of existing works on semantic trajectory pattern mining focus on the entire spatial area, leading to missing some locally significant patterns within a region. Based on this motivation, this paper studies a regional semantic trajectory pattern mining problem, aiming at identifying all the regional sequential patterns in semantic trajectories. Specifically, we propose a new density scheme to quantify the frequency of a particular pattern in space, and thereby formulate a new mining problem of finding all the regions in which such a pattern densely occurs. For the proposed problem, we develop an efficient mining algorithm, called RegMiner (<u>Reg</u>ional Semantic Trajectory Pattern <u>Miner</u>), which effectively reveals movement patterns that are locally frequent in such a region but not necessarily dominant in the entire space. Our empirical study using real trajectory data shows that RegMiner finds many interesting local patterns that are hard to find by a state-of-the-art global pattern mining scheme, and it also runs several orders of magnitude faster than the global pattern mining algorithm.',\n",
       "  'link': 'https://dl.acm.org/doi/10.14778/3151106.3151111',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Near-Optimal Clustering in the k-machine model',\n",
       "  'authors': \"['Sayan Bandyapadhyay', 'Tanmay Inamdar', 'Shreyas Pai', 'Sriram V. Pemmaraju']\",\n",
       "  'date': 'January 2018',\n",
       "  'source': \"ICDCN '18: Proceedings of the 19th International Conference on Distributed Computing and Networking\",\n",
       "  'abstract': 'The clustering problem, in its many variants, has numerous applications in operations research and computer science (e.g., in applications in bioinformatics, image processing, social network analysis, etc.). As sizes of data sets have grown rapidly, researchers have focused on designing algorithms for clustering problems in models of computation suited for large-scale computation such as MapReduce, Pregel, and streaming models. The k-machine model (Klauck et al., SODA 2015) is a simple, message-passing model for large-scale distributed graph processing. This paper considers three of the most prominent examples of clustering problems: the uncapacitated facility location problem, the p-median problem, and the p-center problem and presents O (1)-factor approximation algorithms for these problems running in Õ (n/k) rounds in the k -machine model. These algorithms are optimal upto polylogarithmic factors because this paper also shows Ω (n/k) lower bounds for obtaining poly(n)-factor approximation algorithms for these problems. These are the first results for clustering problems in the k -machine model. We assume that the metric provided as input for these clustering problems in only implicitly provided, as an edge-weighted graph and in a nutshell, our main technical contribution is to show that constant-factor approximation algorithms for all three clustering problems can be obtained by learning only a small portion of the input metric.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3154273.3154317',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Data integration and machine learning: a natural synergy',\n",
       "  'authors': \"['Xin Luna Dong', 'Theodoros Rekatsinas']\",\n",
       "  'date': 'None',\n",
       "  'source': 'Proceedings of the VLDB Endowment',\n",
       "  'abstract': 'As data volume and variety have increased, so have the ties between machine learning and data integration become stronger. For machine learning to be effective, one must utilize data from the greatest possible variety of sources; and this is why data integration plays a key role. At the same time machine learning is driving automation in data integration, resulting in overall reduction of integration costs and improved accuracy. This tutorial focuses on three aspects of the synergistic relationship between data integration and machine learning: (1) we survey how state-of-the-art data integration solutions rely on machine learning-based approaches for accurate results and effective human-in-the-loop pipelines, (2) we review how end-to-end machine learning applications rely on data integration to identify accurate, clean, and relevant data for their analytics exercises, and (3) we discuss open research challenges and opportunities that span across data integration and machine learning.',\n",
       "  'link': 'https://dl.acm.org/doi/10.14778/3229863.3229876',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A machine learning based approach to identify geo-location of Twitter users',\n",
       "  'authors': \"['Aytuğ Onan']\",\n",
       "  'date': 'March 2017',\n",
       "  'source': \"ICC '17: Proceedings of the Second International Conference on Internet of things, Data and Cloud Computing\",\n",
       "  'abstract': \"Twitter, a popular microblogging platform, has attracted great attention. Twitter enables people from all over the world to interact in an extremely personal way. The immense quantity of user-generated text messages become available on Twitter that could potentially serve as an important source of information for researchers and practitioners. The information available on Twitter may be utilized for many purposes, such as event detection, public health and crisis management. In order to effectively coordinate such activities, the identification of Twitter users' geo-locations is extremely important. Though online social networks can provide some sort of geo-location information based on GPS coordinates, Twitter suffers from geo-location sparseness problem. The identification of Twitter users' geo-location based on the content of send out messages, becomes extremely important. In this regard, this paper presents a machine learning based approach to the problem. In this study, our corpora is represented as a word vector. To obtain a classification scheme with high predictive performance, the performance of five classification algorithms, three ensemble methods and two feature selection methods are evaluated. Among the compared algorithms, the highest results (84.85%) is achieved by AdaBoost ensemble of Random Forest, when the feature set is selected with the use of consistency-based feature selection method in conjunction with best first search.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3018896.3018969',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Data mining and image analysis using genetic programming',\n",
       "  'authors': \"['Mahsa Shokri Varniab', 'Chih-Cheng Hung', 'Vahid Khalilzad Sharghi']\",\n",
       "  'date': 'December 2019',\n",
       "  'source': 'ACM SIGAPP Applied Computing Review',\n",
       "  'abstract': 'Genetic programming (GP) is an artificial intelligence technique that benefits from evolutionary computations allowing computers to solve problems automatically. In this paper, we present an optimized genetic-programming-based classifier that directly solves the multi-class classification problems in data mining and image analysis. A new fitness function is proposed for multiclass classification and brain tumor detection, which is validated by 10-fold cross validation. Instead of defining static thresholds as boundaries to differentiate between multiple labels, our work presents a method of classification where a GP system learns the relationships among experiential data and models them mathematically during the evolutionary process. We propose an optimized GP classifier based on a combination of pruning subtrees and a new fitness function. An orthogonal least squares algorithm is also applied in the training phase to create a robust GP classifier. Our approach has been assessed on six multiclass datasets and on a magnetic resonance imaging (MRI) brain image for tumor detection. The results of data classification for Iris, Wine, Glass, Pima, BUPA Liver and Balance Scale datasets are compared with existing algorithms. The high accuracy of brain tumor classification provided by our GP classifier confirms the strong ability of the developed technique for complicated classification problems. We compared our approach in terms of speed with previous GP algorithms as well. The analyzed results illustrate that the developed classifier produces a productive and rapid method for classification tasks that outperforms the previous methods for more challenging multiclass classification problems.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3381307.3381311',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Modeling Customer Experience in a Contact Center through Process Log Mining',\n",
       "  'authors': \"['Teng Fu', 'Guido Zampieri', 'David Hodgson', 'Claudio Angione', 'Yifeng Zeng']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Intelligent Systems and Technology',\n",
       "  'abstract': 'The use of data mining and modeling methods in service industry is a promising avenue for optimizing current processes in a targeted manner, ultimately reducing costs and improving customer experience. However, the introduction of such tools in already established pipelines often must adapt to the way data is sampled and to its content. In this study, we tackle the challenge of characterizing and predicting customer experience having available only process log data with time-stamp information, without any ground truth feedback from the customers. As a case study, we consider the context of a contact center managed by TeleWare and analyze phone call logs relative to a two months span. We develop an approach to interpret the phone call process events registered in the logs and infer concrete points of improvement in the service management. Our approach is based on latent tree modeling and multi-class Naïve Bayes classification, which jointly allow us to infer a spectrum of customer experiences and test their predictability based on the current data sampling strategy. Moreover, such approach can overcome limitations in customer feedback collection and sharing across organizations, thus having wide applicability and being complementary to tools relying on more heavily constrained data.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3468269',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Mining the Human Metabolome for Precision Oncology Research',\n",
       "  'authors': \"['Mercy E. Edoho', 'Moses E. Ekpenyong', 'Aliu B. Momodu', 'Geoffery Joseph']\",\n",
       "  'date': 'August 2020',\n",
       "  'source': \"ICMHI '20: Proceedings of the 4th International Conference on Medical and Health Informatics\",\n",
       "  'abstract': \"Access to clinical data is critical for advancing translational research; but regulatory constraints and policies surrounding the use of clinical data often challenge data access and sharing. Mixed medical datasets (structured and unstructured) are increasingly dominating the clinical information space, hence, demanding AI-driven techniques such as Natural Language Processing-to reorganize them for effective usage. This paper excavates the HMDB (Human Metabolome Database), for efficient knowledge mining, supported by diversely certified oncology physicians and pharmacists' contributions. We propose a novel taxonomy for knowledge representation and establish a universe of discourse for disease clustering and prediction. Excavated data include metabolites and their respective concentration values, age, gender, as well as gene and protein sequences, of normal and abnormal patients. These data were then merged to form an AI-ready 'Omic' technology datasets. Preliminary results reveal that the proposed AI-ready datasets would aid precision oncology research by adding quality analysis to the present HMDB, and for explaining the variations in concentration values of cancer patients.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3418094.3418123',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A Deep Feedforward Neural Network and Shallow Architectures Effectiveness Comparison: Flight Delays Classification Perspective',\n",
       "  'authors': \"['Desmond Bala Bisandu', 'Mohammed Salih Homaid', 'irene Moulitsas', 'Salvatore Filippone']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"ICAAI '21: Proceedings of the 5th International Conference on Advances in Artificial Intelligence\",\n",
       "  'abstract': \"Flight delays have negatively impacted the socio-economics state of passengers, airlines and airports, resulting in huge economic losses. Hence, it has become necessary to correctly predict their occurrences in decision-making because it is important for the effective management of the aviation industry. Developing accurate flight delays classification models depends mostly on the air transportation system complexity and the infrastructure available in airports, which may be a region-specific issue. However, no specific prediction or classification model can handle the individual characteristics of all airlines and airports at the same time. Hence, the need to further develop and compare predictive models for the aviation decision system of the future cannot be over-emphasised. In this research, flight on-time data records from the United State Bureau of Transportation Statistics was employed to evaluate the performances of Deep Feedforward Neural Network, Neural Network, and Support Vector Machine models on a binary classification problem. The research revealed that the models achieved different accuracies of flight delay classifications. The Support Vector Machine had the worst average accuracy than Neural Network and Deep Feedforward Neural Network in the initial experiment. The Deep Feedforward Neural Network outperformed Support Vector Machines and Neural Network with the best average percentage accuracies. Going further to investigate the Deep Feedforward Neural Network architecture on different parameters against itself suggest that training a Deep Feedforward Neural Network algorithm, regardless of data training size, the classification accuracy peaks. We examine which number of epochs works best in our flight delay classification settings for the Deep Feedforward Neural Network. Our experiment results demonstrate that having many epochs affects the convergence rate of the model; unlike when hidden layers are increased, it does not ensure better or higher accuracy in a binary classification of flight delays. Finally, we recommended further studies on the applicability of the Deep Feedforward Neural Network in flight delays prediction with specific case studies of either airlines or airports to check the impact on the model's performance.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3505711.3505712',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'An Interpretable Classification Model Based on Characteristic Element Extraction',\n",
       "  'authors': \"['Mingwei Zhang', 'Xiuxiu He', 'Bin Zhang']\",\n",
       "  'date': 'February 2019',\n",
       "  'source': \"ICMLC '19: Proceedings of the 2019 11th International Conference on Machine Learning and Computing\",\n",
       "  'abstract': \"The process of a classification application is usually dynamic and long. During the process of an application, better classification application effect can be acquired by enlarging and adjusting the training dataset continuously, for example, modifying the wrong labels of original instances. For this kind of dynamic classification applications, how to build an interpretable classifier which can help domain experts to understand each label's meanings reflected from the dataset, then to compare and discriminate them with their own mastered domain knowledge, and finally to adjust and optimize the training set to enhance the effect of classification applications, is a neglected but worth studying issue. Therefore, an interpretable classification model based on characteristic element extraction is proposed in this paper. The proposed classifier is constructed by extracting positive and negative characteristic elements for all class labels which can intuitively reflect their instinct characteristics. Thus, it has high interpretability obviously and can effectively help domain experts optimize classification effect. At the same time, experiment results show that our classifier also has higher accuracy compared with other kinds of classical classifiers. Consequently, the classification model proposed in this paper is effective and efficient, especially in practical applications.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3318299.3318370',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Big Data Financial Algorithm Technology Based on Machine Learning Technology',\n",
       "  'authors': \"['Yiming Zhao']\",\n",
       "  'date': 'December 2021',\n",
       "  'source': 'ICASIT 2021: 2021 International Conference on Aviation Safety and Information Technology',\n",
       "  'abstract': \"With the development and wide application of machine learning technology, the use of machine learning technology for economic algorithm technology research has become a new type of financial technology field. Today's financial big data has penetrated into all walks of life and has become an important factor of production. The extraction and application of massive amounts of data by humans heralds the arrival of a new wave of productivity growth and consumer surplus. Big data originally refers to a large number of data sets generated through batch processing or web search index analysis. This paper uses machine learning technology to explore and research big data financial algorithms, analyze risk control measures, report on the improvement and perfection of traditional finance, and analyze and study the future development of big data finance. The main research content of this paper is the analysis of big data financial algorithm technology by machine learning algorithms. Machine learning technology is one of the main methods to solve big data mining problems. Machine learning technology is a process of self-improvement using the system itself, so that computer programs can automatically improve performance through accumulated experience. This paper analyzes the relevant theories and characteristics of machine learning algorithms, and integrates them into the research of big data economic algorithm technology. The final result of the research shows that when the data volume is 1G, the training time of SVM is 8 minutes, while the training time of Bayesian is 12 minutes, and the data volume is relatively small. The SVM algorithm still has obvious advantages in training time.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3510858.3510934',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Application of Data Mining in College English Discourse Teaching',\n",
       "  'authors': \"['Xue Wang', 'Renqing Hu']\",\n",
       "  'date': 'September 2021',\n",
       "  'source': 'ICISCAE 2021: 2021 4th International Conference on Information Systems and Computer Aided Education',\n",
       "  'abstract': 'Classroom discourse teaching must conform to the principles of wholeness, psychology, schema theory and cognitive theory. Many students are not proficient in reading. Most of the time, they just stay at the superficial level of understanding sentences and do not have a comprehensive and profound understanding of the content through discourse analysis from a deep perspective. This paper constructs a classroom teaching mode guided by schema theory and cognitive theory, with discourse teaching method as the means and developing English ability as the index. This paper also analyzes the characteristics of mind mapping and discourse teaching, as well as the relationship between them. This paper discusses how to apply mind map in discourse teaching. The experimental results show that the teaching method proposed in this paper can be used for reference and inspiration to English teaching.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3482632.3482680',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Learning to Classify: A Flow-Based Relation Network for Encrypted Traffic Classification',\n",
       "  'authors': \"['Wenbo Zheng', 'Chao Gou', 'Lan Yan', 'Shaocong Mo']\",\n",
       "  'date': 'April 2020',\n",
       "  'source': \"WWW '20: Proceedings of The Web Conference 2020\",\n",
       "  'abstract': 'As the size and source of network traffic increase, so does the challenge of monitoring and analyzing network traffic. The challenging problems of classifying encrypted traffic are the imbalanced property of network data, the generalization on an unseen dataset, and overly dependent on data size. In this paper, we propose an application of a meta-learning approach to address these problems in encrypted traffic classification, named Flow-Based Relation Network (RBRN). The RBRN is an end-to-end classification model that learns representative features from the raw flows and then classifies them in a unified framework. Moreover, we design “hallucinator” to produce additional training samples for the imbalanced classification, and then focus on meta-learning to classify unseen categories from few labeled samples. We validate the effectiveness of the RBRN on the real-world network traffic dataset, and the experimental results demonstrate that the RBRN can achieve an excellent classification performance and outperform the state-of-the-art methods on encrypted traffic classification. What is more interesting, our model trained on the real-world dataset can generalize very well to unseen datasets, outperforming multiple state-of-art methods.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3366423.3380090',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Online Embedding and Clustering of Data Streams',\n",
       "  'authors': \"['Alaettin Zubaroğlu', 'Volkan Atalay']\",\n",
       "  'date': 'November 2019',\n",
       "  'source': \"ICBDR '19: Proceedings of the 3rd International Conference on Big Data Research\",\n",
       "  'abstract': 'Number of connected devices is steadily increasing and these devices continuously generate data streams. These data streams are often high dimensional and contain concept drift. Real-time processing of data streams is arousing interest despite many challenges. Clustering is a method that does not need labeled instances (it is unsupervised) and it can be applied with less prior information about the data. These properties make clustering one of the most suitable methods for real-time data stream processing. Moreover, data embedding is a process that may simplify clustering and makes visualization of high dimensional data possible. There exist several data stream clustering algorithms in the literature, however no data stream embedding method exists. UMAP is a data embedding algorithm that is suitable to be applied on data streams, but it cannot adopt concept drift. In this study, we have developed a new method to apply UMAP on data streams, adopt concept drift and cluster embedded data instances using any distance based clustering algorithms.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3372454.3372481',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Sentiment Classification of Chinese Text Based on Extending Semantic Similar Sentiment Words',\n",
       "  'authors': \"['Yanying Mao']\",\n",
       "  'date': 'September 2021',\n",
       "  'source': 'ICISCAE 2021: 2021 4th International Conference on Information Systems and Computer Aided Education',\n",
       "  'abstract': 'None',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3482632.3484084',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'CUSUM Based Concept Drift Detector for Data Stream Clustering',\n",
       "  'authors': \"['Namitha K.', 'G. Santhosh Kumar']\",\n",
       "  'date': 'August 2020',\n",
       "  'source': \"BDIOT '20: Proceedings of the 2020 4th International Conference on Big Data and Internet of Things\",\n",
       "  'abstract': 'The last few decades mark an unprecedented growth in the number of applications producing high-speed data streams. Learning from such fast data streams has many inherent challenges. The dynamic change in the concept of the stream is a significant challenge to be handled by the learning systems. This problem termed concept drift is given due focus in data stream classification scenarios. But, data stream clustering algorithms usually treat concept drift implicitly as part of the learning process. The need for explicit drift detection and adaptation is often neglected. This paper discusses a statistical method of change detection for data stream clustering problems. The change detection is done based on CUSUM test. On identifying a change, the model is re-built using the recent samples from the stream. The change detection process has been validated using real and synthetic datasets.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3421537.3421548',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Global-Local Mutual Attention Model for Text Classification',\n",
       "  'authors': \"['Qianli Ma', 'Liuhong Yu', 'Shuai Tian', 'Enhuan Chen', 'Wing W. Y. Ng']\",\n",
       "  'date': 'None',\n",
       "  'source': 'IEEE/ACM Transactions on Audio, Speech and Language Processing',\n",
       "  'abstract': 'Text classification is a central field of inquiry in natural language processing NLP. Although some models learn local semantic features and global long-term dependencies simultaneously, they simply combine them through concatenation either in a cascade way or in parallel while mutual effects between them are ignored. In this paper, we propose the Global-Local Mutual Attention GLMA model for text classification problems, which introduces a mutual attention mechanism for mutual learning between local semantic features and global long-term dependencies. The mutual attention mechanism consists of a Local-Guided Global-Attention LGGA and a Global-Guided Local-Attention GGLA. The LGGA allows to assign weights and combine global long-term dependencies of word positions that are semantic related. It captures combined semantics and alleviates the gradient vanishing problem. The GGLA automatically assigns more weights to relevant local semantic features, which captures key local semantic information and filters both noises and irrelevant words/phrases. Furthermore, a weighted-over-time pooling operation is developed to aggregate the most informative and discriminative features for classification. Extensive experiments demonstrate that our model obtains the state-of-the-art performance on seven benchmark datasets and sixteen Amazon product reviews datasets. Both the result analysis and the mutual attention weights visualization further demonstrate the effectiveness of the proposed model.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/TASLP.2019.2942160',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Robust Spectral Clustering for Noisy Data: Modeling Sparse Corruptions Improves Latent Embeddings',\n",
       "  'authors': \"['Aleksandar Bojchevski', 'Yves Matkovic', 'Stephan Günnemann']\",\n",
       "  'date': 'August 2017',\n",
       "  'source': \"KDD '17: Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining\",\n",
       "  'abstract': 'Spectral clustering is one of the most prominent clustering approaches. However, it is highly sensitive to noisy input data. In this work, we propose a robust spectral clustering technique able to handle such scenarios. To achieve this goal, we propose a sparse and latent decomposition of the similarity graph used in spectral clustering. In our model, we jointly learn the spectral embedding as well as the corrupted data - thus, enhancing the clustering performance overall. We propose algorithmic solutions to all three established variants of spectral clustering, each showing linear complexity in the number of edges. Our experimental analysis confirms the significant potential of our approach for robust spectral clustering. Supplementary material is available at www.kdd.in.tum.de/RSC.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3097983.3098156',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Spectral Clustering of Large-scale Data by Directly Solving Normalized Cut',\n",
       "  'authors': \"['Xiaojun Chen', 'Weijun Hong', 'Feiping Nie', 'Dan He', 'Min Yang', 'Joshua Zhexue Huang']\",\n",
       "  'date': 'July 2018',\n",
       "  'source': \"KDD '18: Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining\",\n",
       "  'abstract': 'During the past decades, many spectral clustering algorithms have been proposed. However, their high computational complexities hinder their applications on large-scale data. Moreover, most of them use a two-step approach to obtain the optimal solution, which may deviate from the solution by directly solving the original problem. In this paper, we propose a new optimization algorithm, namely Direct Normalized Cut (DNC), to directly optimize the normalized cut model. DNC has a quadratic time complexity, which is a significant reduction comparing with the cubic time complexity of the traditional spectral clustering. To cope with large-scale data, a Fast Normalized Cut (FNC) method with linear time and space complexities is proposed by extending DNC with an anchor-based strategy. In the new method, we first seek a set of anchors and then construct a representative similarity matrix by computing distances between the anchors and the whole data set. To find high quality anchors that best represent the whole data set, we propose a Balanced k-means (BKM) to partition a data set into balanced clusters and use the cluster centers as anchors. Then DNC is used to obtain the final clustering result from the representative similarity matrix. A series of experiments were conducted on both synthetic data and real-world data sets, and the experimental results show the superior performance of BKM, DNC and FNC.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3219819.3220039',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Reinforcement Learning with Sequential Information Clustering in Real-Time Bidding',\n",
       "  'authors': \"['Junwei Lu', 'Chaoqi Yang', 'Xiaofeng Gao', 'Liubin Wang', 'Changcheng Li', 'Guihai Chen']\",\n",
       "  'date': 'November 2019',\n",
       "  'source': \"CIKM '19: Proceedings of the 28th ACM International Conference on Information and Knowledge Management\",\n",
       "  'abstract': 'Display advertising is a billion dollar business which is the primary income of many companies. In this scenario, real-time bidding optimization is one of the most important problems, where the bids of ads for each impression are determined by an intelligent policy such that some global key performance indicators are optimized. Due to the highly dynamic bidding environment, many recent works try to use reinforcement learning algorithms to train the bidding agents. However, as the probability of the occurrence of a particular state is typically low and the state representation in current work lacks sequential information, the convergence speed and performance of deep reinforcement algorithms are disappointing. To tackle these two challenges in the real-time bidding scenario, we propose ClusterA3C, a novel Advantage Asynchronous Actor-Critic (A3C) variant integrated with a sequential information extraction scheme and a clustering based state aggregation scheme. We conduct extensive experiments to validate the proposed scheme on a real-world commercial dataset. Experimental results show that the proposed scheme outperforms the state of the art methods in terms of either performance or convergence speed.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3357384.3358027',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'VRoC: Variational Autoencoder-aided Multi-task Rumor Classifier Based on Text',\n",
       "  'authors': \"['Mingxi Cheng', 'Shahin Nazarian', 'Paul Bogdan']\",\n",
       "  'date': 'April 2020',\n",
       "  'source': \"WWW '20: Proceedings of The Web Conference 2020\",\n",
       "  'abstract': 'Social media became popular and percolated almost all aspects of our daily lives. While online posting proves very convenient for individual users, it also fosters fast-spreading of various rumors. The rapid and wide percolation of rumors can cause persistent adverse or detrimental impacts. Therefore, researchers invest great efforts on reducing the negative impacts of rumors. Towards this end, the rumor classification system aims to to detect, track, and verify rumors in social media. Such systems typically include four components: (i) a rumor detector, (ii) a rumor tracker, (iii) a stance classifier, and (iv) a veracity classifier. In order to improve the state-of-the-art in rumor detection, tracking, and verification, we propose VRoC, a tweet-level variational autoencoder-based rumor classification system. VRoC consists of a co-train engine that trains variational autoencoders (VAEs) and rumor classification components. The co-train engine helps the VAEs to tune their latent representations to be classifier-friendly. We also show that VRoC is able to classify unseen rumors with high levels of accuracy. For the PHEME dataset, VRoC consistently outperforms several state-of-the-art techniques, on both observed and unobserved rumors, by up to 26.9%, in terms of macro-F1 scores.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3366423.3380054',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Speaker and Time-aware Joint Contextual Learning for Dialogue-act Classification in Counselling Conversations',\n",
       "  'authors': \"['Ganeshan Malhotra', 'Abdul Waheed', 'Aseem Srivastava', 'Md Shad Akhtar', 'Tanmoy Chakraborty']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"WSDM '22: Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining\",\n",
       "  'abstract': 'The onset of the COVID-19 pandemic has brought the mental health of people under risk. Social counselling has gained remarkable significance in this environment. Unlike general goal-oriented dialogues, a conversation between a patient and a therapist is considerably implicit, though the objective of the conversation is quite apparent. In such a case, understanding the intent of the patient is imperative in providing effective counselling in therapy sessions, and the same applies to a dialogue system as well. In this work, we take forward a small but an important step in the development of an automated dialogue system for mental-health counselling. We develop a novel dataset, named HOPE, to provide a platform for the dialogue-act classification in counselling conversations. We identify the requirement of such conversation and propose twelve domain-specific dialogue-act (DAC) labels. We collect ~ 12.9K utterances from publicly-available counselling session videos on YouTube, extract their transcripts, clean, and annotate them with DAC labels. Further, we propose SPARTA, a transformer-based architecture with a novel speaker- and time-aware contextual learning for the dialogue-act classification. Our evaluation shows convincing performance over several baselines, achieving state-of-the-art on HOPE. We also supplement our experiments with extensive empirical and qualitative analyses of SPARTA.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3488560.3498509',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'HGCN: A Heterogeneous Graph Convolutional Network-Based Deep Learning Model Toward Collective Classification',\n",
       "  'authors': \"['Zhihua Zhu', 'Xinxin Fan', 'Xiaokai Chu', 'Jingping Bi']\",\n",
       "  'date': 'August 2020',\n",
       "  'source': \"KDD '20: Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining\",\n",
       "  'abstract': 'Collective classification, as an important technique to study networked data, aims to exploit the label autocorrelation for a group of inter-connected entities with complex dependencies. As the emergence of various heterogeneous information networks (HINs), collective classification at present is confronting several severe challenges stemming from the heterogeneity of HINs, such as complex relational hierarchy, potential incompatible semantics and node-context relational semantics. To address the challenges, in this paper, we propose a novel heterogeneous graph convolutional network-based deep learning model, called HGCN, to collectively categorize the entities in HINs. Our work involves three primary contributions: i) HGCN not only learns the latent relations from the relation-sophisticated HINs via multi-layer heterogeneous convolutions, but also captures the semantic incompatibility among relations with properly-learned edge-level filter parameters; ii) to preserve the fine-grained relational semantics of different-type nodes, we propose a heterogeneous graph convolution to directly tackle the original HINs without any in advance transforming the network from heterogeneity to homogeneity; iii) we perform extensive experiments using four real-world datasets to validate our proposed HGCN, the multi-facet results show that our proposed HGCN can significantly improve the performance of collective classification compared with the state-of-the-art baseline methods.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3394486.3403169',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Name-Nationality Classification Technology under Keras Deep Learning',\n",
       "  'authors': \"['Yu Kang']\",\n",
       "  'date': 'May 2020',\n",
       "  'source': 'BDE 2020: Proceedings of the 2020 2nd International Conference on Big Data Engineering',\n",
       "  'abstract': 'To improve the classification efficiency of personnel nationality information, the classification method of personnel nationality information is explored. First, a new classification method is proposed using the Keras model and deep learning theory. Two methods based on support vector machine (SVM) and convolutional neural network classification are proposed. (1) The personal name is input and a set of numbers corresponding to the positions in the alphabet are output orderly. (2) The personal name is input and the number output relies on the number of occurrences of each character of a name, regardless of order. Second, for the problem that the classification accuracy of nationality information by name is not high, the adaptive moment estimation (Adam) algorithm is used to optimize it. Finally, to prove the reliability of the proposed methods, these methods are used to verify the nationality information of Olympic personnel. The results show that comparing the two methods, the classification method that relies on the number of occurrences of characters in the name gets good grades, and the ultimate average score is 90.35. The score of the first method is only 79.34. Through this investigation, it is found that the second method proposed can effectively use the name of the person to determine the nationality information. Applying it in real life can improve the classification efficiency of personnel nationality information.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3404512.3404517',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Understanding Association Between Logged Vehicle Data and Vehicle Marketing Parameters: Using Clustering and Rule-Based Machine Learning',\n",
       "  'authors': \"['Oskar Dahl', 'Fredrik Johansson', 'Reza Khoshkangini', 'Sepideh Pashami', 'Sławomir Nowaczyk', 'Pihl Claes']\",\n",
       "  'date': 'August 2020',\n",
       "  'source': \"IMMS '20: Proceedings of the 3rd International Conference on Information Management and Management Science\",\n",
       "  'abstract': \"Trucks are designed, configured and marketed for various working environments. There lies a concern whether trucks are used as intended by the manufacturer, as usage may impact the longevity, efficiency and productivity of the trucks. In this paper we propose a framework that aims to extract costumers' vehicle behaviours from Logged Vehicle Data (LVD) in order to evaluate whether they align with vehicle configurations, so-called Global Transport Application (GTA) parameters. Gaussian mixture model (GMM)s are employed to cluster and classify various vehicle behaviors from the LVD. Rule-based machine learning (RBML) was applied on the clusters to examine whether vehicle behaviors follow the GTA configuration. Particularly, we propose an approach based on studying associations that is able to extract insights on whether the trucks are used as intended. Experimental results shown that while for the vast majority of the trucks' behaviors seemingly follows their GTA configuration, there are also interesting outliers that warrant further analysis.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3416028.3417215',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Mining Semantic Subspaces to Express Discipline-Specific Similarities',\n",
       "  'authors': \"['Janus Wawrzinek', 'José María González Pinto', 'Wolf-Tilo Balke']\",\n",
       "  'date': 'August 2020',\n",
       "  'source': \"JCDL '20: Proceedings of the ACM/IEEE Joint Conference on Digital Libraries in 2020\",\n",
       "  'abstract': 'Word embeddings enable state-of-the-art NLP workflows in im-portant tasks including semantic similarity matching, NER, question answering, and document classification. Recently also the biomedical field started to use word embeddings to provide new access paths for a better understanding of pharmaceutical entities and their relationships, as well as to predict certain chemical properties. The central idea is to gain access to knowledge embedded, but not explicated in biomedical literature. However, a core challenge is the interpretability of the underly-ing embeddings model. Previous work has attempted to inter-pret the semantics of dimensions in word embeddings models to ease model interpretation when applied to semantic similarity task. To do so, the original embedding space is transformed to a sparse or a more condensed space, which then has to be inter-preted in an exploratory (and hence time-consuming) fashion. However, little has been done to assess in real-time whether specific user-provided semantics are actually reflected in the original embedding space. We solve this problem by extracting a semantic subspace from large embedding spaces that better fits the query semantics defined by a user. Our method builds on least-angle regression to rank dimensions according to given semantics properly, i.e. to uncover a subspace to ease both in-terpretation and exploration of the embedding space. We com-pare our methodology to querying the original space as well as to several other recent approaches and show that our method consistently outperforms all competitors.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3383583.3398523',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Host-Based Virtual Machine Workload Characterization Using Hypervisor Trace Mining',\n",
       "  'authors': \"['Hani Nemati', 'Seyed Vahid Azhari', 'Mahsa Shakeri', 'Michel Dagenais']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Modeling and Performance Evaluation of Computing Systems',\n",
       "  'abstract': 'Cloud computing is a fast-growing technology that provides on-demand access to a pool of shared resources. This type of distributed and complex environment requires advanced resource management solutions that could model virtual machine (VM) behavior. Different workload measurements, such as CPU, memory, disk, and network usage, are usually derived from each VM to model resource utilization and group similar VMs. However, these course workload metrics require internal access to each VM with the available performance analysis toolkit, which is not feasible with many cloud environments privacy policies.In this article, we propose a non-intrusive host-based virtual machine workload characterization using hypervisor tracing. VM blockings duration, along with virtual interrupt injection rates, are derived as features to reveal multiple levels of resource intensiveness. In addition, the VM exit reason is considered, as well as the resource contention rate due to the host and other VMs. Moreover, the processes and threads preemption rates in each VM are extracted using the collected tracing logs. Our proposed approach further improves the selected features by exploiting a page ranking based algorithm to filter non-important processes running on each VM. Once the metric features are defined, a two-stage VM clustering technique is employed to perform both coarse- and fine-grain workload characterization. The inter-cluster and intra-cluster similarity metrics of the silhouette score is used to reveal distinct VM workload groups, as well as the ones with significant overlap. The proposed framework can provide a detailed vision of the underlying behavior of the running VMs. This can assist infrastructure administrators in efficient resource management, as well as root cause analysis.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3460197',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Deep Learning for Neuromarketing; Classification of User Preference using EEG Signals',\n",
       "  'authors': \"['Maryam Alimardani', 'Mory Kaba']\",\n",
       "  'date': 'May 2021',\n",
       "  'source': 'AH2021: 12th Augmented Human International Conference',\n",
       "  'abstract': 'The present study investigates the applicability of deep learning methods in EEG neuromarketing prediction tasks, compared to traditional machine learning approaches. Neuroscientific methods have expanded research capabilities in marketing and created new insights into consumer behavior and decision making processes. Both machine learning and deep learning approaches can be employed to predict relevant consumer preference from brain activity. The former requires extensive signal processing and feature engineering for classification whereas the later relies on raw brain signals and thus avoids time-consuming preprocessing. In this paper, the performance of a machine learning model comprising an ensemble of algorithms was compared to the performance of a convolutional neural network (CNN) on two independently collected EEG datasets, one concerning product choices and the other movie ratings. While both models showed poor performance for prediction of product choices, the convolutional neural network proved more accurate in the prediction of movie ratings. This provides evidence for the superiority of deep learning algorithms in certain neuromarketing prediction tasks. We discuss the limitations and future application opportunities.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3460881.3460930',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Comparison of Land Cover Classification of Ir Sutami Dam Using Machine Learning and Multisource Satellite Imagery',\n",
       "  'authors': \"['Ainia Walidaroyani', 'Fatwa Ramdani', 'Tri Astoto Kurniawan']\",\n",
       "  'date': 'September 2021',\n",
       "  'source': \"SIET '21: Proceedings of the 6th International Conference on Sustainable Information Engineering and Technology\",\n",
       "  'abstract': 'The latest land cover information in the form of maps resulting from image classification can be obtained through remote sensing techniques that utilize satellite imagery, such as Landsat 8 and Sentinel-2. However, no study compares the two satellite images with high classification algorithm accuracy. The previous literature describes several techniques that can be used to classify land cover, but there has been no specific use of similar techniques in the Ir. Sutami. In this study, the author uses a classification technique using the CART (Classification and Regression Tree) algorithm which can present the classification results as a simple tree structure to make the classification process easier and closer. The results of processing Landsat 8 and Sentinel-2 satellite imagery in 2020 show accuracy test results at 91% and 97% for vegetation classes, water bodies, built land, open land, and rice fields. We hope this research can help in providing information to cover the land in the area around Ir. Sutami to avoid the negative impacts of future land-use changes.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3479645.3479681',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': \"ALEX: Active Learning based Enhancement of a Classification Model's EXplainability\",\n",
       "  'authors': \"['Ishani Mondal', 'Debasis Ganguly']\",\n",
       "  'date': 'October 2020',\n",
       "  'source': \"CIKM '20: Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management\",\n",
       "  'abstract': \"An active learning (AL) algorithm seeks to construct an effective classifier with a minimal number of labeled examples in a bootstrapping manner. While standard AL heuristics, such as selecting those points for annotation for which a classification model yields least confident predictions, there has been no empirical investigation to see if these heuristics lead to models that are more interpretable to humans. In the era of data-driven learning, this is an important research direction to pursue. This paper describes our work-in-progress towards developing an AL selection function that in addition to model effectiveness also seeks to improve on the interpretability of a model during the bootstrapping steps. Concretely speaking, our proposed selection function trains an 'explainer' model in addition to the classifier model, and favours those instances where a different part of the data is used, on an average, to explain the predicted class. Initial experiments exhibited encouraging trends in showing that such a heuristic can lead to developing more effective and more explainable end-to-end data-driven classifiers.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3340531.3417456',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A streaming clustering approach using a heterogeneous system for big data analysis',\n",
       "  'authors': \"['Dajung Lee', 'Alric Althoff', 'Dustin Richmond', 'Ryan Kastner']\",\n",
       "  'date': 'November 2017',\n",
       "  'source': \"ICCAD '17: Proceedings of the 36th International Conference on Computer-Aided Design\",\n",
       "  'abstract': 'Data clustering is a fundamental challenge in data analytics. It is the main task in exploratory data mining and a core technique in machine learning. As the volume, variety, velocity, and variability of data grows, we need more efficient data analysis methods that can scale towards increasingly large and high dimensional data sets. We develop a streaming clustering algorithm that is highly amenable to hardware acceleration. Our algorithm eliminates the need to store the data objects, which removes limits on the size of the data that we can analyze. Our algorithm is highly parameterizable, which allows it to fit to the characteristics of the data set, and scale towards the available hardware resources. Our streaming hardware core can handle more than 40 Msamples/s when processing 3-dimensional streaming data and up to 1.78 Msamples/s for 70-dimensional data. To validate the accuracy and performance of our algorithms we compare it with several common clustering techniques on several different applications. The experimental result shows that it outperforms other prior hardware accelerated clustering systems.',\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3199700.3199793',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Classification with incomplete data and ensemble learners for the prediction of cervical cancer risk',\n",
       "  'authors': \"['Pinar Yildirim']\",\n",
       "  'date': 'June 2018',\n",
       "  'source': \"ICIST '18: Proceedings of the International Conference on Intelligent Science and Technology\",\n",
       "  'abstract': 'Incomplete data is an important problem in analyzing medical data sets. In this study, a comparative analysis of ensemble learning algorithms was carried out for the prediction of cervical cancer risk with incomplete data. Cervical cancer is one of the most common cancers for women world-wide, and many researchers focused on this disease. The dataset was collected from UCI Machine Learning Repository. Mean imputation was used to deal with missing values and some ensemble and standalone classifiers were used to analyze the dataset for the evaluation of the performance. This study supported that imputation approaches and ensemble learning can improve the performance of learning algorithms.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3233740.3233741',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Bitcoin Price Prediction Through Opinion Mining',\n",
       "  'authors': \"['Germán Cheuque Cerda', 'Juan L. Reutter']\",\n",
       "  'date': 'May 2019',\n",
       "  'source': \"WWW '19: Companion Proceedings of The 2019 World Wide Web Conference\",\n",
       "  'abstract': 'The Bitcoin protocol and its underlying cryptocurrency have started to shape the way we view digital currency, and opened up a large list of new and interesting challenges. Amongst them, we focus on the question of how is the price of digital currencies affected, which is a natural question especially when considering the price rollercoaster we witnessed for bitcoin in 2017-2018. We work under the hypothesis that price is affected by the web footprint of influential people, we refer to them as crypto-influencers. In this paper we provide neural models for predicting bitcoin price. We compare what happens when the model is fed only with recent price history versus what happens when fed, in addition, with a measure of the positivity or negativity of the sayings of these influencers, measured through a sentiment analysis of their twitter posts. We show preliminary evidence that twitter data should indeed help to predict the price of bitcoin, even though the measures we use in this paper have a lot of room for refinement. In particular, we also discuss the challenges of measuring the correct sensation of these posts, and discuss the work that should help improving our discoveries even further.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3308560.3316454',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'PUMiner: Mining Security Posts from Developer Question and Answer Websites with PU Learning',\n",
       "  'authors': \"['Triet Huynh Minh Le', 'David Hin', 'Roland Croft', 'M. Ali Babar']\",\n",
       "  'date': 'June 2020',\n",
       "  'source': \"MSR '20: Proceedings of the 17th International Conference on Mining Software Repositories\",\n",
       "  'abstract': 'Security is an increasing concern in software development. Developer Question and Answer (Q&A) websites provide a large amount of security discussion. Existing studies have used human-defined rules to mine security discussions, but these works still miss many posts, which may lead to an incomplete analysis of the security practices reported on Q&A websites. Traditional supervised Machine Learning methods can automate the mining process; however, the required negative (non-security) class is too expensive to obtain. We propose a novel learning framework, PUMiner, to automatically mine security posts from Q&A websites. PUMiner builds a context-aware embedding model to extract features of the posts, and then develops a two-stage PU model to identify security content using the labelled Positive and Un-labelled posts. We evaluate PUMiner on more than 17.2 million posts on Stack Overflow and 52,611 posts on Security StackExchange. We show that PUMiner is effective with the validation performance of at least 0.85 across all model configurations. Moreover, Matthews Correlation Coefficient (MCC) of PUMiner is 0.906, 0.534 and 0.084 points higher than one-class SVM, positive-similarity filtering, and one-stage PU models on unseen testing posts, respectively. PUMiner also performs well with an MCC of 0.745 for scenarios where string matching totally fails. Even when the ratio of the labelled positive posts to the un-labelled ones is only 1:100, PUMiner still achieves a strong MCC of 0.65, which is 160% better than fully-supervised learning. Using PUMiner, we provide the largest and up-to-date security content on Q&A websites for practitioners and researchers.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3379597.3387443',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Machine learning algorithms for oncology big data treatment',\n",
       "  'authors': \"['Zouiten Mohammed']\",\n",
       "  'date': 'November 2017',\n",
       "  'source': \"ICCWCS'17: Proceedings of the 2nd International Conference on Computing and Wireless Communication Systems\",\n",
       "  'abstract': 'Two-dimensional arrays of bi-component structures made of cobalt and permalloy elliptical dots with thickness of 25 nm, length 1 m and width of 225 nm, have been prepared by a self-aligned shadow deposition technique. Brillouin light scattering has been exploited to study the frequency dependence of thermally excited magnetic eigenmodes on the intensity of the external magnetic field, applied along the easy axis of the elements. Our work is part of user-centered healthcare decision-making systems based on a process of predicting cancer distribution. This process should lead to a set of knowledge in Datamining, Ontologies and Geographical Information Systems. It is in the same time iterative and interactive. Therefore, it seems essential to take into account principles and methods of Human-Machine Interaction in the development of such systems. In this respect, development of interactive decision-making systems is currently being approached using two opposing approaches. In the first one, technology is fundamental; the second one is user centered placing the human actors in a central position. Although the first approach is still present in healthcare organizations, the current trend is definitely the user centric. In our framework we propose an approach that aims to integrate the steps of the predicting future from data process into a development model enriched from human-machine interactions. Our application context is the fight against breast cancer in hospitals. We demonstrate that medical decision can be based on a spatial analysis of the geographical distribution of many cancers. Several factors explain our choice of datamining for assistance of health decision-makers for learning in the CART algorithm about patients who are future actors of suspicion.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3167486.3167565',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A classification of code changes and test types dependencies for improving machine learning based test selection',\n",
       "  'authors': \"['Khaled Al-Sabbagh', 'Miroslaw Staron', 'Regina Hebig', 'Francisco Gomes']\",\n",
       "  'date': 'August 2021',\n",
       "  'source': 'PROMISE 2021: Proceedings of the 17th International Conference on Predictive Models and Data Analytics in Software Engineering',\n",
       "  'abstract': 'Machine learning has been increasingly used to solve various software engineering tasks. One example of their usage is in regression testing, where a classifier is built using historical code commits to predict which test cases require execution. In this paper, we address the problem of how to link specific code commits to test types to improve the predictive performance of learning models in improving regression testing. We design a dependency taxonomy of the content of committed code and the type of a test case. The taxonomy focuses on two types of code commits: changing memory management and algorithm complexity. We reviewed the literature, surveyed experienced testers from three Swedish-based software companies, and conducted a workshop to develop the taxonomy. The derived taxonomy shows that memory management code should be tested with tests related to performance, load, soak, stress, volume, and capacity; the complexity changes should be tested with the same dedicated tests and maintainability tests. We conclude that this taxonomy can improve the effectiveness of building learning models for regression testing.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3475960.3475987',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Multimodal Deep Learning Based Crop Classification Using Multispectral and Multitemporal Satellite Imagery',\n",
       "  'authors': \"['Krishna Karthik Gadiraju', 'Bharathkumar Ramachandra', 'Zexi Chen', 'Ranga Raju Vatsavai']\",\n",
       "  'date': 'August 2020',\n",
       "  'source': \"KDD '20: Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining\",\n",
       "  'abstract': 'The Food and Agriculture Organization (FAO) of the United Nations predicts that in order to meet the needs of the expected 3 billion population growth by 2050, food production has to increase by 60%. Therefore, monitoring and mapping crops accurately is essential for estimating food production during each crop growing season across the globe. Traditionally, multispectral remote sensing imagery has been widely used for mapping crops worldwide. However, single date imagery does not capture temporal characteristics (phenology) of growing crops, leading to imprecise crop maps and food estimates. On the other hand, purely temporal classification approaches also produce inaccurate crop maps as they do not account for spatial autocorrelations. In this paper, we present a multimodal deep learning solution that jointly exploits spatial-spectral and phenological properties to identify major crop types. Using a two stream architecture, spatial characteristics are captured via a spatial stream consisting of very high resolution images (single date, 1m, 3-spectral bands, USDA NAIP) with a CNN and the phenological characteristics via a temporal stream images (biweekly, 250m, MODIS NDVI) with an LSTM. Experimental results show that the proposed multimodal solution reduces prediction error by 60%.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3394486.3403375',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Combining multiple clustering and network analysis for discoveries in gene expression data',\n",
       "  'authors': \"['Sleiman Alhajj', 'Aya Alhajj', 'Sibel Tariyan Özyer']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"ASONAM '21: Proceedings of the 2021 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining\",\n",
       "  'abstract': 'Clustering is a challenging research task which could benefit a wide range of practical applications, including bioinformatics. It targets success by optimizing a number of objectives, a characteristic mostly ignored by clustering approaches. This paper describes a synthetic clustering algorithm which first applies multi-objective based approach to produce the alternative clustering solutions. Then the best clusters from each solution are selected and combined into a seed for a compact and effective solution which is expected to be better than all the individual solutions because it combines the best of each. This way, the developed algorithm may be classified as a fuzzy clustering approach because each object may belong to more than one cluster in the synthesized solution with a degree of membership in each cluster. Another interesting aspect of the algorithm is that it identifies the outliers. Further, a network is built from the relationships of the objects within the various clusters. The network is analyzed to reveal interesting discoveries not clearly reflected in the clustering outcome. The validity and applicability of the presented methodology has been assessed using synthetic and real data from the cancer.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3487351.3490961',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Machine learning for streaming data: state of the art, challenges, and opportunities',\n",
       "  'authors': \"['Heitor Murilo Gomes', 'Jesse Read', 'Albert Bifet', 'Jean Paul Barddal', 'João Gama']\",\n",
       "  'date': 'December 2019',\n",
       "  'source': 'ACM SIGKDD Explorations Newsletter',\n",
       "  'abstract': 'Incremental learning, online learning, and data stream learning are terms commonly associated with learning algorithms that update their models given a continuous influx of data without performing multiple passes over data. Several works have been devoted to this area, either directly or indirectly as characteristics of big data processing, i.e., Velocity and Volume. Given the current industry needs, there are many challenges to be addressed before existing methods can be efficiently applied to real-world problems. In this work, we focus on elucidating the connections among the current stateof- the-art on related fields; and clarifying open challenges in both academia and industry. We treat with special care topics that were not thoroughly investigated in past position and survey papers. This work aims to evoke discussion and elucidate the current research opportunities, highlighting the relationship of different subareas and suggesting courses of action when possible.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3373464.3373470',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Hotel Reviews Analysis based on Sentiment Classification: Oman Case Study',\n",
       "  'authors': \"['Aiman Moyaid Said', 'Amal Sultan AI Muqrashi']\",\n",
       "  'date': 'April 2020',\n",
       "  'source': \"ICCTA '20: Proceedings of the 2020 6th International Conference on Computer and Technology Applications\",\n",
       "  'abstract': 'Advances in technology and in particular web applications that are related to customer relations management have encouraged users to voice their opinions with the rest of the world. Relevant information needs to be sorted within the large amount data that can employed by the organization to ensure optimal corrective actions. Sentimental Analysis is one such technique that is popular due to its ability to distil relevant information from opinionated text. Managing customer satisfaction and expectations are vital components that add credibility and ensures survival of the organization. It is commonly agreed that these are the strategic components enhance customer allegiance and covet retention. Hotels in particular are wary of the fact that opinions do matter for their reputation and credibility. Hence, they strive to seek feedback from their guests. In this paper, the authors aim to develop a model to classify hotel visitors feedback based on their experience. Four different supervised learning approaches were employed and tested on the collected data set. The result indicates Support Vector Machine (SVM) classifier outperformed the rest of the classifiers in term of effectiveness.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3397125.3397147',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Challenge Paper: The Vision for Time Profiled Temporal Association Mining',\n",
       "  'authors': \"['Vangipuram Radhakrishna', 'Gali Suresh Reddy', 'Puligadda Veereswara Kumar', 'Vinjamuri Janaki']\",\n",
       "  'date': 'None',\n",
       "  'source': 'Journal of Data and Information Quality',\n",
       "  'abstract': 'None',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3404198',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Benchmarking Data Flow Systems for Scalable Machine Learning',\n",
       "  'authors': \"['Christoph Boden', 'Andrea Spina', 'Tilmann Rabl', 'Volker Markl']\",\n",
       "  'date': 'May 2017',\n",
       "  'source': \"BeyondMR'17: Proceedings of the 4th ACM SIGMOD Workshop on Algorithms and Systems for MapReduce and Beyond\",\n",
       "  'abstract': 'Distributed data flow systems such as Apache Spark or Apache Flink are popular choices for scaling machine learning algorithms in production. Industry applications of large scale machine learning such as click-through rate prediction rely on models trained on billions of data points which are both highly sparse and high-dimensional. Existing Benchmarks attempt to assess the performance of data flow systems such as Apache Flink, Spark or Hadoop with non-representative workloads such as WordCount, Grep or Sort. They only evaluate scalability with respect to data set size and fail to address the crucial requirement of handling high dimensional data. We introduce a representative set of distributed machine learning algorithms suitable for large scale distributed settings which have close resemblance to industry-relevant applications and provide generalizable insights into system performance. We implement mathematically equivalent versions of these algorithms in Apache Flink and Apache Spark, tune relevant system parameters and run a comprehensive set of experiments to assess their scalability with respect to both: data set size and dimensionality of the data. We evaluate the systems for data up to four billion data points and 100 million dimensions. Additionally we compare the performance to single-node implementations to put the scalability results into perspective. Our results indicate that while being able to robustly scale with increasing data set sizes, current state of the art data flow systems are surprisingly inefficient at coping with high dimensional data, which is a crucial requirement for large scale machine learning algorithms.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3070607.3070612',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Mining Exoticism from Visual Content with Fusion-based Deep Neural Networks',\n",
       "  'authors': \"['Andrea Ceroni', 'Chenyang Ma', 'Ralph Ewerth']\",\n",
       "  'date': 'June 2018',\n",
       "  'source': \"ICMR '18: Proceedings of the 2018 ACM on International Conference on Multimedia Retrieval\",\n",
       "  'abstract': 'Exoticism is the charm of the unfamiliar, it often means unusual, mystery, and it can evoke the atmosphere of remote lands. Although it has received interest in different arts, like painting and music, no study has been conducted on understanding exoticism from a computational perspective. To the best of our knowledge, this work is the first to explore the problem of exoticism-aware image classification, aiming at automatically measuring the amount of exoticism in images and investigating the significant aspects of the task. The estimation of image exoticism could be applied in fields like advertising and travel suggestion, as well as to increase serendipity and diversity of recommendations and search results. We propose a Fusion-based Deep Neural Network (FDNN) for this task, which combines image representations learned by Deep Neural Networks with visual and semantic hand-crafted features. Comparisons with other Machine Learning models show that our proposed architecture is the best performing one, reaching accuracy over 83% and 91% on two different datasets. Moreover, experiments with classifiers exploiting both visual and semantic features allow to analyze what are the most important aspects for identifying exotic content. Ground truth has been gathered by retrieving exotic and not exotic images through a web search engine by posing queries with exotic and not exotic semantics, and then assessing the exoticism of the retrieved images via a crowdsourcing evaluation. The dataset is publicly released to promote advances in this novel field.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3206025.3206044',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'An Empirical Analysis of Backward Compatibility in Machine Learning Systems',\n",
       "  'authors': \"['Megha Srivastava', 'Besmira Nushi', 'Ece Kamar', 'Shital Shah', 'Eric Horvitz']\",\n",
       "  'date': 'August 2020',\n",
       "  'source': \"KDD '20: Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining\",\n",
       "  'abstract': 'In many applications of machine learning (ML), updates are performed with the goal of enhancing model performance. However, current practices for updating models rely solely on isolated, aggregate performance analyses, overlooking important dependencies, expectations, and needs in real-world deployments. We consider how updates, intended to improve ML models, can introduce new errors that can significantly affect downstream systems and users. For example, updates in models used in cloud-based classification services, such as image recognition, can cause unexpected erroneous behavior in systems that make calls to the services. Prior work has shown the importance of \"backward compatibility\" for maintaining human trust. We study challenges with backward compatibility across different ML architectures and datasets, focusing on common settings including data shifts with structured noise and ML employed in inferential pipelines. Our results show that (i) compatibility issues arise even without data shift due to optimization stochasticity, (ii) training on large-scale noisy datasets often results in significant decreases in backward compatibility even when model accuracy increases, and (iii) distributions of incompatible points align with noise bias, motivating the need for compatibility aware de-noising and robustness methods.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3394486.3403379',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Mining Exoticism from Visual Content with Fusion-based Deep Neural Networks',\n",
       "  'authors': \"['Andrea Ceroni', 'Chenyang Ma', 'Ralph Ewerth']\",\n",
       "  'date': 'June 2018',\n",
       "  'source': \"ICMR '18: Proceedings of the 2018 ACM on International Conference on Multimedia Retrieval\",\n",
       "  'abstract': 'Exoticism is the charm of the unfamiliar, it often means unusual, mystery, and it can evoke the atmosphere of remote lands. Although it has received interest in different arts, like painting and music, no study has been conducted on understanding exoticism from a computational perspective. To the best of our knowledge, this work is the first to explore the problem of exoticism-aware image classification, aiming at automatically measuring the amount of exoticism in images and investigating the significant aspects of the task. The estimation of image exoticism could be applied in fields like advertising and travel suggestion, as well as to increase serendipity and diversity of recommendations and search results. We propose a Fusion-based Deep Neural Network (FDNN) for this task, which combines image representations learned by Deep Neural Networks with visual and semantic hand-crafted features. Comparisons with other Machine Learning models show that our proposed architecture is the best performing one, reaching accuracy over 83% and 91% on two different datasets. Moreover, experiments with classifiers exploiting both visual and semantic features allow to analyze what are the most important aspects for identifying exotic content. Ground truth has been gathered by retrieving exotic and not exotic images through a web search engine by posing queries with exotic and not exotic semantics, and then assessing the exoticism of the retrieved images via a crowdsourcing evaluation. The dataset is publicly released to promote advances in this novel field.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3206025.3206044',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': \"Feature Selection for the Classification of Alzheimer's Disease Data\",\n",
       "  'authors': \"['Hany Alashwal', 'Areeg Abdalla', 'Mohamed El Halaby', 'Ahmed A. Moustafa']\",\n",
       "  'date': 'January 2020',\n",
       "  'source': \"ICSIM '20: Proceedings of the 3rd International Conference on Software Engineering and Information Management\",\n",
       "  'abstract': \"In this paper, we describe the features of our large dataset (6400+ rows and 400+ features) that includes Alzheimer's disease (AD) patients, individuals with mild cognitive impairment (MCI, prodromal stage of Alzheimer's disease), and healthy individuals (without AD or MCI). We also, present a feature selection method applied on the dataset. Unlike prior data mining models that were applied to AD, our dataset is big in nature and includes genetic, neural, nutritional, and cognitive measures of all the individuals. All of these measures in the data have been shown by empirical studies to be related to the development of AD. We used a random forest classifier to discover which features best classify and differentiate between AD patients and healthy individuals. Identifying these features will likely provide evidence for protective factors against the development of AD.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3378936.3378982',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'PrivacyCheck: Automatic Summarization of Privacy Policies Using Data Mining',\n",
       "  'authors': \"['Razieh Nokhbeh Zaeem', 'Rachel L. German', 'K. Suzanne Barber']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Internet Technology',\n",
       "  'abstract': 'Prior research shows that only a tiny percentage of users actually read the online privacy policies they implicitly agree to while using a website. Prior research also suggests that users ignore privacy policies because these policies are lengthy and, on average, require 2 years of college education to comprehend. We propose a novel technique that tackles this problem by automatically extracting summaries of online privacy policies. We use data mining models to analyze the text of privacy policies and answer 10 basic questions concerning the privacy and security of user data, what information is gathered from them, and how this information is used. In order to train the data mining models, we thoroughly study privacy policies of 400 companies (considering 10% of all listings on NYSE, Nasdaq, and AMEX stock markets) across industries. Our free Chrome browser extension, PrivacyCheck, utilizes the data mining models to summarize any HTML page that contains a privacy policy. PrivacyCheck stands out from currently available counterparts because it is readily applicable on any online privacy policy. Cross-validation results show that PrivacyCheck summaries are accurate 40% to 73% of the time. Over 400 independent Chrome users are currently using PrivacyCheck.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3127519',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Association Rule Mining of Anaphora Based on ParCorFull Corpus',\n",
       "  'authors': \"['Siqiao Guo', 'Xianbo Li', 'Zhixin Ma']\",\n",
       "  'date': 'January 2020',\n",
       "  'source': \"ICCDE '20: Proceedings of 2020 6th International Conference on Computing and Data Engineering\",\n",
       "  'abstract': 'Based on the Apriori algorithm, the association rules of the anaphora in ParCorFull corpus are mined, and the similarities and differences between the written and spoken language represented by speech and discussion are compared. The rules of the whole mining show that the most important anaphora types in news, speech and discussion styles are noun phrases and pronouns respectively. In news style, pronoun is easier to realize the function of antecedent, while in speech and discussion style, noun phrase is more commonly used as anaphora. The rules of local mining show that in journalism, verb phrases are generally not omitted, noun phrases are more directly appeared without modification, and pronouns often point to singular person entity objects with the subject. The rules of speech and discussion are more flexible. Subordinate clauses are all used as anaphoric objects in anaphoric without anaphoric functions.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3379247.3379277',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Adaptive-Halting Policy Network for Early Classification',\n",
       "  'authors': \"['Thomas Hartvigsen', 'Cansu Sen', 'Xiangnan Kong', 'Elke Rundensteiner']\",\n",
       "  'date': 'July 2019',\n",
       "  'source': \"KDD '19: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining\",\n",
       "  'abstract': 'Early classification of time series is the prediction of the class label of a time series before it is observed in its entirety. In time-sensitive domains where information is collected over time it is worth sacrificing some classification accuracy in favor of earlier predictions, ideally early enough for actions to be taken. However, since accuracy and earliness are contradictory objectives, a solution must address this challenge to discover task-dependent trade-offs. We design an early classification model, called EARLIEST, which tackles this multi-objective optimization problem, jointly learning (1) to classify time series and (2) at which timestep to halt and generate this prediction. By learning the objectives together, we achieve a user-controlled balance between these contradictory goals while capturing their natural relationship. Our model consists of the novel pairing of a recurrent discriminator network with a stochastic policy network, with the latter learning a halting-policy as a reinforcement learning task. The learned policy interprets representations generated by the recurrent model and controls its dynamics, sequentially deciding whether or not to request observations from future timesteps. For a rich variety of datasets (four synthetic and three real-world), we demonstrate that EARLIEST consistently out-performs state-of-the-art alternatives in accuracy and earliness while discovering signal locations without supervision.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3292500.3330974',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Mutual information using sample variance for text feature selection',\n",
       "  'authors': \"['Deepak Agnihotri', 'Kesari Verma', 'Priyanka Tripathi']\",\n",
       "  'date': 'November 2017',\n",
       "  'source': \"ICCIP '17: Proceedings of the 3rd International Conference on Communication and Information Processing\",\n",
       "  'abstract': 'Feature selection improves the training speed of the classifier without affecting its predictive capability. It selects a subset of most informative words (terms) from the set of all words. Term distribution affects the feature selection process, e.g. an even distribution of terms in a specific class ensures a higher association of these terms with that class, but an even distribution in almost classes shows a lesser association. This paper computes sample variance using standard Mutual Information (MI) method to measure the variations in distribution of terms. MI method assigns a higher rank to the terms distributed in a specific category (i.e. rare terms) which shows it strong influence with the rare terms than common terms (i.e. terms which most frequently in almost classes). To address this issue, a new text feature selection method named Mutual Information Using Sample Variance (MIUSV) is proposed in this paper. It considers sample variance in term distribution while computing the Mutual Information score of the term. Multinomial Naive Bayes (MNB) and k Nearest Neighbor (kNN) classifiers model, check the utilities of the selected terms by the proposed MIUSV. These models classify four standard text data sets, viz. Webkb, 20Newsgroup, Ohsumed10, and Ohsumed23. Two standard performance measures named Macro-F1 and Micro-F1 show a significant improvement in the results using proposed MIUSV method.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3162957.3163054',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A Reduced Network Traffic Method for IoT Data Clustering',\n",
       "  'authors': \"['Ricardo De Azevedo', 'Gabriel Resende Machado', 'Ronaldo Ribeiro Goldschmidt', 'Ricardo Choren']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Knowledge Discovery from Data',\n",
       "  'abstract': 'Internet of Things (IoT) systems usually involve interconnected, low processing capacity, and low memory sensor nodes (devices) that collect data in several sorts of applications that interconnect people and things. In this scenario, mining tasks, such as clustering, have been commonly deployed to detect behavioral patterns from the collected data. The centralized clustering of IoT data demands high network traffic to transmit the data from the devices to a central node, where a clustering algorithm must be applied. This approach does not scale as the number of devices increases, and the amount of data grows. However, distributing the clustering process through the devices may not be a feasible approach as well, since the devices are usually simple and may not have the ability to execute complex procedures. This work proposes a centralized IoT data clustering method that demands reduced network traffic and low processing power in the devices. The proposed method uses a data grid to summarize the information at the devices before transmitting it to the central node, reducing network traffic. After the data transfer, the proposed method applies a clustering algorithm that was developed to process data in the summarized representation. Tests with seven datasets provided experimental evidence that the proposed method reduces network traffic and produces results comparable to the ones generated by DBSCAN and HDBSCAN, two robust centralized clustering algorithms.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3423139',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Cell Mechanics Based Computational Classification of Red Blood Cells Via Machine Intelligence Applied to Morpho-Rheological Markers',\n",
       "  'authors': \"['Yan Ge', 'Philipp Rosendahl', 'Claudio Durán', 'Nicole Töpfner', 'Sara Ciucci', 'Jochen Guck', 'Carlo Vittorio Cannistraci']\",\n",
       "  'date': 'None',\n",
       "  'source': 'IEEE/ACM Transactions on Computational Biology and Bioinformatics',\n",
       "  'abstract': 'Despite fluorescent cell-labelling being widely employed in biomedical studies, some of its drawbacks are inevitable, with unsuitable fluorescent probes or probes inducing a functional change being the main limitations. Consequently, the demand for and development of label-free methodologies to classify cells is strong and its impact on precision medicine is relevant. Towards this end, high-throughput techniques for cell mechanical phenotyping have been proposed to get a multidimensional biophysical characterization of single cells. With this motivation, our goal here is to investigate the extent to which an unsupervised machine learning methodology, which is applied exclusively on morpho-rheological markers obtained by real-time deformability and fluorescence cytometry (RT-FDC), can address the difficult task of providing label-free discrimination of reticulocytes from mature red blood cells. We focused on this problem, since the characterization of reticulocytes (their percentage and cellular features) in the blood is vital in multiple human disease conditions, especially bone-marrow disorders such as anemia and leukemia. Our approach reports promising label-free results in the classification of reticulocytes from mature red blood cells, and it represents a step forward in the development of high-throughput morpho-rheological-based methodologies for the computational categorization of single cells. Besides, our methodology can be an alternative but also a complementary method to integrate with existing cell-labelling techniques.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/TCBB.2019.2945762',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A data modeling approach for classification problems: application to bank telemarketing prediction',\n",
       "  'authors': \"['Stéphane Cédric Koumetio Tekouabou', 'Walid Cherif', 'Hassan Silkan']\",\n",
       "  'date': 'March 2019',\n",
       "  'source': \"NISS '19: Proceedings of the 2nd International Conference on Networking, Information Systems &amp; Security\",\n",
       "  'abstract': 'In this paper, we present a new data modeling approach for five common classification algorithms to optimize the prediction of telemarketing target calls for selling bank long-term deposits. A Portuguese retail bank addressed, from 2008 until 2013, data on its clients, products and social-economic attributes including the effects of the financial crisis. An original set of 150 features has been explored and 21 features are retained for the proposed approach including label. This paper introduces a new modeling approach that preprocessed separately each type of features and normalize them to optimize prediction performance. To evaluate the proposed approach, this paper compares the results obtained with five most known machine learning techniques: Naïve Bayes (NB), Logistic Regression (LR), Decision Trees (DT), Artificial Neural Network (ANN) and Support Vector Machines (SVM) and it yielded better improved performances for all these algorithms in terms of accuracy and f-measure.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3320326.3320389',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Gender Classification from Fingerprint-images using Deep Learning Approach',\n",
       "  'authors': \"['Beanbonyka Rim', 'Junseob Kim', 'Min Hong']\",\n",
       "  'date': 'October 2020',\n",
       "  'source': \"RACS '20: Proceedings of the International Conference on Research in Adaptive and Convergent Systems\",\n",
       "  'abstract': 'Accurate gender classification from fingerprint-images brings benefits to various forensic, security and authentication analysis. Those benefits help to narrow down the space for searching and speed up the process for matching for applications such as automatic fingerprint identification systems (AFIS). However, achieving high prediction accuracy without human intervention (such as preprocessing and hand-crafted feature extraction) is currently and potentially a challenge. Therefore, this paper presents a deep learning method to automatically and conveniently estimate gender from fingerprint-images. In particular, the VGG-19, ResNet-50 and EfficientNet-B3 model were exploited to train from scratch. The raw images of fingerprints were fed into the networks for end-to-end learning. The networks trained on 8,000 images, validated on 1,520 images and tested on 360 images. Our experimental results showed that by comparing between those state-of-the-art models (VGG-19, ResNet-50 and EfficientNet-B3), EfficientNet-B3 model achieved the best accuracy of 97.89%, 69.86% and 63.05% for training, validating, and testing, respectively.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3400286.3418237',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Design, implementation and test of a flexible tor-oriented web mining toolkit',\n",
       "  'authors': \"['Alessandro Celestini', 'Stefano Guarino']\",\n",
       "  'date': 'June 2017',\n",
       "  'source': \"WIMS '17: Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics\",\n",
       "  'abstract': 'Searching and retrieving information from the Web is a primary activity needed to monitor the development and usage of Web resources. Possible benefits include improving user experience (e.g. by optimizing query results) and enforcing data/user security (e.g. by identifying harmful websites). Motivated by the lack of ready-to-use solutions, in this paper we present a flexible and accessible toolkit for structure and content mining, able to crawl, download, extract and index resources from the Web. While being easily configurable to work in the \"surface\" Web, our suite is specifically tailored to explore the Tor dark Web, i.e. the ensemble of Web servers composing the world\\'s most famous darknet. Notably, the toolkit is not just a Web scraper, but it includes two mining modules, respectively able to prepare content to be fed to an (external) semantic engine, and to reconstruct the graph structure of the explored portion of the Web. Other than discussing in detail the design, features and performance of our toolkit, we report the findings of a preliminary run over Tor, that clarify the potential of our solution.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3102254.3102266',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'XAI beyond classification: interpretable neural clustering',\n",
       "  'authors': \"['Xi Peng', 'Yunfan Li', 'Ivor W. Tsang', 'Hongyuan Zhu', 'Jiancheng Lv', 'Joey Tianyi Zhou']\",\n",
       "  'date': 'None',\n",
       "  'source': 'The Journal of Machine Learning Research',\n",
       "  'abstract': 'In this paper, we study two challenging problems in explainable AI (XAI) and data clustering. The first is how to directly design a neural network with inherent interpretability, rather than giving post-hoc explanations of a black-box model. The second is implementing discrete k-means with a differentiable neural network that embraces the advantages of parallel computing, online clustering, and clustering-favorable representation learning. To address these two challenges, we design a novel neural network, which is a differentiable reformulation of the vanilla k-means, called in-Terpretable nEuraL cLustering (TELL). Our contributions are threefold. First, to the best of our knowledge, most existing XAI works focus on supervised learning paradigms. This work is one of the few XAI studies on unsupervised learning, in particular, data clustering. Second, TELL is an interpretable, or the so-called intrinsically explainable and transparent model. In contrast, most existing XAI studies resort to various means for understanding a black-box model with post-hoc explanations. Third, from the view of data clustering, TELL possesses many properties highly desired by k-means, including but not limited to online clustering, plug-and-play module, parallel computing, and provable convergence. Extensive experiments show that our method achieves superior performance comparing with 14 clustering approaches on three challenging data sets.',\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3586589.3586595',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A Geometric Approach for CAD Models Classification based on Shallow Learning',\n",
       "  'authors': \"['Min Wan', 'Hang Guo', 'Wenjuan Jian']\",\n",
       "  'date': 'October 2020',\n",
       "  'source': 'CAIH2020: Proceedings of the 2020 Conference on Artificial Intelligence and Healthcare',\n",
       "  'abstract': 'Automated classification of three-dimensional geometric models is significant for three-dimensional shape retrieval as well as artificial intelligence application in areas such as civil engineering and mechanical engineering. In this paper, we proposed a geometric approach to classify three-dimensional CAD models. More than 16,000 CAD models were collected from 27 construction projects. Bounding boxes, convex hulls, and alpha shapes were computed on all CAD models and corresponding parameters were extracted as features. Classic supervised learning methods were applied to the extracted features and trained classifiers were evaluated on the collected data to show the efficiency and effectiveness of our method.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3433996.3434002',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'HeteGCN: Heterogeneous Graph Convolutional Networks for Text Classification',\n",
       "  'authors': \"['Rahul Ragesh', 'Sundararajan Sellamanickam', 'Arun Iyer', 'Ramakrishna Bairi', 'Vijay Lingam']\",\n",
       "  'date': 'March 2021',\n",
       "  'source': \"WSDM '21: Proceedings of the 14th ACM International Conference on Web Search and Data Mining\",\n",
       "  'abstract': 'We consider the problem of learning efficient and inductive graph convolutional networks for text classification with a large number of examples and features. Existing state-of-the-art graph embedding based methods such as predictive text embedding (PTE) and TextGCN have shortcomings in terms of predictive performance, scalability and inductive capability. To address these limitations, we propose a heterogeneous graph convolutional network (HeteGCN) modeling approach that unites the best aspects of PTE and TextGCN together. The main idea is to learn feature embeddings and derive document embeddings using a HeteGCN architecture with different graphs used across layers. We simplify TextGCN by dissecting into several HeteGCN models which (a) helps to study the usefulness of individual models and (b) offers flexibility in fusing learned embeddings from different models. In effect, the number of model parameters is reduced significantly, enabling faster training and improving performance in small labeled training set scenario. Our detailed experimental studies demonstrate the efficacy of the proposed approach.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3437963.3441746',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Research on Ecological Concept of Environmental Art Design Based on Data Mining Technology',\n",
       "  'authors': \"['Xianghui Zhang']\",\n",
       "  'date': 'September 2021',\n",
       "  'source': 'ICISCAE 2021: 2021 4th International Conference on Information Systems and Computer Aided Education',\n",
       "  'abstract': \"With the rapid development of economy and the continuous improvement of people's living standards, people pay more and more attention to the needs of the spiritual world. Therefore, higher requirements are put forward for environmental art design. With the promotion of global economic integration, people's quality of life has been effectively improved, but at the same time, the rapid development of industrial industries has caused serious pollution to the ecological environment, which has seriously affected people's health and life. At present, people pay more and more attention to the relationship between architecture and ecology, and put forward higher requirements for environmental art design. Infiltrating ecological concept into environmental art design can not only strengthen the artistic effect of design, but also promote the organic combination of ecology and architecture, and realize the common development of ecological benefits and social benefits. Based on data mining technology, this paper explores the principle of adhering to ecological concept in environmental art design, and puts forward the application countermeasures of ecological concept in environmental design, so as to improve the quality of environmental art design and enhance ecological benefits.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3482632.3487505',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Representation Learning for Classification in Heterogeneous Graphs with Application to Social Networks',\n",
       "  'authors': \"['Ludovic Dos Santos', 'Benjamin Piwowarski', 'Ludovic Denoyer', 'Patrick Gallinari']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Knowledge Discovery from Data',\n",
       "  'abstract': 'We address the task of node classification in heterogeneous networks, where the nodes are of different types, each type having its own set of labels, and the relations between nodes may also be of different types. A typical example is provided by social networks where node types may for example be users, content, or films, and relations friendship, like, authorship. Learning and performing inference on such heterogeneous networks is a recent task requiring new models and algorithms. We propose a model, Labeling Heterogeneous Network (LaHNet), a transductive approach to classification that learns to project the different types of nodes into a common latent space. This embedding is learned so as to reflect different characteristics of the problem such as the correlation between node labels, as well as the graph topology. The application focus is on social graphs, but the algorithm is general and can be used for other domains. The model is evaluated on five datasets representative of different instances of social data.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3201603',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Interpolation-based k-means Clustering Improvement for Sparse, High Dimensional Data',\n",
       "  'authors': \"['Wanghu Chen', 'Zhen Tian']\",\n",
       "  'date': 'August 2019',\n",
       "  'source': \"ICCBDC '19: Proceedings of the 2019 3rd International Conference on Cloud and Big Data Computing\",\n",
       "  'abstract': 'The k-means algorithm is characterized by simple implementation and fast speed, and is the most widely used clustering algorithm. Aiming at the shortcomings of k-means algorithm in noise sensitivity in high-dimensional sparse data sets, the IB k-means (Interpolation-based k-means clustering) algorithm is proposed. Based on the k-means algorithm, the genetic algorithm is used for interpolation, which solves the problem that the sparse data in k-means clustering is easy to merge. The experimental results show that compared with several improved k-means-based clustering methods, the proposed method can achieve better clustering effect and better deal with clustering in high-dimensional sparse data.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3358505.3358517',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Shifu: Deep Learning Based Advisor-advisee Relationship Mining in Scholarly Big Data',\n",
       "  'authors': \"['Wei Wang', 'Jiaying Liu', 'Feng Xia', 'Irwin King', 'Hanghang Tong']\",\n",
       "  'date': 'April 2017',\n",
       "  'source': \"WWW '17 Companion: Proceedings of the 26th International Conference on World Wide Web Companion\",\n",
       "  'abstract': 'Scholars in academia are involved in various social relationships such as advisor-advisee relationships. The analysis of such relationship can provide invaluable information for understanding the interactions among scholars as well as providing many researcher-specific applications such as advisor recommendation and academic rising star identification. However, in most cases, high quality advisor-advisee relationship dataset is unavailable. To address this problem, we propose Shifu, a deep-learning-based advisor-advisee relationship identification method which takes into account both the local properties and network characteristics. In particular, we explore how to crawl advisor-advisee pairs from PhDtree project and extract their publication information by matching them with DBLP dataset as the experimental dataset. To the best of our knowledge, no prior effort has been made to address the scientific collaboration network features for relationship identification by exploiting deep learning. Our experiments demonstrate that the proposed method outperforms other state-of-the-art machine learning methods in precision (94%). Furthermore, we apply Shifu to the entire DBLP dataset and obtain a large-scale advisor-advisee relationship dataset.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3041021.3054159',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Customer Churn Prediction In Telecommunication Industry Using Machine Learning Classifiers',\n",
       "  'authors': \"['Nurul Izzati Mohammad', 'Saiful Adli Ismail', 'Mohd Nazri Kama', 'Othman Mohd Yusop', 'Azri Azmi']\",\n",
       "  'date': 'August 2019',\n",
       "  'source': 'ICVISP 2019: Proceedings of the 3rd International Conference on Vision, Image and Signal Processing',\n",
       "  'abstract': 'Customer churn is one of the main problems in telecommunication industry. This study aims to identify the factors that influence customer churn and develop an effective churn prediction model as well as provide best analysis of data visualization results. The dataset has been collected from Kaggle open data website. The proposed methodology for analysis of churn prediction covers several phases: data pre-processing, analysis, implementing machine learning algorithms, evaluation of the classifiers and choose the best one for prediction. Data preprocessing process involved three major action, which are data cleaning, data transformation and feature selection. Machine learning classifiers was chosen are Logistic Regression, Artificial Neural Network and Random Forest. Then, classifiers were evaluated by using performance measurement which are accuracy, precision, recall and error rate in order to find the best classifier. Based on this study, the output shows that logistic regression outperform compared to artificial neural network and random forest.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3387168.3387219',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Extracting Relations Between Organizational Patterns Using Association Mining',\n",
       "  'authors': \"['Shakirullah Waseeb', 'Waheedullah Sulaiman Khail', 'Haji Gul Wahaj', 'Valentino Vranić']\",\n",
       "  'date': 'July 2020',\n",
       "  'source': \"EuroPLoP '20: Proceedings of the European Conference on Pattern Languages of Programs 2020\",\n",
       "  'abstract': 'Patterns are powerful when used in combinations. Identifying relationships between patterns is challenging. The existing approaches and pattern formats reflect the relationships with other patterns in a very informal and traditional way. We are proposing an automatic approach which discover such relationships from the patterns descriptive text using text mining and natural language processing techniques. In this work, we demonstrate how it contributes in inference of relationships and its strength among patterns.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3424771.3424817',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Cluster management framework for autonomic machine learning platform',\n",
       "  'authors': \"['Heejin Kim', 'Younggwan Kim', 'Jiman Hong']\",\n",
       "  'date': 'September 2019',\n",
       "  'source': \"RACS '19: Proceedings of the Conference on Research in Adaptive and Convergent Systems\",\n",
       "  'abstract': 'Autonomic machine learning platforms must provide the necessary management tasks while monitoring the execution status of remotely running machine learning tasks and the performance of the model being trained. In this paper, we design a cluster management framework. The proposed cluster management framework monitors distributed computing resources so that it helps the autonomic machine learning platform to select the proper machine learning algorithm and to execute the proper machine learning model.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3338840.3355691',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Mining string patterns for individuating reading pathologies',\n",
       "  'authors': \"['Fabio Fassetti', 'Ilaria Fassetti']\",\n",
       "  'date': 'April 2018',\n",
       "  'source': \"SAC '18: Proceedings of the 33rd Annual ACM Symposium on Applied Computing\",\n",
       "  'abstract': 'Tachistoscopes are devices that display a word for several seconds and ask the user to write down the word. They have been widely employed to increase recognition speed, to increase reading comprehension and, specially, to individuate reading difficulties and disabilities. Once the therapist is provided with the answers of the patients, a challenging problem is the analysis of the strings to individuate common patterns in the erroneous strings that could raise suspicion of related disabilities. In this direction, this work presents a machine-learning technique aimed at mining exceptional string patterns and precisely designed to tackle the above mentioned problem. The technique is based on non-negative matrix factorization (nnmf) and exploits as features the structure of the words in terms of the letters composing them. To the best of our knowledge this is the first attempt of mining tachistoscope answers to discover intrinsic peculiarities of the words possibly involved in reading disabilities. From the technical point of view, we present a novel variant of nnmf methods with the adjunctive goal of discriminating between sets. The technique has been experimented in a real case study with the help of a speech therapist center.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3167132.3167161',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Text-Independent Speaker ID for Automatic Video Lecture Classification Using Deep Learning',\n",
       "  'authors': \"['Ali Shariq Imran', 'Zenun Kastrati', 'Torbjørn Karl Svendsen', 'Arianit Kurti']\",\n",
       "  'date': 'April 2019',\n",
       "  'source': \"ICCAI '19: Proceedings of the 2019 5th International Conference on Computing and Artificial Intelligence\",\n",
       "  'abstract': 'This paper proposes to use acoustic features employing deep neural network (DNN) and convolutional neural network (CNN) models for classifying video lectures in a massive open online course (MOOC). The models exploit the voice pattern of the lecturer for identification and for classifying the video lecture according to the right speaker category. Filter bank and Mel frequency cepstral coefficient (MFCC) feature along with first and second order derivatives (Δ/ΔΔ) are used as input features to the proposed models. These features are extracted from the speech signal which is obtained from the video lectures by separating the audio from the video using FFmpeg. The deep learning models are evaluated using precision, recall, and F1 score and the obtained accuracy is compared for both acoustic features with traditional machine learning classifiers for speaker identification. A significant improvement of 3% to 7% classification accuracy is achieved over the DNN and twice to that of shallow machine learning classifiers for 2D-CNN with MFCC. The proposed 2D-CNN model with an F1 score of 85.71% for text-independent speaker identification makes it plausible to use speaker ID as a classification approach for organizing video lectures automatically in a MOOC setting.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3330482.3330508',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'The impact of automatic text translation on classification of online discussions for social and cognitive presences',\n",
       "  'authors': \"['Arthur Barbosa', 'Máverick Ferreira', 'Rafael Ferreira Mello', 'Rafael Dueire Lins', 'Dragan Gasevic']\",\n",
       "  'date': 'April 2021',\n",
       "  'source': 'LAK21: LAK21: 11th International Learning Analytics and Knowledge Conference',\n",
       "  'abstract': 'This paper reports the findings of a study that measured the effectiveness of employing automatic text translation methods in automated classification of online discussion messages according to the categories of social and cognitive presences. Specifically, we examined the classification of 1,500 Portuguese and 1,747 English discussion messages using classifiers trained on the datasets before and after the application of text translation. While the English model generated, with the original and translated texts, achieved results (accuracy and Cohen’s κ) similar to those of the previously reported studies, the translation to Portuguese led to a decrease in the performance. The indicates the general viability of the proposed approach when converting the text to English. Moreover, this study highlighted the importance of different features and resources, and the limitations of the resources for Portuguese as reasons of the results obtained.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3448139.3448147',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A Combination of Transfer Learning and Deep Learning for Medicinal Plant Classification',\n",
       "  'authors': \"['Nghia Duong-Trung', 'Luyl-Da Quach', 'Minh-Hoang Nguyen', 'Chi-Ngon Nguyen']\",\n",
       "  'date': 'February 2019',\n",
       "  'source': \"ICIIT '19: Proceedings of the 2019 4th International Conference on Intelligent Information Technology\",\n",
       "  'abstract': \"Medicinal plants are an important element of indigenous medical systems in Viet Nam. These resources are usually regarded as a part of culture's traditional knowledge. One of the prerequisites for any medical recommendation systems and/or applications is accurate identification and classification of medicinal plants. Hence, leveraging technology in automatic classification of these curative herbs has become essential. Unfortunately, building and training a machine learning model from scratch is next to impossible due to the lack of hardware infrastructure and finance support. It painfully restricts the requirements of rapid solutions to deal with the demand. For this purpose, this paper exploits the idea of transfer learning which is the improvement of learning in a new prediction task through the transferability of knowledge from a related prediction task that has already been learned. By utilizing state-of-the-art deep networks re-trained with our collected data, our extensive experiments show that the proposed combination performs perfectly and achieves the classification accuracy of 98.7% within the acceptable training time.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3321454.3321464',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'SDDSMOTE:Synthetic Minority Oversampling Technique based on Sample Density Distribution for Enhanced Classification on Imbalanced Microarray Data',\n",
       "  'authors': \"['Qikang Wan', 'Xiongshi Deng', 'Min Li', 'Haotian Yang']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"ICCDA '22: Proceedings of the 2022 6th International Conference on Compute and Data Analysis\",\n",
       "  'abstract': 'Microarray gene expression data contain an unbalanced distribution of data samples among different classes, which poses a challenge to machine learning-based cancer diagnosis. In addition, microarray data consists of small samples and a huge number of genes, which cause the curse of dimensionality. In order to enhance the performance of learning models on imbalanced microarray data, we propose a novel preprocessing method based on the SMOTE, named SDDSMOTE (Synthetic Minority Oversampling Technique based on Sample Density Distribution). The whole preprocessing includes two steps. First, by using a feature selection technology, irrelevant genes are eliminated and obtaining reduced gene data. Second, SDDSMOTE is used to rebalance the reduced data. We performed comprehensive experiments to compare SDDSMOTE with other state-of-the-art Oversampling algorithms using two Support Vector Machine and Logistic Regression on 8 publicly available microarray expression data sets. The experimental results show that SDDSMOTE outperforms compared algorithms in terms of various evaluation criteria, such as Accuracy, F-score, G-mean, and AUC, which indicates its superiority.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3523089.3523096',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Using constraint mining to analyze software development processes',\n",
       "  'authors': \"['Thomas Krismayer', 'Christoph Mayr-Dorn', 'Johann Tuder', 'Rick Rabiser', 'Paul Grünbacher']\",\n",
       "  'date': 'May 2019',\n",
       "  'source': \"ICSSP '19: Proceedings of the International Conference on Software and System Processes\",\n",
       "  'abstract': 'Most software development organizations nowadays use issue-tracking tools to manage software processes throughout the life-cycle. Still, understanding development processes, keeping track of process execution, and reacting to deviations in projects remains challenging. In particular, the actual process usually differs from the process perceived by developers, making it hard to define the processes developers are expected to carry out. This is further challenged by frequently changing processes and process variations in different projects and teams. In this paper we describe an empirical study in which we applied a constraint mining approach from the field of software monitoring to automatically extract process definitions in the form of constraints. Specifically, we applied the approach to datasets extracted from four real-world projects (using the Jira issue-tracking tool) in a company developing a recreational activities platform. The mined constraints describe the boundaries of the actual processes and thus help to understand process behavior. Constraints can be frequently re-mined to understand process evolution. The mined constraints can also be used to monitor future processes to detect problems in the development process early on. We involved a domain expert to evaluate the usefulness of our results and investigated to what extent the mined constraints reflect the official development process of the company. We also report mining results for different issue types, across projects, and over different time windows.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ICSSP.2019.00021',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Data Mining Techniques in Direct Marketing on Imbalanced Data using Tomek Link Combined with Random Under-sampling',\n",
       "  'authors': \"['Ümit Yılmaz', 'Cengiz Gezer', 'Zafer Aydın', 'V. Çağrı Güngör']\",\n",
       "  'date': 'May 2021',\n",
       "  'source': \"ICISDM '21: Proceedings of the 2021 5th International Conference on Information System and Data Mining\",\n",
       "  'abstract': 'Determining the potential customers is very important in direct marketing. Data mining techniques are one of the most important methods for companies to determine potential customers. However, since the number of potential customers is very low compared to the number of non-potential customers, there is a class imbalance problem that significantly affects the performance of data mining techniques. In this paper, different combinations of basic and advanced resampling techniques such as Synthetic Minority Oversampling Technique (SMOTE), Tomek Link, RUS, and ROS were evaluated to improve the performance of customer classification. Different feature selection techniques are used in order the decrease the number of non-informative features from the data such as Information Gain, Gain Ratio, Chi-squared, and Relief. Classification performance was compared and utilized using several data mining techniques, such as LightGBM, XGBoost, Gradient Boost, Random Forest, AdaBoost, ANN, Logistic Regression, Decision Trees, SVC, Bagging Classifier based on ROC AUC and sensitivity metrics. A combination of Tomek Link and Random Under-Sampling as a resampling technique and Chi-squared method as feature selection algorithm showed superior performance among the other combinations. Detailed performance evaluations demonstrated that with the proposed approach, LightGBM, which is a gradient boosting algorithm based on decision tree, gave the best results among the other classifiers with 0.947 sensitivity and 0.896 ROC AUC value.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3471287.3471299',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Search, Mining, and Their Applications on Mobile Devices: Introduction to the Special Issue',\n",
       "  'authors': \"['Hongning Wang', 'Rui Li', 'Milad Shokouhi', 'Hang Li', 'Yi Chang']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Information Systems',\n",
       "  'abstract': 'In recent years, mobile devices have become the most popular interface for users to retrieve and access information: recent reports show that users spend significantly more time and issue more search queries on mobile devices than on desktops in the United States.1 The accelerated growth of mobile usage brings unique opportunities to the information retrieval and data mining research communities.Mobile devices capture rich contextual and personal signals that can be leveraged to accurately predict users’ intent for serving more relevant content and can even proactively provide novel zero-query recommendations. Apple Siri, Google Now, and Microsoft Cortana are recent examples of such emerging systems. Furthermore, mobile devices constantly generate a huge amount of sensor footprints (e.g., GPS, motion sensors) and user activity data (e.g., used apps) that are often missing from their desktop counterparts. These new sources of implicit and explicit user feedback are valuable for discovering actionable knowledge, and designing better systems that serve each individual the right content at the right time and location. In addition, by aggregating mobile interactions across individuals, one can infer interesting conclusions beyond search and recommendation. Generating real-time traffic estimates is one example of such applications.This special issue focuses on research problems of search, mining, and their applications in mobile devices. Topics of interest in this special issue include but are not limited to mobile data mining and management, mobile search, personalization and recommendation, mobile user interfaces and human-computer interaction, and new applications in the mobile environment. The aim of this special issue is to bring together top experts across multiple disciplines, including information retrieval, data mining, mobile computing, and cyberphysical systems, such that academic and industrial researchers can exchange ideas and share the latest developments on the state of the art and practice of mobile search and mobile data mining.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3086665',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Deep Learning-Based Models for Classification of Invasive Plant Species from Hyperspectral Remotely Sensed Data',\n",
       "  'authors': \"['Abdulla A. Omeer', 'Ratnadeep R. Deshmukh']\",\n",
       "  'date': 'August 2021',\n",
       "  'source': \"DSMLAI '21': Proceedings of the International Conference on Data Science, Machine Learning and Artificial Intelligence\",\n",
       "  'abstract': 'Invasive plant species are plants, which spread extensively outside their native ecosystem. They pose a threat to the environment and economy at global and local scales, making effective mapping and detecting essential. This study aims to develop deep neural network-based classification models for hyperspectral data. The spectral reflectance of ten invasive plant leaves was collected using ASD FieldSpec4 standard Hi-Res device. Samples were collected from Dr. Babasaheb Ambedkar Marathwada university campus and Himayat Bagh garden in Aurangabad city, Maharashtra, India. Two types of deep neural networks (DNN) were applied: the first one is based on the one-dimensional convolutional neural network (1D CNN), and the second is based on the convolutional long short term memory (CNN-LSTM). We proposed and compare the performance of the two models with three existing models, multilayer perceptron (MLP), random forest (RF), and support vector machine (SVM). The CNN-LSTM model achieves the highest discrimination accuracy among all the other models with an overall test accuracy of 99.3% and an F1_score of 0.98. Moreover, the CNN model achieves a high classification accuracy of 97.3% and an F1 score of 0.97, which contains the inception layer. In the CNN-LSTM model, the convolutional layer extracts the input vector of spectral features and feeds them to the LSTM layer to capture contextual information. In 1D CNN, the inception layer concatenates the output of multiple kernel sizes and adds flexibility to the model to improve the classification performance. This study revealed that our proposed models provide better performance than traditional machine learning methods and simple deep learning ones.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3484824.3484884',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Classification of Imbalanced Documents by Feature Selection',\n",
       "  'authors': \"['Yusuke Adachi', 'Naoya Onimura', 'Takanori Yamashita', 'Sachio Hirokawa']\",\n",
       "  'date': 'May 2017',\n",
       "  'source': \"ICCDA '17: Proceedings of the International Conference on Compute and Data Analysis\",\n",
       "  'abstract': \"We previously worked on category classification problem of reuter 's newspaper article using SVM and feature selection. In the study, feature selection by SVM-score [Sakai, Hirokawa, 2012] showed high accuracy. It was also expected to be superior to other standard indicators in case data is imbalanced. This study aimed to show the effectiveness of feature selection by SVM-score in machine learning with imbalanced data. For the reuter's data, F-measure was calculated in the classification experiment of all 13 categories. As a result, feature selection by SVM-score shows high f-measure and precision. In addition, we found feature words of negative example improve the classification performance.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3093241.3093246',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Attributed Collaboration Network Embedding for Academic Relationship Mining',\n",
       "  'authors': \"['Wei Wang', 'Jiaying Liu', 'Tao Tang', 'Suppawong Tuarob', 'Feng Xia', 'Zhiguo Gong', 'Irwin King']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on the Web',\n",
       "  'abstract': 'Finding both efficient and effective quantitative representations for scholars in scientific digital libraries has been a focal point of research. The unprecedented amounts of scholarly datasets, combined with contemporary machine learning and big data techniques, have enabled intelligent and automatic profiling of scholars from this vast and ever-increasing pool of scholarly data. Meanwhile, recent advance in network embedding techniques enables us to mitigate the challenges of large scale and sparsity of academic collaboration networks. In real-world academic social networks, scholars are accompanied with various attributes or features, such as co-authorship and publication records, which result in attributed collaboration networks. It has been observed that both network topology and scholar attributes are important in academic relationship mining. However, previous studies mainly focus on network topology, whereas scholar attributes are overlooked. Moreover, the influence of different scholar attributes are unclear. To bridge this gap, in this work, we present a novel framework of Attributed Collaboration Network Embedding (ACNE) for academic relationship mining. ACNE extracts four types of scholar attributes based on the proposed scholar profiling model, including demographics, research, influence, and sociability. ACNE can learn a low-dimensional representation of scholars considering both scholar attributes and network topology simultaneously. We demonstrate the effectiveness and potentials of ACNE in academic relationship mining by performing collaborator recommendation on two real-world datasets and the contribution and importance of each scholar attribute on scientific collaborator recommendation is investigated. Our work may shed light on academic relationship mining by taking advantage of attributed collaboration network embedding.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3409736',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Transfer Learning for Molecular Cancer Classification Using Deep Neural Networks',\n",
       "  'authors': \"['Rahul K. Sevakula', 'Vikas Singh', 'Nishchal K. Verma', 'Chandan Kumar', 'Yan Cui']\",\n",
       "  'date': 'None',\n",
       "  'source': 'IEEE/ACM Transactions on Computational Biology and Bioinformatics',\n",
       "  'abstract': 'The emergence of deep learning has impacted numerous machine learning based applications and research. The reason for its success lies in two main advantages: 1) it provides the ability to learn very complex non-linear relationships between features and 2) it allows one to leverage information from unlabeled data that does not belong to the problem being handled. This paper presents a transfer learning procedure for cancer classification, which uses feature selection and normalization techniques in conjunction with s sparse auto-encoders on gene expression data. While classifying any two tumor types, data of other tumor types were used in unsupervised manner to improve the feature representation. The performance of our algorithm was tested on 36 two-class benchmark datasets from the GEMLeR repository. On performing statistical tests, it is clearly ascertained that our algorithm statistically outperforms several generally used cancer classification approaches. The deep learning based molecular disease classification can be used to guide decisions made on the diagnosis and treatment of diseases, and therefore may have important applications in precision medicine.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/TCBB.2018.2822803',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Enhanced situation space mining for data streams',\n",
       "  'authors': \"['Yisroel Mirsky', 'Tal Halpern', 'Rishabh Upadhyay', 'Sivan Toledo', 'Yuval Elovici']\",\n",
       "  'date': 'April 2017',\n",
       "  'source': \"SAC '17: Proceedings of the Symposium on Applied Computing\",\n",
       "  'abstract': \"Data streams can capture the situation which an actor is experiencing. Knowledge of the present situation is highly beneficial for a wide range of applications. An algorithm called pcStream can be used to extract situations from a numerical data stream in an unsupervised manner. Although pcStream outperforms other stream clustering algorithms at this task, pcStream has two major flaws. The first is its complexity due to continuously performing principal component analysis (PCA). The second is its difficulty in detecting emerging situations whose distributions overlap in the same feature space. In this paper we introduce pcStream2, a variant of pcStream which employs windowing and persistence in order to distinguish between emerging overlapping concepts. We also propose the use of incremental PCA (IPCA) to reduce the overall complexity and memory requirements of the algorithm. Although any IPCA algorithm can be used, we use a novel IPCA algorithm called Just-In-Time PCA which is better suited for processing streams. JIT-PCA makes intelligent 'short cuts' in order to reduce computations. We provide experimental results on real-world datasets that demonstrates how the proposed improvements make pcStream2 a more accurate and practical tool for situation space mining.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3019612.3019671',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Identification and Classification of Chinese Traditional Musical Instruments Based on Deep Learning Algorithm',\n",
       "  'authors': \"['Peipei Cao']\",\n",
       "  'date': 'January 2021',\n",
       "  'source': 'CONF-CDS 2021: The 2nd International Conference on Computing and Data Science',\n",
       "  'abstract': 'The classification of musical instruments based on deep learning is the application of deep learning in the direction of music information retrieval, which is a hot topic in the field of speech recognition in recent years. Deep learning is an important branch of artificial intelligence and a new direction of data mining in recent years. Deep learning is born from artificial neural networks. It has more hidden layers than shallow neural networks. This is the origin of the word \"depth \". Unlike traditional neural networks, deep learning increases unsupervised learning. Therefore, this paper studies whether we can use the powerful feature extraction ability of deep learning to study the classification algorithm of music genre recognition based on deep confidence network for the recognition and classification of music genres and traditional Chinese musical instruments. The experimental results show that the accuracy of the algorithm is as high as 99.2.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3448734.3450836',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Convolutional Neural Networks for Toxic Comment Classification',\n",
       "  'authors': \"['Spiros V. Georgakopoulos', 'Sotiris K. Tasoulis', 'Aristidis G. Vrahatis', 'Vassilis P. Plagianakos']\",\n",
       "  'date': 'July 2018',\n",
       "  'source': \"SETN '18: Proceedings of the 10th Hellenic Conference on Artificial Intelligence\",\n",
       "  'abstract': \"Flood of information is produced in a daily basis through the global internet usage arising from the online interactive communications among users. While this situation contributes significantly to the quality of human life, unfortunately it involves enormous dangers, since online texts with high toxicity can cause personal attacks, online harassment and bullying behaviors. This has triggered both industrial and research community in the last few years while there are several attempts to identify an efficient model for online toxic comment prediction. However, these steps are still in their infancy and new approaches and frameworks are required. On parallel, the data explosion that appears constantly, makes the construction of new machine learning computational tools for managing this information, an imperative need. Thankfully advances in hardware, cloud computing and big data management allow the development of Deep Learning approaches appearing very promising performance so far. For text classification in particular the use of Convolutional Neural Networks (CNN) have recently been proposed approaching text analytics in a modern manner emphasizing in the structure of words in a document. In this work, we employ this approach to discover toxic comments in a large pool of documents provided by a current Kaggle's competition regarding Wikipedia's talk page edits. To justify this decision we choose to compare CNNs against the traditional bag-of-words approach for text analysis combined with a selection of algorithms proven to be very effective in text classification. The reported results provide enough evidence that CNN enhance toxic comment classification reinforcing research interest towards this direction.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3200947.3208069',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'ROM: A Requirement Opinions Mining Method Preliminary Try Based on Software Review Data',\n",
       "  'authors': \"['Ying Wang', 'Liwei Zheng', 'Ning Li']\",\n",
       "  'date': 'January 2020',\n",
       "  'source': 'ICMSS 2020: Proceedings of the 2020 4th International Conference on Management Engineering, Software Engineering and Service Sciences',\n",
       "  'abstract': 'Requirement opinion mining aims to mine user opinions that can be used to help the mining of software requirements from various data sources. However, in the development of social network systems, software application platforms or stores and other data sources, the massive, noisy, non-standard data, makes the mining of effective requirement opinions more difficult. Therefore, there is less work in software requirements mining based on the data of software review in development social media or application market. This paper attempts to provide some knowledge support for requirement user story establishing in RE based on the opinion mining and clustering of massively software review data. First of all, this paper combines the requirements of the requirements engineering field to define the requirement opinions, functional requirement opinions and non-functional requirements opinions. Secondly, using the deep learning model to classify the functional requirement reviews and non-functional requirements reviews included in the reviews; Based on the differences between functional data and non-functional data, this paper defines three categories in the description of software functional data, and chooses to use sequence labeling methods to identify functional requirements. Then use the K-means clustering method based on word vector to cluster the review data, and combine TF-IDF and syntactic analysis to extract the aspect and aspect requirements or specific requirements of the requirement opinion respectively, so as to realize the requirement opinion mining of software review data. Finally, this article will give a case study based on the user review data of the mobile phone application service platform 360 mobile assistants.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3380625.3380665',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A particle swarm optimization based feature selection approach to transfer learning in classification',\n",
       "  'authors': \"['Bach Hoai Nguyen', 'Bing Xue', 'Peter Andreae']\",\n",
       "  'date': 'July 2018',\n",
       "  'source': \"GECCO '18: Proceedings of the Genetic and Evolutionary Computation Conference\",\n",
       "  'abstract': 'Transfer learning aims to use acquired knowledge from existing (source) domains to improve learning performance on a different but similar (target) domains. Feature-based transfer learning builds a common feature space, which can minimize differences between source and target domains. However, most existing feature-based approaches usually build a common feature space with certain assumptions about the differences between domains. The number of common features needs to be predefined. In this work, we propose a new feature-based transfer learning method using particle swarm optimization (PSO), where a new fitness function is developed to guide PSO to automatically select a number of original features and shift source and target domains to be closer. Classification performance is used in the proposed fitness function to maintain the discriminative ability of selected features in both domains. The use of classification accuracy leads to a minimum number of model assumptions. The proposed algorithm is compared with four state-of-the-art feature-based transfer learning approaches on three well-known real-world problems. The results show that the proposed algorithm is able to extract less than half of the original features with better performance than using all features and outperforms the four benchmark semi-supervised and unsupervised algorithms. This is the first time Evolutionary Computation, especially PSO, is utilized to achieve feature selection for transfer learning.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3205455.3205540',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Data Label System Classification Method of Power Equipment and Customers Based on Data Middle Platform',\n",
       "  'authors': \"['Xiaojing Lin', 'Hu Liu', 'Honggang Wang', 'Shi Liu', 'Min Guo', 'Wenjin Zhou']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': 'AIAM2021: 2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture',\n",
       "  'abstract': 'The development of digital information technology promotes the reform and reform of information digitization of electric power enterprises in the power industry. The role of data in business process is highlighted. As a data product, data label is more and more applied to business system. Power equipment and customers are the two core subjects of power grid power enterprises, including \"generation, transmission, distribution, transformation and utilization\". Through the construction of data label system, the application efficiency and application value of power data can be improved. The construction of corresponding label system can effectively enhance the safe and stable operation level of power grid and the lean management and service level of power grid enterprises. Data label classification is the basis of the construction of the label system, and the scientificity and rationality of the classification are very important to the promotion and application of data labels. This paper focuses on the core management resources of power enterprises, based on the data center of power enterprises, takes the business demand as the core, combines the typical classification methods and relevant theoretical basis, studies the classification method and benchmarking of big data label system of power enterprises Through the statistics of user tags and the calculation of root mean square error, the relevant conclusion is drawn that label classification can optimize customer energy efficiency and realize Internet precision marketing.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3495018.3495436',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Adaptive Spatial Clustering for Multi-Dimensional Data and Its Cloud Model Representation',\n",
       "  'authors': \"['Bin Gao', 'Xinhai Zhang', 'Xiaobin Xu', 'Yifeng Liu']\",\n",
       "  'date': 'April 2020',\n",
       "  'source': \"ICCAI '20: Proceedings of the 2020 6th International Conference on Computing and Artificial Intelligence\",\n",
       "  'abstract': 'In view of the problem that the number of clusters need to be set manually, it is difficult to process the multi-dimensional data effectively, and the clustering results are not described effectively when the multi-dimensional data need to be clustered. This paper proposes a method of adaptive spatial clustering and its cloud model representation for the multi-dimensional data. This method can be used to cluster multi-dimensional spatial data, form qualitative description of clustering results, and realize the reconstruction and verification of qualitative description features. Through simulation experiments, this method can cluster data adaptively without the need to set the number of clusters. At the same time, it has a good ability to abstract and reconstruct digital features.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3404555.3404634',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Water governance network analysis using graphlet mining',\n",
       "  'authors': \"['Apratim Das', 'Mike Drakos', 'Alex Aravind', 'Darwin Horning']\",\n",
       "  'date': 'August 2019',\n",
       "  'source': \"ASONAM '19: Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining\",\n",
       "  'abstract': 'Growing population, urbanization, increased sophistication in life, global warming, and climate change are some of the factors that can worsen the scarcity and quality of water in coming decades. Therefore, a sustainable water governance is essential across the world to face this challenge. In this paper, we study two water governance networks constructed from ground-truth using a new network analysis technique called graphlet analysis. Graphlet is gaining popularity in network analysis due to its power in exposing network structure and functions. To the best of our knowledge, this is the first work to apply graphlet analysis to study water governance network.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3341161.3343696',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A Novel Online Stacked Ensemble for Multi-Label Stream Classification',\n",
       "  'authors': \"['Alican Büyükçakir', 'Hamed Bonab', 'Fazli Can']\",\n",
       "  'date': 'October 2018',\n",
       "  'source': \"CIKM '18: Proceedings of the 27th ACM International Conference on Information and Knowledge Management\",\n",
       "  'abstract': 'As data streams become more prevalent, the necessity for online algorithms that mine this transient and dynamic data becomes clearer. Multi-label data stream classification is a supervised learning problem where each instance in the data stream is classified into one or more pre-defined sets of labels. Many methods have been proposed to tackle this problem, including but not limited to ensemble-based methods. Some of these ensemble-based methods are specifically designed to work with certain multi-label base classifiers; some others employ online bagging schemes to build their ensembles. In this study, we introduce a novel online and dynamically-weighted stacked ensemble for multi-label classification, called GOOWE-ML, that utilizes spatial modeling to assign optimal weights to its component classifiers. Our model can be used with any existing incremental multi-label classification algorithm as its base classifier. We conduct experiments with 4 GOOWE-ML-based multi-label ensembles and 7 baseline models on 7 real-world datasets from diverse areas of interest. Our experiments show that GOOWE-ML ensembles yield consistently better results in terms of predictive performance in almost all of the datasets, with respect to the other prominent ensemble models.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3269206.3271774',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Joint Distributed Representation of Text and Structure of Semi-Structured Documents',\n",
       "  'authors': \"['Abhishek Laddha', 'Salil Joshi', 'Samiulla Shaikh', 'Sameep Mehta']\",\n",
       "  'date': 'July 2018',\n",
       "  'source': \"HT '18: Proceedings of the 29th on Hypertext and Social Media\",\n",
       "  'abstract': 'Majority of textual data over web is in the form of semi-structured documents. Thus, structural skeleton of such documents plays important role in determining the semantics of the data content. Presence of structure sometimes allows us to write simple rules to extract such information, but it may not be always possible due to flexibility in the structure and the frequency with which such structures are altered. In this paper, we propose a joint modeling of text and the associated structure to effectively capture the semantics of the semi-structure documents. The model simultaneously learns the dense continuous representation for word tokens and the structure associated with them. We utilize the context of structures for projection such that similar structures containing semantically similar topics are close to each other in vector space. We explore two semantic text mining tasks over web data to test the effectiveness of our representation viz., document similarity, and table semantic component identification. In context of traditional rule-based approaches, both these tasks demand rich, domain-specific knowledge sources, homogeneous schema for the documents, and rules that capture the semantics. On the other hand, our approach is unsupervised and resource conscious in nature. Despite of working without knowledge resources and large training data, it performs at par with state-of-the-art rule based and other unsupervised approaches.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3209542.3209551',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Explainable Recommendation via Multi-Task Learning in Opinionated Text Data',\n",
       "  'authors': \"['Nan Wang', 'Hongning Wang', 'Yiling Jia', 'Yue Yin']\",\n",
       "  'date': 'June 2018',\n",
       "  'source': \"SIGIR '18: The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval\",\n",
       "  'abstract': \"Explaining automatically generated recommendations allows users to make more informed and accurate decisions about which results to utilize, and therefore improves their satisfaction. In this work, we develop a multi-task learning solution for explainable recommendation. Two companion learning tasks of user preference modeling for recommendation and opinionated content modeling for explanation are integrated via a joint tensor factorization. As a result, the algorithm predicts not only a user's preference over a list of items, i.e., recommendation, but also how the user would appreciate a particular item at the feature level, i.e., opinionated textual explanation. Extensive experiments on two large collections of Amazon and Yelp reviews confirmed the effectiveness of our solution in both recommendation and explanation tasks, compared with several existing recommendation algorithms. And our extensive user study clearly demonstrates the practical value of the explainable recommendations generated by our algorithm.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3209978.3210010',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A Novel Feature Hashing With Efficient Collision Resolution for Bag-of-Words Representation of Text Data',\n",
       "  'authors': \"['Bobby A. Eclarin', 'Arnel C. Fajardo', 'Ruji P. Medina']\",\n",
       "  'date': 'September 2018',\n",
       "  'source': \"NLPIR '18: Proceedings of the 2nd International Conference on Natural Language Processing and Information Retrieval\",\n",
       "  'abstract': \"Text Mining is widely used in many areas transforming unstructured text data from all sources such as patients' record, social media network, insurance data, and news, among others into an invaluable source of information. The Bag Of Words (BoW) representation is a means of extracting features from text data for use in modeling. In text classification, a word in a document is assigned a weight according to its frequency and frequency between different documents; therefore, words together with their weights form the BoW. One way to solve the issue of voluminous data is to use the feature hashing method or hashing trick. However, collision is inevitable and might change the result of the whole process of feature generation and selection. Using the vector data structure, the lookup performance is improved while resolving collision and the memory usage is also efficient.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3278293.3278301',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Towards Sustainable Dairy Management - A Machine Learning Enhanced Method for Estrus Detection',\n",
       "  'authors': \"['Kevin Fauvel', 'Véronique Masson', 'Élisa Fromont', 'Philippe Faverdin', 'Alexandre Termier']\",\n",
       "  'date': 'July 2019',\n",
       "  'source': \"KDD '19: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining\",\n",
       "  'abstract': 'Our research tackles the challenge of milk production resource use efficiency in dairy farms with machine learning methods. Reproduction is a key factor for dairy farm performance since cows milk production begin with the birth of a calf. Therefore, detecting estrus, the only period when the cow is susceptible to pregnancy, is crucial for farm efficiency. Our goal is to enhance estrus detection (performance, interpretability), especially on the currently undetected silent estrus (35% of total estrus), and allow farmers to rely on automatic estrus detection solutions based on affordable data (activity, temperature). In this paper, we first propose a novel approach with real-world data analysis to address both behavioral and silent estrus detection through machine learning methods. Second, we present LCE, a local cascade based algorithm that significantly outperforms a typical commercial solution for estrus detection, driven by its ability to detect silent estrus. Then, our study reveals the pivotal role of activity sensors deployment in estrus detection. Finally, we propose an approach relying on global and local (behavioral versus silent) algorithm interpretability (SHAP) to reduce the mistrust in estrus detection solutions.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3292500.3330712',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'DeepSinger: Singing Voice Synthesis with Data Mined From the Web',\n",
       "  'authors': \"['Yi Ren', 'Xu Tan', 'Tao Qin', 'Jian Luan', 'Zhou Zhao', 'Tie-Yan Liu']\",\n",
       "  'date': 'August 2020',\n",
       "  'source': \"KDD '20: Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining\",\n",
       "  'abstract': 'In this paper, we develop DeepSinger, a multi-lingual multi-singer singing voice synthesis (SVS) system, which is built from scratch using singing training data mined from music websites. The pipeline of DeepSinger consists of several steps, including data crawling, singing and accompaniment separation, lyrics-to-singing alignment, data filtration, and singing modeling. Specifically, we design a lyrics-to-singing alignment model to automatically extract the duration of each phoneme in lyrics starting from coarse-grained sentence level to fine-grained phoneme level, and further design a multi-lingual multi-singer singing model based on a feed-forward Transformer to directly generate linear-spectrograms from lyrics, and synthesize voices using Griffn-Lim. DeepSinger has several advantages over previous SVS systems: 1) to the best of our knowledge, it is the first SVS system that directly mines training data from music websites, 2) the lyrics-to-singing alignment model further avoids any human efforts for alignment labeling and greatly reduces labeling cost, 3) the singing model based on a feed-forward Transformer is simple and efficient, by removing the complicated acoustic feature modeling in parametric synthesis and leveraging a reference encoder to capture the timbre of a singer from noisy singing data, and 4) it can synthesize singing voices in multiple languages and multiple singers. We evaluate DeepSinger on our mined singing dataset that consists of about 92 hours data from 89 singers on three languages (Chinese, Cantonese and English). The results demonstrate that with the singing data purely mined from the Web, DeepSinger can synthesize high-quality singing voices in terms of both pitch accuracy and voice naturalness. Our audio samples are shown in https://speechresearch.github.io/deepsinger/.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3394486.3403249',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': \"Understanding E-learners' Behaviour Using Data Mining Techniques\",\n",
       "  'authors': \"['Muna Al Fanah', 'Muhammad Ayub Ansari']\",\n",
       "  'date': 'March 2019',\n",
       "  'source': \"ICBDE '19: Proceedings of the 2019 International Conference on Big Data and Education\",\n",
       "  'abstract': \"The information from Higher Education Institutions (HEIs) is primarily relevant for decision maker and educators. This study tackles e-learners behaviour using machine learning, particularly association rules and classifiers. Learners are characterized by a set of behaviours and attitudes that determine their learning abilities and skills. Learning from data generated by online learners may have significant impacts, however, few studies cover this resource from machine learning perspectives. We examine different data mining techniques including Random Forests, Logistic Regressions and Bayesian Networks as classifiers used for predicting e-learners' classes (High, Medium and Low). The novelty of this study is that it explores and compares classifiers performance on the behaviour of online learners on four variables: raise hands, visiting IT resources, view announcement and discussion impact on e-learners. The results of this study indicate an 80% accuracy level obtained by Bayesian Networks; in contrast, the Random Forests have only 63% accuracy level and Logistic Regressions for 58%.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3322134.3322145',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Machine Learning Based Graph Mining of Large-scale Network and Optimization',\n",
       "  'authors': \"['Mingyue Liu']\",\n",
       "  'date': 'May 2021',\n",
       "  'source': 'ICAIIS 2021: 2021 2nd International Conference on Artificial Intelligence and Information Systems',\n",
       "  'abstract': 'Network science possesses an unreplaceable status in solving social and scientific problems. This study focuses on investigating machine learning based graph mining of large-scale network and optimization by using the California road network dataset from Stanford as the large-scale social network. After building different neural networks with various hyperparameters and selected learning activation functions, accuracy results were compared and conclusions include: 1) Changing the activation functions does not have much effect on accuracy compared to neural network structures, 2) Changing the learning rate does not have much effect either and 3) exponential linear unit (ELU) is sensitive to the change of hidden layer size and kernel compared to rectifier linear unit (ReLU) and hyperbolic tangent (Tanh), causing decrement of accuracy after growth of hidden layer size brings overfitting. These conclusions were drawn based on numeric experiments where the final accuracy rate kept the average of every ten trials. However, given this specific dataset, the accuracy of all the models remain high so that varying neural network parameters or structures does not present much distinctive divergence.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3469213.3470320',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Deep Learning--based Text Classification: A Comprehensive Review',\n",
       "  'authors': \"['Shervin Minaee', 'Nal Kalchbrenner', 'Erik Cambria', 'Narjes Nikzad', 'Meysam Chenaghlu', 'Jianfeng Gao']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Computing Surveys',\n",
       "  'abstract': 'Deep learning--based models have surpassed classical machine learning--based approaches in various text classification tasks, including sentiment analysis, news categorization, question answering, and natural language inference. In this article, we provide a comprehensive review of more than 150 deep learning--based models for text classification developed in recent years, and we discuss their technical contributions, similarities, and strengths. We also provide a summary of more than 40 popular datasets widely used for text classification. Finally, we provide a quantitative analysis of the performance of different deep learning models on popular benchmarks, and we discuss future research directions.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3439726',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Research on Financial Technology Ability Evaluation of Global Systemically Important Banks Based on Data Mining',\n",
       "  'authors': \"['Kexin Yu']\",\n",
       "  'date': 'September 2021',\n",
       "  'source': 'ICISCAE 2021: 2021 4th International Conference on Information Systems and Computer Aided Education',\n",
       "  'abstract': 'Financial science and technology is a technology-driven financial innovation, which aims to use modern scientific and technological achievements to transform or innovate financial products, business models, business processes, etc., and to promote financial development, improve quality and increase efficiency. This paper uses data mining technology to construct an index of the degree of financial technology application of G-SIBs, and uses the 2019 G-SIBs data to examine the heterogeneous impact of financial technology application on different types of G-SIBs risks. The results show that: with the adjustment of G-SIBs itself and the standardization of supervision, the risk-taking level of G-SIBs is effectively suppressed. Non-systemic and significant banks have good risk-taking ability. In this process, G-SIBs seized some high-quality and low-risk customers of small and medium-sized banks due to its advantages in capital cost, which had crowding-out effect on small and medium-sized banks.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3482632.3482640',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A Gene-disease-based Machine Learning Approach to Identify Prostate Cancer Biomarkers',\n",
       "  'authors': \"['Osama Hamzeh', 'Luis Rueda']\",\n",
       "  'date': 'September 2019',\n",
       "  'source': \"BCB '19: Proceedings of the 10th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics\",\n",
       "  'abstract': \"Identifying biomarkers that can be used to classify certain disease stages, or identify when a disease becomes more aggressive is one of the most important applications of machine learning. Traditional biomarker identification approaches, typically, use machine learning techniques to identify a number of genes and macromolecules as biomarkers that can be used to diagnose specific diseases or states of diseases with very high accuracy, using molecular measurements such as mutations, gene expression, copy number variations, and others. However, Experts' opinions and knowledge is required to validate such findings. We propose a new machine learning model that incorporates a knowledge-based system used to integrate the findings of the DisGeNET database which is a framework that provides proven relationships among diseases and genes. The machine learning pipeline starts by reducing the number of features using a filter based feature selection method. The DisGeNET database is used to score each gene relating to the given cancer name. Then a wrapper-based feature-selection method picks the best set of genes with the highest classification accuracy. The method returned key genes from multiple data sets that classify with high accuracy while being biologically relevant, and no human intervention needed. Initial results provide a high area under the curve with a handful of genes that are already proven to be related to the relevant disease and state based on the latest published medical findings.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3307339.3343479',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Features Exploration for Grades Prediction using Machine Learning',\n",
       "  'authors': \"['Kevin Bouchard', 'Lucas Gonzales', 'Julien Maitre', 'Sébastien Gaboury']\",\n",
       "  'date': 'September 2020',\n",
       "  'source': \"GoodTechs '20: Proceedings of the 6th EAI International Conference on Smart Objects and Technologies for Social Good\",\n",
       "  'abstract': 'The province of Quebec in Canada has begun to implement an important plan to bring a digital shift to the educational system. One of the key aspects of this plan is to implement a global electronic student file system. These electronic files encompass a lot of information that can in turn be used to monitor the progress of the students. In this paper, our team was able to obtain a large dataset from this new technological platform and used it to predict the grade of students. We tested up to 328 features and produced different datasets for classification. Moreover, different features selection methods were used. Finally, we were able to predict the end of the year final grade with up to 75% accuracy.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3411170.3411232',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Emotional Contagion-Based Social Sentiment Mining in Social Networks by Introducing Network Communities',\n",
       "  'authors': \"['Xiaobao Wang', 'Di Jin', 'Mengquan Liu', 'Dongxiao He', 'Katarzyna Musial', 'Jianwu Dang']\",\n",
       "  'date': 'November 2019',\n",
       "  'source': \"CIKM '19: Proceedings of the 28th ACM International Conference on Information and Knowledge Management\",\n",
       "  'abstract': \"The rapid development of social media services has facilitated the communication of opinions through online news, blogs, microblogs, instant-messages, and so on. This article concentrates on the mining of readers' social sentiments evoked by social media materials. Existing methods are only applicable to a minority of social media like news portals with emotional voting information, while ignore the emotional contagion between writers and readers. However, incorporating such factors is challenging since the learned hidden variables would be very fuzzy (because of the short and noisy text in social networks). In this paper, we try to solve this problem by introducing a high-order network structure, i.e. communities. We first propose a new generative model called Community-Enhanced Social Sentiment Mining (CESSM), which 1) considers the emotional contagion between writers and readers to capture precise social sentiment, and 2) incorporates network communities to capture coherent topics. We then derive an inference algorithm based on Gibbs sampling. Empirical results show that, CESSM achieves significantly superior performance against the state-of-the-art techniques for text sentiment classification and interestingness in social sentiment mining.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3357384.3357941',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A novel network traffic classification approach via discriminative feature learning',\n",
       "  'authors': \"['Lixin Zhao', 'Lijun Cai', 'Aimin Yu', 'Zhen Xu', 'Dan Meng']\",\n",
       "  'date': 'March 2020',\n",
       "  'source': \"SAC '20: Proceedings of the 35th Annual ACM Symposium on Applied Computing\",\n",
       "  'abstract': 'Network traffic classification plays an important role in many network monitoring and security tasks. More recently, with the development of deep learning techniques, the performance of network traffic classification has been significantly improved due to the powerful feature representations learned by deep neural networks. Despite the great success that has been achieved, the problems of within-class diversity and between-class similarity are still big challenges. In this paper, we propose to train a CNN model by optimizing a new discriminative objective function, where apart from minimizing the empirical risk, a metric learning regularization term is also imposed on the learned features. This metric learning regularization term enforces the CNN model to learn more discriminative features in the mapped feature space, where the instances from the same class are closer together while the instances of different classes are farther apart. We conduct extensive experiments to evaluate the proposed method on three traffic datasets. The experimental results demonstrate that our proposed method outperforms the existing baseline methods and obtains state-of-the-art results on all the three datasets.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3341105.3373844',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Classification of vessel activity in streaming data',\n",
       "  'authors': \"['Ioannis Kontopoulos', 'Konstantinos Chatzikokolakis', 'Konstantinos Tserpes', 'Dimitris Zissis']\",\n",
       "  'date': 'July 2020',\n",
       "  'source': \"DEBS '20: Proceedings of the 14th ACM International Conference on Distributed and Event-based Systems\",\n",
       "  'abstract': 'In this paper we motivate the need for real-time vessel behaviour classification and describe in detail our event-based classification approach, as implemented in our real-world industry strong maritime event detection service at MarineTraffic.com. A novel approach is presented for the classification of vessel activity from real-time data streams. The proposed solution splits vessel trajectories into multiple overlapping segments and distinguishes the ones in which a vessel is engaged in trawling or longlining operation (e.g. fishing activity) from other segments that a vessel is simply underway from its departure towards its destination. We evaluate the effectiveness of our tool on real-world data, demonstrating that it can practically achieve high accuracy results. We present our results and findings intended for both researchers and practitioners in the field of intelligent ship tracking and surveillance.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3401025.3401763',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Towards safe machine learning for CPS: infer uncertainty from training data',\n",
       "  'authors': \"['Xiaozhe Gu', 'Arvind Easwaran']\",\n",
       "  'date': 'April 2019',\n",
       "  'source': \"ICCPS '19: Proceedings of the 10th ACM/IEEE International Conference on Cyber-Physical Systems\",\n",
       "  'abstract': 'Machine learning (ML) techniques are increasingly applied to decision-making and control problems in Cyber-Physical Systems among which many are safety-critical, e.g., chemical plants, robotics, autonomous vehicles. Despite the significant benefits brought by ML techniques, they also raise additional safety issues because 1) most expressive and powerful ML models are not transparent and behave as a black box and 2) the training data which plays a crucial role in ML safety is usually incomplete. An important technique to achieve safety for ML models is \"Safe Fail\", i.e., a model selects a reject option and applies the backup solution, a traditional controller or a human operator for example, when it has low confidence in a prediction. Data-driven models produced by ML algorithms learn from training data, and hence they are only as good as the examples they have learnt. As pointed in [17], ML models work well in the \"training space\" (i.e., feature space with sufficient training data), but they could not extrapolate beyond the training space. As observed in many previous studies, a feature space that lacks training data generally has a much higher error rate than the one that contains sufficient training samples [31]. Therefore, it is essential to identify the training space and avoid extrapolating beyond the training space. In this paper, we propose an efficient Feature Space Partitioning Tree (FSPT) to address this problem. Using experiments, we also show that, a strong relationship exists between model performance and FSPT score.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3302509.3311038',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Transfer Learning for Classification of Fruit Ripeness Using VGG16',\n",
       "  'authors': \"['Asep Nana Hermana', 'Dewi Rosmala', 'Milda Gustiana Husada']\",\n",
       "  'date': 'January 2021',\n",
       "  'source': \"ICCMB '21: Proceedings of the 2021 4th International Conference on Computers in Management and Business\",\n",
       "  'abstract': 'Early diagnosis of maturity carried out by experts in laboratory tests is often not applicable for fast and inexpensive implementation. Using deep learning, an image of various fruits used as data input. Training deep learning models requires large, hard-to-come datasets to perform the task in order to achieve optimal results. In this study. There are 4 research objects, namely apples, oranges, mangoes, and tomatoes used totaling around 9000 training data. Data were trained using 200 epoch iterations using the transfer learning method with the VGG16 models. At the top layer of both models, the same MLP is applied with several parameters, data is converted from RGB to L * a * b with the aim of being a color descriptor on the fruit. Trained using CNN VGG16 with the transfer learning method. The Dropout 0.5 shows the best performance of experiment with 4 scenario that used different technique and show result the best performance with an average score of accuracy rate from scenario 4 is 92%.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3450588.3450943',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Few-Shot Text and Image Classification via Analogical Transfer Learning',\n",
       "  'authors': \"['Wenhe Liu', 'Xiaojun Chang', 'Yan Yan', 'Yi Yang', 'Alexander G. Hauptmann']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Intelligent Systems and Technology',\n",
       "  'abstract': 'Learning from very few samples is a challenge for machine learning tasks, such as text and image classification. Performance of such task can be enhanced via transfer of helpful knowledge from related domains, which is referred to as transfer learning. In previous transfer learning works, instance transfer learning algorithms mostly focus on selecting the source domain instances similar to the target domain instances for transfer. However, the selected instances usually do not directly contribute to the learning performance in the target domain. Hypothesis transfer learning algorithms focus on the model/parameter level transfer. They treat the source hypotheses as well-trained and transfer their knowledge in terms of parameters to learn the target hypothesis. Such algorithms directly optimize the target hypothesis by the observable performance improvements. However, they fail to consider the problem that instances that contribute to the source hypotheses may be harmful for the target hypothesis, as instance transfer learning analyzed. To relieve the aforementioned problems, we propose a novel transfer learning algorithm, which follows an analogical strategy. Particularly, the proposed algorithm first learns a revised source hypothesis with only instances contributing to the target hypothesis. Then, the proposed algorithm transfers both the revised source hypothesis and the target hypothesis (only trained with a few samples) to learn an analogical hypothesis. We denote our algorithm as Analogical Transfer Learning. Extensive experiments on one synthetic dataset and three real-world benchmark datasets demonstrate the superior performance of the proposed algorithm.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3230709',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Mining Infrequent High-Quality Phrases from Domain-Specific Corpora',\n",
       "  'authors': \"['Li Wang', 'Wei Zhu', 'Sihang Jiang', 'Sheng Zhang', 'Keqiang Wang', 'Yuan Ni', 'Guotong Xie', 'Yanghua Xiao']\",\n",
       "  'date': 'October 2020',\n",
       "  'source': \"CIKM '20: Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management\",\n",
       "  'abstract': 'Phrase mining is a fundamental task for text analysis and has various downstream applications such as named entity recognition, topic modeling, and relation extraction. In this paper, we focus on mining high-quality phrases from domain-specific corpora with special consideration of infrequent ones. Previous methods might miss infrequent high-quality phrases in the candidate selection stage. And these methods rely on explicit features to mine phrases while rarely considering the implicit features. In addition, completeness is rarely explicitly considered in the evaluation of a high-quality phrase. In this paper, we propose a novel approach that exploits a sequence labeling model to capture infrequent phrases. And we employ implicit semantic features and contextual POS tag statistics to measure meaningfulness and completeness, respectively. Experiments over four real-world corpora demonstrate that our method achieves significant improvements over previous state-of-the-art methods across different domains and languages.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3340531.3412029',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Survey on High-Dimensional Medical Data Clustering',\n",
       "  'authors': \"['Velmurugan Arresh Balaji', 'Chulwoong Choi', 'Kyungbaek Kim']\",\n",
       "  'date': 'September 2020',\n",
       "  'source': 'SMA 2020: The 9th International Conference on Smart Media and Applications',\n",
       "  'abstract': \"In a relative less span of time we can process and store a large quantity of data due to technological advancements. There is a rapid change in the nature of data, specifically, the dimensional property of data, mostly in multi and high-dimensional. In terms of heterogeneity of data, Data analysis have becoming a humungous task, Because the volume and complexity in data has been increasing incrementally. In data mining, there is a tool called Data clustering, used in many disciplines in order to extract the meaningful knowledge from seemingly unstructured data. The high-dimensional patient's health records such as immune system status, DICOM Images like CT/PET images, electronic medical records, microarray data like gene expressions, genetic background, etc., In this article we have done a survey on high dimensional medical data clustering and different approaches related to this problem. It also focusses on the real-life applications and recent methods in high dimensional cluster analysis.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3426020.3426071',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Frequent subtree mining on the automata processor: challenges and opportunities',\n",
       "  'authors': \"['Elaheh Sadredini', 'Reza Rahimi', 'Ke Wang', 'Kevin Skadron']\",\n",
       "  'date': 'June 2017',\n",
       "  'source': \"ICS '17: Proceedings of the International Conference on Supercomputing\",\n",
       "  'abstract': 'Frequency counting of complex patterns such as subtrees is more challenging than for simple itemsets and sequences, as the number of possible candidate patterns in a tree is much higher than one-dimensional data structures, with dramatically higher processing times. In this paper, we propose a new and scalable solution for frequent subtree mining (FTM) on the Automata Processor (AP), a new and highly parallel accelerator architecture. We present a multi-stage pruning framework on the AP, called AP-FTM, to reduce the search space of FTM candidates. This achieves up to 353X speedup at the cost of a small reduction in accuracy, on four real-world and synthetic datasets, when compared with PatternMatcher, a practical and exact CPU solution. To provide a fully accurate and still scalable solution, we propose a hybrid method to combine AP-FTM with a CPU exact-matching approach, and achieve up to 262X speedup over PatternMatcher on a challenging database. We also develop a GPU algorithm for FTM, but show that the AP also outperforms this. The results on a synthetic database show the AP advantage grows further with larger datasets.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3079079.3079084',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Fast Outage Analysis of Large-scale Production Clouds with Service Correlation Mining',\n",
       "  'authors': \"['Yaohui Wang', 'Guozheng Li', 'Zijian Wang', 'Yu Kang', 'Yangfan Zhou', 'Hongyu Zhang', 'Feng Gao', 'Jeffrey Sun', 'Li Yang', 'Pochian Lee', 'Zhangwei Xu', 'Pu Zhao', 'Bo Qiao', 'Liqun Li', 'Xu Zhang', 'Qingwei Lin']\",\n",
       "  'date': 'May 2021',\n",
       "  'source': \"ICSE '21: Proceedings of the 43rd International Conference on Software Engineering\",\n",
       "  'abstract': 'Cloud-based services are surging into popularity in recent years. However, outages, i.e., severe incidents that always impact multiple services, can dramatically affect user experience and incur severe economic losses. Locating the root-cause service, i.e., the service that contains the root cause of the outage, is a crucial step to mitigate the impact of the outage. In current industrial practice, this is generally performed in a bootstrap manner and largely depends on human efforts: the service that directly causes the outage is identified first, and the suspected root cause is traced back manually from service to service during diagnosis until the actual root cause is found. Unfortunately, production cloud systems typically contain a large number of interdependent services. Such a manual root cause analysis is often time-consuming and labor-intensive. In this work, we propose COT, the first outage triage approach that considers the global view of service correlations. COT mines the correlations among services from outage diagnosis data. After learning from historical outages, COT can infer the root cause of emerging ones accurately. We implement COT and evaluate it on a real-world dataset containing one year of data collected from Microsoft Azure, one of the representative cloud computing platforms in the world. Our experimental results show that COT can reach a triage accuracy of 82.1%~83.5%, which outperforms the state-of-the-art triage approach by 28.0%~29.7%.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ICSE43902.2021.00085',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A User Profile Analysis Framework Driven by Distributed Machine Learning for Big Data',\n",
       "  'authors': \"['Xiaodong Wang', 'Qing Wang', 'Ye Tao']\",\n",
       "  'date': 'July 2019',\n",
       "  'source': 'AICS 2019: Proceedings of the 2019 International Conference on Artificial Intelligence and Computer Science',\n",
       "  'abstract': 'In recent years, big data has become the new focus of attention from all walks of life. The valuable information contained in big data becomes the driving force for people to process and analyze big data. Big data analytics helps enterprises to take better decisions to improve business output. As a user description tool, user profile is widely used in various fields. However, it is difficult to deal with large-scale datasets using traditional methods since the established processes was not designed to handle large volumes of data. In this paper, we propose a user profile analysis framework using machine learning approach which apply advanced machine learning programs to solve industrial scale problems. And this approach can be effective to speculate real and potential needs of various groups of users and precisely extract individual characteristics and group generality. By introducing high-level data parallel framework, the process of large-scale data processing can be executed efficiently. We use real-world data to validate the effectiveness of the proposed framework.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3349341.3349431',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Classifying Feature Models Maintainability based on Machine Learning Algorithms',\n",
       "  'authors': \"['Publio Silva', 'Carla I. M. Bezerra', 'Rafael Lima', 'Ivan Machado']\",\n",
       "  'date': 'October 2020',\n",
       "  'source': \"SBCARS '20: Proceedings of the 14th Brazilian Symposium on Software Components, Architectures, and Reuse\",\n",
       "  'abstract': 'Maintenance in the context of SPLs is a topic of interest, and that still needs further investigation. There are several ways to evaluate the maintainability of a feature model (FM), one of which is a manual or automated analysis of quality measures. However, the use of measures does not allow to evaluate the FM quality as a whole, as each measure considers a specific characteristic of FM. In general, the measures have wide ranges of values and do not have a clear definition of what is appropriate and inappropriate. In this context, the goal of this work is to investigate the use of machine learning techniques to classify the feature model maintainability. The research questions investigated in the study were: (i) how could machine learning techniques aid to classify FMs maintainability; and, (ii) which FM classification model has the best accuracy and precision. In this work, we proposed an approach for FM maintainability classification using machine learning technics. For that, we used a dataset of 15 FM maintainability measures calculated for 326 FMs, and we used machine learning algorithms to clustering. After this, we used thresholds to evaluate the general maintainability of each cluster. With this, we built 5 maintainability classification models that have been evaluated with the accuracy and precision metrics.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3425269.3425276',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A Comparative Study for Classification on Different Domain',\n",
       "  'authors': \"['Noviyanti Tri Maretta Sagala', 'Jenq-Haur Wang']\",\n",
       "  'date': 'February 2018',\n",
       "  'source': \"ICMLC '18: Proceedings of the 2018 10th International Conference on Machine Learning and Computing\",\n",
       "  'abstract': 'There is no individual classification technique has been shown to deal with all kinds of classification problems. The objective is to select the technique which more possibly reaches the best performance for any domain of data set. We focus on classifying datasets in different domains and properties such as numerical, categorical, and textual. We deal with one versus all strategy to handle multi-class problems. In the experiment, we compared the performance of 4 classification techniques namely Boosted C5.0, KNN, Naïve Bayes, and SVM on 10-fold cross-validation on different number of features. For numerical data set (low and high dimensional data set), the performance of KNN was better than other classification methods. For a categorical and textual data set, Naïve Bayes and SVM were outperformed, respectively.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3195106.3195129',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Clustering Stability via Concept-based Nonnegative Matrix Factorization',\n",
       "  'authors': \"['Nghia Duong-Trung', 'Minh-Hoang Nguyen', 'Hanh T. H. Nguyen']\",\n",
       "  'date': 'January 2019',\n",
       "  'source': \"ICMLSC '19: Proceedings of the 3rd International Conference on Machine Learning and Soft Computing\",\n",
       "  'abstract': 'One of the most important contributions of topic modeling is to accurately and the ectively discover and classify documents in a collection of texts by a number of clusters/topics. However, finding an appropriate number of topics is a particularly challenging model selection question. In this context, we introduce a new unsupervised conceptual stability framework to access the validity of a clustering solution. We integrate the proposed framework into nonnegative matrix factorization (NMF) to guide the selection of desired number of topics. Our model provides a exible way to enhance the interpretation of NMF for the effective clustering solutions. The work presented in this paper crosses the bridge between stability-based validation of clustering solutions and NMF in the context of unsupervised learning. We perform a thorough evaluation of our approach over a wide range of real-world datasets and compare it to current state-of-the-art which are two NMF-based approaches and four Latent Dirichlet Allocation (LDA) based models. the quantitative experimental results show that integrating such conceptual stability analysis into NMF can lead to significant improvements in the document clustering and information retrieval the ectiveness.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3310986.3310991',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Classification of Grain Discoloration via Transfer Learning and Convolutional Neural Networks',\n",
       "  'authors': \"['Nghia Duong-Trung', 'Luyl-Da Quach', 'Minh-Hoang Nguyen', 'Chi-Ngon Nguyen']\",\n",
       "  'date': 'January 2019',\n",
       "  'source': \"ICMLSC '19: Proceedings of the 3rd International Conference on Machine Learning and Soft Computing\",\n",
       "  'abstract': 'Grain discoloration disease of rice is an emerging threat to rice harvest in Vietnam as well as all over the world and it acquires specific attention as it results in qualitative loss of harvested crop. An accurate classification is preliminary to any kind of intervention. Unfortunately, collecting enough grain discoloration data as well as building and training a machine learning model from scratch is next to impossible due to the lack of hardware infrastructure and finance support. It painfully restricts the needs of rapid solutions to deal with the disease. For this purpose, this paper exploits the idea of transfer learning which is the improvement of learning in a new prediction task through the transfer of knowledge from a related prediction task that has already been learned. By utilizing convolutional neural networks trained with our collected data, our experiment shows that the proposed idea performs perfectly and achieves the classification accuracy of 88.2% with the acceptable training time on a normal laptop.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3310986.3310997',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Recurrent Halting Chain for Early Multi-label Classification',\n",
       "  'authors': \"['Thomas Hartvigsen', 'Cansu Sen', 'Xiangnan Kong', 'Elke Rundensteiner']\",\n",
       "  'date': 'August 2020',\n",
       "  'source': \"KDD '20: Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining\",\n",
       "  'abstract': 'Early multi-label classification of time series, the assignment of a label set to a time series before the series is entirely observed, is critical for time-sensitive domains such as healthcare. In such cases, waiting too long to classify can render predictions useless, regardless of their accuracy, while predicting prematurely can result in potentially costly erroneous results. When predicting multiple labels (for example, types of infections), dependencies between labels can be learned and leveraged to improve overall accuracy. Together, reliably predicting the correct label set of a time series while observing as few timesteps as possible is challenging because these goals are contradictory in that fewer timesteps often means worse accuracy. To achieve early yet sufficiently accurate predictions, correlations between labels must be accounted for since direct evidence of some labels may only appear late in the series. We design an effective solution to this open problem, the Recurrent Halting Chain (RHC), that for the first time integrates key innovations in both Early and Multi-label Classification into one multi-objective model. RHC uses a recurrent neural network to jointly model raw time series as well as correlations between labels, resulting in a novel order-free classifier chain that tackles this time-sensitive multi-label learning task. Further, RHC employs a reinforcement learning-based halting network to decide at each timestep which, if any, classes should be predicted, learning to build the label set over time. Using two real-world time-sensitive datasets and popular multi-label metrics, we show that RHC outperforms recent alternatives by predicting more-accurate label sets earlier.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3394486.3403191',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': \"A Novel Classification Scheme of Moving Targets at Sea Based on Ward's and K-means Clustering\",\n",
       "  'authors': \"['Yan Jiang', 'Bo Li', 'Hao Zhang', 'Quming Luo', 'Pengxin Zhou']\",\n",
       "  'date': 'October 2018',\n",
       "  'source': \"CSAE '18: Proceedings of the 2nd International Conference on Computer Science and Application Engineering\",\n",
       "  'abstract': \"Based1 on the structure database technology, Ward's and K-means clustering, a classification and identification scheme is proposed for the monitoring data of the moving targets at sea. First, a structural database is built to store the monitored data. Secondly, by analyzing the movement rules of ships which derived from the automatic identification system(AIS), the identification features of the moving targets at sea are obtained and extracted them from the monitored data. And then, the Ward's clustering is used to classify the feature data. Finally, the K-means clustering is used to identify the existing formation ships. The simulation results show that the proposed scheme is applicable to classify and identify the moving targets at sea.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3207677.3278058',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Learning to Cluster Documents into Workspaces Using Large Scale Activity Logs',\n",
       "  'authors': \"['Weize Kong', 'Michael Bendersky', 'Marc Najork', 'Brandon Vargo', 'Mike Colagrosso']\",\n",
       "  'date': 'August 2020',\n",
       "  'source': \"KDD '20: Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining\",\n",
       "  'abstract': \"Google Drive is widely used for managing personal and work-related documents in the cloud. To help users organize their documents in Google Drive, we develop a new feature to allow users to create a set of working files for ongoing easy access, called workspace. A workspace is a cluster of documents, but unlike a typical document cluster, it contains documents that are not only topically coherent, but are also useful in the ongoing user tasks. To alleviate the burden of creating workspaces manually, we automatically cluster documents into suggested workspaces. We go beyond the textual similarity-based unsupervised clustering paradigm and instead directly learn from users' activity for document clustering. More specifically, we extract co-access signals (i.e., whether a user accessed two documents around the same time) to measure document relatedness. We then use a neural document similarity model that incorporates text, metadata, as well as co-access features. Since human labels are often difficult or expensive to collect, we extract weak labels based on co-access data at large scale for model training. Our offline and online experiments based on Google Drive show that (a) co-access features are very effective for document clustering; (b) our weakly supervised clustering achieves comparable or even better performance compared to the models trained with human labels; and (c) the weakly supervised method leads to better workspace suggestions that the users accept more often in the production system than baseline approaches.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3394486.3403291',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'English Teaching Ability Evaluation Based on Big Data Fuzzy k-means Clustering Algorithm',\n",
       "  'authors': \"['Wei Zhang']\",\n",
       "  'date': 'January 2021',\n",
       "  'source': 'CONF-CDS 2021: The 2nd International Conference on Computing and Data Science',\n",
       "  'abstract': 'In recent years, various data acquisition methods have produced a large number of data acquisition methods from multiple perspectives. Mufti view data clustering is of great significance and challenge in large-scale data analysis. In particular, the importance of clustering analysis in data mining is discussed. Based on the practical application and theoretical research of data mining, the research status of data mining technology at home and abroad is analyzed. The fuzzy membership method is used to describe the probability that the sample belongs to a certain class. Firstly, the constraint parameter analysis model is established. The evaluation model suitable for modern teaching mode such as \"micro course\" and \"flipped classroom\" is constructed, and fuzzy clustering technology is adopted. The number of clusters depends on the distance between the cluster node and the whole cluster.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3448734.3450832',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A state-of-the-art review of machine learning techniques for fraud detection research',\n",
       "  'authors': \"['Sinayobye Janvier Omar', 'Kiwanuka Fred', 'Kaawaase Kyanda Swaib']\",\n",
       "  'date': 'May 2018',\n",
       "  'source': \"SEiA '18: Proceedings of the 2018 International Conference on Software Engineering in Africa\",\n",
       "  'abstract': 'The area of fraud detection1 has been traditionally correlated with data mining and text mining. Even before the \"big data\" phenomena started in 2008, text mining and data mining were used as instruments of fraud detection. However, the limited technological capabilities of the pre-big data technologies made it very difficult for researchers to run fraud detection algorithms on large amounts of data. This paper reviews the existing research done in fraud detection across different areas with the aim of investigating the machine learning techniques used and find out their strengths and weaknesses. It used the systematic quantitative literature review methodology to review the research studies in the field of fraud detection research within the last decade using machine learning techniques. Various combinations of keywords were used to identify the pertinent articles and were retrieved from ACM Digital Library, IEEE Xplorer Digital Library, Science Direct, Springer Link, etc. This search used a sample of 80 relevant articles (peer-reviewed journals articles and conference papers). The most used machine learning techniques were identified, and their strengths and weaknesses. Finally, the conclusion, limitations and future work have been shown.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3195528.3195534',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Citrus Fruits Diseases Detection and Classification Using Transfer Learning',\n",
       "  'authors': \"['Ashok Kumar Saini', 'Roheet Bhatnagar', 'Devesh Kumar Srivastava']\",\n",
       "  'date': 'August 2021',\n",
       "  'source': \"DSMLAI '21': Proceedings of the International Conference on Data Science, Machine Learning and Artificial Intelligence\",\n",
       "  'abstract': 'Diseases in plants have a devastating effect on food protection and may significantly reduce the worth and amount of farming products. In extreme circumstances, plant illnesses may destroy the entire crop. Thus, in agriculture, an automated plant disease detection and diagnosis system are widely needed. Many approaches have been suggested to solve this problem till now, but deep learning has been the preferred solution because of its outstanding results. Electronic apparatus is used to recognize and even monitor crop diseases rather than manual observation. In this paper, a transfer learning-based model is proposed, which used deep convolutional neural networks for identifying and classifying citrus fruits diseases. The InceptionV3, ResNet50, VGG16 and VGG19 models are pre-trained model on a large dataset (ImageNet) used to improve prediction accuracy. Data augmentation is used for getting better accuracy of classification. We collected various performance parameters for comparing accuracy on our dataset; Our results show that VGG19 got the highest accuracy, 99.89%. Our experimental results show that transfer learning provides better prediction with minimal computational resources.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3484824.3484893',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': \"Data Mining and Analysis of College Student's Network Learning Behavior Factors\",\n",
       "  'authors': \"['Xiang Yu', 'Liu Jian min']\",\n",
       "  'date': 'May 2021',\n",
       "  'source': \"ICDEL '21: Proceedings of the 2021 6th International Conference on Distance Education and Learning\",\n",
       "  'abstract': \"This study intends to explore the influencing factors of online learning behavior of college students from the perspective of learners' perception. By modifying and expanding the expectation confirmation model (ECM), and drawing on the relevant scales to form a questionnaire suitable for this study, this paper attempts to build a research model on the influencing factors of college student's online learning willingness and satisfaction. Through the empirical study of college students, with the help of SPSS and Amos and other related software, this paper analyzes the collected effective samples in order to obtain the factor model and research conclusion that have certain explanatory power on the online learning behavior of college students.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3474995.3475031',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Incremental Temporal Pattern Mining Using Efficient Batch-Free Stream Clustering',\n",
       "  'authors': \"['Yifeng Lu', 'Marwan Hassani', 'Thomas Seidl']\",\n",
       "  'date': 'June 2017',\n",
       "  'source': \"SSDBM '17: Proceedings of the 29th International Conference on Scientific and Statistical Database Management\",\n",
       "  'abstract': 'This paper address the problem of temporal pattern mining from multiple data streams containing temporal events. Temporal events are considered as real world events aligned with comprehensive starting and ending timing information rather than simple integer timestamps. Predefined relations, such as \"before\" and \"after\", describe the heterogeneous relationships hidden in temporal data with limited diversity. In this work, the relationships among events are learned dynamically from the temporal information. Each event is treated as an object with a label and numerical attributes. An online-offline model is used as the primary structure for analyzing the evolving multiple streams. Different distance functions on temporal events and sequences can be applied depending on the application scenario. A prefix tree is introduced for a fast incremental pattern update. Events in the real world usually persist for some period. It is more natural to model events as intervals with temporal information rather than as points on the timeline. Based on the representation proposed in this work, our approach can also be extended to handle interval data. Experiments show how the method, with richer information and more accurate results than the state-of-the-art, processes both point-based and interval-based event streams efficiently.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3085504.3085511',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Qualitative Activity Recognition using Machine and Deep Learning: Experimenting with Data-Human Interfaces for non Data-scientists',\n",
       "  'authors': \"['Norman Riedel', 'Alessia Angeli', 'Gustavo Marfia']\",\n",
       "  'date': 'September 2019',\n",
       "  'source': \"GoodTechs '19: Proceedings of the 5th EAI International Conference on Smart Objects and Technologies for Social Good\",\n",
       "  'abstract': 'Data science has become more and more powerful as the development of algorithms and computing power has made huge progresses. Researchers have used machine learning and deep learning algorithms in order to solve hard problems in a variety of domains. Nevertheless, data science will only show its true potential the moment that all categories of interested parties (not only specialist data scientists) will be capable of understanding and interacting with data. This work, hence, is not about data science in a classical way, but about exploring the data-human interface that may be possible to establish utilizing available tools, with little or no background knowledge of how a data science pipeline works. In this paper we show how, within a Sports Science degree course, where no specific knowledge regarding algorithms, statistics or data science is acquired, it was possible to obtain significant results in relation to a data analysis problem, using off-the-shelf application packages. In particular, we here show how, without any specific customization, it has been possible to employ the machine and deep learning algorithms offered by the publicly available platforms, to solve a fitness exercise classification problem, obtaining performances that would have been deemed remarkable until not long ago.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3342428.3342671',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Research on Emotion Classification Based on Clustering Algorithm',\n",
       "  'authors': \"['Hanwei Liu', 'Huiling Cai', 'Qingcheng Lin', 'Xuefeng Li', 'Hui Xiao', 'Lei Wang']\",\n",
       "  'date': 'May 2021',\n",
       "  'source': 'ICAIIS 2021: 2021 2nd International Conference on Artificial Intelligence and Information Systems',\n",
       "  'abstract': 'Emotion recognition, especially facial expression recognition (FER), has played a vital role in understanding human cognition. The current work focuses on the classification, learning and analysis of the six basic emotions (happy, sadness, fear, disgust, anger, and surprise) and other in-depth research fields. However, from the perspective of psychology, human emotions are subjective and complex, and the definition of emotion categories is also controversial, which has an important impact on the accuracy of the analysis results. This paper focuses on the basic issues of emotion classification, presets the position of complex emotions, and uses the improved k-means clustering algorithm to reclassify emotion categories with different emotions based on the subjective voting results of the FER+ face emotion data set. The recognition accuracy is used as objective data to classify the subjective emotion categories, and finally, the recognition accuracy of the emotion classification categories is used as a verification method to prove that the reclassified emotion categories can significantly improve the results of its classification, learning and analysis.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3469213.3470689',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Defect Classification using Pressure Change of Sleeve Soldering Machine',\n",
       "  'authors': \"['Yoshinobu Fukumitsu', 'Keita Nakamichi', 'Hidetake Uwano', 'Hiroshi Fukuoka']\",\n",
       "  'date': 'March 2021',\n",
       "  'source': 'ICRCA 2021: 2021 the 5th International Conference on Robotics, Control and Automation',\n",
       "  'abstract': \"Abstract: The solder joint significantly affects the quality of the electronic equipment. Recent researches focus on the automatic inspection of solder joints to detect the fault with high accuracy. The sleeve soldering system is one of the soldering equipment. The system puts a heated ceramic sleeve over the through-hole of the print circuit board and melts the solder piece dropped into the sleeve. The system also feeds a certain amount of nitrogen gas into the sleeve continuously, and the gas goes out through the lower end of the sleeve. Pressure in the sleeve is changed by narrowing down or blocking the exit hole in each soldering process, such as the sleeve approaches to the print circuit board, drop off the solder piece, and solder melting. Here, the pressure at each process may differ between correct and incorrect soldering. In this paper, the authors classify the correct and incorrect soldering from the pressure change's features. The results of the experiment show that both correct and incorrect are classified with 98.3% accuracy.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3471985.3472371',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A Deep Q-Learning Sanitization Approach for Privacy Preserving Data Mining',\n",
       "  'authors': \"['Usman Ahmed', 'Jerry Chun-Wei Lin', 'Gautam Srivastava', 'Youcef Djenouri']\",\n",
       "  'date': 'January 2021',\n",
       "  'source': \"ICDCN '21: Adjunct Proceedings of the 2021 International Conference on Distributed Computing and Networking\",\n",
       "  'abstract': 'With the establishment of the 5G network, a number of data-intensive applications will be developed. Privacy of information over the network is increasingly relevant, and require protection. The privacy of information while utilizing data is a trade-off that needs to be addressed. In this paper, we propose data privacy of 5G connected devices over heterogeneous networks (5G-Hetnets). A deep Q learning (DQL) based technique is applied to sensitize sensitive information from a given database while keeping the balance between privacy protection and knowledge discovery during the sanitization process. It takes transaction states as input and results in state and action pair. The DQL discovers the transactions dynamically, then the sanitization operation hide the sensitive information by minimizing side effects. The proposed approach shows significant improvement of performance compared to greedy and meta-heuristics and heuristics approaches.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3427477.3429990',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Detection of AI-Manipulated Fake Faces via Mining Generalized Features',\n",
       "  'authors': \"['Yang Yu', 'Rongrong Ni', 'Wenjie Li', 'Yao Zhao']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Multimedia Computing, Communications, and Applications',\n",
       "  'abstract': 'Recently, AI-manipulated face techniques have developed rapidly and constantly, which has raised new security issues in society. Although existing detection methods consider different categories of fake faces, the performance on detecting the fake faces with “unseen” manipulation techniques is still poor due to the distribution bias among cross-manipulation techniques. To solve this problem, we propose a novel framework that focuses on mining intrinsic features and further eliminating the distribution bias to improve the generalization ability. First, we focus on mining the intrinsic clues in the channel difference image (CDI) and spectrum image (SI) view of two different aspects, including the camera imaging process and the indispensable step in AI manipulation process. Then, we introduce the Octave Convolution and an attention-based fusion module to effectively and adaptively mine intrinsic features from CDI and SI view of these two different but intrinsic aspects. Finally, we design an alignment module to eliminate the bias of manipulation techniques to obtain a more generalized detection framework. We evaluate the proposed framework on four categories of fake faces datasets with the most popular and state-of-the-art manipulation techniques and achieve very competitive performances. We further conduct experiments on cross-manipulation techniques, and the results of our method show the superior advantages on improving generalization ability.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3499026',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Imbalanced Breast Cancer Classification Using Transfer Learning',\n",
       "  'authors': \"['Rishav Singh', 'Tanveer Ahmed', 'Abhinav Kumar', 'Amit Kumar Singh', 'Anil Kumar Pandey', 'Sanjay Kumar Singh']\",\n",
       "  'date': 'None',\n",
       "  'source': 'IEEE/ACM Transactions on Computational Biology and Bioinformatics',\n",
       "  'abstract': 'Accurate breast cancer detection using automated algorithms remains a problem within the literature. Although a plethora of work has tried to address this issue, an exact solution is yet to be found. This problem is further exacerbated by the fact that most of the existing datasets are imbalanced, i.e., the number of instances of a particular class far exceeds that of the others. In this paper, we propose a framework based on the notion of transfer learning to address this issue and focus our efforts on histopathological and imbalanced image classification. We use the popular VGG-19 as the base model and complement it with several state-of-the-art techniques to improve the overall performance of the system. With the ImageNet dataset taken as the source domain, we apply the learned knowledge in the target domain consisting of histopathological images. With experimentation performed on a large-scale dataset consisting of 277,524 images, we show that the framework proposed in this paper gives superior performance than those available in the existing literature. Through numerical simulations conducted on a supercomputer, we also present guidelines for work in transfer learning and imbalanced image classification.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/TCBB.2020.2980831',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Rectified Meta-learning from Noisy Labels for Robust Image-based Plant Disease Classification',\n",
       "  'authors': \"['Deming Zhai', 'Ruifeng Shi', 'Junjun Jiang', 'Xianming Liu']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Multimedia Computing, Communications, and Applications',\n",
       "  'abstract': 'Plant diseases serve as one of main threats to food security and crop production. It is thus valuable to exploit recent advances of artificial intelligence to assist plant disease diagnosis. One popular approach is to transform this problem as a leaf image classification task, which can be then addressed by the powerful convolutional neural networks (CNNs). However, the performance of CNN-based classification approach depends on a large amount of high-quality manually labeled training data, which inevitably introduce noise on labels in practice, leading to model overfitting and performance degradation. To overcome this problem, we propose a novel framework that incorporates rectified meta-learning module into common CNN paradigm to train a noise-robust deep network without using extra supervision information. The proposed method enjoys the following merits: (i) A rectified meta-learning is designed to pay more attention to unbiased samples, leading to accelerated convergence and improved classification accuracy. (ii) Our method is free on assumption of label noise distribution, which works well on various kinds of noise. (iii) Our method serves as a plug-and-play module, which can be embedded into any deep models optimized by gradient descent-based method. Extensive experiments are conducted to demonstrate the superior performance of our algorithm over the state-of-the-arts.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3472809',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Do Switches Dream of Machine Learning?: Toward In-Network Classification',\n",
       "  'authors': \"['Zhaoqi Xiong', 'Noa Zilberman']\",\n",
       "  'date': 'November 2019',\n",
       "  'source': \"HotNets '19: Proceedings of the 18th ACM Workshop on Hot Topics in Networks\",\n",
       "  'abstract': 'Machine learning is currently driving a technological and societal revolution. While programmable switches have been proven to be useful for in-network computing, machine learning within programmable switches had little success so far. Not using network devices for machine learning has a high toll, given the known power efficiency and performance benefits of processing within the network. In this paper, we explore the potential use of commodity programmable switches for in-network classification, by mapping trained machine learning models to match-action pipelines. We introduce IIsy, a software and hardware based prototype of our approach, and discuss the suitability of mapping to different targets. Our solution can be generalized to additional machine learning algorithms, using the methods presented in this work.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3365609.3365864',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Effects of Mining Parameters on the Performance of the Sequence Pattern Variants Analyzing Method Applied to Electronic Medical Record Systems',\n",
       "  'authors': \"['Hieu Hanh Le', 'Tatsuhiro Yamada', 'Yuichi Honda', 'Masaaki Kayahara', 'Muneo Kushima', 'Kenji Araki', 'Haruo Yokota']\",\n",
       "  'date': 'December 2019',\n",
       "  'source': 'iiWAS2019: Proceedings of the 21st International Conference on Information Integration and Web-based Applications &amp; Services',\n",
       "  'abstract': \"Sequential pattern mining (SPM) is widely used for data mining and knowledge discovery in various application domains. Recently, we have proposed an analyzing method to evaluate the sequence pattern variant (SPV) that is the original sequence containing frequent patterns including variants. Such a study is meaningful for medical tasks such as improving the quality of a disease's treatment method. This paper aims to evaluate the effectiveness of the proposed analyzing method in more detail when it was applied to Electronic Medical Record Systems. Using a real dataset, it is observed that the analyzing method is successful in statistically discovering the meaningful indicators that are leading to the difference between comparative SPVs, such as complicated risk, severity risk of the disease, the length of stay in the hospital and the total medical cost. Moreover, it is observed that the length of stay and the medical cost can gain more benefit from increasing the significance level parameter used in comparing the SPVs.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3366030.3366074',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Ultrasound Tongue Image Classification using Transfer Learning',\n",
       "  'authors': \"['Yi Feng', 'Xianglin Wang']\",\n",
       "  'date': 'November 2019',\n",
       "  'source': \"DMIP '19: Proceedings of the 2019 2nd International Conference on Digital Medicine and Image Processing\",\n",
       "  'abstract': 'The ultrasound image of the tongue consists of high-level speckle noise, and efficient approach to interpret the image sequences is desired. Automatic ultrasound tongue image classification is of great interest for the clinical linguists, as hand labeling is costly. In this paper, we explore the classification of midsagittal tongue gestures by employing transfer- learning, which can be effective with limited labeled data size. Within the transfer-learning framework, four state- of-the-art convolutional neural network (CNN) architectures are used to make a quantitatively comparison. Classification experiments are conducted using the data from two females. Based on the experimental results, we observed that the learned knowledge from one subject can be transferred to improve the classification accuracy of another subject.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3379299.3379301',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'An application of spectral clustering approach to detect communities in data modeled by graphs',\n",
       "  'authors': \"['Zakariyaa Ait El Mouden', 'Abdeslam Jakimi', 'Moha Hajar']\",\n",
       "  'date': 'March 2019',\n",
       "  'source': \"NISS '19: Proceedings of the 2nd International Conference on Networking, Information Systems &amp; Security\",\n",
       "  'abstract': 'Graph clustering is a popular classification technique with numerous algorithms, with a high number of published proposals, this field keeps expanding. Spectral clustering is one of graph clustering algorithms and one of the most active tools in machine learning community in general and unsupervised classification methods especially, with several applications in different fields this technique has shown its performance and its ability to deal with different data formats. In this paper we present an application of spectral clustering to detect communities in data from real world after modeling those data by graphs. We present also a comparison between the obtained results from the two most known families of spectral clustering using the unnormalized and the normalized algorithms. We finally discuss the obtained results in the output of this application and present our future works.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3320326.3320330',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Evolutionary feature manipulation in data mining/big data',\n",
       "  'authors': \"['Bing Xue', 'Mengjie Zhang']\",\n",
       "  'date': 'March 2017',\n",
       "  'source': 'ACM SIGEVOlution',\n",
       "  'abstract': 'Known as the GIGO (Garbage In, Garbage Out) principle, the quality of the input data highly influences or even determines the quality of the output of any machine learning, big data and data mining algorithm. The input data which is often represented by a set of features may suffer from many issues. Feature manipulation is an effective means to improve the feature set quality, but it is a challenging task. Evolutionary computation (EC) techniques have shown advantages and achieved good performance in feature manipulation. This paper reviews recent advances on EC based feature manipulation methods in classifcation, clustering, regression, incomplete data, and image analysis, to provide the community the state-of-the-art work in the field.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3089251.3089252',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Hybrid Human-Machine Classification System for Cultural Heritage Data',\n",
       "  'authors': \"['Shaban Shabani', 'Maria Sokhn', 'Heiko Schuldt']\",\n",
       "  'date': 'October 2020',\n",
       "  'source': \"SUMAC'20: Proceedings of the 2nd Workshop on Structuring and Understanding of Multimedia heritAge Contents\",\n",
       "  'abstract': \"The advancement of digital technologies has helped cultural heritage organizations to digitize their data collections and improve the accessibility via online platforms. These platforms have enabled citizens to contribute to the process of digital preservation of cultural heritage by sharing documents and their knowledge. However, many historical datasets have problems due to incomplete metadata. To solve this issue, cultural heritage organizations heavily depend on domain experts. In this paper, we address the issue of completing the metadata of historical digital collections. For this, we introduce a new hybrid human-machine model. This model jointly integrates predictions of a deep multi-input model and inferred labels from multiple crowd judgements. The multi-input model uses visual features extracted from the images and textual features from the metadata, complemented with Wikipedia classes of concepts extracted in the text. On the crowd answer aggregation, our method considers the workers' reliability scores. This score is based on the performance of workers' task history and their performance in our task. We have applied our hybrid approach to a culture heritage platform and the evaluations show that it outperforms both deep learning and crowdsourcing when applied individually.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3423323.3423413',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'PETGEN: Personalized Text Generation Attack on Deep Sequence Embedding-based Classification Models',\n",
       "  'authors': \"['Bing He', 'Mustaque Ahamad', 'Srijan Kumar']\",\n",
       "  'date': 'August 2021',\n",
       "  'source': \"KDD '21: Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining\",\n",
       "  'abstract': \"What should a malicious user write next to fool a detection model? Identifying malicious users is critical to ensure the safety and integrity of internet platforms. Several deep learning based detection models have been created. However, malicious users can evade deep detection models by manipulating their behavior, rendering these models of little use. The vulnerability of such deep detection models against adversarial attacks is unknown. Here we create a novel adversarial attack model against deep user sequence embedding-based classification models, which use the sequence of user posts to generate user embeddings and detect malicious users. In the attack, the adversary generates a new post to fool the classifier. We propose a novel end-to-end Personalized Text Generation Attack model, called PETGEN, that simultaneously reduces the efficacy of the detection model and generates posts that have several key desirable properties. Specifically, PETGEN generates posts that are personalized to the user's writing style, have knowledge about a given target context, are aware of the user's historical posts on the target context, and encapsulate the user's recent topical interests. We conduct extensive experiments on two real-world datasets (Yelp and Wikipedia, both with ground-truth of malicious users) to show that PETGEN significantly reduces the performance of popular deep user sequence embedding-based classification models. PETGEN outperforms five attack baselines in terms of text quality and attack efficacy in both white-box and black-box classifier settings. Overall, this work paves the path towards the next generation of adversary-aware sequence classification models.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3447548.3467390',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Construction and Practice of Virtual Simulation Experimental Teaching Platform for Metal and Nonmetal Mines Underground Mining',\n",
       "  'authors': \"['Nan Yao', 'Lihua Ke', 'Jianlong Sheng', 'Wenjing Li', 'Dong Yin']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ICFET '21: Proceedings of the 7th International Conference on Frontiers of Educational Technologies\",\n",
       "  'abstract': 'In order to develop the virtual simulation experiment teaching platform for underground mining of metal and nonmetal mines, the connotation of the course knowledge system around the complete process of underground mining was fully excavated firstly. On this basis, the knowledge system framework of the virtual simulation teaching platform was established. Finally, with the goal of improving the function operation and interactivity of the platform system, the function design of the virtual simulation experiment platform was carried out. The platform has been applied well in the daily teaching of mining engineering major in Wuhan University of Science and Technology. This paper can provide reference for the development of virtual simulation experimental platform for similar majors.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3473141.3473220',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A Study of MOOC Course Review Topics Mining Based on LDA Topic Model',\n",
       "  'authors': \"['Xiao Yang-cai', 'Wang Rui']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"AADNIC-ABMECR '21: Proceedings of the 3rd Africa-Asia Dialogue Network (AADN) International Conference on Advances in Business Management and Electronic Commerce Research\",\n",
       "  'abstract': \"In order to dig deeper into the implied thematic information about online course review data on MOOC learning platforms and obtain the topic concerns of course learners in the process of participating in online courses, as a demand guide to improve the quality level of online classes. This study analyzes the course review data in the form of word cloud map for word frequency, and at the same time, uses LDA topic model for semantic analysis of online course review data to extract learners' topic concerns. The results show that learners focus on course content, lecture style, course discussion, learning resources, architecture, teacher quality, exercise explanation and sound effect in the learning process of MOOC online education platform . By mining online course review data for underlying themes, it is possible to understand learners' demand tendencies, which is meaningful and valuable for improving teaching quality.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503491.3503498',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Protein Family Classification from Scratch: A CNN Based Deep Learning Approach',\n",
       "  'authors': \"['Da Zhang', 'Mansur R. Kabuka']\",\n",
       "  'date': 'None',\n",
       "  'source': 'IEEE/ACM Transactions on Computational Biology and Bioinformatics',\n",
       "  'abstract': 'Next-generation sequencing techniques provide us with an opportunity for generating sequenced proteins and identifying the biological families and functions of these proteins. However, compared with identified proteins, uncharacterized proteins consist of a notable percentage of the overall proteins in the bioinformatics research field. Traditional family classification methods often devote themselves to extracting N-Gram features from sequences while ignoring motif information as well as affinity information between motifs and adjacent amino acids. Previous clustering-based algorithms have typically been used to define protein features with domain knowledge and annotate protein families based on extensive data samples. In this paper, we apply CNN based amino acid representation learning with limited characterized proteins to explore the performances of annotated protein families by taking into account the amino acid location information. Additionally, we apply the method to all reviewed protein sequences with their families retrieved from the UniProt database to evaluate our approach. Last but not least, we verify our model using those unreviewed protein records, which is typically ignored by other methods.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/TCBB.2020.2966633',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Mining Historical Data towards Interference Management in Wireless SDNs',\n",
       "  'authors': \"['Maryam Karimi', 'Prashant Krishnamurthy', 'James Joshi', 'David Tipper']\",\n",
       "  'date': 'November 2017',\n",
       "  'source': \"Q2SWinet '17: Proceedings of the 13th ACM Symposium on QoS and Security for Wireless and Mobile Networks\",\n",
       "  'abstract': 'WiFi networks often seek to reduce interference through network planning, macroscopic self-organization (e.g. channel switching) or network management. In this paper, we explore the use of historical data to automatically predict traffic bottlenecks and make rapid decisions in a wireless (WiFi-like) network on a smaller scale. This is now possible with software defined networks (SDN), whose controllers can have a global view of traffic flows in a network. Models such as classification trees can be used to quickly make decisions on how to manage network resources based on the quality needs, service level agreement or other criteria provided by a network administrator. The objective of this paper is to use data generated by simulation tools to see if such classification models can be developed and to evaluate their efficacy. For this purpose, extensive simulation data was collected and data mining techniques were then used to develop QoS prediction trees. Such trees can predict the maximum delay that results due to specific traffic situations with specific parameters. We evaluated these decision/classification trees by placing them in an SDN controller. OpenFlow cannot directly provide the necessary information for managing wireless networks so we used POX messenger to set up an agent on each AP for adjusting the network. Finally we explored the possibility of updating the tree using feedback that the controller receives from hosts. Our results show that such trees are effective and can be used to manage the network and decrease maximum packet delay.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3132114.3132125',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Augment to Prevent: Short-Text Data Augmentation in Deep Learning for Hate-Speech Classification',\n",
       "  'authors': \"['Georgios Rizos', 'Konstantin Hemker', 'Björn Schuller']\",\n",
       "  'date': 'November 2019',\n",
       "  'source': \"CIKM '19: Proceedings of the 28th ACM International Conference on Information and Knowledge Management\",\n",
       "  'abstract': 'In this paper, we address the issue of augmenting text data in supervised Natural Language Processing problems, exemplified by deep online hate speech classification. A great challenge in this domain is that although the presence of hate speech can be deleterious to the quality of service provided by social platforms, it still comprises only a tiny fraction of the content that can be found online, which can lead to performance deterioration due to majority class overfitting. To this end, we perform a thorough study on the application of deep learning to the hate speech detection problem: a) we propose three text-based data augmentation techniques aimed at reducing the degree of class imbalance and to maximise the amount of information we can extract from our limited resources and b) we apply them on a selection of top-performing deep architectures and hate speech databases in order to showcase their generalisation properties. The data augmentation techniques are based on a) synonym replacement based on word embedding vector closeness, b) warping of the word tokens along the padded sequence or c) class-conditional, recurrent neural language generation. Our proposed framework yields a significant increase in multi-class hate speech detection, outperforming the baseline in the largest online hate speech database by an absolute 5.7% increase in Macro-F1 score and 30% in hate speech class recall.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3357384.3358040',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Functional module extraction from gene expression data using data mining techniques',\n",
       "  'authors': \"['Monica Jha', 'Swarup Roy']\",\n",
       "  'date': 'May 2019',\n",
       "  'source': 'ACM SIGBioinformatics Record',\n",
       "  'abstract': \"A set of correlated and co-expressed genes, often referred as a functional module, play a synergistic role during any disease or any biological activities. Genes participating in a common module may cause clinically similar diseases and shares the common genetic origin of their associated disease phenotypes. Identifying such modules may be helpful in system level understanding of biological and cellular processes or pathophysiologic basis of associated diseases. As a result, detecting such functional modules is an active research issue in the area of computational biology.Many techniques have been proposed so far to find functional modules based on gene co-regulation or co-expression data. These methods are broadly categorized into nonnetwork based gene expression clustering techniques and network-based methods that extract modules from gene co-expression networks using expression data sources. We surved main approaches for obtaining modules, and we evaluated their performance regarding finding biologically significant gene modules in the light of both microarray and RNASeq data. No prior effort, other than independent assessment, has been made so far to evaluate their performances in an integrated way in the light of both microarray and RNASeq data.It could be observed that these methods are basically based on certain features and several other features are ignored. No single technique appears to be effective in all respect. Therefore, there is a possibility that some significant modules might be missed out. Keeping this in view we came up with a solution which would engulf the goodness of all the methods into one. We proposed a multilayer ensemble approach based on few well-known module detection techniques into one. We observed that ensemble of techniques enhances the quality of modules in terms biological significance. We evaluated the effectiveness of the ensemble approach in detecting disease specific modules.Often, a set of genes found to be responsible for dual (or even more) functionalities while participating in multiple overlapping module formation. A more compact form of overlapping module structure is intrinsic structure, where a set of genes within a module playing additional role despite its parent role where it belongs to. We proposed a unique way of detecting such module structures by using the ensemble of modules obtained by the subspace clustering. We used the concept of frequent itemset mining, a step in Association Rule Mining, to derive such compact modules as subspace clusters. To the best of our knowledge, no prior work is attempted so far to detect overlapping and intrinsic modules simultaneously from ensemble outcomes.Finally, we proposed a ranking method to infer disease responsible key genes based on gene expression data. We used top ranked disease significant modules derived by our ensemble method and based on significance of the module with respect to disease pathways. We applied our ranking method in Breast Cancer and Alzheimer's Disease (AD). We inferred top genes and assessed their significance related to the disease with the help of various gene-disease association databases. Experimental results revealed that BRCA1, BRCA2, PTEN, ABI1 and CASP8 are the top key genes in Breast Cancer, whereas, MAPK1, APP, CASP7, APOE and PSEN1 are the key players in AD.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3383672.3383674',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A Study on ELM(Election pledge management for Local governors Model) Based on Machine Learning -Focused on On-Nara Document System-',\n",
       "  'authors': \"['Hong-Jae Lee', 'Kyeong-Seok Han', 'Tae-Hyun Kwon', 'Sang-Ung Han']\",\n",
       "  'date': 'January 2019',\n",
       "  'source': \"ICSIM '19: Proceedings of the 2nd International Conference on Software Engineering and Information Management\",\n",
       "  'abstract': \"The background of this paper is new social trend of more public's interest in the implementation of the pledge of the local governors who were elected by citizens. In these days the election pledge for enhanced local governmental policies became more important. The objective of this paper is to suggest the model of election pledge management for local government heads based on machine learning focused on On-Nara document system. The system is currently used by Korean governmental organizations for document processes. The methods to prove a comparative advantage of the proposed model are the comparison tests between As-Is system and To-Be system based on a few criteria such as time, efficiency and extraction rate. Through this model, local governors could present systematic goals and road map of pledges in order to get closer to citizens and local residents. In other words, this study proposes a model, so called ELM(Election pledge management for Local governors Model), for efficiently extracting necessary data from planned and implemented details of pledge projects that are prepared in the form of unstructured documents. We carried out research to prove empirically our machine learning-based model is more efficient than current semi-manual system with some automated processes in order to manage efficiently the pledge project implementation of local governors to get the results. In conclusion, this research proved that the proposed model is more competitive than the existing models. In the 4th industrial revolution era the new approach using machine learning and big data will become more popular.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3305160.3305208',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Acoustic Classification of Bird Species Using Wavelets and Learning Algorithms',\n",
       "  'authors': \"['Song Yang', 'Ryan Frier', 'Qiang Shi']\",\n",
       "  'date': 'February 2021',\n",
       "  'source': \"ICMLC '21: Proceedings of the 2021 13th International Conference on Machine Learning and Computing\",\n",
       "  'abstract': 'In this project, we derived an effective and efficient mathematical algorithm to identify bird species based on bird calls. Classifying bird species can be useful in real applications, such as determining the health of an ecosystem, or identifying hazardous species of birds near airports and reducing the bird-aircraft strikes. Having well-trained ornithologists to identify the characteristics of birds requires many man hours, and the results may be subjective. Our research was intended to develop a semi-automatic classification algorithm. We first performed a wavelet decomposition algorithm over more than 1200 syllables from 12 different bird species, and then extracted a set of eight parameters from each instance. The dataset formed by the instances and associated parameters was used to train and test different classifiers. Our results showed that among all the classifiers we tested, Cubic Support Vector Machine and Random Forest achieved the highest classification rates, each of which was over 93%.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3457682.3457692',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Stock Price Analysis with Natural Language Processing and Machine Learning',\n",
       "  'authors': \"['Sukanchalika Boonmatham', 'Phayung Meesad']\",\n",
       "  'date': 'July 2020',\n",
       "  'source': \"IAIT '20: Proceedings of the 11th International Conference on Advances in Information Technology\",\n",
       "  'abstract': 'Finding stock price classification based on Thai news corporate is a challenging task. In this research, we try to build machine learning models that capture the relationship of news and stock prices of several companies. In this work, eight companies were selected randomly from Industry Group Index and Sectoral Index. Corporate news articles from the eight selected companies were collected along with their stock prices. Two of traditional machine learning models and two deep learning models were used in this study for comparison purpose. The models were based on Support Vector Machine (SVM), Multilayer Perceptron (MLP), Long Short-Term Memory (LSTM), and Gated Recurrent Unit (GRU). Using news articles as inputs, the models were trained to classify stock prices into two classes: Up and Down of the stock closing price. For classification performance, Accuracy, Precision, Recall and F1 were used. The results showed that GRU had highest average accuracy, precision, recall and F1 higher than other model values with 0.79, 0.79, 0.79, 0.79, respectively.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3406601.3406652',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Medical Image Classification based on an Adaptive Size Deep Learning Model',\n",
       "  'authors': \"['Xiangbin Liu', 'Jiesheng He', 'Liping Song', 'Shuai Liu', 'Gautam Srivastava']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Multimedia Computing, Communications, and Applications',\n",
       "  'abstract': 'With the rapid development of Artificial Intelligence (AI), deep learning has increasingly become a research hotspot in various fields, such as medical image classification. Traditional deep learning models use Bilinear Interpolation when processing classification tasks of multi-size medical image dataset, which will cause the loss of information of the image, and then affect the classification effect. In response to this problem, this work proposes a solution for an adaptive size deep learning model. First, according to the characteristics of the multi-size medical image dataset, the optimal size set module is proposed in combination with the unpooling process. Next, an adaptive deep learning model module is proposed based on the existing deep learning model. Then, the model is fused with the size fine-tuning module used to process multi-size medical images to obtain a solution of the adaptive size deep learning model. Finally, the proposed solution model is applied to the pneumonia CT medical image dataset. Through experiments, it can be seen that the model has strong robustness, and the classification effect is improved by about 4% compared with traditional algorithms.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3465220',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Detecting Atrial Fibrillation from Single-Lead ECG Using Unbalanced Multi-classification Support Vector Machine',\n",
       "  'authors': \"['Xingming Mei', 'Nini Rao', 'Quanchi Li', 'Chengsi Luo', 'Biwott Fleix Kipkurui', 'Hongxiu Jiang']\",\n",
       "  'date': 'June 2019',\n",
       "  'source': \"ICMLT '19: Proceedings of the 2019 4th International Conference on Machine Learning Technologies\",\n",
       "  'abstract': 'Atrial fibrillation (AF) is an common arrhythmia. The incidence of AF has been increasing with the acceleration of urbanization and social aging. Therefore, the wearable ECG acquisition devices with single-lead ECG came out for early diagnosis, monitoring and management of AF. However, it is still great challenge to accurately detect AF from massive ECG data. This study proposed a method detecting AF from single-lead ECG signals based on unbalanced multi-classification support vector machine(SVM). The novel method first screened 73 effective features by correlation analysis from 110 candidate features, which have been confirmed to be associated with AF in literature. Then, an unbalanced four-class SVM classifier was designed to detect four types of ECG signals (including AF, other arrhythmia, artifactual and normal) based on the distribution of different types of ECG data. Finally, the data provided by the PhysioNet/Computing in Cardiology Challenge 2017 confirmed that the proposed method had a overall good performance compared with five other related methods. Also, the data from MIT Arrhythmia Database and the MIT Atrial Fibrillation Database confirmed the robustness of proposed method with AF detection score of > 0.97 and with the scores of > 0.9 in other arrhythmia, artifactual and normal. The proposed method has a good application prospect in AF aided diagnosis, monitoring and management of AF.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3340997.3341004',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Medical Image Classification based on an Adaptive Size Deep Learning Model',\n",
       "  'authors': \"['Xiangbin Liu', 'Jiesheng He', 'Liping Song', 'Shuai Liu', 'Gautam Srivastava']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Multimedia Computing, Communications, and Applications',\n",
       "  'abstract': 'With the rapid development of Artificial Intelligence (AI), deep learning has increasingly become a research hotspot in various fields, such as medical image classification. Traditional deep learning models use Bilinear Interpolation when processing classification tasks of multi-size medical image dataset, which will cause the loss of information of the image, and then affect the classification effect. In response to this problem, this work proposes a solution for an adaptive size deep learning model. First, according to the characteristics of the multi-size medical image dataset, the optimal size set module is proposed in combination with the unpooling process. Next, an adaptive deep learning model module is proposed based on the existing deep learning model. Then, the model is fused with the size fine-tuning module used to process multi-size medical images to obtain a solution of the adaptive size deep learning model. Finally, the proposed solution model is applied to the pneumonia CT medical image dataset. Through experiments, it can be seen that the model has strong robustness, and the classification effect is improved by about 4% compared with traditional algorithms.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3465220',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Three trillion lines: infrastructure for mining GitHub in the classroom',\n",
       "  'authors': \"['Toni Mattis', 'Patrick Rein', 'Robert Hirschfeld']\",\n",
       "  'date': 'March 2020',\n",
       "  'source': \"Programming '20: Companion Proceedings of the 4th International Conference on Art, Science, and Engineering of Programming\",\n",
       "  'abstract': 'The increasing interest in collaborative software development on platforms like GitHub has led to the availability of large amounts of data about development activities. The GHTorrent project has recorded a significant proportion of GitHub’s public event stream and hosts the currently largest public dataset of meta-data about open-source development. We describe our infrastructure that makes this data locally available to researchers and students, examples for research activities carried out on this infrastructure, and what we learned from building the system. We identify a need for domain-specific tools, especially databases, that can deal with large-scale code repositories and associated meta-data and outline open challenges to use them more effectively for research and machine learning settings.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3397537.3397551',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A deep learning fusion model for brain disorder classification: Application to distinguishing schizophrenia and autism spectrum disorder',\n",
       "  'authors': \"['Yuhui Du', 'Bang Li', 'Yuliang Hou', 'Vince D. Calhoun']\",\n",
       "  'date': 'September 2020',\n",
       "  'source': \"BCB '20: Proceedings of the 11th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics\",\n",
       "  'abstract': 'Deep learning has shown a great promise in classifying brain disorders due to its powerful ability in learning optimal features by nonlinear transformation. However, given the high-dimension property of neuroimaging data, how to jointly exploit complementary information from multimodal neuroimaging data in deep learning is difficult. In this paper, we propose a novel multilevel convolutional neural network (CNN) fusion method that can effectively combine different types of neuroimage-derived features. Importantly, we incorporate a sequential feature selection into the CNN model to increase the feature interpretability. To evaluate our method, we classified two symptom-related brain disorders using large-sample multi-site data from 335 schizophrenia (SZ) patients and 380 autism spectrum disorder (ASD) patients within a cross-validation procedure. Brain functional networks, functional network connectivity, and brain structural morphology were employed to provide possible features. As expected, our fusion method outperformed the CNN model using only single type of features, as our method yielded higher classification accuracy (with mean accuracy >85%) and was more reliable across multiple runs in differentiating the two groups. We found that the default mode, cognitive control, and subcortical regions contributed more in their distinction. Taken together, our method provides an effective means to fuse multimodal features for the diagnosis of different psychiatric and neurological disorders.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3388440.3412478',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': \"Prediction of Internet Users' Emotion Tendency in Emergency Events Based on Data Mining\",\n",
       "  'authors': \"['Peng ZHANG', 'Jing LIU', 'Qipeng LI', 'Juan WANG', 'Chenyang Zhao']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': 'AIAM2021: 2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture',\n",
       "  'abstract': 'Through the emotional tendency analysis of subjective texts in emergency events, it can effectively predict the trend of online public opinion, and provide theoretical support for the government and relevant departments to guide online public opinion in the first time. This article uses web crawler technology to crawl and cancel the comment information and personal information of netizens in typical cases of anti-rescue, and uses the emotion analysis method based on emotion dictionary to analyze the emotion of the crawled data. By drawing charts of the proportions of various emotional tendencies of different groups of people, studying the characteristics of emotional tendencies of different groups of people, and predicting the emotional tendencies in emergency events based on the laws drawn from typical cases. And for the predicted groups of people who are prone to negative emotions, the principles and methods for firefighting teams to deal with emergencies are put forward to provide a reference for firefighting teams to deal with online public opinion on emergency events.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3495018.3495094',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A Deep Learning–based Approach for Emotions Classification in Big Corpus of Imbalanced Tweets',\n",
       "  'authors': \"['Nasir Jamal', 'Chen Xianqiao', 'Fadi Al-Turjman', 'Farhan Ullah']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Asian and Low-Resource Language Information Processing',\n",
       "  'abstract': \"Emotions detection in natural languages is very effective in analyzing the user's mood about a concerned product, news, topic, and so on. However, it is really a challenging task to extract important features from a burst of raw social text, as emotions are subjective with limited fuzzy boundaries. These subjective features can be conveyed in various perceptions and terminologies. In this article, we proposed an IoT-based framework for emotions classification of tweets using a hybrid approach of Term Frequency Inverse Document Frequency (TFIDF) and deep learning model. First, the raw tweets are filtered using the tokenization method for capturing useful features without noisy information. Second, the TFIDF statistical technique is applied to estimate the importance of features locally as well as globally. Third, the Adaptive Synthetic (ADASYN) class balancing technique is applied to solve the imbalance class issue among different classes of emotions. Finally, a deep learning model is designed to predict the emotions with dynamic epoch curves. The proposed methodology is analyzed on two different Twitter emotions datasets. The dynamic epoch curves are shown to show the behavior of test and train data points. It is proved that this methodology outperformed the popular state-of-the-art methods.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3410570',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Dynamic Graph Learning Convolutional Networks for Semi-supervised Classification',\n",
       "  'authors': \"['Sichao Fu', 'Weifeng Liu', 'Weili Guan', 'Yicong Zhou', 'Dapeng Tao', 'Changsheng Xu']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Multimedia Computing, Communications, and Applications',\n",
       "  'abstract': 'Over the past few years, graph representation learning (GRL) has received widespread attention on the feature representations of the non-Euclidean data. As a typical model of GRL, graph convolutional networks (GCN) fuse the graph Laplacian-based static sample structural information. GCN thus generalizes convolutional neural networks to acquire the sample representations with the variously high-order structures. However, most of existing GCN-based variants depend on the static data structural relationships. It will result in the extracted data features lacking of representativeness during the convolution process. To solve this problem, dynamic graph learning convolutional networks (DGLCN) on the application of semi-supervised classification are proposed. First, we introduce a definition of dynamic spectral graph convolution operation. It constantly optimizes the high-order structural relationships between data points according to the loss values of the loss function, and then fits the local geometry information of data exactly. After optimizing our proposed definition with the one-order Chebyshev polynomial, we can obtain a single-layer convolution rule of DGLCN. Due to the fusion of the optimized structural information in the learning process, multi-layer DGLCN can extract richer sample features to improve classification performance. Substantial experiments are conducted on citation network datasets to prove the effectiveness of DGLCN. Experiment results demonstrate that the proposed DGLCN obtains a superior classification performance compared to several existing semi-supervised classification models.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3412846',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Classification vs regression in overparameterized regimes: does the loss function matter?',\n",
       "  'authors': \"['Vidya Muthukumar', 'Adhyyan Narang', 'Vignesh Subramanian', 'Mikhail Belkin', 'Daniel Hsu', 'Anant Sahai']\",\n",
       "  'date': 'None',\n",
       "  'source': 'The Journal of Machine Learning Research',\n",
       "  'abstract': 'We compare classification and regression tasks in an overparameterized linear model with Gaussian features. On the one hand, we show that with sufficient overparameterization all training points are support vectors: solutions obtained by least-squares minimum-norm interpolation, typically used for regression, are identical to those produced by the hard-margin support vector machine (SVM) that minimizes the hinge loss, typically used for training classifiers. On the other hand, we show that there exist regimes where these interpolating solutions generalize well when evaluated by the 0-1 test loss function, but do not generalize if evaluated by the square loss function, i.e. they approach the null risk. Our results demonstrate the very different roles and properties of loss functions used at the training phase (optimization) and the testing phase (generalization).',\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3546258.3546480',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Deep Learning Classification for Epilepsy Detection Using a Single Channel Electroencephalography (EEG)',\n",
       "  'authors': \"['Jianguo Liu', 'Blake Woodson']\",\n",
       "  'date': 'July 2019',\n",
       "  'source': \"ICDLT '19: Proceedings of the 2019 3rd International Conference on Deep Learning Technologies\",\n",
       "  'abstract': \"The classification of brain signals using machine learning techniques is a powerful tool in the detection of brain disorders such as Alzheimer's disease and epilepsy. One of the most often used signals is electroencephalography (EEG). Different classification methods have been applied to the analysis of EEG waves with success, such as the support vec- tor machine. We apply deep learning classification to EEG signals for the detection of epilepsy using a convolutional neural network. Besides that this approach advantageously requires no feature extraction, another novelty of this ap- proach is that high classification accuracy can be achieved using only a single channel EEG. Numerical experimentation on a standard test data set shows a 90% or higher accuracy on average.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3342999.3343008',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Heart Disease Prediction Using Data Mining Techniques',\n",
       "  'authors': \"['Ching-seh Mike Wu', 'Mustafa Badshah', 'Vishwa Bhagwat']\",\n",
       "  'date': 'July 2019',\n",
       "  'source': 'DSIT 2019: Proceedings of the 2019 2nd International Conference on Data Science and Information Technology',\n",
       "  'abstract': \"Studies have shown that heart diseases have emerged as the number one cause of deaths. Heart disease is accountable for deaths in all age groups and is common among males and females. A good solution to this problem is to be able to predict what a patient's health status will be like in the future so the doctors can start treatment much sooner which will yield better results. It's a lot better than acting at the last minute where the patient is already at risk and hence the prediction of heart disease is widely researched area. A lot of research and technological advancement has been recorded in similar fields. This paper aims to report about taking advantage of the various data mining techniques and develop prediction models for heart disease survivability.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3352411.3352413',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Non-negative Matrix Factorization for Overlapping Clustering of Customer Inquiry and Review Data',\n",
       "  'authors': \"['Zekun Yang']\",\n",
       "  'date': 'February 2018',\n",
       "  'source': \"ICMLC '18: Proceedings of the 2018 10th International Conference on Machine Learning and Computing\",\n",
       "  'abstract': 'Considering the complexity of clustering text datasets in terms of informal user generated content and the fact that there are multiple labels for each data point in many informal user generated content datasets, this paper focuses on Non-negative Matrix Factorization (NMF) algorithms for Overlapping Clustering of customer inquiry and review data, which has seldom been discussed in previous literature. We extend the use of Semi-NMF and Convex-NMF to Overlapping Clustering and develop a procedure of applying SemiNMF and Convex-NMF on Overlapping Clustering of text data. The developed procedure is tested based on customer review and inquiry datasets. The results of comparing SemiNMF and Convex-NMF with a baseline model demonstrate that they have advantages over the baseline model, since they do not need to adjust parameters to obtain similarly strong clustering performances. Moreover, we compare different methods of picking labels for generating Overlapping Clustering results from Soft Clustering algorithms, and it is concluded that thresholding by mean method is a simpler and relatively more reliable method compared to maximum n method.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3195106.3195110',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Efficient Mining Multi-Mers in a Variety of Biological Sequences',\n",
       "  'authors': \"['Jingsong Zhang', 'Jianmei Guo', 'Ming Zhang', 'Xiangtian Yu', 'Xiaoqing Yu', 'Weifeng Guo', 'Tao Zeng', 'Luonan Chen']\",\n",
       "  'date': 'None',\n",
       "  'source': 'IEEE/ACM Transactions on Computational Biology and Bioinformatics',\n",
       "  'abstract': 'Counting the occurrence frequency of each k-mer in a biological sequence is a preliminary yet important step in many bioinformatics applications. However, most k-mer counting algorithms rely on a given k to produce single-length k-mers, which is inefficient for sequence analysis for different k. Moreover, existing k-mer counters focus more on DNA and RNA sequences and less on protein ones. In practice, the analysis of k-mers in protein sequences can provide substantial biological insights in structure, function, and evolution. To this end, an efficient algorithm, called MulMer (Multiple-Mer mining), is proposed to mine k-mers of various lengths termed multi-mers via inverted-index technique, which is orders of magnitude faster than the conventional forward-index methods. Moreover, to the best of our knowledge, MulMer is the first able to mine multi-mers in a variety of sequences, including DNA, RNA, and protein sequences.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/TCBB.2018.2828313',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Acoustic Classification of Bird Species Using Wavelets and Learning Algorithms',\n",
       "  'authors': \"['Song Yang', 'Ryan Frier', 'Qiang Shi']\",\n",
       "  'date': 'February 2021',\n",
       "  'source': \"ICMLC '21: Proceedings of the 2021 13th International Conference on Machine Learning and Computing\",\n",
       "  'abstract': 'In this project, we derived an effective and efficient mathematical algorithm to identify bird species based on bird calls. Classifying bird species can be useful in real applications, such as determining the health of an ecosystem, or identifying hazardous species of birds near airports and reducing the bird-aircraft strikes. Having well-trained ornithologists to identify the characteristics of birds requires many man hours, and the results may be subjective. Our research was intended to develop a semi-automatic classification algorithm. We first performed a wavelet decomposition algorithm over more than 1200 syllables from 12 different bird species, and then extracted a set of eight parameters from each instance. The dataset formed by the instances and associated parameters was used to train and test different classifiers. Our results showed that among all the classifiers we tested, Cubic Support Vector Machine and Random Forest achieved the highest classification rates, each of which was over 93%.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3457682.3457692',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Multi-label Categorization of French Death Certificates using NLP and Machine Learning',\n",
       "  'authors': \"['Allaouzi Imane', 'Ben Ahmed Mohamed']\",\n",
       "  'date': 'March 2017',\n",
       "  'source': \"BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications\",\n",
       "  'abstract': 'The medical information represents an invaluable source of knowledge concerning the medical history of the patient, but the manner of their presentation make it badly exploited. The idea of this paper is based on the analysis of the death reports written in natural language, which are rich of information, and can be exploited in the calculation of mortality statistics, giving preventive solutions, as well as, help medical professional in their research. This paper proposes our approach to the task of Multi-label Categorization of French death certificates according to ICD-10 (International Classification of Diseases) codes. This approach is based on Machine learning techniques, which is evaluated over CépiDC corpus. The experiment showed that our approach gives interesting results, with an average F1-measue of 79.02%.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3090354.3090384',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Deep Learning-Based Tool for Automatic Feature Marking, Cropping, Visualization, and Classification of Chest Radiographs',\n",
       "  'authors': \"['Geeta Rani', 'Akruti Sinha', 'Mahin Anup', 'Vijaypal Singh Dhaka']\",\n",
       "  'date': 'August 2021',\n",
       "  'source': \"DSMLAI '21': Proceedings of the International Conference on Data Science, Machine Learning and Artificial Intelligence\",\n",
       "  'abstract': \"The prime objective of this research is to develop an automatic tool 'Lung-Infection Visualizer' for marking the Region of Infection and cropping of the marked region in chest radiographs. The tool is also integrated with the feature extractor, feature visualization algorithm, and deep learning-based classifier. Thus, it facilitates the radiology experts where they can easily mark the infected region and visualize the region of infection. In this manuscript, the authors employ the template-based and Brute Force approach of feature mapping. Further, they applied the ResNet, Faster Recurrent Neural Network, XceptionNet, and VGG-16 deep learning-based classifiers for classifying the chest radiographs into bacterial pneumonia, viral pneumonia, COVID-19, and Normal classes. The authors also fine-tune the model parameters and hyperparameters for optimizing the performance of the deep learning-based models. The comparison in the performance proves that the VGG-16 model reports the highest accuracy of 90.07% and outperforms the other models on the dataset of 5,499 chest radiographs used for this research. The cropping tool is registered as Intellectual Property Rights in the name of authors with the registration number SW-14092/2021. And the title 'AutoCrop Tool'.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3484824.3484922',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Filter Selection Methods for Multiclass Classification',\n",
       "  'authors': \"['Rhodessa J. Cascaro', 'Bobby D. Gerardo', 'Ruji P. Medina']\",\n",
       "  'date': 'October 2019',\n",
       "  'source': 'ICCBD 2019: Proceedings of the 2nd International Conference on Computing and Big Data',\n",
       "  'abstract': 'Feature selection is used in choosing relevant features that contribute to the predictive power of a machine learning model. Irrelevant features tend to decrease model accuracy and cause overfitting. Feature selection is the solution to dimensionality problems, especially that data nowadays are unstructured. There are three types of feature selection techniques; filter, wrapper and embedded. Filter methods uses statistical scoring and ranks features by the score. Wrapper methods uses a model to select features and evaluates according to model accuracy. Embedded methods combine the properties of both previous algorithms and selects features in the process of training. This study is focused on the filter types, specifically Chi-square, Information Gain and Relief. On the other hand, multiclass classification is a task that involves classifying instances into three or more classes. This paper aims to compare the performance of the Support Vector Machine (SVM) multiclass classifier when entered with feature subsets generated from three different filter feature selection methods. The dataset is a clothing review text data taken from Kaggle. It contains multiple classes with 23486 review instances. Since filter selection methods utilize ranking, CHI, IG, and Relief were able to select and rank almost the same number of features. When the data was fed into SVM, CHI garnered 66.84% accuracy, IG got 32.90% accuracy while Relief obtained 29.69% accuracy. Experiments on SVM showed that among the generated data subset from the three filter selection methods, the subset using CHI yielded higher accuracy, precision, recall and F1 score compared to others.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3366650.3366655',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Data-Centric Explanations: Explaining Training Data of Machine Learning Systems to Promote Transparency',\n",
       "  'authors': \"['Ariful Islam Anik', 'Andrea Bunt']\",\n",
       "  'date': 'May 2021',\n",
       "  'source': \"CHI '21: Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems\",\n",
       "  'abstract': \"Training datasets fundamentally impact the performance of machine learning (ML) systems. Any biases introduced during training (implicit or explicit) are often reflected in the system's behaviors leading to questions about fairness and loss of trust in the system. Yet, information on training data is rarely communicated to stakeholders. In this work, we explore the concept of data-centric explanations for ML systems that describe the training data to end-users. Through a formative study, we investigate the potential utility of such an approach, including the information about training data that participants find most compelling. In a second study, we investigate reactions to our explanations across four different system scenarios. Our results suggest that data-centric explanations have the potential to impact how users judge the trustworthiness of a system and to assist users in assessing fairness. We discuss the implications of our findings for designing explanations to support users’ perceptions of ML systems.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3411764.3445736',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Studying the Classification Accuracy Performance when Representation is Changed on Several Classifier Techniques',\n",
       "  'authors': \"['Ehab A. Omar Elfallah', 'Wisam H. Benamer']\",\n",
       "  'date': 'March 2017',\n",
       "  'source': 'HP3C-2017: Proceedings of the International Conference on High Performance Compilation, Computing and Communications',\n",
       "  'abstract': 'Introduction: During the process of building a predictive data mining module achieving the highest accuracy is major concern by all researchers. Studying the impact of data representation on the performance of classification accuracy is essential. Recent researches travel among classifiers techniques looking for suitable and higher classification accuracy to build strong modules. Adding extra dimensional by focusing on the reflects that data representation might have on the classification accuracy data mining predictive techniques is the ultimate goal of this research. Methods: In this research seven different data representations were performed on several classifier techniques. These representations were AS_IS representation and three from the binary section and three from normalization section. The binary section included simple binary representation, flag representation and thermometer representation while the normalization section included min max normalization, sigmoidal normalization and standard deviation normalization. These seven representations were applied on eight classifiers Neural Network, Logistic Regression, K nearest Neighbor, Support Vector Machine, Classification Tree, Naive Bayesian, Rule based and Random Forest Decision Tree. Moreover, two datasets have been used for testing the performance of classification accuracy, namely Wisconsin Breast Cancer and German Credit and these two datasets have Boolean target class. Results: The fourteen data representations were raised from two datasets Wisconsin Breast Cancer and German Credit with seven different data representations for each. These data representations were performed on several classifier techniques using Orange software. The results achieved showed variation of the performance among all classifier in classification accuracy. Excluding Naive Bayesian which had over 60 % different from the lowest to the highest accuracy, all other classifier techniques had diverging on classification accuracy around 4.2%.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3069593.3069597',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Comparative analysis of deep transfer learning performance on crop classification',\n",
       "  'authors': \"['Krishna Karthik Gadiraju', 'Ranga Raju Vatsavai']\",\n",
       "  'date': 'November 2020',\n",
       "  'source': \"BigSpatial '20: Proceedings of the 9th ACM SIGSPATIAL International Workshop on Analytics for Big Geospatial Data\",\n",
       "  'abstract': 'Building accurate machine learning models for mapping crops using remote sensing imagery is a challenging task. Traditional solutions include per-pixel based and object-based classification solutions using coarse resolution imagery such as MODIS and LANDSAT. These models also require the development of hand-crafted features. In addition, these methods are computationally inefficient when used with very high resolution (VHR) imagery. Deep learning methods, with their ability to automatically learn relevant features provide an efficient alternative. However, deep learning methods are data-hungry, and require large amounts of labeled data sets to achieve accurate results. This is a challenge to the remote sensing community, since there is a lack of large labeled data sets in the domain. Transfer learning is a strategy that has been widely adapted across several domains and has allowed for the usage of deep neural networks even with limited labeled data. In this paper, we perform a comparative analysis of various transfer learning strategies for the domain of crop classification. We perform our experiments using three well-known neural networks and evaluate various strategies such as: (i) training the model from scratch using random weight initialization, (ii) simply using the pretrained models as feature extractors, (iii) finetuning deep CNNs by freezing the early layers of a pretrained CNN, and (iv) using the Imagenet pretrained weights as initialization, and their impact on classification performance.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3423336.3431369',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Graph Classification using Structural Attention',\n",
       "  'authors': \"['John Boaz Lee', 'Ryan Rossi', 'Xiangnan Kong']\",\n",
       "  'date': 'July 2018',\n",
       "  'source': \"KDD '18: Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining\",\n",
       "  'abstract': 'Graph classification is a problem with practical applications in many different domains. To solve this problem, one usually calculates certain graph statistics (i.e., graph features) that help discriminate between graphs of different classes. When calculating such features, most existing approaches process the entire graph. In a graphlet-based approach, for instance, the entire graph is processed to get the total count of different graphlets or subgraphs. In many real-world applications, however, graphs can be noisy with discriminative patterns confined to certain regions in the graph only. In this work, we study the problem of attention-based graph classification. The use of attention allows us to focus on small but informative parts of the graph, avoiding noise in the rest of the graph. We present a novel RNN model, called the Graph Attention Model (GAM), that processes only a portion of the graph by adaptively selecting a sequence of \"informative\" nodes. Experimental results on multiple real-world datasets show that the proposed method is competitive against various well-known methods in graph classification even though our method is limited to only a portion of the graph.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3219819.3219980',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Continual Learning for Sentiment Classification by Iterative Networks Combination',\n",
       "  'authors': \"['Shupeng Wang', 'Junhao Liu']\",\n",
       "  'date': 'December 2021',\n",
       "  'source': \"CSAI '21: Proceedings of the 2021 5th International Conference on Computer Science and Artificial Intelligence\",\n",
       "  'abstract': 'Deep neural networks have been reported to exhibit excellent performance on sentiment classification, where the impressive results are obtained with static model incapable of adapting their behavior to new domains over time. However, in practice, static models may become intractable quickly due to storage constraints or privacy issues. In this paper, we propose a novel continual learning approach for sentiment classification by iteratively combining (CSIC) the original network trained on old tasks and the fine-tuned network trained on new tasks with knowledge distillation, which adapts continually and keeps on learning over time without increasing the size of the network. We conduct extensive experiments on 16 popular review corpora. Experimental results demonstrate that the proposed CSIC method significantly outperforms the strong baselines for sentiment classification in continual learning scenario.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3507548.3507571',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'The Epidemiological Characteristics of Bronchopneumonias in Children Based on WEB Data Mining',\n",
       "  'authors': \"['Yonghong Ma', 'Jiao Tan', 'Dongning Zhang', 'Ke Men', 'Mingjuan Shi', 'Ying Cao']\",\n",
       "  'date': 'August 2021',\n",
       "  'source': \"ICIMTECH 21: <italic toggle='yes'>Retracted on September 15, 2021</italic>The Sixth International Conference on Information Management and Technology\",\n",
       "  'abstract': 'NOTICE OF RETRACTION: While investigating potential publication-related misconduct in connection with the ICIMTech 2021 Conference Proceedings, serious concerns were raised that cast doubt on the integrity of the peer-review process and all papers published in the Proceedings of this Conference. The integrity of the entire Conference has been called into question. As a result, of its investigation, ACM has decided to retract the Entire Conference Proceedings and all related papers from the ACM Digital Library.None of the papers from this Proceeding should be cited in the literature because of the questionable integrity of the peer review process for this Conference.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3465631.3465931',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Generating Better Search Engine Text Advertisements with Deep Reinforcement Learning',\n",
       "  'authors': \"['J. Weston Hughes', 'Keng-hao Chang', 'Ruofei Zhang']\",\n",
       "  'date': 'July 2019',\n",
       "  'source': \"KDD '19: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining\",\n",
       "  'abstract': 'Deep Reinforcement Learning has been applied in a number of fields to directly optimize non-differentiable reward functions, including in sequence to sequence settings using Self Critical Sequence Training (SCST). Previously, SCST has primarily been applied to bring conditional language models closer to the distribution of their training set, as in traditional neural machine translation and abstractive summarization. We frame the generation of search engine text ads as a sequence to sequence problem, and consider two related goals: to generate ads similar to those a human would write, and to generate ads with high click-through rates. We jointly train a model to minimize cross-entropy on an existing corpus of Landing Page/Text Ad pairs using typical sequence to sequence training techniques while also optimizing the expected click-through rate (CTR) as predicted by an existing oracle model using SCST. Through joint training we achieve a 6.7% increase in expected CTR without a meaningful drop in ROUGE score. Human experiments demonstrate that SCST training produces significantly more attractive ads without reducing grammatical quality.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3292500.3330754',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Legal Summarization for Multi-role Debate Dialogue via Controversy Focus Mining and Multi-task Learning',\n",
       "  'authors': \"['Xinyu Duan', 'Yating Zhang', 'Lin Yuan', 'Xin Zhou', 'Xiaozhong Liu', 'Tianyi Wang', 'Ruocheng Wang', 'Qiong Zhang', 'Changlong Sun', 'Fei Wu']\",\n",
       "  'date': 'November 2019',\n",
       "  'source': \"CIKM '19: Proceedings of the 28th ACM International Conference on Information and Knowledge Management\",\n",
       "  'abstract': 'Multi-role court debate is a critical component in a civil trial where parties from different camps (plaintiff, defendant, witness, judge, etc.) actively involved. Unlike other types of dialogue, court debate can be lengthy, and important information, with respect to the controversy focus(es), often hides within the redundant and colloquial dialogue data. Summarizing court debate can be a novel but significant task to assist judge to effectively make the legal decision for the target trial. In this work, we propose an innovative end-to-end model to address this problem. Unlike prior summarization efforts, the proposed model projects the multi-role debate into the controversy focus space, which enables high-quality essential utterance(s) extraction in terms of legal knowledge and judicial factors. An extensive set of experiments with a large civil trial dataset shows that the proposed model can provide more accurate and readable summarization against several alternatives in the multi-role court debate scene.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3357384.3357940',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Beyond Statistical Relations: Integrating Knowledge Relations into Style Correlations for Multi-Label Music Style Classification',\n",
       "  'authors': \"['Qianwen Ma', 'Chunyuan Yuan', 'Wei Zhou', 'Jizhong Han', 'Songlin Hu']\",\n",
       "  'date': 'January 2020',\n",
       "  'source': \"WSDM '20: Proceedings of the 13th International Conference on Web Search and Data Mining\",\n",
       "  'abstract': 'Automatically labeling multiple styles for every song is a comprehensive application in all kinds of music websites. Recently, some researches explore review-driven multi-label music style classification and exploit style correlations for this task. However, their methods focus on mining the statistical relations between different music styles and only consider shallow style relations. Moreover, these statistical relations suffer from the underfitting problem because some music styles have little training data. To tackle these problems, we propose a novel knowledge relations integrated framework (KRF) to capture the complete style correlations, which jointly exploits the inherent relations between music styles according to external knowledge and their statistical relations. Based on the two types of relations, we use graph convolutional network to learn the deep correlations between styles automatically. Experimental results show that our framework significantly outperforms the state-of-the-art methods. Further studies demonstrate that our framework can effectively alleviate the underfitting problem and learn meaningful style correlations.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3336191.3371838',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Mining Exploratory Behavior to Improve Mobile App Recommendations',\n",
       "  'authors': \"['Jiangning He', 'Hongyan Liu']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Information Systems',\n",
       "  'abstract': 'With the widespread usage of smart phones, more and more mobile apps are developed every day, playing an increasingly important role in changing our lifestyles and business models. In this trend, it becomes a hot research topic for developing effective mobile app recommender systems in both industry and academia. Compared with existing studies about mobile app recommendations, our research aims to improve the recommendation effectiveness based on analyzing a psychological trait of human beings, exploratory behavior, which refers to a type of variety-seeking behavior in unfamiliar domains. To this end, we propose a novel probabilistic model named Goal-oriented Exploratory Model (GEM), integrating exploratory behavior identification with personalized item recommendation. An algorithm combining collapsed Gibbs sampling and Expectation Maximization is developed for model learning and inference. Through extensive experiments conducted on a real dataset, the proposed model demonstrates superior recommendation performances and good interpretability compared with state-of-art recommendation methods. Moreover, empirical analyses on exploratory behavior find that individuals with a strong exploratory tendency exhibit behavioral patterns of variety seeking, risk taking, and higher involvement. Besides, mobile apps that are less popular or in the long tail possess greater potential of arousing exploratory behavior in individuals.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3072588',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Resolving Ambiguity in Sentiment Classification: The Role of Dependency Features',\n",
       "  'authors': \"['Shuyuan Deng', 'Atish P. Sinha', 'Huimin Zhao']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Management Information Systems',\n",
       "  'abstract': 'Sentiment analysis has become popular in business intelligence and analytics applications due to the great need for learning insights from the vast amounts of user generated content on the Internet. One major challenge of sentiment analysis, like most text classification tasks, is finding structures from unstructured texts. Existing sentiment analysis techniques employ the supervised learning approach and the lexicon scoring approach, both of which largely rely on the representation of a document as a collection of words and phrases. The semantic ambiguity (i.e., polysemy) of single words and the sparsity of phrases negatively affect the robustness of sentiment analysis, especially in the context of short social media texts. In this study, we propose to represent texts using dependency features. We test the effectiveness of dependency features in supervised sentiment classification. We compare our method with the current standard practice using a labeled data set containing 170,874 microblogging messages. The combination of unigram features and dependency features significantly outperformed other popular types of features.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3046684',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Deep Learning Approach for Singer Voice Classification of Vietnamese Popular Music',\n",
       "  'authors': \"['Toan Pham Van', 'Ngoc Tran Ngo Quang', 'Ta Minh Thanh']\",\n",
       "  'date': 'December 2019',\n",
       "  'source': \"SoICT '19: Proceedings of the 10th International Symposium on Information and Communication Technology\",\n",
       "  'abstract': \"Singer voice classification is a meaningful task in the digital era. With a huge number of songs today, identifying a singer is very helpful for music information retrieval, music properties indexing, and so on. In this paper, we propose a new method to identify the singer's name based on analysis of Vietnamese popular music. We employ the use of vocal segment detection and singing voice separation as the preprocessing steps. The purpose of these steps is to extract the singer's voice from the mixture sound. In order to build a singer classifier, we propose a neural network architecture working with Mel Frequency Cepstral Coefficient (MFCC) as extracted input features from said vocal. To verify the accuracy of our methods, we evaluate on a dataset of 300 Vietnamese songs from 18 famous singers. We achieve an accuracy of 92.84% with 5-fold stratified cross-validation, the best result compared to other methods on the same data set.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3368926.3369700',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Residual Deep Learning System for Mass Segmentation and Classification in Mammography',\n",
       "  'authors': \"['Dina Abdelhafiz', 'Sheida Nabavi', 'Reda Ammar', 'Clifford Yang', 'Jinbo Bi']\",\n",
       "  'date': 'September 2019',\n",
       "  'source': \"BCB '19: Proceedings of the 10th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics\",\n",
       "  'abstract': 'Automatic extraction of breast mass in mammogram (MG) images is a challenging task due to the varying sizes, shapes, and textures of masses. Moreover, the density of MGs makes mass detection very challenging since masses can be hidden in dense MGs. In this paper, we propose a residual deep learning (DL) system for mass segmentation and classification in mammography. The overall proposed system consists of two cascaded parts: 1) a residual attention U-Net model (RU-Net) to precisely segment mass lesions in MG images, followed by 2) a ResNet classifier to classify the detected binary segmented lesions into benign or malignant. The proposed semantic based CNN model, RU-Net, has the basic architecture of the U-Net model, which extracts contextual information combining low-level feature with high-level ones. We have modified the U-Net structure by adding residual attention modules in order to preserve the spatial and context information, help the network have deeper architecture, and handles the gradient vanishing problem. We compared the performance of the proposed RU-Net model with those of state-of-the-art two semantic segmentation models, and two object detectors using public databases. We also examined the effect of the breast density on the accuracy of localizing and segmenting the breast masses. Our proposed model shows superior performance compared to the other DL methods in detecting and segmenting masses, especially for heterogeneously dense and dense MG images, in terms of intersection over union (IOU) and the Dice index coefficient (DI). Moreover, our results show that the cascaded ResNet model, trained using binary-scale images, classify the masses to benign or malignant with higher accuracy compared to the ResNet model that is trained on gray-scale images.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3307339.3342157',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Mining E-Commerce Query Relations using Customer Interaction Networks',\n",
       "  'authors': \"['Bijaya Adhikari', 'Parikshit Sondhi', 'Wenke Zhang', 'Mohit Sharma', 'B. Aditya Prakash']\",\n",
       "  'date': 'April 2018',\n",
       "  'source': \"WWW '18: Proceedings of the 2018 World Wide Web Conference\",\n",
       "  'abstract': 'Customer Interaction Networks (CINs) are a natural framework for representing and mining customer interactions with E-Commerce search engines. Customer interactions begin with the submission of a query formulated based on an initial product intent, followed by a sequence of product engagement and query reformulation actions. Engagement with a product (e.g. clicks) indicates its relevance to the customer»s product intent. Reformulation to a new query indicates either dissatisfaction with current results, or an evolution in the customer»s product intent. Analyzing such interactions within and across sessions, enables us to discover various query-query and query-product relationships. In this work, we begin by studying the properties of CINs developed using Walmart.com»s product search logs. We observe that the properties exhibited by CINs make it possible to mine intent relationships between queries based purely on their structural information. We show how these relations can be exploited for a) clustering queries based on intents, b) significantly improve search quality for poorly performing queries, and c) identify the most influential (aka. »critical») queries whose performance have the highest impact on performance of other queries.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3178876.3186174',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'CCTV News Broadcast Information Mining: Keyword Extraction Based on Semantic Model and Statistics Visualization',\n",
       "  'authors': \"['Yujie Xie', 'Fenghai Liu']\",\n",
       "  'date': 'October 2020',\n",
       "  'source': 'AIAM2020: Proceedings of the 2nd International Conference on Artificial Intelligence and Advanced Manufacture',\n",
       "  'abstract': 'CCTV News Broadcast is one of the most popular news programs in China, and it is also the most important propaganda platform in China. CCTV News Broadcast is established to \"Improve the quality of publicity\", so it is \"A product of visual culture of national ideology\" and \"Taking politics as the standard\" is primary appeal. [1] At present, there is little research on the text of CCTV News Broadcast. This paper focuses on the CCTV News Broadcast, using the visualization model based statistics and semantic based keyword extraction model (SKE) to extract the text features of CCTV News Broadcast. It can help the public quickly capture the key information of CCTV News Broadcast. Moreover, this paper also forms a set of Chinese corpus with keywords tagging in the field of CCTV News Broadcast. It provides important data support for machine learning method and subsequent research. In addition, aiming at some important problems found in this paper, this paper proposes further research direction for text data processing in CCTV News Broadcast field.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3421766.3421827',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Few-Shot Learning in Object Classification using Meta-Learning with Between-Class Attribute Transfer',\n",
       "  'authors': \"['Majed Alsadhan', 'William H. Hsu']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"ICMLC '22: Proceedings of the 2022 14th International Conference on Machine Learning and Computing\",\n",
       "  'abstract': 'We present a novel framework for the problem of transfer learning between few-shot source and target domains, using synthetic attributes in addition to convolutional neural networks that are pre-trained on larger image corpora. In these corpora, no labeled instances of the target domains are present, though they may contain instances of their superclasses. Using probabilistic inference over predicted classes and inferred attributes, we developed a meta-learning ensemble method that builds upon that of [10]. This paper introduces the new framework BCAT (Between-Class Attribute Transfer), adapting inter-class attribute transfer designed for zero-shot learning (ZSL), combined with fusing transfer learning and probabilistic priors, and thereby extending and improving upon existing deep meta-learning models for FSL. We show how probabilistic learning architectures can be adapted to use state-of-the-field deep learning components in this framework. We applied our technique to four baseline convnet-based FSL ensembles and boosted accuracy by up to 6.24% for 1-shot learning and up to 4.11% for 5-shot learning on the mini-ImageNet dataset, the best result of which is competitive with the current state of the field; using the same technique, we improved accuracy by up to 7.83% for 1-shot learning and up to 3.67% for 5-shot learning on the tiered-ImageNet dataset.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3529836.3529914',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Adaptive Low-Rank Multi-Label Active Learning for Image Classification',\n",
       "  'authors': \"['Jian Wu', 'Anqian Guo', 'Victor S. Sheng', 'Pengpeng Zhao', 'Zhiming Cui', 'Hua Li']\",\n",
       "  'date': 'October 2017',\n",
       "  'source': \"MM '17: Proceedings of the 25th ACM international conference on Multimedia\",\n",
       "  'abstract': 'Multi-label active learning for image classification has attracted great attention over recent years and a lot of relevant works are published continuously. However, there still remain some problems that need to be solved, such as existing multi-label active learning algorithms do not reflect on the cleanness of sample data and their ways on label correlation mining are defective. For one thing, sample data is usually contaminated in reality, which disturbs the estimation of data distribution and further hinders the model training. For another, previous approaches for label relationship exploration are purely based on the observed label distribution of an incomplete training set, which cannot provide sufficiently efficient information. To address these issues, we propose a novel adaptive low-rank multi-label active learning algorithm, called LRMAL. Specifically, we first use low-rank matrix recovery to learn an effective low-rank feature representation from the noisy data. In a subsequent sampling phase, we make use of its superiorities to evaluate the general informativeness of each unlabeled example-label pair. Based on an intrinsic mapping relation between the example space and the label space of a certain multi-label dataset, we recover the incomplete labels of a training set for a more comprehensive label correlation mining. Furthermore, to reduce the redundancy among the selected example-label pairs, we use a diversity measurement to diversify the sampled data. Finally, an effective sampling strategy is developed by integrating these two aspects of potential information with uncertainty based on an adaptive integration scheme. Experimental results demonstrate the effectiveness of our approach.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3123266.3123388',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Effective Clustering for Single Cell Sequencing Cancer Data',\n",
       "  'authors': \"['Simone Ciccolella', 'Murray D. Patterson', 'Paola Bonizzoni', 'Gianluca Della Vedova']\",\n",
       "  'date': 'September 2019',\n",
       "  'source': \"BCB '19: Proceedings of the 10th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics\",\n",
       "  'abstract': 'Background. Single cell sequencing (SCS) technologies provide a level of resolution that makes it indispensable for inferring from a sequenced tumor, evolutionary trees or phylogenies representing an accumulation of cancerous mutations. A drawback of SCS is elevated false negative and missing value rates, resulting in a large space of possible solutions, which in turn makes infeasible using some approaches and tools. While this has not inhibited the development of methods for inferring phylogenies from SCS data, the continuing increase in size and resolution of these data begin to put a strain on such methods. One possible solution is to reduce the size of an SCS instance --- usually represented as a matrix of presence, absence and missing values of the mutations found in the different sequenced cells --- and to infer the tree from this reduced-size instance. Previous approaches have used k-means to this end, clustering groups of mutations and/or cells, and using these means as the reduced instance. Such an approach typically uses the Euclidean distance for computing means. However, since the values in these matrices are of a categorical nature (having the three categories: present, absent and missing), we explore techniques for clustering categorical data --- commonly used in data mining and machine learning --- to SCS data, with this goal in mind. Results. In this work, we present a new clustering procedure aimed at clustering categorical vector, or matrix data --- here representing SCS instances, called celluloid. We demonstrate that celluloid clusters mutations with high precision: never pairing too many mutations that are unrelated in the ground truth, but also obtains accurate results in terms of the phylogeny inferred downstream from the reduced instance produced by this method. Finally, we demonstrate the usefulness of a clustering step by applying the entire pipeline (clustering + inference method) to a real dataset, showing a significant reduction in the runtime, raising considerably the upper bound on the size of SCS instances which can be solved in practice. Availability. Our approach, celluloid: clustering single cell sequencing data around centroids is available at https://github.com/AlgoLab/celluloid/ under an MIT license.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3307339.3342149',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'An Ensemble Deep Active Learning Method for Intent Classification',\n",
       "  'authors': \"['Leihan Zhang', 'Le Zhang']\",\n",
       "  'date': 'December 2019',\n",
       "  'source': \"CSAI '19: Proceedings of the 2019 3rd International Conference on Computer Science and Artificial Intelligence\",\n",
       "  'abstract': \"Intent classification plays a primary and critical role in intelligent dialogue systems. However, faced with the lack of labeled data, the training of robust intent classification model is time-consuming and costly. Thanks to the powerful pre-trained model and active learning, it's possible to construct an integrated method to fulfill this task efficiently. Therefore, we propose an ensemble deep active learning method, which constructs intent classifier based on BERT and uses an ensemble sampling method to choose informative data for efficient training. Experimental results on both Chinese and English intent classification datasets suggest that the proposed ensemble deep active learning method can achieve state-of-the-art performance with less than half of the training data. In addition, the performance of the proposed method is stable and scalable for both datasets. In general, the proposed method shows substantial advantages in building intent classifier across different datasets.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3374587.3374611',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Multiclass Sentiment Classification of Online Health Forums using Both Domain-independent and Domain-specific Features',\n",
       "  'authors': '[\\'Rana Alnashwan\\', \\'Humphrey Sorensen\\', \"Adrian O\\'Riordan\", \\'Cathal Hoare\\']',\n",
       "  'date': 'December 2017',\n",
       "  'source': \"BDCAT '17: Proceedings of the Fourth IEEE/ACM International Conference on Big Data Computing, Applications and Technologies\",\n",
       "  'abstract': 'Online health-related discussion provides a rich source of information for both informing the public and providing feedback to health professionals to detect trends and inform policy. However, there are few studies that focus on analysing sentiment in medical forum discourse. Online health communities devoted to specific medical conditions and health-related problems support people with similar conditions, enabling them to exchange personal experiences. Analysing sentiment expressed by members of a health community in medical forum discourse can be valuable for identifying a particular aspect of the information space. In this paper, we identify sentiments expressed on online medical forums discussing Lyme disease. There are two goals in our research. First, to identify a set of categories that can represent a comprehensive connotation of emotions expressed in the discussions, while also being adequately distinct for the purposes of machine learning. Second, to identify the sentiments expressed by participants in individual posts. Three types of feature (content-free, content-specific and meta-level) are extracted and inductive learning algorithms utilized to build a feature-based classification model for an automated multi-class classification model. The experimental results demonstrate the effectiveness of our approach.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3148055.3148058',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Coupling risk attitude and motion data mining in a preemtive construction safety framework',\n",
       "  'authors': \"['Khandakar M. Rashid', 'Songjukta Datta', 'Amir H. Behzadan']\",\n",
       "  'date': 'December 2017',\n",
       "  'source': \"WSC '17: Proceedings of the 2017 Winter Simulation Conference\",\n",
       "  'abstract': \"Construction sites comprise constantly moving heterogeneous resources that operate in close proximity of each other. The sporadic nature of field tasks creates an accident prone physical space surrounding workers. Despite efforts to improve site safety using location-aware proximity sensing techniques, major scientific gaps still remain in reliably forecasting impending hazardous scenarios before they occur. In the research presented in this paper, spatiotemporal data of workers and site hazards is fused with a quantifiable model of an individual's attitude toward risk to generate proximity-based safety alerts in real time. In particular, a worker's risk index is formulated and coupled with robust hidden Markov model (HMM)-based trajectory prediction to approximate his/her future position, and detect imminent contact collisions. The designed methodology is explained and assessed using several experiments emulating interactions between site workers and hazards. Preliminary results demonstrate the effectiveness of the designed methods in robustly predicting potential collision events.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3242181.3242386',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'DECAF: Deep Extreme Classification with Label Features',\n",
       "  'authors': \"['Anshul Mittal', 'Kunal Dahiya', 'Sheshansh Agrawal', 'Deepak Saini', 'Sumeet Agarwal', 'Purushottam Kar', 'Manik Varma']\",\n",
       "  'date': 'March 2021',\n",
       "  'source': \"WSDM '21: Proceedings of the 14th ACM International Conference on Web Search and Data Mining\",\n",
       "  'abstract': 'Extreme multi-label classification (XML) involves tagging a data point with its most relevant subset of labels from an extremely large label set, with several applications such as product-to-product recommendation with millions of products. Although leading XML algorithms scale to millions of labels, they largely ignore label metadata such as textual descriptions of the labels. On the other hand, classical techniques that can utilize label metadata via representation learning using deep networks struggle in extreme settings. This paper develops the DECAF algorithm that addresses these challenges by learning models enriched by label metadata that jointly learn model parameters and feature representations using deep networks and offer accurate classification at the scale of millions of labels. DECAF makes specific contributions to model architecture design, initialization, and training, enabling it to offer up to 2-6% more accurate prediction than leading extreme classifiers on publicly available benchmark product-to-product recommendation datasets, such as LF-AmazonTitles-1.3M. At the same time, DECAF was found to be up to 22x faster at inference than leading deep extreme classifiers, which makes it suitable for real-time applications that require predictions within a few milliseconds. The code for DECAF is available at the following URL: https://github.com/Extreme-classification/DECAF',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3437963.3441807',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Historical Context-based Style Classification of Painting Images via Label Distribution Learning',\n",
       "  'authors': \"['Jufeng Yang', 'Liyi Chen', 'Le Zhang', 'Xiaoxiao Sun', 'Dongyu She', 'Shao-Ping Lu', 'Ming-Ming Cheng']\",\n",
       "  'date': 'October 2018',\n",
       "  'source': \"MM '18: Proceedings of the 26th ACM international conference on Multimedia\",\n",
       "  'abstract': 'Analyzing and categorizing the style of visual art images, especially paintings, is gaining popularity owing to its importance in understanding and appreciating the art. The evolution of painting style is both continuous, in a sense that new styles may inherit, develop or even mutate from their predecessors and multi-modal because of various issues such as the visual appearance, the birthplace, the origin time and the art movement. Motivated by this peculiarity, we introduce a novel knowledge distilling strategy to assist visual feature learning in the convolutional neural network for painting style classification. More specifically, a multi-factor distribution is employed as soft-labels to distill complementary information with visual input, which extracts from different historical context via label distribution learning. The proposed method is well-encapsulated in a multi-task learning framework which allows end-to-end training. We demonstrate the superiority of the proposed method over the state-of-the-art approaches on Painting91, OilPainting, and Pandora datasets.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3240508.3240593',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'STAR: Noisy Semi-Supervised Transfer Learning for Visual Classification',\n",
       "  'authors': \"['Hasib Zunair', 'Yan Gobeil', 'Samuel Mercier', 'Abdessamad Ben Hamza']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"MMSports'21: Proceedings of the 4th International Workshop on Multimedia Content Analysis in Sports\",\n",
       "  'abstract': 'Semi-supervised learning (SSL) has proven to be effective at leveraging large-scale unlabeled data to mitigate the dependency on labeled data in order to learn better models for visual recognition and classification tasks. However, recent SSL methods rely on unlabeled image data at a scale of billions to work well. This becomes infeasible for tasks with relatively fewer unlabeled data in terms of runtime, memory and data acquisition. To address this issue, we propose noisy semi-supervised transfer learning, an efficient SSL approach that integrates transfer learning and self-training with noisy student into a single framework, which is tailored for tasks that can leverage unlabeled image data on a scale of thousands. We evaluate our method on both binary and multi-class classification tasks, where the objective is to identify whether an image displays people practicing sports or the type of sport, as well as to identify the pose from a pool of popular yoga poses. Extensive experiments and ablation studies demonstrate that by leveraging unlabeled data, our proposed framework significantly improves visual classification, especially in multi-class classification settings compared to state-of-the-art methods. Moreover, incorporating transfer learning not only improves classification performance, but also requires 6x less compute time and 5x less memory. We also show that our method boosts robustness of visual classification models, even without specifically optimizing for adversarial robustness.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3475722.3482791',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Fingerprint classification based on a Q-Gaussian multiclass support vector machine',\n",
       "  'authors': \"['Mohamed Hammad', 'Kuanquan Wang']\",\n",
       "  'date': 'April 2017',\n",
       "  'source': \"ICBEA '17: Proceedings of the 2017 International Conference on Biometrics Engineering and Application\",\n",
       "  'abstract': 'Accurate recognition and actual classification of fingerprint are vital and necessary for fingerprint identification. Previous researchers have used many classification algorithms to develop fingerprint classification model, but they still have some certain problems like time of implementation to do the task, cost of implementation, working on non-linear features, working on multi-dimensional features and under or over learning problems. In this paper, a Q-Gaussian multi-class support vector machine (QG-MSVM) for fingerprint classification is proposed in which Q-Gaussian function is incorporated into SVM as a kernel function. The proposed method is tested in CASIA, FVC2000, FVC2002 and FVC2004 databases and compared with the MSVM methods with linear kernel, Gaussian Radial Basis Function kernel (RBF), Polynomial kernel and other state-of-the-art methods. The experimental results show that QG-MSVM demonstrates better performance than other classifiers and overcome many MSVM problems. The overall performance of the QG-MSVM classifier is comprehensively superior to all others.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3077829.3077836',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Machine Learning Based Unified Framework for Diabetes Prediction',\n",
       "  'authors': \"['S. M. Hasan Mahmud', 'Md Altab Hossin', 'Md. Razu Ahmed', 'Sheak Rashed Haider Noori', 'Md Nazirul Islam Sarkar']\",\n",
       "  'date': 'August 2018',\n",
       "  'source': 'BDET 2018: Proceedings of the 2018 International Conference on Big Data Engineering and Technology',\n",
       "  'abstract': \"Machine learning gained a significant position in healthcare services (HCS) due to its ability to improve the disease prediction in HCS. Machine learning techniques and artificial intelligence have already been worked in the HCS area. Recently, diabetes is a notable public chronic disease worldwide. It is growing rapidly because of bad lifestyles, taking more junk food and also lake of health awareness. Therefore, there is a need of framework that can effectively track and monitor people's diabetes and health condition within an application view. In this study, we proposed a framework for real time diabetes prediction, monitoring and application (DPMA). Our objective is to develop an optimized and efficient machine learning (ML) application which can effectually recognize and predict the condition of the diabetes. In this work, five most important machine learning classification techniques were considered for predicting diabetes. However, we use different evaluation criteria to investigate the performance of these classification techniques. In addition, performance measurement of the classification techniques was evaluated by applying the 10-fold cross validation method. The analysis results show that Naïve Bayes achieved highest performance than the other classifiers, obtaining the F1 measure of 0.74.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3297730.3297737',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Deep Learning meets Knowledge Graphs for Scholarly Data Classification',\n",
       "  'authors': \"['Fabian Hoppe', 'Danilo Dessì', 'Harald Sack']\",\n",
       "  'date': 'April 2021',\n",
       "  'source': \"WWW '21: Companion Proceedings of the Web Conference 2021\",\n",
       "  'abstract': 'The amount of scientific literature continuously grows, which poses an increasing challenge for researchers to manage, find and explore research results. Therefore, the classification of scientific work is widely applied to enable the retrieval, support the search of suitable reviewers during the reviewing process, and in general to organize the existing literature according to a given schema. The automation of this classification process not only simplifies the submission process for authors, but also ensures the coherent assignment of classes. However, especially fine-grained classes and new research fields do not provide sufficient training data to automatize the process. Additionally, given the large number of not mutual exclusive classes, it is often difficult and computationally expensive to train models able to deal with multi-class multi-label settings. To overcome these issues, this work presents a preliminary Deep Learning framework as a solution for multi-label text classification for scholarly papers about Computer Science. The proposed model addresses the issue of insufficient data by utilizing the semantics of classes, which is explicitly provided by latent representations of class labels. This study uses Knowledge Graphs as a source of these required external class definitions by identifying corresponding entities in DBpedia to improve the overall classification.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3442442.3451361',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'New Incremental Learning Algorithm for Semi-Supervised Support Vector Machine',\n",
       "  'authors': \"['Bin Gu', 'Xiao-Tong Yuan', 'Songcan Chen', 'Heng Huang']\",\n",
       "  'date': 'July 2018',\n",
       "  'source': \"KDD '18: Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining\",\n",
       "  'abstract': 'Semi-supervised learning is especially important in data mining applications because it can make use of plentiful unlabeled data to train the high-quality learning models. Semi-Supervised Support Vector Machine (S3VM) is a powerful semi-supervised learning model. However, the high computational cost and non-convexity severely impede the S3VM method in large-scale applications. Although several learning algorithms were proposed for S3VM, scaling up S3VM is still an open problem. To address this challenging problem, in this paper, we propose a new incremental learning algorithm to scale up S3VM (IL-S3VM) based on the path following technique in the framework of Difference of Convex (DC) programming. The traditional DC programming based algorithms need multiple outer loops and are not suitable for incremental learning, and traditional path following algorithms are limited to convex problems. Our new IL-S3VM algorithm based on the path-following technique can directly update the solution of S3VM to converge to a local minimum within one outer loop so that the efficient incremental learning can be achieved. More importantly, we provide the finite convergence analysis for our new algorithm. To the best of our knowledge, our new IL-S3VM algorithm is the first efficient path following algorithm for a non-convex problem (i.e., S3VM) with local minimum convergence guarantee. Experimental results on a variety of benchmark datasets not only confirm the finite convergence of IL-S3VM, but also show a huge reduction of computational time compared with existing batch and incremental learning algorithms, while retaining the similar generalization performance.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3219819.3220092',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'The Evolution of the Industry 4.0: A Retrospective Analysis Using Text Mining',\n",
       "  'authors': \"['Zeynep Didem Unutmaz Durmuşoğlu', 'Pınar Kocabey Çiftçi']\",\n",
       "  'date': 'June 2018',\n",
       "  'source': \"ICEMIS '18: Proceedings of the Fourth International Conference on Engineering &amp; MIS 2018\",\n",
       "  'abstract': 'Industry 4.0 and its applications have attracted the interest of both experts of industry and researchers of academy. This interest started to build a vast body of literature on the related topics and caused to raise a new question about how the related literature has changed over time. In this context, the major objective of the presented study is to provide insight about the scientific researches on industry 4.0 using the publications from the Thomson Reuters Web of Knowledge database during the period of 2007-2017. For this aim, the retrieved academic studies were analyzed using quantitative and text mining analyses to observe the change in number of publications over years and to gain insight about the textual structure. The results of the quantitative analysis showed that the popularity of industry 4.0 in the academy has risen significantly and reached the peak point in 2017 with 2658 articles. According to the text mining study, the most important research topics related to the industry 4.0 field are \"mobile\", \"internet of things\" and \"cloud computing\".',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3234698.3234757',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Mining High Utility Itemsets with Hill Climbing and Simulated Annealing',\n",
       "  'authors': \"['M. Saqib Nawaz', 'Philippe Fournier-Viger', 'Unil Yun', 'Youxi Wu', 'Wei Song']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Management Information Systems',\n",
       "  'abstract': 'High utility itemset mining (HUIM) is the task of finding all items set, purchased together, that generate a high profit in a transaction database. In the past, several algorithms have been developed to mine high utility itemsets (HUIs). However, most of them cannot properly handle the exponential search space while finding HUIs when the size of the database and total number of items increases. Recently, evolutionary and heuristic algorithms were designed to mine HUIs, which provided considerable performance improvement. However, they can still have a long runtime and some may miss many HUIs. To address this problem, this article proposes two algorithms for HUIM based on Hill Climbing (HUIM-HC) and Simulated Annealing (HUIM-SA). Both algorithms transform the input database into a bitmap for efficient utility computation and for search space pruning. To improve population diversity, HUIs discovered by evolution are used as target values for the next population instead of keeping the current optimal values in the next population. Through experiments on real-life datasets, it was found that the proposed algorithms are faster than state-of-the-art heuristic and evolutionary HUIM algorithms, that HUIM-SA discovers similar HUIs, and that HUIM-SA evolves linearly with the number of iterations.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3462636',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Analysis of unstructured text data for a person social profile',\n",
       "  'authors': \"['Alexey Y. Timonin', 'Alexander S. Bozhday', 'Alexander M. Bershadsky']\",\n",
       "  'date': 'September 2017',\n",
       "  'source': \"eGose '17: Proceedings of the Internationsl Conference on Electronic Governance and Open Society: Challenges in Eurasia\",\n",
       "  'abstract': 'The greatest scientific interest for analysts are Internet open social data, because it has a direct link with all kinds of human activity. However, these data are not suitable for the application in its original form. Information should be presented in a structured, convenient, human-readable form which is called a social profile. The social profile building is carried out through the analysis of the filtered Internet open source data. Analysis of personal profile data is achieved through the use of mathematical set theory, Big Data software, NoSQL data stores and analytic tools for social media. This article discusses methods of unstructured textual data analysis in relation to a social profile. Special attention is given to the search of implicit dependences in texts using visual analysis and natural language processing means. Phase of the textual data analysis is the most important in terms of results and complicated to implement. There is the possibility to partially automate the process of information analyzing through the use of visual analysis, natural language processing (NLP), neural networks and specialized algorithms. Resulted data provide a detailed in-depth review of the social profile entities and relations. It can be used in further deeper social researches.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3129757.3129758',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Meta-Learning for Few-Shot Time Series Classification',\n",
       "  'authors': \"['Jyoti Narwariya', 'Pankaj Malhotra', 'Lovekesh Vig', 'Gautam Shroff', 'T. V. Vishnu']\",\n",
       "  'date': 'January 2020',\n",
       "  'source': 'CoDS COMAD 2020: Proceedings of the 7th ACM IKDD CoDS and 25th COMAD',\n",
       "  'abstract': 'Deep neural networks (DNNs) have achieved state-of-the-art results on time series classification (TSC) tasks. In this work, we focus on leveraging DNNs in the often-encountered practical scenario where access to labeled training data is difficult, and where DNNs would be prone to overfitting. We leverage recent advancements in gradient-based meta-learning, and propose an approach to train a residual neural network with convolutional layers as a meta-learning agent for few-shot TSC. The network is trained on a diverse set of few-shot tasks sampled from various domains (e.g. healthcare, activity recognition, etc.) such that it can solve a target task from another domain using only a small number of training samples from the target task. Most existing meta-learning approaches are limited in practice as they assume a fixed number of target classes across tasks. We overcome this limitation in order to train a common agent across domains with each domain having different number of target classes, we utilize a triplet-loss based learning procedure that does not require any constraints to be enforced on the number of classes for the few-shot TSC tasks. To the best of our knowledge, we are the first to use meta-learning based pre-training for TSC. Our approach sets a new benchmark for few-shot TSC, outperforming several strong baselines on few-shot tasks sampled from 41 datasets in UCR TSC Archive. We observe that pre-training under the meta-learning paradigm allows the network to quickly adapt to new unseen tasks with small number of labeled instances.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3371158.3371162',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Research on animal image classification based on transfer learning',\n",
       "  'authors': \"['Man Hu', 'Fucheng You']\",\n",
       "  'date': 'November 2020',\n",
       "  'source': \"EITCE '20: Proceedings of the 2020 4th International Conference on Electronic Information Technology and Computer Engineering\",\n",
       "  'abstract': 'Training a convolutional neural network requires a large number of sample data and a large amount of computing power. In some practical application scenarios, there may be difficulties in sample data collection and complex network model construction. To improve the classification accuracy and fitting speed of the convolutional neural network, a transfer learning classification method for an animal image is proposed. The fully connected layer of the pre-trained ResNet18 network is modified, and the eight animals in the animal-10 dataset on Kaggle are used to fine-tune the network model. The best classification accuracy of the obtained network model for a single animal is 97%, and the classification accuracy for all animals is 92%. Compared with the model that did not use transfer learning training, this kind of transfer learning network model has a great improvement in accuracy and fitting speed.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3443467.3443849',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Clustering without Over-Representation',\n",
       "  'authors': \"['Sara Ahmadian', 'Alessandro Epasto', 'Ravi Kumar', 'Mohammad Mahdian']\",\n",
       "  'date': 'July 2019',\n",
       "  'source': \"KDD '19: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining\",\n",
       "  'abstract': 'In this paper we consider clustering problems in which each point is endowed with a color. The goal is to cluster the points to minimize the classical clustering cost but with the additional constraint that no color is over-represented in any cluster. This problem is motivated by practical clustering settings, e.g., in clustering news articles where the color of an article is its source, it is preferable that no single news source dominates any cluster. For the most general version of this problem, we obtain an algorithm that has provable guarantees of performance; our algorithm is based on finding a fractional solution using a linear program and rounding the solution subsequently. For the special case of the problem where no color has an absolute majority in any cluster, we obtain a simpler combinatorial algorithm also with provable guarantees. Experiments on real-world data shows that our algorithms are effective in finding good clustering without over-representation.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3292500.3330987',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Clustering without Over-Representation',\n",
       "  'authors': \"['Sara Ahmadian', 'Alessandro Epasto', 'Ravi Kumar', 'Mohammad Mahdian']\",\n",
       "  'date': 'July 2019',\n",
       "  'source': \"KDD '19: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining\",\n",
       "  'abstract': 'In this paper we consider clustering problems in which each point is endowed with a color. The goal is to cluster the points to minimize the classical clustering cost but with the additional constraint that no color is over-represented in any cluster. This problem is motivated by practical clustering settings, e.g., in clustering news articles where the color of an article is its source, it is preferable that no single news source dominates any cluster. For the most general version of this problem, we obtain an algorithm that has provable guarantees of performance; our algorithm is based on finding a fractional solution using a linear program and rounding the solution subsequently. For the special case of the problem where no color has an absolute majority in any cluster, we obtain a simpler combinatorial algorithm also with provable guarantees. Experiments on real-world data shows that our algorithms are effective in finding good clustering without over-representation.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3292500.3330987',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Research on animal image classification based on transfer learning',\n",
       "  'authors': \"['Man Hu', 'Fucheng You']\",\n",
       "  'date': 'November 2020',\n",
       "  'source': \"EITCE '20: Proceedings of the 2020 4th International Conference on Electronic Information Technology and Computer Engineering\",\n",
       "  'abstract': 'Training a convolutional neural network requires a large number of sample data and a large amount of computing power. In some practical application scenarios, there may be difficulties in sample data collection and complex network model construction. To improve the classification accuracy and fitting speed of the convolutional neural network, a transfer learning classification method for an animal image is proposed. The fully connected layer of the pre-trained ResNet18 network is modified, and the eight animals in the animal-10 dataset on Kaggle are used to fine-tune the network model. The best classification accuracy of the obtained network model for a single animal is 97%, and the classification accuracy for all animals is 92%. Compared with the model that did not use transfer learning training, this kind of transfer learning network model has a great improvement in accuracy and fitting speed.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3443467.3443849',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'HINFShot: A Challenge Dataset for Few-Shot Node Classification in Heterogeneous Information Network',\n",
       "  'authors': \"['Zifeng Zhuang', 'Xintao Xiang', 'Siteng Huang', 'Donglin Wang']\",\n",
       "  'date': 'August 2021',\n",
       "  'source': \"ICMR '21: Proceedings of the 2021 International Conference on Multimedia Retrieval\",\n",
       "  'abstract': 'Few-shot learning aims to generalize to novel classes. It has achieved great success in image and text classification tasks. Inspired by such success, few-shot node classification in homogeneous graph has attracted much attention but few works have begun to study this problem in Heterogeneous Information Network (HIN) so far. We consider few-shot learning in HIN and study a pioneering problem HIN Few-Shot Node Classification (HIN-FSNC) that aims to generalize the node types with sufficient labeled samples to unseen node types with only few-labeled samples. However, existing HIN datasets contain just one labeled node type, which means they cannot meet the setting of unseen node types. To facilitate the investigation of HIN-FSNC, we propose a large-scale academic HIN dataset called HINFShot. It contains 1,235,031 nodes with four node types (author, paper, venue, institution) and all the nodes regardless of node type are divided into 80 classes. Finally, we conduct extensive experiments on HINFShot and the result indicates a significant challenge of identifying novel classes of unseen node types in HIN-FSNC.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3460426.3463614',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A preconception gender assessment using data mining techniques based on implementation of natural laws & favoring factors',\n",
       "  'authors': \"['Shaista Sabir', 'Usman Qamar', 'Tanveer Ahmed', 'Mubashir Ali']\",\n",
       "  'date': 'October 2017',\n",
       "  'source': \"IML '17: Proceedings of the 1st International Conference on Internet of Things and Machine Learning\",\n",
       "  'abstract': 'Healthcare field1 is vital organization of our society as it directly affects the living being A balanced society is the need of the hour that we can achieve by a balanced family structure. In all over the world especially in Asia and Africa, couples show a preference for a particular gender of child, either male or female (1). This preference may be the result of economic, social pressure, custom of the people or it may simply be due to the reason of \"Gender balanced family\" (2). A lot of research in medical field is present which shows how to achieve the goal of getting a child of desired gender in a way that is more natural. Similarly, in this era of advanced technology data mining techniques are becoming more and more popular in medical field. In this paper, we analyzed different research methods in medical field based on Natural Laws & Favoring Factors, extracted different macroscopic factors affecting directly the possible gender of offspring in the womb of the mother before conception and generated our dataset using these macroscopic factors. Then we applied different Data mining classification techniques on our dataset to classify the possible gender as male or female.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3109761.3158405',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Geographical Entity Community Mining Based on Spatial and Semantic Association',\n",
       "  'authors': \"['Mengyu Yan', 'Ning Jing', 'Zhinong Zhong', 'Ye Wu']\",\n",
       "  'date': 'October 2019',\n",
       "  'source': \"CSAE '19: Proceedings of the 3rd International Conference on Computer Science and Application Engineering\",\n",
       "  'abstract': \"The relevance of geographic entities has always been the focus of research on geographic information retrieval, geographic knowledge graph and recommendation systems. Traditional research methods, which use spatial or semantic similarity to calculate the correlation between regions, have certain one-sidedness and limitations. The network topology can clearly represent the relationship between entities. However, semantic relationships are difficult to define, so there are few cases where network-related algorithms are used to solve the relevance of geographic entities. With the development of the Internet, web pages provide people with a huge amount of information, and geographical names as a key element are often ignored by researchers, and the rich semantic information contained in it needs further research. This study attempts to explore the geographic entity relevance of integrated semantics and spatial factors based on textual data from a network perspective. Based on the community mining algorithm, the experiment studies the aggregation characteristics of geographic entities and can find areas that are close to each other and tightly related, which is more satisfied with people's common sense.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3331453.3361652',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Review On Sentiment Analysis of Twitter Posts About News Headlines Using Machine Learning Approaches and Naïve Bayes Classifier',\n",
       "  'authors': \"['Thinesharan Vaseeharan', 'Achala Aponso']\",\n",
       "  'date': 'February 2020',\n",
       "  'source': 'ICCAE 2020: Proceedings of the 2020 12th International Conference on Computer and Automation Engineering',\n",
       "  'abstract': \"In today's world there are so much micro blogging sites, among all twitter is one of the popular site. It has become an important part for all individuals, politicians, companies, celebrities, etc. Almost all the major news outlets have Twitter account where they post news headlines for their followers. People with Twitter accounts can reply or retweet the news headlines. Twitter users who have an account can also post news headlines from any other news outlets. When people post, reply or retweet news posts on Twitter, it is obvious that they are expressing their sentiments through that. The main aim of the paper is to extract subjectivity of opinions of people about particular news in Twitter. Specially, the interest is in determining the sentiment of Twitter posts about particular news. This paper Explores Naïve Bayes Classifier for textual classification and various twitter-specific sentiment analysis studies applied to Twitter data and their Outcomes.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3384613.3384650',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Web Video Clustering Based on Emotion Category',\n",
       "  'authors': \"['Vinath Mekthanavanh', 'Tianrui Li', 'Jie Hu', 'Yan Yang']\",\n",
       "  'date': 'August 2018',\n",
       "  'source': 'BDET 2018: Proceedings of the 2018 International Conference on Big Data Engineering and Technology',\n",
       "  'abstract': 'Web video clustering is a fundamental task in the field of social media mining. Automatically web video categorization methods enable users to find video corresponding to their interests. However, all the previous studies are only conducted on the default categories given by the website, i.e., 15 categories of YouTube. To date, clustering based on emotion category has not been a factor considered in this area. Therefore, in this paper, we propose a method to cluster YouTube videos into six emotion categories (e.g., angry, disgust, happy, horror, sad, surprise) with expect to improve video search results. The YouTube data is collected. Word embedding is utilized for transforming the video document into vectors which are then used in a clustering task. Clustering ensemble is employed to obtain final results. We compare the performance of this method with a state-of-the-art technology, i.e., Term Frequency-Inverse Document Frequency based on Vector Space Model. The experiment is implemented on a benchmark dataset for web video analysis. The results show that the best performance is achieved by applying clustering ensemble which reflects the feasibility of clustering web videos into suitable emotion categories.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3297730.3297736',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Analyze NYC Transportation to Mitigate Speeding and Explore New Business Models Using Machine Learning',\n",
       "  'authors': \"['Joe Ma', 'Dong Si']\",\n",
       "  'date': 'May 2017',\n",
       "  'source': \"ICCDA '17: Proceedings of the International Conference on Compute and Data Analysis\",\n",
       "  'abstract': \"Many cities have been releasing their traffic data for companies to do the data analytics for business and other purposes. In this paper, we propose different classification models to analyze the places that most of vehicles speed and the places that most of vehicles visited in specific time range. The result of it can be helpful for the government allocating the police in the correct timing and places to catch vehicles speeding. In addition, by knowing the result of the major places that most of vehicles visited, it's helpful for gas companies to decide where they should build gas stations.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3093241.3093291',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Towards mining answer edits to extract evolution patterns in stack overflow',\n",
       "  'authors': \"['Themistoklis Diamantopoulos', 'Maria-Ioanna Sifaki', 'Andreas L. Symeonidis']\",\n",
       "  'date': 'May 2019',\n",
       "  'source': \"MSR '19: Proceedings of the 16th International Conference on Mining Software Repositories\",\n",
       "  'abstract': 'The current state of practice dictates that in order to solve a problem encountered when building software, developers ask for help in online platforms, such as Stack Overflow. In this context of collaboration, answers to question posts often undergo several edits to provide the best solution to the problem stated. In this work, we explore the potential of mining Stack Overflow answer edits to extract common patterns when answering a post. In particular, we design a similarity scheme that takes into account the text and code of answer edits and cluster edits according to their semantics. Upon applying our methodology, we provide frequent edit patterns and indicate how they could be used to answer future research questions. Assessing our approach indicates that it can be effective for identifying commonly applied edits, thus illustrating the transformation path from the initial answer to the optimal solution.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/MSR.2019.00043',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Research on Mental Health Rating Algorithm Based on Big Data Classification Algorithm',\n",
       "  'authors': \"['Xiaokun Zhang']\",\n",
       "  'date': 'September 2021',\n",
       "  'source': 'ICISCAE 2021: 2021 4th International Conference on Information Systems and Computer Aided Education',\n",
       "  'abstract': \"In the era of big data, the industries of informatization construction and digital office are increasing, and big data mining has been widely used, which is both an opportunity and a challenge. Facing all kinds of data generated by society and people, it is imperative to classify them accurately, mine effective information and improve the utilization rate of information. College students, as a special group of society, are facing pressure from all sides, and there are more and more mental health problems, which lead to extreme events. How to find valuable information from huge test data has always been a difficult problem in the field of mental health testing. Based on Apriori algorithm, this paper researches association rules mining, finds out the relationship between the main factors and symptoms of college students' mental health, and provides support for solving college students' mental health problems and developing mental health test system. The research shows that Apriori improved algorithm reduces the workload of data calculation and improves the mining efficiency.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3482632.3482656',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Towards Integrated Classification Lexicon for Handling Unknown Words in Chinese-Vietnamese Neural Machine Translation',\n",
       "  'authors': \"['Wanjin Che', 'Zhengtao Yu', 'Zhiqiang Yu', 'Yonghua Wen', 'Junjun Guo']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Asian and Low-Resource Language Information Processing',\n",
       "  'abstract': 'In Neural Machine Translation (NMT), due to the limitations of the vocabulary, unknown words cannot be translated properly, which brings suboptimal performance of the translation system. For resource-scarce NMT that have small-scale training corpus, the effect is amplified. The traditional approach of amplifying the scale of the corpus is not applicable, because the parallel corpus is difficult to obtain in a resource-scarce setting; however, it is easy to obtain and utilize external knowledge, bilingual lexicon, and other resources. Therefore, we propose classification lexicon approach for processing unknown words in the Chinese-Vietnamese NMT task. Specifically, three types of unknown Chinese-Vietnamese words are classified and their corresponding classification lexicon are constructed by word alignment, Wikipedia extraction, and rule-based methods, respectively. After translation, the unknown words are restored by lexicon for post-processing. Experiment results on Chinese-Vietnamese, English-Vietnamese, and Mongolian-Chinese translations show that our approach significantly improves the accuracy and the performance of NMT especially in a resource-scarce setting.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3373267',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Classification and exploration of TSM log file based on datamining Algorithms',\n",
       "  'authors': \"['Jamal El abdelkhalki', 'Mohamed Ben ahmed', 'Boudhir Hakim Anouar']\",\n",
       "  'date': 'November 2017',\n",
       "  'source': \"ICCWCS'17: Proceedings of the 2nd International Conference on Computing and Wireless Communication Systems\",\n",
       "  'abstract': 'analyzing the log file for software or device provides a focal point for making incremental improvements; it is the performed step to start the incident analysis. Although, log messages format or contents may not always be fully documented, and described in many different formats. It makes the log analysis task more difficult, affects the correction deadline of incidents and therefore involves a high financial risk. In this paper, we survey the log file analysis and the existing systems elaborated to resolve current issue. Then, we propose a methodology to support the log analysis in the complex environment related to big data issues. Finally, we illustrate our proposal on the file log of the Tivoli Storage Manager (TSM) and provide a discussion of the result clusters.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3167486.3167553',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Mining patterns from genetic improvement experiments',\n",
       "  'authors': \"['Oliver Krauss', 'Hanspeter Mössenböck', 'Michael Affenzeller']\",\n",
       "  'date': 'May 2019',\n",
       "  'source': \"GI '19: Proceedings of the 6th International Workshop on Genetic Improvement\",\n",
       "  'abstract': 'When conducting genetic improvement experiments, a large amount of individuals (≈ population size * generations) is created and evaluated. The corresponding experiments contain valuable data concerning the fitness of individuals for the defined criteria, such as run-time performance, memory use or robustness. This publication presents an approach to utilize this information in order to identify recurring context independent patterns in abstract syntax trees (ASTs). These patterns can be applied for restricting the search space (in the form of anti-patterns) or for grafting operators in the population. Future work includes an evaluation of this approach, as well as extending it with wildcards and class hierarchies for larger and more generalized patterns.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/GI.2019.00015',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Discovery of Functional Motifs from the Interface Region of Oligomeric Proteins Using Frequent Subgraph Mining',\n",
       "  'authors': \"['Tanay Kumar Saha', 'Ataur Katebi', 'Wajdi Dhifli', 'Mohammad Al Hasan']\",\n",
       "  'date': 'None',\n",
       "  'source': 'IEEE/ACM Transactions on Computational Biology and Bioinformatics',\n",
       "  'abstract': 'Modeling the interface region of a protein complex paves the way for understanding its dynamics and functionalities. Existing works model the interface region of a complex by using different approaches, such as, the residue composition at the interface region, the geometry of the interface residues, or the structural alignment of interface regions. These approaches are useful for ranking a set of docked conformation or for building scoring function for protein-protein docking, but they do not provide a generic and scalable technique for the extraction of interface patterns leading to functional motif discovery. In this work, we model the interface region of a protein complex by graphs and extract interface patterns of the given complex in the form of frequent subgraphs. To achieve this, we develop a scalable algorithm for frequent subgraph mining. We show that a systematic review of the mined subgraphs provides an effective method for the discovery of functional motifs that exist along the interface region of a given protein complex. In our experiments, we use three PDB protein structure datasets. The first two datasets are composed of PDB structures from different conformations of two dimeric protein complexes: HIV-1 protease 329 structures, and triosephosphate isomerase TIM 86 structures. The third dataset is a collection of different enzyme structures protein structures from the six top-level enzyme classes, namely: Oxydoreductase, Transferase, Hydrolase, Lyase, Isomerase, and Ligase. We show that for the first two datasets, our method captures the locking mechanism at the dimeric interface by taking into account the spatial positioning of the interfacial residues through graphs. Indeed, our frequent subgraph mining based approach discovers the patterns representing the dimerization lock which is formed at the base of the structure in 323 of the 329 HIV-1 protease structures. Similarly, for 86 TIM structures, our approach discovers the dimerization lock formation in 50 structures. For the enzyme structures, we show that we are able to capture the functional motifs active sites that are specific to each of the six top-level classes of enzymes through frequent subgraphs.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/TCBB.2017.2756879',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Comparative Study of Arabic Text Categorization Using Feature Selection Techniques and Four Classifier Models',\n",
       "  'authors': \"['Said Bahassine', 'Abdellah Madani', 'Mohamed Kissi']\",\n",
       "  'date': 'September 2020',\n",
       "  'source': \"SITA'20: Proceedings of the 13th International Conference on Intelligent Systems: Theories and Applications\",\n",
       "  'abstract': 'Text classification is the process of assigning appropriate categories to free text according to its content. It is one of the important task in Text mining. Numerous studies have been conducted for natural languages processing using Japanese, French, Latin and Turkish documents, but the number of works related to the text written in Arabic language is still limited. In this paper we conduct a comparative study of three methods of feature selection using four well-known classifiers namely: Decision Tree, Naive Bayes, K-Nearest Neighbors and Support Vector Machine. A corpus contained 250 Arabic text belonging into five classes: sport, politics, economics, culture and art, and society. The data set is used to evaluate and compare the effectiveness of the obtained model. The experimental results reveal that using improved Chi-square method as feature selection and Support Vector Machine as classifier outperforms other combinations in terms of precision. This combination significantly improves the performance of Arabic text classification model. The highest value of precision measure for this model is 89.9%.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3419604.3419778',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': \"Application Data Mining Technology in Monitoring Strategies of College Students' English Learning Motivation\",\n",
       "  'authors': \"['Shuang Liu']\",\n",
       "  'date': 'September 2021',\n",
       "  'source': 'ICISCAE 2021: 2021 4th International Conference on Information Systems and Computer Aided Education',\n",
       "  'abstract': \"With the advent of the era of big data, how to apply big data technology to English learning has become a problem that people care about. This research mainly discusses the monitoring strategies of college students' English learning motivation based on data mining. After the collection of Web logs, an information matrix of online learning motivation is formed. Frequent user groups are retrieved through frequently visited page sets, and similar user groups are obtained through association rule analysis, and then related page sets are obtained from frequently visited page sets according to the set distance threshold between pages. Finally, perform cluster analysis on user information to obtain their preferences. Using gender as a categorical variable, the test found that there are significant differences in the intrinsic interest motivation and overall learning motivation dimensions of male and female students. The score of boys (2.6) and the score of girls in this item (3.873) are higher than those of boys. Therefore, the students' learning interest and motivation can be cultivated in a targeted manner, thereby effectively improving the students' academic performance.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3482632.3484150',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Imbalanced big data classification: a distributed implementation of SMOTE',\n",
       "  'authors': \"['Avnish Kumar Rastogi', 'Nitin Narang', 'Zamir Ahmad Siddiqui']\",\n",
       "  'date': 'January 2018',\n",
       "  'source': \"Workshops ICDCN '18: Proceedings of the Workshop Program of the 19th International Conference on Distributed Computing and Networking\",\n",
       "  'abstract': 'In the domain of machine learning, quality of data is most critical component for building good models. Predictive analytics is an AI stream used to predict future events based on historical learnings and is used in diverse fields like predicting online frauds, oil slicks, intrusion attacks, credit defaults, prognosis of disease cells etc. Unfortunately, in most of these cases, traditional learning models fail to generate required results due to imbalanced nature of data. Here imbalance denotes small number of instances belonging to the class under prediction like fraud instances in the total online transactions. The prediction in imbalanced classification gets further limited due to factors like small disjuncts which get accentuated during the partitioning of data when learning at scale. Synthetic generation of minority class data (SMOTE [<u>1</u>]) is one pioneering approach by Chawla [<u>1</u>] to offset said limitations and generate more balanced datasets. Although there exists a standard implementation of SMOTE in python, it is unavailable for distributed computing environments for large datasets. Bringing SMOTE to distributed environment under spark is the key motivation for our research. In this paper we present our algorithm, observations and results for synthetic generation of minority class data under spark using Locality Sensitivity Hashing [LSH]. We were able to successfully demonstrate a distributed version of Spark SMOTE which generated quality artificial samples preserving spatial distribution1.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3170521.3170535',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Nonparametric Classification of Satellite Images',\n",
       "  'authors': \"['Romans Dinuls', 'Ints Mednieks']\",\n",
       "  'date': 'July 2018',\n",
       "  'source': \"ICoMS '18: Proceedings of the 2018 1st International Conference on Mathematics and Statistics\",\n",
       "  'abstract': 'The task of classifying the objects on a satellite image into predefined categories is the topic of the article. The problems arising while designing a practicable classifier are discussed. The general conditions for robustness of a classifier are provided. To solve the problems mentioned, a robust classification approach is proposed aiming at completely nonparametric unsupervised clustering with consequent association of the clusters with target categories using multiple sources of the testing and training data. The nonparametric clustering used is primarily based on ranking and grouping. Completely nonparametric cluster union and cleaning procedures are presented; theoretical basics for other parts of the approach are provided. The software implementation and complexity of the methodology are discussed. The approach aims at getting the highest possible classification accuracy under real conditions for images with more than 100 million pixels.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3274250.3274260',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Automatic Validation of Textual Attribute Values in E-commerce Catalog by Learning with Limited Labeled Data',\n",
       "  'authors': \"['Yaqing Wang', 'Yifan Ethan Xu', 'Xian Li', 'Xin Luna Dong', 'Jing Gao']\",\n",
       "  'date': 'August 2020',\n",
       "  'source': \"KDD '20: Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining\",\n",
       "  'abstract': 'Product catalogs are valuable resources for eCommerce website. In the catalog, a product is associated with multiple attributes whose values are short texts, such as product name, brand, functionality and flavor. Usually individual retailers self-report these key values, and thus the catalog information unavoidably contains noisy facts. It is very important to validate the correctness of these values in order to improve shopper experiences and enable more effective product recommendation. Due to the huge volume of products, an effective automatic validation approach is needed. In this paper, we propose to develop an automatic validation approach that verifies the correctness of textual attribute values for products. This can be formulated as a task as cross-checking a textual attribute value against product profile, which is a short textual description of the product on eCommerce website. Although existing deep neural network models have shown success in conducting cross-checking between two pieces of texts, their success has to be dependent upon a large set of quality labeled data, which are hard to obtain in this validation task: products span a variety of categories. Due to the category difference, annotation has to be done on all the categories, which is impossible to achieve in real practice. To address the aforementioned challenges, we propose a novel meta-learning latent variable approach, called MetaBridge, which can learn transferable knowledge from a subset of categories with limited labeled data and capture the uncertainty of never-seen categories with unlabeled data. More specifically, we make the following contributions. (1) We formalize the problem of validating the textual attribute values of products from a variety of categories as a natural language inference task in the few-shot learning setting, and propose a meta-learning latent variable model to jointly process the signals obtained from product profiles and textual attribute values. (2) We propose to integrate meta learning and latent variable in a unified model to effectively capture the uncertainty of various categories. With this model, annotation costs can be significantly reduced as we make best use of labeled data from limited categories. (3) We propose a novel objective function based on latent variable model in the few-shot learning setting, which ensures distribution consistency between unlabeled and labeled data and prevents overfitting by sampling different records from the learned distribution. Extensive experiments on real eCommerce datasets from hundreds of categories demonstrate the effectiveness of MetaBridge on textual attribute validation and its outstanding performance compared with state-of-the-art approaches.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3394486.3403303',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Text Classification Method with Combination of Fuzzy Relation and Feature Distribution Variance',\n",
       "  'authors': \"['Wei Liu', 'Renze Xiong', 'Ning N Cheng', 'Yiming Y Sun']\",\n",
       "  'date': 'October 2020',\n",
       "  'source': \"CCRIS '20: Proceedings of the 2020 1st International Conference on Control, Robotics and Intelligent System\",\n",
       "  'abstract': 'To accurately express the fuzzy relation between word features and texts, and fuzzy relation between word features and categories respectively. A text classification method is proposed based on Fuzzy Relation and Feature Distribution Variance (FRFDV). This method firstly performs feature reduction and category feature word extraction according to the distribution of features in inter-category and intra-category. Then the method defines the word feature set, test text set and category set as fuzzy sets. Next, each text and category are represented respectively by defining the membership function of the word feature set to the test text set and the category set. When using word feature sets to represent categories, pay attention to the membership degree of features to categories and their distribution between categories; when using feature sets to represent test texts, give categorical feature words and non-categorical feature words with different weights. Finally, the fuzzy set correlation formula is used to calculate the correlation between the text and each category, and the category with the largest correlation is the category of the text. Comparing with the XGBOOST [Fang, 2020, Gong and Wang, 2018] algorithm and SVM algorithm, it is proved that the text classification method based on FRFDV is feasible. The accuracy of the results is higher by 2 % and 4 % respectively.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3437802.3437829',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Combining Active Learning and Data Augmentation for Image Classification',\n",
       "  'authors': \"['Yu Ma', 'Shaoxing Lu', 'Erya Xu', 'Tian Yu', 'Lijian Zhou']\",\n",
       "  'date': 'September 2020',\n",
       "  'source': \"ICBDT '20: Proceedings of the 3rd International Conference on Big Data Technologies\",\n",
       "  'abstract': 'To solve the problem that the data annotation in image classification task requires a lot of time and economic costs, and a large number of unlabeled images cannot be effectively utilized in reality, an image classification method combining active learning algorithm and data augmentation is proposed. First, data augmentation is performed on a small number of labeled samples, the classification model is initially trained, and then, according to the sampling strategy of active learning, the samples are selected and labeled by experts, which are the most conducive to model training from the unlabeled set. The labeled set is updated by adding the new labeled samples. The same process is performed until the requirements are met. In this paper, experiments are carried out on the digits dataset and the Cifar10 set. The results show that the image classification method proposed in this paper can effectively enhance the accuracy of image classification.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3422713.3422726',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Inventory Management of Automobile After-sales Parts Based on Data Mining',\n",
       "  'authors': \"['Qun Liu', 'Kehua Miao', 'Kaihong Lin']\",\n",
       "  'date': 'June 2019',\n",
       "  'source': \"HPCCT '19: Proceedings of the 2019 3rd High Performance Computing and Cluster Technologies Conference\",\n",
       "  'abstract': \"The inventory management of automotive aftermarket parts is of great significance to the after-sales activities of automobile dealers and the reduction of operating costs. In view of the problem of insufficient utilization of automobile after-sales service data, it is necessary to introduce data mining methods to further analyze and mine data. Taking the historical sales data of auto parts as the mining object, K-means clustering algorithm and LSTM recurrent neural network were applied, and the Python tool was used to develop the automobile after-sales parts classification model and the parts inventory prediction model. The classification results can be used to analyze whether the dealer's inventory structure is reasonable. The forecast results can predict the demand for parts in the next stage. Comprehensive classification and prediction results, the study provides reference for the auto dealer to determine the variety structure and quantity structure of the auto parts.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3341069.3342975',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Automatic Classification of Glaciers from Sentinel-2 Imagery Using A Novel Deep Learning Model',\n",
       "  'authors': \"['Shuai Yan', 'Linlin Xu', 'Rui Wu']\",\n",
       "  'date': 'November 2019',\n",
       "  'source': \"ICAIP '19: Proceedings of the 2019 3rd International Conference on Advances in Image Processing\",\n",
       "  'abstract': 'The Sentinel-2 imagery provides accessible multispectral imagery, allowing better operation monitoring of glacier for climate change research, sea level rise and human life. Nevertheless, automatic glacial classification from Sentinel-2 is a challenging due to factors such as complex environment, different resolution bands and noisy or correlation in the spectral or spatial domain. In this paper, we propose an automatic glacier discrimination approach named MSSUnet to address several key research issues. First, a spatial-spectral module is used to adaptively learning the feature from different spectral band and neighboring pixels, which can better learn spatial-spectral features and reduce the impact of noise. Second, a band fusion method is applied to achieve fusion of different resolution bands in Sentinel-2 and reduce the interference of additional information. Furthermore, the proposed MSSUNet is compared with several existing neural networks on Sentinel-2 imagery to justify the advantage and improvement of the proposed approach. Experimental results show the improved performance of our proposed network over the other approaches.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3373419.3373460',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Making Machine-Learning Applications for Time-Series Sensor Data Graphical and Interactive',\n",
       "  'authors': \"['Seungjun Kim', 'Dan Tasse', 'Anind K. Dey']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Interactive Intelligent Systems',\n",
       "  'abstract': 'The recent profusion of sensors has given consumers and researchers the ability to collect significant amounts of data. However, understanding sensor data can be a challenge, because it is voluminous, multi-sourced, and unintelligible. Nonetheless, intelligent systems, such as activity recognition, require pattern analysis of sensor data streams to produce compelling results; machine learning (ML) applications enable this type of analysis. However, the number of ML experts able to proficiently classify sensor data is limited, and there remains a lack of interactive, usable tools to help intermediate users perform this type of analysis. To learn which features these tools must support, we conducted interviews with intermediate users of ML and conducted two probe-based studies with a prototype ML and visual analytics system, Gimlets. Our system implements ML applications for sensor-based time-series data as a novel domain-specific prototype that integrates interactive visual analytic features into the ML pipeline. We identify future directions for usable ML systems based on sensor data that will enable intermediate users to build systems that have been prohibitively difficult.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/2983924',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Toward an Effective Analysis of COVID-19 Moroccan Business Survey Data using Machine Learning Techniques',\n",
       "  'authors': \"['Imane Lasri', 'Anouar RiadSolh', 'Mourad El Belkacemi']\",\n",
       "  'date': 'February 2021',\n",
       "  'source': \"ICMLC '21: Proceedings of the 2021 13th International Conference on Machine Learning and Computing\",\n",
       "  'abstract': 'COVID-19 pandemic has gravely affected our societies and economies with severe consequences. To contain the spread of the disease, most governments around the world authorized unprecedented measures, including Morocco, which has closed the borders and adopted full lockdown between March and June 2020. However, these measures have resulted in economic loss and have led to dramatic changes in how businesses act and consumers behave. The main focus of this study was to examine the impact of the full lockdown on Moroccan enterprises based on the COVID-19 Moroccan business survey carried out by the High Commission for Planning (HCP). A three-stage analysis method was employed. First, multiple correspondence analysis (MCA) was used to reduce the dimensionality of the categorical variables, and k-means clustering algorithm was used to cluster the data, then decision tree algorithm was performed in order to interpret each cluster and the maximum accuracy achieved is 84.45%. Compared with the decision tree algorithm, an artificial neural network (ANN) with stratified 10-fold cross-validation was applied to the dataset and has reached an accuracy of 83.4%. The simulation results confirm the effectiveness of the proposed techniques for analyzing survey data.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3457682.3457690',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Transportation mode detection using machine learning techniques on mobile phone sensor data',\n",
       "  'authors': \"['Ifigenia Drosouli', 'Athanasios Voulodimos', 'Georgios Miaoulis']\",\n",
       "  'date': 'June 2020',\n",
       "  'source': \"PETRA '20: Proceedings of the 13th ACM International Conference on PErvasive Technologies Related to Assistive Environments\",\n",
       "  'abstract': \"The everyday use of means of transportation by millions of people combined with the continuous spreading of smartphones which are now equipped with various sensors, imply the existence of abundance of real-world transportation-related data and make Transportation Mode Detection (TMD) an interesting research field, essential to urban transportation planning, development of context-aware applications and physical and mental health improvement. The main objective of this work is to develop a machine learning methodology for classifying eight different transportation modes, including: still, walk, run, bike, car, bus, train, and subway, using data from smartphones sensors. To this end, publicly available datasets were used. For example, a subset of the original SHL dataset, including data obtained from one participant's smartphone embedded sensors (accelerometer, magnetometer, gyroscope, pressure sensor), being recorded for 68 days. As classifiers, eight Machine Learning algorithms were employed. The classifiers were firstly developed without Dimensionality Reduction (DR) and then with a DR feature extraction algorithm (Principal Component Analysis - PCA) so as to explore the possibility of using lighter models and potentially improve performance. After dimensionality reduction, the algorithms that performed best, accomplished a very good classification result in all classes while training time was significantly reduced.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3389189.3397996',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'MICK: A Meta-Learning Framework for Few-shot Relation Classification with Small Training Data',\n",
       "  'authors': \"['Xiaoqing Geng', 'Xiwen Chen', 'Kenny Q. Zhu', 'Libin Shen', 'Yinggong Zhao']\",\n",
       "  'date': 'October 2020',\n",
       "  'source': \"CIKM '20: Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management\",\n",
       "  'abstract': 'Few-shot relation classification seeks to classify incoming query instances after meeting only few support instances. This ability is gained by training with large amount of in-domain annotated data. In this paper, we tackle an even harder problem by further limiting the amount of data available at training time. We propose a few-shot learning framework for relation classification, which is particularly powerful when the training data is very small. In this framework, models not only strive to classify query instances, but also seek underlying knowledge about the support instances to obtain better instance representations. The framework also includes a method for aggregating cross-domain knowledge into models by open-source task enrichment. Additionally, we construct a brand new dataset: the TinyRel-CM dataset, a few-shot relation classification dataset in health domain with purposely small training data and challenging relation classes. Experimental results demonstrate that our framework brings performance gains for most underlying classification models, outperforms the state-of-the-art results given small training data, and achieves competitive results with sufficiently large training data.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3340531.3411858',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': \"Comparison of different model's performances in task of document classification\",\n",
       "  'authors': \"['Kristijan Spirovski', 'Evgenija Stevanoska', 'Andrea Kulakov', 'Zaneta Popeska', 'Goran Velinov']\",\n",
       "  'date': 'June 2018',\n",
       "  'source': \"WIMS '18: Proceedings of the 8th International Conference on Web Intelligence, Mining and Semantics\",\n",
       "  'abstract': \"Although the number of additional resources in Macedonian which can be used for solving information retrieval problem (or general Natural Language Processing problem) is very limited, models exist which are general enough and do not need additional knowledge about the language. This paper presents a document classification model, that doesn't rely on any language specific additional resources. The model is trained and tested on a set of news articles extracted from Macedonian websites, and each document is labeled with a class representing one of the twelve category sections from which the documents were extracted. The goal of this paper is to test different methods for feature selection and choice of vocabulary. Furthermore, we choose a model which gives the best accuracy for document classification task and we make sensitivity analysis on its architecture in order to further improve its performance. Although similar research already exists, this paper aims to combine different experiments and test them on Macedonian language documents. The models used in this paper are Random Forest (RF), Support Vector Machines (SVM) and Neural Network (NN). The performed experiments showed that the best accuracy is achieved when each document is represented as tf-idf vector, the vocabulary contains equal number of representative words from each class, and simple Neural Network with 3 hidden layers is used as a model. The main conclusion is that a language independent model for solving document classification problem can be successfully build for Macedonian language, achieving around 80% accuracy on the test set.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3227609.3227668',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Mining points of interest via address embeddings: an unsupervised approach',\n",
       "  'authors': \"['Abhinav Ganesan', 'Anubhav Gupta', 'Jose Mathew']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"LocalRec '21: Proceedings of the 5th ACM SIGSPATIAL International Workshop on Location-based Recommendations, Geosocial Networks and Geoadvertising\",\n",
       "  'abstract': 'Digital maps are commonly used across the globe for exploring places that users are interested in, commonly referred to as points of interest (PoI). In online food delivery platforms, PoIs could represent any major private compounds where customers could order from such as hospitals, residential complexes, office complexes, educational institutes and hostels. In this work, we propose an end-to-end unsupervised system design for obtaining polygon representations of PoIs (PoI polygons) from address locations and address texts. We preprocess the address texts using locality names and generate embeddings for the address texts using a deep learning-based architecture, viz. RoBERTa, trained on our internal address dataset. The PoI candidates are identified by jointly clustering the anonymised customer phone GPS locations (obtained during address onboarding) and the embeddings of the address texts. The final list of PoI polygons is obtained from these PoI candidates using novel post-processing steps that involve density-based cluster refinement and graph-based technique for cluster merging. This algorithm identified 74.8 % more PoIs than those obtained using the Mummidi-Krumm baseline algorithm run on our internal dataset. We use area-based precision and recall metrics to evaluate the performance of the algorithm. The proposed algorithm achieves a median area precision of 98 %, a median recall of 8 %, and a median F-score of 0.15. In order to improve the recall of the algorithmic polygons, we post-process them using building footprint polygons from the OpenStreetMap (OSM) database. The post-processing algorithm involves reshaping the algorithmic polygon using intersecting polygons and closed private roads from the OSM database, and accounting for intersection with public roads on the OSM database. We achieve a median area recall of 70 %, a median area precision of 69 %, and a median F-score of 0.69 on these post-processed polygons. The ground truth polygons for the evaluation of the metrics were obtained using manual validation of the algorithmic polygons obtained from the Mummidi-Krumm baseline approach. These polygons are not used to train the proposed algorithm pipeline, and hence, the algorithm is unsupervised.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3486183.3491002',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Machine Learning for Makers: Interactive Sensor Data Classification Based on Augmented Code Examples',\n",
       "  'authors': \"['David A. Mellis', 'Ben Zhang', 'Audrey Leung', 'Björn Hartmann']\",\n",
       "  'date': 'June 2017',\n",
       "  'source': \"DIS '17: Proceedings of the 2017 Conference on Designing Interactive Systems\",\n",
       "  'abstract': \"Although many software libraries and hardware modules support reading data from sensors, makers of interactive systems often struggle to extract higher-level information from raw sensor data. Available general-purpose machine learning (ML) libraries remain difficult to use for non-experts. Prior research has sought to bridge this gap through domain-specific user interfaces for particular types of sensors or algorithms. Our ESP (Example-based Sensor Prediction) system introduces a more general approach in which interactive visualizations and control interfaces are dynamically generated from augmented code examples written by experts. ESP's augmented examples allow experts to write logic that guides makers through important steps such as sensor calibration, parameter tuning, and assessing signal quality and classification performance. Writing augmented examples requires additional effort. ESP leverages a fundamental dynamic of online communities: experts are often willing to invest such effort to teach and train novices. Thus support for particular sensing domains does not have to be hard-wired a priori by system authors, but can be provided later by its community of users. We illustrate ESP's flexibility by detailing pipelines for four distinct sensors and classification algorithms. We validated the usability and flexibility of our example-based approach through a one-day workshop with 11 participants.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3064663.3064735',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Mining drug-drug interactions for healthcare professionals',\n",
       "  'authors': \"['Lizzy Farrugia', 'Charlie Abela']\",\n",
       "  'date': 'January 2020',\n",
       "  'source': 'APPIS 2020: Proceedings of the 3rd International Conference on Applications of Intelligent Systems',\n",
       "  'abstract': 'The fourth leading cause of death in the US are Adverse Drug Reactions (ADRs)1 that can be brought about through Drug-Drug Interactions (DDIs). In this paper, we propose medicX, a system that can detect DDIs in biomedical texts by leveraging on different machine learning techniques. The main components within medicX are the Drug Named Entity Recognition (DNER) component and the DDI Identification component. The DNER component was evaluated using the CHEMDNER and the DDIExtraction 2013 (DDI2013) challenge corpora. On the other hand, the DDI Identification component was evaluated using the DDI2013 challenge corpus. The DNER component is implemented using an approach based on LSTM-CRF. This method achieves an F1-score of 84.89% when it is trained and evaluated on the DDI2013 corpus, which is 1.43% higher than the system that placed first in the DDI2013 challenge. On the other hand, the DDI Identification component is implemented using a two-stage rich feature-based linear-kernel SVM. This classifier achieves an F1-score of 66.18%, as compared to the SVM state-of-the-art DDI system that reported an F1-score of 71.79%.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3378184.3378196',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Emoji-Powered Representation Learning for Cross-Lingual Sentiment Classification',\n",
       "  'authors': \"['Zhenpeng Chen', 'Sheng Shen', 'Ziniu Hu', 'Xuan Lu', 'Qiaozhu Mei', 'Xuanzhe Liu']\",\n",
       "  'date': 'May 2019',\n",
       "  'source': \"WWW '19: The World Wide Web Conference\",\n",
       "  'abstract': 'Sentiment classification typically relies on a large amount of labeled data. In practice, the availability of labels is highly imbalanced among different languages, e.g., more English texts are labeled than texts in any other languages, which creates a considerable inequality in the quality of related information services received by users speaking different languages. To tackle this problem, cross-lingual sentiment classification approaches aim to transfer knowledge learned from one language that has abundant labeled examples (i.e., the source language, usually English) to another language with fewer labels (i.e., the target language). The source and the target languages are usually bridged through off-the-shelf machine translation tools. Through such a channel, cross-language sentiment patterns can be successfully learned from English and transferred into the target languages. This approach, however, often fails to capture sentiment knowledge specific to the target language, and thus compromises the accuracy of the downstream classification task. In this paper, we employ emojis, which are widely available in many languages, as a new channel to learn both the cross-language and the language-specific sentiment patterns. We propose a novel representation learning method that uses emoji prediction as an instrument to learn respective sentiment-aware representations for each language. The learned representations are then integrated to facilitate cross-lingual sentiment classification. The proposed method demonstrates state-of-the-art performance on benchmark datasets, which is sustained even when sentiment labels are scarce.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3308558.3313600',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Mining Classification Rules for HIV-1 Protease Cleavage Sites Using Simplified Swarm Optimization',\n",
       "  'authors': \"['Alice Yeh', 'Wei-Chang Yeh']\",\n",
       "  'date': 'October 2019',\n",
       "  'source': 'AIAM 2019: Proceedings of the 2019 International Conference on Artificial Intelligence and Advanced Manufacturing',\n",
       "  'abstract': 'HIV-1 protease is a crucial enzyme in the HIV life cycle and serves to cleave the polyprotein that is involved in the formation of mature viruses. Due to its sensitivity and important function in virion creation, predicting cleavage classification can aid in the development of HIV-1 protease inhibitors that can improve antiretroviral therapy. Currently available methods are less effective at maintaining high prediction accuracy and consistency when applied to data with class bias and can be simplified and optimized. A prediction method that focused solely on sequential data was proposed and used in this study. A simplified swarm optimization (SSO) algorithm was applied to classifying HIV-1 protease cleavage data, which consisted of octamers that would be cleaved between the fourth and fifth amino acid, and orthogonal array testing was incorporated to improve efficiency. The prediction accuracy was assessed by applying the SSO algorithm to datasets found in the UCI Machine Learning Repository. Our experimental results show that SSO is an effective predictor of HIV-1 protease cleavage, exhibiting prediction accuracy that compared favorably to existing methods for both data with little class bias and data that contains class bias. Additionally, our prediction accuracy results suggest that the use of physicochemical features, as opposed to solely sequential features, does not improve performance.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3358331.3358335',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': \"Sichuan Consumers' Mining of Agricultural Product Brand Value Based on Big Data\",\n",
       "  'authors': \"['Huaxue Zhuang', 'Shoudong Chen', 'Zheng Wang', 'Yinjiang Tu', 'Xiaoyun Xie', 'Liangqiang Li']\",\n",
       "  'date': 'August 2021',\n",
       "  'source': \"ICIMTECH 21: <italic toggle='yes'>Retracted on September 15, 2021</italic>The Sixth International Conference on Information Management and Technology\",\n",
       "  'abstract': 'NOTICE OF RETRACTION: While investigating potential publication-related misconduct in connection with the ICIMTech 2021 Conference Proceedings, serious concerns were raised that cast doubt on the integrity of the peer-review process and all papers published in the Proceedings of this Conference. The integrity of the entire Conference has been called into question. As a result, of its investigation, ACM has decided to retract the Entire Conference Proceedings and all related papers from the ACM Digital Library.None of the papers from this Proceeding should be cited in the literature because of the questionable integrity of the peer review process for this Conference.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3465631.3465641',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Learning Hierarchal Channel Attention for Fine-grained Visual Classification',\n",
       "  'authors': \"['Xiang Guan', 'Guoqing Wang', 'Xing Xu', 'Yi Bin']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"MM '21: Proceedings of the 29th ACM International Conference on Multimedia\",\n",
       "  'abstract': 'Learning delicate feature representation of object parts plays a critical role in fine-grained visual classification tasks. However, advanced deep convolutional neural networks trained for general visual classification tasks usually tend to focus on the coarse-grained information while ignoring the fine-grained one, which is of great significance for learning discriminative representation. In this work, we explore the great merit of multi-modal data in introducing semantic knowledge and sequential analysis techniques in learning hierarchical feature representation for generating discriminative fine-grained features. To this end, we propose a novel approach, termed Channel Cusum Attention ResNet (CCA-ResNet ), for multi-modal joint learning of fine-grained representation. Specifically, we use feature-level multi-modal alignment to connect image and text classification models for joint multi-modal training. Through joint training, image classification models trained with semantic level labels tend to focus on the most discriminative parts, which enhances the cognitive ability of the model. Then, we propose a Channel Cusum Attention (CCA ) mechanism to equip feature maps with hierarchical properties through unsupervised reconstruction of local and global features. The benefits brought by the CCA are in two folds: a) allowing fine-grained features from early layers to be preserved in the forward propagation of deep networks; b) leveraging the hierarchical properties to facilitate multi-modal feature alignment. We conduct extensive experiments to verify that our proposed model can achieve state-of-the-art performance on a series of fine-grained visual classification benchmarks.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3474085.3475184',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Predictive Modeling of HR Dynamics Using Machine Learning',\n",
       "  'authors': \"['Ilze Birzniece', 'Ilze Andersone', 'Agris Nikitenko', 'Liga Zvirbule']\",\n",
       "  'date': 'March 2022',\n",
       "  'source': \"ICMLT '22: Proceedings of the 2022 7th International Conference on Machine Learning Technologies\",\n",
       "  'abstract': 'Voluntary employee turnover is an essential threat to companies due to the loss of institutional expertise and costs associated with recruitment. In this paper, we continue to tackle the retention problem by developing a machine learning (ML) based solution and providing a prototype tool to predict potential turnover. Both unsupervised and supervised ML methods are applied to identify the appropriate technique. K-Means clustering algorithm with PCA did not show significant results, whereas CART decision tree algorithm reached 89 % accuracy on the test set and 84 % accuracy on the validation set. Having satisfactory classification results are only part of a successful solution for modelling human resource (HR) dynamics. An important yet often underestimated aspect is representation according to end-user needs, which rarely are ML experts. To mitigate the risks of misinterpretation of classification results and take full advantage of decision support, we emphasize engineering the output of classification results for HR employees. We validated the classification system on data sets containing records of more than 2000 employees working in technology companies in Latvia from the year 2014 up to date.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3529399.3529403',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Phenotype Prediction from Metagenomic Data Using Clustering and Assembly with Multiple Instance Learning (CAMIL)',\n",
       "  'authors': \"['Mohammad Arifur Rahman', 'Nathan LaPierre', 'Huzefa Rangwala']\",\n",
       "  'date': 'None',\n",
       "  'source': 'IEEE/ACM Transactions on Computational Biology and Bioinformatics',\n",
       "  'abstract': 'The recent advent of Metagenome Wide Association Studies (MGWAS) provides insight into the role of microbes on human health and disease. However, the studies present several computational challenges. In this paper, we demonstrate a novel, efficient, and effective Multiple Instance Learning (MIL) based computational pipeline to predict patient phenotype from metagenomic data. MIL methods have the advantage that besides predicting the clinical phenotype, we can infer the instance level label or role of microbial sequence reads in the specific disease. Specifically, we use a Bag of Words method, which has been shown to be one of the most effective and efficient MIL methods. This involves assembly of the metagenomic sequence data, clustering of the assembled contigs, extracting features from the contigs, and using an SVM classifier to predict patient labels and identify the most relevant sequence clusters. With the exception of the given labels for the patients, this entire process is de novo (unsupervised). We call our pipeline &#x201C;CAMIL&#x201D;, which stands for Clustering and Assembly with Multiple Instance Learning. We use multiple state-of-the-art clustering methods for feature extraction, evaluation, and comparison of the performance of our proposed approach for each of these clustering methods. We also present a fast and scalable pre-clustering algorithm as a preprocessing step for our proposed pipeline. Our approach achieves efficiency by partitioning the large number of sequence reads into groups (called canopies) using locality sensitive hashing (LSH). These canopies are then refined by using state-of-the-art sequence clustering algorithms. We use data from a well-known MGWAS study of patients with Type-2 Diabetes and show that our pipeline significantly outperforms the classifier used in that paper, as well as other common MIL methods.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/TCBB.2017.2758782',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Comparison of Manifold Learning and Deep Learning on Target Classification',\n",
       "  'authors': \"['Huang Cheng', 'Wang Hongmei']\",\n",
       "  'date': 'September 2017',\n",
       "  'source': \"ICCBDC '17: Proceedings of the 2017 International Conference on Cloud and Big Data Computing\",\n",
       "  'abstract': \"With the development of artificial intelligence, classification tasks become more and more popular, but the amount of data is growing dramatically. There are mainly two ways to deal with this problem, one is to reduce the data dimensions directly, the other one is to take advantage of all data through deep learning. In this paper, we will compare these two data processing methods. The first way is to reduce the extracted features' dimensions through manifold learning and then feed into classifiers, and the other way is to deal it directly with deep learning. The experimental results show that deep learning has a better ability than manifold learning in the classification task.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3141128.3141137',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A novel support vector machine algorithm for missing data',\n",
       "  'authors': \"['Mengeheng Zhu', 'Hong Shi']\",\n",
       "  'date': 'March 2018',\n",
       "  'source': \"ICIAI '18: Proceedings of the 2nd International Conference on Innovation in Artificial Intelligence\",\n",
       "  'abstract': 'Missing data problem often occurs in data analysis. The most common way to solve this problem is imputation. But imputation methods are only suitable for dealing with a low proportion of missing data, when assuming that missing data satisfies MCAR (Missing Completely at Random) or MAR (Missing at Random). In this paper, considering the reasons for missing data, we propose a novel support vector machine method using a new kernel function to solve the problem with a relatively large proportion of missing data. This method makes full use of observed data to reduce the error caused by filling a large number of missing values. We validate our method on 4 data sets from UCI Repository of Machine Learning. The accuracy, F-score, Kappa statistics and recall are used to evaluate the performance. Experimental results show that our method achieve significant improvement in terms of classification results compared with common imputation methods, even when the proportion of missing data is high.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3194206.3194214',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Fast One-class Classification using Class Boundary-preserving Random Projections',\n",
       "  'authors': \"['Arindam Bhattacharya', 'Sumanth Varambally', 'Amitabha Bagchi', 'Srikanta Bedathur']\",\n",
       "  'date': 'August 2021',\n",
       "  'source': \"KDD '21: Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining\",\n",
       "  'abstract': 'Several applications, like malicious URL detection and web spam detection, require classification on very high-dimensional data. In such cases anomalous data is hard to find but normal data is easily available. As such it is increasingly common to use a one-class classifier (OCC). Unfortunately, most OCC algorithms cannot scale to datasets with extremely high dimensions. In this paper, we present Fast Random projection-based One-Class Classification (FROCC), an extremely efficient, scalable and easily parallelizable method for one-class classification with provable theoretical guarantees. Our method is based on the simple idea of transforming the training data by projecting it onto a set of random unit vectors that are chosen uniformly and independently from the unit sphere, and bounding the regions based on separation of the data. FROCC can be naturally extended with kernels. We provide a new theoretical framework to prove that that FROCC generalizes well in the sense that it is stable and has low bias for some parameter settings. We then develop a fast scalable approximation of FROCC using vectorization, exploiting data sparsity and parallelism to develop a new implementation called ParDFROCC. ParDFROCC achieves up to 2 percent points better ROC than the next best baseline, with up to 12× speedup in training and test times over a range of state-of-the-art benchmarks for the OCC task.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3447548.3467440',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Protocol Keywords Extraction Method Based on Frequent Item-Sets Mining',\n",
       "  'authors': \"['Gaochao Li', 'Qiang Qian', 'Zhonghua Wang', 'Xin Zou', 'Xunxun Chen', 'Xiao Wu']\",\n",
       "  'date': 'April 2018',\n",
       "  'source': \"ICISS '18: Proceedings of the 1st International Conference on Information Science and Systems\",\n",
       "  'abstract': 'Network application identification technology is widely used in the fields of network management, network optimization and intrusion detection and so on. And among the methods, the DPI (Deep Packet Inspection) is the most popular one with high accuracy relaying on a small amount of payload data. However, DPI depends on the effective protocol keywords. In order to cope with the speed of the applications updating, we proposed a protocol keywords extraction method for unencrypted network applications based on frequent itemsets mining. It contains two major steps: Firstly, we generate candidate words by using unsupervised methods and reduce the word set size with rules of words length and position. Then, we extract effective protocol keywords with frequent item-sets mining method and remove the noise words and redundant words by evaluating the candidate word co-occurrence relationship. The experiment result shows that our method shrinks the size of the keywords set and is better at extracting the real protocol keywords compared with Proword.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3209914.3209937',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A Cluster Switching Method for Sampling Imbalanced Data',\n",
       "  'authors': \"['Wanthanee Prachuabsupakij', 'Supaporn Simcharoen']\",\n",
       "  'date': 'March 2018',\n",
       "  'source': \"ISMSI '18: Proceedings of the 2nd International Conference on Intelligent Systems, Metaheuristics &amp; Swarm Intelligence\",\n",
       "  'abstract': 'Classification on imbalanced data is one of the most interesting in data mining challenge. In this paper, a new repetitive sampling method, namely ClusIM is proposed to improve the prediction performance on imbalanced dataset using Clustering Switching Method based on K-means algorithm in order to generate new subset in attempting to reduce the overlapping between the minority class instances and majority class instances in each subset. Then, SMOTE algorithm is used to operate on each subset according to imbalance ratio of the subset. It generates the synthetic instances of the minority class. The ClusIM will generate two-balanced final training set, which are classified using SVM and to combine the model through maximum probability vote. Our experiments are based on six imbalanced data sets from UCI and one real-world dataset, comparing ClusIM with four well-known classification algorithms (SVM, Bagging, AdaboostM1, and AdaCost). Amongst the compared algorithms, ClusIM has higher F-measure and G-mean results than the other methods. This study supported that ClusIM is capable of improving the performance of learning algorithm on imbalanced dataset.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3206185.3206192',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Swarm Intelligence Approaches for Parameter Setting of Deep Learning Neural Network: Case Study on Phishing Websites Classification',\n",
       "  'authors': \"['Grega Vrbančič', 'Iztok Fister', 'Vili Podgorelec']\",\n",
       "  'date': 'June 2018',\n",
       "  'source': \"WIMS '18: Proceedings of the 8th International Conference on Web Intelligence, Mining and Semantics\",\n",
       "  'abstract': 'In last decades, the web and online services have revolutionized the modern world. However, by increasing our dependence on online services, as a result, online security threats are also increasing rapidly. One of the most common online security threats is a so-called Phishing attack, the purpose of which is to mimic a legitimate website such as online banking, e-commerce or social networking website in order to obtain sensitive data such as user-names, passwords, financial and health-related information from potential victims. The problem of detecting phishing websites has been addressed many times using various methodologies from conventional classifiers to more complex hybrid methods. Recent advancements in deep learning approaches suggested that the classification of phishing websites using deep learning neural networks should outperform the traditional machine learning algorithms. However, the results of utilizing deep neural networks heavily depend on the setting of different learning parameters. In this paper, we propose a swarm intelligence based approach to parameter setting of deep learning neural network. By applying the proposed approach to the classification of phishing websites, we were able to improve their detection when compared to existing algorithms.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3227609.3227655',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Mining mobile app markets for prioritization of security assessment effort',\n",
       "  'authors': \"['Alireza Sadeghi', 'Naeem Esfahani', 'Sam Malek']\",\n",
       "  'date': 'September 2017',\n",
       "  'source': 'WAMA 2017: Proceedings of the 2nd ACM SIGSOFT International Workshop on App Market Analytics',\n",
       "  'abstract': 'Like any other software engineering activity, assessing the security of a software system entails prioritizing the resources and minimizing the risks. Techniques ranging from the manual inspection to automated static and dynamic analyses are commonly employed to identify security vulnerabilities prior to the release of the software. However, none of these techniques is perfect, as static analysis is prone to producing lots of false positives and negatives, while dynamic analysis and manual inspection are unwieldy, both in terms of required time and cost. This research aims to improve these techniques by mining relevant information from vulnerabilities found in the app markets. The approach relies on the fact that many modern software systems, in particular mobile software, are developed using rich application development frameworks (ADF), allowing us to raise the level of abstraction for detecting vulnerabilities and thereby making it possible to classify the types of vulnerabilities that are encountered in a given category of application. By coupling this type of information with severity of the vulnerabilities, we are able to improve the efficiency of static and dynamic analyses, and target the manual effort on the riskiest vulnerabilities.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3121264.3121265',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': \"How do visual explanations foster end users' appropriate trust in machine learning?\",\n",
       "  'authors': \"['Fumeng Yang', 'Zhuanyi Huang', 'Jean Scholtz', 'Dustin L. Arendt']\",\n",
       "  'date': 'March 2020',\n",
       "  'source': \"IUI '20: Proceedings of the 25th International Conference on Intelligent User Interfaces\",\n",
       "  'abstract': \"We investigated the effects of example-based explanations for a machine learning classifier on end users' appropriate trust. We explored the effects of spatial layout and visual representation in an in-person user study with 33 participants. We measured participants' appropriate trust in the classifier, quantified the effects of different spatial layouts and visual representations, and observed changes in users' trust over time. The results show that each explanation improved users' trust in the classifier, and the combination of explanation, human, and classification algorithm yielded much better decisions than the human and classification algorithm separately. Yet these visual explanations lead to different levels of trust and may cause inappropriate trust if an explanation is difficult to understand. Visual representation and performance feedback strongly affect users' trust, and spatial layout shows a moderate effect. Our results do not support that individual differences (e.g., propensity to trust) affect users' trust in the classifier. This work advances the state-of-the-art in trust-able machine learning and informs the design and appropriate use of automated systems.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3377325.3377480',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Customer Segmentation With Machine Learning: New Strategy For Targeted Actions',\n",
       "  'authors': \"['Lahcen Abidar', 'Dounia Zaidouni', 'Abdeslam Ennouaary']\",\n",
       "  'date': 'September 2020',\n",
       "  'source': \"SITA'20: Proceedings of the 13th International Conference on Intelligent Systems: Theories and Applications\",\n",
       "  'abstract': 'Customers Segmentation has been a topic of interest for a lot of industry, academics, and marketing leaders. The potential value of a customer to a company can be a core ingredient in decision-making. One of the big challenges in customer-based organizations is customer cognition, understanding the difference between them, and scoring them. But now with all capabilities we have, using new technologies like machine learning algorithm and data treatment we can create a very powerful framework that allow us to best understand customers needs and behaviors, and act appropriately to satisfy their needs. In the present paper, we propose a new model based on RFM model Recency, Frequency, and Monetary and k-mean algorithm to resolve those challenges. This model will allow us to use clustering, scoring, and distribution to have a clear idea about what action we should take to improve customer satisfaction.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3419604.3419794',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Using Data Mining Methods to Detect Medical Fraud',\n",
       "  'authors': \"['Long-Sheng Chen', 'Jia-Chuan Chen']\",\n",
       "  'date': 'July 2020',\n",
       "  'source': \"ICMECG '20: Proceedings of the 7th International Conference on Management of e-Commerce and e-Government\",\n",
       "  'abstract': 'Medical fraudulent activities have made medical insurance expenditures rise year by year. This not only increases the burden on the medical and financial system, but also makes it difficult for many people in need to obtain these resources. Therefore, how to solve this problem has become one of critical issues. Therefore, this study aims to establish a predictive model of medical insurance fraud through data mining methods, and attempts to discover important factors affecting fraud. In this work, we will use Decision Tree (DT), Support Vector Machines (SVM), and Back Propagation Neural Networks (BPN) to establish classification models. A comparison of these three methods will be done. And, we will use decision trees to extract important factors that could provide important information for effectively detect medical fraud. Hopefully, we can effectively reduce the negative impact of medical insurance fraud.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3409891.3409902',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Faster Secure Data Mining via Distributed Homomorphic Encryption',\n",
       "  'authors': \"['Junyi Li', 'Heng Huang']\",\n",
       "  'date': 'August 2020',\n",
       "  'source': \"KDD '20: Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining\",\n",
       "  'abstract': 'Due to the rising privacy demand in data mining, Homomorphic Encryption (HE) is receiving more and more attention recently for its capability to do computations over the encrypted field. By using the HE technique, it is possible to securely outsource model learning to the not fully trustful but powerful public cloud computing environments. However, HE-based training scales badly because of the high computation complexity. It is still an open problem whether it is possible to apply HE to large-scale problems. In this paper, we propose a novel general distributed HE-based data mining framework towards one step of solving the scaling problem. The main idea of our approach is to use the slightly more communication overhead in exchange of shallower computational circuit in HE, so as to reduce the overall complexity. We verify the efficiency and effectiveness of our new framework by testing over various data mining algorithms and benchmark data-sets. For example, we successfully train a logistic regression model to recognize the digit 3 and 8 within around 5 minutes, while a centralized counterpart needs almost 2 hours.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3394486.3403321',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A Multi-task Learning Approach Based on Convolutional Neural Network for Acoustic Scene Classification',\n",
       "  'authors': \"['Kuilong Xu', 'Shilei Huang', 'Gang Cheng', 'Xiao Song']\",\n",
       "  'date': 'December 2019',\n",
       "  'source': \"ACAI '19: Proceedings of the 2019 2nd International Conference on Algorithms, Computing and Artificial Intelligence\",\n",
       "  'abstract': 'Acoustic Scene Classification (ASC) aim to recognize an acoustic scene in audio signal records. The acoustic scene is a mixture of background sounds and various sound events, and sound events often determine the type of acoustic scene. However, in many research methods for acoustic scene classification, only a few people have noticed the important information of sound events. In this paper, we combine the ASC task and Sound Event Detection (SED) task, and propose a new CNN approach with multi-task Learning (MTL), which uses SED as an auxiliary task to pay more attention to the information of the sound event in the model. Besides, in view of the characteristic of the sound event with high-energy time-frequency components, we use Global Max Pooling (GMP) instead of the Fully Connected layer (FC) in the traditional CNN. The advantage is that the model focused on distinct high-energy time-frequency components of audio signals (sound event). Finally, extensive experiments are carried out on the TUT acoustic scene 2017 dataset. Our proposed CNN approach with MTL shows better generalization, and improves the Unweighted Average Recall (UAR) of 5.2% over the DCASE 2017 ASC baseline system.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3377713.3377720',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Query-Based Machine Learning Model for Data Analysis of Infrasonic Signals in Wireless Sensor Networks',\n",
       "  'authors': \"['Ray-I Chang', 'Chien-Chang Huang', 'Liang-Bin Lai', 'Chia-Yun Lee']\",\n",
       "  'date': 'February 2018',\n",
       "  'source': \"ICDSP '18: Proceedings of the 2nd International Conference on Digital Signal Processing\",\n",
       "  'abstract': 'As infrasonic signals can through objects and propagate at a long distance, infrasound sensors are widely applied in wireless sensor networks to monitor environment events of a large area. The signal conditions are usually complex and have various characteristics while monitoring the large area. Different features in both time and frequency domains should be extracted and considered. Big data increases the computation complexity, and the wrong selection of features may decreases the accuracy in event prediction. To overcome this problem, a query-based-learning method is applied to select the proper features for smart edge computing in machine learning. Experimental results show that the proposed method provides good performance when comparing with previous feature selection methods.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3193025.3193031',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Prediction of Dengue Disease through Data Mining by using Modified Apriori Algorithm',\n",
       "  'authors': \"['Iqra Jahangir', 'Abdul-Basit', 'Abdul Hannan', 'Sameen Javed']\",\n",
       "  'date': 'July 2018',\n",
       "  'source': \"ICCES'18: Proceedings of the 4th ACM International Conference of Computing for Engineering and Sciences\",\n",
       "  'abstract': 'Dengue is a threatening ailment and it is caused by the bite of a female mosquito. Pakistan has been a victim of this disease from a couple of years. Life of many people is endangered from it. People were facing a problem for the detection of this disease because of the limited resources of time and money. Lives of the people can be saved if the dengue is predicted in early stages. The present research will present a dengue prediction methodology by using the data mining technique. Association rule mining is used in this paper for the prediction of this disease. First, the data was collected which includes patients having dengue or not, then it was used in Weka data mining tool by applying Apriori algorithm. The rules derived from this algorithm were further used for generating a proposed rule which is used for the prediction of dengue in patients. Generated rule was able to identify 75% of records correctly.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3213187.3287612',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Stable classification',\n",
       "  'authors': \"['Dimitris Bertsimas', 'Jack Dunn', 'Ivan Paskov']\",\n",
       "  'date': 'None',\n",
       "  'source': 'The Journal of Machine Learning Research',\n",
       "  'abstract': 'We address the problem of instability of classification models: small changes in the training data leading to large changes in the resulting model and predictions. This phenomenon is especially well established for single tree based methods such as CART, however it is present in all classification methods. We apply robust optimization to improve the stability of four of the most commonly used classification methods: Random Forests, Logistic Regression, Support Vector Machines, and Optimal Classification Trees. Through experiments on 30 data sets with sizes ranging between 102 and 104 observations and features, we show that our approach (a) leads to improvements in stability, and in some cases accuracy, compared to the original methods, with the gains in stability being particularly significant (even, surprisingly, for those methods that were previously thought to be stable, such as Random Forests) and (b) has computational times comparable with (and indeed in some cases even faster than) the original methods allowing the method to be very scalable.',\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3586589.3586885',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Gaussian Processes for Rumour Stance Classification in Social Media',\n",
       "  'authors': \"['Michal Lukasik', 'Kalina Bontcheva', 'Trevor Cohn', 'Arkaitz Zubiaga', 'Maria Liakata', 'Rob Procter']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Information Systems',\n",
       "  'abstract': 'Social media tend to be rife with rumours while new reports are released piecemeal during breaking news. Interestingly, one can mine multiple reactions expressed by social media users in those situations, exploring their stance towards rumours, ultimately enabling the flagging of highly disputed rumours as being potentially false. In this work, we set out to develop an automated, supervised classifier that uses multi-task learning to classify the stance expressed in each individual tweet in a conversation around a rumour as either supporting, denying or questioning the rumour. Using a Gaussian Process classifier, and exploring its effectiveness on two datasets with very different characteristics and varying distributions of stances, we show that our approach consistently outperforms competitive baseline classifiers. Our classifier is especially effective in estimating the distribution of different types of stance associated with a given rumour, which we set forth as a desired characteristic for a rumour-tracking system that will show both ordinary users of Twitter and professional news practitioners how others orient to the disputed veracity of a rumour, with the final aim of establishing its actual truth value.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3295823',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Toward an Effective Analysis of COVID-19 Moroccan Business Survey Data using Machine Learning Techniques',\n",
       "  'authors': \"['Imane Lasri', 'Anouar RiadSolh', 'Mourad El Belkacemi']\",\n",
       "  'date': 'February 2021',\n",
       "  'source': \"ICMLC '21: Proceedings of the 2021 13th International Conference on Machine Learning and Computing\",\n",
       "  'abstract': 'COVID-19 pandemic has gravely affected our societies and economies with severe consequences. To contain the spread of the disease, most governments around the world authorized unprecedented measures, including Morocco, which has closed the borders and adopted full lockdown between March and June 2020. However, these measures have resulted in economic loss and have led to dramatic changes in how businesses act and consumers behave. The main focus of this study was to examine the impact of the full lockdown on Moroccan enterprises based on the COVID-19 Moroccan business survey carried out by the High Commission for Planning (HCP). A three-stage analysis method was employed. First, multiple correspondence analysis (MCA) was used to reduce the dimensionality of the categorical variables, and k-means clustering algorithm was used to cluster the data, then decision tree algorithm was performed in order to interpret each cluster and the maximum accuracy achieved is 84.45%. Compared with the decision tree algorithm, an artificial neural network (ANN) with stratified 10-fold cross-validation was applied to the dataset and has reached an accuracy of 83.4%. The simulation results confirm the effectiveness of the proposed techniques for analyzing survey data.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3457682.3457690',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Inferring Dynamic User Interests in Streams of Short Texts for User Clustering',\n",
       "  'authors': \"['Shangsong Liang', 'Zhaochun Ren', 'Yukun Zhao', 'Jun Ma', 'Emine Yilmaz', 'Maarten De Rijke']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Information Systems',\n",
       "  'abstract': 'User clustering has been studied from different angles. In order to identify shared interests, behavior-based methods consider similar browsing or search patterns of users, whereas content-based methods use information from the contents of the documents visited by the users. So far, content-based user clustering has mostly focused on static sets of relatively long documents. Given the dynamic nature of social media, there is a need to dynamically cluster users in the context of streams of short texts. User clustering in this setting is more challenging than in the case of long documents, as it is difficult to capture the users’ dynamic topic distributions in sparse data settings. To address this problem, we propose a dynamic user clustering topic model (UCT). UCT adaptively tracks changes of each user’s time-varying topic distributions based both on the short texts the user posts during a given time period and on previously estimated distributions. To infer changes, we propose a Gibbs sampling algorithm where a set of word pairs from each user is constructed for sampling. UCT can be used in two ways: (1) as a short-term dependency model that infers a user’s current topic distribution based on the user’s topic distributions during the previous time period only, and (2) as a long-term dependency model that infers a user’s current topic distributions based on the user’s topic distributions during multiple time periods in the past. The clustering results are explainable and human-understandable, in contrast to many other clustering algorithms. For evaluation purposes, we work with a dataset consisting of users and tweets from each user. Experimental results demonstrate the effectiveness of our proposed short-term and long-term dependency user clustering models compared to state-of-the-art baselines.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3072606',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A New Contrast Pattern-Based Classification for Imbalanced Data',\n",
       "  'authors': \"['Xiangtao Chen', 'Yajing Gao', 'Siqi Ren']\",\n",
       "  'date': 'September 2018',\n",
       "  'source': \"ISCSIC '18: Proceedings of the 2nd International Symposium on Computer Science and Intelligent Control\",\n",
       "  'abstract': 'Contrast pattern-based classifiers become more understandable and accurate on binary classification. However, these classifiers do not achieve good performance on class imbalance problems. Thus, this paper introduces a new contrast pattern-based classifier for class imbalance problems. The proposed method selects the appropriate contrast patterns by quality measures. Then we combine the quality measure of the pattern and class confidence proportion with the class imbalance level at the classification stage of the model. The simulation results show that our proposed outperforms the current contrast pattern-based classifiers and other state-of-the-art classifiers not directly based on contrast patterns for class imbalance problems.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3284557.3284708',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': \"How do visual explanations foster end users' appropriate trust in machine learning?\",\n",
       "  'authors': \"['Fumeng Yang', 'Zhuanyi Huang', 'Jean Scholtz', 'Dustin L. Arendt']\",\n",
       "  'date': 'March 2020',\n",
       "  'source': \"IUI '20: Proceedings of the 25th International Conference on Intelligent User Interfaces\",\n",
       "  'abstract': \"We investigated the effects of example-based explanations for a machine learning classifier on end users' appropriate trust. We explored the effects of spatial layout and visual representation in an in-person user study with 33 participants. We measured participants' appropriate trust in the classifier, quantified the effects of different spatial layouts and visual representations, and observed changes in users' trust over time. The results show that each explanation improved users' trust in the classifier, and the combination of explanation, human, and classification algorithm yielded much better decisions than the human and classification algorithm separately. Yet these visual explanations lead to different levels of trust and may cause inappropriate trust if an explanation is difficult to understand. Visual representation and performance feedback strongly affect users' trust, and spatial layout shows a moderate effect. Our results do not support that individual differences (e.g., propensity to trust) affect users' trust in the classifier. This work advances the state-of-the-art in trust-able machine learning and informs the design and appropriate use of automated systems.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3377325.3377480',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A Data Mining Framework for Valuing Large Portfolios of Variable Annuities',\n",
       "  'authors': \"['Guojun Gan', 'Jimmy Xiangji Huang']\",\n",
       "  'date': 'August 2017',\n",
       "  'source': \"KDD '17: Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining\",\n",
       "  'abstract': 'A variable annuity is a tax-deferred retirement vehicle created to address concerns that many people have about outliving their assets. In the past decade, the rapid growth of variable annuities has posed great challenges to insurance companies especially when it comes to valuing the complex guarantees embedded in these products. In this paper, we propose a novel data mining framework to address the computational issue associated with the valuation of large portfolios of variable annuity contracts. The data mining framework consists of two major components: a data clustering algorithm which is used to select representative variable annuity contracts, and a regression model which is used to predict quantities of interest for the whole portfolio based on the representative contracts. A series of numerical experiments are conducted on a portfolio of synthetic variable annuity contracts to demonstrate the performance of our proposed data mining framework in terms of accuracy and speed. The experimental results show that our proposed framework is able to produce accurate estimates of various quantities of interest and can reduce the runtime significantly.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3097983.3098013',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Classification of Acute Stress-Induced Response Patterns',\n",
       "  'authors': \"['Luca Abel', 'Robert Richer', 'Arne Küderle', 'Stefan Gradl', 'Bjoern M. Eskofier', 'Nicolas Rohleder']\",\n",
       "  'date': 'May 2019',\n",
       "  'source': \"PervasiveHealth'19: Proceedings of the 13th EAI International Conference on Pervasive Computing Technologies for Healthcare\",\n",
       "  'abstract': 'Modern machine learning techniques enable new possibilities for the analysis of psychological data. In the field of health psychology, it is of interest to explore the biological processes triggered by acute stress. This work introduces a method to automatically classify individuals into distinct stress responder groups based on these biological processes. Two important stress-sensitive markers were used: Salivary cortisol and Interleukin-6 (IL-6) in blood plasma. Controlled stress was induced using the Trier Social Stress Test on two consecutive days. Results show that Support Vector Machines performed best on the given dataset. We distinguished four different cortisol and three different IL-6 responder types with high mean accuracies (92.2 % ± 9.7 % and 91.2 % ± 6.3 %, respectively). Classification results were mainly limited by class imbalances and high intra-class standard deviations. Whereas promising as a first application of machine learning on such datasets, generalizability and real-world applicability of our results need to be proven by further research.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3329189.3329231',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Cross-Domain Classification Model With Knowledge Utilization Maximization for Recognition of Epileptic EEG Signals',\n",
       "  'authors': \"['Kaijian Xia', 'TongGuang Ni', 'Hongsheng Yin', 'Bo Chen']\",\n",
       "  'date': 'None',\n",
       "  'source': 'IEEE/ACM Transactions on Computational Biology and Bioinformatics',\n",
       "  'abstract': 'Conventional classification models for epileptic EEG signal recognition need sufficient labeled samples as training dataset. In addition, when training and testing EEG signal samples are collected from different distributions, for example, due to differences in patient groups or acquisition devices, such methods generally cannot perform well. In this paper, a cross-domain classification model with knowledge utilization maximization called CDC-KUM is presented, which takes advantage of the data global structure provided by the labeled samples in the related domain and unlabeled samples in the current domain. Through mapping the data into kernel space, the pairwise constraint regularization term is combined together the predictive differences of the labeled data in the source domain. Meanwhile, the soft clustering regularization term using quadratic weights and Gini-Simpson diversity is applied to exploit the distribution information of unlabeled data in the target domain. Experimental results show that CDC-KUM model outperformed several traditional non-transfer and transfer classification methods for recognition of epileptic EEG signals.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/TCBB.2020.2973978',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Detecting spam tweets using machine learning and effective preprocessing',\n",
       "  'authors': \"['Berk Kardaş', 'İsmail Erdem Bayar', 'Tansel Özyer', 'Reda Alhajj']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"ASONAM '21: Proceedings of the 2021 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining\",\n",
       "  'abstract': 'Nowadays, with the rapid increase in popularity of online social networks (OSNs), these platforms are realized as ideal places for spammers. Unfortunately, these spammers can easily publish malicious content, advertise phishing scams by taking advantage of OSNs. Therefore, effective identification and filtering of spam tweets will be beneficial to both OSNs and users. However, it is becoming increasingly difficult to check and eliminate spam tweets due to this great flow of posts. Motivated by these observations, in this paper we propose an approach for the detection of spam tweets using machine learning and effective preprocessing techniques. The approach proposes the advantages of the preprocessing and which of these preprocessing techniques are the most effective. To compare these techniques UtkML Twitter spam dataset is used in testing. After the most effective methods determined, the detection accuracy of the spam tweets will be better optimized by combining them. We have evaluated our solution with four different machine learning algorithms namely - Naïve Bayes Classifier, Neural Network, Logistic Regression and Support Vector Machine. With SVM Classifier, we are able to achieve an accuracy of 93.02%. Experimental results show that our approach can improve the performance of spam tweet classification effectively.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3487351.3490968',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Fruit-fly Inspired Neighborhood Encoding for Classification',\n",
       "  'authors': \"['Kaushik Sinha', 'Parikshit Ram']\",\n",
       "  'date': 'August 2021',\n",
       "  'source': \"KDD '21: Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining\",\n",
       "  'abstract': 'Inspired by the fruit-fly olfactory circuit, the Fly Bloom Filter is able to efficiently summarize the data with a single pass and has been used for novelty detection. We propose a new classifier that effectively encodes the different local neighborhoods for each class with a per-class Fly Bloom Filter. The inference on test data requires an efficient Flyhash[6] operation followed by a high-dimensional, but very sparse, dot product with the per-class Bloom Filters. On the theoretical side, we establish conditions under which the predictions of our proposed classifier agrees with the predictions of the nearest neighbor classifier. We extensively evaluate our proposed scheme with 71 data sets of varied data dimensionality to demonstrate that the predictive performance of our proposed neuroscience inspired classifier is competitive to the nearest-neighbor classifiers and other single-pass classifiers.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3447548.3467246',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Fast One-class Classification using Class Boundary-preserving Random Projections',\n",
       "  'authors': \"['Arindam Bhattacharya', 'Sumanth Varambally', 'Amitabha Bagchi', 'Srikanta Bedathur']\",\n",
       "  'date': 'August 2021',\n",
       "  'source': \"KDD '21: Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining\",\n",
       "  'abstract': 'Several applications, like malicious URL detection and web spam detection, require classification on very high-dimensional data. In such cases anomalous data is hard to find but normal data is easily available. As such it is increasingly common to use a one-class classifier (OCC). Unfortunately, most OCC algorithms cannot scale to datasets with extremely high dimensions. In this paper, we present Fast Random projection-based One-Class Classification (FROCC), an extremely efficient, scalable and easily parallelizable method for one-class classification with provable theoretical guarantees. Our method is based on the simple idea of transforming the training data by projecting it onto a set of random unit vectors that are chosen uniformly and independently from the unit sphere, and bounding the regions based on separation of the data. FROCC can be naturally extended with kernels. We provide a new theoretical framework to prove that that FROCC generalizes well in the sense that it is stable and has low bias for some parameter settings. We then develop a fast scalable approximation of FROCC using vectorization, exploiting data sparsity and parallelism to develop a new implementation called ParDFROCC. ParDFROCC achieves up to 2 percent points better ROC than the next best baseline, with up to 12× speedup in training and test times over a range of state-of-the-art benchmarks for the OCC task.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3447548.3467440',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'CurGraph: Curriculum Learning for Graph Classification',\n",
       "  'authors': \"['Yiwei Wang', 'Wei Wang', 'Yuxuan Liang', 'Yujun Cai', 'Bryan Hooi']\",\n",
       "  'date': 'April 2021',\n",
       "  'source': \"WWW '21: Proceedings of the Web Conference 2021\",\n",
       "  'abstract': 'Graph neural networks (GNNs) have achieved state-of-the-art performance on graph classification tasks. Existing work usually feeds graphs to GNNs in random order for training. However, graphs can vary greatly in their difficulty for classification, and we argue that GNNs can benefit from an easy-to-difficult curriculum, similar to the learning process of humans. Evaluating the difficulty of graphs is challenging due to the high irregularity of graph data. To address this issue, we present the CurGraph (Curriculum Learning for Graph Classification) framework, that analyzes the graph difficulty in the high-level semantic feature space. Specifically, we use the infomax method to obtain graph-level embeddings and a neural density estimator to model the embedding distributions. Then we calculate the difficulty scores of graphs based on the intra-class and inter-class distributions of their embeddings. Given the difficulty scores, CurGraph first exposes a GNN to easy graphs, before gradually moving on to hard ones. To provide a soft transition from easy to hard, we propose a smooth-step method, which utilizes a time-variant smooth function to filter out hard graphs. Thanks to CurGraph, a GNN learns from the graphs at the border of its capability, neither too easy or too hard, to gradually expand its border at each training step. Empirically, CurGraph yields significant gains for popular GNN models on graph classification and enables them to achieve superior performance on miscellaneous graphs.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3442381.3450025',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Spatio-Temporal Clustering of Traffic Data with Deep Embedded Clustering',\n",
       "  'authors': \"['Reza Asadi', 'Amelia Regan']\",\n",
       "  'date': 'November 2019',\n",
       "  'source': \"PredictGIS'19: Proceedings of the 3rd ACM SIGSPATIAL International Workshop on Prediction of Human Mobility\",\n",
       "  'abstract': 'Traffic data is a challenging spatio-temporal data, and a multivariate time series data with spatial similarities. Clustering of traffic data is a fundamental tool for various machine learning tasks including anomaly detection, missing data imputation and short term forecasting problems. In this paper, first, we formulate a spatio-temporal clustering problem and define temporal and spatial clusters. Then, we propose an approach for finding temporal and spatial clusters with a deep embedded clustering model. The proposed approach is examined on traffic flow data. In the analysis, we present the properties of clusters and patterns in the dataset. The analysis shows that the temporal and spatial clusters have meaningful relationships with temporal and spatial patterns in traffic data, and the clustering method effectively finds similarities in traffic data.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3356995.3364537',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A new self-organizing map based algorithm for multi-label stream classification',\n",
       "  'authors': \"['Ricardo Cerri', 'Joel David C. Junior', 'Elaine. R. Faria', 'João Gama']\",\n",
       "  'date': 'March 2021',\n",
       "  'source': \"SAC '21: Proceedings of the 36th Annual ACM Symposium on Applied Computing\",\n",
       "  'abstract': 'Several algorithms have been proposed for offline multi-label classification. However, applications in areas such as traffic monitoring, social networks, and sensors produce data continuously, the so called data streams, posing challenges to batch multi-label learning. With the lack of stationarity in the distribution of data streams, new algorithms are needed to online adapt to such changes (concept drift). Also, in realistic applications, changes occur in scenarios with infinitely delayed labels, where the true classes of the arrival instances are never available. We propose an online unsupervised incremental method based on self-organizing maps for multi-label stream classification in scenarios with infinitely delayed labels. We consider the existence of an initial set of labeled instances to train a self-organizing map for each label. The learned models are then used and adapted in an evolving stream to classify new instances, considering that their classes will never be available. We adapt to incremental concept drifts by online updating the weight vectors of winner neurons and the dataset label cardinality. Predictions are obtained using the Bayes rule and the outputs of each neuron, adapting the prior probabilities and conditional probabilities of the classes in the stream. Experiments using synthetic and real datasets show that our method is highly competitive with several ones from the literature, in both stationary and concept drift scenarios.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3412841.3441922',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'The Dynamic Hyper-ellipsoidal Micro-Clustering for Evolving Data Stream Using Only Incoming Datum',\n",
       "  'authors': \"['Narongrid Tangpathompong', 'Ureerat Suksawatchon', 'Jakkarin Suksawatchon']\",\n",
       "  'date': 'July 2017',\n",
       "  'source': \"ICIIP '17: Proceedings of the 2nd International Conference on Intelligent Information Processing\",\n",
       "  'abstract': 'Data stream clustering is becoming the efficient method to cluster an online massive data. The clustering task requires a process capable of partitioning data continuously with incremental learning method. In this paper, we present a new clustering method, called DyHEMstream, which is online and offline algorithm. In online phase, dynamic hyper-ellipsoidal micro-cluster is proposed used to keep summary information about evolving data stream based on new incoming data sample. The shape of proposed micro-cluster can represent the incoming data better than traditional micro-cluster. The algorithm processes each data point in one-pass fashion without storing the entire data set. In offline phase, each cluster is generated by expanding hyper-ellipsoidal micro-clusters to form the final clusters. The DyHEMstream algorithm is evaluated on various synthetic data sets using different quality metrics compared with a famous data stream clustering -- DenStream. Based on purity, Rand index, and Jaccard index, DyHEMstrem is very efficient than DenStream in term of clustering quality in different shapes, sizes, and densities in noisy data.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3144789.3144818',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Anomaly Detection in Business Process based on Data Stream Mining',\n",
       "  'authors': \"['Gabriel Marques Tavares', 'Victor G. Turrisi da Costa', 'Vinicius Eiji Martins', 'Paolo Ceravolo', 'Sylvio Barbon']\",\n",
       "  'date': 'June 2018',\n",
       "  'source': \"SBSI '18: Proceedings of the XIV Brazilian Symposium on Information Systems\",\n",
       "  'abstract': 'Identifying fraudulent or anomalous business procedures is today a key challenge for organisations of any dimension. Nevertheless, the continuous nature of business conveys to the continuous acquisition of data in support of business process monitoring. In light of this, we propose a method for online anomaly detection in business processes. From a stream of events, our approach extract cases descriptors and applies a density-based clustering technique to detect outliers. We applied our method to a real-life dataset, and we used streaming clustering measures for evaluating performances. In particular, we obtained Cluster Mapping Measure of 95.3% and Homogeneity of 98.1% discovering anomalous cases in real-time.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3229345.3229362',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Exploring Classification of SPECT MPI images applying convolutional neural networks',\n",
       "  'authors': \"['Nikolaos Papandrianos', 'Anna Feleki', 'Elpiniki Papageorgiou']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"PCI '21: Proceedings of the 25th Pan-Hellenic Conference on Informatics\",\n",
       "  'abstract': 'The main goal of this research paper is to address the problem of SPECT myocardial perfusion imaging (MPI) diagnosis, exploring the capabilities of convolutional neural networks (CNN). Up to date, very few research studies have been conducted regarding the application of machine learning algorithms focusing on efficient structures of convolutional neural networks (CNNs) for the diagnosis of ischemia in MPI images. In the presented work, the dataset consists of SPECT images in stress and rest representation, and a two-class classification problem corresponding to 262 normal and 251 ischemic cases is explored. The data augmentation technique was used for increasing the number of the training dataset by rotating the images and zooming randomly. In this research study, a simple but robust CNN model for automatic classification of MPI images in two categories, was applied, after a proper exploration process concerning different values for number of layers, dense nodes, convolutional parameters as well as batch size and pixel size. The proposed CNN achieved an accuracy of 90.2075% and an AUC value of 93.77%. The results proved that the convolutional neural network is able to differentiate between normal and ischemic cases and would be a great assist to medical industry, when researching myocardial perfusion images. The model we are proposing is considered a valuable asset for this medical classification problem, as it manages to produce more reliable results compared to traditional clinical methods.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503823.3503911',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Local Constraint and Label Embedding Multi-layer Dictionary Learning for Sperm Head Classification',\n",
       "  'authors': \"['Tongguang Ni', 'Yan Ding', 'Jing Xue', 'Kaijian Xia', 'Xiaoqing Gu', 'Yizhang Jiang']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Multimedia Computing, Communications, and Applications',\n",
       "  'abstract': 'Morphological classification of human sperm heads is a key technology for diagnosing male infertility. Due to its sparse representation and learning capability, dictionary learning has shown remarkable performance in human sperm head classification. To promote the discriminability of the classification model, a novel local constraint and label embedding multi-layer dictionary learning model called LCLM-MDL is proposed in this study. Based on the multi-layer dictionary learning framework, two dictionaries are built on the basis of Laplacian regularized constraint and label embedding term in each layer, and the two dictionaries are approximated to each other as much as possible, so as to well exploit the nonlinear structure and discriminability features of the morphology of human sperm heads. In addition, to promote the robustness of the model, the asymmetric Huber loss is adopted in the last layer of LCLM-MDL, which approximates the misclassification error by using the absolute error function. Finally, the experimental results on HuSHeM dataset demonstrate the validity of the LCLM-MDL.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3458927',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'SISA: Set-Centric Instruction Set Architecture for Graph Mining on Processing-in-Memory Systems',\n",
       "  'authors': \"['Maciej Besta', 'Raghavendra Kanakagiri', 'Grzegorz Kwasniewski', 'Rachata Ausavarungnirun', 'Jakub Beránek', 'Konstantinos Kanellopoulos', 'Kacper Janda', 'Zur Vonarburg-Shmaria', 'Lukas Gianinazzi', 'Ioana Stefan', 'Juan Gómez Luna', 'Jakub Golinowski', 'Marcin Copik', 'Lukas Kapp-Schwoerer', 'Salvatore Di Girolamo', 'Nils Blach', 'Marek Konieczny', 'Onur Mutlu', 'Torsten Hoefler']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"MICRO '21: MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture\",\n",
       "  'abstract': 'Simple graph algorithms such as PageRank have been the target of numerous hardware accelerators. Yet, there also exist much more complex graph mining algorithms for problems such as clustering or maximal clique listing. These algorithms are memory-bound and thus could be accelerated by hardware techniques such as Processing-in-Memory (PIM). However, they also come with non-straightforward parallelism and complicated memory access patterns. In this work, we address this problem with a simple yet surprisingly powerful observation: operations on sets of vertices, such as intersection or union, form a large part of many complex graph mining algorithms, and can offer rich and simple parallelism at multiple levels. This observation drives our cross-layer design, in which we (1) expose set operations using a novel programming paradigm, (2) express and execute these operations efficiently with carefully designed set-centric ISA extensions called SISA, and (3) use PIM to accelerate SISA instructions. The key design idea is to alleviate the bandwidth needs of SISA instructions by mapping set operations to two types of PIM: in-DRAM bulk bitwise computing for bitvectors representing high-degree vertices, and near-memory logic layers for integer arrays representing low-degree vertices. Set-centric SISA-enhanced algorithms are efficient and outperform hand-tuned baselines, offering more than 10 × speedup over the established Bron-Kerbosch algorithm for listing maximal cliques. We deliver more than 10 SISA set-centric algorithm formulations, illustrating SISA’s wide applicability.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3466752.3480133',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Recurrent Attention Walk for Semi-supervised Classification',\n",
       "  'authors': \"['Uchenna Akujuobi', 'Qiannan Zhang', 'Han Yufei', 'Xiangliang Zhang']\",\n",
       "  'date': 'January 2020',\n",
       "  'source': \"WSDM '20: Proceedings of the 13th International Conference on Web Search and Data Mining\",\n",
       "  'abstract': 'In this paper, we study the graph-based semi-supervised learning for classifying nodes in attributed networks, where the nodes and edges possess content information. Recent approaches like graph convolution networks and attention mechanisms have been proposed to ensemble the first-order neighbors and incorporate the relevant neighbors. However, it is costly (especially in memory) to consider all neighbors without a prior differentiation. We propose to explore the neighborhood in a reinforcement learning setting and find a walk path well-tuned for classifying the unlabelled target nodes. We let an agent (of node classification task) walk over the graph and decide where to move to maximize classification accuracy. We define the graph walk as a partially observable Markov decision process (POMDP). The proposed method is flexible for working in both transductive and inductive setting. Extensive experiments on four datasets demonstrate that our proposed method outperforms several state-of-the-art methods. Several case studies also illustrate the meaningful movement trajectory made by the agent.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3336191.3371853',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'PinText: A Multitask Text Embedding System in Pinterest',\n",
       "  'authors': \"['Jinfeng Zhuang', 'Yu Liu']\",\n",
       "  'date': 'July 2019',\n",
       "  'source': \"KDD '19: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining\",\n",
       "  'abstract': 'Text embedding is a fundamental component for extracting text features in production-level data mining and machine learning systems given textual information is the most ubiqutious signals. However, practitioners often face the tradeoff between effectiveness of underlying embedding algorithms and cost of training and maintaining various embedding results in large-scale applications. In this paper, we propose a multitask text embedding solution called PinText for three major vertical surfaces including homefeed, related pins, and search in Pinterest, which consolidates existing text embedding algorithms into a single solution and produces state-of-the-art performance. Specifically, we learn word level semantic vectors by enforcing that the similarity between positive engagement pairs is larger than the similarity between a randomly sampled background pairs. Based on the learned semantic vectors, we derive embedding vector of a user, a pin, or a search query by simply averaging its word level vectors. In this common compact vector space, we are able to do unified nearest neighbor search with hashing by Hadoop jobs or dockerized images on Kubernetes cluster. Both offline evaluation and online experiments show effectiveness of this PinText system and save storage cost of multiple open-sourced embeddings significantly.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3292500.3330671',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Machine Learning Techniques for Heart Disease Datasets: A Survey',\n",
       "  'authors': \"['Younas Khan', 'Usman Qamar', 'Nazish Yousaf', 'Aimal Khan']\",\n",
       "  'date': 'February 2019',\n",
       "  'source': \"ICMLC '19: Proceedings of the 2019 11th International Conference on Machine Learning and Computing\",\n",
       "  'abstract': 'Heart Failure (HF) has been proven one of the leading causes of death that is why an accurate and timely prediction of HF risks is extremely essential. Clinical methods, for instance, angiography is the best and most effective way of diagnosing HF, however, studies show that it is not only costly but has side effects as well. Lately, machine learning techniques have been used for the stated purpose. This survey paper aims to present a systematic literature review based on 35 journal articles published since 2012, where state of the art machine learning classification techniques have been implemented on heart disease datasets. This study critically analyzes the selected papers and finds gaps in the existing literature and is assistive for researchers who intend to apply machine learning in medical domains, particularly on heart disease datasets. The survey finds out that the most popular classification techniques are Support Vector Machine, Neural Networks, and ensemble classifiers.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3318299.3318343',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Collaborative SQL-injections detection system with machine learning',\n",
       "  'authors': \"['Moisés Lodeiro-Santiago', 'Cándido Caballero-Gil', 'Pino Caballero-Gil']\",\n",
       "  'date': 'October 2017',\n",
       "  'source': \"IML '17: Proceedings of the 1st International Conference on Internet of Things and Machine Learning\",\n",
       "  'abstract': \"Data mining and information extraction from data is a field that has gained relevance in recent years thanks to techniques based on artificial intelligence and use of machine and deep learning. The main aim of the present work is the development of a tool based on a previous behaviour study of security audit tools (oriented to SQL pentesting) with the purpose of creating testing sets capable of performing an accurate detection of a SQL attack. The study is based on the information collected through the generated web server logs in a pentesting laboratory environment. Then, making use of the common extracted patterns from the logs, each attack vector has been classified in risk levels (dangerous attack, normal attack, non-attack, etc.). Finally, a training with the generated data was performed in order to obtain a classifier system that has a variable performance between 97 and 99 percent in positive attack detection. The training data is shared to other servers in order to create a distributed network capable of deciding if a query is an attack or is a real petition and inform to connected clients in order to block the petitions from the attacker's IP.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3109761.3158395',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A Classification Model for Thai Statement Sentiments by Deep Learning Techniques',\n",
       "  'authors': \"['Pakawan Pugsee', 'Nitikorn Ongsirimongkol']\",\n",
       "  'date': 'November 2019',\n",
       "  'source': \"CIIS '19: Proceedings of the 2019 2nd International Conference on Computational Intelligence and Intelligent Systems\",\n",
       "  'abstract': 'At present, many organizations realized the importance of sentiment analysis for consumer reviews. The positive and negative comments can help to evaluate the user satisfaction of products and services to control and improve their qualities. In addition, the deep learning techniques are very interesting methods for current researches in the data mining field. Therefore, this research studied on the deep learning techniques to analyzed user reviews and comments in Thai Language from the TripAdvisor website. To begin with, user comments in four categories: hotels, restaurants, tourist attractions, and airlines were collected and tested on the combination of two basic deep learning technique that are convolutional neural network and long-short term memory. All user comments were divided into individual statements to classify into three groups: positive feelings, negative feelings, non-expressed feelings or neutrality. The research results found that the best classification model is the combination of three convolutional neural networks with 32, 64, and 128 filters, respectively, and the kernel size of 2 equal to the three components. Moreover, the performance of the proposed classification model was evaluated by accuracy, precision, and recall values which were higher than 80% in positive and negative groups, including F1 score about 0.8.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3372422.3372448',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Ship Classification for Space-based AIS Data Using 1D-CNN',\n",
       "  'authors': \"['Yitao Wang', 'Lei Yang', 'Xin Song']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"EITCE '21: Proceedings of the 2021 5th International Conference on Electronic Information Technology and Computer Engineering\",\n",
       "  'abstract': \"The wild application of AIS (Automatic Identification System) provides large quantities of data for maritime traffic research. There are many ships with types unknown in AIS data. The data-driven methods to identify the ship's class is one of the research hotspots in AIS data mining. In this paper, a 1D-CNN (one-dimensional convolutional neural network) method is applied to classify ships from the distribution of ships motion features, which achieves an accuracy of 78.83%. Moreover, the importance of features is discussed, which explains how the 1D-CNN works in this task and the reason why some classes of ships are mislabeled. The method in this paper is proved to be effective in ship classification using dynamic data in AIS.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3501409.3501560',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A Deep Learning-based Approach for Human Posture Classification',\n",
       "  'authors': \"['Jui-Sheng Hung', 'Pin-Ling Liu', 'Chien-Chi Chang']\",\n",
       "  'date': 'April 2020',\n",
       "  'source': \"MSIE '20: Proceedings of the 2020 2nd International Conference on Management Science and Industrial Engineering\",\n",
       "  'abstract': 'Lifting posture is considered as a leading factor in low back injuries in the workplace. Hence, it is necessary to evaluate the risk of various lifting tasks. Classifying postures is important before performing an ergonomic task assessment. Recently, many studies have revealed that the deep learning method has a high accuracy in identifying human postures. However, few studies have explored how the deep learning method can be applied to classify different postures during a lifting task. The objective of this study was to develop a deep learning technique-based model for classifying three states of postures (squatting, standing and stooping) during a lifting task. A dataset comprising 2,600 various static images (squatting, standing and stooping) taken from 0° and 90° camera view angles and their corresponding 3D joint coordinate data recorded by the marker-based motion tracking system was used in this study. The images were randomly divided into training (1,300 images), validation (650 images) and testing (650 images) datasets. After all of the images were cropped to a fixed size, the training dataset was processed in the neural network as the input, and the validation dataset was used to revise the weight of the model while training to build the classifying model. Finally, the testing dataset was processed as input for classifying three static postures using the proposed model. A classification based on the 3D coordinate data captured by the marker-based motion tracking system was used as the reference to validate the accuracy of this classifying model. Overall, the model developed in this study reached 91.23% accuracy. The accuracy of correctly classifying the squatting, standing and stooping postures is 94.35%, 98.33% and 75.86%, respectively. In addition, this model showed a nearly equivalent accuracy for identifying the images taken from 0° (91.64%) and 90° (90.86%) cameras. The results of this preliminary test showed that the deep learning method has the potential to classify different static postures within a lifting pattern.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3396743.3396763',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Deep Learning of Pre-Classification for Fast Image Retrieval',\n",
       "  'authors': \"['Fan Liu', 'Bin Wang', 'Qian Zhang']\",\n",
       "  'date': 'December 2018',\n",
       "  'source': \"ACAI '18: Proceedings of the 2018 International Conference on Algorithms, Computing and Artificial Intelligence\",\n",
       "  'abstract': 'Two dominant aspects of image retrieval are feature extraction and similarity metrics. Both of them have a great influence on the accuracy and efficiency of image retrieval. Traditional approaches always leverage pre-defined features to represent images without exploiting semantic information hidden in the dataset. In this paper, we propose a simple yet practical approach, namely deep learning of pre-classification (DLPC), which integrates classification into the image retrieval framework. Specifically, DLPC learns a Convolutional Neural Network (CNN) model via transfer learning, which can simultaneously finish feature extraction and image pre-classification. The results of image pre-classification provide feedback information indicating that features of images belonging to the same class should be stored together. For new-coming query images, they can easily find their similar images from the image library, as the similarity metric is performed only on images in the same category according to the pre-classification results. Extended experiments are performed on the Wang dataset and the Pet Dataset. Experimental results on these two large scale image datasets validate the promising performance of our method compared with the state-of-the-art approaches.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3302425.3302436',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Research on Hainan Cross-border Tourism Image Elements Based on Tourist Perception and Network Communication Based on Data Mining',\n",
       "  'authors': \"['Xiuqing Fu', 'Zhiguo Zheng', 'Bo Xu']\",\n",
       "  'date': 'May 2021',\n",
       "  'source': 'ICAIIS 2021: 2021 2nd International Conference on Artificial Intelligence and Information Systems',\n",
       "  'abstract': \"Tourism has been cultivated as an important industry in the national economy. With the great attention of all sectors of society to tourism, the development speed of China's tourism industry has been accelerated in an all-round way. People are increasingly aware that a good tourism image is one of the core competitiveness of tourism destinations. This paper combines theoretical analysis and empirical investigation, from the perspective of tourists' perception, obtains the attribute characteristics of Hainan tourism destination image perceived by tourists based on data mining software and network communication, and establishes analysis categories by using content analysis method. It also analyzes the image, tourism image, the components of tourism image and the influencing factors of the formation of tourism image, and probes into the spread of cross-border tourism image in Hainan.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3469213.3472789',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Open-world Learning and Application to Product Classification',\n",
       "  'authors': \"['Hu Xu', 'Bing Liu', 'Lei Shu', 'P. Yu']\",\n",
       "  'date': 'May 2019',\n",
       "  'source': \"WWW '19: The World Wide Web Conference\",\n",
       "  'abstract': 'Classic supervised learning makes the closed-world assumption that the classes seen in testing must have appeared in training. However, this assumption is often violated in real-world applications. For example, in a social media site, new topics emerge constantly and in e-commerce, new categories of products appear daily. A model that cannot detect new/unseen topics or products is hard to function well in such open environments. A desirable model working in such environments must be able to (1) reject examples from unseen classes (not appeared in training) and (2) incrementally learn the new/unseen classes to expand the existing model. This is called open-world learning (OWL). This paper proposes a new OWL method based on meta-learning. The key novelty is that the model maintains only a dynamic set of seen classes that allows new classes to be added or deleted with no need for model re-training. Each class is represented by a small set of training examples. In testing, the meta-classifier only uses the examples of the maintained seen classes (including the newly added classes) on-the-fly for classification and rejection. Experimental results with e-commerce product classification show that the proposed method is highly effective1.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3308558.3313644',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Blind Source Separation Approach for Audio Signals based on Support Vector Machine Classification',\n",
       "  'authors': \"['H. Abouzid', 'O. Chakkor']\",\n",
       "  'date': 'November 2017',\n",
       "  'source': \"ICCWCS'17: Proceedings of the 2nd International Conference on Computing and Wireless Communication Systems\",\n",
       "  'abstract': 'Audio signals are surrounding us everywhere, existing in many forms (speech, music, noise background, ...), but they exist all mixed together and separating them is a real serious problem. It is required to arrange them in order to be separated to use them an easy way in such many various applications such as blind source separation, extraction of speech segments, audio visual analysis,.... In this work, we introduce a new method to separate audio signals arrived mixed to a couple of microphones implemented on a head of a humanoid robot to solve the blind source separation (BSS) problem using the support vector machine (SVM). Thus, we provide a theoretical introduction to present the SVM method which has frequently been proposed for classification and regression tasks. The observations are classified by SVM method using some standard recordings which have been taken in a room. The experimental results after using the SVM technique are given at the end of this paper.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3167486.3167526',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Hidden stratification causes clinically meaningful failures in machine learning for medical imaging',\n",
       "  'authors': \"['Luke Oakden-Rayner', 'Jared Dunnmon', 'Gustavo Carneiro', 'Christopher Re']\",\n",
       "  'date': 'April 2020',\n",
       "  'source': \"CHIL '20: Proceedings of the ACM Conference on Health, Inference, and Learning\",\n",
       "  'abstract': 'Machine learning models for medical image analysis often suffer from poor performance on important subsets of a population that are not identified during training or testing. For example, overall performance of a cancer detection model may be high, but the model may still consistently miss a rare but aggressive cancer subtype. We refer to this problem as hidden stratification, and observe that it results from incompletely describing the meaningful variation in a dataset. While hidden stratification can substantially reduce the clinical efficacy of machine learning models, its effects remain difficult to measure. In this work, we assess the utility of several possible techniques for measuring hidden stratification effects, and characterize these effects both via synthetic experiments on the CIFAR-100 benchmark dataset and on multiple real-world medical imaging datasets. Using these measurement techniques, we find evidence that hidden stratification can occur in unidentified imaging subsets with low prevalence, low label quality, subtle distinguishing features, or spurious correlates, and that it can result in relative performance differences of over 20% on clinically important subsets. Finally, we discuss the clinical implications of our findings, and suggest that evaluation of hidden stratification should be a critical component of any machine learning deployment in medical imaging.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3368555.3384468',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Under-bagging nearest neighbors for imbalanced classification',\n",
       "  'authors': \"['Hanyuan Hang', 'Yuchao Cai', 'Hanfang Yang', 'Zhouchen Lin']\",\n",
       "  'date': 'None',\n",
       "  'source': 'The Journal of Machine Learning Research',\n",
       "  'abstract': 'In this paper, we propose an ensemble learning algorithm called under-bagging k-nearest neighbors (under-bagging k-NN) for imbalanced classification problems. On the theoretical side, by developing a new learning theory analysis, we show that with properly chosen parameters, i.e., the number of nearest neighbors k, the expected sub-sample size s, and the bagging rounds B, optimal convergence rates for under-bagging k-NN can be achieved under mild assumptions w.r.t. the arithmetic mean (AM) of recalls. Moreover, we show that with a relatively small B, the expected sub-sample size s can be much smaller than the number of training data n at each bagging round, and the number of nearest neighbors k can be reduced simultaneously, especially when the data are highly imbalanced, which leads to substantially lower time complexity and roughly the same space complexity. On the practical side, we conduct numerical experiments to verify the theoretical results on the benefits of the under-bagging technique by the promising AM performance and efficiency of our proposed algorithm.',\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3586589.3586707',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Mining Significant Microblogs for Misinformation Identification: An Attention-Based Approach',\n",
       "  'authors': \"['Qiang Liu', 'Feng Yu', 'Shu Wu', 'Liang Wang']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Intelligent Systems and Technology',\n",
       "  'abstract': 'With the rapid growth of social media, massive misinformation is also spreading widely on social media, e.g., Weibo and Twitter, and brings negative effects to human life. Today, automatic misinformation identification has drawn attention from academic and industrial communities. Whereas an event on social media usually consists of multiple microblogs, current methods are mainly constructed based on global statistical features. However, information on social media is full of noise, which should be alleviated. Moreover, most of the microblogs about an event have little contribution to the identification of misinformation, where useful information can be easily overwhelmed by useless information. Thus, it is important to mine significant microblogs for constructing a reliable misinformation identification method. In this article, we propose an attention-based approach for identification of misinformation (AIM). Based on the attention mechanism, AIM can select microblogs with the largest attention values for misinformation identification. The attention mechanism in AIM contains two parts: content attention and dynamic attention. Content attention is the calculated-based textual features of each microblog. Dynamic attention is related to the time interval between the posting time of a microblog and the beginning of the event. To evaluate AIM, we conduct a series of experiments on the Weibo and Twitter datasets, and the experimental results show that the proposed AIM model outperforms the state-of-the-art methods.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3173458',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Meta-Learning for Neural Relation Classification with Distant Supervision',\n",
       "  'authors': \"['Zhenzhen Li', 'Jian-Yun Nie', 'Benyou Wang', 'Pan Du', 'Yuhan Zhang', 'Lixin Zou', 'Dongsheng Li']\",\n",
       "  'date': 'October 2020',\n",
       "  'source': \"CIKM '20: Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management\",\n",
       "  'abstract': 'Distant supervision provides a means to create a large number of weakly labeled data at low cost for relation classification. However, the resulting labeled instances are very noisy, containing data with wrong labels. Many approaches have been proposed to select a subset of reliable instances for neural model training, but they still suffer from noisy labeling problem or underutilization of the weakly-labeled data. To better select more reliable training instances, we introduce a small amount of manually labeled data as reference to guide the selection process. In this paper, we propose a meta-learning based approach, which learns to reweight noisy training data under the guidance of reference data. As the clean reference data is usually very small, we propose to augment it by dynamically distilling the most reliable elite instances from the noisy data. Experiments on several datasets demonstrate that the reference data can effectively guide the selection of training data, and our augmented approach consistently improves the performance of relation classification comparing to the existing state-of-the-art methods.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3340531.3412039',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Merging Live Video Feeds for Remote Monitoring of a Mining Machine',\n",
       "  'authors': \"['Andrew T Flangas', 'Javad Sattarvand', 'Sergiu M Dascalu', 'Frederick C Harris']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"ESSE '21: Proceedings of the 2021 European Symposium on Software Engineering\",\n",
       "  'abstract': 'This research entails using virtual reality to interpret video recordings in Unity from cameras on an unmanned machine used for mining excavations. The purpose of using a machine of this nature is to send it into hazardous mining environments rather than sending workers and having their lives jeopardized. This work is significant because it demonstrates how two separate fields, such as virtual reality and robotics, can be combined to complete useful tasks. It also illustrates how machines can be used to replace workers in hazardous conditions not only in the field of mining, but in other fields as well. The main contribution of the work presented in this paper is the creation of a panorama of live video feeds captured by several webcams, which can be seen using a VR headset. As also described in the paper the software developed for this engineering application has been created using appropriate software engineering techniques and tools. Results of merging live video feeds and testing camera placements are also presented and planned directions of future work are outlined.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3501774.3501776',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Imbalanced Time Series Classification for Flight Data Analyzing with Nonlinear Granger Causality Learning',\n",
       "  'authors': \"['Hao Huang', 'Chenxiao Xu', 'Shinjae Yoo', 'Weizhong Yan', 'Tianyi Wang', 'Feng Xue']\",\n",
       "  'date': 'October 2020',\n",
       "  'source': \"CIKM '20: Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management\",\n",
       "  'abstract': \"Identifying the faulty class of multivariate time series is crucial for today's flight data analysis. However, most of the existing time series classification methods suffer from imbalanced data and lack of model interpretability, especially on flight data of which faulty events are usually uncommon with a limited amount of data. Here, we present a neural network classification model for imbalanced multivariate time series by leveraging the information learned from normal class, which can also learn the nonlinear Granger causality for each class, so that we can pinpoint how time series classes differ from each other. Experiments on simulated data and real flight data shows that this model can achieve high accuracy of identifying anomalous flights.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3340531.3412710',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Correlation-Based Incremental Learning Network with Sliding Window for Perfume Classification',\n",
       "  'authors': \"['Panida Lorwongtrakool', 'Phayung Meesad']\",\n",
       "  'date': 'July 2020',\n",
       "  'source': \"IAIT '20: Proceedings of the 11th International Conference on Advances in Information Technology\",\n",
       "  'abstract': 'Contamination inspection or quality inspection of raw materials or products is a very important task, especially in the perfume industry that requires an expert for inspection. However, the human nose has limitations such as fatigue, which affects the accuracy. Therefore, an electronic nose or sensor array has been developed to assist in the inspection. The signal data from electronic nose is fed into machine learning models to learn and process. Since the data change over time, the input data fluctuate according to the changing environment. In addition, when there are new data with features that change from the original patterns, the classification outcome may not be correct and the model will not be able to classify as effective as the original model. Therefore, to solve the problem mentioned this research proposes Correlation-Based Incremental Learning Network with Sliding Window (CILNS), which learns automatically by adapting to new data while maintaining the existing knowledge. The experiments were conducted on classifying perfumes. The experimental data were divided into 4 batches. Batch 1 was used as the training data and the other batches were used as the testing data. The proposed algorithm was compared with other well-known classifiers. The results showed that the proposed CILNS algorithm model 4-1 (Window size = 4, step size = 1) provides the highest accuracy of 95.16%.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3406601.3406649',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Using Machine Learning Classifiers to Identify the Critical Proteins in Down Syndrome',\n",
       "  'authors': \"['Handan Kulan', 'Tamer Dag']\",\n",
       "  'date': 'October 2018',\n",
       "  'source': \"ICCBB '18: Proceedings of the 2018 2nd International Conference on Computational Biology and Bioinformatics\",\n",
       "  'abstract': 'Pharmacotherapies of intellectual disability (ID) are largely unknown as the abnormalities at the complex molecular level which causes ID are difficult to understand. Down syndrome (DS) which is the prevalent cause of ID and caused by an extra copy of the human chromosome21 (Hsa21) has been investigated on protein levels by using the Ts65Dn mouse model of DS which are orthologs of %50 of Hsa21 classical protein coding genes. Recent works have applied the classification methods to understand critical factors in DS as it is believed that the problem was naturally related to classification problem since the determination of proteins discriminatory between classes of mice was required. In this study, we apply forward feature selection method to identify correlated proteins and their interactions in DS. After identification, we report supervised learning model of expression levels of selected proteins in order to understand the critical proteins for diagnosing and explaining DS. The proposed technique depicts optimum classification results achieved by optimizing parameters with grid search. When compared with the former work, our classification results give higher accuracy.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3290818.3290831',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Clustering based personality prediction on turkish tweets',\n",
       "  'authors': \"['Esen Tutaysalgir', 'Pinar Karagoz', 'Ismail H. Toroslu']\",\n",
       "  'date': 'August 2019',\n",
       "  'source': \"ASONAM '19: Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining\",\n",
       "  'abstract': 'In this paper, we present a framework for predicting the personality traits by analyzing tweets written in Turkish. The prediction model is constructed with a clustering based approach. Since the model is based on linguistic features, it is language specific. The prediction model uses features applicable to Turkish language and related to writing style of Turkish Twitter users. Our approach uses anonymous BIG5 questionnaire scores of volunteer participants as the ground truth in order to generate personality model from Twitter posts. Experiment results show that constructed model can predict personality traits of Turkish Twitter users with relatively small errors.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3341161.3343513',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'IPGOD: An Integrated Visualization Platform Based on Big Data Mining and Cloud Computing',\n",
       "  'authors': \"['Wei-Yu Chen', 'Peggy Joy Lu', 'Steven Shiau']\",\n",
       "  'date': 'May 2019',\n",
       "  'source': \"ICBDC '19: Proceedings of the 4th International Conference on Big Data and Computing\",\n",
       "  'abstract': 'With big data analytics and open data mining becoming increasingly important in this information explosion era, a highly efficient approach to providing an integrated service is by combining these two topics. Therefore, to maximize the convenience of Taiwan\\'s open data utilization and to enrich users\\' experiences with big data analytics, this paper proposes the Integrated Platform for Government Open Data (IPGOD). The platform consists of a \"Data System\" based on a cloud data warehouse and an \"Analytics System\" based on machine learning utilities; these two systems can work individually or in an integrated manner. Moreover, we leverage the Apache Spark cloud platform to enhance low latency response and high performance. The experimental results demonstrate that the proposed IPGOD realizes the open data warehouse effectively and derives machine learning visualization in a user-friendly and intelligent way.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3335484.3335494',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Fast, Accurate, and Flexible Algorithms for Dense Subtensor Mining',\n",
       "  'authors': \"['Kijung Shin', 'Bryan Hooi', 'Christos Faloutsos']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Knowledge Discovery from Data',\n",
       "  'abstract': 'Given a large-scale and high-order tensor, how can we detect dense subtensors in it? Can we spot them in near-linear time but with quality guarantees? Extensive previous work has shown that dense subtensors, as well as dense subgraphs, indicate anomalous or fraudulent behavior (e.g., lockstep behavior in social networks). However, available algorithms for detecting dense subtensors are not satisfactory in terms of speed, accuracy, and flexibility. In this work, we propose two algorithms, called M-Zoom and M-Biz, for fast and accurate dense-subtensor detection with various density measures. M-Zoom gives a lower bound on the density of detected subtensors, while M-Biz guarantees the local optimality of detected subtensors. M-Zoom and M-Biz can be combined, giving the following advantages: (1) Scalable: scale near-linearly with all aspects of tensors and are up to 114× faster than state-of-the-art methods with similar accuracy, (2) Provably accurate: provide a guarantee on the lowest density and local optimality of the subtensors they find, (3) Flexible: support multi-subtensor detection and size bounds as well as diverse density measures, and (4) Effective: successfully detected edit wars and bot activities in Wikipedia, and spotted network attacks from a TCP dump with near-perfect accuracy (AUC = 0.98).',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3154414',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Learning to Selectively Transfer: Reinforced Transfer Learning for Deep Text Matching',\n",
       "  'authors': \"['Chen Qu', 'Feng Ji', 'Minghui Qiu', 'Liu Yang', 'Zhiyu Min', 'Haiqing Chen', 'Jun Huang', 'W. Bruce Croft']\",\n",
       "  'date': 'January 2019',\n",
       "  'source': \"WSDM '19: Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining\",\n",
       "  'abstract': 'Deep text matching approaches have been widely studied for many applications including question answering and information retrieval systems. To deal with a domain that has insufficient labeled data, these approaches can be used in a Transfer Learning (TL) setting to leverage labeled data from a resource-rich source domain. To achieve better performance, source domain data selection is essential in this process to prevent the \"negative transfer\" problem. However, the emerging deep transfer models do not fit well with most existing data selection methods, because the data selection policy and the transfer learning model are not jointly trained, leading to sub-optimal training efficiency. In this paper, we propose a novel reinforced data selector to select high-quality source domain data to help the TL model. Specifically, the data selector \"acts\" on the source domain data to find a subset for optimization of the TL model, and the performance of the TL model can provide \"rewards\" in turn to update the selector. We build the reinforced data selector based on the actor-critic framework and integrate it to a DNN based transfer learning model, resulting in a Reinforced Transfer Learning (RTL) method. We perform a thorough experimental evaluation on two major tasks for text matching, namely, paraphrase identification and natural language inference. Experimental results show the proposed RTL can significantly improve the performance of the TL model. We further investigate different settings of states, rewards, and policy optimization methods to examine the robustness of our method. Last, we conduct a case study on the selected data and find our method is able to select source domain data whose Wasserstein distance is close to the target domain data. This is reasonable and intuitive as such source domain data can provide more transferability power to the model.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3289600.3290978',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Deep learning-based semantic classification of EMF-related scientific literature',\n",
       "  'authors': \"['Kwanghee Won', 'Hyung-do Choi', 'Sung Shin']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': 'ACM SIGAPP Applied Computing Review',\n",
       "  'abstract': 'Semantic classification of scientific literature using machine learning approaches is challenging due to the difficulties in labeling data and the length of the texts [2, 7]. Most of the work has been done for keyword-based categorization tasks, which take care of occurrence of important terms, whereas semantic classification requires understanding of terms and the meaning of sentences in a context. In this study, we have evaluated neural network models on a semantic classification task using 1091 labeled EMF-related scientific papers listed in the Powerwatch study. The EMF-related papers are labeled into three categories: positive, null finding, and neither. We have conducted neural architecture and hyperparameter search to find the most suitable model for the task. In experiments, we compared the performance of several neural network models in terms of classification accuracy. In addition, we have tested two different types of attention mechanisms. First, a Fully Convolutional Neural Network (FCN) has been used to identify important sentences in the text for the semantic classification. Second, the Transformer, a self-attention-based model, has been tested on the dataset. The experimental result showed that the BiLSTM performed best on both unbalanced and balanced data and the FCN was able to identify important parts in input texts.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3477127.3477131',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Adaptive Execution of Continuous and Data-intensive Workflows with Machine Learning',\n",
       "  'authors': \"['Sérgio Esteves', 'Helena Galhardas', 'Luís Veiga']\",\n",
       "  'date': 'November 2018',\n",
       "  'source': \"Middleware '18: Proceedings of the 19th International Middleware Conference\",\n",
       "  'abstract': 'To extract value from evergrowing volumes of data and to drive decision making, organizations frequently resort to the composition of data processing workflows. The typical workflow model enforces strict temporal synchronization across processing steps without accounting the actual effect of intermediate computations on the final workflow output. However, this is not the most desirable in a multitude of scenarios. We identify a class of applications for continuous data processing where the workflow output changes slowly and without great significance in a short time window, thus squandering compute resources with current approaches. To overcome such inefficiency, we introduce a novel workflow model, for continuous and data-intensive processing, capable of relaxing triggering semantics according to the impact that input data is assessed to have on changing the workflow output. To estimate this impact, learn the correlation between input and output variation, and guarantee correctness within a given tolerated error constant, we rely on Machine Learning. The functionality of this model is implemented in SmartFlux, a middleware framework which can be integrated with existing workflow managers. Experimental results indicate substantial savings in resource usage, while not deviating the workflow output beyond a small error constant with a high confidence level.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3274808.3274827',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': \"Mining Tourist's Perception toward Indonesia Tourism Destination Using Sentiment Analysis and Topic Modelling\",\n",
       "  'authors': \"['Herry Irawan', 'Gina Akmalia', 'Riefvan Achmad Masrury']\",\n",
       "  'date': 'September 2019',\n",
       "  'source': \"CCIOT '19: Proceedings of the 2019 4th International Conference on Cloud Computing and Internet of Things\",\n",
       "  'abstract': \"Indonesia's Tourism industries is the second-best contributor to country's foreign exchange income for years. The growth it produced were in range of 7 to 10 percent per year since 2009. In reality, Bali is always at the top of the mind among majority of international tourists inspite of many hidden gems with spectacular qualities to rival Bali's popularity. Therefore, Indonesia's government has set new tourist destinations to increase their presence thus increasing visitation numbers. Researches on utilizing big data to support industry 4.0 model in tourism businesses are encouraged as part of national research priorities. Data Analytics models such as Sentiment Analysis and Topic Modelling can be used to reveal hidden patterns from abundant user-generated content data available in social media sites, one of which, TripAdvisor. This study aims to mine the visitors' perceptions of 10 most visited sites in Indonesia. Emotions and topics discussed in comments are two features to be extracted. Using data mining framework, five types of emotion and topics related to tourism were discovered. Data collection was done using Parsehub to acquire 3494 comments generated in 4 years. Both Sentiment Analysis and Topic Modelling were processed in Orange 3. Results shows that Joy is the most prominent emotion accompanying visitors' experiences. Topic modeling shows several important keywords toward preferences. Results of this study can be used to improve Indonesia's tourism stakeholder's decision quality especially in terms of marketing and operations.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3361821.3361829',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Multiple Classification with Split Learning',\n",
       "  'authors': \"['Jongwon Kim', 'Sungho Shin', 'Yeonguk Yu', 'Junseok Lee', 'Kyoobin Lee']\",\n",
       "  'date': 'September 2020',\n",
       "  'source': 'SMA 2020: The 9th International Conference on Smart Media and Applications',\n",
       "  'abstract': 'Privacy issues were raised in the process of training deep learning in medical, mobility, and other fields. To solve this problem, we present privacy-preserving distributed deep learning method that allow clients to learn a variety of data without direct exposure. We divided a single deep learning architecture into a common extractor, a cloud model and a local classifier for the distributed learning. First, the common extractor, which is used by local clients, extracts secure features from the input data. The secure features also take the role that the cloud model can employ various task and diverse types of data. The feature contain the most important information that helps to proceed various task. Second, the cloud model including most parts of the whole training model gets the embedded features from the massive local clients, and performs most of deep learning operations which takes severe computing cost. After the operations in cloud model finished, outputs of the cloud model send back to local clients. Finally, the local classifier determined classification results and delivers the results to local clients. When clients train models, our model does not directly expose sensitive information to exterior network. During the test, the average performance improvement was 2.63% over the existing local training model. However, in a distributed environment, there is a possibility of inversion attack due to exposed features. For this reason, we experimented with the common extractor to prevent data restoration. The quality of restoration of the original image was tested by adjusting the depth of the common extractor. As a result, we found that the deeper the common extractor, the restoration score decreased to 89.74.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3426020.3426131',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Multi-task learning for smile detection, emotion recognition and gender classification',\n",
       "  'authors': \"['Dinh Viet Sang', 'Le Tran Bao Cuong', 'Vu Van Thieu']\",\n",
       "  'date': 'December 2017',\n",
       "  'source': \"SoICT '17: Proceedings of the 8th International Symposium on Information and Communication Technology\",\n",
       "  'abstract': 'Facial expression analysis plays a key role in analyzing emotions and human behaviors. Smile detection, emotion recognition and gender classification are special tasks in facial expression analysis with various potential applications. In this paper, we propose an effective architecture of Convolutional Neural Network (CNN) which can jointly learn representations for three tasks: smile detection, emotion recognition and gender classification. In addition, this model can be trained from multiple sources of data with different kinds of task-specific class labels. The extensive experiments show that our model achieves superior accuracy over recent state-of-the-art techniques in all of three tasks on popular benchmarks. We also show that the joint learning helps the tasks with less data considerably benefit from other tasks with richer data.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3155133.3155207',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Complementing machine learning classifiers via dynamic symbolic execution: \"human vs. bot generated\" tweets',\n",
       "  'authors': \"['Sohil L. Shrestha', 'Saroj Panda', 'Christoph Csallner']\",\n",
       "  'date': 'May 2018',\n",
       "  'source': \"RAISE '18: Proceedings of the 6th International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering\",\n",
       "  'abstract': 'Recent machine learning approaches for classifying text as human-written or bot-generated rely on training sets that are large, labeled diligently, and representative of the underlying domain. While valuable, these machine learning approaches ignore programs as an additional source of such training sets. To address this problem of incomplete training sets, this paper proposes to systematically supplement existing training sets with samples inferred via program analysis. In our preliminary evaluation, training sets enriched with samples inferred via dynamic symbolic execution were able to improve machine learning classifier accuracy for simple string-generating programs.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3194104.3194111',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Multiple Classification with Split Learning',\n",
       "  'authors': \"['Jongwon Kim', 'Sungho Shin', 'Yeonguk Yu', 'Junseok Lee', 'Kyoobin Lee']\",\n",
       "  'date': 'September 2020',\n",
       "  'source': 'SMA 2020: The 9th International Conference on Smart Media and Applications',\n",
       "  'abstract': 'Privacy issues were raised in the process of training deep learning in medical, mobility, and other fields. To solve this problem, we present privacy-preserving distributed deep learning method that allow clients to learn a variety of data without direct exposure. We divided a single deep learning architecture into a common extractor, a cloud model and a local classifier for the distributed learning. First, the common extractor, which is used by local clients, extracts secure features from the input data. The secure features also take the role that the cloud model can employ various task and diverse types of data. The feature contain the most important information that helps to proceed various task. Second, the cloud model including most parts of the whole training model gets the embedded features from the massive local clients, and performs most of deep learning operations which takes severe computing cost. After the operations in cloud model finished, outputs of the cloud model send back to local clients. Finally, the local classifier determined classification results and delivers the results to local clients. When clients train models, our model does not directly expose sensitive information to exterior network. During the test, the average performance improvement was 2.63% over the existing local training model. However, in a distributed environment, there is a possibility of inversion attack due to exposed features. For this reason, we experimented with the common extractor to prevent data restoration. The quality of restoration of the original image was tested by adjusting the depth of the common extractor. As a result, we found that the deeper the common extractor, the restoration score decreased to 89.74.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3426020.3426131',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Roman Urdu reviews dataset for aspect based opinion mining',\n",
       "  'authors': \"['Rabail Zahid', 'Muhammad Owais Idrees', 'Hasan Mujtaba', 'Mirza Omer Beg']\",\n",
       "  'date': 'September 2020',\n",
       "  'source': \"ASE '20: Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering\",\n",
       "  'abstract': \"Social media, today, demonstrates the rapid growth of modern society as it becomes the main platform for Internet users to communicate and express themselves. People around the world, use a number of devices and resources to access the Internet, set up social networks, conduct online business, e-commerce, e-surveys, etc. Currently, social media is not only a technology that provides information to consumers, it also encourages users to connect and share their views and perspectives. It leads to an increase in inspiration towards Opinion Mining (OM), which is important for both customers and companies in making decisions. Individuals like to see the opinions provided by other customers about a particular product or a service. Companies need to analyze their customer's feedback to strengthen their business decisions. A lot of research has been performed in various languages in the field of Aspect Based OM (ABOM). However, there are still certain languages that need to be explored, such as Roman Urdu (RU). This paper presents a proposed reviews data-set (a RU data-set) of mobile reviews that has been manually annotated with multi-aspect sentiment labels at the sentence-level. It presents base-line results using different Machine Learning (ML) algorithms. The results demonstrate 71% F1-score for aspect detection and 64% for aspect-based polarity.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3417113.3423377',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Multi-task learning for smile detection, emotion recognition and gender classification',\n",
       "  'authors': \"['Dinh Viet Sang', 'Le Tran Bao Cuong', 'Vu Van Thieu']\",\n",
       "  'date': 'December 2017',\n",
       "  'source': \"SoICT '17: Proceedings of the 8th International Symposium on Information and Communication Technology\",\n",
       "  'abstract': 'Facial expression analysis plays a key role in analyzing emotions and human behaviors. Smile detection, emotion recognition and gender classification are special tasks in facial expression analysis with various potential applications. In this paper, we propose an effective architecture of Convolutional Neural Network (CNN) which can jointly learn representations for three tasks: smile detection, emotion recognition and gender classification. In addition, this model can be trained from multiple sources of data with different kinds of task-specific class labels. The extensive experiments show that our model achieves superior accuracy over recent state-of-the-art techniques in all of three tasks on popular benchmarks. We also show that the joint learning helps the tasks with less data considerably benefit from other tasks with richer data.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3155133.3155207',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Arthroscopic Tool Classification using Deep Learning',\n",
       "  'authors': \"['Bryce Palmer', 'Gunnar Sundberg', 'James Dials', 'Bayazit Karaman', 'Doga Demirel', 'Muhammad Abid', 'Tansel Halic', 'Shahryar Ahmadi']\",\n",
       "  'date': 'May 2020',\n",
       "  'source': \"ICISDM '20: Proceedings of the 2020 the 4th International Conference on Information System and Data Mining\",\n",
       "  'abstract': \"Shoulder arthroscopy is a common surgery to diagnose and treat tears to improve patient's quality of life. Quality of cleaning the tear during shoulder arthroscopy significantly affects the outcome of the surgery. Appropriate cleaning is necessary to reduce healing time and avoid feature pain in the area. In this paper, we used convolutional neural networks to automatically differentiate between two tools-electrocautery and shaver tools- that are used during the cleaning phase of a shoulder arthroscopy. We captured images from the actual shoulder arthroscopy videos. We used 8,691 images that contain the shaver tool, 7,773 images that contain the electrocautery tool, and 4,834 images that contain no tools. Our results showed that average accuracy of our model is 99.1(+/- 0.49) %. For the electrocautery tool precision and sensitivity was calculated as 0.988 and 0. 988, respectively. For the shaver tool precision and sensitivity was calculated as 0.993 and 0. 988, respectively. For the no tool scenes precision and sensitivity was calculated as 1.0 and 1. 0, respectively.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3404663.3404672',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Zigbee-based remote environmental monitoring for smart industrial mining',\n",
       "  'authors': \"['Abdellah Chehri', 'Rachid Saadane']\",\n",
       "  'date': 'October 2019',\n",
       "  'source': \"SCA '19: Proceedings of the 4th International Conference on Smart City Applications\",\n",
       "  'abstract': 'Wireless sensor networks (WSNs) consist of large number of small and low-cost devices equipped with sensing and communication facilities to monitor the environment. The collected data are transmitted to one or more base stations which can attach to other networks and/or databases. WSNs show particular promises in applications that involve complex, human-made systems such as underground mines, factory and industrial installation. In this paper, smart sensor network architecture for temperature and fire monitoring in underground mine is evaluated. Based on application requirements and site surveys, we develop a general architecture for this class of industrial applications. The architecture is based on multiple complementary wireless communications access networks between the environment and external environment, by using IEEE802.15/ZigBee, IEEE 802.11 and the Internet.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3368756.3369099',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Clustering stream data by exploring the evolution of density mountain',\n",
       "  'authors': \"['Shufeng Gong', 'Yanfeng Zhang', 'Ge Yu']\",\n",
       "  'date': 'None',\n",
       "  'source': 'Proceedings of the VLDB Endowment',\n",
       "  'abstract': 'Stream clustering is a fundamental problem in many streaming data analysis applications. Comparing to classical batch-mode clustering, there are two key challenges in stream clustering: (i) Given that input data are changing continuously, how to incrementally update their clustering results efficiently? (ii) Given that clusters continuously evolve with the evolution of data, how to capture the cluster evolution activities? Unfortunately, most of existing stream clustering algorithms can neither update the cluster result in real-time nor track the evolution of clusters.In this paper, we propose a stream clustering algorithm EDMStream by exploring the Evolution of Density Mountain. The density mountain is used to abstract the data distribution, the changes of which indicate data distribution evolution. We track the evolution of clusters by monitoring the changes of density mountains. We further provide efficient data structures and filtering schemes to ensure that the update of density mountains is in real-time, which makes online clustering possible. The experimental results on synthetic and real datasets show that, comparing to the state-of-the-art stream clustering algorithms, e.g., D-Stream, DenStream, DBSTREAM and MR-Stream, our algorithm is able to response to a cluster update much faster (say 7-15x faster than the best of the competitors) and at the same time achieve comparable cluster quality. Furthermore, EDMStream successfully captures the cluster evolution activities.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3186728.3164136',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Helicobacter Pylori Infection Classification Based on Convolutional Neural Network and Self-Supervised Learning',\n",
       "  'authors': \"['Guo-Zhang Jian', 'Guo-Shiang Lin', 'Chuin-Mu Wang', 'Sheng-Lei Yan']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ICGSP '21: Proceedings of the 5th International Conference on Graphics and Signal Processing\",\n",
       "  'abstract': 'In this paper, a computer-aided diagnosis (CAD) method based on self-supervised learning was proposed for helicobacter pylori (HP) infection classification. The proposed method is composed of an encoder and a prediction head. The encoder can be trained by using self-supervised learning and contrastive loss. After obtaining the trained encoder, the prediction head can be trained by using the small medical image dataset. To evaluate the performance of the proposed method, some medical images are collected for testing. According to experimental results, the F1-score rates of the CAD system based on VGGNet-16 are 0.89 and 0.9 for HP+ and HP- images, respectively. The results show that the proposed method composed of VGGNet-16 and a multi-layer neural network can distinguish HP+ images from HP- images well. Compared with ResNet-50 and InceptionV3, VGGNet-16 can achieve a better classification performance. The experimental results show that VGG-16 can extract useful features from endoscopic images for HP infection classification via self-supervised contrastive learning.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3474906.3474912',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Efficient Graph Convolution for Joint Node Representation Learning and Clustering',\n",
       "  'authors': \"['Chakib Fettal', 'Lazhar Labiod', 'Mohamed Nadif']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"WSDM '22: Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining\",\n",
       "  'abstract': 'Attributed graphs are used to model a wide variety of real-world networks. Recent graph convolutional network-based representation learning methods have set state-of-the-art results on the clustering of attributed graphs. However, these approaches deal with clustering as a downstream task while better performances can be attained by incorporating the clustering objective into the representation learning process. In this paper, we propose, in a unified framework, an objective function taking into account both tasks simultaneously. Based on a variant of the simple graph convolutional network, our model does clustering by minimizing the difference between the convolved node representations and their reconstructed cluster representatives. We showcase the efficiency of the derived algorithm against state-of-the-art methods both in terms of clustering performance and computational cost on thede facto benchmark graph clustering datasets. We further demonstrate the usefulness of the proposed approach for graph visualization through generating embeddings that exhibit a clustering structure.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3488560.3498533',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Study on Classification of Flue-cured Tobacco Planting area Based on Different Clustering Analysis Methods',\n",
       "  'authors': \"['Xu Ruyan', 'Hu Zongyu', 'Xu Qiang', 'Chen Haiqing', 'Li Shaopeng', 'Chu Xu']\",\n",
       "  'date': 'December 2020',\n",
       "  'source': \"EBIMCS '20: Proceedings of the 2020 3rd International Conference on E-Business, Information Management and Computer Science\",\n",
       "  'abstract': 'In order to analyze the quality similarity of tobacco leaves in different tobacco planting areas, 21 quality indices of flue-cured tobacco were collected from different production areas, extracted principal components by factor analysis, classified by 3 clustering analysis methods, and the classification results were compared and statistically tested. The result indicated that: (1) The quality of tobacco leaves was different, and high degree of information overlap was detected among different indices of tobacco appearance, chemical and sensory quality; (2) The cumulative variance contribution rate of 5 extracted principal component factors was 85.445%, and the eigenvalues were 7.761, 4.758, 2.472, 1.674 and 1.278, respectively; (3) The results of 3 cluster analysis methods were not the same. The results of the weighted principal component distance cluster and the weighted principal component cluster were similar, which were different from the general principal component cluster; (4) The results of statistical test showed that the weighted principal component distance cluster method had the largest F-test value (5.900), the smallest sum of squares within the group (8.164), and the largest sum of squares between groups (19.267). And the weighting results for different principal component factors were more reasonable which also had more objective classification results. The cluster results of weighted principal component distance and weighted principal component were better than that of general principal component. And the cluster results of weighted principal component distance were more interpretable which had better the statistical test results.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3453187.3453416',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Application of data mining for young children education using emotion information',\n",
       "  'authors': \"['Liu Yue', 'Zhang Chunhong', 'Tian Chujie', 'Zhao Xiaomeng', 'Zhang Ruizhi', 'Ji Yang']\",\n",
       "  'date': 'July 2018',\n",
       "  'source': \"DSIT '18: Proceedings of the 2018 International Conference on Data Science and Information Technology\",\n",
       "  'abstract': \"The current preschool education is still facing many difficulties. First, unlike primary and secondary schools which have test scores, there are few good ways to assess the learning situation of young children. Second, it is hard for teachers and parents to give thoughtful care all the time to each child. In particular, recent incidents of child abuse have been exposed frequently, causing social panic. However, as research of young children's mental health and emotion is still in its infancy due to the lack of relevant data, methods that focus on measuring and analyzing young children's mental health and emotional states are lacking. Furthermore, on the one hand, the principal goal of preschool education is to stimulate interest and enhance cognitive ability. On the other hand, assimilation and accommodation which are two specific stages of cognitive development require an active learner, not a passive one. Thus, emotion analysis can solve these problems to some extent. In this paper, we design an intelligent system, obtaining video clips in a kindergarten's classroom and managing to leverage the emotion data to portray cognitive learning rules and mental states of young children. At the same time, with the augmentation by data analysis, it brings broader applications developing educational policy and teaching practice. For example, children with abnormal behaviors such as violent mood swings and unusual time nodes during courses can be detected and notified to parents and teachers. And it is meaningful to measure the children's acceptance and preference of course activities and content which has been explained in our experiments.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3239283.3239321',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Exploiting implicit beliefs to resolve sparse usage problem in usage-based specification mining',\n",
       "  'authors': \"['Samantha Syeda Khairunnesa', 'Hoan Anh Nguyen', 'Tien N. Nguyen', 'Hridesh Rajan']\",\n",
       "  'date': 'None',\n",
       "  'source': 'Proceedings of the ACM on Programming Languages',\n",
       "  'abstract': 'Frameworks and libraries provide application programming interfaces (APIs) that serve as building blocks in modern software development. As APIs present the opportunity of increased productivity, it also calls for correct use to avoid buggy code. The usage-based specification mining technique has shown great promise in solving this problem through a data-driven approach. These techniques leverage the use of the API in large corpora to understand the recurring usages of the APIs and infer behavioral specifications (preconditions and postconditions) from such usages. A challenge for such technique is thus inference in the presence of insufficient usages, in terms of both frequency and richness. We refer to this as a \"sparse usage problem.\" This paper presents the first technique to solve the sparse usage problem in usage-based precondition mining. Our key insight is to leverage implicit beliefs to overcome sparse usage. An implicit belief (IB) is the knowledge implicitly derived from the fact about the code. An IB about a program is known implicitly to a programmer via the language\\'s constructs and semantics, and thus not explicitly written or specified in the code. The technical underpinnings of our new precondition mining approach include a technique to analyze the data and control flow in the program leading to API calls to infer preconditions that are implicitly present in the code corpus, a catalog of 35 code elements in total that can be used to derive implicit beliefs from a program, and empirical evaluation of all of these ideas. We have analyzed over 350 millions lines of code and 7 libraries that suffer from the sparse usage problem. Our approach realizes 6 implicit beliefs and we have observed that adding single-level context sensitivity can further improve the result of usage based precondition mining. The result shows that we achieve overall 60% in precision and 69% in recall and the accuracy is relatively improved by 32% in precision and 78% in recall compared to base usage-based mining approach for these libraries.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3133907',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Deep Learning for Plant Species Classification Using Leaf Vein Morphometric',\n",
       "  'authors': \"['Jing wei Tan', 'Siow-Wee Chang', 'Sameem Abdul-Kareem', 'Hwa Jen Yap', 'Kien-Thai Yong']\",\n",
       "  'date': 'None',\n",
       "  'source': 'IEEE/ACM Transactions on Computational Biology and Bioinformatics',\n",
       "  'abstract': 'An automated plant species identification system could help botanists and layman in identifying plant species rapidly. Deep learning is robust for feature extraction as it is superior in providing deeper information of images. In this research, a new CNN-based method named D-Leaf was proposed. The leaf images were pre-processed and the features were extracted by using three different Convolutional Neural Network (CNN) models namely pre-trained AlexNet, fine-tuned AlexNet, and D-Leaf. These features were then classified by using five machine learning techniques, namely, Support Vector Machine (SVM), Artificial Neural Network (ANN), k-Nearest-Neighbor (k-NN), Na&#x00EF;ve-Bayes (NB), and CNN. A conventional morphometric method computed the morphological measurements based on the Sobel segmented veins was employed for benchmarking purposes. The D-Leaf model achieved a comparable testing accuracy of 94.88 percent as compared to AlexNet (93.26 percent) and fine-tuned AlexNet (95.54 percent) models. In addition, CNN models performed better than the traditional morphometric measurements (66.55 percent). The features extracted from the CNN are found to be fitted well with the ANN classifier. D-Leaf can be an effective automated system for plant species identification as shown by the experimental results.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/TCBB.2018.2848653',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Classification of Wildlife Based on Transfer Learning',\n",
       "  'authors': \"['Xihao Wang', 'Peihan Li', 'Chengxi Zhu']\",\n",
       "  'date': 'December 2020',\n",
       "  'source': \"ICVIP '20: Proceedings of the 2020 4th International Conference on Video and Image Processing\",\n",
       "  'abstract': 'Wildlife is an important biological resource in China. Classifying images of wildlife through computer technology can help people identify wildlife, which is of great significance to help people understand and protect wildlife. Therefore, this issue is worth studying. Traditional methods mostly use standard Convolutional Neural Networks (CNN) to classify wild animal images, but these methods have disadvantages such as slow computing speed, long time consumption and low accuracy. With an attempt to address such issues, this paper proposes a method based on transfer-learning for classifying wild animal images. By using the pre-trained model it can save a lot of training time. The experimental results on Oregon Wildlife, using a public wildlife data set, show that the method proposed in this paper achieved 99.01% accuracy and is 57.82% more accurate than the standard Convolutional Neural Networks (CNN) method. Moreover, in terms of running time, the method presented in this paper has achieved higher efficiency, and the training time is 50% shorter than the standard method, which proves the superiority of the method proposed in this paper.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3447450.3447487',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Mining User Reviews for Mobile App Comparisons',\n",
       "  'authors': \"['Yuanchun Li', 'Baoxiong Jia', 'Yao Guo', 'Xiangqun Chen']\",\n",
       "  'date': 'None',\n",
       "  'source': 'Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies',\n",
       "  'abstract': 'As the number of mobile apps keeps increasing, users often need to compare many apps, in order to choose one that best fits their needs. Fortunately, as there are so many users sharing an app market, it is likely that some other users with the same preferences have already made the comparisons and shared their opinions. For example, a user may state that an app is better in power consumption than another app in a review, then the review would help other users who care about battery life while choosing apps. This paper presents a method to identify comparative reviews for mobile apps from an app market, which can be used to provide fine-grained app comparisons based on different topics. According to experiments on 5 million reviews from Google Play and manual assessments on 900 reviews, our method is able to identify opinions accurately and provide meaningful comparisons between apps, which could in turn help users find desired apps based on their preferences.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3130935',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Self-supervised Learning for Small Shot COVID-19 Classification',\n",
       "  'authors': \"['Yujie Zhu']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ITCC '21: Proceedings of the 2021 3rd International Conference on Information Technology and Computer Communications\",\n",
       "  'abstract': 'Recently, COVID-19 has become one of the most severe and widespread diseases with an increasing number of infections and deaths. An accurate and high-speed automatic classifier will increase the efficiency of diagnosis and reduce fatigue misdiagnosis. Given the contradiction that many previous classifiers require a large amount of data for training while it is difficult to collect the medical images of COVID-19 with labels, we propose a classification model based on self-supervised learning and transfer learning, which uses rotation and division as labels and then transfers the parameters to the classifier. It solves the overfitting problem caused by insufficient data set and improves the accuracy by nearly 30%',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3473465.3473472',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Clustering stream data by exploring the evolution of density mountain',\n",
       "  'authors': \"['Shufeng Gong', 'Yanfeng Zhang', 'Ge Yu']\",\n",
       "  'date': 'None',\n",
       "  'source': 'Proceedings of the VLDB Endowment',\n",
       "  'abstract': 'Stream clustering is a fundamental problem in many streaming data analysis applications. Comparing to classical batch-mode clustering, there are two key challenges in stream clustering: (i) Given that input data are changing continuously, how to incrementally update their clustering results efficiently? (ii) Given that clusters continuously evolve with the evolution of data, how to capture the cluster evolution activities? Unfortunately, most of existing stream clustering algorithms can neither update the cluster result in real-time nor track the evolution of clusters.In this paper, we propose a stream clustering algorithm EDMStream by exploring the Evolution of Density Mountain. The density mountain is used to abstract the data distribution, the changes of which indicate data distribution evolution. We track the evolution of clusters by monitoring the changes of density mountains. We further provide efficient data structures and filtering schemes to ensure that the update of density mountains is in real-time, which makes online clustering possible. The experimental results on synthetic and real datasets show that, comparing to the state-of-the-art stream clustering algorithms, e.g., D-Stream, DenStream, DBSTREAM and MR-Stream, our algorithm is able to response to a cluster update much faster (say 7-15x faster than the best of the competitors) and at the same time achieve comparable cluster quality. Furthermore, EDMStream successfully captures the cluster evolution activities.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3164135.3164136',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Application of Machine Learning Algorithms in Audit Data Analysis',\n",
       "  'authors': \"['Jianyu Zhou']\",\n",
       "  'date': 'December 2021',\n",
       "  'source': 'ICASIT 2021: 2021 International Conference on Aviation Safety and Information Technology',\n",
       "  'abstract': 'In recent years, with the rapid development of information technology, computer technology and the Internet, various sectors of society have collected a large amount of data. At present, traditional statistical analysis models have limitations. Machine learning system is currently one of the main means to effectively solve problems such as data development and mining. Machine learning is a process of self-improvement using the computer system itself. Therefore, computer applications written by computers can be automated by accumulating practical experience. This article aims to study the application of machine learning algorithms in audit data analysis. Based on the analysis of audit information construction, audit data analysis system design principles, and audit data analysis system non-functional requirements analysis, the audit data analysis system is designed. The association rule algorithm in machine learning is used in audit data mining. Finally, the performance of the system is tested. The test results show that the performance of the system designed in this paper is unified with the pre-demand, which shows that the effectiveness of the system can be satisfied.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3510858.3510881',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Challenges in Classifying Privacy Policies by Machine Learning with Word-based Features',\n",
       "  'authors': \"['Keishiro Fukushima', 'Toru Nakamura', 'Daisuke Ikeda', 'Shinsaku Kiyomoto']\",\n",
       "  'date': 'March 2018',\n",
       "  'source': 'ICCSP 2018: Proceedings of the 2nd International Conference on Cryptography, Security and Privacy',\n",
       "  'abstract': 'In this paper, we discuss challenges when we try to automatically classify privacy policies using machine learning with words as the features. Since it is difficult for general public to understand privacy policies, it is necessary to support them to do that. To this end, the authors believe that machine learning is one of the promising ways because users can grasp the meaning of policies through outputs by a machine learning algorithm. Our final goal is to develop a system which automatically translates privacy policies into privacy labels [1]. Toward this goal, we classify sentences in privacy policies with category labels, using popular machine learning algorithms, such as a naive Bayes classifier.We choose these algorithms because we could use trained classifiers to evaluate keywords appropriate for privacy labels. Therefore, we adopt words as the features of those algorithms. Experimental results show about 85% accuracy. We think that much higher accuracy is necessary to achieve our final goal. By changing learning settings, we identified one reason of low accuracies such that privacy policies include many sentences which are not direct description of information about categories. It seems that such sentences are redundant but maybe they are essential in case of legal documents in order to prevent misinterpreting. Thus, it is important for machine learning algorithms to handle these redundant sentences appropriately.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3199478.3199486',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Learning Interpretable Metric between Graphs: Convex Formulation and Computation with Graph Mining',\n",
       "  'authors': \"['Tomoki Yoshida', 'Ichiro Takeuchi', 'Masayuki Karasuyama']\",\n",
       "  'date': 'July 2019',\n",
       "  'source': \"KDD '19: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining\",\n",
       "  'abstract': 'Graph is a standard approach to modeling structured data. Although many machine learning methods depend on the metric of the input objects, defining an appropriate distance function on graph is still a controversial issue. We propose a novel supervised metric learning method for a subgraph-based distance, called interpretable graph metric learning (IGML). IGML optimizes the distance function in such a way that a small number of important subgraphs can be adaptively selected. This optimization is computationally intractable with naive application of existing optimization algorithms. We construct a graph mining based efficient algorithm to deal with this computational difficulty. Important advantages of our method are 1) guarantee of the optimality from the convex formulation, and 2) high interpretability of results. To our knowledge, none of the existing studies provide an interpretable subgraph-based metric in a supervised manner. In our experiments, we empirically verify superior or comparable prediction performance of IGML to other existing graph classification methods which do not have clear interpretability. Further, we demonstrate usefulness of IGML through some illustrative examples of extracted subgraphs and an example of data analysis on the learned metric space.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3292500.3330845',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Fruit Image Classification Based on MobileNetV2 with Transfer Learning Technique',\n",
       "  'authors': \"['Qian Xiang', 'Xiaodan Wang', 'Rui Li', 'Guoling Zhang', 'Jie Lai', 'Qingshuang Hu']\",\n",
       "  'date': 'October 2019',\n",
       "  'source': \"CSAE '19: Proceedings of the 3rd International Conference on Computer Science and Application Engineering\",\n",
       "  'abstract': \"Fruit image classification is the key technology for robotic picking which can tremendously save costs and effectively improve fruit producer's competitiveness in the international fruit market. In the image classification field, deep learning technologies especially DCNNs are state-of-the-art technologies and have achieved remarkable success. But the requirements of high computation and storage resources prohibit the usages of DCNNs on resource-limited environments such as automatic harvesting robots. Therefore, we need to choose a lightweight neural network to achieve the balance of resource limitations and recognition accuracy. In this paper, a fruit image classification method based on a lightweight neural network MobileNetV2 with transfer learning technique was used to recognize fruit images. We used a MobileNetV2 network pre-trained by ImageNet dataset as a base network and then replace the top layer of the base network with a conventional convolution layer and a Softmax classifier. We applied dropout to the new-added conv2d at the same time to reduce overfitting. The pre-trained MobileNetV2 was used to extract features and the Softmax classifier was used to classify features. We trained this new model in two stages using Adam optimizer of different learning rate. This method finally achieved a classification accuracy of 85.12% in our fruit image dataset including 3670 images of 5 fruits. Compared with other network such as MobileNetV1, InceptionV3 and DenseNet121, this hybrid network implemented by Google open source deep learning framework Tensorflow can make a good compromise between accuracy and speed. Since MobileNetV2 is a lightweight neural network, the method in this paper can be deployed in low-power and limited-computing devices such as mobile phone.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3331453.3361658',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Research on Classification of Scientific and Technological Documents Based on Naive Bayes',\n",
       "  'authors': \"['Hong Zhang', 'Hanshuo Wei', 'Yeye Tang', 'Qiumei Pu']\",\n",
       "  'date': 'February 2019',\n",
       "  'source': \"ICMLC '19: Proceedings of the 2019 11th International Conference on Machine Learning and Computing\",\n",
       "  'abstract': 'Text classification is an important step for text mining in the direction of data mining. Today, text categorization techniques are widely used in various fields, such as user behavior analysis in shopping recommendation systems, and spam filtering, but text categories based on scientific literature are seldom studied. This article uses biological material information. The scientific literature of the aspect is text, and the naive Bayesian method is used to classify the literature into different topic types. It is evaluated through the model test standard in data mining to verify the validity of the method. Finally, the research trend of biological materials A simple analysis was performed.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3318299.3318330',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Environment Agnostic Invariant Risk Minimization for Classification of Sequential Datasets',\n",
       "  'authors': \"['Praveen Venkateswaran', 'Vinod Muthusamy', 'Vatche Isahagian', 'Nalini Venkatasubramanian']\",\n",
       "  'date': 'August 2021',\n",
       "  'source': \"KDD '21: Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining\",\n",
       "  'abstract': 'The generalization of predictive models that follow the standard risk minimization paradigm of machine learning can be hindered by the presence of spurious correlations in the data. Identifying invariant predictors while training on data from multiple environments can influence models to focus on features that have an invariant causal relationship with the target, while reducing the effect of spurious features. Such invariant risk minimization approaches heavily rely on clearly defined environments and data being perfectly segmented into these environments for training. However, in real-world settings, perfect segmentation is challenging to achieve and these environment-aware approaches prove to be sensitive to segmentation errors. In this work, we present an environment-agnostic approach to develop generalizable models for classification tasks in sequential datasets without needing prior knowledge of environments. We show that our approach results in models that can generalize to out-of-distribution data and are not influenced by spurious correlations. We evaluate our approach on real-world sequential datasets from various domains.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3447548.3467324',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Research on Key Technologies of Customer Consultation Hotspots Mining',\n",
       "  'authors': \"['Zhang Mingzhu', 'Cui Xiuqing', 'Chen Yan', 'Liu Yuxi', 'Zhao Jiakui', 'Ouyang Hong', 'Yuan Bao']\",\n",
       "  'date': 'March 2019',\n",
       "  'source': \"ICIAI '19: Proceedings of the 2019 3rd International Conference on Innovation in Artificial Intelligence\",\n",
       "  'abstract': 'In order to better support customer consultation hotspot analysis, improving the accuracy of identifying customer intent is necessary. In this paper, we focus on the research of new words found in one of the key technologies of customer consultation hotspot mining. According to industry characteristics, most of the power vocabulary consists of professional terms, synthetic words, and abbreviations. To solve the new word discovery problem of power industry corpus, we proposed a new word recognition method. This method transforms the problem of new word discovery into the problem of calculating the probability of word formation and the annotation of word position. Especially, based on the analysis and mining of large-scale power industry corpus, the influence of mutual information-information entropy and conditional random field algorithm on the discovery results of new words in the power industry is compared.. Experiment results show that based on the example of 150M power industry documents, the mutual information-information entropy algorithm tends to identify high-frequency power professional vocabulary and synthetic vocabulary. Besides, the conditional random field has an outstanding performance in the mining industry and can better identify the power industry abbreviations.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3319921.3319944',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Design and Application of University Evaluation Performance Intelligent Analysis Based on Data Mining',\n",
       "  'authors': \"['Cheng Wei']\",\n",
       "  'date': 'June 2019',\n",
       "  'source': \"ICMLT '19: Proceedings of the 2019 4th International Conference on Machine Learning Technologies\",\n",
       "  'abstract': \"Colleges and universities usually have problems in implementing their performance evaluation plans due to the deep involvement of subjective factors in making the plan and limited performance evaluation methods. Analysis methods based on big data could help get natural rules existing in data and improve the efficiency of analysis. By using data mining technology to analyze existing information related to teachers in higher learning institutions, the author aims to build a performance evaluation platform based on a multi-angle and multi-tech framework for analyzing teachers' information and optimizing teachers' performance evaluation plan. This intelligent platform could help reduce the influence of subjective factors in implementing performance evaluation plans, expand performance evaluation methods, monitor teaching activities and research achievements dynamically, and offer supports in decision-making while developing a more reasonable performance evaluation plan.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3340997.3341014',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Generative adversarial network for improving deep learning based malware classification',\n",
       "  'authors': \"['Yan Lu', 'Jiang Li']\",\n",
       "  'date': 'December 2019',\n",
       "  'source': \"WSC '19: Proceedings of the Winter Simulation Conference\",\n",
       "  'abstract': 'The generative adversarial network (GAN) had been successfully applied in many domains in the past, the GAN network provides a new approach for solving computer vision, object detection and classification problems by learning, mimicking and generating any distribution of data. One of the difficulties in deep learning-based malware detection and classification tasks is lacking of training malware samples. With insufficient training data the classification performance of the deep model could be compromised significantly. To solve this issue, in this paper, we propose a method which uses the Deep Convolutional Generative Adversarial Network (DCGAN) to generate synthetic malware samples. Our experiment results show that by using the DCGAN generated adversarial synthetic malware samples, the classification accuracy of the classifier --- a 18-layer deep residual network is significantly improved by approximately 6%.',\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3400397.3400445',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'E-commerce Customer Segmentation via Unsupervised Machine Learning',\n",
       "  'authors': \"['Boyu Shen']\",\n",
       "  'date': 'January 2021',\n",
       "  'source': 'CONF-CDS 2021: The 2nd International Conference on Computing and Data Science',\n",
       "  'abstract': 'Customer segmentation through data mining could help companies conduct customer-oriented marketing and build differentiated strategies targeted at diverse customers. However, there has not been a guideline for systematic implementation of customer segmentation given the raw transaction data. This study focuses on a real-world database from an online transaction platform with the purpose to develop a guideline for customer segmentation for the business. Since the raw data are unlabeled, unsupervised machine learning methods are utilized. This study firstly employs the RFM model to create behavioral features; next, the TF-IDF method is applied to the product descriptions to generate product categories; then, K-means clustering algorithm is used to group customers. After customers are grouped, association rules mining by Apriori Algorithm is used to analyze purchased products. Principle Component Analysis (PCA) and T-Distributed Stochastic Neighbor Embedding (T-sne) methods are utilized to reduce the dimension of data in order to create visualizations. Finally, some concrete recommendations for the business based on the results are provided accordingly.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3448734.3450775',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'KNN-fuzzy classification for cloud service selection',\n",
       "  'authors': \"['Humaira Nadeem', 'Imran Mujaddid Rabbani', 'Muhammad Aslam', 'Martinez Enriquez A. M']\",\n",
       "  'date': 'June 2018',\n",
       "  'source': \"ICFNDS '18: Proceedings of the 2nd International Conference on Future Networks and Distributed Systems\",\n",
       "  'abstract': 'Cloud computing is an emerging technology that provides services to its users via Internet. It also allows sharing of resources there by reducing cost, money and space. With the popularity of cloud and its advantages, the trend of information industry shifting towards cloud services is increasing tremendously. Different cloud service providers are there on internet to provide services to the users. These services provided have certain parameters to provide better usage. It is difficult for the users to select a cloud service that is best suited to their requirements. Our proposed approach is based on data mining classification technique with fuzzy logic. Proposed algorithm uses cloud service design factors (security, agility and assurance etc.) and international standards to suggest the cloud service. The main objective of this research is to enable the end cloud users to choose best service as per their requirements and meeting international standards. We test our system with major cloud provider Google, Microsoft and Amazon.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3231053.3231133',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Minimally-Supervised Structure-Rich Text Categorization via Learning on Text-Rich Networks',\n",
       "  'authors': \"['Xinyang Zhang', 'Chenwei Zhang', 'Xin Luna Dong', 'Jingbo Shang', 'Jiawei Han']\",\n",
       "  'date': 'April 2021',\n",
       "  'source': \"WWW '21: Proceedings of the Web Conference 2021\",\n",
       "  'abstract': 'Text categorization is an essential task in Web content analysis. Considering the ever-evolving Web data and new emerging categories, instead of the laborious supervised setting, in this paper, we focus on the minimally-supervised setting that aims to categorize documents effectively, with a couple of seed documents annotated per category. We recognize that texts collected from the Web are often structure-rich, i.e., accompanied by various metadata. One can easily organize the corpus into a text-rich network, joining raw text documents with document attributes, high-quality phrases, label surface names as nodes, and their associations as edges. Such a network provides a holistic view of the corpus’ heterogeneous data sources and enables a joint optimization for network-based analysis and deep textual model training. We therefore propose a novel framework for minimally supervised categorization by learning from the text-rich network. Specifically, we jointly train two modules with different inductive biases – a text analysis module for text understanding and a network learning module for class-discriminative, scalable network learning. Each module generates pseudo training labels from the unlabeled document set, and both modules mutually enhance each other by co-training using pooled pseudo labels. We test our model on two real-world datasets. On the challenging e-commerce product categorization dataset with 683 categories, our experiments show that given only three seed documents per category, our framework can achieve an accuracy of about 92%, significantly outperforming all compared methods; our accuracy is only less than 2% away from the supervised BERT model trained on about 50K labeled documents.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3442381.3450114',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Transductive Event Classification through Heterogeneous Networks',\n",
       "  'authors': \"['Brucce Neves dos Santos', 'Rafael Geraldeli Rossi', 'Ricardo Marcondes Marcacini']\",\n",
       "  'date': 'October 2017',\n",
       "  'source': \"WebMedia '17: Proceedings of the 23rd Brazillian Symposium on Multimedia and the Web\",\n",
       "  'abstract': 'Events can be defined as \"something that occurs at specific place and time associated with some specific actions\". In general, events extracted from news articles and social networks are used to map the information from web to the various phenomena that occur in our physical world. One of the main steps to perform this relationship is the use of machine learning algorithms for event classification, which has received great attention in the web document engineering field in recent years. Traditional machine learning algorithms are based on vector space model representations and supervised classification. However, events are composed of multiple representations such as textual data, temporal information, geographic location and other types of metadata. All these representations are poorly represented together in a vector space model. Moreover, supervised classification requires the labeling of a significant sample of events to construct a training set for learning process, thereby hampering the practical application of event classification. In this paper, we propose a method called TECHN (Transductive Event Classification through Heterogeneous Networks), which considers event metadata as different objects in an heterogeneous network. Besides, the TECHN method has the ability to automatically learn which types of network objects (event metadata) are most efficient in the classification task. In addition, our TECHN method is based on a transductive classification that considers both labeled events and a vast amount of unlabeled events. The experimental results show that TECHN method obtains promising results, especially when we consider different weights of importance for each type of event metadata and a small set of labeled events.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3126858.3126893',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Functional analysis of the 2020 U.S. elections on Twitter and Facebook using machine learning',\n",
       "  'authors': \"['Saud Alashri', 'Turki Alalola']\",\n",
       "  'date': 'December 2020',\n",
       "  'source': \"ASONAM '20: Proceedings of the 12th IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining\",\n",
       "  'abstract': 'Social Networking Sites (SNS), such as Facebook and Twitter, are important tools for political campaigns. A line of related work analyzed political campaigns online. The initial efforts in analyzing campaign discourse functions relied on human analysis, which is time consuming and does not scale well with big data. To address these gaps, we propose a model to detect the type of campaign topics: Policy vs. Character, and how the public (commentators) responded to these messages. The proposed model yielded an accuracy of 78% (F-measure) in detecting post type. Moreover, experimental results show the analysis of commentators linguistic and psychological characteristics.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ASONAM49781.2020.9381302',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Data Mining the Smartphone Manipulation Skills in a Coffee Farming Community: A Step for Risk Analysis',\n",
       "  'authors': \"['Melidiossa V. Pagudpud', 'Thelma D. Palaoag']\",\n",
       "  'date': 'February 2019',\n",
       "  'source': \"ICSCA '19: Proceedings of the 2019 8th International Conference on Software and Computer Applications\",\n",
       "  'abstract': \"The Philippines is largely an agricultural country, and the importance of coffee in the Philippines cannot be undervalued. However, the coffee plantations are generally confronted with various insect pests and diseases. This is the reason why authorities continue to look for solutions through technological applications for coffee farming. Smartphones are becoming a functional tool in agriculture because its mobility served as an advantage to agriculture. However, challenges regarding the level of ICT, particularly of smartphones technology usage among the rural community is low due to limited knowledge and skills. Thus, this study has the primary objective to apply data mining to the smartphone manipulation skills of possible users' dataset in the province of Quirino, Philippines. Specifically, it sought to determine the optimal number of the types of potential users and to identify the different types of possible users and their skills that emerged from the clustering. The result shows that k=4 is the best choice for the dataset. The four clusters formed to represent the four groups of possible users are the good users with 25 instances, skilled users with 35 instances, the users with limited skills with 83 instances and finally, the expert users with 32 instances. Each of the groups possesses their distinct skills which emerged from the clustering technique implemented.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3316615.3316693',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Visualization of educational data mined from the moodle e-learning platform',\n",
       "  'authors': \"['Elias Misailidis', 'Angelos Charitopoulos', 'Maria Rangoussi']\",\n",
       "  'date': 'November 2018',\n",
       "  'source': \"PCI '18: Proceedings of the 22nd Pan-Hellenic Conference on Informatics\",\n",
       "  'abstract': 'Educational data, collected when learners interact with an e-learning platform and stored in the platform databases, are sources of valuable information for the improvement of the educational process, of the learning outcomes and of the learning experience. Data visualization is a domain that has received considerable attention recently, in connection to the Data Mining paradigm. The visualization of educational data mined from e-learning platforms and analyzed to extract answers to research questions or models for student behavior is an open and challenging task. This paper presents the design, development and use of a new plug-in for the visualization of student evaluation data drawn from a moodle platform. The new plug-in is embedded in an active, operational moodle server, with satisfactory visualization results obtained on real-field undergraduate student data collected over the period of the last three academic years.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3291533.3291568',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'API Usage Change Rules Mining based on Fine-grained Call Dependency Analysis',\n",
       "  'authors': \"['Ping Yu', 'Fei Yang', 'Chun Cao', 'Hao Hu', 'Xiaoxing Ma']\",\n",
       "  'date': 'September 2017',\n",
       "  'source': \"Internetware '17: Proceedings of the 9th Asia-Pacific Symposium on Internetware\",\n",
       "  'abstract': 'Software frameworks are widely used in application development. But APIs of a framework may change when it evolves to accommodate new feature requests or to fix bugs. Those changes may break existing client programs of the framework, so client programs need to be migrated to the updated release when the framework evolves. Some technologies (e.g. call dependency analysis) have been proposed to find replacement APIs between the old and new framework releases. However, existing approaches based on call dependency analysis take whole method body as an analysis unit. The context in which a method is called is ignored. In this paper, we present a fine-grained approach named AUC-Miner to infer API usage change rules between two releases of the framework. To take method invocation context into consideration, we propose an approach to get more precise call relationship changes by code splitting. We also analyze indirect method invocations to re-fine call dependency analysis. After elaborating API usage change transactions, we adopt frequent item-set mining to generate API replacement rules. Text similarity and some heuristics to identify evolution of root methods are also applied in the mining progress. The evaluation of AUC-Miner on three popular frameworks shows that its precision is higher than basic call dependency analysis and another API replacement recommendation tool named AURA.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3131704.3131707',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Construction of a bank customer data warehouse and an application of data mining',\n",
       "  'authors': \"['Shaoying Cui', 'Ning Ding']\",\n",
       "  'date': 'February 2018',\n",
       "  'source': \"ICMLC '18: Proceedings of the 2018 10th International Conference on Machine Learning and Computing\",\n",
       "  'abstract': 'In this era of strong competition, data mining can provide effective support to bank operators in their effort to analyze and forecast the real needs of their customers. Applying bank data mining results to the actual business enables banks to develop products that not only meet customer needs but also deliver maximum bank profitability. In this paper, a data mining model for bank customers is established, and after extraction and transformation, the data are loaded into a data warehouse specifically built for that purpose. Using the data mining platform, we conduct multidimensional analysis of the customer information to uncover characteristics of their preferences. The goal of this research is to help facilitate new ideas and methods for the analysis and prediction of bank customer data.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3195106.3195178',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Towards Clustering Validation in Big Data Context',\n",
       "  'authors': \"['Soumeya Zerabi', 'Souham Meshoul', 'Amina Merniz', 'Radia Melal']\",\n",
       "  'date': 'March 2017',\n",
       "  'source': \"BDCA'17: Proceedings of the 2nd international Conference on Big Data, Cloud and Applications\",\n",
       "  'abstract': 'Clustering1is an essential task in many areas such as machine learning, data mining and computer vision among others. Cluster validation aims to assess the quality of partitions obtained by clustering algorithms. Several indexes have been developed for cluster validation purpose. They can be external or internal depending on the availability of ground truth clustering. This paper deals with the issue of cluster validation of large data set. Indeed, in the era of big data this task becomes even more difficult to handle and requires parallel and distributed approaches. In this work, we are interested in external validation indexes. More specifically, this paper proposes a model for purity based cluster validation in parallel and distributed manner using Map-Reduce paradigm in order to be able to scale with increasing dataset sizes. The experimental results show that our proposed model is valid and achieves properly cluster validation of large datasets.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3090354.3090370',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Clustering Data Stream with Rough Set',\n",
       "  'authors': \"['Renxia Wan', 'Yanyan Li']\",\n",
       "  'date': 'October 2019',\n",
       "  'source': \"ICCPR '19: Proceedings of the 2019 8th International Conference on Computing and Pattern Recognition\",\n",
       "  'abstract': 'In this paper, the upper and lower approximations of rough set are introduced to describe the micro-cluster feature in the procedure of clustering uncertain data stream. The proposed algorithm employs presents the micro-cluster timestamp with the time decay and uses agglomerative clustering method to emerge new cluster in the buffer of outliers. Experimental results show that the proposed algorithm can generate natural clusters and outperforms the existing method in term of accuracy.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3373509.3373521',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Predicting Vocational Personality Type from Socio-demographic Features Using Machine Learning Methods',\n",
       "  'authors': \"['Eugenia Bogacheva', 'Filipp Tatarenko', 'Ivan Smetannikov']\",\n",
       "  'date': 'October 2020',\n",
       "  'source': \"CCRIS '20: Proceedings of the 2020 1st International Conference on Control, Robotics and Intelligent System\",\n",
       "  'abstract': 'This study aimed to apply supervised machine learning techniques to one domain of psychological research: vocational interests. Socio-demographic factors can be considered strong predictors of vocational interests, which might have far-reaching practical implications for professional counselling and social network analysis. The dataset used in this study is a collection of answers to the RIASEC (Holland Codes) psychological test. Different Machine Learning architectures were used to predict RIASEC scales using socio-demographic features. The problem was treated as a multioutput regression task, multiclass and multilabel classification. The following models were used: independent regression, regression chains, three-letter code classification, inferring label relations. Models comparison showed that the models that exploit intercorrelations between RIASEC scales yielded the best results.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3437802.3437819',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Application of Data Mining Techniques for Predicting Student Success in English Exit Exam',\n",
       "  'authors': \"['Wichai Puarungroj', 'Narong Boonsirisumpun', 'Pathapong Pongpatrakant', 'Suchada Phromkhot']\",\n",
       "  'date': 'January 2018',\n",
       "  'source': \"IMCOM '18: Proceedings of the 12th International Conference on Ubiquitous Information Management and Communication\",\n",
       "  'abstract': \"The students' English proficiency has become an important requirement for job seeking after graduation. The universities in non-native English speaking countries find their challenges in improving their students' English language skills. Loei Rajabhat University has dealt with this issue for a long time by delivering various English language courses and tests to students. These activities have been carried out repeatedly year by year as a common routine. However, the real status of student success and the predictors of this issue have never known. This research, therefore, explored the available data: English test results (English placement test and exit exam) and data of students, who graduated in 2013, 2014, and 2015 by using the decision tree technique (C4.5). The research constructed and tested classification models for predicting student success in English exit exam. The research results also suggest that the English placement test result was a key attribute for predicting the result of English exit exam. These two attributes were plotted against each other by using the scatter plot where the regression analysis was carried out and the regression line and equation were generated to predict the students' English exit exam scores.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3164541.3164638',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Early Abnormal Heartbeat Multistage Classification by using Decision Tree and K-Nearest Neighbor',\n",
       "  'authors': \"['Mohamad Sabri bin Sinal', 'Eiji Kamioka']\",\n",
       "  'date': 'December 2018',\n",
       "  'source': \"AICCC '18: Proceedings of the 2018 Artificial Intelligence and Cloud Computing Conference\",\n",
       "  'abstract': 'Heart diseases contribute to the highest cause of death around the world particularly for middle aged and elderly people. There are various types of heart disease symptoms. One of the most common types is Arrhythmia which is considered as a dangerous heart condition since the symptom itself may initiate more chronic heart diseases and result in death if it is not treated earlier. However, the detection of Arrhythmia by humans is regarded as a challenging task because the natures of the symptom appear at random times. Therefore, an automatic detection method of abnormal heartbeat in ECG (electrocardiogram) data is needed to overcome the issue. In this paper, a novel multistage classification approach using K-Nearest Neighbor and decision tree of the 3 segments in the ECG cycle is proposed to detect Arrhythmia heartbeat from the early minute of ECG data. Specific attributes based on feature extraction in each heartbeat are used to classify the Normal Sinus Rhythm and Arrhythmia. The experimental result shows that the proposed multistage classification approach is able to detect the Arrhythmia heartbeat with 90.6% accuracy for the P and the Q peak segments, 91.1% accuracy for the Q, R and S peak segments and lastly, 97.7% accuracy for the S and the T peak segments, outperforming the other data mining techniques.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3299819.3299848',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Structure-Aware Deep Learning for Product Image Classification',\n",
       "  'authors': \"['Zhineng Chen', 'Shanshan Ai', 'Caiyan Jia']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Multimedia Computing, Communications, and Applications',\n",
       "  'abstract': 'Automatic product image classification is a task of crucial importance with respect to the management of online retailers. Motivated by recent advancements of deep Convolutional Neural Networks (CNN) on image classification, in this work we revisit the problem in the context of product images with the existence of a predefined categorical hierarchy and attributes, aiming to leverage the hierarchy and attributes to improve classification accuracy. With these structure-aware clues, we argue that more advanced deep models could be developed beyond the flat one-versus-all classification performed by conventional CNNs. To this end, novel efforts of this work include a salient-sensitive CNN that gazes into the product foreground by inserting a dedicated spatial attention module; a multiclass regression-based refinement that is expected to predict more accurately by merging prediction scores from multiple preceding CNNs, each corresponding to a distinct classifier in the hierarchy; and a multitask deep learning architecture that effectively explores correlations among categories and attributes for categorical label prediction. Experimental results on nearly 1 million real-world product images basically validate the effectiveness of the proposed efforts individually and jointly, from which performance gains are observed.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3231742',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Compressing and mining social network data',\n",
       "  'authors': \"['Connor C. J. Hryhoruk', 'Carson K. Leung']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"ASONAM '21: Proceedings of the 2021 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining\",\n",
       "  'abstract': 'Nowadays, social networking is popular. As such, numerous social networking sites (e.g., Facebook, YouTube, Instagram) are generating very large volumes of social data rapidly. Valuable knowledge and information is embedded into these big social data. As the social network can be very sparse, it is awaiting to be (a) compressed via social network data compression and (b) analyzed and mined via social network analysis and mining. We present in this paper a solution for compressing and mining social networks. It gives an interpretable compressed representation of sparse social network, and discovers interesting patterns from the social network. Results of our evaluation show the effectiveness of our solution in explaining the compression and mining of the sparse social network data.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3487351.3489472',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Semi-Supervised Granular Classification Framework for Resource Constrained Short-texts: Towards Retrieving Situational Information During Disaster Events',\n",
       "  'authors': \"['Samujjwal Ghosh', 'Maunendra Sankar Desarkar']\",\n",
       "  'date': 'July 2020',\n",
       "  'source': \"WebSci '20: Proceedings of the 12th ACM Conference on Web Science\",\n",
       "  'abstract': 'During the time of disasters, lots of short-texts are generated containing crucial situational information. Proper extraction and identification of situational information might be useful for various rescue and relief operations. Few specific types of infrequent situational information might be critical. However, obtaining labels for those resource-constrained classes is challenging as well as expensive. Supervised methods pose limited usability in such scenarios. To overcome this challenge, we propose a semi-supervised learning framework which utilizes abundantly available unlabelled data by self-learning. The proposed framework improves the performance of the classifier for resource-constrained classes by selectively incorporating highly confident samples from unlabelled data for self-learning. Incremental incorporation of unlabelled data, as and when they become available, is suitable for ongoing disaster mitigation. Experiments on three disaster-related datasets show that such improvement results in overall performance increase over standard supervised approach.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3394231.3397892',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Machine Learning in Tourism',\n",
       "  'authors': \"['Fatemehalsadat Afsahhosseini', 'Yaseen Al-Mulla']\",\n",
       "  'date': 'September 2020',\n",
       "  'source': \"MLMI '20: Proceedings of the 2020 3rd International Conference on Machine Learning and Machine Intelligence\",\n",
       "  'abstract': 'Machine Learning is a subset of Artificial Intelligence, which is a process of learning from different types of data to make accurate predictions. Data in tourism is various such as Statistics, Photos, Maps, and Texts. Also, each tourism cycle has different stages: Pre, During, and After Trip. In this paper application of machine learning in tourism related data and trip stages are introduced in detailed.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3426826.3426837',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Improvements of fuzzy C-means clustering performance using particle swarm optimization on student grouping based on learning activity in a digital learning media',\n",
       "  'authors': '[\\'Ahmad Afif Supianto\\', \"Nur Sa\\'diyah\", \\'Candra Dewi\\', \\'Retno Indah Rokhmawati\\', \\'Satrio Agung Wicaksono\\', \\'Hanifah Muslimah Az-Zahra\\', \\'Satrio Hadi Wijoyo\\', \\'Yusuke Hayashi\\', \\'Tsukasa Hirashima\\']',\n",
       "  'date': 'November 2020',\n",
       "  'source': \"SIET '20: Proceedings of the 5th International Conference on Sustainable Information Engineering and Technology\",\n",
       "  'abstract': \"The field of learning media has been developing rapidly in recent years, especially in an effort to support students' learning process. The amount of recorded learning process data has also significantly increased. The recorded data represents the students' thinking process in building a solution for a problem. The sheer size of the recorded data proves to be quite a challenge in an effort to mine the students' thinking process, especially when done manually. Additionally, to group the recorded data into clusters is also another form of challenge that needs to be faced. In general, the entire process of mining students' thinking patterns aims to utilize the data to gather hidden information which can also be used to give appropriate and proper feedback to the students. This paper aims to employ the Fuzzy C-Means and Particle Swarm Optimization (FCMPSO) method to cluster students based on their learning activity to a digital learning media and compare its performance to original Fuzzy C-Means (FCM) method. Particle Swarm Optimization (PSO) algorithm is proposed to optimize the performance of the FCM algorithm, in which this algorithm is inherently sensitive towards centroid on the initial clustering process that utilizes the Silhouette coefficient as an evaluation method. Based on the experiments that have been done to 12 assignments, each assignment forms a different number of optimal clusters. This shows that each student faces and uses different strategies to solve their assignments. The formed groups are dominated by two major clusters, namely the high-performance students, and the low-performance students. Additionally, the adaptation of PSO to FCM improves the clustering quality significantly based on the observed average Silhouette coefficient.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3427423.3427449',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Trajectory mining from VMS data for identifying fishing tackles',\n",
       "  'authors': \"['Sathorn Pornsupikul', 'Luepol Pipanmaekaporn', 'Suwatchai Kamonsantiroj']\",\n",
       "  'date': 'September 2017',\n",
       "  'source': \"ICRCA '17: Proceedings of the 2nd International Conference on Robotics, Control and Automation\",\n",
       "  'abstract': 'Automatic identification of fishing equipment has a big impact on fisheries managements and illegal fishing surveillance. For many years, existing approaches to recognize fishing gear types have been proposed based on analysis of Vessel Monitoring System (VMS) data. However, the ship tracking data typically contain irrelevant and meaningless information that can limit their effectiveness. An innovative approach present in this paper is to identify types of fishing equipment from VMS records. Our approach first tries to identify activities of interest in a fishing using an unsupervised way. It then generates possible trajectories for the local movements and performs feature extraction. Two types of trajectory-based features are extracted to describe both global and local characteristics of fishing movement patterns. We finally perform dimension reduction and build the classifier using machine learning. Experiments conducted on historical VMS records from 180 commercial fishing boats with three major types of fishing gears in Thailand show that our approach achieves encouraging performance of recognition rates.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3141166.3141174',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Neyman-Pearson classification: parametrics and sample size requirement',\n",
       "  'authors': \"['Xin Tong', 'Lucy Xia', 'Jiacheng Wang', 'Yang Feng']\",\n",
       "  'date': 'None',\n",
       "  'source': 'The Journal of Machine Learning Research',\n",
       "  'abstract': 'The Neyman-Pearson (NP) paradigm in binary classification seeks classifiers that achieve a minimal type II error while enforcing the prioritized type I error controlled under some user-specified level α. This paradigm serves naturally in applications such as severe disease diagnosis and spam detection, where people have clear priorities among the two error types. Recently, Tong et al. (2018) proposed a nonparametric umbrella algorithm that adapts all scoring-type classification methods (e.g., logistic regression, support vector machines, random forest) to respect the given type I error (i.e., conditional probability of classifying a class 0 observation as class 1 under the 0-1 coding) upper bound α with high probability, without specific distributional assumptions on the features and the responses. Universal the umbrella algorithm is, it demands an explicit minimum sample size requirement on class 0, which is often the more scarce class, such as in rare disease diagnosis applications. In this work, we employ the parametric linear discriminant analysis (LDA) model and propose a new parametric thresholding algorithm, which does not need the minimum sample size requirements on class 0 observations and thus is suitable for small sample applications such as rare disease diagnosis. Leveraging both the existing nonparametric and the newly proposed parametric thresholding rules, we propose four LDA-based NP classifiers, for both low- and high-dimensional settings. On the theoretical front, we prove NP oracle inequalities for one proposed classifier, where the rate for excess type II error benefits from the explicit parametric model assumption. Furthermore, as NP classifiers involve a sample splitting step of class 0 observations, we construct a new adaptive sample splitting scheme that can be applied universally to NP classifiers, and this adaptive strategy reduces the type II error of these classifiers. The proposed NP classifiers are implemented in the R package nproc.',\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3455716.3455728',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Popularity Prediction of Social Media based on Multi-Modal Feature Mining',\n",
       "  'authors': \"['Chih-Chung Hsu', 'Li-Wei Kang', 'Chia-Yen Lee', 'Jun-Yi Lee', 'Zhong-Xuan Zhang', 'Shao-Min Wu']\",\n",
       "  'date': 'October 2019',\n",
       "  'source': \"MM '19: Proceedings of the 27th ACM International Conference on Multimedia\",\n",
       "  'abstract': 'Popularity prediction of social media becomes a more attractive issue in recent years. It consists of multi-type data sources such as image, meta-data, and text information. In order to effectively predict the popularity of a specified post in the social network, fusing multi-feature from heterogeneous data is required. In this paper, a popularity prediction framework for social media based on multi-modal feature mining is presented. First, we discover image semantic features by extracting their image descriptions generated by image captioning. Second, an effective text-based feature engineering is used to construct an effective word-to-vector model. The trained word-to-vector model is used to encode the text information and the semantic image features. Finally, an ensemble regression approach is proposed to aggregate these encoded features and learn the final regressor. Extensive experiments show that the proposed method significantly outperforms other state-of-the-art regression models. We also show that the multi-modal approach could effectively improve the performance in the social media prediction challenge.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3343031.3356064',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Impact-Learning: A Robust Machine Learning Algorithm',\n",
       "  'authors': \"['Md. Kowsher', 'Anik Tahabilder', 'Saydul Akbar Murad']\",\n",
       "  'date': 'July 2020',\n",
       "  'source': \"ICCCM '20: Proceedings of the 8th International Conference on Computer and Communications Management\",\n",
       "  'abstract': 'The ultimate goal of this research paper is to introduce a robust machine learning algorithm called Impact-Learning, which is being used widely to achieve more advanced results on many machine-learning related challenges. Impact learning is a supervised machine learning algorithm for resolving classification and linear or polynomial regression knowledge from examples. It also contributes to analyzing systems for competitive data. This algorithm is unique for being capable of learning from a competition, which is the impact of independent features. In other words, it is trained by the impacts of the features from the intrinsic rate of natural increase (RNI). The input to the Impact Learning is a training set of numerical data. In this work, we used six datasets related to regressions and classifications as the experiment of the Impact Learning, and the comparison indicates that at outperforms other standard machine learning regressions and classifications algorithms such as Random forest tree, SVM, Naive Bayes, Logistic regression and so forth.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3411174.3411185',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Nepal Stock Market Movement Prediction with Machine Learning',\n",
       "  'authors': \"['Shunan Zhao']\",\n",
       "  'date': 'May 2021',\n",
       "  'source': \"ICISDM '21: Proceedings of the 2021 5th International Conference on Information System and Data Mining\",\n",
       "  'abstract': 'Financial market predicting is a popular theme of lots of researches in recent years. However, the majority of previous studies are focus on markets in great countries like China and United States, while some small countries are drawn less attention. To cover this shortage in current literature, we determined to use and compare 17 types of machine learning models to foresee Nepal market in this paper. Based on stock prices, 10 technical indicators were computed as input features. In addition, we also added emotional factors extracted from financial news to improve the prediction performance, which was evaluated by accuracy and F1 score. We predicted whether the closing price would rise or descend after three horizons: 1-day movement, 15-day movement and 30-day movement. From our experiment results, we found that linear SVM and XGBoost perform best and are the best options for further consideration in the trading process.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3471287.3471289',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'DIDroid: Android Malware Classification and Characterization Using Deep Image Learning',\n",
       "  'authors': \"['Abir Rahali', 'Arash Habibi Lashkari', 'Gurdip Kaur', 'Laya Taheri', 'FRANCOIS GAGNON', 'Frédéric Massicotte']\",\n",
       "  'date': 'November 2020',\n",
       "  'source': \"ICCNS '20: Proceedings of the 2020 10th International Conference on Communication and Network Security\",\n",
       "  'abstract': 'The unrivaled threat of android malware is the root cause of various security problems on the internet. Although there are remarkable efforts in detection and classification of android malware based on machine learning techniques, a small number of attempts are made to classify and characterize it using deep learning. Detecting android malware in smartphones is an essential target for cyber community to get rid of menacing malware samples. This paper proposes an image-based deep neural network method to classify and characterize android malware samples taken from a huge malware dataset with 12 prominent malware categories and 191 eminent malware families. This work successfully demonstrates the use of deep image learning to classify and characterize android malware with an accuracy of 93.36% and log loss of less than 0.20 for training and testing set.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3442520.3442522',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Weakly-Supervised Deep Learning for Domain Invariant Sentiment Classification',\n",
       "  'authors': \"['Pratik Kayal', 'Mayank Singh', 'Pawan Goyal']\",\n",
       "  'date': 'January 2020',\n",
       "  'source': 'CoDS COMAD 2020: Proceedings of the 7th ACM IKDD CoDS and 25th COMAD',\n",
       "  'abstract': 'The task of learning a sentiment classification model that adapts well to any target domain, different from the source domain, is a challenging problem. Majority of the existing approaches focus on learning a common representation by leveraging both source and target data during training. In this paper, we introduce a two-stage training procedure that leverages weakly supervised datasets for developing simple lift-and-shift-based predictive models without being exposed to the target domain during the training phase. Experimental results show that transfer with weak supervision from a source domain to various target domains provides performance very close to that obtained via supervised training on the target domain itself.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3371158.3371194',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Amazon SageMaker Clarify: Machine Learning Bias Detection and Explainability in the Cloud',\n",
       "  'authors': \"['Michaela Hardt', 'Xiaoguang Chen', 'Xiaoyi Cheng', 'Michele Donini', 'Jason Gelman', 'Satish Gollaprolu', 'John He', 'Pedro Larroy', 'Xinyu Liu', 'Nick McCarthy', 'Ashish Rathi', 'Scott Rees', 'Ankit Siva', 'ErhYuan Tsai', 'Keerthan Vasist', 'Pinar Yilmaz', 'Muhammad Bilal Zafar', 'Sanjiv Das', 'Kevin Haas', 'Tyler Hill', 'Krishnaram Kenthapadi']\",\n",
       "  'date': 'August 2021',\n",
       "  'source': \"KDD '21: Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining\",\n",
       "  'abstract': 'Understanding the predictions made by machine learning (ML) models and their potential biases remains a challenging and labor-intensive task that depends on the application, the dataset, and the specific model. We present Amazon SageMaker Clarify, an explainability feature for Amazon SageMaker that launched in December 2020, providing insights into data and ML models by identifying biases and explaining predictions. It is deeply integrated into Amazon SageMaker, a fully managed service that enables data scientists and developers to build, train, and deploy ML models at any scale. Clarify supports bias detection and feature importance computation across the ML lifecycle, during data preparation, model evaluation, and post-deployment monitoring. We outline the desiderata derived from customer input, the modular architecture, and the methodology for bias and explanation computations. Further, we describe the technical challenges encountered and the tradeoffs we had to make. For illustration, we discuss two customer use cases. We present our deployment results including qualitative customer feedback and a quantitative evaluation. Finally, we summarize lessons learned, and discuss best practices for the successful adoption of fairness and explanation tools in practice.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3447548.3467177',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'An Improved Pedestrian Motion Tracking System Assisted by Machine Learning?',\n",
       "  'authors': \"['Yuming Chen', 'Wei Li', 'Zhuoyin Si']\",\n",
       "  'date': 'July 2019',\n",
       "  'source': 'DSIT 2019: Proceedings of the 2019 2nd International Conference on Data Science and Information Technology',\n",
       "  'abstract': 'Pedestrian motion tracking based on the Micro-electromechanical inertial measurement unit (MEMS-IMU) possesses the unique advantages in terms of autonomy and anti-disturbance, which has been widely approved as a promising selection in indoor positioning. For the traditional pedestrian indoor positioning system, the accuracy of Zero-velocity interval detection and the stability of Kalman Filter (KF) are the key problem that restricting the development of indoor positioning system. To overcome the drawbacks of traditional Zero-velocity detection algorithm in robustness and adaptability, a novel Zero-velocity detection algorithm based on Support Vector Machine (SVM) is proposed. The construction of the SVM model based on the characteristics of pedestrian foot movement, and genetic algorithm is utilized to online optimizing the key parameters of the SVM. Then, incorporating the concept of strong tracking into the KF to improve the accuracy and stability of KF, a novel Zero Velocity Update (ZUPT) method based on the strong tracking KF (STKF) is presented. The experiment results show that the proposed method has better positioning accuracy and robustness than traditional indoor positioning method.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3352411.3352427',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Mining cross-domain apps for software evolution: a feature-based approach',\n",
       "  'authors': \"['MD Kafil Uddin', 'Qiang He', 'Jun Han', 'Caslon Chua']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"ASE '21: Proceedings of the 36th IEEE/ACM International Conference on Automated Software Engineering\",\n",
       "  'abstract': \"The skyrocketing growth of mobile apps and mobile devices has significantly fueled the competition among app developers. They have leveraged the app store capabilities to analyse app data and identify app improvement opportunities. Existing research has shown that app developers mostly rely on in-domain (i.e., same domain or same app) data to improve their apps. However, relying on in-domain data results in low diversity and lacks novelty in recommended features. In this work, we present an approach that automatically identifies, classifies and ranks relevant popular features from cross-domain apps for recommendation to any given target app. It includes the following three steps: 1) identify cross-domain apps that are relevant to the target app in terms of their features; 2) filter and group semantically the features of the relevant cross-domain apps that are complementary to the target app; 3) rank and prioritize the complementary cross-domain features (in terms of their domain, app, feature and popularity characteristics) for adoption by the target app's developers. We have run extensive experiments on 100 target apps from 10 categories over 15,200 cross-domain apps from 31 categories. The experimental results have shown that our approach to identifying, grouping and ranking complementary cross-domain features for recommendation has achieved an accuracy level of over 89%. Our semantic feature grouping technique has also significantly outperformed two existing baseline techniques. The empirical evaluation validates the efficacy of our approach in providing personalised feature recommendation and enhancing app's user serendipity.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ASE51524.2021.9678514',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Automatic Classification of Exudates in Color Fundus Images Using an Augmented Deep Learning Procedure',\n",
       "  'authors': \"['Lei Wang', 'Ying Huang', 'Bing Lin', 'Wencan Wu', 'Hao Chen', 'Jiantao Pu']\",\n",
       "  'date': 'August 2019',\n",
       "  'source': 'ISICDM 2019: Proceedings of the Third International Symposium on Image Computing and Digital Medicine',\n",
       "  'abstract': 'Automatic classification of hard and soft exudates in color fundus images is very helpful for computer-aided diagnosis of retina related diseases, such as diabetic retinopathy (DR). In this study, we developed a novel method for this purpose based on the emerging deep learning technology known as convolutional neural networks (CNNs) by leveraging its strength of explicitly extracting the underlying image textures. We specifically investigate whether the emphasis of the image characteristic within an exudate spot could improve the classification performance. To verify this, we collected a database of fundus image that contains soft and hard exudates. The exudate regions were cropped from fundus images. There are a total of 550 cropped image patches (275 hard and 275 soft) with a fixed dimension of 128×128 pixels. These patches were further thresholded to exclude image background, resulting in another version of image patches merely containing exudate regions. Each version of image patches was randomly divided into 440 for training and 110 for testing, and then fed into the developed deep learning network in a separate or combinatorial way. Experimental results showed that the classification accuracy of this method was 93.41% when the thresholded version of the dataset was used as an augmented learning procedure, as compared to 90.80% and 87.41% when the original and background excluded datasets were used for training, respectively. This suggests that the augmented CNN can provide more accurate classification performance when the region-of-interest (ROI) and the original images were integrated.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3364836.3364843',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Few-Shot Ensemble Learning for Video Classification with SlowFast Memory Networks',\n",
       "  'authors': \"['Mengshi Qi', 'Jie Qin', 'Xiantong Zhen', 'Di Huang', 'Yi Yang', 'Jiebo Luo']\",\n",
       "  'date': 'October 2020',\n",
       "  'source': \"MM '20: Proceedings of the 28th ACM International Conference on Multimedia\",\n",
       "  'abstract': 'In the era of big data, few-shot learning has recently received much attention in multimedia analysis and computer vision due to its appealing ability of learning from scarce labeled data. However, it has been largely underdeveloped in the video domain, which is even more challenging due to the huge spatial-temporal variability of video data. In this paper, we address few-shot video classification by learning an ensemble of SlowFast networks augmented with memory units. Specifically, we introduce a family of few-shot learners based on SlowFast networks which are used to extract informative features at multiple rates, and we incorporate a memory unit into each network to enable encoding and retrieving crucial information instantly. Furthermore, we propose a choice controller network to leverage the diversity of few-shot learners by learning to adaptively assign a confidence score to each SlowFast memory network, leading to a strong classifier for enhanced prediction. Experimental results on two widely-adopted video datasets demonstrate the effectiveness of the proposed method, as well as its superior performance over the state-of-the-art approaches.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3394171.3416269',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Pornographic content classification using deep-learning',\n",
       "  'authors': \"['André Tabone', 'Kenneth Camilleri', 'Alexandra Bonnici', 'Stefania Cristina', 'Reuben Farrugia', 'Mark Borg']\",\n",
       "  'date': 'August 2021',\n",
       "  'source': \"DocEng '21: Proceedings of the 21st ACM Symposium on Document Engineering\",\n",
       "  'abstract': 'Controlling the distribution of sensitive content such as pornography has become paramount with the ever-growing accessibility to the internet. Manual filtering of such large volumes of data is practically impossible, thus, the automatic detection of said material is sought after by Law Enforcement Agencies (LEAs) and has been tackled in various manners. However, the sorting of flagged pornographic documents is still done manually using scales that describe hierarchical degrees of content severity. In this paper, we address pornography detection by creating a model capable of locating and labelling sexual organs in images and extend this model to perform image classification to provide the user with one of 19 semantically meaningful descriptors of the content. Generating these descriptors serves as a proof of concept before approaching LEAs to work with illegal CSA material and scales such as COPINE. After creating our own custom sexual organ object detection dataset for the task at hand, we achieved an object detection mean average precision score of 63.63% and a top-3 classification accuracy of 87.78%.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3469096.3469867',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Improvements of fuzzy C-means clustering performance using particle swarm optimization on student grouping based on learning activity in a digital learning media',\n",
       "  'authors': '[\\'Ahmad Afif Supianto\\', \"Nur Sa\\'diyah\", \\'Candra Dewi\\', \\'Retno Indah Rokhmawati\\', \\'Satrio Agung Wicaksono\\', \\'Hanifah Muslimah Az-Zahra\\', \\'Satrio Hadi Wijoyo\\', \\'Yusuke Hayashi\\', \\'Tsukasa Hirashima\\']',\n",
       "  'date': 'November 2020',\n",
       "  'source': \"SIET '20: Proceedings of the 5th International Conference on Sustainable Information Engineering and Technology\",\n",
       "  'abstract': \"The field of learning media has been developing rapidly in recent years, especially in an effort to support students' learning process. The amount of recorded learning process data has also significantly increased. The recorded data represents the students' thinking process in building a solution for a problem. The sheer size of the recorded data proves to be quite a challenge in an effort to mine the students' thinking process, especially when done manually. Additionally, to group the recorded data into clusters is also another form of challenge that needs to be faced. In general, the entire process of mining students' thinking patterns aims to utilize the data to gather hidden information which can also be used to give appropriate and proper feedback to the students. This paper aims to employ the Fuzzy C-Means and Particle Swarm Optimization (FCMPSO) method to cluster students based on their learning activity to a digital learning media and compare its performance to original Fuzzy C-Means (FCM) method. Particle Swarm Optimization (PSO) algorithm is proposed to optimize the performance of the FCM algorithm, in which this algorithm is inherently sensitive towards centroid on the initial clustering process that utilizes the Silhouette coefficient as an evaluation method. Based on the experiments that have been done to 12 assignments, each assignment forms a different number of optimal clusters. This shows that each student faces and uses different strategies to solve their assignments. The formed groups are dominated by two major clusters, namely the high-performance students, and the low-performance students. Additionally, the adaptation of PSO to FCM improves the clustering quality significantly based on the observed average Silhouette coefficient.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3427423.3427449',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Rhabdomyosarcoma Histology Classification using Ensemble of Deep Learning Networks',\n",
       "  'authors': \"['Saloni Agarwal', 'Mohamedelfatih Eltigani', 'Osman Abaker', 'Xinyi Zhang', 'Ovidiu Daescu', 'Donald A. Barkauskas', 'Erin R. Rudzinski', 'Patrick Leavey']\",\n",
       "  'date': 'September 2020',\n",
       "  'source': \"BCB '20: Proceedings of the 11th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics\",\n",
       "  'abstract': 'A significant number of machine learning methods have been developed to identify major tumor types in histology images, yet much less is known about automatic classification of tumor subtypes. Rhabdomyosarcoma (RMS), the most common type of soft tissue cancer in children, has several subtypes, the most common being Embryonal, Alveolar, and Spindle Cell. Classifying RMS to the right subtype is critical, since subtypes are known to respond to different treatment protocols. Manual classification requires high expertise and is time consuming due to subtle variance in appearance of histopathology images. In this paper, we introduce and compare machine learning based architectures for automatic classification of Rhabdomyosarcoma into the three major subtypes, from whole slide images (WSI). For training purpose, we only know the class assigned to a WSI, having no manual annotations on the image, while most related work on tumor classification requires manual region or nuclei annotations on WSIs. To predict the class of a new WSI we first divide it into tiles, predict the class of each tile, then use thresholding with soft voting to convert tile level predictions to WSI level prediction. We obtain 94.87% WSI tumor subtype classification accuracy on a large and diverse test dataset. We achieve such accurate classification at 5X magnification level of WSIs, departing from related work, that uses 20X or 10X for best results. A direct advantage of our method is that both training and testing can be performed much faster computationally due to the lower image resolution.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3388440.3412486',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Mining micro-influencers from social media posts',\n",
       "  'authors': \"['Simone Leonardi', 'Diego Monti', 'Giuseppe Rizzo', 'Maurizio Morisio']\",\n",
       "  'date': 'March 2020',\n",
       "  'source': \"SAC '20: Proceedings of the 35th Annual ACM Symposium on Applied Computing\",\n",
       "  'abstract': 'Micro-influencers have triggered the interest of commercial brands, public administrations, and other stakeholders because of their demonstrated capability of sensitizing people within their close reach. However, due to their lower visibility in social media platforms, they are challenging to be identified. This work proposes an approach to automatically detect micro-influencers and to highlight their personality traits and community values by computationally analyzing their writings. We introduce two learning methods to retrieve Five Factor Model and Basic Human Values scores. These scores are then used as feature vectors of a Support Vector Machines classifier. We define a set of rules to create a micro-influencer gold standard dataset of more than two million tweets and we compare our approach with three baseline classifiers. The experimental results favor recall meaning that the approach is inclusive in the identification.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3341105.3373954',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Multi-Task Pharmacovigilance Mining from Social Media Posts',\n",
       "  'authors': \"['Shaika Chowdhury', 'Chenwei Zhang', 'Philip S. Yu']\",\n",
       "  'date': 'April 2018',\n",
       "  'source': \"WWW '18: Proceedings of the 2018 World Wide Web Conference\",\n",
       "  'abstract': 'Social media has grown to be a crucial information source for pharmacovigilance studies where an increasing number of people post adverse reactions to medical drugs that are previously unreported. Aiming to effectively monitor various aspects of Adverse Drug Reactions (ADRs) from diversely expressed social medical posts, we propose a multi-task neural network framework that learns several tasks associated with ADR monitoring with different levels of supervisions collectively. Besides being able to correctly classify ADR posts and accurately extract ADR mentions from online posts, the proposed framework is also able to further understand reasons for which the drug is being taken, known as »indications», from the given social media post. A coverage-based attention mechanism is adopted in our framework to help the model properly identify »phrasal» ADRs and Indications that are attentive to multiple words in a post. Our framework is applicable in situations where limited parallel data for different pharmacovigilance tasks are available. We evaluate the proposed framework on real-world Twitter datasets, where the proposed model outperforms the state-of-the-art alternatives of each individual task consistently.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3178876.3186053',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Structure-Aware Deep Learning for Product Image Classification',\n",
       "  'authors': \"['Zhineng Chen', 'Shanshan Ai', 'Caiyan Jia']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Multimedia Computing, Communications, and Applications',\n",
       "  'abstract': 'Automatic product image classification is a task of crucial importance with respect to the management of online retailers. Motivated by recent advancements of deep Convolutional Neural Networks (CNN) on image classification, in this work we revisit the problem in the context of product images with the existence of a predefined categorical hierarchy and attributes, aiming to leverage the hierarchy and attributes to improve classification accuracy. With these structure-aware clues, we argue that more advanced deep models could be developed beyond the flat one-versus-all classification performed by conventional CNNs. To this end, novel efforts of this work include a salient-sensitive CNN that gazes into the product foreground by inserting a dedicated spatial attention module; a multiclass regression-based refinement that is expected to predict more accurately by merging prediction scores from multiple preceding CNNs, each corresponding to a distinct classifier in the hierarchy; and a multitask deep learning architecture that effectively explores correlations among categories and attributes for categorical label prediction. Experimental results on nearly 1 million real-world product images basically validate the effectiveness of the proposed efforts individually and jointly, from which performance gains are observed.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3231742',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Mining and exploration of attributed graphs: theory and applications',\n",
       "  'authors': \"['Mehdi Kargar', 'Morteza Zihayat', 'Jaroslaw Szlichta']\",\n",
       "  'date': 'November 2019',\n",
       "  'source': \"CASCON '19: Proceedings of the 29th Annual International Conference on Computer Science and Software Engineering\",\n",
       "  'abstract': \"Much of the world's high-quality enterprise and social data are structured or at least semi-structured. This includes large-scale relational databases, knowledge graphs, and social networks. Almost all structured and semi-structured enterprise data can be modeled as attributed graphs, meaning that their nodes are labelled with textual information such as personal data, expertise or interests. Over the past decade, we have witnessed a number of rigorous studies on mining graphs for interesting patterns (e.g., subgraphs), but we have not seen much progress in pattern mining over attributed graphs. In this workshop, we present recent progress in building efficient and effective systems to empower users to mine and explore attributed graphs and how we incorporate IBM technologies to build such systems. First, as for any data-driven platform, we have to make sure that our system is built based on reliable data. Therefore, we present challenges and different approaches of cleaning attributed graphs. Then, we provide different search systems to explore attributed graphs, with a focus on systems that assist non-technical users (the systems that provide a Google-like search experience). Finally, our focus will be on mining attributed graphs for different purposes including user modeling and significant pattern discovery. We also discuss a variety of applications of such systems in different domains, and specifically how we use these systems to improve the user experience in IBM products.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3370272.3370339',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Lateralized learning for robustness against adversarial attacks in a visual classification system',\n",
       "  'authors': \"['Abubakar Siddique', 'Will N. Browne', 'Gina M. Grimshaw']\",\n",
       "  'date': 'June 2020',\n",
       "  'source': \"GECCO '20: Proceedings of the 2020 Genetic and Evolutionary Computation Conference\",\n",
       "  'abstract': 'Deep learning is an important field of machine learning. It is playing a critical role in a variety of applications ranging from self-driving cars to security and surveillance. However, deep networks have deep flaws. For example, they are highly vulnerable to adversarial attacks. One reason may be the homogeneous nature of their knowledge representation, which allows a single disruptive pattern to cause miss-classification. Biological intelligence has lateral asymmetry, which allows heterogeneous, modular learning at different levels of abstraction, enabling different representations of the same object. This work aims to incorporate lateralization and modular learning at different levels of abstraction in an evolutionary machine learning system. The results of image classification tasks show that the lateralized system efficiently learns hierarchical distributions of knowledge, demonstrating performance that is similar to (or better than) other state-of-the-art deep systems as it reasons using multiple representations. Crucially, the novel system outperformed all the state-of-the-art deep models for the classification of normal and adversarial images by 0.43% -- 2.56% and 2.15% -- 25.84%, respectively. Lateralisation enabled the system to exhibit robustness beyond previous work, which advocates for the creation of data sets that enable components of objects and the objects themselves to be learned specifically or in an end-to-end manner.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3377930.3390164',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Impact-Learning: A Robust Machine Learning Algorithm',\n",
       "  'authors': \"['Md. Kowsher', 'Anik Tahabilder', 'Saydul Akbar Murad']\",\n",
       "  'date': 'July 2020',\n",
       "  'source': \"ICCCM '20: Proceedings of the 8th International Conference on Computer and Communications Management\",\n",
       "  'abstract': 'The ultimate goal of this research paper is to introduce a robust machine learning algorithm called Impact-Learning, which is being used widely to achieve more advanced results on many machine-learning related challenges. Impact learning is a supervised machine learning algorithm for resolving classification and linear or polynomial regression knowledge from examples. It also contributes to analyzing systems for competitive data. This algorithm is unique for being capable of learning from a competition, which is the impact of independent features. In other words, it is trained by the impacts of the features from the intrinsic rate of natural increase (RNI). The input to the Impact Learning is a training set of numerical data. In this work, we used six datasets related to regressions and classifications as the experiment of the Impact Learning, and the comparison indicates that at outperforms other standard machine learning regressions and classifications algorithms such as Random forest tree, SVM, Naive Bayes, Logistic regression and so forth.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3411174.3411185',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Identifying important citations using contextual information from full text',\n",
       "  'authors': \"['Saeed-Ul Hassan', 'Anam Akram', 'Peter Haddawy']\",\n",
       "  'date': 'June 2017',\n",
       "  'source': \"JCDL '17: Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries\",\n",
       "  'abstract': 'In this paper we address the problem of classifying cited work into important and non-important to the developments presented in a research publication. This task is vital for the algorithmic techniques that detect and follow emerging research topics and to qualitatively measure the impact of publications in increasingly growing scholarly big data. We consider cited work as important to a publication if that work is used or extended in some way. If a reference is cited as background work or for the purpose of comparing results, the cited work is considered to be non-important. By employing five classification techniques (Support Vector Machine, Naïve Bayes, Decision Tree, K-Nearest Neighbors and Random Forest) on an annotated dataset of 465 citations, we explore the effectiveness of eight previously published features and six novel features (including context based, cue words based and textual based). Within this set, our new features are among the best performing. Using the Random Forest classifier we achieve an overall classification accuracy of 0.91 AUC.',\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3200334.3200340',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Negative results on mining crypto-API usage rules in Android apps',\n",
       "  'authors': \"['Jun Gao', 'Pingfan Kong', 'Li Li', 'Tegawendé F. Bissyandé', 'Jacques Klein']\",\n",
       "  'date': 'May 2019',\n",
       "  'source': \"MSR '19: Proceedings of the 16th International Conference on Mining Software Repositories\",\n",
       "  'abstract': 'Android app developers recurrently use crypto-APIs to provide data security to app users. Unfortunately, misuse of APIs only creates an illusion of security and even exposes apps to systematic attacks. It is thus necessary to provide developers with a statically-enforceable list of specifications of crypto-API usage rules. On the one hand, such rules cannot be manually written as the process does not scale to all available APIs. On the other hand, a classical mining approach based on common usage patterns is not relevant in Android, given that a large share of usages include mistakes. In this work, building on the assumption that \"developers update API usage instances to fix misuses\", we propose to mine a large dataset of updates within about 40 000 real-world app lineages to infer API usage rules. Eventually, our investigations yield negative results on our assumption that API usage updates tend to correct misuses. Actually, it appears that updates that fix misuses may be unintentional: the same misuses patterns are quickly re-introduced by subsequent updates.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/MSR.2019.00065',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'ECG Heartbeat Classification: An Exploratory Study',\n",
       "  'authors': \"['Loren Kersey', 'Kat Lilly', 'Noel Park']\",\n",
       "  'date': 'December 2018',\n",
       "  'source': \"AIW'18: Proceedings of the Australasian Joint Conference on Artificial Intelligence - Workshops\",\n",
       "  'abstract': 'This paper presents an exploratory study of the ECG heart beat classification problem that employs a number of machine learning models. Using the feature schemes of the state-of-the-art, we investigate the classification both for individual and cross-patient scenarios. A cascade system made of a neural network and a support vector machine is proposed and gives competitive performance on a benchmark dataset.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3314487.3314491',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Improving Classification Performance of Deep Learning Models Using Bio-Inspired Computing',\n",
       "  'authors': \"['Vaishali Baviskar', 'Madhushi Verma', 'Pradeep Chatterjee']\",\n",
       "  'date': 'August 2021',\n",
       "  'source': 'IC3-2021: Proceedings of the 2021 Thirteenth International Conference on Contemporary Computing',\n",
       "  'abstract': 'Deep learning models have paved the way towards generating high-efficiency classification systems for multiple applications. These applications include lung disease classification, electrocardiogram classification, electroencephalogram classification; forest cover classification, etc. All these applications rely on efficient feature selection capabilities of deep learning models. Models like convolutional neural network (CNN), recurrent neural networks (RNNs), long-short-term-memory (LSTM) etc. are used for this purpose. These models tend to evaluate all possible feature combinations via iterative window-based feature processing. Thereby trying to cover indefinite number of feature combinations in order to classify a definite number of features into a definite number of classes. All these models have a stopping-criteria, which depends upon the error rate difference of previous current iteration. If the error rate is less than a particular threshold, and number of iterations are above a certain predefined value, then training of these networks is stopped. This property of deep learning models limits their real-time performance, because training stops even if the accuracy is lower than expected. The reason for this low accuracy is high dimensionality of search space, due to which selection of the most optimum features is skipped. In order to reduce the probability of such conditions, this text proposes a bio-inspired Genetic Algorithm model for accuracy-based feature selection. The selected features are given to different deep learning models like LSTM RNN, and their internal performance is evaluated. Here, heart failure disease dataset from kaggle is used, and it is observed that due to pre-feature selection process, overall accuracy of these models is improved by 10, while precision, recall fMeasure scores are improved by 15 for heart disease data sets. The specificity and sensitivity performance is improved by 20 when compared with RNN and LSTM models individually.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3474124.3474174',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Classification of Contractual Conflicts via Learning of Semantic Representations',\n",
       "  'authors': \"['João Paulo Aires', 'Roger Granada', 'Juarez Monteiro', 'Rodrigo Coelho Barros', 'Felipe Meneguzzi']\",\n",
       "  'date': 'May 2019',\n",
       "  'source': \"AAMAS '19: Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems\",\n",
       "  'abstract': 'Contracts are the main medium through which parties formalize their trade relations, be they the exchange of goods or the specification of mutual obligations. While electronic contracts allow automated processes to verify their correctness, most agreements in the real world are still written in natural language, which need substantial human revision effort to eliminate possible conflicting statements in long and complex contracts. In this paper, we formalize a typology of conflict types between clauses suitable for machine learning and develop techniques to review contracts by learning to identify and classify such conflicts, facilitating the task of contract revision. We evaluate the effectiveness of our techniques using a manually annotated contract conflict corpus with results close to the current state-of-the-art for conflict identification, while introducing a more complex classification task of such conflicts for which our method surpasses the state-of-the art method.',\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3306127.3331911',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Semi-Supervised Granular Classification Framework for Resource Constrained Short-texts: Towards Retrieving Situational Information During Disaster Events',\n",
       "  'authors': \"['Samujjwal Ghosh', 'Maunendra Sankar Desarkar']\",\n",
       "  'date': 'July 2020',\n",
       "  'source': \"WebSci '20: Proceedings of the 12th ACM Conference on Web Science\",\n",
       "  'abstract': 'During the time of disasters, lots of short-texts are generated containing crucial situational information. Proper extraction and identification of situational information might be useful for various rescue and relief operations. Few specific types of infrequent situational information might be critical. However, obtaining labels for those resource-constrained classes is challenging as well as expensive. Supervised methods pose limited usability in such scenarios. To overcome this challenge, we propose a semi-supervised learning framework which utilizes abundantly available unlabelled data by self-learning. The proposed framework improves the performance of the classifier for resource-constrained classes by selectively incorporating highly confident samples from unlabelled data for self-learning. Incremental incorporation of unlabelled data, as and when they become available, is suitable for ongoing disaster mitigation. Experiments on three disaster-related datasets show that such improvement results in overall performance increase over standard supervised approach.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3394231.3397892',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Negative Confidence-Aware Weakly Supervised Binary Classification for Effective Review Helpfulness Classification',\n",
       "  'authors': \"['Xi Wang', 'Iadh Ounis', 'Craig Macdonald']\",\n",
       "  'date': 'October 2020',\n",
       "  'source': \"CIKM '20: Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management\",\n",
       "  'abstract': \"The incompleteness of positive labels and the presence of many unlabelled instances are common problems in binary classification applications such as in review helpfulness classification. Various studies from the classification literature consider all unlabelled instances as negative examples. However, a classification model that learns to classify binary instances with incomplete positive labels while assuming all unlabelled data to be negative examples will often generate a biased classifier. In this work, we propose a novel Negative Confidence-aware Weakly Supervised approach (NCWS), which customises a binary classification loss function by discriminating the unlabelled examples with different negative confidences during the classifier's training. NCWS allows to effectively, unbiasedly identify and separate positive and negative instances after its integration into various binary classifiers from the literature, including SVM, CNN and BERT-based classifiers. We use the review helpfulness classification as a test case for examining the effectiveness of our NCWS approach. We thoroughly evaluate NCWS by using three different datasets, namely one from Yelp (venue reviews), and two from Amazon (Kindle and Electronics reviews). Our results show that NCWS outperforms strong baselines from the literature including an existing SVM-based approach (i.e. SVM-P), the positive and unlabelled learning-based approach (i.e. C-PU) and the positive confidence-based approach (i.e. P-conf) in addressing the classifier's bias problem. Moreover, we further examine the effectiveness of NCWS by using its classified helpful reviews in a state-of-the-art review-based venue recommendation model (i.e. DeepCoNN) and demonstrate the benefits of using NCWS in enhancing venue recommendation effectiveness in comparison to the baselines.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3340531.3411978',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A Hybrid Similarity Measure Based on Binary and Decimal Data for Data Mining',\n",
       "  'authors': \"['Soyeong Jeong']\",\n",
       "  'date': 'April 2019',\n",
       "  'source': \"ICCAI '19: Proceedings of the 2019 5th International Conference on Computing and Artificial Intelligence\",\n",
       "  'abstract': \"We suggest a new similarity measure to improve the quality of data mining, especially for recommender system. A similarity measure is widely used for classification, clustering, anomaly detection and so on. Many recommender systems predict unrated score through clustering similar users. This method is so called collaborative filtering(CF), which is being widely used. In CF, how to define a similarity measure is a major concern. Conventional measures based on Pearson Correlation Coefficient(PCC) are hard to reflect the implicit and explicit information at the same time. We propose a hybrid similarity measure, named BD PCC, which is a type of PCC, named after the first letter of 'Binary' and 'Decimal' types respectively. As we suggest from its name, BD PCC is defined by concatenating two PCCs on two different types of data. Although other hybrid measures need some processes to concatenate, BD PCC is free from scale issue. Because it consists of both PCCs unlike other hybrid measures consisting of values in different ranges. Since PCC for binary data can be defined if the user bought at least one item, BD PCC relieves the sparsity of data. We tested the proposed similarity measure in recommender systems and the prediction accuracy has been improved for real data sets, MovieLens 100K[8], MovieLens 1M[8], MovieLens latest small[8], and FilmTrust 35K[9].\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3330482.3330520',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Exploring the limits of transfer learning with a unified text-to-text transformer',\n",
       "  'authors': \"['Colin Raffel', 'Noam Shazeer', 'Adam Roberts', 'Katherine Lee', 'Sharan Narang', 'Michael Matena', 'Yanqi Zhou', 'Wei Li', 'Peter J. Liu']\",\n",
       "  'date': 'None',\n",
       "  'source': 'The Journal of Machine Learning Research',\n",
       "  'abstract': 'Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pretraining objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.',\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3455716.3455856',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A Behavior-cluster Based Imbalanced Classification Method for Credit Card Fraud Detection',\n",
       "  'authors': \"['Qi Li', 'Yu Xie']\",\n",
       "  'date': 'July 2019',\n",
       "  'source': 'DSIT 2019: Proceedings of the 2019 2nd International Conference on Data Science and Information Technology',\n",
       "  'abstract': 'Credit card fraud detection has been paid more and more attention by researchers. The credit card transactions are represented by highly imbalanced data sets. The number of genuine transactions is far more than fraudulent transactions, which will greatly affect the detection of fraud. Existing methods mainly consider how to balance the two classes only based on data volume, without considering the complexity of user behavior in credit card transactions, that is, the behavior noise. In this paper, we propose a behavior-cluster based imbalanced classification method. The main idea is to divide user behaviors into several group behaviors, remove behavior noise, and then hierarchical sampling. Experiments on a large scale credit card transaction data provided by a financial institution and 18 UCI data sets show that our method is superior to the existing method.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3352411.3352433',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Adversarial classification: necessary conditions and geometric flows',\n",
       "  'authors': \"['Nicol´s García Trillos', 'Ryan Murray']\",\n",
       "  'date': 'None',\n",
       "  'source': 'The Journal of Machine Learning Research',\n",
       "  'abstract': 'We study a version of adversarial classification where an adversary is empowered to corrupt data inputs up to some distance ε, using tools from variational analysis. In particular, we describe necessary conditions associated with the optimal classifier subject to such an adversary. Using the necessary conditions, we derive a geometric evolution equation which can be used to track the change in classification boundaries as ε varies. This evolution equation may be described as an uncoupled system of differential equations in one dimension, or as a mean curvature type equation in higher dimension. In one dimension, and under mild assumptions on the data distribution, we rigorously prove that one can use the initial value problem starting from ε = 0, which is simply the Bayes classifier, in order to solve for the global minimizer of the adversarial problem for small values of ε. In higher dimensions we provide a similar result, albeit conditional to the existence of regular solutions of the initial value problem. In the process of proving our main results we obtain a result of independent interest connecting the original adversarial problem with an optimal transport problem under no assumptions on whether classes are balanced or not. Numerical examples illustrating these ideas are also presented.',\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3586589.3586776',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Rock classification model based on transfer learning and convolutional neural network',\n",
       "  'authors': \"['Huaian Yi', 'Jinzhao Su', 'Runji Fang']\",\n",
       "  'date': 'July 2021',\n",
       "  'source': \"ICIIP '21: Proceedings of the 6th International Conference on Intelligent Information Processing\",\n",
       "  'abstract': 'None',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3480571.3480595',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Weakly-supervised Learning Using Pretraining for Classification in HER2 Immunohistochemistry Image of Breast Cancer',\n",
       "  'authors': \"['Zhengnan Wang', 'Yeting Ma', 'Yali Zheng', 'Peiqin Feng', 'Fangbo Yu']\",\n",
       "  'date': 'March 2021',\n",
       "  'source': \"ICMAI '21: Proceedings of the 2021 6th International Conference on Mathematics and Artificial Intelligence\",\n",
       "  'abstract': 'Recently supervised deep learning method has achieved good performance in image classification tasks. However, it is very difficult to annotate pathological images accurately for supervised learning tasks. So, the limited amount of labeled data brings great challenges to the supervised learning model. In this paper we propose a weakly-supervised learning method which combines the pretraining technology of transfer learning with deep learning in the HER2 immunohistochemistry (IHC) pathological image classification task of breast cancer. It is worth mentioning that on the network architecture of VGG16 model, we train the model with three different images from pathological images, apply the pretraining model to the HER2 IHC image classification task of breast cancer. The experimental results show that the weakly-supervised learning implemented by the pretraining technology of transfer learning can significantly improve the performance of HER2 IHC pathological image classification task.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3460569.3460586',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Clustering Individual Transactional Data for Masses of Users',\n",
       "  'authors': \"['Riccardo Guidotti', 'Anna Monreale', 'Mirco Nanni', 'Fosca Giannotti', 'Dino Pedreschi']\",\n",
       "  'date': 'August 2017',\n",
       "  'source': \"KDD '17: Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining\",\n",
       "  'abstract': 'Mining a large number of datasets recording human activities for making sense of individual data is the key enabler of a new wave of personalized knowledge-based services. In this paper we focus on the problem of clustering individual transactional data for a large mass of users. Transactional data is a very pervasive kind of information that is collected by several services, often involving huge pools of users. We propose txmeans, a parameter-free clustering algorithm able to efficiently partitioning transactional data in a completely automatic way. Txmeans is designed for the case where clustering must be applied on a massive number of different datasets, for instance when a large set of users need to be analyzed individually and each of them has generated a long history of transactions. A deep experimentation on both real and synthetic datasets shows the practical effectiveness of txmeans for the mass clustering of different personal datasets, and suggests that txmeans outperforms existing methods in terms of quality and efficiency. Finally, we present a personal cart assistant application based on txmeans',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3097983.3098034',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Improved classification rates for localized SVMs',\n",
       "  'authors': \"['Ingrid Blaschzyk', 'Ingo Steinwart']\",\n",
       "  'date': 'None',\n",
       "  'source': 'The Journal of Machine Learning Research',\n",
       "  'abstract': 'Localized support vector machines solve SVMs on many spatially defined small chunks and besides their computational benefit compared to global SVMs one of their main characteristics is the freedom of choosing arbitrary kernel and regularization parameter on each cell. We take advantage of this observation to derive global learning rates for localized SVMs with Gaussian kernels and hinge loss. It turns out that our rates outperform under suitable sets of assumptions known classification rates for localized SVMs, for global SVMs, and other learning algorithms based on e.g., plug-in rules or trees. The localized SVM rates are achieved under a set of margin conditions, which describe the behavior of the data-generating distribution, and no assumption on the existence of a density is made. Moreover, we show that our rates are obtained adaptively, that is without knowing the margin parameters in advance. The statistical analysis of the excess risk relies on a simple partitioning based technique, which splits the input space into a subset that is close to the decision boundary and into a subset that is sufficiently far away. A crucial condition to derive then improved global rates is a margin condition that relates the distance to the decision boundary to the amount of noise.',\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3586589.3586754',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'A machine learning model to classify the feature model maintainability',\n",
       "  'authors': \"['Publio Silva', 'Carla I. M. Bezerra', 'Ivan Machado']\",\n",
       "  'date': 'September 2021',\n",
       "  'source': \"SPLC '21: Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A\",\n",
       "  'abstract': \"Software Product Lines (SPL) are generally specified using a Feature Model (FM), an artifact designed in the early stages of the SPL development life cycle. This artifact can quickly become too complex, which makes it challenging to maintain an SPL. Therefore, it is essential to evaluate the artifact's maintainability continuously. The literature brings some approaches that evaluate FM maintainability through the aggregation of maintainability measures. Machine Learning (ML) models can be used to create these approaches. They can aggregate the values of independent variables into a single target data, also called a dependent variable. Besides, when using white-box ML models, it is possible to interpret and explain the ML model results. This work proposes white-box ML models intending to classify the FM maintainability based on 15 measures. To build the models, we performed the following steps: (i) we compared two approaches to evaluate the FM maintainability through a human-based oracle of FM maintainability classifications; (ii) we used the best approach to pre-classify the ML training dataset; (iii) we generated three ML models and compared them against classification accuracy, precision, recall, F1 and AUC-ROC; and, (iv) we used the best model to create a mechanism capable of providing improvement indicators to domain engineers. The best model used the decision tree algorithm that obtained accuracy, precision, and recall of 0.81, F1-Score of 0.79, and AUC-ROC of 0.91. Using this model, we could reduce the number of measures needed to evaluate the FM maintainability from 15 to 9 measures.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3461001.3471152',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Multi-View Mammographic Density Classification by Dilated and Attention-Guided Residual Learning',\n",
       "  'authors': \"['Cheng Li', 'Jingxu Xu', 'Qiegen Liu', 'Yongjin Zhou', 'Lisha Mou', 'Zuhui Pu', 'Yong Xia', 'Hairong Zheng', 'Shanshan Wang']\",\n",
       "  'date': 'None',\n",
       "  'source': 'IEEE/ACM Transactions on Computational Biology and Bioinformatics',\n",
       "  'abstract': 'Breast density is widely adopted to reflect the likelihood of early breast cancer development. Existing methods of mammographic density classification either require steps of manual operations or achieve only moderate classification accuracy due to the limited model capacity. In this study, we present a radiomics approach based on dilated and attention-guided residual learning for the task of mammographic density classification. The proposed method was instantiated with two datasets, one clinical dataset and one publicly available dataset, and classification accuracies of 88.7 and 70.0 percent were obtained, respectively. Although the classification accuracy of the public dataset was lower than the clinical dataset, which was very likely related to the dataset size, our proposed model still achieved a better performance than the naive residual networks and several recently published deep learning-based approaches. Furthermore, we designed a multi-stream network architecture specifically targeting at analyzing the multi-view mammograms. Utilizing the clinical dataset, we validated that multi-view inputs were beneficial to the breast density classification task with an increase of at least 2.0 percent in accuracy and the different views lead to different model classification capacities. Our method has a great potential to be further developed and applied in computer-aided diagnosis systems. Our code is available at <uri>https://github.com/lich0031/Mammographic_Density_Classification</uri>.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/TCBB.2020.2970713',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Diagnosis of Methylmalonic Acidemia using Machine Learning Methods',\n",
       "  'authors': \"['Xin Li', 'Xiaoxing Yang', 'Wushao Wen']\",\n",
       "  'date': 'June 2019',\n",
       "  'source': \"ICMLT '19: Proceedings of the 2019 4th International Conference on Machine Learning Technologies\",\n",
       "  'abstract': \"Methylmalonic acidemia (MMA) is an autosomal recessive metabolic disorder. Traditional diagnosis needs physicians' personal level of professional medical knowledge and clinical experience. In this paper, we employ machine learning methods to diagnose MMA based on patients' laboratory blood tests and laboratory urine tests, in order to make a timely diagnosis and reduce dependence on physicians' personal level of professional medical knowledge and clinical experience. By comparing different machine learning algorithms for diagnosing MMA, we obtain the following conclusions: (a) machine learning methods can perform well for diagnosing MMA (all established predictive models obtain high accuracies and AUC values which are greater than 0.85 over all data sets, and some of these results are even more than 0.98); (b) random forest algorithm performs best among the compared algorithms; and (c) diagnosis based on the data combining both urine tests and blood tests is better than diagnosis based on single test alone in general. The conclusions show that applying machine learning algorithms to the diagnosis of MMA can achieve good performance. Thus, it is credible to build machine learning models to give an initial diagnosis without professional medical knowledge.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3340997.3341000',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Machine Learning-based Pipeline for Enterprise Cross-department Conflict Data Management',\n",
       "  'authors': \"['Guannan Wang', 'Wenjia Wang', 'Xueliang Song', 'Jiayi Chen']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': 'ICCBD 2021: 2021 4th International Conference on Computing and Big Data',\n",
       "  'abstract': 'None',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3507524.3507530',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Classifying News Media Coverage for Corruption Risks Management with Deep Learning and Web Intelligence',\n",
       "  'authors': \"['Albert Weichselbraun', 'Sandro Hörler', 'Christian Hauser', 'Anina Havelka']\",\n",
       "  'date': 'June 2020',\n",
       "  'source': 'WIMS 2020: Proceedings of the 10th International Conference on Web Intelligence, Mining and Semantics',\n",
       "  'abstract': \"A substantial number of international corporations have been affected by corruption. The research presented in this paper introduces the Integrity Risks Monitor, an analytics dashboard that applies Web Intelligence and Deep Learning to english and german-speaking documents for the task of (i) tracking and visualizing past corruption management gaps and their respective impacts, (ii) understanding present and past integrity issues, (iii) supporting companies in analyzing news media for identifying and mitigating integrity risks. Afterwards, we discuss the design, implementation, training and evaluation of classification components capable of identifying English documents covering the integrity topic of corruption. Domain experts created a gold standard dataset compiled from Anglo-American media coverage on corruption cases that has been used for training and evaluating the classifier. The experiments performed to evaluate the classifiers draw upon popular algorithms used for text classification such as Naïve Bayes, Support Vector Machines (SVM) and Deep Learning architectures (LSTM, BiLSTM, CNN) that draw upon different word embeddings and document representations. They also demonstrate that although classical machine learning approaches such as Naïve Bayes struggle with the diversity of the media coverage on corruption, state-of-the art Deep Learning models perform sufficiently well in the project's context.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3405962.3405988',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Nepal Stock Market Movement Prediction with Machine Learning',\n",
       "  'authors': \"['Shunan Zhao']\",\n",
       "  'date': 'May 2021',\n",
       "  'source': \"ICISDM '21: Proceedings of the 2021 5th International Conference on Information System and Data Mining\",\n",
       "  'abstract': 'Financial market predicting is a popular theme of lots of researches in recent years. However, the majority of previous studies are focus on markets in great countries like China and United States, while some small countries are drawn less attention. To cover this shortage in current literature, we determined to use and compare 17 types of machine learning models to foresee Nepal market in this paper. Based on stock prices, 10 technical indicators were computed as input features. In addition, we also added emotional factors extracted from financial news to improve the prediction performance, which was evaluated by accuracy and F1 score. We predicted whether the closing price would rise or descend after three horizons: 1-day movement, 15-day movement and 30-day movement. From our experiment results, we found that linear SVM and XGBoost perform best and are the best options for further consideration in the trading process.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3471287.3471289',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'The Need for a Multimodal Means of Effective Digital Learning through Data Mining and Institutional Knowledge Repository: A Proposed System for Polytechnics in Northern Nigeria',\n",
       "  'authors': \"['Aminu Abbas Gumel', 'Abdullahi Bashir Abdullahi', 'Ugochukwu Matthew O.']\",\n",
       "  'date': 'April 2019',\n",
       "  'source': \"ICCTA '19: Proceedings of the 2019 5th International Conference on Computer and Technology Applications\",\n",
       "  'abstract': 'Educational institutions, information and knowledge givers in Nigeria are faced with numerous challenges of carrying everyone on the same page through the lack of proper digital learning and knowledge preservation tools simply regarded as a digital library. In any learning institution, digital libraries and institutional repositories are the keys to a qualitative knowledge stockpiling and delivery. The main aim of the paper is to propose a viable means of effective digital learning for the Polytechnics in Northern Nigeria. This paper also attempted to highlight the major information system components needed to develop a digital learning that will fit the Institutions with an approach through Multimedia data mining and suggests an Institutional Knowledge repository for proper academic data storage such as journals, thesis, lecture notes, dissertations, undergraduate student projects etc. The proposed system is expected to serve as a step towards bridging the gap between the polytechnics learning system and the tertiary institutions in the developed nations.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3323933.3324068',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Machine Learning Algorithms on Botnet Traffic: Ensemble and Simple Algorithms',\n",
       "  'authors': \"['Rob McKay', 'Brian Pendleton', 'James Britt', 'Ben Nakhavanit']\",\n",
       "  'date': 'March 2019',\n",
       "  'source': \"ICCDA '19: Proceedings of the 2019 3rd International Conference on Compute and Data Analysis\",\n",
       "  'abstract': 'The authors introduce the Bronte machine learning evaluation study for consistent detection of malware, specifically honed for botnets. Machine learning algorithms are already being used to detect malware in dynamic environments. This evaluation utilizes a static measurement approach that could be implemented on edge network devices. It was generated from conversation-based network traffic. This study fully enumerated the network traffic features to allow various machine learning algorithms to build various training sets to deploy against dual test sets. Utilizing the Waikato Environment for Knowledge Analysis (WEKA) datamining and analysis tool, various algorithmic experiments were deployed against the modern and large CICIDS2017 dataset. This evaluation study aimed to push non-IP address features through a series of machine learning classifiers. The study was conducted differently and more methodically than other related studies by using three highly randomized training sets and two test data sets. The test sets were different in that one was a real world based 98.9 benign traffic and one was 50/50 benign to bot traffic. The instance based nearest neighbor and decision tree classifiers ranked highest only using the training sets; but the J48, an expanded ID3 decision tree classifier, clearly produced the highest predictions against both test sets.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3314545.3314569',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Evaluation of Machine Learning-based Anomaly Detection Algorithms on an Industrial Modbus/TCP Data Set',\n",
       "  'authors': \"['Simon Duque Anton', 'Suneetha Kanoor', 'Daniel Fraunholz', 'Hans Dieter Schotten']\",\n",
       "  'date': 'August 2018',\n",
       "  'source': \"ARES '18: Proceedings of the 13th International Conference on Availability, Reliability and Security\",\n",
       "  'abstract': 'In the context of the Industrial Internet of Things, communication technology, originally used in home and office environments, is introduced into industrial applications. Commercial off-the-shelf products, as well as unified and well-established communication protocols make this technology easy to integrate and use. Furthermore, productivity is increased in comparison to classic industrial control by making systems easier to manage, set up and configure. Unfortunately, most attack surfaces of home and office environments are introduced into industrial applications as well, which usually have very few security mechanisms in place. Over the last years, several technologies tackling that issue have been researched. In this work, machine learning-based anomaly detection algorithms are employed to find malicious traffic in a synthetically generated data set of Modbus/TCP communication of a fictitious industrial scenario. The applied algorithms are Support Vector Machine (SVM), Random Forest, k-nearest neighbour and k-means clustering. Due to the synthetic data set, supervised learning is possible. Support Vector Machine and k-nearest neighbour perform well with different data sets, while k-nearest neighbour and k-means clustering do not perform satisfactorily.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3230833.3232818',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Machine Learning for Identifying Group Trajectory Outliers',\n",
       "  'authors': \"['Asma Belhadi', 'Youcef Djenouri', 'Djamel Djenouri', 'Tomasz Michalak', 'Jerry Chun-Wei Lin']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Management Information Systems',\n",
       "  'abstract': 'Prior works on the trajectory outlier detection problem solely consider individual outliers. However, in real-world scenarios, trajectory outliers can often appear in groups, e.g., a group of bikes that deviates to the usual trajectory due to the maintenance of streets in the context of intelligent transportation. The current paper considers the Group Trajectory Outlier (GTO) problem and proposes three algorithms. The first and the second algorithms are extensions of the well-known DBSCAN and kNN algorithms, while the third one models the GTO problem as a feature selection problem. Furthermore, two different enhancements for the proposed algorithms are proposed. The first one is based on ensemble learning and computational intelligence, which allows for merging algorithms’ outputs to possibly improve the final result. The second is a general high-performance computing framework that deals with big trajectory databases, which we used for a GPU-based implementation. Experimental results on different real trajectory databases show the scalability of the proposed approaches.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3430195',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Multi-Initialization Graph Meta-Learning for Node Classification',\n",
       "  'authors': \"['Feng Zhao', 'Donglin Wang', 'Xintao Xiang']\",\n",
       "  'date': 'August 2021',\n",
       "  'source': \"ICMR '21: Proceedings of the 2021 International Conference on Multimedia Retrieval\",\n",
       "  'abstract': 'Meta-learning aims to acquire common knowledge from a large amount of similar tasks and then adapts to unseen tasks within few gradient updates. Existing graph meta-learning algorithms show appealing performance in a variety of domains such as node classification and link prediction. These methods find a single common initialization for entire tasks and ignore the diversity of task distributions, which might be insufficient for multi-modal tasks. Recent approaches adopt modulation network to generate task-specific parameters for further achieving multiple initializations, which shows excellent performance for multi-modal image classification. However, different from image classification, how to design an effective modulation network to handle graph-structure dataset is still challenging. In this paper, we propose a Multi-Initialization Graph Meta-Learning (MI-GML) network for graph node classification, mainly consisting of local and global modulation neworks and meta learner. In terms of modulation network, we exploit local and global graph structure information to extract task-specific modulation parameters. On this basis, the meta learner is further modulated by the corresponding modulation parameter to produce task-specific representation for node classification. Experimental results on three graph-structure datasets demonstrate the effectiveness of MI-GML in few-shot node classification tasks.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3460426.3463604',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Classification of Glomeruli with Membranous Nephropathy on Renal Digital Pathological Images with Deep Learning',\n",
       "  'authors': \"['Fang Hao', 'Ming Li', 'Xueyu Liu', 'Xinyu Li', 'Junhong Yue', 'Weixia Han']\",\n",
       "  'date': 'October 2020',\n",
       "  'source': 'CAIH2020: Proceedings of the 2020 Conference on Artificial Intelligence and Healthcare',\n",
       "  'abstract': 'Membranous nephropathy (MN) is the one of the most common pathological types that cause adult nephrotic syndrome (NS). Recently, the incidence of MN has shown a clear upward trend. Nevertheless, there is no more accurate and fast artificial intelligence algorithm for diagnose of MN which work is laborintensive and time-consuming if it is done manually. In this article, MN-Net, a CNN-based method, is applied to glomeruli detection and classification on whole slide images (WSIs). This work is mainly divided into two parts, a glomerulus detection network and a classification network. The detection network is utilized to locate glomeruli on WSIs. Multiple instance learning (MIL), a weakly supervised classification network following detection network classifies the glomeruli detected earlier. Our network is training on PASM-stained WSIs of 1281 cases collected from multi-centers. Experimental results prove that our method is effective with a high precision of 99.66% for glomeruli detection and 99.53% for MN glomeruli classification on this dataset. In summary, this method has been proved to be an effective method with advantages of speed, high accuracy, strong robustness, and low cost of data annotation that can be applied to the diagnosis of renal pathology. In the future, this method can also be extended to the classification of other glomerular diseases under light microscope (LM). The introduction of glomerular basement membrane (GBM) segmentation and measurement models can further improve the reliability of this model.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3433996.3434486',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'What Is Hard about Teaching Machine Learning to Non-Majors? Insights from Classifying Instructors’ Learning Goals',\n",
       "  'authors': \"['Elisabeth Sulmont', 'Elizabeth Patitsas', 'Jeremy R. Cooperstock']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Computing Education',\n",
       "  'abstract': 'Given its societal impacts and applications to numerous fields, machine learning (ML) is an important topic to understand for many students outside of computer science and statistics. However, machine-learning education research is nascent, and research on this subject for non-majors thus far has only focused on curricula and courseware. We interviewed 10 instructors of ML courses for non-majors, inquiring as to what their students find both easy and difficult about machine learning. While ML has a reputation for having algorithms that are difficult to understand, in practice our participating instructors reported that it was not the algorithms that were difficult to teach, but the higher-level design decisions. We found that the learning goals that participants described as hard to teach were consistent with higher levels of the Structure of Observed Learning Outcomes (SOLO) taxonomy, such as making design decisions and comparing/contrasting models. We also found that the learning goals that were described as easy to teach, such as following the steps of particular algorithms, were consistent with the lower levels of the SOLO taxonomy. Realizing that higher-SOLO learning goals are more difficult to teach is useful for informing course design, public outreach, and the design of educational tools for teaching ML.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3336124',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Entropy Slicing Extraction and Transfer Learning Classification for Early Diagnosis of Alzheimer Diseases with sMRI',\n",
       "  'authors': \"['S. Sambath Kumar', 'M. Nandhini']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Multimedia Computing, Communications, and Applications',\n",
       "  'abstract': 'Alzheimer’s Disease (AD) is an irreversible neurogenerative disorder that undergoes progressive decline in memory and cognitive function and is characterized by structural brain Magnetic Resonance Images (sMRI). In recent years, sMRI data has played a vital role in the evaluation of brain anatomical changes, leading to early detection of AD through deep networks. The existing AD problems such as preprocessing complexity and unreliability are major concerns at present. To overcome these, a model (FEESCTL) has been proposed with an entropy slicing for feature extraction and Transfer Learning for classification. In the present study, the entropy image slicing method is attempted for selecting the most informative MRI slices during training stages. The ADNI dataset is trained on Transfer Learning adopted by VGG-16 network for classifying the AD with normal individuals. The experimental results reveal that the proposed model has achieved an accuracy level of 93.05%, 86.39%, 92.00% for binary classifications (AD/MCI, MCI/CN, AD/CN) and 93.12% for ternary classification (AD/MCI/CN), respectively, and henceforth the efficiency in diagnosing AD is proved through comparative analysis.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3383749',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Classification by Retrieval: Binarizing Data and Classifiers',\n",
       "  'authors': \"['Fumin Shen', 'Yadong Mu', 'Yang Yang', 'Wei Liu', 'Li Liu', 'Jingkuan Song', 'Heng Tao Shen']\",\n",
       "  'date': 'August 2017',\n",
       "  'source': \"SIGIR '17: Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval\",\n",
       "  'abstract': 'This paper proposes a generic formulation that significantly expedites the training and deployment of image classification models, particularly under the scenarios of many image categories and high feature dimensions. As the core idea, our method represents both the images and learned classifiers using binary hash codes, which are simultaneously learned from the training data. Classifying an image thereby reduces to retrieving its nearest class codes in the Hamming space. Specifically, we formulate multiclass image classification as an optimization problem over binary variables. The optimization alternatingly proceeds over the binary classifiers and image hash codes. Profiting from the special property of binary codes, we show that the sub-problems can be efficiently solved through either a binary quadratic program (BQP) or a linear program. In particular, for attacking the BQP problem, we propose a novel bit-flipping procedure which enjoys high efficacy and a local optimality guarantee. Our formulation supports a large family of empirical loss functions and is, in specific, instantiated by exponential and linear losses. Comprehensive evaluations are conducted on several representative image benchmarks. The experiments consistently exhibit reduced computational and memory complexities of model training and deployment, without sacrificing classification accuracy.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3077136.3080767',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'TipTap: Approximate Mining of Frequent k-Subgraph Patterns in Evolving Graphs',\n",
       "  'authors': \"['Muhammad Anis Uddin Nasir', 'Cigdem Aslay', 'Gianmarco De Francisci Morales', 'Matteo Riondato']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Knowledge Discovery from Data',\n",
       "  'abstract': '“Perhaps he could dance first and think afterwards, if it isn’t too much to ask him.”S.\\xa0Beckett, Waiting for GodotGiven a labeled graph, the collection of k-vertex induced connected subgraph patterns that appear in the graph more frequently than a user-specified minimum threshold provides a compact summary of the characteristics of the graph, and finds applications ranging from biology to network science. However, finding these patterns is challenging, even more so for dynamic graphs that evolve over time, due to the streaming nature of the input and the exponential time complexity of the problem. We study this task in both incremental and fully-dynamic streaming settings, where arbitrary edges can be added or removed from the graph. We present TipTap, a suite of algorithms to compute high-quality approximations of the frequent k-vertex subgraphs w.r.t.\\xa0a given threshold, at any time (i.e., point of the stream), with high probability. In contrast to existing state-of-the-art solutions that require iterating over the entire set of subgraphs in the vicinity of the updated edge, TipTap operates by efficiently maintaining a uniform sample of connected k-vertex subgraphs, thanks to an optimized neighborhood-exploration procedure. We provide a theoretical analysis of the proposed algorithms in terms of their unbiasedness and of the sample size needed to obtain a desired approximation quality. Our analysis relies on sample-complexity bounds that use Vapnik–Chervonenkis dimension, a key concept from statistical learning theory, which allows us to derive a sufficient sample size that is independent from the size of the graph. The results of our empirical evaluation demonstrates that TipTap returns high-quality results more efficiently and accurately than existing baselines.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3442590',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Interactive Document Clustering Revisited: A Visual Analytics Approach',\n",
       "  'authors': \"['Ehsan Sherkat', 'Seyednaser Nourashrafeddin', 'Evangelos E. Milios', 'Rosane Minghim']\",\n",
       "  'date': 'March 2018',\n",
       "  'source': \"IUI '18: 23rd International Conference on Intelligent User Interfaces\",\n",
       "  'abstract': \"Document clustering is an efficient way to get insight into large text collections. Due to the personalized nature of document clustering, even the best fully automatic algorithms cannot create clusters that accurately reflect the user»s perspectives. To incorporate the user»s perspective in the clustering process and, at the same time, effectively visualize document collections to enhance user's sense-making of data, we propose a novel visual analytics system for interactive document clustering. We built our system on top of clustering algorithms that can adapt to user's feedback. First, the initial clustering is created based on the user-defined number of clusters and the selected clustering algorithm. Second, the clustering result is visualized to the user. A collection of coordinated visualization modules and document projection is designed to guide the user towards a better insight into the document collection and clusters. The user changes clusters and key-terms iteratively as a feedback to the clustering algorithm until the result is satisfactory. In key-term based interaction, the user assigns a set of key-terms to each target cluster to guide the clustering algorithm. A set of quantitative experiments, a use case, and a user study have been conducted to show the advantages of the approach for document analytics based on clustering.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3172944.3172964',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Data Mining in Health Care Sector: Literature Notes',\n",
       "  'authors': \"['Ahed Abugabah', 'Ahmad Al Smadi', 'Alaa Abuqabbeh']\",\n",
       "  'date': 'November 2019',\n",
       "  'source': \"CIIS '19: Proceedings of the 2019 2nd International Conference on Computational Intelligence and Intelligent Systems\",\n",
       "  'abstract': 'A standout amongst the most essential strides of the knowledge discovery in database KDD is data mining. Data mining is defined as a basic advance during the time spent learning discovery in databases in which understanding strategies are utilized in order to pattern discovery. Due to the huge amount of data available within the healthcare systems, data mining is important for the healthcare sector in the clinical and diagnosis diseases. However, data mining and healthcare organizations have developed some of dependable early discovery frameworks and different healthcare related frameworks from the clinical treatment and analysis information. The main motivation of this paper is to give a survey of data extraction in health care. In addition, the benefits and obstacles of the use of data extraction strategies in health care and therapeutic information have been thought.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3372422.3372451',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Classification of Bacterial and Viral Childhood Pneumonia Using Deep Learning in Chest Radiography',\n",
       "  'authors': \"['Xianghong Gu', 'Liyan Pan', 'Huiying Liang', 'Ran Yang']\",\n",
       "  'date': 'March 2018',\n",
       "  'source': \"ICMIP '18: Proceedings of the 3rd International Conference on Multimedia and Image Processing\",\n",
       "  'abstract': \"Over decades, computer aided diagnosis (CAD) system has been investigated for detection of lung diseases based on chest X-ray images. Incited by the great success of deep learning, in this work, we propose a novel CAD system to identify bacterial and viral pneumonia in chest radiography. The method consists of two parts, lung regions identification and pneumonia category classification. First, left and right lung regions are segmented and extracted with a fully convolutional networks (FCN) model. The model is trained and tested on the open Japanese society of radiological technology database (JSRT, 241 images) and Montgomery County, Md (MC, 138 images) dataset. After segmentation, a deep convolutional neural network (DCNN) model is used to classify the target lung regions. Then, based on the DCNN model, features of the target lung regions are extracted automatically and the performance is compared with that of manual features. Finally, the DCNN features and manual features are fused together and are put into support vector machines (SVM) classifier for binary classification. The proposed method is evaluated on a dataset of Guangzhou Women and Children's Medical Center, China, with 4,513 pediatric patients in total, aged from 1 to 9 years old, during the period from 2003 to 2017. The performances are measured by different criteria: accuracy, precision, sensitivity, specificity and area under the curve (AUC), which is a comprehensive criterion. The experimental results showed better accuracy (0.8048±0.0202) and sensitivity (0.7755±0.0296) in extracting features by DCNN with transfer learning. The values of AUC varied from 0.6937 to 0.8234. And an ensemble of different kinds of features slightly improved the AUC value from 0.8160±0.0162 to 0.8234±0.0014.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3195588.3195597',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'},\n",
       " {'title': 'Deep Learning Based Tumor Type Classification Using Gene Expression Data',\n",
       "  'authors': \"['Boyu Lyu', 'Anamul Haque']\",\n",
       "  'date': 'August 2018',\n",
       "  'source': \"BCB '18: Proceedings of the 2018 ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics\",\n",
       "  'abstract': \"The differential analysis is the most significant part of RNA-Seq analysis. Conventional methods of the differential analysis usually match the tumor samples to the normal samples, which are both from the same tumor type. Such method would fail in differentiating tumor types because it lacks the knowledge from other tumor types. The Pan-Cancer Atlas provides us with abundant information on 33 prevalent tumor types which could be used as prior knowledge to generate tumor-specific biomarkers. In this paper, we embedded the high dimensional RNA-Seq data into 2-D images and used a convolutional neural network to make classification of the 33 tumor types. The final accuracy we got was 95.59%. Furthermore, based on the idea of Guided Grad Cam, as to each class, we generated significance heat-map for all the genes. By doing functional analysis on the genes with high intensities in the heat-maps, we validated that these top genes are related to tumor-specific pathways, and some of them have already been used as biomarkers, which proved the effectiveness of our method. As far as we know, we are the first to apply a convolutional neural network on Pan-Cancer Atlas for the classification of tumor types, and we are also the first to use gene's contribution in classification to the importance of genes to identify candidate biomarkers. Our experiment results show that our method has a good performance and could also apply to other genomics data.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3233547.3233588',\n",
       "  'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'}]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('tmccmldm_category.json', 'r', encoding='utf-8') as json_file:\n",
    "    data_category0 = json.load(json_file)\n",
    "data_category0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "ccfb97b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_category = pd.DataFrame(data_category0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "a0b79960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df_category.link.value_counts() == 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "3d0e9ff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Text mining for incoming tasks based on the urgency/importance factors and task classification using machine learning tools',\n",
       " 'authors': \"['Yasser Ali Alshehri']\",\n",
       " 'date': 'March 2020',\n",
       " 'source': \"ICCDA '20: Proceedings of the 2020 4th International Conference on Compute and Data Analysis\",\n",
       " 'abstract': 'In workplaces, there is a massive amount of unstructured data from different sources. In this paper, we present a case study that explains how can through communications between employees, we can help to prioritize tasks requests to increase the efficiency of their works for both technical and non-technical workers. This involves managing daily incoming tasks based on their level of urgency and importance.To allow all workers to utilize the urgency-importance matrix as a time-management tool, we need to automate this tool. The textual content of incoming tasks are analyzed, and metrics related to urgency and importance are extracted. A third factor (i.e., the response variable) is defined based on the two input variables (urgency and importance). Then, machine learning applied to the data to predict the class of incoming tasks based on data outcome desired. We used ordinal regression, neural networks, and decision tree algorithms to predict the four levels of task priority. We measure the performance of all using recalls, precisions, and F-scores. All classifiers perform higher than 89% in terms of all measures.',\n",
       " 'link': 'https://dl.acm.org/doi/10.1145/3388142.3388153',\n",
       " 'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'}"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "7d603e72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "999"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "37a1dd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_category = []\n",
    "unique_links = set()\n",
    "TARGET_COUNT = 977\n",
    "\n",
    "while len(data_category) < TARGET_COUNT:\n",
    "    random_item = random.choice(data_category0)\n",
    "    link = random_item['link']\n",
    "\n",
    "    if link not in unique_links and random_item['abstract']:\n",
    "        unique_links.add(link)\n",
    "        data_category.append(random_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "65b08d00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "977"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "a22c1a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_json_files(['ha_category.json', 'fl_category.json', 'se_category.json', 'sc_category.json', 'cs_category.json', 'db_category.json'], 'not_tmccmldm_categories.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "cf19cd85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Reinhardt: Real-time Reconfigurable Hardware Architecture for Regular Expression Matching in DPI',\n",
       "  'authors': \"['Taejune Park', 'Jaehyun Nam', 'Seung Ho Na', 'Jaewoong Chung', 'Seungwon Shin']\",\n",
       "  'date': 'December 2021',\n",
       "  'source': \"ACSAC '21: Proceedings of the 37th Annual Computer Security Applications Conference\",\n",
       "  'abstract': 'Regular expression (regex) matching is an integral part of deep packet inspection (DPI) but a major bottleneck due to its low performance. For regex matching (REM) acceleration, FPGA-based studies have emerged and exploited parallelism by matching multiple regex patterns concurrently. However, even though guaranteeing high-performance, existing FPGA-based regex solutions do not still support dynamic updates in run time. Hence, it was inappropriate as a DPI function due to frequently altered malicious signatures. In this work, we introduce Reinhardt, a real-time reconfigurable hardware architecture for REM. Reinhardt represents regex patterns as a combination of reconfigurable cells in hardware and updates regex patterns in real-time while providing high performance. We implement the prototype using NetFPGA-SUME, and our evaluation demonstrates that Reinhardt updates hundreds of patterns within a second and achieves up to 10 Gbps throughput (max. hardware bandwidth). Our case studies show that Reinhardt can operate as NIDS/NIPS and as the REM accelerator for them.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3485832.3485878',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'FLASH: Fast Neural Architecture Search with Hardware Optimization',\n",
       "  'authors': \"['Guihong Li', 'Sumit K. Mandal', 'Umit Y. Ogras', 'Radu Marculescu']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Embedded Computing Systems',\n",
       "  'abstract': 'Neural architecture search (NAS) is a promising technique to design efficient and high-performance deep neural networks (DNNs). As the performance requirements of ML applications grow continuously, the hardware accelerators start playing a central role in DNN design. This trend makes NAS even more complicated and time-consuming for most real applications. This paper proposes FLASH, a very fast NAS methodology that co-optimizes the DNN accuracy and performance on a real hardware platform. As the main theoretical contribution, we first propose the NN-Degree, an analytical metric to quantify the topological characteristics of DNNs with skip connections (e.g., DenseNets, ResNets, Wide-ResNets, and MobileNets). The newly proposed NN-Degree allows us to do training-free NAS within one second and build an accuracy predictor by training as few as 25 samples out of a vast search space with more than 63 billion configurations. Second, by performing inference on the target hardware, we fine-tune and validate our analytical models to estimate the latency, area, and energy consumption of various DNN architectures while executing standard ML datasets. Third, we construct a hierarchical algorithm based on simplicial homology global optimization (SHGO) to optimize the model-architecture co-design process, while considering the area, latency, and energy consumption of the target hardware. We demonstrate that, compared to the state-of-the-art NAS approaches, our proposed hierarchical SHGO-based algorithm enables more than four orders of magnitude speedup (specifically, the execution time of the proposed algorithm is about 0.1 seconds). Finally, our experimental evaluations show that FLASH is easily transferable to different hardware architectures, thus enabling us to do NAS on a Raspberry Pi-3B processor in less than 3 seconds.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3476994',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'When Massive GPU Parallelism Ain’t Enough: A Novel Hardware Architecture of 2D-LSTM Neural Network',\n",
       "  'authors': \"['Vladimir Rybalkin', 'Jonas Ney', 'Menbere Kina Tekleyohannes', 'Norbert Wehn']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Reconfigurable Technology and Systems',\n",
       "  'abstract': 'Multidimensional Long Short-Term Memory (MD-LSTM) neural network is an extension of one-dimensional LSTM for data with more than one dimension. MD-LSTM achieves state-of-the-art results in various applications, including handwritten text recognition, medical imaging, and many more. However, its implementation suffers from the inherently sequential execution that tremendously slows down both training and inference compared to other neural networks.The main goal of the current research is to provide acceleration for inference of MD-LSTM. We advocate that Field-Programmable Gate Array (FPGA) is an alternative platform for deep learning that can offer a solution when the massive parallelism of GPUs does not provide the necessary performance required by the application.In this article, we present the first hardware architecture for MD-LSTM. We conduct a systematic exploration to analyze a tradeoff between precision and accuracy. We use a challenging dataset for semantic segmentation, namely historical document image binarization from the DIBCO 2017 contest and a well-known MNIST dataset for handwritten digit recognition. Based on our new architecture, we implement FPGA-based accelerators that outperform Nvidia Geforce RTX 2080 Ti with respect to throughput by up to 9.9 and Nvidia Jetson AGX Xavier with respect to energy efficiency by up to 48. Our accelerators achieve higher throughput, energy efficiency, and resource efficiency than FPGA-based implementations of convolutional neural networks (CNNs) for semantic segmentation tasks. For the handwritten digit recognition task, our FPGA implementations provide higher accuracy and can be considered as a solution when accuracy is a priority. Furthermore, they outperform earlier FPGA implementations of one-dimensional LSTMs with respect to throughput, energy efficiency, and resource efficiency.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3469661',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'One Proxy Device Is Enough for Hardware-Aware Neural Architecture Search',\n",
       "  'authors': \"['Bingqian Lu', 'Jianyi Yang', 'Weiwen Jiang', 'Yiyu Shi', 'Shaolei Ren']\",\n",
       "  'date': 'None',\n",
       "  'source': 'Proceedings of the ACM on Measurement and Analysis of Computing Systems',\n",
       "  'abstract': 'Convolutional neural networks (CNNs) are used in numerous real-world applications such as vision-based autonomous driving and video content analysis. To run CNN inference on various target devices, hardware-aware neural architecture search (NAS) is crucial. A key requirement of efficient hardware-aware NAS is the fast evaluation of inference latencies in order to rank different architectures. While building a latency predictor for each target device has been commonly used in state of the art, this is a very time-consuming process, lacking scalability in the presence of extremely diverse devices. In this work, we address the scalability challenge by exploiting latency monotonicity --- the architecture latency rankings on different devices are often correlated. When strong latency monotonicity exists, we can re-use architectures searched for one proxy device on new target devices, without losing optimality. In the absence of strong latency monotonicity, we propose an efficient proxy adaptation technique to significantly boost the latency monotonicity. Finally, we validate our approach and conduct experiments with devices of different platforms on multiple mainstream search spaces, including MobileNet-V2, MobileNet-V3, NAS-Bench-201, ProxylessNAS and FBNet. Our results highlight that, by using just one proxy device, we can find almost the same Pareto-optimal architectures as the existing per-device NAS, while avoiding the prohibitive cost of building a latency predictor for each device.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3491046',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Hardware architecture and software stack for PIM based on commercial DRAM technology',\n",
       "  'authors': \"['Sukhan Lee', 'Shin-haeng Kang', 'Jaehoon Lee', 'Hyeonsu Kim', 'Eojin Lee', 'Seungwoo Seo', 'Hosang Yoon', 'Seungwon Lee', 'Kyounghwan Lim', 'Hyunsung Shin', 'Jinhyun Kim', 'Seongil O', 'Anand Iyer', 'David Wang', 'Kyomin Sohn', 'Nam Sung Kim']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': 'Emerging applications such as deep neural network demand high off-chip memory bandwidth. However, under stringent physical constraints of chip packages and system boards, it becomes very expensive to further increase the bandwidth of off-chip memory. Besides, transferring data across the memory hierarchy constitutes a large fraction of total energy consumption of systems, and the fraction has steadily increased with the stagnant technology scaling and poor data reuse characteristics of such emerging applications. To cost-effectively increase the bandwidth and energy efficiency, researchers began to reconsider the past processing-in-memory (PIM) architectures and advance them further, especially exploiting recent integration technologies such as 2.5D/3D stacking. Albeit the recent advances, no major memory manufacturer has developed even a proof-of-concept silicon yet, not to mention a product. This is because the past PIM architectures often require changes in host processors and/or application code which memory manufacturers cannot easily govern. In this paper, elegantly tackling the aforementioned challenges, we propose an innovative yet practical PIM architecture. To demonstrate its practicality and effectiveness at the system level, we implement it with a 20nm DRAM technology, integrate it with an unmodified commercial processor, develop the necessary software stack, and run existing applications without changing their source code. Our evaluation at the system level shows that our PIM improves the performance of memory-bound neural network kernels and applications by 11.2X and 3.5X, respectively. Atop the performance improvement, PIM also reduces the energy per bit transfer by 3.5X, and the overall energy efficiency of the system running the applications by 3.2X.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00013',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A Lightweight Architecture for Hardware-Based Security in the Emerging Era of Systems of Systems',\n",
       "  'authors': \"['Nico Mexis', 'Nikolaos Athanasios Anagnostopoulos', 'Shuai Chen', 'Jan Bambach', 'Tolga Arul', 'Stefan Katzenbeisser']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Journal on Emerging Technologies in Computing Systems',\n",
       "  'abstract': 'In recent years, a new generation of the Internet of Things (IoT 2.0) is emerging, based on artificial intelligence, the blockchain technology, machine learning, and the constant consolidation of pre-existing systems and subsystems into larger systems. In this work, we construct and examine a proof-of-concept prototype of such a system of systems, which consists of heterogeneous commercial off-the-shelf components, and utilises diverse communication protocols. We recognise the inherent need for lightweight security in this context, and address it by employing a low-cost state-of-the-art security solution. Our solution is based on a novel hardware and software co-engineering paradigm, utilising well-known software-based cryptographic algorithms, in order to maximise the security potential of the hardware security primitive (a Physical Unclonable Function) that is used as a security anchor. The performance of the proposed security solution is evaluated, proving its suitability even for real-time applications. Additionally, the Dolev-Yao attacker model is considered in order to assess the resilience of our solution towards attacks against the confidentiality, integrity, and availability of the examined system of systems. In this way, it is confirmed that the proposed solution is able to address the emerging security challenges of the oncoming era of systems of systems.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3458824',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Algorithm-hardware co-design of a discontinuous Galerkin shallow-water model for a dataflow architecture on FPGA',\n",
       "  'authors': \"['Tobias Kenter', 'Adesh Shambhu', 'Sara Faghih-Naini', 'Vadym Aizinger']\",\n",
       "  'date': 'July 2021',\n",
       "  'source': \"PASC '21: Proceedings of the Platform for Advanced Scientific Computing Conference\",\n",
       "  'abstract': 'We present the first FPGA implementation of the full simulation pipeline of a shallow water code based on the discontinuous Galerkin method. Using OpenCL and following an algorithm-hardware codesign approach, the software reference is transformed into a dataflow architecture that can process a full mesh element per clock cycle. The novel projection approach on the algorithmic level complements the pipeline and memory optimizations in the hardware design. With this, the FPGA kernels for different polynomial orders outperform the CPU reference by 43x -- 144x in a strong scaling benchmark scenario. A performance model can explain the measured FPGA performance of up to 717 GFLOPs accurately.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3468267.3470617',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Solver-Aided Constant-Time Hardware Verification',\n",
       "  'authors': \"['Klaus v. Gleissenthall', 'Rami Gökhan Kıcı', 'Deian Stefan', 'Ranjit Jhala']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"CCS '21: Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security\",\n",
       "  'abstract': \"We present Xenon, a solver-aided, interactive method for formally verifying that Verilog hardware executes in constant-time. Xenon scales to realistic hardware designs by drastically reducing the effort needed to localize the root cause of verification failures via a new notion of constant-time counterexamples, which Xenon uses to synthesize a minimal set of secrecy assumptions in an interactive verification loop. To reduce verification time Xenon exploits modularity in Verilog code via module summaries, thereby avoiding duplicate work across multiple module instantiations. We show how Xenon's assumption synthesis and summaries enable us to verify different kinds of circuits, including a highly modular AES- 256 implementation where modularity cuts verification from six hours to under three seconds, and the ScarVside-channel hardened RISC-V micro-controller whose size exceeds previously verified designs by an order of magnitude. In a small study, we also find that Xenon helps non-expert users complete verification tasks correctly and faster than previous state-of-art tools.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3460120.3484810',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'On the Co-Design of Quantum Software and Hardware',\n",
       "  'authors': \"['Gushu Li', 'Anbang Wu', 'Yunong Shi', 'Ali Javadi-Abhari', 'Yufei Ding', 'Yuan Xie']\",\n",
       "  'date': 'September 2021',\n",
       "  'source': \"NANOCOM '21: Proceedings of the Eight Annual ACM International Conference on Nanoscale Computing and Communication\",\n",
       "  'abstract': 'A quantum computing system naturally consists of two components, the software system and the hardware system. Quantum applications are programmed using the quantum software and then executed on the quantum hardware. However, the performance of existing quantum computing system is still limited. Solving a practical problem that is beyond the capability of classical computers on a quantum computer has not yet been demonstrated. In this review, we point out that the quantum software and hardware systems should be designed collaboratively to fully exploit the potential of quantum computing. We first review three related works, including one hardware-aware quantum compiler optimization, one application-aware quantum hardware architecture design flow, and one co-design approach for the emerging quantum computational chemistry. Then we discuss some potential future directions following the co-design principle.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3477206.3477464',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Algorithm-hardware Co-design of Attention Mechanism on FPGA Devices',\n",
       "  'authors': \"['Xinyi Zhang', 'Yawen Wu', 'Peipei Zhou', 'Xulong Tang', 'Jingtong Hu']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Embedded Computing Systems',\n",
       "  'abstract': 'Multi-head self-attention (attention mechanism) has been employed in a variety of fields such as machine translation, language modeling, and image processing due to its superiority in feature extraction and sequential data analysis. This is benefited from a large number of parameters and sophisticated model architecture behind the attention mechanism. To efficiently deploy attention mechanism on resource-constrained devices, existing works propose to reduce the model size by building a customized smaller model or compressing a big standard model. A customized smaller model is usually optimized for the specific task and needs effort in model parameters exploration. Model compression reduces model size without hurting the model architecture robustness, which can be efficiently applied to different tasks. The compressed weights in the model are usually regularly shaped (e.g. rectangle) but the dimension sizes vary (e.g. differs in rectangle height and width). Such compressed attention mechanism can be efficiently deployed on CPU/GPU platforms as their memory and computing resources can be flexibly assigned with demand. However, for Field Programmable Gate Arrays (FPGAs), the data buffer allocation and computing kernel are fixed at run time to achieve maximum energy efficiency. After compression, weights are much smaller and different in size, which leads to inefficient utilization of FPGA on-chip buffer. Moreover, the different weight heights and widths may lead to inefficient FPGA computing kernel execution. Due to the large number of weights in the attention mechanism, building a unique buffer and computing kernel for each compressed weight on FPGA is not feasible. In this work, we jointly consider the compression impact on buffer allocation and the required computing kernel during the attention mechanism compressing. A novel structural pruning method with memory footprint awareness is proposed and the associated accelerator on FPGA is designed. The experimental results show that our work can compress Transformer (an attention mechanism based model) by 95x. The developed accelerator can fully utilize the FPGA resource, processing the sparse attention mechanism with the run-time throughput performance of 1.87 Tops in ZCU102 FPGA.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3477002',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Co-designing hardware and models for efficient on-device ML inference',\n",
       "  'authors': \"['Matthew Mattina']\",\n",
       "  'date': 'July 2021',\n",
       "  'source': \"ISLPED '21: Proceedings of the ACM/IEEE International Symposium on Low Power Electronics and Design\",\n",
       "  'abstract': 'Deep learning inference at the edge continues to deliver state of the art results on real-world applications involving images, video, speech, and human activity. The workhorse behind these advances---increasingly complex neural network models---continue to grow in size and computational requirements. These advances place significant demand on the energy-constrained hardware platforms responsible for executing such models and are driving trends like low-precision number formats, network pruning, and complexity-reducing network transforms. This talk will discuss emerging research aimed at co-designing neural networks and hardware to enable even larger, more complex models to operate on highly-constrained hardware platforms.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISLPED52811.2021.9502470',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'BitX: Empower Versatile Inference with Hardware Runtime Pruning',\n",
       "  'authors': \"['Hongyan Li', 'Hang Lu', 'Jiawen Huang', 'Wenxu Wang', 'Mingzhe Zhang', 'Wei Chen', 'Liang Chang', 'Xiaowei Li']\",\n",
       "  'date': 'August 2021',\n",
       "  'source': \"ICPP '21: Proceedings of the 50th International Conference on Parallel Processing\",\n",
       "  'abstract': 'Classic DNN pruning mostly leverages software-based methodologies to tackle the accuracy/speed tradeoff, which involves complicated procedures like critical parameter searching, fine-tuning and sparse training to find the best plan. In this paper, we explore the opportunities of hardware runtime pruning and propose a hardware runtime pruning methodology, termed as “BitX” to empower versatile DNN inference. It targets the abundant useless bits in the parameters, pinpoints and prunes these bits on-the-fly in the proposed BitX accelerator. The versatility of BitX lies in: (1) software effortless; (2) orthogonal to the software-based pruning; and (3) multi-precision support (including both floating point and fixed point). Empirical studies on image classification and object detection models highlight the following results: (1) up to 4.82x speedup over the original non-pruned DNN and 14.76x speedup collaborated with the software-pruned DNN; (2) up to 0.07% and 0.9% higher accuracy for the floating-point and fixed-point DNN, respectively; (3) 2.00x and 3.79x performance improvement over the state-of-the-art accelerators, with 0.039 mm2 and 68.62 mW (floating-point 32), 36.41 mW(16-bit fixed point) power consumption under TSMC 28 nm technology library.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3472456.3472513',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Side-channel Resistant Implementations of a Novel Lightweight Authenticated Cipher with Application to Hardware Security',\n",
       "  'authors': \"['Abubakr Abdulgadir', 'Sammy Lin', 'Farnoud Farahmand', 'Jens-Peter Kaps', 'Kris Gaj']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"GLSVLSI '21: Proceedings of the 2021 on Great Lakes Symposium on VLSI\",\n",
       "  'abstract': 'Lightweight authenticated ciphers are crucial in many resource-constrained applications, including hardware security. To protect Intellectual Property (IPs) from theft and reverse-engineering, multiple obfuscation methods have been developed. An essential component of such schemes is the need for secrecy and authenticity of the obfuscation keys. Such keys may need to be exchanged through the unprotected channels, and their recovery attempted using side-channel attacks. However, the use of the current AES-GCM standard to protected key exchange requires a substantial area and power overhead. NIST is currently coordinating a standardization process to select lightweight algorithms for resource-constrained applications. Although security against cryptanalysis is paramount, cost, performance, and resistance to side-channel attacks are among the most important selection criteria. Since the cost of protection against side-channel attacks is a function of the algorithm, quantifying this cost is necessary for estimating its cost and performance in real-world applications. In this work, we investigate side-channel resistant lightweight implementations of an authenticated cipher TinyJAMBU, one of ten finalists in the current NIST LWC standardization process. Our results demonstrate that these implementations achieve robust security against side-channel attacks while keeping the area and power consumption significantly lower than it is possible using the current standards.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3453688.3461761',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Software-hardware co-optimization for computational chemistry on superconducting quantum processors',\n",
       "  'authors': \"['Gushu Li', 'Yunong Shi', 'Ali Javadi-Abhari']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': \"Computational chemistry is the leading application to demonstrate the advantage of quantum computing in the near term. However, large-scale simulation of chemical systems on quantum computers is currently hindered due to a mismatch between the computational resource needs of the program and those available in today's technology. In this paper we argue that significant new optimizations can be discovered by co-designing the application, compiler, and hardware. We show that multiple optimization objectives can be coordinated through the key abstraction layer of Pauli strings, which are the basic building blocks of computational chemistry programs. In particular, we leverage Pauli strings to identify critical program components that can be used to compress program size with minimal loss of accuracy. We also leverage the structure of Pauli string simulation circuits to tailor a novel hardware architecture and compiler, leading to significant execution overhead reduction by up to 99%. While exploiting the high-level domain knowledge reveals significant optimization opportunities, our hardware/software framework is not tied to a particular program instance and can accommodate the full family of computational chemistry problems with such structure. We believe the co-design lessons of this study can be extended to other domains and hardware technologies to hasten the onset of quantum advantage.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00070',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Clio: a hardware-software co-designed disaggregated memory system',\n",
       "  'authors': \"['Zhiyuan Guo', 'Yizhou Shan', 'Xuhao Luo', 'Yutong Huang', 'Yiying Zhang']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'Memory disaggregation has attracted great attention recently because of its benefits in efficient memory utilization and ease of management. So far, memory disaggregation research has all taken one of two approaches: building/emulating memory nodes using regular servers or building them using raw memory devices with no processing power. The former incurs higher monetary cost and faces tail latency and scalability limitations, while the latter introduces performance, security, and management problems.  Server-based memory nodes and memory nodes with no processing power are two extreme approaches. We seek a sweet spot in the middle by proposing a hardware-based memory disaggregation solution that has the right amount of processing power at memory nodes. Furthermore, we take a clean-slate approach by starting from the requirements of memory disaggregation and designing a memory-disaggregation-native system.  We built Clio, a disaggregated memory system that virtualizes, protects, and manages disaggregated memory at hardware-based memory nodes. The Clio hardware includes a new virtual memory system, a customized network system, and a framework for computation offloading. In building Clio, we not only co-design OS functionalities, hardware architecture, and the network system, but also co-design compute nodes and memory nodes. Our FPGA prototype of Clio demonstrates that each memory node can achieve 100\\xa0Gbps throughput and an end-to-end latency of 2.5\\xa0µ s at median and 3.2\\xa0µ s at the 99th percentile. Clio also scales much better and has orders of magnitude lower tail latency than RDMA. It has 1.1× to 3.4× energy saving compared to CPU-based and SmartNIC-based disaggregated memory systems and is 2.7× faster than software-based SmartNIC solutions.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503222.3507762',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'CirFix: automatically repairing defects in hardware design code',\n",
       "  'authors': \"['Hammad Ahmad', 'Yu Huang', 'Westley Weimer']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'This paper presents CirFix, a framework for automatically repairing defects in hardware designs implemented in languages like Verilog. We propose a novel fault localization approach based on assignments to wires and registers, and a fitness function tailored to the hardware domain to bridge the gap between software-level automated program repair and hardware descriptions. We also present a benchmark suite of 32 defect scenarios corresponding to a variety of hardware projects. Overall, CirFix produces plausible repairs for 21/32 and correct repairs for 16/32 of the defect scenarios. This repair rate is comparable to that of successful program repair approaches for software, indicating CirFix is effective at bringing over the benefits of automated program repair to the hardware domain for the first time.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503222.3507763',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Hardware Information Flow Tracking',\n",
       "  'authors': \"['Wei Hu', 'Armaiti Ardeshiricham', 'Ryan Kastner']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Computing Surveys',\n",
       "  'abstract': 'Information flow tracking (IFT) is a fundamental computer security technique used to understand how information moves through a computing system. Hardware IFT techniques specifically target security vulnerabilities related to the design, verification, testing, manufacturing, and deployment of hardware circuits. Hardware IFT can detect unintentional design flaws, malicious circuit modifications, timing side channels, access control violations, and other insecure hardware behaviors. This article surveys the area of hardware IFT. We start with a discussion on the basics of IFT, whose foundations were introduced by Denning in the 1970s. Building upon this, we develop a taxonomy for hardware IFT. We use this to classify and differentiate hardware IFT tools and techniques. Finally, we discuss the challenges yet to be resolved. The survey shows that hardware IFT provides a powerful technique for identifying hardware security vulnerabilities, as well as verifying and enforcing hardware security properties.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3447867',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Debugging in the brave new world of reconfigurable hardware',\n",
       "  'authors': \"['Jiacheng Ma', 'Gefei Zuo', 'Kevin Loughlin', 'Haoyang Zhang', 'Andrew Quinn', 'Baris Kasikci']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': \"Software and hardware development cycles have traditionally been quite distinct. Software allows post-deployment patches, which leads to a rapid development cycle. In contrast, hardware bugs that are found after fabrication are extremely costly to fix (and sometimes even unfixable), so the traditional hardware development cycle involves massive investment in extensive simulation and formal verification. Reconfigurable hardware, such as a Field Programmable Gate Array (FPGA), promises to propel hardware development towards an agile software-like development approach, since it enables a hardware developer to patch bugs that are detected during on-chip testing or in production. Unfortunately, FPGA programmers lack bug localization tools amenable to this rapid development cycle, since past tools mainly find bugs via simulation and verification. To develop hardware bug localization tools for a rapid development cycle, a thorough understanding of the symptoms, root causes, and fixes of hardware bugs is needed.   In this paper, we first study bugs in existing FPGA designs and produce a testbed of reliably-reproducible bugs. We classify the bugs according to their intrinsic properties, symptoms, and root causes. We demonstrate that many hardware bugs are comparable to software bug counterparts, and would benefit from similar techniques for bug diagnosis and repair. Based upon our findings, we build a novel collection of hybrid static/dynamic program analysis and monitoring tools for debugging FPGA designs, showing that our tools enable a software-like development cycle by effectively reducing developers' manual efforts for bug localization.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503222.3507701',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Enhancements for Hardware-based IEEE802.1CB embedded in Automotive Gateway System-on-Chip',\n",
       "  'authors': \"['Angela Gonzalez Mariño', 'Abdoul Aziz Kane', 'Francesc Fons', 'Juan Manuel Moreno Arostegui']\",\n",
       "  'date': 'December 2021',\n",
       "  'source': \"ANCS '21: Proceedings of the Symposium on Architectures for Networking and Communications Systems\",\n",
       "  'abstract': 'In this work, authors present a Hardware based strategy for IEEE802.1CB Network Reliability embedded in Automotive Gateways (GW). It is a new approach for HW efficient and cost-effective integration of Frame Replication and Elimination for Reliability (FRER) algorithm within automotive Network-on-Chip / System-on-Chip. In essence, it is a HW architecture that permits to manage the complex integration of IEEE802.1CB within In-Vehicle Networks. The FRER algorithm is split into smaller functionalities which are allocated across the different processing stages of the GW, maximizing device and network performance. The proposed architecture has a strong focus on Functional Safety, introducing important enhancements to overcome IEEE802.1CB limitations identified in the state of the art: data content verification, meaningful network diagnosability and performance verification. It moves a step forward towards fail operational systems and the compliance with ISO 26262, contributing to future autonomous driving networking solutions.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3493425.3502754',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'PRISM: Strong Hardware Isolation-based Soft-Error Resilient Multicore Architecture with High Performance and Availability at Low Hardware Overheads',\n",
       "  'authors': \"['Hamza Omar', 'Omer Khan']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Architecture and Code Optimization',\n",
       "  'abstract': 'Multicores increasingly deploy safety-critical parallel applications that demand resiliency against soft-errors to satisfy the safety standards. However, protection against these errors is challenging due to complex communication and data access protocols that aggressively share on-chip hardware resources. Research has explored various temporal and spatial redundancy-based resiliency schemes that provide multicores with high soft-error coverage. However, redundant execution incurs performance overheads due to interference effects induced by aggressive resource sharing. Moreover, these schemes require intrusive hardware modifications and fall short in providing efficient system availability guarantees. This article proposes PRISM, a resilient multicore architecture that incorporates strong hardware isolation to form redundant clusters of cores, ensuring a non-interference-based redundant execution environment. A soft error in one cluster does not effect the execution of the other cluster, resulting in high system availability. Implementing strong isolation for shared hardware resources, such as queues, caches, and networks requires logic for partitioning. However, it is less intrusive as complex hardware modifications to protocols, such as hardware cache coherence, are avoided. The PRISM approach is prototyped on a real Tilera Tile-Gx72 processor that enables primitives to implement the proposed cluster-level hardware resource isolation. The evaluation shows performance benefits from avoiding destructive hardware interference effects with redundant execution, while delivering superior system availability.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3450523',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'HeapCheck: Low-cost Hardware Support for Memory Safety',\n",
       "  'authors': \"['Gururaj Saileshwar', 'Rick Boivie', 'Tong Chen', 'Benjamin Segal', 'Alper Buyuktosunoglu']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Architecture and Code Optimization',\n",
       "  'abstract': 'Programs written in C/C++ are vulnerable to memory-safety errors like buffer-overflows and use-after-free. While several mechanisms to detect such errors have been previously proposed, they suffer from a variety of drawbacks, including poor performance, imprecise or probabilistic detection of errors, or requiring invasive changes to the ISA, binary-layout, or source-code that results in compatibility issues. As a result, memory-safety errors continue to be hard to detect and a principal cause of security problems.In this work, we present a minimally invasive and low-cost hardware-based memory-safety checking framework for detecting out-of-bounds accesses and use-after-free errors. The key idea of our mechanism is to re-purpose some of the “unused bits” in a pointer in 64-bit architectures to store an index into a bounds information table that can be used to catch out-bounds errors and use-after-free errors without any change to the binary layout. Using this memory-safety checking framework, we enable HeapCheck, a design for detecting Out-of-bounds and Use-after-free accesses for heap-objects, that are responsible for the majority of memory-safety errors in the wild. Our evaluations using C/C++ SPEC CPU 2017 workloads on Gem5 show that our solution incurs 1.5% slowdown on average, using an 8 KB on-chip SRAM cache for caching bounds-information. Our mechanism allows detection of out-of-bounds errors in user-code as well as in unmodified shared-library functions. Our mechanism has detected out-of-bounds accesses in 87 lines of code in the SPEC CPU 2017 benchmarks, primarily in Glibc\\xa0v2.27 functions, that, to our knowledge, have not been previously detected even with popular tools like Address Sanitizer.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3495152',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Conware: Automated Modeling of Hardware Peripherals',\n",
       "  'authors': \"['Chad Spensky', 'Aravind Machiry', 'Nilo Redini', 'Colin Unger', 'Graham Foster', 'Evan Blasband', 'Hamed Okhravi', 'Christopher Kruegel', 'Giovanni Vigna']\",\n",
       "  'date': 'May 2021',\n",
       "  'source': \"ASIA CCS '21: Proceedings of the 2021 ACM Asia Conference on Computer and Communications Security\",\n",
       "  'abstract': 'Emulation is at the core of many security analyses. However, emulating embedded systems is still not possible in most cases. To facilitate this critical analysis, we present Conware, a hardware emulation framework that can automatically generate models for hardware peripherals, which alleviates one of the major challenges currently hindering embedded systems emulation. Conware enables individual peripherals to be modeled, exported, and combined with other peripherals in a pluggable fashion. Conware achieves this by first obtaining a recording of the low-level hardware interactions between the firmware and the peripheral, using either existing methods or our source-code instrumentation technique. These recordings are then used to create high-fidelity automata representations of the peripheral using novel automata-generation techniques. The various models can then be merged to facilitate full-system emulation of any embedded firmware that uses any of the modeled peripherals, even if that specific firmware or its target hardware was never directly instrumented. Indeed, we demonstrate that Conware is able to successfully emulate a peripheral-heavy firmware binary that was never instrumented, by merging the models of six unique peripherals that were trained on a development board using only the vendor-provided example code.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3433210.3437532',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Hardware Security for and beyond CMOS Technology',\n",
       "  'authors': \"['Johann Knechtel']\",\n",
       "  'date': 'March 2021',\n",
       "  'source': \"ISPD '21: Proceedings of the 2021 International Symposium on Physical Design\",\n",
       "  'abstract': 'As with most aspects of electronic systems and integrated circuits, hardware security has traditionally evolved around the dominant CMOS technology. However, with the rise of various emerging technologies, whose main purpose is to overcome the fundamental limitations for scaling and power consumption of CMOS technology, unique opportunities arise to advance the notion of hardware security. In this paper, I first provide an overview on hardware security in general. Next, I review selected emerging technologies, namely (i) spintronics, (ii) memristors, (iii) carbon nanotubes and related transistors, (iv) nanowires and related transistors, and (v) 3D and 2.5D integration. I then discuss their application to advance hardware security and also outline related challenges.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3439706.3446902',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Hardware Performance Counters: Ready-Made vs Tailor-Made',\n",
       "  'authors': \"['Abraham Peedikayil Kuruvila', 'Anushree Mahapatra', 'Ramesh Karri', 'Kanad Basu']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Embedded Computing Systems',\n",
       "  'abstract': 'Micro-architectural footprints can be used to distinguish one application from another. Most modern processors feature hardware performance counters to monitor the various micro-architectural events when an application is executing. These ready-made hardware performance counters can be used to create program fingerprints and have been shown to successfully differentiate between individual applications. In this paper, we demonstrate how ready-made hardware performance counters, due to their coarse-grain nature (low sampling rate and bundling of similar events, e.g., number of instructions instead of number of add instructions), are insufficient to this end. This observation motivates exploration of tailor-made hardware performance counters to capture fine-grain characteristics of the programs. As a case study, we evaluate both ready-made and tailor-made hardware performance counters using post-quantum cryptographic key encapsulation mechanism implementations. Machine learning models trained on tailor-made hardwareperformance counter streams demonstrate that they can uniquely identify the behavior of every post-quantum cryptographic key encapsulation mechanism algorithm with at least 98.99% accuracy.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3476996',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Hardware Acceleration for Embedded Keyword Spotting: Tutorial and Survey',\n",
       "  'authors': \"['J. S. P. Giraldo', 'Marian Verhelst']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Embedded Computing Systems',\n",
       "  'abstract': 'In recent years, Keyword Spotting (KWS) has become a crucial human–machine interface for mobile devices, allowing users to interact more naturally with their gadgets by leveraging their own voice. Due to privacy, latency and energy requirements, the execution of KWS tasks on the embedded device itself instead of in the cloud, has attracted significant attention from the research community. However, the constraints associated with embedded systems, including limited energy, memory, and computational capacity, represent a real challenge for the embedded deployment of such interfaces. In this article, we explore and guide the reader through the design of KWS systems. To support this overview, we extensively survey the different approaches taken by the recent state-of-the-art (SotA) at the algorithmic, architectural, and circuit level to enable KWS tasks in edge, devices. A quantitative and qualitative comparison between relevant SotA hardware platforms is carried out, highlighting the current design trends, as well as pointing out future research directions in the development of this technology.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3474365',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Integrated Hardware Garbage Collection',\n",
       "  'authors': \"['Andrés Amaya García', 'David May', 'Ed Nutting']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Embedded Computing Systems',\n",
       "  'abstract': 'Garbage collected programming languages, such as Python and C#, have accelerated software development. These modern languages increase productivity and software reliability as they provide high-level data representation and control structures. Modern languages are widely used in software development for mobile, desktop, and server devices, but their adoption is limited in real-time embedded systems.There is clear interest in supporting modern languages in embedded devices as emerging markets, like the Internet of Things, demand ever smarter and more reliable products. Multiple commercial and open-source projects, such as Zerynth and MicroPython, are attempting to provide support. But these projects rely on software garbage collectors that impose high overheads and introduce unpredictable pauses, preventing their use in many embedded applications. These limitations arise from the unsuitability of conventional processors for performing efficient, predictable garbage collection.We propose the Integrated Hardware Garbage Collector (IHGC); a garbage collector tightly coupled with the processor that runs continuously in the background. Further, we introduce a static analysis technique to guarantee that real-time programs are never paused by the collector. Our design allocates a memory cycle to the collector when the processor is not using the memory. The IHGC achieves this by careful division of collection work into single-memory-access steps that are interleaved with the processor’s memory accesses. As a result, our collector eliminates run-time overheads and enables real-time program analysis.The principles behind the IHGC can be used in conjunction with existing architectures. For example, we simulated the IHGC alongside the ARMv6-M architecture. Compared to a conventional processor, our experiments indicate that the IHGC offers 1.5–7 times better performance for programs that rely on garbage collection. The IHGC delivers the benefits of garbage-collected languages with real-time performance but without the complexity and overheads inherent in software collectors.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3450147',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Hardware Context Switch-based Cryptographic Accelerator for Handling Multiple Streams',\n",
       "  'authors': \"['Arif Sasongko', 'I. M. Narendra Kumara', 'Arief Wicaksana', 'Frédéric Rousseau', 'Olivier Muller']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Reconfigurable Technology and Systems',\n",
       "  'abstract': 'The confidentiality and integrity of a stream has become one of the biggest issues in telecommunication. The best available algorithm handling the confidentiality of a data stream is the symmetric key block cipher combined with a chaining mode of operation such as cipher block chaining (CBC) or counter mode (CTR). This scheme is difficult to accelerate using hardware when multiple streams coexist. This is caused by the computation time requirement and mainly by management of the streams. In most accelerators, computation is treated at the block-level rather than as a stream, making the management of multiple streams complex. This article presents a solution combining CBC and CTR modes of operation with a hardware context switching. The hardware context switching allows the accelerator to treat the data as a stream. Each stream can have different parameters: key, initialization value, state of counter. Stream switching was managed by the hardware context switching mechanism. A high-level synthesis tool was used to generate the context switching circuit. The scheme was tested on three cryptographic algorithms: AES, DES, and BC3. The hardware context switching allowed the software to manage multiple streams easily, efficiently, and rapidly. The software was freed of the task of managing the stream state. Compared to the original algorithm, about 18%–38% additional logic elements were required to implement the CBC or CTR mode and the additional circuits to support context switching. Using this method, the performance overhead when treating multiple streams was low, and the performance was comparable to that of existing hardware accelerators not supporting multiple streams.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3460941',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A Runtime Reconfigurable Design of Compute-in-Memory–Based Hardware Accelerator for Deep Learning Inference',\n",
       "  'authors': \"['Anni Lu', 'Xiaochen Peng', 'Yandong Luo', 'Shanshi Huang', 'Shimeng Yu']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Design Automation of Electronic Systems',\n",
       "  'abstract': 'Compute-in-memory (CIM) is an attractive solution to address the “memory wall” challenges for the extensive computation in deep learning hardware accelerators. For custom ASIC design, a specific chip instance is restricted to a specific network during runtime. However, the development cycle of the hardware is normally far behind the emergence of new algorithms. Although some of the reported CIM-based architectures can adapt to different deep neural network (DNN) models, few details about the dataflow or control were disclosed to enable such an assumption. Instruction set architecture (ISA) could support high flexibility, but its complexity would be an obstacle to efficiency. In this article, a runtime reconfigurable design methodology of CIM-based accelerators is proposed to support a class of convolutional neural networks running on one prefabricated chip instance with ASIC-like efficiency. First, several design aspects are investigated: (1) the reconfigurable weight mapping method; (2) the input side of data transmission, mainly about the weight reloading; and (3) the output side of data processing, mainly about the reconfigurable accumulation. Then, a system-level performance benchmark is performed for the inference of different DNN models, such as VGG-8 on a CIFAR-10 dataset and AlexNet GoogLeNet, ResNet-18, and DenseNet-121 on an ImageNet dataset to measure the trade-offs between runtime reconfigurability, chip area, memory utilization, throughput, and energy efficiency.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3460436',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Effective simulation and debugging for a high-level hardware language using software compilers',\n",
       "  'authors': \"['Clément Pit-Claudel', 'Thomas Bourgeat', 'Stella Lau', 'Arvind', 'Adam Chlipala']\",\n",
       "  'date': 'April 2021',\n",
       "  'source': \"ASPLOS '21: Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'Rule-based hardware-design languages (RHDLs) promise to enhance developer productivity by offering convenient abstractions. Advanced compiler technology keeps the cost of these abstractions low, generating circuits with excellent area and timing properties.   Unfortunately, comparatively little effort has been spent on building simulators and debuggers for these languages, so users often simulate and debug their designs at the RTL level. This is problematic because generated circuits typically suffer from poor readability, as compiler optimizations can break high-level abstractions. Worse, optimizations that operate under the assumption that concurrency is essentially free yield faster circuits but often actively hurt simulation performance on platforms with limited concurrency, like desktop computers or servers.   This paper demonstrates the benefits of completely separating the simulation and synthesis pipelines. We propose a new approach, yielding the first compiler designed for effective simulation and debugging of a language in the Bluespec family. We generate cycle-accurate C++ models that are readable, compatible with a wide range of traditional software-debugging tools, and fast (often two to three times faster than circuit-level simulation). We achieve these results by optimizing for sequential performance and using static analysis to minimize redundant work. The result is a vastly improved hardware-design experience, which we demonstrate on embedded processor designs and DSP building blocks using performance benchmarks and debugging case studies.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3445814.3446720',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'The hardware lottery',\n",
       "  'authors': \"['Sara Hooker']\",\n",
       "  'date': 'December 2021',\n",
       "  'source': 'Communications of the ACM',\n",
       "  'abstract': 'After decades of incentivizing the isolation of hardware, software, and algorithm development, the catalysts for closer collaboration are changing the paradigm.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3467017',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Hardware Support to Improve Fuzzing Performance and Precision',\n",
       "  'authors': \"['Ren Ding', 'Yonghae Kim', 'Fan Sang', 'Wen Xu', 'Gururaj Saileshwar', 'Taesoo Kim']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"CCS '21: Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security\",\n",
       "  'abstract': 'Coverage-guided fuzzing is considered one of the most efficient bug-finding techniques, given its number of bugs reported. However, coverage tracing provided by existing software-based approaches, such as source instrumentation and dynamic binary translation, can incur large overhead. Hindered by the significantly lowered execution speed, it also becomes less beneficial to improve coverage feedback by incorporating additional execution states. In this paper, we propose SNAP, a customized hardware platform that implements hardware primitives to enhance the performance and precision of coverage-guided fuzzing. By sitting at the bottom of the computer stack, SNAP leverages the existing CPU pipeline and micro-architectural features to provide coverage tracing and rich execution semantics with near-zero cost regardless of source code availability. Prototyped as a synthesized RISC-V BOOM processor on FPGA, SNAP incurs a barely 3.1% tracing overhead on the SPEC benchmarks while achieving a 228x higher fuzzing throughput than the existing software-based solution. Posing only a 4.8% area and 6.5% power overhead, SNAP is highly practical and can be adopted by existing CPU architectures with minimal changes.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3460120.3484573',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A Software/Hardware Co-Design of Crystals-Dilithium Signature Scheme',\n",
       "  'authors': \"['Zhen Zhou', 'Debiao He', 'Zhe Liu', 'Min Luo', 'Kim-Kwang Raymond Choo']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Reconfigurable Technology and Systems',\n",
       "  'abstract': 'As quantum computers become more affordable and commonplace, existing security systems that are based on classical cryptographic primitives, such as RSA and Elliptic Curve Cryptography (ECC), will no longer be secure. Hence, there has been interest in designing post-quantum cryptographic (PQC) schemes, such as those based on lattice-based cryptography (LBC). The potential of LBC schemes is evidenced by the number of such schemes passing the selection of NIST PQC Standardization Process Round-3. One such scheme is the Crystals-Dilithium signature scheme, which is based on the hard module-lattice problem. However, there is no efficient implementation of the Crystals-Dilithium signature scheme. Hence, in this article, we present a compact hardware architecture containing elaborate modular multiplication units using the Karatsuba algorithm along with smart generators of address sequence and twiddle factors for NTT, which can complete polynomial addition/multiplication with the parameter setting of Dilithium in a short clock period. Also, we propose a fast software/hardware co-design implementation on Field Programmable Gate Array (FPGA) for the Dilithium scheme with a tradeoff between speed and resource utilization. Our co-design implementation outperforms a pure C implementation on a Nios-II processor of the platform Altera DE2-115, in the sense that our implementation is 11.2 and 7.4 times faster for signature and verification, respectively. In addition, we also achieve approximately 51% and 31% speed improvement for signature and verification, in comparison to the pure C implementation on processor ARM Cortex-A9 of ZYNQ-7020 platform.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3447812',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Hardware-accelerated Simulation-based Inference of Stochastic Epidemiology Models for COVID-19',\n",
       "  'authors': \"['Sourabh Kulkarni', 'Mario Michael Krell', 'Seth Nabarro', 'Csaba Andras Moritz']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Journal on Emerging Technologies in Computing Systems',\n",
       "  'abstract': 'Epidemiology models are central to understanding and controlling large-scale pandemics. Several epidemiology models require simulation-based inference such as Approximate Bayesian Computation (ABC) to fit their parameters to observations. ABC inference is highly amenable to efficient hardware acceleration. In this work, we develop parallel ABC inference of a stochastic epidemiology model for COVID-19. The statistical inference framework is implemented and compared on Intel’s Xeon CPU, NVIDIA’s Tesla V100 GPU, Google’s V2 Tensor Processing Unit (TPU), and the Graphcore’s Mk1 Intelligence Processing Unit (IPU), and the results are discussed in the context of their computational architectures. Results show that TPUs are 3×, GPUs are 4×, and IPUs are 30× faster than Xeon CPUs. Extensive performance analysis indicates that the difference between IPU and GPU can be attributed to higher communication bandwidth, closeness of memory to compute, and higher compute power in the IPU. The proposed framework scales across 16 IPUs, with scaling overhead not exceeding 8% for the experiments performed. We present an example of our framework in practice, performing inference on the epidemiology model across three countries and giving a brief overview of the results.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3471188',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Understanding and utilizing hardware transactional memory capacity',\n",
       "  'authors': \"['Zixian Cai', 'Stephen M. Blackburn', 'Michael D. Bond']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': 'ISMM 2021: Proceedings of the 2021 ACM SIGPLAN International Symposium on Memory Management',\n",
       "  'abstract': \"Hardware transactional memory (HTM) provides a simpler programming model than lock-based synchronization. However, HTM has limits that mean that transactions may suffer costly capacity aborts. Understanding HTM capacity is therefore critical. Unfortunately, crucial implementation details are undisclosed. In practice HTM capacity can manifest in puzzling ways. It is therefore unsurprising that the literature reports results that appear to be highly contradictory, reporting capacities that vary by nearly three orders of magnitude. We conduct an in-depth study into the causes of HTM capacity aborts using four generations of Intel's Transactional Synchronization Extensions (TSX). We identify the apparent contradictions among prior work, and shed new light on the causes of HTM capacity aborts. In doing so, we reconcile the apparent contradictions. We focus on how replacement policies and the status of the cache can affect HTM capacity.   One source of surprising behavior appears to be the cache replacement policies used by the processors we evaluated. Both invalidating the cache and warming it up with the transactional working set can significantly improve the read capacity of transactions across the microarchitectures we tested. A further complication is that a physically indexed LLC will typically yield only half the total LLC capacity. We found that methodological differences in the prior work led to different warmup states and thus to their apparently contradictory findings. This paper deepens our understanding of how the underlying implementation and cache behavior affect the apparent capacity of HTM. Our insights on how to increase the read capacity of transactions can be used to optimize HTM applications, particularly those with large read-mostly transactions, which are common in the context of optimistic parallelization.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3459898.3463901',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A Hardware Accelerator for Protocol Buffers',\n",
       "  'authors': \"['Sagar Karandikar', 'Chris Leary', 'Chris Kennelly', 'Jerry Zhao', 'Dinesh Parimi', 'Borivoje Nikolic', 'Krste Asanovic', 'Parthasarathy Ranganathan']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"MICRO '21: MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture\",\n",
       "  'abstract': 'Serialization frameworks are a fundamental component of scale-out systems, but introduce significant compute overheads. However, they are amenable to acceleration with specialized hardware. To understand the trade-offs involved in architecting such an accelerator, we present the first in-depth study of serialization framework usage at scale by profiling Protocol Buffers (“protobuf”) usage across Google’s datacenter fleet. We use this data to build HyperProtoBench, an open-source benchmark representative of key serialization-framework user services at scale. In doing so, we identify key insights that challenge prevailing assumptions about serialization framework usage.  We use these insights to develop a novel hardware accelerator for protobufs, implemented in RTL and integrated into a RISC-V SoC. Applications can easily harness the accelerator, as it integrates with a modified version of the open-source protobuf library and is wire-compatible with standard protobufs. We have fully open-sourced our RTL, which, to the best of our knowledge, is the only such implementation currently available to the community.  We also present a first-of-its-kind, end-to-end evaluation of our entire RTL-based system running hyperscale-derived benchmarks and microbenchmarks. We boot Linux on the system using FireSim to run these benchmarks and implement the design in a commercial 22nm FinFET process to obtain area and frequency metrics. We demonstrate an average 6.2 × to 11.2 × performance improvement vs. our baseline RISC-V SoC with BOOM OoO cores and despite the RISC-V SoC’s weaker uncore/supporting components, an average 3.8 × improvement vs. a Xeon-based server.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3466752.3480051',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Dead flit attack on NoC by hardware trojan and its impact analysis',\n",
       "  'authors': \"['Mohammad Humam Khan', 'Ruchika Gupta', 'John Jose', 'Sukumar Nandi']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"NoCArc '21: Proceedings of the 14th International Workshop on Network on Chip Architectures\",\n",
       "  'abstract': 'With the advancement in VLSI technology, Tiled Chip Multicore Processors (TCMPs) with packet switched Network-on-Chip (NoC) have emerged as the most popular design choice for compute and data intensive embedded and parallel systems. Tight time-to-market constraints and budget limitations have forced the designers to explore the possibilities of using several third party Intellectual Property (IP) cores. Use of such unsecured inexpensive third party IPs may pose severe security challenges that are not detected at manufacturing and testing phases. Recent research shows that manipulation of the NoC packet content by Hardware Trojan (HT) has the potential to disrupt the on-chip communication resulting in application level stalling. We model a novel HT that alters the common prefix field of NoC packets leading to the creation of dead flits in router buffers. We introduce two variants of this proposed HT: one that modifies head flit to body flit and another one that modifies the body flit to head flit. We analyze the HT impact at core level, cache level, and NoC level. The experimental analysis on a 16-core TCMP demonstrates that the proposed HT significantly reduces IPC, increases the average cache miss penalty, and increases the average buffer occupancy of selected packets in NoC.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3477231.3490425',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Packet header attack by hardware trojan in NoC based TCMP and its impact analysis',\n",
       "  'authors': \"['Vedika J. Kulkarni', 'R. Manju', 'Ruchika Gupta', 'John Jose', 'Sukumar Nandi']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"NOCS '21: Proceedings of the 15th IEEE/ACM International Symposium on Networks-on-Chip\",\n",
       "  'abstract': 'With the advancement of VLSI technology, Tiled Chip Multicore Processors (TCMP) with packet switched Network-on-Chip (NoC) have been emerged as the backbone of the modern data intensive parallel systems. Due to tight time-to-market constraints, manufacturers are exploring the possibility of integrating several third-party Intellectual Property (IP) cores in their TCMP designs. Presence of malicious Hardware Trojan (HT) in the NoC routers can adversely affect communication between tiles leading to degradation of overall system performance. In this paper, we model an HT mounted on the input buffers of NoC routers that can alter the destination address field of selected NoC packets. We study the impact of such HTs and analyse its first and second order impacts at the core level, cache level, and NoC level both quantitatively and qualitatively. Our experimental study shows that the proposed HT can bring application to a complete halt by stalling instruction issue and can significantly impact the miss penalty of L1 caches. The impact of re-transmission techniques in the context of HT impacted packets getting discarded is also studied. We also expose the unrealistic assumptions and unacceptable latency overheads of existing mitigation techniques for packet header attacks and emphasise the need for alternative cost effective HT management techniques for the same.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3479876.3481597',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Hardware Trojan Horse Detection through Improved Switching of Dormant Nets',\n",
       "  'authors': \"['Tapobrata Dhar', 'Surajit Kumar Roy', 'Chandan Giri']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Journal on Emerging Technologies in Computing Systems',\n",
       "  'abstract': 'Covert Hardware Trojan Horses (HTH) introduced by malicious attackers during the fabless manufacturing process of integrated circuits (IC) have the potential to cause malignant functions within the circuit. This article employs a Design-for-Security technique to detect any HTHs present in the circuit by inserting tri-state buffers (TSB) in the ICs that inject the internal nets with weighted logic values during the test phase. This increases the transitions in the logic values of the nets within the IC, thereby stimulating any inserted HTH circuits. The TSBs are efficiently inserted in the IC considering various circuit parameters and testability measures to bolster the transitions in logic values of the nets throughout the IC while minimising the area overhead. Simulation results show a significant increase in transitions in logic values within HTH triggers using this method, thus aiding in their detection through side-channel analysis or direct activation of the payload.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3439951',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Better Keep Cash in Your Boots - Hardware Wallets are the New Single Point of Failure',\n",
       "  'authors': \"['Adrian Dabrowski', 'Katharina Pfeffer', 'Markus Reichel', 'Alexandra Mai', 'Edgar R. Weippl', 'Michael Franz']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"DeFi '21: Proceedings of the 2021 ACM CCS Workshop on Decentralized Finance and Security\",\n",
       "  'abstract': \"Hardware wallets are currently considered the most secure way to manage cryptocurrency keys and sign transactions. However, previous publications show that such tokens can be replaced or manipulated in a number of hard-to-detect ways pre- or post-delivery to the user and that implemented (remote) attestation and authenticity checks fail their purpose for multiple reasons. We analyzed the architecture of current products by examining their initialization procedure and attestation methods. Unlike previous publications, we found that tightened attestation and communications encryption will not solve the fundamental architectural flaws sustainably. We conclude that the architecture of current-generation cryptocurrency hardware wallets missed the opportunity for a resilient design by copying the PC's wallet architecture and thus merely shifting the single point of trust from the PC to the hardware wallet. We advocate a mutually verified architecture through changes to BIP32/BIP44 wallet architectures to incorporate collaborative signatures and key generation. This way, neither a compromised wallet nor a compromised PC can meaningfully manipulate keys or transactions.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3464967.3488588',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Revamping storage class memory with hardware automated memory-over-storage solution',\n",
       "  'authors': \"['Jie Zhang', 'Miryeong Kwon', 'Donghyun Gouk', 'Sungjoon Koh', 'Nam Sung Kim', 'Mahmut Taylan Kandemir', 'Myoungsoo Jung']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': 'Large persistent memories such as NVDIMM have been perceived as a disruptive memory technology, because they can maintain the state of a system even after a power failure and allow the system to recover quickly. However, overheads incurred by a heavy software-stack intervention seriously negate the benefits of such memories. First, to significantly reduce the software stack overheads, we propose HAMS, a hardware automated Memory-over-Storage (MoS) solution. Specifically, HAMS aggregates the capacity of NVDIMM and ultra-low latency flash archives (ULL-Flash) into a single large memory space, which can be used as a working memory expansion or persistent memory expansion, in an OS-transparent manner. HAMS resides in the memory controller hub and manages its MoS address pool over conventional DDR and NVMe interfaces; it employs a simple hardware cache to serve all the memory requests from the host MMU after mapping the storage space of ULL-Flash to the memory space of NVDIMM. Second, to make HAMS more energy-efficient and reliable, we propose an \"advanced HAMS\" which removes unnecessary data transfers between NVDIMM and ULL-Flash after optimizing the datapath and hardware modules of HAMS. This approach unleashes the ULL-Flash and its NVMe controller from the storage box and directly connects the HAMS datapath to NVDIMM over the conventional DDR4 interface. Our evaluations show that HAMS and advanced HAMS can offer 97% and 119% higher system performance than a software-based NVDIMM design, while costing 41% and 45% lower energy, respectively.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00065',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A Pipelined Multi-Tiered Hardware Acceleration Approach Towards Content Addressable Binary Arithmetic',\n",
       "  'authors': \"['Shunjun Qian', 'Yendo Hu', 'Yiliang Wu', 'Weijie Yang', 'Xue Bai', 'Rui Shao']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"EITCE '21: Proceedings of the 2021 5th International Conference on Electronic Information Technology and Computer Engineering\",\n",
       "  'abstract': 'This paper proposes a low-latency multi-layer pipelined hardware implementation scheme of context based adaptive binary arithmetic coding (CABAC) for H.265/HEVC, which improves the coding performance and reduces the encoding latency with less hardware overhead. It is verified, debugged and optimized on the FPGA hardware platform. From the synthesis results on Xilinx K7 series FPGA, it can be seen that at 200MHz clock, the CABAC encoding of this scheme only uses 12K LUT resources, supports 1080P60Hz video encoding, and the encoding latency is as low as 6.7us. Experiments and calculations show that the proposed scheme can complete 8K UHD real-time video encoding using 59.71K logic gates at 800MHz clock on ASIC chip.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3501409.3501638',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'InTrust-IoT: Intelligent Ecosystem based on Power Profiling of Trusted device(s) in IoT for Hardware Trojan Detection',\n",
       "  'authors': \"['Hawzhin Mohammed', 'Faiq Khalid', 'Paul Sawyer', 'Gabriella Cataloni', 'Syed Rafay Hasan']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"HASP '21: Proceedings of the 10th International Workshop on Hardware and Architectural Support for Security and Privacy\",\n",
       "  'abstract': 'Modern Resource-Constrained (RC) Internet of Things (IoT) devices are subject to several types of attacks, including hardware-level attacks. Most of the existing state-of-the-art solutions are invasive, require expensive design time interventions, or need dataset generation from non-trusted RC-IoT devices or both. We argue that the health of modern RC-IoT devices requires a final line of defense against possible hardware attacks that go undetected during the IC design and test process. Hence, in this paper, we propose a defense methodology against non-zero-day and zero-day attacks, leveraging machine learning techniques trained on the dataset obtained without design time intervention and using ‘only’ trusted IoT devices. In the process, a complete eco-system is developed where data is generated through a trusted group of devices, and machine learning is done on these trusted datasets. Next, this trusted trained model is deployed in regular IoT systems that contain untrusted devices, where the attack on untrusted devices can be detected in real-time. Our results indicate that for non-zero-day attacks, the proposed technique can concurrently detect DoS and power depletion attacks with an accuracy of about 80%. Similarly, zero-day attack experiments are able to detect the attack without fail as well.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3505253.3505262',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'MNEMOSENE: Tile Architecture and Simulator for Memristor-based Computation-in-memory',\n",
       "  'authors': \"['Mahdi Zahedi', 'Muah Abu Lebdeh', 'Christopher Bengel', 'Dirk Wouters', 'Stephan Menzel', 'Manuel Le Gallo', 'Abu Sebastian', 'Stephan Wong', 'Said Hamdioui']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Journal on Emerging Technologies in Computing Systems',\n",
       "  'abstract': 'In recent years, we are witnessing a trend toward in-memory computing for future generations of computers that differs from traditional von-Neumann architecture in which there is a clear distinction between computing and memory units. Considering that data movements between the central processing unit (CPU) and memory consume several orders of magnitude more energy compared to simple arithmetic operations in the CPU, in-memory computing will lead to huge energy savings as data no longer needs to be moved around between these units. In an initial step toward this goal, new non-volatile memory technologies, e.g., resistive RAM (ReRAM) and phase-change memory (PCM), are being explored. This has led to a large body of research that mainly focuses on the design of the memory array and its peripheral circuitry. In this article, we mainly focus on the tile architecture (comprising a memory array and peripheral circuitry) in which storage and compute operations are performed in the (analog) memory array and the results are produced in the (digital) periphery. Such an architecture is termed compute-in-memory-periphery (CIM-P). More precisely, we derive an abstract CIM-tile architecture and define its main building blocks. To bridge the gap between higher-level programming languages and the underlying (analog) circuit designs, an instruction-set architecture is defined that is intended to control and, in turn, sequence the operations within this CIM tile to perform higher-level more complex operations. Moreover, we define a procedure to pipeline the CIM-tile operations to further improve the performance. To simulate the tile and perform design space exploration considering different technologies and parameters, we introduce the fully parameterized first-of-its-kind CIM tile simulator and compiler. Furthermore, the compiler is technology-aware when scheduling the CIM-tile instructions. Finally, using the simulator, we perform several preliminary design space explorations regarding the three competing technologies, ReRAM, PCM, and STT-MRAM concerning CIM-tile parameters, e.g., the number of ADCs. Additionally, we investigate the effect of pipelining in relation to the clock speeds of the digital periphery assuming the three technologies. In the end, we demonstrate that our simulator is also capable of reporting energy consumption for each building block within the CIM tile after the execution of in-memory kernels considering the data-dependency on the energy consumption of the memory array. All the source codes are publicly available.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3485824',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Tapeout of a RISC-V crypto chip with hardware trojans: a case-study on trojan design and pre-silicon detectability',\n",
       "  'authors': \"['Alexander Hepp', 'Georg Sigl']\",\n",
       "  'date': 'May 2021',\n",
       "  'source': \"CF '21: Proceedings of the 18th ACM International Conference on Computing Frontiers\",\n",
       "  'abstract': 'This paper presents design and integration of four hardware Trojans (HTs) into a post-quantum-crypto-enhanced RISC-V micro-controller, which was taped-out in September 2020. We cover multiple HTs ranging from a simple denial-of-service HT to a side-channel HT transmitting arbitrary information to external observers. For each HT, we give estimations of the detectability by the microcontroller-integration team using design tools or by simulation. We conclude that some HTs are easily detected by design-tool warnings. Other powerful HTs, modifying software control flow, cause little disturbance, but require covert executable code modifications. With this work, we strengthen awareness for HT risks and present a realistic testing device for HT detection tools.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3457388.3458869',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Attack Mitigation of Hardware Trojans for Thermal Sensing via Micro-ring Resonator in Optical NoCs',\n",
       "  'authors': \"['Jun Zhou', 'Mengquan Li', 'Pengxing Guo', 'Weichen Liu']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Journal on Emerging Technologies in Computing Systems',\n",
       "  'abstract': 'As an emerging role in new-generation on-chip communication, optical networks-on-chip (ONoCs) provide ultra-high bandwidth, low latency, and low power dissipation for data transfers. However, the thermo-optic effects of the photonic devices have a great impact on the operating performance and reliability of ONoCs, where the thermal-aware control with accurate measurements, e.g., thermal sensing, is typically applied to alleviate it. Besides, the temperature-sensitive ONoCs are prone to be attacked by the hardware Trojans (HTs) covertly embedded in the counterfeit integrated circuits (ICs) from the malicious third-party vendors, leading to performance degradation, denial-of-service (DoS), or even permanent damages. In this article, we focus on the tampering and snooping attacks during the thermal sensing via micro-ring resonator (MR) in ONoCs. Based on the provided workflow and attack model, a new structure of the anti-HT module is proposed to verify and protect the obtained data from the thermal sensor for attacks in its optical sampling and electronic transmission processes. In addition, we present the detection scheme based on the spiking neural networks (SNNs) to implement an accurate classification of the network security statuses for further high-level control. Evaluation results indicate that, with less than 1% extra area of a tile, our approach can significantly enhance the hardware security of thermal sensing for ONoC with trivial costs of up to 8.73%, 5.32%, and 6.14% in average latency, execution time, and energy consumption, respectively.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3433676',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'DirectNVM: Hardware-accelerated NVMe SSDs for High-performance Embedded Computing',\n",
       "  'authors': \"['Yu Zou', 'Amro Awad', 'Mingjie Lin']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Embedded Computing Systems',\n",
       "  'abstract': 'With data-intensive artificial intelligence (AI) and machine learning (ML) applications rapidly surging, modern high-performance embedded systems, with heterogeneous computing resources, critically demand low-latency and high-bandwidth data communication. As such, the newly emerging NVMe (Non-Volatile Memory Express) protocol, with parallel queuing, access prioritization, and optimized I/O arbitration, starts to be widely adopted as a de facto fast I/O communication interface. However, effectively leveraging the potential of modern NVMe storage proves to be nontrivial and demands fine-grained control, high processing concurrency, and application-specific optimization. Fortunately, modern FPGA devices, capable of efficient parallel processing and application-specific programmability, readily meet the underlying physical layer requirements of the NVMe protocol, therefore providing unprecedented opportunities to implementing a rich-featured NVMe middleware to benefit modern high-performance embedded computing.In this article, we present how to rethink existing accessing mechanisms of NVMe storage and devise innovative hardware-assisted solutions to accelerating NVMe data access performance for the high-performance embedded computing system. Our key idea is to exploit the massively parallel I/O queuing capability, provided by the NVMe storage system, through leveraging FPGAs’ reconfigurability and native hardware computing power to operate transparently to the main processor. Specifically, our DirectNVM system aims at providing effective hardware constructs for facilitating high-performance and scalable userspace storage applications through (1) hardening all the essential NVMe driver functionalities, therefore avoiding expensive OS syscalls and enabling zero-copy data access from the application, (2) relying on hardware for the I/O communication control instead of relying on OS-level interrupts that can significantly reduce both total I/O latency and its variance, and (3) exposing cutting-edge and application-specific weighted-round-robin I/O traffic scheduling to the userspace.To validate our design methodology, we developed a complete DirectNVM system utilizing the Xilinx Zynq MPSoC architecture that incorporates a high-performance application processor (APU) equipped with DDR4 system memory and a hardened configurable PCIe Gen3 block in its programmable logic part. We then measured the storage bandwidth and I/O latency of both our DirectNVM system and a conventional OS-based system when executing the standard FIO benchmark suite\\xa0[2]. Specifically, compared against the PetaLinux built-in kernel driver code running on a Zynq MPSoC, our DirectNVM has shown to achieve up to 18.4× higher throughput and up to 4.5× lower latency. To ensure the fairness of our performance comparison, we also measured our DirectNVM system against the Intel SPDK\\xa0[26], a highly optimized userspace asynchronous NVMe I/O framework running on a X86 PC system. Our experiment results have shown that our DirectNVM, even running on a considerably less powerful embedded ARM processor than a full-scale AMD processor, achieved up to 2.2× higher throughput and 1.3× lower latency. Furthermore, by experimenting with a multi-threading test case, we have demonstrated that our DirectNVM’s weighted-round-robin scheduling can significantly optimize the bandwidth allocation between latency-constraint frontend applications and other backend applications in real-time systems. Finally, we have developed a theoretical framework of performance modeling with classic queuing theory that can quantitatively define the relationship between a system’s I/O performance and its I/O implementation.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3463911',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Reliability analysis of a spiking neural network hardware accelerator',\n",
       "  'authors': \"['Theofilos Spyrou', 'Sarah A. El-Sayed', 'Engin Afacan', 'Luis A. Camuñas-Mesa', 'Bernabé Linares-Barranco', 'Haralampos-G. Stratigopoulos']\",\n",
       "  'date': 'March 2022',\n",
       "  'source': \"DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe\",\n",
       "  'abstract': 'Despite the parallelism and sparsity in neural network models, their transfer into hardware unavoidably makes them susceptible to hardware-level faults. Hardware-level faults can occur either during manufacturing, such as physical defects and process-induced variations, or in the field due to environmental factors and aging. The performance under fault scenarios needs to be assessed so as to develop cost-effective fault-tolerance schemes. In this work, we assess the resilience characteristics of a hardware accelerator for Spiking Neural Networks (SNNs) designed in VHDL and implemented on an FPGA. The fault injection experiments pinpoint the parts of the design that need to be protected against faults, as well as the parts that are inherently fault-tolerant.',\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3539845.3539935',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'FTT-NAS: Discovering Fault-tolerant Convolutional Neural Architecture',\n",
       "  'authors': \"['Xuefei Ning', 'Guangjun Ge', 'Wenshuo Li', 'Zhenhua Zhu', 'Yin Zheng', 'Xiaoming Chen', 'Zhen Gao', 'Yu Wang', 'Huazhong Yang']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Design Automation of Electronic Systems',\n",
       "  'abstract': 'With the fast evolvement of embedded deep-learning computing systems, applications powered by deep learning are moving from the cloud to the edge. When deploying neural networks (NNs) onto the devices under complex environments, there are various types of possible faults: soft errors caused by cosmic radiation and radioactive impurities, voltage instability, aging, temperature variations, malicious attackers, and so on. Thus, the safety risk of deploying NNs is now drawing much attention. In this article, after the analysis of the possible faults in various types of NN accelerators, we formalize and implement various fault models from the algorithmic perspective. We propose Fault-Tolerant Neural Architecture Search (FT-NAS) to automatically discover convolutional neural network (CNN) architectures that are reliable to various faults in nowadays devices. Then, we incorporate fault-tolerant training (FTT) in the search process to achieve better results, which is referred to as FTT-NAS. Experiments on CIFAR-10 show that the discovered architectures outperform other manually designed baseline architectures significantly, with comparable or fewer floating-point operations (FLOPs) and parameters. Specifically, with the same fault settings, F-FTT-Net discovered under the feature fault model achieves an accuracy of 86.2% (VS. 68.1% achieved by MobileNet-V2), and W-FTT-Net discovered under the weight fault model achieves an accuracy of 69.6% (VS. 60.8% achieved by ResNet-18). By inspecting the discovered architectures, we find that the operation primitives, the weight quantization range, the capacity of the model, and the connection pattern have influences on the fault resilience capability of NN models.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3460288',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Inferring DNN layer-types through a Hardware Performance Counters based Side Channel Attack',\n",
       "  'authors': \"['Bhargav Achary Dandpati Kumar', 'Sai Chandra Teja R', 'Sparsh Mittal', 'Biswabandan Panda', 'C. Krishna Mohan']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"AIMLSystems '21: Proceedings of the First International Conference on AI-ML Systems\",\n",
       "  'abstract': 'Recent trends of the use of deep neural networks (DNNs) in mission-critical applications have increased the threats of microarchitectural attacks on DNN models. Recently, researchers have proposed techniques for inferring the DNN model based on microarchitecture-level clues. However, existing techniques require prior knowledge of victim models, lack generality, or provide incomplete information of the victim model architecture. This paper proposes an attack that leaks the layer-type of DNNs using hardware performance monitoring counters (PMCs).  Our attack works by profiling low-level hardware events and then analyzes this data using machine learning algorithms. We also apply techniques for removing the class imbalance in the PMC traces and for removing the noise. We present microarchitectural insights (hardware PMCs such as cache accesses/misses, branch instructions, and total instructions) that correlate with the characteristics of DNN layers. The extracted models are also helpful for crafting adversarial inputs. Our attack does not require any prior knowledge of the DNN architecture and still infers the layer-types of the DNN with high accuracy (above 90%). We have released the traces for public use at https://github.com/bhargavarch/DNN_RevEngg_PMC_Dataset.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3486001.3486224',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A Comprehensive Survey of Attacks without Physical Access Targeting Hardware Vulnerabilities in IoT/IIoT Devices, and Their Detection Mechanisms',\n",
       "  'authors': \"['Nikolaos-Foivos Polychronou', 'Pierre-Henri Thevenon', 'Maxime Puys', 'Vincent Beroulle']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Design Automation of Electronic Systems',\n",
       "  'abstract': 'With the advances in the field of the Internet of Things (IoT) and Industrial IoT (IIoT), these devices are increasingly used in daily life or industry. To reduce costs related to the time required to develop these devices, security features are usually not considered. This situation creates a major security concern. Many solutions have been proposed to protect IoT/IIoT against various attacks, most of which are based on attacks involving physical access. However, a new class of attacks has emerged targeting hardware vulnerabilities in the micro-architecture that do not require physical access. We present attacks based on micro-architectural hardware vulnerabilities and the side effects they produce in the system. In addition, we present security mechanisms that can be implemented to address some of these attacks. Most of the security mechanisms target a small set of attack vectors or a single specific attack vector. As many attack vectors exist, solutions must be found to protect against a wide variety of threats. This survey aims to inform designers about the side effects related to attacks and detection mechanisms that have been described in the literature. For this purpose, we present two tables listing and classifying the side effects and detection mechanisms based on the given criteria.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3471936',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Security Threat Analyses and Attack Models for Approximate Computing Systems: From Hardware and Micro-architecture Perspectives',\n",
       "  'authors': \"['Pruthvy Yellu', 'Landon Buell', 'Miguel Mark', 'Michel A. Kinsy', 'Dongpeng Xu', 'Qiaoyan Yu']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Design Automation of Electronic Systems',\n",
       "  'abstract': 'Approximate computing (AC) represents a paradigm shift from conventional precise processing to inexact computation but still satisfying the system requirement on accuracy. The rapid progress on the development of diverse AC techniques allows us to apply approximate computing to many computation-intensive applications. However, the utilization of AC techniques could bring in new unique security threats to computing systems. This work does a survey on existing circuit-, architecture-, and compiler-level approximate mechanisms/algorithms, with special emphasis on potential security vulnerabilities. Qualitative and quantitative analyses are performed to assess the impact of the new security threats on AC systems. Moreover, this work proposes four unique visionary attack models, which systematically cover the attacks that build covert channels, compensate approximation errors, terminate normal error resilience mechanisms, and propagate additional errors. To thwart those attacks, this work further offers the guideline of countermeasure designs. Several case studies are provided to illustrate the implementation of the suggested countermeasures.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3442380',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A Quality-assured Approximate Hardware Accelerators–based on Machine Learning and Dynamic Partial Reconfiguration',\n",
       "  'authors': \"['Mahmoud Masadeh', 'Yassmeen Elderhalli', 'Osman Hasan', 'Sofiene Tahar']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Journal on Emerging Technologies in Computing Systems',\n",
       "  'abstract': 'Machine learning is widely used these days to extract meaningful information out of the Zettabytes of sensors data collected daily. All applications require analyzing and understanding the data to identify trends, e.g., surveillance, exhibit some error tolerance. Approximate computing has emerged as an energy-efficient design paradigm aiming to take advantage of the intrinsic error resilience in a wide set of error-tolerant applications. Thus, inexact results could reduce power consumption, delay, area, and execution time. To increase the energy-efficiency of machine learning on FPGA, we consider approximation at the hardware level, e.g., approximate multipliers. However, errors in approximate computing heavily depend on the application, the applied inputs, and user preferences. However, dynamic partial reconfiguration has been introduced, as a key differentiating capability in recent FPGAs, to significantly reduce design area, power consumption, and reconfiguration time by adaptively changing a selective part of the FPGA design without interrupting the remaining system. Thus, integrating “Dynamic Partial Reconfiguration” (DPR) with “Approximate Computing” (AC) will significantly ameliorate the efficiency of FPGA-based design approximation. In this article, we propose hardware-efficient quality-controlled approximate accelerators, which are suitable to be implemented in FPGA-based machine learning algorithms as well as any error-resilient applications. Experimental results using three case studies of image blending, audio blending, and image filtering applications demonstrate that the proposed adaptive approximate accelerator satisfies the required quality with an accuracy of 81.82%, 80.4%, and 89.4%, respectively. On average, the partial bitstream was found to be 28.6\\\\(\\\\) smaller than the full bitstream.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3462329',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'ARES: Persistently Secure Non-Volatile Memory with Processor-transparent and Hardware-friendly Integrity Verification and Metadata Recovery',\n",
       "  'authors': \"['Yu Zou', 'Kazi Abu Zubair', 'Mazen Alwadi', 'Rakin Muhammad Shadab', 'Sanjay Gandham', 'Amro Awad', 'Mingjie Lin']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Embedded Computing Systems',\n",
       "  'abstract': 'Emerging byte-addressable Non-Volatile Memory (NVM) technology, although promising superior memory density and ultra-low energy consumption, poses unique challenges to achieving persistent data privacy and computing security, both of which are critically important to the embedded and IoT applications. Specifically, to successfully restore NVMs to their working states after unexpected system crashes or power failure, maintaining and recovering all the necessary security-related metadata can severely increase memory traffic, degrade runtime performance, exacerbate write endurance problem, and demand costly hardware changes to off-the-shelf processors.In this article, we designed and implemented ARES, a new FPGA-assisted processor-transparent security mechanism that aims at efficiently and effectively achieving all three aspects of a security triad—confidentiality, integrity, and recoverability—in modern embedded computing. Given the growing prominence of CPU-FPGA heterogeneous computing architectures, ARES leverages FPGA’s hardware reconfigurability to offload performance-critical and security-related functions to the programmable hardware without microprocessors’ involvement. In particular, recognizing that the traditional Merkle tree caching scheme cannot fully exploit FPGA’s parallelism due to its sequential and recursive function calls, we (1) proposed a Merkle tree cache architecture that partitions a unified cache into multiple levels with parallel accesses and (2) further designed a novel Merkle tree scheme that flattened and reorganized the computation in the traditional Merkle tree verification and update processes to fully exploit the parallel cache ports and to fully pipeline time-consuming hashing operations. Beyond that, to accelerate the metadata recovery process, multiple parallel recovery units are instantiated to recover counter metadata and multiple Merkle sub-trees.Our hardware prototype of the ARES system on a Xilinx U200 platform shows that ARES achieved up to 1.4× lower latency and 2.6× higher throughput against the baseline implementation, while metadata recovery time was shortened by 1.8 times. When integrated with an embedded processor, neither hardware changes nor software changes are required. We also developed a theoretical framework to analytically model and explain experimental results.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3492735',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Scalable hardware acceleration of non-maximum suppression',\n",
       "  'authors': \"['Chunyun Chen', 'Tianyi Zhang', 'Zehui Yu', 'Adithi Raghuraman', 'Shwetalaxmi Udayan', 'Jie Lin', 'Mohamed M. Sabry Aly']\",\n",
       "  'date': 'March 2022',\n",
       "  'source': \"DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe\",\n",
       "  'abstract': 'Non-maximum Suppression (NMS) in one- and two-stage object detection deep neural networks (e.g., SSD and Faster-RCNN) is becoming the computation bottleneck. In this paper, we introduce a hardware acceleration for the scalable PSRR-MaxpoolNMS algorithm. Our architecture shows 75.0X and 305X speedups compared to the software implementation of the PSRR-MaxpoolNMS as well as the hardware implementations of GreedyNMS, respectively, while simultaneously achieving comparable Mean Average Precision (mAP) to software-based floating-point implementations. Our architecture is 13.4X faster than the state-of-the-art NMS one. Our accelerator supports both one- and two-stage detectors, while supporting very high input resolutions (i.e., FHD)---essential input size for better detection accuracy.',\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3539845.3539874',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'REMOT: A Hardware-Software Architecture for Attention-Guided Multi-Object Tracking with Dynamic Vision Sensors on FPGAs',\n",
       "  'authors': \"['Yizhao Gao', 'Song Wang', 'Hayden Kwok-Hay So']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"FPGA '22: Proceedings of the 2022 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays\",\n",
       "  'abstract': 'In contrast to conventional vision sensors that produce images of the entire field-of-view at a fixed frame rate, dynamic vision sensors (DVS) are neuromorphic devices that only produce sparse events in response to changes in light intensity local to each pixel, making them promising technologies for use in demanding edge scenarios where energy-efficient intelligent computations are needed. While several early research have demonstrated promising results in performing high-level machine vision tasks using vision events only, these algorithms are often too complex for real-time deployments in edge systems with limited processing and storage capabilities. In this work, a novel hardware-software architecture, called REMOT, is proposed to leverage the unique properties of DVS to perform real-time multi-object tracking (MOT) on FPGAs. REMOT incorporates a parallel set of reconfigurable hardware attention units (AUs) that work in tandem with a modular attention-guided software framework running in the attached processor. Each hardware AU autonomously adjusts its region of attention by processing each vision event as they are produced by the DVS. Using information aggregated by the AUs, high-level analyses are performed in software. To demonstrate the flexibility and modularity of REMOT, a family of MOT algorithms with different hardware-software configurations and tradeoffs have been implemented on 2 different edge reconfigurable systems. Experimental results show that REMOT is capable of processing 0.43-2.22 million events per second at 1.75-5.68 watts, making them suitable for real-time operations while maintaining good MOT accuracy in our target datasets. When compared with a software-only implementation using the same edge platforms, our HW-SW implementation results in up to 33.6 times higher event processing throughput and 25.9 times higher power efficiency.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3490422.3502365',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Crumbs: Utilizing Functional Programming for Hardware Trace Data Analysis',\n",
       "  'authors': \"['Max Brand', 'Albrecht Mayer', 'Frank Slomka']\",\n",
       "  'date': 'April 2021',\n",
       "  'source': \"RTNS '21: Proceedings of the 29th International Conference on Real-Time Networks and Systems\",\n",
       "  'abstract': 'As modern system-on-chip devices are getting more complex, so does their software. Luckily, the observational capabilities of such devices has also increased by providing interfaces for non-intrusive hardware tracing. With these tracing facilities, it is possible to record the device’s state over time. The acquired trace data is very helpful for debugging and system validation. However, it is tedious to analyze trace data by hand as it consists of numerous trace messages without high-level information of the system. Due to this hardware-related information, we need better tools and descriptions for its analysis. Therefore, we propose Crumbs, a formalism that can be used to specify trace data and algorithms for trace data analysis. Moreover, it can easily be translated to a functional programming language, providing an easy execution of the formally designed algorithm. The paper is accompanied by three different use cases designed with Crumbs and implemented in Haskell.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3453417.3453418',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'ShuntFlowPlus: An Efficient and Scalable Dataflow Accelerator Architecture for Stream Applications',\n",
       "  'authors': \"['Shijun Gong', 'Jiajun Li', 'Wenyan Lu', 'Guihai Yan', 'Xiaowei Li']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Journal on Emerging Technologies in Computing Systems',\n",
       "  'abstract': 'Streaming processing is an important and growing class of applications for analyzing continuous streams in real time. In such applications, sliding-window aggregation (SWAG) is a widely used approach, and general-purpose processors cannot efficiently handle SWAG because of the specific computation patterns. This article proposes an efficient dataflow accelerator architecture for ubiquitous SWAGs, called ShuntFlowPlus. ShuntFlowPlus supports two main categories of SWAGs that are widely used in streaming processing. Meanwhile, we propose a shunt rule to enable ShuntFlowPlus to efficiently handle SWAGs with arbitrary parameters. Furthermore, we propose a novel realization scheme of SWAG kernels based on buffer sharing to maximize buffer utilization. As a case study, we implemented ShuntFlowPlus on an Altera Arria 10 AX115N FPGA board at 150 MHz and compared it to previous approaches. The experimental results show that ShuntFlowPlus provides a tremendous throughput and latency advantage over CPU and GPU implementations on both reduce-like and index-like SWAGs. Compare to ShuntFlow, 41% of buffer resources are saved.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3453164',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'CeUPF: Offloading 5G User Plane Function to Programmable Hardware Base on Co-existence Architecture',\n",
       "  'authors': \"['Zhou Cong', 'Zhao Baokang', 'Wang Baosheng', 'Yuan Yulei']\",\n",
       "  'date': 'December 2021',\n",
       "  'source': \"ACM ICEA '21: Proceedings of the 2021 ACM International Conference on Intelligent Computing and its Emerging Applications\",\n",
       "  'abstract': \"Growing 5G1 applications require a user plane that has a high forward throughput and low packets loss. To meet these requirements, 5G acceleration means emerge, and User Plane Function (UPF) even more. The state of the art UPF acceleration is divided to software acceleration and hardware offloading. Overcoming these limitations between them, we design coexistence architecture for UPF, named CeUPF. We design and describe CeUPF's rule, which enables the CeUPF to reuse existing 5G user plane software function and provides high-performance hardware forwarding. UPF's transmitting function is implemented by offloading to smart NIC and P4 switch. We evaluate and compare the performance of the architecture. Our results show that offloading to P4 switch is better. Comparing with benchmark, CeUPF's bandwidth is promoted 10-33 times, and throughput is promoted 2.12-2.67 times. Our work also presents an open source platform to validate CeUPF, which is consistent with UPF.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3491396.3506526',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Hardware Trust and Assurance through Reverse Engineering: A Tutorial and Outlook from Image Analysis and Machine Learning Perspectives',\n",
       "  'authors': \"['Ulbert J. Botero', 'Ronald Wilson', 'Hangwei Lu', 'Mir Tanjidur Rahman', 'Mukhil A. Mallaiyan', 'Fatemeh Ganji', 'Navid Asadizanjani', 'Mark M. Tehranipoor', 'Damon L. Woodard', 'Domenic Forte']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Journal on Emerging Technologies in Computing Systems',\n",
       "  'abstract': 'In the context of hardware trust and assurance, reverse engineering has been often considered as an illegal action. Generally speaking, reverse engineering aims to retrieve information from a product, i.e., integrated circuits (ICs) and printed circuit boards (PCBs) in hardware security-related scenarios, in the hope of understanding the functionality of the device and determining its constituent components. Hence, it can raise serious issues concerning Intellectual Property (IP) infringement, the (in)effectiveness of security-related measures, and even new opportunities for injecting hardware Trojans. Ironically, reverse engineering can enable IP owners to verify and validate the design. Nevertheless, this cannot be achieved without overcoming numerous obstacles that limit successful outcomes of the reverse engineering process. This article surveys these challenges from two complementary perspectives: image processing and machine learning. These two fields of study form a firm basis for the enhancement of efficiency and accuracy of reverse engineering processes for both PCBs and ICs. In summary, therefore, this article presents a roadmap indicating clearly the actions to be taken to fulfill hardware trust and assurance objectives.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3464959',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Generating correct initial page tables from formal hardware descriptions',\n",
       "  'authors': \"['Reto Achermann', 'David Cock', 'Roni Haecki', 'Nora Hossle', 'Lukas Humbel', 'Timothy Roscoe', 'Daniel Schwyn']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"PLOS '21: Proceedings of the 11th Workshop on Programming Languages and Operating Systems\",\n",
       "  'abstract': 'Modern hardware platforms are increasingly complex and heterogeneous. System software uses a hodgepodge of different mechanisms and representations to express the memory topology of the target platform. Considerable maintenance effort is required to keep them in sync while often sharing is impossible due to hard-coded values. Incorrect platform-specific values in the hardware initialization sequence can lead to security critical and hard-to-find bugs because of misconfigured translation hardware, inaccessible devices, or the use of bad pointers. We present a better way for system software to express and initialize memory hardware. We adopt an existing, powerful hardware description language, and efficiently compile it to generate correct initial page tables and memory maps for OS kernels and firmware from a single system description. We evaluate our system on multiple architectures and platforms, and demonstrate that we can use the generated data structures to successfully initialize translation hardware, devices, memory maps, and allocators enabling easy support of new hardware platforms.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3477113.3487270',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Hardware acceleration of explainable machine learning',\n",
       "  'authors': \"['Zhixin Pan', 'Prabhat Mishra']\",\n",
       "  'date': 'March 2022',\n",
       "  'source': \"DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe\",\n",
       "  'abstract': \"Machine learning (ML) is successful in achieving human-level performance in various fields. However, it lacks the ability to explain an outcome due to its black-box nature. While recent efforts on explainable ML has received significant attention, the existing solutions are not applicable in real-time systems since they map interpretability as an optimization problem, which leads to numerous iterations of time-consuming complex computations. To make matters worse, existing implementations are not amenable for hardware-based acceleration. In this paper, we propose an efficient framework to enable acceleration of explainable ML procedure with hardware accelerators. We explore the effectiveness of both Tensor Processing Unit (TPU) and Graphics Processing Unit (GPU) based architectures in accelerating explainable ML. Specifically, this paper makes three important contributions. (1) To the best of our knowledge, our proposed work is the first attempt in enabling hardware acceleration of explainable ML. (2) Our proposed solution exploits the synergy between matrix convolution and Fourier transform, and therefore, it takes full advantage of TPU's inherent ability in accelerating matrix computations. (3) Our proposed approach can lead to real-time outcome interpretation. Extensive experimental evaluation demonstrates that proposed approach deployed on TPU can provide drastic improvement in interpretation time (39x on average) as well as energy efficiency (69x on average) compared to existing acceleration techniques.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3539845.3540108',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Comparative Analysis and Enhancement of CFG-based Hardware-Assisted CFI Schemes',\n",
       "  'authors': \"['Stefan Tauner', 'Mario Telesklav']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Embedded Computing Systems',\n",
       "  'abstract': 'Subverting the flow of instructions (e.g., by use of code-reuse attacks) still poses a serious threat to the security of today’s systems. Various control flow integrity (CFI) schemes have been proposed as a powerful technique to detect and mitigate such attacks. In recent years, many hardware-assisted implementations of CFI enforcement based on control flow graphs (CFGs) have been presented by academia. Such approaches check whether control flow transfers follow the intended CFG by limiting the valid target addresses. However, these papers all target different platforms and were evaluated with different sets of benchmark applications, which makes quantitative comparisons hardly possible.For this paper, we have implemented multiple promising CFG-based CFI schemes on a common platform comprising a RISC-V within FPGA. By porting almost 40 benchmark applications to this system we can present a meaningful comparison of the various techniques in terms of run-time performance, hardware utilization, and binary size. In addition, we present an enhanced CFI approach that is inspired by what we consider the best concepts and ideas of previously proposed mechanisms. We have made this approach more practical and feature-complete by tackling some problems largely ignored previously. We show with this fine-grained scheme that CFI can be achieved with even less overheads than previously demonstrated.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3476989',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Transitioning Spiking Neural Network Simulators to Heterogeneous Hardware',\n",
       "  'authors': \"['Quang Anh Pham Nguyen', 'Philipp Andelfinger', 'Wen Jun Tan', 'Wentong Cai', 'Alois Knoll']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Modeling and Computer Simulation',\n",
       "  'abstract': 'Spiking neural networks (SNN) are among the most computationally intensive types of simulation models, with node counts on the order of up to 1011. Currently, there is intensive research into hardware platforms suitable to support large-scale SNN simulations, whereas several of the most widely used simulators still rely purely on the execution on CPUs. Enabling the execution of these established simulators on heterogeneous hardware allows new studies to exploit the many-core hardware prevalent in modern supercomputing environments, while still being able to reproduce and compare with results from a vast body of existing literature. In this article, we propose a transition approach for CPU-based SNN simulators to enable the execution on heterogeneous hardware (e.g., CPUs, GPUs, and FPGAs), with only limited modifications to an existing simulator code base and without changes to model code. Our approach relies on manual porting of a small number of core simulator functionalities as found in common SNN simulators, whereas the unmodified model code is analyzed and transformed automatically. We apply our approach to the well-known simulator NEST and make a version executable on heterogeneous hardware available to the community. Our measurements show that at full utilization, a single GPU achieves the performance of about 9 CPU cores. A CPU-GPU co-execution with load balancing is also demonstrated, which shows better performance compared to CPU-only or GPU-only execution. Finally, an analytical performance model is proposed to heuristically determine the optimal parameters to execute the heterogeneous NEST.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3422389',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'QIHE: Quantifying the Importance of Hardware Events with Respect to Performance of Mobile Processors',\n",
       "  'authors': \"['Chenghao Ouyang', 'Qiannan Wang', 'Zhibin Yu']\",\n",
       "  'date': 'May 2021',\n",
       "  'source': \"ICBDC '21: Proceedings of the 6th International Conference on Big Data and Computing\",\n",
       "  'abstract': \"Nowadays, an increasing number of applications run on mobile smartphones, making people's life much more convenient than ever before. In particular, the number of mobile phones running Android operating systems equipped with ARM processors is growing steadily, accounting for more than 50% of the global mobile phone market share. Therefore, the performance of these mobile phones still needs to be improved. Hardware (microarchitecture) events of the mobile processors contain the fundamental causes of their performance bottlenecks. However, it is challenging to clearly understand the details of the impact of the micro-architecture on the processor due to: 1) the difficulty of obtaining values of micro-architecture events, and 2) the large number (more than 200) of micro-architecture events. This paper proposes QIHE, a hybrid methodology which encompasses not only a way of collecting micro-architecture events, but also quantifying the importance of them with respect to performance. This method first collect 126 micro-architecture events for each of 70 applications on two mobile phones. Subsequently, it need quantify the importance of the events with respect to performance by using a machine learning algorithm — SGBRT (Stochastic Gradient Boosted Regression Tree). Finally, the 13 most important microarchitecture events are identified for all the applications running on the two mobile phones. These events can be used to optimize the processor microarchitecture as well as the performance of the applications.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3469968.3469999',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Integration verification across software and hardware for a simple embedded system',\n",
       "  'authors': \"['Andres Erbsen', 'Samuel Gruetter', 'Joonwon Choi', 'Clark Wood', 'Adam Chlipala']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': 'PLDI 2021: Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation',\n",
       "  'abstract': 'The interfaces between layers of a system are susceptible to bugs if developers of adjacent layers proceed under subtly different assumptions. Formal verification of two layers against the same formal model of the interface between them can be used to shake out these bugs. Doing so for every interface in the system can, in principle, yield unparalleled assurance of the correctness and security of the system as a whole. However, there have been remarkably few efforts that carry out this exercise, and all of them have simplified the task by restricting interactivity of the application, inventing new simplified instruction sets, and using unrealistic input and output mechanisms. We report on the first verification of a realistic embedded system, with its application software, device drivers, compiler, and RISC-V processor represented inside the Coq proof assistant as one mathematical object, with a machine-checked proof of functional correctness. A key challenge is structuring the proof modularly, so that further refinement of the components or expansion of the system can proceed without revisiting the rest of the system.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3453483.3454065',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Weaving Schematics and Code: Interactive Visual Editing for Hardware Description Languages',\n",
       "  'authors': \"['Richard Lin', 'Rohit Ramesh', 'Nikhil Jain', 'Josephine Koe', 'Ryan Nuqui', 'Prabal Dutta', 'Bjoern Hartmann']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"UIST '21: The 34th Annual ACM Symposium on User Interface Software and Technology\",\n",
       "  'abstract': 'In many engineering disciplines such as circuit board, chip, and mechanical design, a hardware description language (HDL) approach provides important benefits over direct manipulation interfaces by supporting concepts like abstraction and generator meta-programming. While several such HDLs have emerged recently and promised power and flexibility, they also present challenges – especially to designers familiar with current graphical workflows. In this work, we investigate an IDE approach to provide a graphical editor for a board-level circuit design HDL. Unlike GUI builders which convert an entire diagram to code, we instead propose generating equivalent HDL from individual graphical edit actions. By keeping code as the primary design input, we preserve the full power of the underlying HDL, while remaining useful even to advanced users. We discuss our concept, design considerations such as performance, system implementation, and report on the results of an exploratory remote user study with four experienced hardware designers.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3472749.3474804',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'RecPipe: Co-designing Models and Hardware to Jointly Optimize Recommendation Quality and Performance',\n",
       "  'authors': \"['Udit Gupta', 'Samuel Hsia', 'Jeff Zhang', 'Mark Wilkening', 'Javin Pombra', 'Hsien-Hsin Sean Lee', 'Gu-Yeon Wei', 'Carole-Jean Wu', 'David Brooks']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"MICRO '21: MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture\",\n",
       "  'abstract': 'Deep learning recommendation systems must provide high quality, personalized content under strict tail-latency targets and high system loads. This paper presents RecPipe, a system to jointly optimize recommendation quality and inference performance. Central to RecPipe is decomposing recommendation models into multi-stage pipelines to maintain quality while reducing compute complexity and exposing distinct parallelism opportunities. RecPipe implements an inference scheduler to map multi-stage recommendation engines onto commodity, heterogeneous platforms (e.g., CPUs, GPUs). While the hardware-aware scheduling improves ranking efficiency, the commodity platforms suffer from many limitations requiring specialized hardware. Thus, we design RecPipeAccel (RPAccel), a custom accelerator that jointly optimizes quality, tail-latency, and system throughput. RPAccel is designed specifically to exploit the distinct design space opened via RecPipe. In particular, RPAccel processes queries in sub-batches to pipeline recommendation stages, implements dual static and dynamic embedding caches, a set of top-k filtering units, and a reconfigurable systolic array. Compared to previously proposed specialized recommendation accelerators and at iso-quality, we demonstrate that RPAccel improves latency and throughput by 3 × and 6 ×.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3466752.3480127',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Hardware Secure Execution and Simulation Model Correlation using IFT on RISC-V',\n",
       "  'authors': \"['Geraldine Shirley Nicholas', 'Bhavin Thakar', 'Fareena Saqib']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"GLSVLSI '21: Proceedings of the 2021 on Great Lakes Symposium on VLSI\",\n",
       "  'abstract': 'With Heterogeneous architectures and IoT devices connecting to billions of devices in the network, securing the application and tracking the data flow from different untrusted communication channels during run time and protecting the return address is an essential aspect of system integrity. In this work, we propose a correlated hardware and software-based information flow tracking mechanism to track the data using tagged logic. This scheme leverages the open-source benefits of RISC V by extending the architecture with security policies providing precise coarse grain management along with a simulation model with minimal overhead.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3453688.3461517',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'NeuroEngine: a hardware-based event-driven simulation system for advanced brain-inspired computing',\n",
       "  'authors': \"['Hunjun Lee', 'Chanmyeong Kim', 'Yujin Chung', 'Jangwoo Kim']\",\n",
       "  'date': 'April 2021',\n",
       "  'source': \"ASPLOS '21: Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'Brain-inspired computing aims to understand the cognitive mechanisms of a brain and apply them to advance various areas in computer science. Deep learning is an example to greatly improve the field of pattern recognition and classification by utilizing an artificial neural network (ANN). To exploit advanced mechanisms of a brain and thus make more great advances, researchers need a methodology that can simulate neural networks with higher computational capabilities such as advanced spiking neural networks (SNNs) with two-stage neurons and synaptic delays. However, existing SNN simulation methodologies are too slow and energy-inefficient due to their software-based simulation or hardware-based but time-driven execution mechanisms.  In this paper, we present NeuroEngine, a fast and energy-efficient hardware-based system to efficiently simulate advanced SNNs. The key idea is to design an accelerator to enable event-driven simulations of the SNNs at a minimum cost. NeuroEngine achieves high speed and energy efficiency by carefully architecting its datapath and memory units to take the best advantage of the event-driven mechanism while satisfying all the important requirements to simulate our target SNNs. For high performance and energy efficiency, NeuroEngine applies a simpler datapath, multi-queue scheduler, and lazy update to minimize its neuron computation and event scheduling overhead. Then, we build an end-to-end simulation system by implementing a programming interface and a compilation toolchain for NeuroEngine hardware. Our evaluations show that NeuroEngine greatly improves the harmonic mean performance and energy efficiency by 4.30× and 2.60×, respectively, over the state-of-the-art time-driven simulator.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3445814.3446738',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A Scalable Cluster-based Hierarchical Hardware Accelerator for a Cortically Inspired Algorithm',\n",
       "  'authors': \"['Sumon Dey', 'Lee Baker', 'Joshua Schabel', 'Weifu Li', 'Paul D. Franzon']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Journal on Emerging Technologies in Computing Systems',\n",
       "  'abstract': 'This article describes a scalable, configurable and cluster-based hierarchical hardware accelerator through custom hardware architecture for Sparsey, a cortical learning algorithm. Sparsey is inspired by the operation of the human cortex and uses a Sparse Distributed Representation to enable unsupervised learning and inference in the same algorithm. A distributed on-chip memory organization is designed and implemented in custom hardware to improve memory bandwidth and accelerate the memory read/write operations for synaptic weight matrices. Bit-level data are processed from distributed on-chip memory and custom multiply-accumulate hardware is implemented for binary and fixed-point multiply-accumulation operations. The fixed-point arithmetic and fixed-point storage are also adapted in this implementation. At 16 nm, the custom hardware of Sparsey achieved an overall 24.39×  speedup, 353.12× energy efficiency per frame, and 1.43× reduction in silicon area against a state-of-the-art GPU.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3447777',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Low-overhead Hardware Supervision for Securing an IoT Bluetooth-enabled Device: Monitoring Radio Frequency and Supply Voltage',\n",
       "  'authors': \"['Abdelrahman Elkanishy', 'Paul M. Furth', 'Derrick T. Rivera', 'Abdel-Hameed A. Badawy']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Journal on Emerging Technologies in Computing Systems',\n",
       "  'abstract': 'Over the past decade, the number of Internet of Things (IoT) devices increased tremendously. In particular, the Internet of Medical Things (IoMT) and the Industrial Internet of Things (IIoT) expanded dramatically. Resource restrictions on IoT devices and the insufficiency of software security solutions raise the need for smart Hardware-Assisted Security (HAS) solutions. These solutions target one or more of the three C’s of IoT devices: Communication, Control, and Computation. Communication is an essential technology in the development of IoT. Bluetooth is a widely-used wireless communication protocol in small portable devices due to its low energy consumption and high transfer rates. In this work, we propose a supervisory framework to monitor and verify the operation of a Bluetooth system-on-chip (SoC) in real-time. To verify the operation of the Bluetooth SoC, we classify its transmission state in real-time to ensure a secure connection. Our overall classification accuracy is measured as 98.7%. We study both power supply current (IVDD) and RF domains to maximize the classification performance and minimize the overhead of our proposed supervisory system.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3468064',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'CICERO: A Domain-Specific Architecture for Efficient Regular Expression Matching',\n",
       "  'authors': \"['Daniele Parravicini', 'Davide Conficconi', 'Emanuele Del Sozzo', 'Christian Pilato', 'Marco D. Santambrogio']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Embedded Computing Systems',\n",
       "  'abstract': 'Regular Expression (RE) matching is a computational kernel used in several applications. Since RE complexity and data volumes are steadily increasing, hardware acceleration is gaining attention also for this problem. Existing approaches have limited flexibility as they require a different implementation for each RE. On the other hand, it is complex to map efficient RE representations like non-deterministic finite-state automata onto software-programmable engines or parallel architectures. In this work, we present CICERO\\xa0, an end-to-end framework composed of a domain-specific architecture and a companion compilation framework for RE matching. Our solution is suitable for many applications, such as genomics/proteomics and natural language processing. CICERO aims at exploiting the intrinsic parallelism of non-deterministic representations of the REs. CICERO can trade-off accelerators’ efficiency and processors’ flexibility thanks to its programmable architecture and the compilation framework. We implemented CICERO prototypes on embedded FPGA achieving up to 28.6× and 20.8× more energy efficiency than embedded and mainstream processors, respectively. Since it is a programmable architecture, it can be implemented as a custom ASIC that is orders of magnitude more energy-efficient than mainstream processors.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3476982',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'DiAG: a dataflow-inspired architecture for general-purpose processors',\n",
       "  'authors': \"['Dong Kai Wang', 'Nam Sung Kim']\",\n",
       "  'date': 'April 2021',\n",
       "  'source': \"ASPLOS '21: Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': \"The end of Dennard scaling and decline of Moore's law has prompted the proliferation of hardware accelerators for a wide range of application domains. Yet, at the dawn of an era of specialized computing, left behind the trend is the general-purpose processor that is still most easily programmed and widely used but has seen incremental changes for decades. This work uses an accelerator-inspired approach to rethink CPU microarchitecture to improve its energy efficiency while retaining its generality. We propose DiAG, a dataflow-based general-purpose processor architecture that can minimize latency by exploiting instruction-level parallelism or maximize throughput by exploiting data-level parallelism. DiAG is designed to support any RISC-like instruction set without explicitly requiring specialized languages, libraries, or compilers. Central to this architecture is the abstraction of the register file as register 'lanes' that allow implicit construction of the program's dataflow graph in hardware. At the cost of increased area, DiAG offers three main benefits over conventional out-of-order microarchitectures: reduced front-end overhead, efficient instruction reuse, and thread-level pipelining. We implement a DiAG prototype that supports the RISC-V ISA in SystemVerilog and evaluate its performance, power consumption, and area with EDA tools. In the tested Rodinia and SPEC CPU2017 benchmarks, DiAG configured with 512 PEs achieves a 1.18x speedup and 1.63x improvement in energy efficiency against an aggressive out-of-order CPU baseline.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3445814.3446703',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Novel Reconfigurable Hardware Systems for Tumor Growth Prediction',\n",
       "  'authors': \"['Konstantinos Malavazos', 'Maria Papadogiorgaki', 'Pavlos Malakonakis', 'Ioannis Papaefstathiou']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Computing for Healthcare',\n",
       "  'abstract': 'An emerging trend in biomedical systems research is the development of models that take full advantage of the increasing available computational power to manage and analyze new biological data as well as to model complex biological processes. Such biomedical models require significant computational resources, since they process and analyze large amounts of data, such as medical image sequences. We present a family of advanced computational models for the prediction of the spatio-temporal evolution of glioma and their novel implementation in state-of-the-art FPGA devices. Glioma is a rapidly evolving type of brain cancer, well known for its aggressive and diffusive behavior. The developed system simulates the glioma tumor growth in the brain tissue, which consists of different anatomic structures, by utilizing MRI slices. The presented models have been proved highly accurate in predicting the growth of the tumor, whereas the developed innovative hardware system, when implemented on a low-end, low-cost FPGA, is up to 85% faster than a high-end server consisting of 20 physical cores (and 40 virtual ones) and more than 28× more energy-efficient than it; the energy efficiency grows up to 50× and the speedup up to 14× if the presented designs are implemented in a high-end FPGA. Moreover, the proposed reconfigurable system, when implemented in a large FPGA, is significantly faster than a high-end GPU (i.e., from 80% and up to 250% faster), for the majority of the models, while it is also significantly better (i.e., from 80% to over 1,600%) in terms of power efficiency, for all the implemented models.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3454126',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Sparsity-aware and re-configurable NPU architecture for samsung flagship mobile SoC',\n",
       "  'authors': \"['Jun-Woo Jang', 'Sehwan Lee', 'Dongyoung Kim', 'Hyunsun Park', 'Ali Shafiee Ardestani', 'Yeongjae Choi', 'Channoh Kim', 'Yoojin Kim', 'Hyeongseok Yu', 'Hamzah Abdel-Aziz', 'Jun-Seok Park', 'Heonsoo Lee', 'Dongwoo Lee', 'Myeong Woo Kim', 'Hanwoong Jung', 'Heewoo Nam', 'Dongguen Lim', 'Seungwon Lee', 'Joon-Ho Song', 'Suknam Kwon', 'Joseph Hassoun', 'SukHwan Lim', 'Changkyu Choi']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': 'Of late, deep neural networks have become ubiquitous in mobile applications. As mobile devices generally require immediate response while maintaining user privacy, the demand for on-device machine learning technology is on the increase. Nevertheless, mobile devices suffer from restricted hardware resources, whereas deep neural networks involve considerable computation and communication. Therefore, the implementation of a neural-network specialized hardware accelerator, generally called neural processing unit (NPU), has started to gain attention for the mobile application processor (AP). However, NPUs for commercial mobile AP face two challenges that are difficult to realize simultaneously: execution of a wide range of applications and efficient performance. In this paper, we propose a flexible but efficient NPU architecture for a Samsung flagship mobile system-on-chip (SoC). To implement an efficient NPU, we design an energy-efficient inner-product engine that utilizes the input feature map sparsity. We propose a re-configurable MAC array to enhance the flexibility of the proposed NPU, dynamic internal memory port assignment to maximize on-chip memory bandwidth utilization, and efficient architecture to support mixed-precision arithmetic. We implement the proposed NPU using the Samsung 5nm library. Our silicon measurement experiments demonstrate that the proposed NPU achieves 290.7 FPS and 13.6 TOPS/W, when executing an 8-bit quantized Inception-v3 model [1] with a single NPU core. In addition, we analyze the proposed zero-skipping architecture in detail. Finally, we present the findings and lessons learned when implementing the commercial mobile NPU and interesting avenues for future work.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00011',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Sanger: A Co-Design Framework for Enabling Sparse Attention using Reconfigurable Architecture',\n",
       "  'authors': \"['Liqiang Lu', 'Yicheng Jin', 'Hangrui Bi', 'Zizhang Luo', 'Peng Li', 'Tao Wang', 'Yun Liang']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"MICRO '21: MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture\",\n",
       "  'abstract': 'In recent years, attention-based models have achieved impressive performance in natural language processing and computer vision applications by effectively capturing contextual knowledge from the entire sequence. However, the attention mechanism inherently contains a large number of redundant connections, imposing a heavy computational burden on model deployment. To this end, sparse attention has emerged as an attractive approach to reduce the computation and memory footprint, which involves the sampled dense-dense matrix multiplication (SDDMM) and sparse-dense matrix multiplication (SpMM) at the same time, thus requiring the hardware to eliminate zero-valued operations effectively. Existing techniques based on irregular sparse patterns or regular but coarse-grained patterns lead to low hardware efficiency or less computation saving.  This paper proposes Sanger, a framework that harvests sparsity in the attention mechanism through synergistic hardware and software co-design. The software part prunes the attention matrix into a dynamic structured pattern, and the hardware part features a reconfigurable architecture that exploits such patterns. Specifically, we dynamically sparsify vanilla attention based on a quantized prediction of the attention matrix. Then, the sparse mask is re-arranged into structured blocks that are more amenable to hardware implementation. The hardware design of Sanger features a score-stationary dataflow that keeps sparse scores stationary in the PE to avoid decoding overhead. Using this dataflow and a reconfigurable systolic array design, we can unify the computation of SDDMM and SpMM operations. Typically, the PEs can be configured during runtime to support different data access and partial sum accumulation schemes. Experiments on BERT show that Sanger can prune the model to 0.08 - 0.27 sparsity without accuracy loss, achieving 4.64X, 22.7X, 2.39X, and 1.47X speedup compared to V100 GPU, AMD Ryzen Threadripper 3970X CPU, as well as the state-of-the-art attention accelerators A3 and SpAtten.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3466752.3480125',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Configurable Multi-directional Systolic Array Architecture for Convolutional Neural Networks',\n",
       "  'authors': \"['Rui Xu', 'Sheng Ma', 'Yaohua Wang', 'Xinhai Chen', 'Yang Guo']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Architecture and Code Optimization',\n",
       "  'abstract': 'The systolic array architecture is one of the most popular choices for convolutional neural network hardware accelerators. The biggest advantage of the systolic array architecture is its simple and efficient design principle. Without complicated control and dataflow, hardware accelerators with the systolic array can calculate traditional convolution very efficiently. However, this advantage also brings new challenges to the systolic array. When computing special types of convolution, such as the small-scale convolution or depthwise convolution, the processing element (PE) utilization rate of the array decreases sharply. The main reason is that the simple architecture design limits the flexibility of the systolic array.In this article, we design a configurable multi-directional systolic array (CMSA) to address these issues. First, we added a data path to the systolic array. It allows users to split the systolic array through configuration to speed up the calculation of small-scale convolution. Second, we redesigned the PE unit so that the array has multiple data transmission modes and dataflow strategies. This allows users to switch the dataflow of the PE array to speed up the calculation of depthwise convolution. In addition, unlike other works, we only make a few changes and modifications to the existing systolic array architecture. It avoids additional hardware overheads and can be easily deployed in application scenarios that require small systolic arrays such as mobile terminals. Based on our evaluation, CMSA can increase the PE utilization rate by up to 1.6 times compared to the typical systolic array when running the last layers of ResNet-18. When running depthwise convolution in MobileNet, CMSA can increase the utilization rate by up to 14.8 times. At the same time, CMSA and the traditional systolic arrays are similar in area and energy consumption.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3460776',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'BlastFunction: A Full-stack Framework Bringing FPGA Hardware Acceleration to Cloud-native Applications',\n",
       "  'authors': \"['Andrea Damiani', 'Giorgia Fiscaletti', 'Marco Bacis', 'Rolando Brondolin', 'Marco D. Santambrogio']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Reconfigurable Technology and Systems',\n",
       "  'abstract': '“Cloud-native” is the umbrella adjective describing the standard approach for developing applications that exploit cloud infrastructures’ scalability and elasticity at their best. As the application complexity and user-bases grow, designing for performance becomes a first-class engineering concern. As an answer to these needs, heterogeneous computing platforms gained widespread attention as powerful tools to continue meeting SLAs for compute-intensive cloud-native workloads. We propose BlastFunction, an FPGA-as-a-Service full-stack framework to ease FPGAs’ adoption for cloud-native workloads, integrating with the vast spectrum of fundamental cloud models. At the IaaS level, BlastFunction time-shares FPGA-based accelerators to provide multi-tenant access to accelerated resources without any code rewriting. At the PaaS level, BlastFunction accelerates functionalities leveraging the serverless model and scales functions proactively, depending on the workload’s performance. Further lowering the FPGAs’ adoption barrier, an accelerators’ registry hosts accelerated functions ready to be used within cloud-native applications, bringing the simplicity of a SaaS-like approach to the developers. After an extensive experimental campaign against state-of-the-art cloud scenarios, we show how BlastFunction leads to higher performance metrics (utilization and throughput) against native execution, with minimal latency and overhead differences. Moreover, the scaling scheme we propose outperforms the main serverless autoscaling algorithms in workload performance and scaling operation amount.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3472958',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'ProSE: the architecture and design of a protein discovery engine',\n",
       "  'authors': \"['Eyes Robson', 'Ceyu Xu', 'Lisa Wu Wills']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'Protein language models have enabled breakthrough approaches to protein structure prediction, function annotation, and drug discovery. A primary limitation to the widespread adoption of these powerful models is the high computational cost associated with the training and inference of these models, especially at longer sequence lengths. We present the architecture, microarchitecture, and hardware implementation of a protein design and discovery accelerator, ProSE (Protein Systolic Engine). ProSE has a collection of custom heterogeneous systolic arrays and special functions that process transfer learning model inferences efficiently. The architecture marries SIMD-style computations with systolic array architectures, optimizing coarse-grained operation sequences across model layers to achieve efficiency without sacrificing generality. ProSE performs Protein BERT inference at up to 6.9× speedup and 48× power efficiency (performance/Watt) compared to one NVIDIA A100 GPU. ProSE achieves up to 5.5 × (12.7×) speedup and 173× (249×) power efficiency compared to TPUv3 (TPUv2).',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503222.3507722',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Communication algorithm-architecture co-design for distributed deep learning',\n",
       "  'authors': \"['Jiayi Huang', 'Pritam Majumder', 'Sungkeun Kim', 'Abdullah Muzahid', 'Ki Hwan Yum', 'Eun Jung Kim']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': 'Large-scale distributed deep learning training has enabled developments of more complex deep neural network models to learn from larger datasets for sophisticated tasks. In particular, distributed stochastic gradient descent intensively invokes all-reduce operations for gradient update, which dominates communication time during iterative training epochs. In this work, we identify the inefficiency in widely used all-reduce algorithms, and the opportunity of algorithm-architecture co-design. We propose MULTITREE all-reduce algorithm with topology and resource utilization awareness for efficient and scalable all-reduce operations, which is applicable to different interconnect topologies. Moreover, we co-design the network interface to schedule and coordinate the all-reduce messages for contention-free communications, working in synergy with the algorithm. The flow control is also simplified to exploit the bulk data transfer of big gradient exchange. We evaluate the co-design using different all-reduce data sizes for synthetic study, demonstrating its effectiveness on various interconnection network topologies, in addition to state-of-the-art deep neural networks for real workload experiments. The results show that MULTITREE achieves 2.3X and 1.56X communication speedup, as well as up to 81% and 30% training time reduction compared to ring all-reduce and state-of-the-art approaches, respectively.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00023',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Design of Picture Classification System Based on Embedded PYNQ Architecture',\n",
       "  'authors': \"['Shengkai Wang', 'Jun Li', 'Yuan Meng']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"EITCE '21: Proceedings of the 2021 5th International Conference on Electronic Information Technology and Computer Engineering\",\n",
       "  'abstract': 'With the rapid development of deep learning, traditional convolutional neural networks have achieved success in various fields. In order to meet the situation that FPGAs and other devices have less available resources, a picture classification system that satisfies convolutional neural network inference with less resources, small size, and high throughput is designed. This design is based on the PYNQ development board, using the design idea of software and hardware coordination, quantified convolutional neural network is applied on the development board, reducing computational redundancy, taking advantage of FPGA parallel computing, and constructing a picture classification system based on the PYNQ architecture. Experiments show that the quantified network model has low power consumption on the PYNQ-Z1 development board with limited resources, and can ideally complete the classification task. This paper realizes the rapid deployment of quantitative neural network, which has higher efficiency and good acceleration performance.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3501409.3501486',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'The Role of Edge Offload for Hardware - Accelerated Mobile Devices',\n",
       "  'authors': \"['Mahadev Satyanarayanan', 'Nathan Beckmann', 'Grace A. Lewis', 'Brandon Lucia']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': 'GetMobile: Mobile Computing and Communications',\n",
       "  'abstract': 'This position paper examines a spectrum of approaches to overcoming the limited computing power of mobile devices caused by their need to be small, lightweight and energy efficient. At one extreme is offloading of compute-intensive operations to a cloudlet nearby. At the other extreme is the use of fixed-function hardware accelerators on mobile devices. Between these endpoints lie various configurations of programmable hardware accelerators. We explore the strengths and weaknesses of these approaches and conclude that they are, in fact, complementary. Based on this insight, we advocate a softwarehardware co-evolution path that combines their strengths.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3486880.3486882',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'The Business Case for Media Architecture: Modelling Project Benefits to Justify Investment',\n",
       "  'authors': \"['Niels Wouters', 'Franz Wohlgezogen', 'Kim Halskov']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': 'MAB20: Media Architecture Biennale 20',\n",
       "  'abstract': 'A growing portfolio of global media architecture projects and sustained research interest in the domain suggest that the discipline is here to stay. With practical knowledge becoming easily accessible to clients, architects and urban planners, we notice a shift from traditional advertising screens to integrated context-aware installations. The challenge now becomes to understand investment return of media architecture in order to ensure ongoing support by clients and funders. In this paper we study The Digital Bricks, a 208 megapixel media façade integrated within a university building. We describe the project vision, engagement strategy and design outcome, and analyse in detail the business case for the project. We share considerations to support development of business cases for media architecture projects that favour engagement, cultural and innovation capacity over financial returns. As the discipline matures, our insights will help in the endeavour to convince clients to invest in media architecture that inspires and engages audiences.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3469410.3469416',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Software-driven Security Attacks: From Vulnerability Sources to Durable Hardware Defenses',\n",
       "  'authors': \"['Lauren Biernacki', 'Mark Gallagher', 'Zhixing Xu', 'Misiker Tadesse Aga', 'Austin Harris', 'Shijia Wei', 'Mohit Tiwari', 'Baris Kasikci', 'Sharad Malik', 'Todd Austin']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Journal on Emerging Technologies in Computing Systems',\n",
       "  'abstract': 'There is an increasing body of work in the area of hardware defenses for software-driven security attacks. A significant challenge in developing these defenses is that the space of security vulnerabilities and exploits is large and not fully understood. This results in specific point defenses that aim to patch particular vulnerabilities. While these defenses are valuable, they are often blindsided by fresh attacks that exploit new vulnerabilities. This article aims to address this issue by suggesting ways to make future defenses more durable based on an organization of security vulnerabilities as they arise throughout the program life cycle. We classify these vulnerability sources through programming, compilation, and hardware realization, and we show how each source introduces unintended states and transitions into the implementation. Further, we show how security exploits gain control by moving the implementation to an unintended state using knowledge of these sources and how defenses work to prevent these transitions. This framework of analyzing vulnerability sources, exploits, and defenses provides insights into developing durable defenses that could defend against broader categories of exploits. We present illustrative case studies of four important attack genealogies—showing how they fit into the presented framework and how the sophistication of the exploits and defenses have evolved over time, providing us insights for the future.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3456299',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Automatic generation of architecture-level models from RTL designs for processors and accelerators',\n",
       "  'authors': \"['Yu Zeng', 'Aarti Gupta', 'Sharad Malik']\",\n",
       "  'date': 'March 2022',\n",
       "  'source': \"DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe\",\n",
       "  'abstract': 'Hardware platforms comprise general-purpose processors and application-specific accelerators. Unlike processors, application-specific accelerators often do not have clearly specified architecture-level models/specifications (the instruction set architecture or ISA). This poses challenges to the development and verification/validation of firmware/software for these accelerators. Manually writing architecture-level models takes great effort and is error-prone. When Register-Transfer Level (RTL) designs are available, they can be a source from which to automatically derive the architecture-level models. In this work, we propose an approach for automatically generating architecture-level models for processors as well as accelerators from their RTL designs. In previous work we showed how to automatically extract the architectural state variables (ASVs) from RTL designs. (These are the state variables that are persistent across instructions.) In this work we present an algorithm for generating the update functions of the model: how the ASVs and outputs are updated by each instruction. Experiments on several processors and accelerators demonstrate that our approach can cover a wide range of hardware features and generate high-quality architecture-level models within reasonable time.',\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3539845.3539954',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Timing-Optimized Hardware Implementation to Accelerate Polynomial Multiplication in the NTRU Algorithm',\n",
       "  'authors': \"['Eros Camacho-Ruiz', 'Santiago Sánchez-Solano', 'Piedad Brox', 'Macarena C. Martínez-Rodríguez']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Journal on Emerging Technologies in Computing Systems',\n",
       "  'abstract': 'Post-quantum cryptographic algorithms have emerged to secure communication channels between electronic devices faced with the advent of quantum computers. The performance of post-quantum cryptographic algorithms on embedded systems has to be evaluated to achieve a good trade-off between required resources (area) and timing. This work presents two optimized implementations to speed up the NTRUEncrypt algorithm on a system-on-chip. The strategy is based on accelerating the most time-consuming operation that is the truncated polynomial multiplication. Hardware dedicated modules for multiplication are designed by exploiting the presence of consecutive zeros in the coefficients of the blinding polynomial. The results are validated on a PYNQ-Z2 platform that includes a Zynq-7000 SoC from Xilinx and supports a Python-based programming environment. The optimized version that exploits the presence of double, triple, and quadruple consecutive zeros offers the best performance in timing, in addition to considerably reducing the possibility of an information leakage against an eventual attack on the device, making it practically negligible.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3445979',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'In-fat pointer: hardware-assisted tagged-pointer spatial memory safety defense with subobject granularity protection',\n",
       "  'authors': \"['Shengjie Xu', 'Wei Huang', 'David Lie']\",\n",
       "  'date': 'April 2021',\n",
       "  'source': \"ASPLOS '21: Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'Programming languages like C and C++ are not memory-safe because they provide programmers with low-level pointer manipulation primitives. The incorrect use of these primitives can result in bugs and security vulnerabilities: for example, spatial memory safety errors can be caused by dereferencing pointers outside the legitimate address range belonging to the corresponding object. While a range of schemes to provide protection against these vulnerabilities have been proposed, they all suffer from the lack of one or more of low performance overhead, compatibility with legacy code, or comprehensive protection for all objects and subobjects.   We present In-Fat Pointer, the first hardware-assisted defense that can achieve spatial memory safety at subobject granularity while maintaining compatibility with legacy code and low overhead. In-Fat Pointer improves the protection granularity of tagged-pointer schemes using object metadata, which is efficient and binary-compatible for object-bound spatial safety. Unlike previous work that devotes all pointer tag bits to object metadata lookup, In-Fat Pointer uses three complementary object metadata schemes to reduce the number pointer tag bits needed for metadata lookup, allowing it to use the left-over bits, along with in-memory type metadata, to refine the object bounds to subobject granularity. We show that this approach provides practical protection of fine-grained spatial memory safety.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3445814.3446761',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Fundamentals of Physical Computing: Determining Key Concepts in Embedded Systems and Hardware/Software Co-Design',\n",
       "  'authors': \"['Mareen Przybylla', 'Andreas Grillenberger']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"WiPSCE '21: Proceedings of the 16th Workshop in Primary and Secondary Computing Education\",\n",
       "  'abstract': 'Studies have shown that teachers find it difficult to prepare contents in the area of embedded systems and hardware/software co-design for school students. The goal of this paper is to support them by obtaining a clearly structured representation of the key concepts from this area in order to be able to derive concrete competence goals based on them later on. We apply a method for identifying the key concepts of a subject area within computer science, which has already been tested in the field of data management, to embedded systems and the related hardware/software co-design. Here, we present the procedure (literature selection, content analysis, concept clustering, and structuring) and the results of this process.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3481312.3481352',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Joint Program and Layout Transformations to Enable Convolutional Operators on Specialized Hardware Based on Constraint Programming',\n",
       "  'authors': \"['Dennis Rieber', 'Axel Acosta', 'Holger Fröning']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Architecture and Code Optimization',\n",
       "  'abstract': 'The success of Deep Artificial Neural Networks (DNNs) in many domains created a rich body of research concerned with hardware accelerators for compute-intensive DNN operators. However, implementing such operators efficiently with complex hardware intrinsics such as matrix multiply is a task not yet automated gracefully. Solving this task often requires joint program and data layout transformations. First solutions to this problem have been proposed, such as TVM, UNIT, or ISAMIR, which work on a loop-level representation of operators and specify data layout and possible program transformations before the embedding into the operator is performed. This top-down approach creates a tension between exploration range and search space complexity, especially when also exploring data layout transformations such as im2col, channel packing, or padding.In this work, we propose a new approach to this problem. We created a bottom-up method that allows the joint transformation of both computation and data layout based on the found embedding. By formulating the embedding as a constraint satisfaction problem over the scalar dataflow, every possible embedding solution is contained in the search space. Adding additional constraints and optimization targets to the solver generates the subset of preferable solutions.An evaluation using the VTA hardware accelerator with the Baidu DeepBench inference benchmark shows that our approach can automatically generate code competitive to reference implementations. Further, we show that dynamically determining the data layout based on intrinsic and workload is beneficial for hardware utilization and performance. In cases where the reference implementation has low hardware utilization due to its fixed deployment strategy, we achieve a geomean speedup of up to × 2.813, while individual operators can improve as much as × 170.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3487922',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'MERCI: efficient embedding reduction on commodity hardware via sub-query memoization',\n",
       "  'authors': \"['Yejin Lee', 'Seong Hoon Seo', 'Hyunji Choi', 'Hyoung Uk Sul', 'Soosung Kim', 'Jae W. Lee', 'Tae Jun Ham']\",\n",
       "  'date': 'April 2021',\n",
       "  'source': \"ASPLOS '21: Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'Deep neural networks (DNNs) with embedding layers are widely adopted to capture complex relationships among entities within a dataset. Embedding layers aggregate multiple embeddings — a dense vector used to represent the complicated nature of a data feature— into a single embedding; such operation is called embedding reduction. Embedding reduction spends a significant portion of its runtime on reading embeddings from memory and thus is known to be heavily memory-bandwidth-bound. Recent works attempt to accelerate this critical operation, but they often require either hardware modifications or emerging memory technologies, which makes it hardly deployable on commodity hardware. Thus, we propose MERCI, Memoization for Embedding Reduction with ClusterIng, a novel memoization framework for efficient embedding reduction. MERCI provides a mechanism for memoizing partial aggregation of correlated embeddings and retrieving the memoized partial result at a low cost. MERCI substantially reduces the number of memory accesses by 44% (29%), leading to 102% (74%) throughput improvement on real machines and 40.2% (28.6%) energy savings at the expense of 8×(1×) additional memory usage.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3445814.3446717',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A Genetic-algorithm-based Approach to the Design of DCT Hardware Accelerators',\n",
       "  'authors': \"['Mario Barbareschi', 'Salvatore Barone', 'Alberto Bosio', 'Jie Han', 'Marcello Traiola']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Journal on Emerging Technologies in Computing Systems',\n",
       "  'abstract': 'As modern applications demand an unprecedented level of computational resources, traditional computing system design paradigms are no longer adequate to guarantee significant performance enhancement at an affordable cost. Approximate Computing (AxC) has been introduced as a potential candidate to achieve better computational performances by relaxing non-critical functional system specifications. In this article, we propose a systematic and high-abstraction-level approach allowing the automatic generation of near Pareto-optimal approximate configurations for a Discrete Cosine Transform (DCT) hardware accelerator. We obtain the approximate variants by using approximate operations, having configurable approximation degree, rather than full-precise ones. We use a genetic searching algorithm to find the appropriate tuning of the approximation degree, leading to optimal tradeoffs between accuracy and gains. Finally, to evaluate the actual HW gains, we synthesize non-dominated approximate DCT variants for two different target technologies, namely, Field Programmable Gate Arrays (FPGAs) and Application Specific Integrated Circuits (ASICs). Experimental results show that the proposed approach allows performing a meaningful exploration of the design space to find the best tradeoffs in a reasonable time. Indeed, compared to the state-of-the-art work on approximate DCT, the proposed approach allows an 18% average energy improvement while providing at the same time image quality improvement.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3501772',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Hardware Implementation of Hierarchical Temporal Memory Algorithm',\n",
       "  'authors': \"['Weifu Li', 'Paul Franzon', 'Sumon Dey', 'Joshua Schabel']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Journal on Emerging Technologies in Computing Systems',\n",
       "  'abstract': 'Hierarchical temporal memory (HTM) is an un-supervised machine learning algorithm that can learn both spatial and temporal information of input. It has been successfully applied to multiple areas. In this paper, we propose a multi-level hierarchical ASIC implementation of HTM, referred to as processor core, to support both spatial and temporal pooling. To improve the unbalanced workload in HTM, the proposed design provides different mapping methods for the spatial and temporal pooling, respectively. In the proposed design, we implement a distributed memory system by assigning one dedicated memory bank to each level of hierarchy to improve the memory bandwidth utilization efficiency. Finally, the hot-spot operations are optimized using a series of customized units. Regarding scalability, we propose a ring-based network consisting of multiple processor cores to support a larger HTM network. To evaluate the performance of our proposed design, we map an HTM network that includes 2,048 columns and 65,536 cells on both the proposed design and NVIDIA Tesla K40c GPU using the KTH database as input. The latency and power of the proposed design is 6.04 ms and 4.1 W using GP 65 nm technology. Compared to the equivalent GPU implementation, the latency and power is improved 12.45× and 57.32×, respectively.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3479430',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'FPGAs in Client Compute Hardware: Despite certain challenges, FPGAs provide security and performance benefits over ASICs.',\n",
       "  'authors': \"['Michael Mattioli']\",\n",
       "  'date': 'November-December 2021',\n",
       "  'source': 'Queue',\n",
       "  'abstract': 'FPGAs (field-programmable gate arrays) are remarkably versatile. They are used in a wide variety of applications and industries where use of ASICs (application-specific integrated circuits) is less economically feasible. Despite the area, cost, and power challenges designers face when integrating FPGAs into devices, they provide significant security and performance benefits. Many of these benefits can be realized in client compute hardware such as laptops, tablets, and smartphones.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3512327',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Intermittent-Aware Neural Architecture Search',\n",
       "  'authors': \"['Hashan Roshantha Mendis', 'Chih-Kai Kang', 'Pi-cheng Hsiu']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Embedded Computing Systems',\n",
       "  'abstract': 'The increasing paradigm shift towards intermittent computing has made it possible to intermittently execute deep neural network (DNN) inference on edge devices powered by ambient energy. Recently, neural architecture search (NAS) techniques have achieved great success in automatically finding DNNs with high accuracy and low inference latency on the deployed hardware. We make a key observation, where NAS attempts to improve inference latency by primarily maximizing data reuse, but the derived solutions when deployed on intermittently-powered systems may be inefficient, such that the inference may not satisfy an end-to-end latency requirement and, more seriously, they may be unsafe given an insufficient energy budget. This work proposes iNAS, which introduces intermittent execution behavior into NAS to find accurate network architectures with corresponding execution designs, which can safely and efficiently execute under intermittent power. An intermittent-aware execution design explorer is presented, which finds the right balance between data reuse and the costs related to intermittent inference, and incorporates a preservation design search space into NAS, while ensuring the power-cycle energy budget is not exceeded. To assess an intermittent execution design, an intermittent-aware abstract performance model is presented, which formulates the key costs related to progress preservation and recovery during intermittent inference. We implement iNAS on top of an existing NAS framework and evaluate their respective solutions found for various datasets, energy budgets and latency requirements, on a Texas Instruments device. Compared to those NAS solutions that can safely complete the inference, the iNAS solutions reduce the intermittent inference latency by 60% on average while achieving comparable accuracy, with an average 7% increase in search overhead.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3476995',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Relevance and Applicability of Hardware-independent Pointing Transfer Functions',\n",
       "  'authors': \"['Raiza Hanada', 'Damien Masson', 'Géry Casiez', 'Mathieu Nancel', 'Sylvain Malacria']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"UIST '21: The 34th Annual ACM Symposium on User Interface Software and Technology\",\n",
       "  'abstract': 'Pointing transfer functions remain predominantly expressed in pixels per input counts, which can generate different visual pointer behaviors with different input and output devices; we show in a first controlled experiment that even small hardware differences impact pointing performance with functions defined in this manner. We also demonstrate the applicability of “hardware-independent” transfer functions defined in physical units. We explore two methods to maintain hardware-independent pointer performance in operating systems that require hardware-dependent definitions: scaling them to the resolutions of the input and output devices, or selecting the OS acceleration setting that produces the closest visual behavior. In a second controlled experiment, we adapted a baseline function to different screen and mouse resolutions using both methods, and the resulting functions provided equivalent performance. Lastly, we provide a tool to calculate equivalent transfer functions between hardware setups, allowing users to match pointer behavior with different devices, and researchers to tune and replicate experiment conditions. Our work emphasizes, and hopefully facilitates, the idea that operating systems should have the capability to formulate pointing transfer functions in physical units, and to adjust them automatically to hardware setups.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3472749.3474767',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'RSSD: defend against ransomware with hardware-isolated network-storage codesign and post-attack analysis',\n",
       "  'authors': \"['Benjamin Reidys', 'Peng Liu', 'Jian Huang']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'Encryption ransomware has become a notorious malware. It encrypts user data on storage devices like solid-state drives (SSDs) and demands a ransom to restore data for users. To bypass existing defenses, ransomware would keep evolving and performing new attack models. For instance, we identify and validate three new attacks, including (1) garbage-collection (GC) attack that exploits storage capacity and keeps writing data to trigger GC and force SSDs to release the retained data; (2) timing attack that intentionally slows down the pace of encrypting data and hides its I/O patterns to escape existing defense; (3) trimming attack that utilizes the trim command available in SSDs to physically erase data.   To enhance the robustness of SSDs against these attacks, we propose RSSD, a ransomware-aware SSD. It redesigns the flash management of SSDs for enabling the hardware-assisted logging, which can conservatively retain older versions of user data and received storage operations in time order with low overhead. It also employs hardware-isolated NVMe over Ethernet to expand local storage capacity by transparently offloading the logs to remote cloud/servers in a secure manner. RSSD enables post-attack analysis by building a trusted evidence chain of storage operations to assist the investigation of ransomware attacks. We develop RSSD with a real-world SSD FPGA board. Our evaluation shows that RSSD can defend against new and future ransomware attacks, while introducing negligible performance overhead.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503222.3507773',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Developing a Hardware Approach to Simulating Quantum Computing Using an Optimization Algorithm',\n",
       "  'authors': \"['Valery Pukhovsky', 'Sergey Gushanskiy', 'Viktor Potapov']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"PCI '21: Proceedings of the 25th Pan-Hellenic Conference on Informatics\",\n",
       "  'abstract': 'In the past few decades, there has been an acute problem of creating a quantum computer that uses quantum mechanical effects such as quantum parallelism and quantum entanglement for computations. Using these mechanisms, the quantum computer is able to solve some of the NP-class problems in polynomial time. A hardware approach to modeling quantum computing is considered, and a general mathematical model of the operation of a quantum computer is described, and a technique for mathematical modeling of quantum computing is presented. At the stage of consideration of the method of mathematical modeling, the most resource-intensive parts of the model are indicated. Also, issues related to data parallelization on a hardware accelerator, which are planned to be used to simulate quantum computing, were considered, and algorithms for the operation of this type of accelerator were given. A more compact data format is proposed, which is recommended to be used when implementing the accelerator. Using the proposed format, it is possible to reduce the resources spent on elementary arithmetic operations. The possibility of introducing an optimization algorithm to minimize the time spent in computations, as well as to speed up the execution of quantum algorithms in simulators of a quantum computer at the stage of the effect of quantum gates on the quantum register model, is proposed. The results of the optimization algorithm and its comparison with the classical mathematical approach using a software model are presented.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503823.3503894',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A Task-Parallel and Reconfigurable FPGA-Based Hardware Implementation of Extreme Learning Machine',\n",
       "  'authors': \"['Hui Huang', 'Hai-Jun Rong', 'Zhao-Xu Yang']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"ASSE' 22: 2022 3rd Asia Service Sciences and Software Engineering Conference\",\n",
       "  'abstract': 'Extreme learning machine (ELM) is an emerging machine learning algorithm and widely used in various real-world applications due to its extremely fast training speed, good generalization and universal approximation capability. In order to further explore the ELM to be used in practical embedded systems, a task-parallel and reconfigurable FPGA-based hardware architecture of ELM algorithm is presented in this paper. The proposed architecture performs the on-chip machine learning for both training and prediction phases which are implemented parameterizably based on the reconfigurable parameters. Meanwhile, the task-parallel efforts are focused on the training phase to improve the computational efficiency by resolving the serial computations into subtasks for task-parallel computations. In addition, the on-chip block RAMs reuse scheme is also applied in proposed architecture for saving on-chip resource consumption. The experimental results show that the proposed ELM architecture can achieve similar accuracy compared with floating-point implementation on Matlab and outperform the recently published ELM implementations in terms of hardware performance, power consumption and resource utilization.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3523181.3523209',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Improving Power of DSP and CNN Hardware Accelerators Using Approximate Floating-point Multipliers',\n",
       "  'authors': \"['Vasileios Leon', 'Theodora Paparouni', 'Evangelos Petrongonas', 'Dimitrios Soudris', 'Kiamal Pekmestzi']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Embedded Computing Systems',\n",
       "  'abstract': 'Approximate computing has emerged as a promising design alternative for delivering power-efficient systems and circuits by exploiting the inherent error resiliency of numerous applications. The current article aims to tackle the increased hardware cost of floating-point multiplication units, which prohibits their usage in embedded computing. We introduce AFMU (Approximate Floating-point MUltiplier), an area/power-efficient family of multipliers, which apply two approximation techniques in the resource-hungry mantissa multiplication and can be seamlessly extended to support dynamic configuration of the approximation levels via gating signals. AFMU offers large accuracy configuration margins, provides negligible logic overhead for dynamic configuration, and detects unexpected results that may arise due to the approximations. Our evaluation shows that AFMU delivers energy gains in the range 3.6%–53.5% for half-precision and 37.2%–82.4% for single-precision, in exchange for mean relative error around 0.05%–3.33% and 0.01%–2.20%, respectively. In comparison with state-of-the-art multipliers, AFMU exhibits up to 4–6× smaller error on average while delivering more energy-efficient computing. The evaluation in image processing shows that AFMU provides sufficient quality of service, i.e., more than 50db PSNR and near 1 SSIM values, and up to 57.4% power reduction. When used in floating-point CNNs, the accuracy loss is small (or zero), i.e., up to 5.4% for MNIST and CIFAR-10, in exchange for up to 63.8% power gain.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3448980',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'The Open Domain-Specific Architecture: Next Steps to Production',\n",
       "  'authors': \"['Bapiraju Vinnakota']\",\n",
       "  'date': 'September 2021',\n",
       "  'source': \"NANOCOM '21: Proceedings of the Eight Annual ACM International Conference on Nanoscale Computing and Communication\",\n",
       "  'abstract': 'Domain-specific architectures (DSAs) are expected to play an increasing role in hyperscale data centers and other applications. The Open Domain-Specific Architecture is an industry consortium that aims to enable chiplet-based designs for DSAs and a marketplace to source chiplets. The ODSA aims to build on recent growth in commercial chiplet-based products from major vendors. This paper reviews the significant activities in the ODSA - an open chiplet interface, open prototypes and workflows and open data for community learning. The paper concludes with a discussion on the readiness of open efforts to enable chiplet-based product development.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3477206.3477462',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Data-flow-sensitive fault-space pruning for the injection of transient hardware faults',\n",
       "  'authors': \"['Oskar Pusz', 'Christian Dietrich', 'Daniel Lohmann']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': 'LCTES 2021: Proceedings of the 22nd ACM SIGPLAN/SIGBED International Conference on Languages, Compilers, and Tools for Embedded Systems',\n",
       "  'abstract': 'In the domain of safety-critical systems, fault injection campaigns on ISA-level have become a widespread approach to systematically assess the resilience of a system with respect to transient hardware faults. However, experimentally injecting all possible faults to achieve full fault-space coverage is infeasible in practice. Hence, pruning techniques, such as def/use pruning are commonly applied to reduce the campaign size by grouping injections that surely provoke the same erroneous behavior. We describe data-flow pruning, a new data-flow sensitive fault-space pruning method that extends on def/use-pruning by also considering the instructions’ semantics when deriving fault-equivalence sets. By tracking the information flow for each bit individually across the respective instructions and considering their fault-masking capability, data-flow pruning (DFP) has to plan fewer pilot injections as it derives larger fault-equivalence sets. Like def/use pruning, DFP is precise and complete and it can be used as a direct replacement/alternative in existing software-based fault-injection tools. Our prototypical implementation so far considers local fault equivalence for five types of instructions. In our experimental evaluation, this already reduces the number of necessary injections by up to 18 percent compared to def/use pruning.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3461648.3463851',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Capability Boehm: challenges and opportunities for garbage collection with capability hardware',\n",
       "  'authors': \"['Dejice Jacob', 'Jeremy Singer']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': 'VEE 2022: Proceedings of the 18th ACM SIGPLAN/SIGOPS International Conference on Virtual Execution Environments',\n",
       "  'abstract': 'The Boehm-Demers-Weiser Garbage Collector (BDWGC) is a widely used, production-quality memory management framework for C and C++ applications. In this work, we describe our experiences in adapting BDWGC for modern capability hardware, in particular the CHERI system, which provides guarantees about memory safety due to runtime enforcement of fine-grained pointer bounds and permissions. Although many libraries and applications have been ported to CHERI already, to the best of our knowledge this is the first analysis of the complexities of transferring a garbage collector to CHERI. We describe various challenges presented by the CHERI micro-architectural constraints, along with some significant opportunities for runtime optimization. Since we do not yet have access to capability hardware, we present a limited study of software event counts on emulated micro-benchmarks. This experience report should be helpful to other systems implementors as they attempt to support the ongoing CHERI initiative.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3516807.3516823',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A reference architecture for functional interoperability in robotics',\n",
       "  'authors': \"['László Németh', 'Gustavo Quiros Araya', 'William Regli', 'András Varró']\",\n",
       "  'date': 'May 2021',\n",
       "  'source': \"Destion '21: Proceedings of the Workshop on Design Automation for CPS and IoT\",\n",
       "  'abstract': 'The ability of robotic platforms to be resilient to changes in demand, changes in the environment, changes in the dimensions or weight of the workpiece, replacement of a robot, in other words to be functionally interoperable, is a vexing issue for the entire industry, and it is particularly important for small and medium enterprises (SMEs) where these kinds of changes are the norm. We propose the concept of functional interoperability, propose interoperability scenarios as a way to measuring it and based on our experiences in an ARM funded project we propose an architecture, and a few variations on it, which exhibit the much desired behaviour.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3445034.3460506',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': \"You don't need a Microservices Architecture (yet): Monoliths may do the trick\",\n",
       "  'authors': \"['Dimitrios Gravanis', 'George Kakarontzas', 'Vassilis Gerogiannis']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"ESSE '21: Proceedings of the 2021 European Symposium on Software Engineering\",\n",
       "  'abstract': 'Within the past decade, the advent of cloud computing in terms of infrastructure, technology stacks, availability of services and tooling, along with the gradual improvement of its market environment, has driven many organizations to either consider or migrate many existing software systems to the cloud, either fully or partially. A common predicament in most cases, is the existence of a complex, monolithic application, potentially considered legacy at the time, that was not designed to be cloud-native and therefore requires a degree of redesign/reimplementation in order to benefit from cloud deployment. In such cases, the decomposition of the monolith to a set of loosely coupled, highly cohesive and self-contained microservices is a valid recommendation, provided that the organization is prepared to withstand the additional cost, in terms of human and financial resources, along with the unavoidable development overhead, which is inevitable during the early stages. However, the tendency of the tech world to embrace new trends and jump on hype trains for fear of obsoletion, has led to an excessive adoption of the microservices architecture (MSA), even in cases where such an architecture is not viable for the organization, or does not derive from any business requirements. This research focuses on establishing the position of a traditional monolith in the modern software architecture landscape and determine use cases that can still benefit from this paradigm, as well as use cases that could benefit from a partial or full transition to microservices architectures instead.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3501774.3501780',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': '\"The Network Is an Excuse\": Hardware Maintenance Supporting Community',\n",
       "  'authors': \"['Philip Garrison', 'Esther Han Beol Jang', 'Michael A. Lithgow', 'Nicolás Andrés Pace']\",\n",
       "  'date': 'None',\n",
       "  'source': 'Proceedings of the ACM on Human-Computer Interaction',\n",
       "  'abstract': 'The global community networking movement promotes locally-managed network infrastructure as a strategy for affordable Internet connectivity. This case study investigates a group of collectively managed WiFi Internet networks in Argentina and the technologists who design the networking hardware and software. Members of these community networks collaborate on maintenance and repair and practice new forms of collective work. Drawing on Actor-Network Theory, we show that the networking technologies play a role in the social relations of their maintenance and that they are intentionally configured to do so. For technology designers and deployers, we suggest a path beyond designing for easy repair: since every breakdown is an opportunity to learn, we should design for accessible repair experiences that enable effective collaborative learning.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3479608',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Formal Verification of a Multiprocessor Hypervisor on Arm Relaxed Memory Hardware',\n",
       "  'authors': \"['Runzhou Tao', 'Jianan Yao', 'Xupeng Li', 'Shih-Wei Li', 'Jason Nieh', 'Ronghui Gu']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"SOSP '21: Proceedings of the ACM SIGOPS 28th Symposium on Operating Systems Principles\",\n",
       "  'abstract': \"Concurrent systems software is widely-used, complex, and error-prone, posing a significant security risk. We introduce VRM, a new framework that makes it possible for the first time to verify concurrent systems software, such as operating systems and hypervisors, on Arm relaxed memory hardware. VRM defines a set of synchronization and memory access conditions such that a program that satisfies these conditions can be mostly verified on a sequentially consistent hardware model and the proofs will automatically hold on relaxed memory hardware. VRM can be used to verify concurrent kernel code that is not data race free, including code responsible for managing shared page tables in the presence of relaxed MMU hardware. Using VRM, we verify the security guarantees of a retrofitted implementation of the Linux KVM hypervisor on Arm. For multiple versions of KVM, we prove KVM's security properties on a sequentially consistent model, then prove that KVM satisfies VRM's required program conditions such that its security proofs hold on Arm relaxed memory hardware. Our experimental results show that the retrofit and VRM conditions do not adversely affect the scalability of verified KVM, as it performs similar to unmodified KVM when concurrently running many multiprocessor virtual machines with real application workloads on Arm multiprocessor server hardware. Our work is the first machine-checked proof for concurrent systems software on Arm relaxed memory hardware.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3477132.3483560',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'HECTOR-V: A Heterogeneous CPU Architecture for a Secure RISC-V Execution Environment',\n",
       "  'authors': \"['Pascal Nasahl', 'Robert Schilling', 'Mario Werner', 'Stefan Mangard']\",\n",
       "  'date': 'May 2021',\n",
       "  'source': \"ASIA CCS '21: Proceedings of the 2021 ACM Asia Conference on Computer and Communications Security\",\n",
       "  'abstract': 'To ensure secure and trustworthy execution of applications in potentially insecure environments, vendors frequently embed trusted execution environments (TEE) into their systems. Applications executed in this safe, isolated space are protected from adversaries, including a malicious operating system. TEEs are usually build by integrating protection mechanisms directly into the processor or by using dedicated external secure elements. However, both of these approaches only cover a narrow threat model resulting in limited security guarantees. Enclaves nested into the application processor typically provide weak isolation between the secure and non-secure domain, especially when considering side-channel attacks. Although external secure elements do provide strong isolation, the slow communication interface to the application processor is exposed to adversaries and restricts the use cases. Independently of the used approach, TEEs often lack the possibility to establish secure communication to peripherals, and most operating systems executed inside TEEs do not provide state-of-the-art defense strategies, making them vulnerable to various attacks. We argue that TEEs, such as Intel SGX or ARM TrustZone, implemented on the main application processor, are insecure, especially when considering side-channel attacks. In this paper, we demonstrate how a heterogeneous multicore architecture can be utilized to realize a secure TEE design. We directly embed a secure processor into our HECTOR-V architecture to provide strong isolation between the secure and non-secure domain. The tight coupling of the TEE and the application processor enables HECTOR-V to provide mechanisms for establishing secure communication channels between different devices. We further introduce RISC-V Secure Co-Processor (RVSCP), a security-hardened processor tailored for TEEs. To secure applications executed inside the TEE, RVSCP provides hardware enforced control-flow integrity and rigorously restricts I/O accesses to certain execution states. RVSCP reduces the trusted computing base to a minimum by providing operating system services directly in hardware.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3433210.3453112',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Authenticated key-value stores with hardware enclaves',\n",
       "  'authors': \"['Kai Li', 'Yuzhe Tang', 'Qi Zhang', 'Jianliang Xu', 'Ju Chen']\",\n",
       "  'date': 'December 2021',\n",
       "  'source': \"Middleware '21: Proceedings of the 22nd International Middleware Conference: Industrial Track\",\n",
       "  'abstract': 'Authenticated data storage on an untrusted platform is an important computing paradigm for cloud applications ranging from data outsourcing, to cryptocurrency and general transparency logs. These modern applications increasingly feature update-intensive workloads, whereas existing authenticated data structures (ADSs) designed with in-place updates are inefficient to handle such workloads. This work addresses the issue and presents a novel authenticated log-structured merge tree (eLSM) based key-value store built on Intel SGX. We present a system design that runs the code of eLSM store inside enclave. To circumvent the limited enclave memory (128 MB with the latest Intel CPUs), we propose to place the memory buffer of the eLSM store outside the enclave and protect the buffer using a new authenticated data structure by digesting individual LSM-tree levels. We design protocols to support data integrity, (range) query completeness, and freshness. Our protocol causes small proofs by including the Merkle proofs at selected levels. We implement eLSM on top of Google LevelDB and Facebook RocksDB with minimal code change and performance interference. We evaluate the performance of eLSM under the YCSB workload benchmark and show a performance advantage of up to 4.5X speedup.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3491084.3491425',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'ECCNAS: Efficient Crowd Counting Neural Architecture Search',\n",
       "  'authors': \"['Yabin Wang', 'Zhiheng Ma', 'Xing Wei', 'Shuai Zheng', 'Yaowei Wang', 'Xiaopeng Hong']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Multimedia Computing, Communications, and Applications',\n",
       "  'abstract': 'Recent solutions to crowd counting problems have already achieved promising performance across various benchmarks. However, applying these approaches to real-world applications is still challenging, because they are computation intensive and lack the flexibility to meet various resource budgets. In this article, we propose an efficient crowd counting neural architecture search (ECCNAS) framework to search efficient crowd counting network structures, which can fill this research gap. A novel search from pre-trained strategy enables our cross-task NAS to explore the significantly large and flexible search space with less search time and get more proper network structures. Moreover, our well-designed search space can intrinsically provide candidate neural network structures with high performance and efficiency. In order to search network structures according to hardwares with different computational performance, we develop a novel latency cost estimation algorithm in our ECCNAS. Experiments show our searched models get an excellent trade-off between computational complexity and accuracy and have the potential to deploy in practical scenarios with various resource budgets. We reduce the computational cost, in terms of multiply-and-accumulate (MACs), by up to 96% with comparable accuracy. And we further designed experiments to validate the efficiency and the stability improvement of our proposed search from pre-trained strategy.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3465455',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Pythia: A Customizable Hardware Prefetching Framework Using Online Reinforcement Learning',\n",
       "  'authors': \"['Rahul Bera', 'Konstantinos Kanellopoulos', 'Anant Nori', 'Taha Shahroodi', 'Sreenivas Subramoney', 'Onur Mutlu']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"MICRO '21: MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture\",\n",
       "  'abstract': 'Past research has proposed numerous hardware prefetching techniques, most of which rely on exploiting one specific type of program context information (e.g., program counter, cacheline address, or delta between cacheline addresses) to predict future memory accesses. These techniques either completely neglect a prefetcher’s undesirable effects (e.g., memory bandwidth usage) on the overall system, or incorporate system-level feedback as an afterthought to a system-unaware prefetch algorithm. We show that prior prefetchers often lose their performance benefit over a wide range of workloads and system configurations due to their inherent inability to take multiple different types of program context and system-level feedback information into account while prefetching. In this paper, we make a case for designing a holistic prefetch algorithm that learns to prefetch using multiple different types of program context and system-level feedback information inherent to its design.  To this end, we propose Pythia, which formulates the prefetcher as a reinforcement learning agent. For every demand request, Pythia observes multiple different types of program context information to make a prefetch decision. For every prefetch decision, Pythia receives a numerical reward that evaluates prefetch quality under the current memory bandwidth usage. Pythia uses this reward to reinforce the correlation between program context information and prefetch decision to generate highly accurate, timely, and system-aware prefetch requests in the future. Our extensive evaluations using simulation and hardware synthesis show that Pythia outperforms two state-of-the-art prefetchers (MLOP and Bingo) by 3.4% and 3.8% in single-core, 7.7% and 9.6% in twelve-core, and 16.9% and 20.2% in bandwidth-constrained core configurations, while incurring only 1.03% area overhead over a desktop-class processor and no software changes in workloads. The source code of Pythia can be freely downloaded from https://github.com/CMU-SAFARI/Pythia.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3466752.3480114',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Taurus: a data plane architecture for per-packet ML',\n",
       "  'authors': \"['Tushar Swamy', 'Alexander Rucker', 'Muhammad Shahbaz', 'Ishan Gaur', 'Kunle Olukotun']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': \"Emerging applications---cloud computing, the internet of things, and augmented/virtual reality---demand responsive, secure, and scalable datacenter networks. These networks currently implement simple, per-packet, data-plane heuristics (e.g., ECMP and sketches) under a slow, millisecond-latency control plane that runs data-driven performance and security policies. However, to meet applications' service-level objectives (SLOs) in a modern data center, networks must bridge the gap between line-rate, per-packet execution and complex decision making.   In this work, we present the design and implementation of Taurus, a data plane for line-rate inference. Taurus adds custom hardware based on a flexible, parallel-patterns (MapReduce) abstraction to programmable network devices, such as switches and NICs; this new hardware uses pipelined SIMD parallelism to enable per-packet MapReduce operations (e.g., inference). Our evaluation of a Taurus switch ASIC---supporting several real-world models---shows that Taurus operates orders of magnitude faster than a server-based control plane while increasing area by 3.8% and latency for line-rate ML models by up to 221 ns. Furthermore, our Taurus FPGA prototype achieves full model accuracy and detects two orders of magnitude more events than a state-of-the-art control-plane anomaly-detection system.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503222.3507726',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Profiling Neural Blocks and Design Spaces for Mobile Neural Architecture Search',\n",
       "  'authors': \"['Keith G. Mills', 'Fred X. Han', 'Jialin Zhang', 'Seyed Saeed Changiz Rezaei', 'Fabian Chudak', 'Wei Lu', 'Shuo Lian', 'Shangling Jui', 'Di Niu']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"CIKM '21: Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management\",\n",
       "  'abstract': 'Neural architecture search automates neural network design and has achieved state-of-the-art results in many deep learning applications. While recent literature has focused on designing networks to maximize accuracy, little work has been conducted to understand the compatibility of architecture design spaces to varying hardware. In this paper, we analyze the neural blocks used to build Once-for-All (MobileNetV3), ProxylessNAS and ResNet families, in order to understand their predictive power and inference latency on various devices, including Huawei Kirin 9000 NPU, RTX 2080 Ti, AMD Threadripper 2990WX, and Samsung Note10. We introduce a methodology to quantify the friendliness of neural blocks to hardware and the impact of their placement in a macro network on overall network performance via only end-to-end measurements. Based on extensive profiling results, we derive design insights and apply them to hardware-specific search space reduction. We show that searching in the reduced search space generates better accuracy-latency Pareto frontiers than searching in the original search spaces, customizing architecture search according to the hardware. Moreover, insights derived from measurements lead to notably higher ImageNet top-1 scores on all search spaces investigated.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3459637.3481944',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Neural architecture search as program transformation exploration',\n",
       "  'authors': '[\\'Jack Turner\\', \\'Elliot J. Crowley\\', \"Michael F. P. O\\'Boyle\"]',\n",
       "  'date': 'April 2021',\n",
       "  'source': \"ASPLOS '21: Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'Improving the performance of deep neural networks (DNNs) is important to both the compiler and neural architecture search (NAS) communities. Compilers apply program transformations in order to exploit hardware parallelism and memory hierarchy. However, legality concerns mean they fail to exploit the natural robustness of neural networks. In contrast, NAS techniques mutate networks by operations such as the grouping or bottlenecking of convolutions, exploiting the resilience of DNNs. In this work, we express such neural architecture operations as program transformations whose legality depends on a notion of representational capacity. This allows them to be combined with existing transformations into a unified optimization framework. This unification allows us to express existing NAS operations as combinations of simpler transformations. Crucially, it allows us to generate and explore new tensor convolutions. We prototyped the combined framework in TVM and were able to find optimizations across different DNNs, that significantly reduce inference time - over 3× in the majority of cases. Furthermore, our scheme dramatically reduces NAS search time.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3445814.3446753',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Noema: Hardware-Efficient Template Matching for Neural Population Pattern Detection',\n",
       "  'authors': \"['Ameer M. S. Abdelhadi', 'Eugene Sha', 'Ciaran Bannon', 'Hendrik Steenland', 'Andreas Moshovos']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"MICRO '21: MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture\",\n",
       "  'abstract': 'Repeating patterns of activity across neurons is thought to be key to understanding how the brain represents, reacts, and learns. Advances in imaging and electrophysiology allow us to observe activities of groups of neurons in real-time, with ever increasing detail. Detecting patterns over these activity streams is an effective means to explore the brain, and to detect memories, decisions, and perceptions in real-time while driving effectors such as robotic arms, or augmenting and repairing brain function. Template matching is a popular algorithm for detecting recurring patterns in neural populations and has primarily been implemented on commodity systems. Unfortunately, template matching is memory intensive and computationally expensive. This has prevented its use in portable applications, such as neuroprosthetics, which are constrained by latency, form-factor, and energy. We present Noema a dedicated template matching hardware accelerator that overcomes these limitations. Noema is designed to overcome the key bottlenecks of existing implementations: binning that converts the incoming bit-serial neuron activity streams into a stream of aggregate counts, memory storage and traffic for the templates and the binned stream, and the extensive use of floating-point arithmetic. The key innovation in Noema is a reformulation of template matching that enables computations to proceed progressively as data is received without binning while generating numerically identical results. This drastically reduces latency when most computations can now use simple, area- and energy efficient bit- and integer-arithmetic units. Furthermore, Noema implements template encoding to greatly reduce template memory storage and traffic. Noema is a hierarchical and scalable design where the bulk of its units are low-cost and can be readily replicated and their frequency can be adjusted to meet a variety of energy, area, and computation constraints.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3466752.3480121',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Controllable Person Image Synthesis GAN and Its Reconfigurable Energy-efficient Hardware Implementation',\n",
       "  'authors': \"['Shaoyue Lin', 'Yanjun Zhang']\",\n",
       "  'date': 'March 2022',\n",
       "  'source': \"ICIAI '22: Proceedings of the 2022 6th International Conference on Innovation in Artificial Intelligence\",\n",
       "  'abstract': 'At this stage, how to controllably generate higher quality person image is still the challenge of person image synthesis. At the same time, the update of image synthesis network is far ahead of its hardware implementation. Therefore, this paper proposes a GAN network for person image synthesis that can generate high quality person image with controllable pose and attributes. The newly designed network is more convenient for hardware implementation while ensuring that the generated image is controllable. This paper also designs a synthesizable library for GAN to pursue faster hardware reconfiguration. We completed the new model proposed in this paper based on this library. Finally, the proposed network achieves better results both quantitatively and qualitatively compared with previous work. Compared with GPU and CPU, the hardware implementation based on FPGA can achieve the highest energy efficient of 73.67 GOPS / W.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3529466.3529500',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'JetStream: Graph Analytics on Streaming Data with Event-Driven Hardware Accelerator',\n",
       "  'authors': \"['Shafiur Rahman', 'Mahbod Afarin', 'Nael Abu-Ghazaleh', 'Rajiv Gupta']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"MICRO '21: MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture\",\n",
       "  'abstract': 'Graph Processing is at the core of many critical emerging workloads operating on unstructured data, including social network analysis, bioinformatics, and many others. Many applications operate on graphs that are constantly changing, i.e., new nodes and edges are added or removed over time. In this paper, we present JetStream, a hardware accelerator for evaluating queries over streaming graphs and capable of handling additions, deletions, and updates of edges. JetStream extends a recently proposed event-based accelerator for graph workloads to support streaming updates. It handles both accumulative and monotonic graph algorithms via an event-driven computation model that limits accesses to a smaller subset of the graph vertices, efficiently reuses the prior query results to eliminate redundancy, and optimizes the memory access pattern for enhanced memory bandwidth utilization. To the best of our knowledge, JetStream is the first graph accelerator that supports streaming graphs, reducing the computation time by 90% compared with cold-start computation using an existing accelerator. In addition, JetStream achieves about 18 × speedup over KickStarter and GraphBolt software frameworks at the large baseline batch sizes that these systems use with significantly higher speedup at smaller batch sizes.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3466752.3480126',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Hardware-In-The-Loop Labs for SCADA Cybersecurity Awareness and Training',\n",
       "  'authors': \"['Maxime Puys', 'Pierre-Henri Thevenon', 'Stéphane Mocanu']\",\n",
       "  'date': 'August 2021',\n",
       "  'source': \"ARES '21: Proceedings of the 16th International Conference on Availability, Reliability and Security\",\n",
       "  'abstract': 'In this paper, we present a SCADA cybersecurity awareness and training program based on a Hands-On training using two twin cyber-ranges named WonderICS and G-ICS. These labs are built using a Hardware-In-the-Loop simulation system of the physical process developed by the two partners. The cyber-ranges allow replication of realistic Advanced Persistent Threat (APT) attacks and demonstration of known vulnerabilities, as they rely on real industrial control devices and softwares. In this work, we present both the demonstration scenarios used for awareness on WonderICS and the training programs developed for graduate students on G-ICS.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3465481.3469185',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Omegaflow: a high-performance dependency-based architecture',\n",
       "  'authors': \"['Yaoyang Zhou', 'Zihao Yu', 'Chuanqi Zhang', 'Yinan Xu', 'Huizhe Wang', 'Sa Wang', 'Ninghui Sun', 'Yungang Bao']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ICS '21: Proceedings of the ACM International Conference on Supercomputing\",\n",
       "  'abstract': 'This paper investigates how to better track and deliver dependency in dependency-based cores to exploit instruction-level parallelism (ILP) as much as possible. To this end, we first propose an analytical performance model for the state-of-art dependency-based core, Forwardflow, and figure out two vital factors affecting its upper bound of performance. Then we propose Omegaflow,a dependency-based architecture adopting three new techniques, which respond to the discovered factors. Experimental results show that Omegaflow improves IPC by 24.6% compared to the state-of-the-art design, approaching the performance of the OoO architecture with an ideal scheduler (94.4%) without increasing the clock cycle and consumes only 8.82% more energy than Forwardflow.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3447818.3460367',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Searching a Hierarchically Aggregated Fusion Architecture for Fast Multi-Modality Image Fusion',\n",
       "  'authors': \"['Risheng Liu', 'Zhu Liu', 'Jinyuan Liu', 'Xin Fan']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"MM '21: Proceedings of the 29th ACM International Conference on Multimedia\",\n",
       "  'abstract': 'Multi-modality image fusion refers to generating a complementary image that integrates typical characteristics from source images. In recent years, we have witnessed the remarkable progress of deep learning models for multi-modality fusion. Existing CNN-based approaches strain every nerve to design various architectures for realizing these tasks in an end-to-end manner. However, these handcrafted designs are unable to cope with the high demanding fusion tasks, resulting in blurred targets and lost textural details. To alleviate these issues, in this paper, we propose a novel approach, aiming at searching effective architectures according to various modality principles and fusion mechanisms. Specifically, we construct a hierarchically aggregated fusion architecture to extract and refine fused features from feature-level and object-level fusion perspectives, which is responsible for obtaining complementary target/detail representations. Then by investigating diverse effective practices, we composite a more flexible fusion-specific search space. Motivated by the collaborative principle, we employ a new search strategy with different principled losses and hardware constraints for sufficient discovery of components. As a result, we can obtain a task-specific architecture with fast inference time. Extensive quantitative and qualitative results demonstrate the superiority and versatility of our method against state-of-the-art methods.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3474085.3475299',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Supporting Autonomous Vehicle Applications on the Heterogeneous System Architecture',\n",
       "  'authors': \"['Nandinbaatar Tsog', 'Marielle Gallardo', 'Sweta Chakraborty', 'Torbjörn Martinson', 'Alexandra Hengl', 'Magnus Moberg', 'Adem Sen', 'Mobyen Uddin Ahmed', 'Shahina Begum', 'Moris Behnam', 'Mikael Sjödin', 'Saad Mubeen']\",\n",
       "  'date': 'May 2021',\n",
       "  'source': 'ECBS 2021: 7th Conference on the Engineering of Computer Based Systems',\n",
       "  'abstract': 'The contemporary processors are unable to meet the increasing data-intensive and computation-demanding requirements in autonomous vehicle software applications. Recently, the new Heterogeneous System Architecture (HSA) has emerged as a promising solution to meet these requirements. The HSA reduces the latency of data exchange between the compute units and cache-coherent shared memory, which is not supported by the non-HSA compliant heterogeneous platforms with acceleration support. The main goal of the paper is to investigate the performance gain by the HSA and conduct a comparative evaluation of the HSA and non-HSA compliant heterogeneous platforms. The paper aims at evaluating these platforms by using two computation-intensive software functions in autonomous vehicles, namely the object detection and vehicle movement. In order to achieve this goal, the CUDA-accelerated source code of the functions is ported from a non-HSA compliant heterogeneous platform to the HSA platform. In this regard, the paper presents the architecture of a proof-of-concept prototype and provides evaluation using the prototype.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3459960.3459970',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Algorithm-hardware co-design for efficient brain-inspired hyperdimensional learning on edge',\n",
       "  'authors': \"['Yang Ni', 'Yeseong Kim', 'Tajana Rosing', 'Mohsen Imani']\",\n",
       "  'date': 'March 2022',\n",
       "  'source': \"DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe\",\n",
       "  'abstract': 'Machine learning methods have been widely utilized to provide high quality for many cognitive tasks. Running sophisticated learning tasks requires high computational costs to process a large amount of learning data. Brain-inspired Hyperdimensional Computing (HDC) is introduced as an alternative solution for lightweight learning on edge devices. However, HDC models still rely on accelerators to ensure real-time and efficient learning. These hardware designs are not commercially available and need a relatively long period to synthesize and fabricate after deriving the new applications. In this paper, we propose an efficient framework for accelerating the HDC at the edge by fully utilizing the available computing power. We optimize the HDC through algorithm-hardware co-design of the host CPU and existing low-power machine learning accelerators, such as Edge TPU. We interpret the lightweight HDC learning model as a hyper-wide neural network to take advantage of the accelerator and machine learning platform. We further improve the runtime cost of training by employing a bootstrap aggregating algorithm called bagging while maintaining the learning quality. We evaluate the performance of the proposed framework with several applications. Joint experiments on mobile CPU and the Edge TPU show that our framework achieves 4.5X faster training and 4.2X faster inference compared to the baseline platform. In addition, our framework achieves 19.4X faster training and 8.9X faster inference as compared to embedded ARM CPU, Raspberry Pi, that consumes similar power consumption.',\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3539845.3539920',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Design and Evaluation of a Tunable PUF Architecture for FPGAs',\n",
       "  'authors': \"['Franz-Josef Streit', 'Paul Krüger', 'Andreas Becher', 'Stefan Wildermann', 'Jürgen Teich']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Reconfigurable Technology and Systems',\n",
       "  'abstract': 'FPGA-based Physical Unclonable Functions (PUF) have emerged as a viable alternative to permanent key storage by turning effects of inaccuracies during the manufacturing process of a chip into a unique, FPGA-intrinsic secret. However, many fixed PUF designs may suffer from unsatisfactory statistical properties in terms of uniqueness, uniformity, and robustness. Moreover, a PUF signature may alter over time due to aging or changing operating conditions, rendering a PUF insecure in the worst case. As a remedy, we propose CHOICE, a novel class of FPGA-based PUF designs with tunable uniqueness and reliability characteristics. By the use of addressable shift registers available on an FPGA, we show that a wide configuration space for adjusting a device-specific PUF response is obtained without any sacrifice of randomness. In particular, we demonstrate the concept of address-tunable propagation delays, whereby we are able to increase or decrease the probability of obtaining “1”s in the PUF response. Experimental evaluations on a group of six 28\\xa0nm Xilinx Artix-7 FPGAs show that CHOICE PUFs provide a large range of configurations to allow a fine-tuning to an average uniqueness between 49% and 51%, while simultaneously achieving bit error rates below 1.5%, thus outperforming state-of-the-art PUF designs. Moreover, with only a single FPGA slice per PUF bit, CHOICE is one of the smallest PUF designs currently available for FPGAs. It is well-known that signal propagation delays are affected by temperature, as the operating temperature impacts the internal currents of transistors that ultimately make up the circuit. We therefore comprehensively investigate how temperature variations affect the PUF response and demonstrate how the tunability of CHOICE enables us to determine configurations that show a high robustness to such variations. As a case study, we present a cryptographic key generation scheme based on CHOICE PUF responses as device-intrinsic secret and investigate the design objectives resource costs, performance, and temperature robustness to show the practicability of our approach.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3491237',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Revamping hardware persistency models: view-based and axiomatic persistency models for Intel-x86 and Armv8',\n",
       "  'authors': \"['Kyeongmin Cho', 'Sung-Hwan Lee', 'Azalea Raad', 'Jeehoon Kang']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': 'PLDI 2021: Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation',\n",
       "  'abstract': 'Non-volatile memory (NVM) is a cutting-edge storage technology that promises the performance of DRAM with the durability of SSD. Recent work has proposed several persistency models for mainstream architectures such as Intel-x86 and Armv8, describing the order in which writes are propagated to NVM. However, these models have several limitations; most notably, they either lack operational models or do not support persistent synchronization patterns.  We close this gap by revamping the existing persistency models. First, inspired by the recent work on promising semantics, we propose a unified operational style for describing persistency using views, and develop view-based operational persistency models for Intel-x86 and Armv8, thus presenting the first operational model for Armv8 persistency. Next, we propose a unified axiomatic style for describing hardware persistency, allowing us to recast and repair the existing axiomatic models of Intel-x86 and Armv8 persistency. We prove that our axiomatic models are equivalent to the authoritative semantics reviewed by Intel and Arm engineers. We further prove that each axiomatic hardware persistency model is equivalent to its operational counterpart. Finally, we develop a persistent model checking algorithm and tool, and use it to verify several representative examples.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3453483.3454027',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A high performance architecture for object detection in drones',\n",
       "  'authors': \"['Ioannis Kwnsantinos Galanakis', 'Athanasios Milidonis', 'Ioannis Voyiatzis']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"PCI '21: Proceedings of the 25th Pan-Hellenic Conference on Informatics\",\n",
       "  'abstract': 'Nowadays the use of drones in daily life is becoming more and more frequent. One important service of drones is object detection which is used in surveillance and search-and-rescue missions. High performance is a critical requirement for object detection, since drones travel at high flying speeds. Current systems mainly use software platforms which have limited performance. In this paper a hardware architecture is presented for object detection in drones. The ability of the proposed architecture to exploit parallelization in object detection tasks more efficiently than in software platforms, offers lower execution time and more accurate and faster results. The proposed architecture is implemented using an FPGA platform. Experimental results show the benefits of this architecture compared to a software platform in terms of performance and detection-precision.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503823.3503868',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'All-Optical Neural Network Tanh Architecture with MZI',\n",
       "  'authors': \"['Ruizhen Wu', 'Jingjing Chen', 'Ping Huang', 'Lin Wang']\",\n",
       "  'date': 'September 2021',\n",
       "  'source': \"MLMI '21: Proceedings of the 2021 4th International Conference on Machine Learning and Machine Intelligence\",\n",
       "  'abstract': \"Artificial neural networks (ANNs) have been widely used for industrial applications and have played a more and more important role in fundamental research but the electronic-based integrated circuits are limited by Moore's law. Optical neural networks (ONNs) can process information in parallel and have low energy advantages. The MZI with Gridnet or FFTnet can realize the convolution calculation is already proved by lots of researches. But the activation functions still have to use the DA/ADC to do the photoelectric conversion and then be calculated in electronic-based hardware systems. So an optical Tanh architecture with MZI is proposed in this paper for the all-optical neural network's activation function. Which can achieve the accuracy of Lenet-5 with MNIST dataset to 86.49%.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3490725.3490740',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'STAP: An Architecture and Design Tool for Automata Processing on Memristor TCAMs',\n",
       "  'authors': \"['João Paulo Cardoso de Lima', 'Marcelo Brandalero', 'Michael Hübner', 'Luigi Carro']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Journal on Emerging Technologies in Computing Systems',\n",
       "  'abstract': 'Accelerating finite-state automata benefits several emerging application domains that are built on pattern matching. In-memory architectures, such as the Automata Processor (AP), are efficient to speed them up, at least for outperforming traditional von-Neumann architectures. In spite of the AP’s massive parallelism, current APs suffer from poor memory density, inefficient routing architectures, and limited capabilities. Although these limitations can be lessened by emerging memory technologies, its architecture is still the major source of huge communication demands and lack of scalability. To address these issues, we present STAP, a Scalable TCAM-based architecture for Automata Processing. STAP adopts a reconfigurable array of processing elements, which are based on memristive Ternary CAMs (TCAMs), to efficiently implement Non-deterministic finite automata (NFAs) through proper encoding and mapping methods. The CAD tool for STAP integrates the design flow of automata applications, a specific mapping algorithm, and place and route tools for connecting processing elements by RRAM-based programmable interconnects. Results showed 1.47× higher throughput when processing 16-bit input symbols, and improvements of 3.9× and 25× on state and routing densities over the state-of-the-art AP, while preserving 104 programming cycles.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3450769',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Hardware Acceleration of High-Performance Computational Flow Dynamics Using High-Bandwidth Memory-Enabled Field-Programmable Gate Arrays',\n",
       "  'authors': \"['Tom Hogervorst', 'Răzvan Nane', 'Giacomo Marchiori', 'Tong Dong Qiu', 'Markus Blatt', 'Alf Birger Rustad']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Reconfigurable Technology and Systems',\n",
       "  'abstract': 'Scientific computing is at the core of many High-Performance Computing applications, including computational flow dynamics. Because of the utmost importance to simulate increasingly larger computational models, hardware acceleration is receiving increased attention due to its potential to maximize the performance of scientific computing. Field-Programmable Gate Arrays could accelerate scientific computing because of the possibility to fully customize the memory hierarchy important in irregular applications such as iterative linear solvers. In this article, we study the potential of using Field-Programmable Gate Arrays in High-Performance Computing because of the rapid advances in reconfigurable hardware, such as the increase in on-chip memory size, increasing number of logic cells, and the integration of High-Bandwidth Memories on board. To perform this study, we propose a novel Sparse Matrix-Vector multiplication unit and an ILU0 preconditioner tightly integrated with a BiCGStab solver kernel. We integrate the developed preconditioned iterative solver in Flow from the Open Porous Media project, a state-of-the-art open source reservoir simulator. Finally, we perform a thorough evaluation of the FPGA solver kernel in both stand-alone mode and integrated in the reservoir simulator, using the NORNE field, a real-world case reservoir model using a grid with more than 105 cells and using three unknowns per cell.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3476229',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Neuromorphic Design Using Reward-based STDP Learning on Event-Based Reconfigurable Cluster Architecture',\n",
       "  'authors': \"['Mahyar Shahsavari', 'David Thomas', 'Andrew Brown', 'Wayne Luk']\",\n",
       "  'date': 'July 2021',\n",
       "  'source': 'ICONS 2021: International Conference on Neuromorphic Systems 2021',\n",
       "  'abstract': 'Neuromorphic computing systems simulate spiking neural networks that are used for research into how biological neural networks function, as well as for applied engineering such as robotics, pattern recognition, and machine learning. In this paper, we present a neuromorphic system based on an asynchronous event-based hardware platform. We represent three algorithms for implementing spiking networks on our asynchronous hardware platform. We also discuss different trade-offs between synchronisation and messaging costs. A reinforcement learning method known as Reward-modulated STDP is presented as an online learning algorithm in the network. We evaluate the system performance in a single box of our designed architecture using 6000 concurrent hardware threads and demonstrate scaling to networks with up to 2 million neurons and 400 million synapses. The performance of our architecture is also compared to existing neuromorphic platforms, showing a 20 times speed-up over the Brian simulator on an x86 machine, and a 16 times speed-up over a 48-chip SpiNNaker node.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3477145.3477151',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'TrustCross: Enabling Confidential Interoperability across Blockchains Using Trusted Hardware',\n",
       "  'authors': \"['Ying Lan', 'Jianbo Gao', 'Yue Li', 'Ke Wang', 'Yuesheng Zhu', 'Zhong Chen']\",\n",
       "  'date': 'December 2021',\n",
       "  'source': \"ICBTA '21: Proceedings of the 2021 4th International Conference on Blockchain Technology and Applications\",\n",
       "  'abstract': 'With the rapid development of blockchain technology, different types of blockchains are adopted and interoperability across blockchains has received widespread attention. There have been many cross-chain solutions proposed in recent years, including notary scheme, sidechain, and relay chain. However, most of the existing platforms do not take confidentiality into account, although privacy has become an important concern for blockchain. In this paper, we present TrustCross, a privacy- preserving cross-chain platform to enable confidential interoperability across blockchains. The key insight behind TrustCross is to encrypt cross-chain communication data on the relay chain with the assistance of trusted execution environment and employ fine-grained access control to protect user privacy. Our experimental results show that TrustCross achieves reasonable latency and high scalability on the contract calls across heterogeneous blockchains.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3510487.3510491',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Accelerating Neural Architecture Search with Rank-Preserving Surrogate Models',\n",
       "  'authors': \"['Hadjer Benmeziane', 'Hamza Ouarnoughi', 'Kaoutar El Maghraoui', 'Smail Niar']\",\n",
       "  'date': 'August 2021',\n",
       "  'source': 'ArabWIC 2021: The 7th Annual International Conference on Arab Women in Computing in Conjunction with the 2nd Forum of Women in Research',\n",
       "  'abstract': 'Over the past years, deep learning has enabled significant progress in several tasks, such as image recognition, speech recognition and language modelling. Novel Neural architectures are behind this achievement. However, manually designing these architectures by human experts is time-consuming and error-prone. Neural architecture search (NAS) automates the design process by searching for the best architecture in a huge search space. This search process requires evaluating each sampled architecture via time-consuming training. To speed up NAS algorithms, several existing approaches use surrogate models that predict the neural architectures’ precision instead of training each sampled one. In this paper, we propose RS-NAS for Rank-preserving Surrogate model in NAS, a surrogate model trained with a rank-preserving loss function. We posit that the search algorithm doesn’t need to know the exact accuracy of a candidate architecture but instead needs to know if it is better or worse than others. We thoroughly experiment and validate our surrogate models with state-of-the-art search algorithms. Using the rank-preserving surrogate models, local search in DARTS finds a 2% more accurate architecture than using the NAS-Bench-301 surrogate model on the same search time. The code and models are available: https://github.com/IHIaadj/ranked_nas',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3485557.3485579',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Design of a Robust Memristive Spiking Neuromorphic System with Unsupervised Learning in Hardware',\n",
       "  'authors': \"['Md Musabbir Adnan', 'Sagarvarma Sayyaparaju', 'Samuel D. Brown', 'Mst Shamim Ara Shawkat', 'Catherine D. Schuman', 'Garrett S. Rose']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Journal on Emerging Technologies in Computing Systems',\n",
       "  'abstract': 'Spiking neural networks (SNN) offer a power efficient, biologically plausible learning paradigm by encoding information into spikes. The discovery of the memristor has accelerated the progress of spiking neuromorphic systems, as the intrinsic plasticity of the device makes it an ideal candidate to mimic a biological synapse. Despite providing a nanoscale form factor, non-volatility, and low-power operation, memristors suffer from device-level non-idealities, which impact system-level performance. To address these issues, this article presents a memristive crossbar-based neuromorphic system using unsupervised learning with twin-memristor synapses, fully digital pulse width modulated spike-timing-dependent plasticity, and homeostasis neurons. The implemented single-layer SNN was applied to a pattern-recognition task of classifying handwritten-digits. The performance of the system was analyzed by varying design parameters such as number of training epochs, neurons, and capacitors. Furthermore, the impact of memristor device non-idealities, such as device-switching mismatch, aging, failure, and process variations, were investigated and the resilience of the proposed system was demonstrated.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3451210',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Accelerate hardware logging for efficient crash consistency in persistent memory',\n",
       "  'authors': \"['Zhiyuan Lu', 'Jianhui Yue', 'Yifu Deng', 'Yifeng Zhu']\",\n",
       "  'date': 'March 2022',\n",
       "  'source': \"DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe\",\n",
       "  'abstract': 'While logging has been adopted in persistent memory (PM) to support crash consistency, logging incurs severe performance overhead. This paper discovers two common factors that contribute to the inefficiency of logging: (1) load imbalance among memory banks, and (2) constraints of intra-record ordering. Overloaded memory banks may significantly prolong the waiting time of log requests targeting these banks. To address this issue, we propose a novel log entry allocation scheme (LALEA) that reshapes the traffic distribution over PM banks. In addition, the intra-record ordering between a header and its log entries decreases the degree of parallelism in log operations. We design a log metadata buffering scheme (BLOM) that eliminates the intra-record ordering constraints. These two proposed log optimizations are general and can be applied to many existing designs. We evaluate our designs using both micro-benchmarks and real PM applications. Our experimental results show that LALEA and BLOM can achieve 54.04% and 17.16% higher transaction throughput on average, compared to two state-of-the-art designs, respectively.',\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3539845.3539939',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Synthesizing Formal Models of Hardware from RTL for Efficient Verification of Memory Model Implementations',\n",
       "  'authors': \"['Yao Hsiao', 'Dominic P. Mulligan', 'Nikos Nikoleris', 'Gustavo Petri', 'Caroline Trippel']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"MICRO '21: MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture\",\n",
       "  'abstract': 'Modern hardware complexity makes it challenging to determine if a given microarchitecture adheres to a particular memory consistency model (MCM). This observation inspired the Check tools, which formally check that a specific microarchitecture correctly implements an MCM with respect to a suite of litmus test programs. Unfortunately, despite their effectiveness and efficiency, the Check tools must be supplied a microarchitecture in the guise of a manually constructed axiomatic specification, called a μspec model.  To facilitate MCM verification—and enable the Check tools to consume processor RTL directly—we introduce a methodology and associated tool, rtl2μspec, for automatically synthesizing μspec models from processor designs written in Verilog or SystemVerilog, with the help of modest user-provided design metadata. As a case study, we use rtl2μspec to facilitate the Check-based verification of the four-core RISC-V V-scale (multi-V-scale) processor’s MCM implementation. We show that rtl2μspec can synthesize a complete, and proven correct by construction, μspec model from the SystemVerilog design of the multi-V-scale processor in 6.84 minutes. Subsequent Check-based MCM verification of the synthesized μspec model takes less than one second per litmus test.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3466752.3480087',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A Cost-Efficient Digital ESN Architecture on FPGA for OFDM Symbol Detection',\n",
       "  'authors': \"['Victor M. Gan', 'Yibin Liang', 'Lianjun Li', 'Lingjia Liu', 'Yang Yi']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Journal on Emerging Technologies in Computing Systems',\n",
       "  'abstract': 'The echo state network (ESN) is a recently developed machine-learning paradigm whose processing capabilities rely on the dynamical behavior of recurrent neural networks. Its performance outperforms traditional recurrent neural networks in nonlinear system identification and temporal information processing applications. We design and implement a cost-efficient ESN architecture on field-programmable gate array (FPGA) that explores the full capacity of digital signal processor blocks on low-cost and low-power FPGA hardware. Specifically, our scalable ESN architecture on FPGA exploits Xilinx DSP48E1 units to cut down the need of configurable logic blocks. The proposed architecture includes a linear combination processor with negligible deployment of configurable logic blocks and a high-accuracy nonlinear function approximator. Our work is verified with the prediction task on the classical NARMA dataset and a symbol detection task for orthogonal frequency division multiplexing systems using a wireless communication testbed built on a software-defined radio platform. Experiments and performance measurement show that the new ESN architecture is capable of processing real-world data efficiently for low-cost and low-power applications.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3440017',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'DECA: DoD Enterprise Cloud Architecture Concept for Cloud-Based Cross Domain Solutions',\n",
       "  'authors': \"['Leo Aguilera', 'Doug Jacobson']\",\n",
       "  'date': 'December 2021',\n",
       "  'source': \"AICCC '21: Proceedings of the 2021 4th Artificial Intelligence and Cloud Computing Conference\",\n",
       "  'abstract': 'The Department of Defense (DoD) battlefield exists both in cyber and the physical world. Information sharing is a top priority for the DoD in support of our warfighters and allies. To maintain military technological advantage and superiority, access to information and the capacity to process it are critical components for empowering the warfighter for mission success. The volume of information shared has increased exponentially, necessitating the development of a DoD enterprise cloud capable of sustaining, and supporting strategic worldwide DoD missions through effective information sharing. However, the existing U.S. Government cloud design does not support enterprise use, and legacy software and hardware applications such as a Cross Domain Solution (CDS) will need to be re-architected, certified, accredited, and authorized for future enterprise cloud use. A CDS is a requirement for information sharing in both unclassified and classified systems and information transmission from one system to another, but there must also be a DoD enterprise cloud structure to leverage the CDS in the U.S. Government cloud. The purpose of this research is to explore the future possibilities of using an enterprise cloud CDS and to present a conceptual design for a DoD enterprise cloud architecture that will save the DoD time and money in the certification process while also allowing efficient information sharing across multiple DoD Command and Control (C2) systems. To have this architecture design approved and accredited by the DoD for future use, we adhere to the Federal Risk and Authorization Management Program (FedRAMP), a process required for federal agency cloud deployments and the National Institute of Standards and Technology (NIST) standards. We use existing systems from across the DoD and allies found in the open literature as a baseline.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3508259.3508283',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Reducing network latency in industrial IoT systems using hardware-software complex based on injection',\n",
       "  'authors': \"['Alexey Slepnev', 'Danila Matveev', 'Evgenii Karelin', 'Ivan Tumanov', 'Ivan Vnukov', 'Ruslan Kirichek', 'Igor Kandakov']\",\n",
       "  'date': 'December 2021',\n",
       "  'source': 'ICFNDS 2021: The 5th International Conference on Future Networks &amp; Distributed Systems',\n",
       "  'abstract': 'Over the past couple of years, the Internet and data networks have evolved noticeably. The number of IoT devices is increasing every year, and according to forecasts, it will continue to grow. Because of this, the shortcomings of today’s network architecture are becoming more and more apparent. In this article, we present a universal system for optimizing networks, in which it will be built.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3508072.3508227',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Automating the design flow under dynamic partial reconfiguration for hardware-software co-design in FPGA SoC',\n",
       "  'authors': \"['Biruk Seyoum', 'Marco Pagani', 'Alessandro Biondi', 'Giorgio Buttazzo']\",\n",
       "  'date': 'March 2021',\n",
       "  'source': \"SAC '21: Proceedings of the 36th Annual ACM Symposium on Applied Computing\",\n",
       "  'abstract': 'Despite its benefits, hardware acceleration under dynamic partial reconfiguration (DPR) has not been fully leveraged by many system designers, mostly due to the complexities of the DPR design flow and the lack of efficient design tools to automate the design process. Furthermore, making such a design approach suitable for real-time embedded systems requires the need for extending the standard DPR design flow with additional design steps, which have to accurately account for the timing behavior of the software and hardware components of the design, as well as of the components of the computing platform (e.g., the reconfiguration interface). To address this problem, this paper presents DART, a tool that fully automates the design flow in a real-time DPR-based system that comprises both software and hardware components. The tool targets the Zynq 7-series and Ultrascale+ FPGA-based SoCs by Xilinx. It aims at alleviating the manual effort required by state-of-the-art tools while not expecting high expertise in the design of programmable logic components under DPR. To this purpose, it fully automates the partitioning, floorplanning, and implementation (routing and bitstream generation) phases, generating a set of bitstreams starting from a set of tasks annotated with high-level timing requirements. The tool leverages mathematical optimization to solve the partitioning and floorplanning problems, and relies on a set of auto-generated scripts that interact with the vendor tools to mobilize the synthesis and implementation stages. DART has been experimentally evaluated with a case study application from an accelerated image processing system.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3412841.3441928',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A Spiking Neuromorphic Architecture Using Gated-RRAM for Associative Memory',\n",
       "  'authors': \"['Alexander Jones', 'Aaron Ruen', 'Rashmi Jha']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Journal on Emerging Technologies in Computing Systems',\n",
       "  'abstract': 'This work reports a spiking neuromorphic architecture for associative memory simulated in a SPICE environment using recently reported gated-RRAM (resistive random-access memory) devices as synapses alongside neurons based on complementary metal-oxide semiconductors (CMOSs). The network utilizes a Verilog A model to capture the behavior of the gated-RRAM devices within the architecture. The model uses parameters obtained from experimental gated-RRAM devices that were fabricated and tested in this work. Using these devices in tandem with CMOS neuron circuitry, our results indicate that the proposed architecture can learn an association in real time and retrieve the learned association when incomplete information is provided. These results show the promise for gated-RRAM devices for associative memory tasks within a spiking neuromorphic architecture framework.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3461667',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Multi-task Command and Control System based on Cloud Architecture',\n",
       "  'authors': \"['Guang Hu', 'Mingmei Zhang', 'Ming Ni', 'Wanzeng Cai', 'Junjie Wang']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"SSIP '21: Proceedings of the 2021 4th International Conference on Sensors, Signal and Image Processing\",\n",
       "  'abstract': 'This paper designs a multi-task command and control system based on cloud architecture. It can greatly facilitate the deployment of multiple command and control tasks. The cloud architecture in this system includes virtualized cloud, container cloud, and desktop cloud. The management nodes in the container cloud and the underlying virtual machines in the desktop cloud are deployed on the virtualized cloud. In order to make full use of and ensure the reliability of data, the data of virtualized cloud, container cloud, and desktop cloud are all stored on distributed storage systems. The system can greatly reduce the operational complexity of operation and maintenance personnel through introducing unified cloud management and comprehensive operation and maintenance.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3502814.3502821',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'An urban sensing architecture as essential infrastructure for future cities',\n",
       "  'authors': \"['Vijay Kumar', 'George Oikonomou', 'Theo Tryfonas']\",\n",
       "  'date': 'December 2021',\n",
       "  'source': \"UCC '21: Proceedings of the 14th IEEE/ACM International Conference on Utility and Cloud Computing Companion\",\n",
       "  'abstract': \"Climate change and migration have become one of the most challenging problems for our civilization. In this context, city councils work hard to manage essential services for citizens such as waste collection, street lamp lighting, and water supply. Increasingly, digitalization and the Internet of Things (IoT) help cities improve services, increase productivity and reduce costs. However, to understand how this may happen, we explore the urban sensing capabilities from citizen- to city-scale, how sensing at different levels is interlinked, and the challenges of managing innovations based on IoT data and devices. Local authorities collaborate with researchers and deploy testbeds as a part of demonstration and research projects to perform the above data collection, improve city services, and support innovation. The data gathered is about indoor and outdoor environmental conditions, energy usage, built environment, structural health monitoring. Such monitoring requires IT infrastructure at three different tiers: at the endpoint, edge, and cloud. Managing infrastructure at all tiers with provisioning, connectivity, security updates of devices, user data privacy controls, visualization of data, multi-tenancy of applications, and network resilience, is challenging. So, in turn, we focus on performing a systematic study of the technical and non-technical challenges faced during the implementation, management, and deployment of devices into citizens' homes and public spaces. Our third piece of work explores IoT edge applications' resiliency and reliability requirements that vary from non-critical (best delivery efforts) to safety-critical with time-bounded guarantees. We investigate how to meet IoT application mixed-criticality QoS requirements in multi-communication networks. Finally, to demonstrate the principles of our framework in the real world, we implement an open-source air quality platform Open City Air Quality Platform (OpenCAQP), that merges a wide range of data sources and air pollution parameters into a single platform. The OpenCAQP allows citizens, environmentalists, data analysts, and developers to access and visualize that data.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3492323.3503507',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Leveraging Targeted Value Prediction to Unlock New Hardware Strength Reduction Potential',\n",
       "  'authors': \"['Arthur Perais']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"MICRO '21: MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture\",\n",
       "  'abstract': 'Value Prediction (VP) is a microarchitectural technique that speculatively breaks data dependencies to increase the available Instruction Level Parallelism (ILP) in general purpose processors. Despite recent proposals, VP remains expensive and has intricate interactions with several stages of the classical superscalar pipeline. In this paper, we revisit and simplify VP by leveraging the irregular distribution of the values produced during the execution of common programs.  First, we demonstrate that a reasonable fraction of the performance uplift brought by a full VP infrastructure can be obtained by predicting only a few ”usual suspects” values. Furthermore, we show that doing so allows to greatly simplify VP operation as well as reduce the value predictor footprint. Lastly, we show that these Minimal and Targeted VP infrastructures conceptually enable Speculative Strength Reduction (SpSR), a rename-time optimization whereby instructions can disappear at rename in the presence of specific operand values.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3466752.3480050',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Hardware acceleration of tensor-structured multilevel ewald summation method on MDGRAPE-4A, a special-purpose computer system for molecular dynamics simulations',\n",
       "  'authors': \"['Gentaro Morimoto', 'Yohei M. Koyama', 'Hao Zhang', 'Teruhisa S. Komatsu', 'Yousuke Ohno', 'Keigo Nishida', 'Itta Ohmura', 'Hiroshi Koyama', 'Makoto Taiji']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"SC '21: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis\",\n",
       "  'abstract': 'We developed MDGRAPE-4A, a special-purpose computer system for molecular dynamics simulations, consisting of 512 nodes of custom system-on-a-chip LSIs with dedicated processor cores and interconnects designed to achieve strong scalability for biomolecular simulations. To reduce the global communications required for the evaluation of Coulomb interactions, we conducted a co-design of the MDGRAPE-4A and the novel algorithm, tensor-structured multilevel Ewald summation method (TME), which produced hardware modules on the custom LSI circuit for particle-grid operations and for grid-grid separable convolutions on a 3D torus network. We implemented the convolution for the top-level grid potentials by using 3D FFTs on an FPGA, along with an FPGA-based octree network to gather grid charges. The elapsed time for the long-range part of Coulomb is 50 μs, which can mostly overlap with those for the short-range part, and the additional cost is approximately 10 μs/step, which is only a 5% performance loss.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3458817.3476190',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'GPU Domain Specialization via Composable On-Package Architecture',\n",
       "  'authors': \"['Yaosheng Fu', 'Evgeny Bolotin', 'Niladrish Chatterjee', 'David Nellans', 'Stephen W. Keckler']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Architecture and Code Optimization',\n",
       "  'abstract': 'As GPUs scale their low-precision matrix math throughput to boost deep learning (DL) performance, they upset the balance between math throughput and memory system capabilities. We demonstrate that a converged GPU design trying to address diverging architectural requirements between FP32 (or larger)-based HPC and FP16 (or smaller)-based DL workloads results in sub-optimal configurations for either of the application domains. We argue that a Composable On-PAckage GPU (COPA-GPU) architecture to provide domain-specialized GPU products is the most practical solution to these diverging requirements. A COPA-GPU leverages multi-chip-module disaggregation to support maximal design reuse, along with memory system specialization per application domain. We show how a COPA-GPU enables DL-specialized products by modular augmentation of the baseline GPU architecture with up to 4× higher off-die bandwidth, 32× larger on-package cache, and 2.3× higher DRAM bandwidth and capacity, while conveniently supporting scaled-down HPC-oriented designs. This work explores the microarchitectural design necessary to enable composable GPUs and evaluates the benefits composability can provide to HPC, DL training, and DL inference. We show that when compared to a converged GPU design, a DL-optimized COPA-GPU featuring a combination of 16× larger cache capacity and 1.6× higher DRAM bandwidth scales per-GPU training and inference performance by 31% and 35%, respectively, and reduces the number of GPU instances by 50% in scale-out training scenarios.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3484505',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Study of the Utility of Text Classification Based Software Architecture Recovery Method RELAX for Maintenance',\n",
       "  'authors': \"['Daniel Link', 'Kamonphop Srisopha', 'Barry Boehm']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"ESEM '21: Proceedings of the 15th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)\",\n",
       "  'abstract': \"Background. The software architecture recovery method RELAX produces a concern-based architectural view of a software system graphically and textually from that system's source code. The method has been implemented in software which can recover the architecture of systems whose source code is written in Java. Aims. Our aim was to find out whether the availability of architectural views produced by RELAX can help maintainers who are new to a project in becoming productive with development tasks sooner, and how they felt about working in such an environment. Method. We conducted a user study with nine participants. They were subjected to a controlled experiment in which maintenance success and speed with and without access to RELAX recovery results were compared to each other. Results. We have observed that employing architecture views produced by RELAX helped participants reduce time to get started on maintenance tasks by a factor of 5.38 or more. While most participants were unable to finish their tasks within the allotted time when they did not have recovery results available, all of them finished them successfully when they did. Additionally, participants reported that these views were easy to understand, helped them to learn the system's structure and enabled them to compare different versions of the system. Conclusions. Through the speedup to the start of maintenance experienced by the participants as well as in their formed opinions, RELAX has shown itself to be a valuable help that could provide the basis of further tools that specifically support the development process with a focus on maintenance.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3475716.3484194',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Research on Network Architecture Design Based on Artificial Intelligence Application Technology',\n",
       "  'authors': \"['Jing Guo']\",\n",
       "  'date': 'January 2022',\n",
       "  'source': \"icWCSN '22: Proceedings of the 2022 9th International Conference on Wireless Communication and Sensor Networks\",\n",
       "  'abstract': 'Abstract: With the continuous development of AI technology, the training of massive data and the emergence of large-scale models have made stand-alone model training increasingly unable to meet the needs of AI applications. Distributed machine learning technologies (such as data parallelism and model parallelism) have appeared at historic moments and will have extreme large-scale application scenarios. At present, the training speed of distributed machine learning models is slow, and the scale of model parameters is still the main problem in this field. From the perspective of model parallelism, this article aims to design the optimal division method for different models under model parallelism by analyzing the structure of the existing AI application model. According to the framework structure of artificial intelligence application model, design the model optimization partition strategy and model based on parallelism. A network architecture suitable for accelerating AI application training, focusing on solving technical problems, such as network architecture design based on AI applications and model optimization and partitioning under model parallelization.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3514105.3514124',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Neural Cascade Architecture With Triple-Domain Loss for Speech Enhancement',\n",
       "  'authors': \"['Heming Wang', 'DeLiang Wang']\",\n",
       "  'date': 'None',\n",
       "  'source': 'IEEE/ACM Transactions on Audio, Speech and Language Processing',\n",
       "  'abstract': 'This paper proposes a neural cascade architecture to address the monaural speech enhancement problem. The cascade architecture is composed of three modules which optimize in turn enhanced speech with respect to the magnitude spectrogram, the time-domain signal and the complex spectrogram. Each module takes as input the noisy speech and the output obtained from the previous module, and generates a prediction of the respective target. Our model is trained in an end-to-end manner, using a triple-domain loss function that accounts for three domains of signal representation. Experimental results on the WSJ0 SI-84 corpus show that the proposed model outperforms other strong speech enhancement baselines in terms of objective speech quality and intelligibility.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/TASLP.2021.3138716',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Timing-Driven X-architecture Steiner Minimum Tree Construction Based on Social Learning Multi-Objective Particle Swarm Optimization',\n",
       "  'authors': \"['Xiaohua Chen', 'Ruping Zhou', 'Genggeng Liu', 'Zhen Chen', 'Wenzhong Guo']\",\n",
       "  'date': 'April 2021',\n",
       "  'source': \"WWW '21: Companion Proceedings of the Web Conference 2021\",\n",
       "  'abstract': 'The construction of timing-driven Steiner minimum tree is a critical issue in VLSI routing design. Meanwhile, since the interconnection model of X-architecture can make full use of routing resources compared to the traditional Manhattan architecture, constructing a Timing-Driven X-architecture Steiner Minimum Tree (TDXSMT) is of great significance to improving routing performance. In this paper, an efficient algorithm based on Social Learning Multi-Objective Particle Swarm Optimization (SLMOPSO) is proposed to construct a TDXSMT with minimizing the maximum source-to-sink pathlength. An X-architecture Prim-Dijkstra model is presented to construct an initial Steiner tree which can optimize both the wirelength and the maximum source-to-sink pathlength. In order to find a better solution, an SLMOPSO method based on the nearest and best select strategy is presented to improve the global exploration capability of the algorithm. Besides, the mutation and crossover operators are utilized to achieve the discrete particle update process, thereby better solving the discrete TDXSMT problem. The experimental results indicate that the proposed algorithm has an excellent trade-off between the wirelength and maximum source-to-sink pathlength of the routing tree and can greatly optimize the timing delay.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3442442.3451143',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Break dancing: low overhead, architecture neutral software branch tracing',\n",
       "  'authors': \"['Gabriel Marin', 'Alexey Alexandrov', 'Tipp Moseley']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': 'LCTES 2021: Proceedings of the 22nd ACM SIGPLAN/SIGBED International Conference on Languages, Compilers, and Tools for Embedded Systems',\n",
       "  'abstract': 'Sampling-based Feedback Directed Optimization (FDO) methods like AutoFDO and BOLT that employ profiles collected in live production environments, are commonly used in datacenter applications to attain significant performance benefits without the toil of maintaining representative load tests. Sampled profiles rely on hardware facilities like Intel’s Last Branch Record (LBR) which are not currently available even on popular CPUs from ARM or AMD. Since not all architectures include a hardware LBR feature, we present an architecture neutral approach to collect LBR-like data. We use sampling and limited program tracing to capture LBR-like data from optimized and unmodified applications binaries. Since the implementation is in user space, we can collect arbitrarily long LBR buffers, and by varying the sampling rate, we can adjust the runtime overhead to arbitrarily low values. We target runtime overheads of <2% when the profiler is on and zero when it’s off. This amortizes to negligible fleet-wide collection cost given the size of a modern production fleet. We implemented a profiler that uses this method of software branch tracing. We also analyzed its overhead and the similarity of the data it collects to the Intel LBR hardware using the SPEC2006 benchmarks. Results demonstrate profile quality and optimization efficacy at parity with LBR-based AutoFDO and the target profiling overhead being achievable even without implementing any advanced tuning.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3461648.3463853',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Research on the information System architecture design framework and reference resources of American Army',\n",
       "  'authors': \"['Ping Jian']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"AISS '21: Proceedings of the 3rd International Conference on Advanced Information Science and System\",\n",
       "  'abstract': 'The concept of architecture is put forward by the US Army, which refers to the composition structure of the system and its mutual relationship, as well as the principles and guidelines guiding the design and development of the system. Architecture design technology and its reference resources have important theoretical and technical support for the top-level design of information system. Based on the analysis of architecture technology and function, this paper systematically studies the connotation and development history of the architecture design framework of the US army, and sorts out the main reference resources of the architecture design of the US army, such as the common joint task list, information system interoperability level model, joint technology architecture. It provides useful reference for the theoretical and technical research of information system top-level design.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503047.3503094',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Neural Cascade Architecture for Multi-Channel Acoustic Echo Suppression',\n",
       "  'authors': \"['Hao Zhang', 'DeLiang Wang']\",\n",
       "  'date': 'None',\n",
       "  'source': 'IEEE/ACM Transactions on Audio, Speech and Language Processing',\n",
       "  'abstract': 'Traditional acoustic echo cancellation (AEC) works by identifying an acoustic impulse response using adaptive algorithms. This paper proposes a neural cascade architecture for joint acoustic echo and noise suppression to address both single-channel and multi-channel AEC (MCAEC) problems. The proposed cascade architecture consists of two modules. A convolutional recurrent network (CRN) is employed in the first module for complex spectral mapping. Its output is fed as an additional input to the second module, where a long short-term memory network (LSTM) is utilized for magnitude mask estimation. The entire architecture is trained in an end-to-end manner with the two modules optimized jointly using a single loss function. The final output is generated using the enhanced phase and magnitude obtained from the first and the second module, respectively. The cascade architecture enables the proposed method to obtain robust magnitude estimation as well as phase enhancement. The proposed method is investigated under different AEC setups. We find that the deep learning based approach avoids the no-uniqueness problem in traditional MCAEC. For MCAEC setups with multiple microphones, combining deep MCAEC with supervised beamforming further improves the system performance. Evaluation results show that the proposed approach effectively suppresses acoustic echo and noise while preserving speech quality, and consistently outperforms related methods under different setups.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/TASLP.2022.3192104',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A Novelty-Centric Agent Architecture for Changing Worlds',\n",
       "  'authors': \"['Faizan Muhammad', 'Vasanth Sarathy', 'Gyan Tatiya', 'Shivam Goel', 'Saurav Gyawali', 'Mateo Guaman', 'Jivko Sinapov', 'Matthias Scheutz']\",\n",
       "  'date': 'May 2021',\n",
       "  'source': \"AAMAS '21: Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems\",\n",
       "  'abstract': 'Open-world AI requires artificial agents to cope with novelties that arise during task performance, i.e., they must (1) detect novelties, (2) characterize them, in order to (3) accommodate them, especially in cases where sudden changes to the environment make task accomplishment impossible without utilizing the novelty. We present a formal framework and implementation thereof in a cognitive agent for novelty handling and demonstrate the efficacy of the proposed methods for detecting and handling a large set of novelties in a crafting task in a simulated environment. We discuss the success of the proposed knowledge-based methods and propose heuristic extensions that will further improve novelty handling in open-worlds tasks.',\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3463952.3464062',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Pareto-optimal progressive neural architecture search',\n",
       "  'authors': \"['Eugenio Lomurno', 'Stefano Samele', 'Matteo Matteucci', 'Danilo Ardagna']\",\n",
       "  'date': 'July 2021',\n",
       "  'source': \"GECCO '21: Proceedings of the Genetic and Evolutionary Computation Conference Companion\",\n",
       "  'abstract': 'Neural Architecture Search (NAS) is the process of automating architecture engineering, searching for the best deep learning configuration. One of the main NAS approaches proposed in the literature, Progressive Neural Architecture Search (PNAS), seeks for the architectures with a sequential model-based optimization strategy: it defines a common recursive structure to generate the networks, whose number of building blocks rises through iterations. However, NAS algorithms are generally designed for an ideal setting without considering the needs and the technical constraints imposed by practical applications. In this paper, we propose a new architecture search named Pareto-Optimal Progressive Neural Architecture Search (POPNAS) that combines the benefits of PNAS to a time-accuracy Pareto optimization problem. POPNAS adds a new time predictor to the existing approach to carry out a joint prediction of time and accuracy for each candidate neural network, searching through the Pareto front. This allows us to reach a trade-off between accuracy and training time, identifying neural network architectures with competitive accuracy in the face of a drastically reduced training time.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3449726.3463146',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Rapid Detection Solution for Fabric Printing Based on Dual Architecture Design of FPGA and Neural Network',\n",
       "  'authors': \"['Feng Li', 'Qinggang Xi', 'Yiqing Feng']\",\n",
       "  'date': 'May 2021',\n",
       "  'source': 'ICFEICT 2021: International Conference on Frontiers of Electronics, Information and Computation Technologies',\n",
       "  'abstract': 'A rapid detection solution for fabric printing based on dual architecture design of FPGA and neural network is proposed to address the problems of low accuracy and slow speed of fabric printing positioning in modern textile fabric manufacturing companies. The detection solution combines the popular deep neural networks with FPGA(Field Programmable Logic Gate Arrays) by selecting a lightweight deep neural network that is compatible with fabric print localization and using an improved method in the network training process. Then we design a hardware architecture solution that is compatible with the network to accelerate the convolutional pooling and other operations of the network, and finally building a complete fabric printing detection solution. The experimental results show that the combination of the lightweight deep neural network and the designed hardware architecture solution can effectively localize the fabric prints while maximizing the hardware resources and meeting the basic real-time requirements. The designed scheme is less powerful compared to GPU and faster compared to CPU.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3474198.3478157',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Emotional AI in Healthcare: a pilot architecture proposal to merge emotion recognition tools',\n",
       "  'authors': \"['Samuel Marcos', 'Francisco José García Peñalvo', 'Andrea Vázquez Ingelmo']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"TEEM'21: Ninth International Conference on Technological Ecosystems for Enhancing Multiculturality (TEEM&apos;21)\",\n",
       "  'abstract': 'The use of emotional artificial intelligence (EAI) looks promising and is continuing to improve during the last years. However, in order to effectively use EAI to help in the diagnose and treat health conditions there are still significant challenges to be tackled. Because EAI is still under development, one of the most important challenges is to integrate the technology into the health provision process. In this sense, it is important to complement EAI technologies with expert supervision, and to provide health professionals with the necessary tools to make the best of EAI without a deep knowledge of the technology. The present work aims to provide an initial architecture proposal for making use of different available technologies for emotion recognition, where their combination could enhance emotion detection. The proposed architecture is based on an evolutionary approach so to be integrated in digital health ecosystems, so new modules can be easily integrated. In addition, internal data exchange utilizes Robot Operating System (ROS) syntax, so it can also be suitable for physical agents.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3486011.3486472',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A Real-Time Effective Fusion-Based Image Defogging Architecture on FPGA',\n",
       "  'authors': \"['Gaoming Du', 'Jiting Wu', 'Hongfang Cao', 'Kun Xing', 'Zhenmin Li', 'Duoli Zhang', 'Xiaolei Wang']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Multimedia Computing, Communications, and Applications',\n",
       "  'abstract': 'Foggy weather reduces the visibility of photographed objects, causing image distortion and decreasing overall image quality. Many approaches (e.g., image restoration, image enhancement, and fusion-based methods) have been proposed to work out the problem. However, most of these defogging algorithms are facing challenges such as algorithm complexity or real-time processing requirements. To simplify the defogging process, we propose a fusional defogging algorithm on the linear transmission of gray single-channel. This method combines gray single-channel linear transform with high-boost filtering according to different proportions. To enhance the visibility of the defogging image more effectively, we convert the RGB channel into a gray-scale single channel without decreasing the defogging results. After gray-scale fusion, the data in the gray-scale domain should be linearly transmitted. With the increasing real-time requirements for clear images, we also propose an efficient real-time FPGA defogging architecture. The architecture optimizes the data path of the guided filtering to speed up the defogging speed and save area and resources. Because the pixel reading order of mean and square value calculations are identical, the shift register in the box filter after the average and the computation of the square values is separated from the box filter and put on the input terminal for sharing, saving the storage area. What’s more, using LUTs instead of the multiplier can decrease the time delays of the square value calculation module and increase efficiency. Experimental results show that the linear transmission can save 66.7% of the total time. The architecture we proposed can defog efficiently and accurately, meeting the real-time defogging requirements on 1920 × 1080 image size.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3446241',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Minimally Invasive HW/SW Co-debug Live Visualization on Architecture Level',\n",
       "  'authors': \"['Pascal Pieper', 'Ralf Wimmer', 'Gerhard Angst', 'Rolf Drechsler']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"GLSVLSI '21: Proceedings of the 2021 on Great Lakes Symposium on VLSI\",\n",
       "  'abstract': 'We present a tool that allows developers to debug hard- and software and their interaction in an early design stage. We combine a SystemC virtual prototype (VP) with an easily configurable and interactive graphical user interface and a standard software debugger. The graphical user interface visualizes the internal state of the hardware. At the same time, the software debugger monitors and allows to manipulate the state of the software. This co-visualization supports design understanding and live debugging of the HW/SW interaction. We demonstrate its usefulness with a case-study where we debug an OLED display driver running on a RISC-V VP.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3453688.3461524',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'SAGE: A Split-Architecture Methodology for Efficient End-to-End Autonomous Vehicle Control',\n",
       "  'authors': \"['Arnav Malawade', 'Mohanad Odema', 'Sebastien Lajeunesse-degroot', 'Mohammad Abdullah Al Faruque']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Embedded Computing Systems',\n",
       "  'abstract': 'Autonomous vehicles (AV) are expected to revolutionize transportation and improve road safety significantly. However, these benefits do not come without cost; AVs require large Deep-Learning (DL) models and powerful hardware platforms to operate reliably in real-time, requiring between several hundred watts to one kilowatt of power. This power consumption can dramatically reduce vehicles’ driving range and affect emissions. To address this problem, we propose SAGE: a methodology for selectively offloading the key energy-consuming modules of DL architectures to the cloud to optimize edge, energy usage while meeting real-time latency constraints. Furthermore, we leverage Head Network Distillation (HND) to introduce efficient bottlenecks within the DL architecture in order to minimize the network overhead costs of offloading with almost no degradation in the model’s performance. We evaluate SAGE using an Nvidia Jetson TX2 and an industry-standard Nvidia Drive PX2 as the AV edge, devices and demonstrate that our offloading strategy is practical for a wide range of DL models and internet connection bandwidths on 3G, 4G LTE, and WiFi technologies. Compared to edge-only computation, SAGE reduces energy consumption by an average of 36.13%, 47.07%, and 55.66% for an AV with one low-resolution camera, one high-resolution camera, and three high-resolution cameras, respectively. SAGE also reduces upload data size by up to 98.40% compared to direct camera offloading.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3477006',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'ELSA: hardware-software co-design for efficient, lightweight self-attention mechanism in neural networks',\n",
       "  'authors': \"['Tae Jun Ham', 'Yejin Lee', 'Seong Hoon Seo', 'Soosung Kim', 'Hyunji Choi', 'Sung Jun Jung', 'Jae W. Lee']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': 'The self-attention mechanism is rapidly emerging as one of the most important key primitives in neural networks (NNs) for its ability to identify the relations within input entities. The self-attention-oriented NN models such as Google Transformer and its variants have established the state-of-the-art on a very wide range of natural language processing tasks, and many other self-attention-oriented models are achieving competitive results in computer vision and recommender systems as well. Unfortunately, despite its great benefits, the self-attention mechanism is an expensive operation whose cost increases quadratically with the number of input entities that it processes, and thus accounts for a significant portion of the inference runtime. Thus, this paper presents ELSA (Efficient, Lightweight Self-Attention), a hardware-software co-designed solution to substantially reduce the runtime as well as energy spent on the self-attention mechanism. Specifically, based on the intuition that not all relations are equal, we devise a novel approximation scheme that significantly reduces the amount of computation by efficiently filtering out relations that are unlikely to affect the final output. With the specialized hardware for this approximate self-attention mechanism, ELSA achieves a geomean speedup of 58.1X as well as over three orders of magnitude improvements in energy efficiency compared to GPU on self-attention computation in modern NN models while maintaining less than 1% loss in the accuracy metric.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00060',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'High-Performance and Energy-Efficient 3D Manycore GPU Architecture for Accelerating Graph Analytics',\n",
       "  'authors': \"['Dwaipayan Choudhury', 'Aravind Sukumaran Rajam', 'Ananth Kalyanaraman', 'Partha Pratim Pande']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Journal on Emerging Technologies in Computing Systems',\n",
       "  'abstract': \"Recent advances in GPU-based manycore accelerators provide the opportunity to efficiently process large-scale graphs on chip. However, real world graphs have a diverse range of topology and connectivity patterns (e.g., degree distributions) that make the design of input-agnostic hardware architectures a challenge. Network-on-Chip (NoC)-based architectures provide a way to overcome this challenge as the architectural topology can be used to approximately model the expected traffic patterns that emerge from graph application workloads. In this paper, we first study the mix of long- and short-range traffic patterns generated on-chip using graph workloads, and subsequently use the findings to adapt the design of an optimal NoC-based architecture. In particular, by leveraging emerging three-dimensional (3D) integration technology, we propose design of a small-world NoC (SWNoC)-enabled manycore GPU architecture, where the placement of the links connecting the streaming multiprocessors (SM) and the memory controllers (MC) follow a power-law distribution. The proposed 3D manycore GPU architecture outperforms the traditional planar (2D) counterparts in both performance and energy consumption. Moreover, by adopting a joint performance-thermal optimization strategy, we address the thermal concerns in a 3D design without noticeably compromising the achievable performance. The 3D integration technology is also leveraged to incorporate Near Data Processing (NDP) to complement the performance benefits introduced by the SWNoC architecture. As graph applications are inherently memory intensive, off-chip data movement gives rise to latency and energy overheads in the presence of external DRAM. In conventional GPU architectures, as the main memory layer is not integrated with the logic, off-chip data movement negatively impacts overall performance and energy consumption. We demonstrate that NDP significantly reduces the overheads associated with such frequent and irregular memory accesses in graph-based applications. The proposed SWNoC-enabled NDP framework that integrates 3D memory (like Micron's HMC) with a massive number of GPU cores achieves 29.5% performance improvement and 30.03% less energy consumption on average compared to a conventional planar Mesh-based design with external DRAM.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3482880',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A Digital Twin Architecture for Wireless Networked Adaptive Active Noise Control',\n",
       "  'authors': \"['Chuang Shi', 'Feiyu Du', 'Qianyang Wu']\",\n",
       "  'date': 'None',\n",
       "  'source': 'IEEE/ACM Transactions on Audio, Speech and Language Processing',\n",
       "  'abstract': 'The active noise control (ANC) is a complementary technique to the passive noise control (PNC) to reduce the low frequency noise. The ANC controller can be implemented by pre-trained filters or adaptive filters. The adaptive ANC controller is advantageous in its adaptation to environmental changes. However, the algorithm complexity of the adaptive ANC controller increases with the scale of ANC applications, making it difficult to be carried out on low-cost processors. To resolve this problem, cloud computing should be utilized in ANC systems, and thus the wireless networked ANC system is proposed. Since it is crucial for ANC controllers to generate the anti-noise wave in real time, this paper formulates a digital twin architecture that implements the control filter adaptation in the cloud and the anti-noise signal generation on the local controller, respectively. A digital twin filtered-reference least mean squares (DT-FxLMS) algorithm is proposed to coordinate the digital twin with the local controller. Simulation and experiment results demonstrate the effectiveness and efficiency of the wireless networked ANC system based on the digital twin architecture.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/TASLP.2022.3199992',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'EfficientTDNN: Efficient Architecture Search for Speaker Recognition',\n",
       "  'authors': \"['Rui Wang', 'Zhihua Wei', 'Haoran Duan', 'Shouling Ji', 'Yang Long', 'Zhen Hong']\",\n",
       "  'date': 'None',\n",
       "  'source': 'IEEE/ACM Transactions on Audio, Speech and Language Processing',\n",
       "  'abstract': 'Convolutional neural networks (CNNs), such as the time-delay neural network (TDNN), have shown their remarkable capability in learning speaker embedding. However, they meanwhile bring a huge computational cost in storage size, processing, and memory. Discovering the specialized CNN that meets a specific constraint requires a substantial effort of human experts. Compared with hand-designed approaches, neural architecture search (NAS) appears as a practical technique in automating the manual architecture design process and has attracted increasing interest in spoken language processing tasks such as speaker recognition. In this paper, we propose EfficientTDNN, an efficient architecture search framework consisting of a TDNN-based supernet and a TDNN-NAS algorithm. The proposed supernet introduces temporal convolution of different ranges of the receptive field and feature aggregation of various resolutions from different layers to TDNN. On top of it, the TDNN-NAS algorithm quickly searches for the desired TDNN architecture via weight-sharing subnets, which surprisingly reduces computation while handling the vast number of devices with various resources requirements. Experimental results on the VoxCeleb dataset show the proposed EfficientTDNN enables approximate <inline-formula><tex-math notation=\"LaTeX\">$10^{13}$</tex-math></inline-formula> architectures concerning depth, kernel, and width. Considering different computation constraints, it achieves a 2.20&#x0025; equal error rate (EER) with 204 M multiply-accumulate operations (MACs), 1.41&#x0025; EER with 571 M MACs as well as 0.94&#x0025; EER with 1.45 G MACs. Comprehensive investigations suggest that the trained supernet generalizes subnets not sampled during training and obtains a favorable trade-off between accuracy and efficiency.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/TASLP.2022.3182856',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Neural Architecture Search for LF-MMI Trained Time Delay Neural Networks',\n",
       "  'authors': \"['Shoukang Hu', 'Xurong Xie', 'Mingyu Cui', 'Jiajun Deng', 'Shansong Liu', 'Jianwei Yu', 'Mengzhe Geng', 'Xunying Liu', 'Helen Meng']\",\n",
       "  'date': 'None',\n",
       "  'source': 'IEEE/ACM Transactions on Audio, Speech and Language Processing',\n",
       "  'abstract': 'State-of-the-art automatic speech recognition (ASR) system development is data and computation intensive. The optimal design of deep neural networks (DNNs) for these systems often require expert knowledge and empirical evaluation. In this paper, a range of neural architecture search (NAS) techniques are used to automatically learn two types of hyper-parameters of factored time delay neural networks (TDNN-Fs): i) the left and right splicing context offsets; and ii) the dimensionality of the bottleneck linear projection at each hidden layer. These techniques include the differentiable neural architecture search (DARTS) method integrating architecture learning with lattice-free MMI training; Gumbel-Softmax and pipelined DARTS methods reducing the confusion over candidate architectures and improving the generalization of architecture selection; and Penalized DARTS incorporating resource constraints to balance the trade-off between performance and system complexity. Parameter sharing among TDNN-F architectures allows an efficient search over up to <inline-formula><tex-math notation=\"LaTeX\">$7^{28}$</tex-math></inline-formula> different systems. Statistically significant word error rate (WER) reductions of up to 1.2&#x0025; absolute and relative model size reduction of 31&#x0025; were obtained over a state-of-the-art 300-hour Switchboard corpus trained baseline LF-MMI TDNN-F system featuring speed perturbation, i-Vector and learning hidden unit contribution (LHUC) based speaker adaptation as well as RNNLM rescoring. Performance contrasts on the same task against recent end-to-end systems reported in the literature suggest the best NAS auto-configured system achieves state-of-the-art WERs of 9.9&#x0025; and 11.1&#x0025; on the NIST Hub5&#x2019; 00 and Rt03 s test sets respectively with up to 96&#x0025; model size reduction. Further analysis using Bayesian learning shows that the proposed NAS approaches can effectively minimize the structural redundancy in the TDNN-F systems and reduce their model parameter uncertainty. Consistent performance improvements were also obtained on a UASpeech dysarthric speech recognition task.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/TASLP.2022.3153253',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Trojan Awakener: Detecting Dormant Malicious Hardware Using Laser Logic State Imaging',\n",
       "  'authors': \"['Thilo Krachenfels', 'Jean-Pierre Seifert', 'Shahin Tajik']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"ASHES '21: Proceedings of the 5th Workshop on Attacks and Solutions in Hardware Security\",\n",
       "  'abstract': 'The threat of hardware Trojans (HTs) and their detection is a widely studied field. While the effort for inserting a Trojan into an application-specific integrated circuit (ASIC) can be considered relatively high, especially when trusting the chip manufacturer, programmable hardware is vulnerable to Trojan insertion even after the product has been shipped or during usage. At the same time, detecting dormant HTs with small or zero-overhead triggers and payloads on these platforms is still a challenging task, as the Trojan might not get activated during the chip verification using logical testing or physical measurements. In this work, we present a novel Trojan detection approach based on a technique known from integrated circuit (IC) failure analysis, capable of detecting virtually all classes of dormant Trojans. Using laser logic state imaging (LLSI), we show how supply voltage modulations can awaken inactive Trojans, making them detectable using laser voltage imaging techniques. Therefore, our technique does not require triggering the Trojan. To support our claims, we present two case studies on 28 nm SRAM- and flash-based field-programmable gate arrays (FPGAs). We demonstrate how to detect with high confidence small changes in sequential and combinatorial logic as well as in the routing configuration of FPGAs in a non-invasive manner. Finally, we discuss the practical applicability of our approach on dormant analog Trojans in ASICs.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3474376.3487282',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Power delivery and thermal-aware arm-based multi-tier 3D architecture',\n",
       "  'authors': \"['Lingjun Zhu', 'Tuan Ta', 'Rossana Liu', 'Rahul Mathur', 'Xiaoqing Xu', 'Shidhartha Das', 'Ankit Kaul', 'Alejandro Rico', 'Doug Joseph', 'Brian Cline', 'Sung Kyu Lim']\",\n",
       "  'date': 'July 2021',\n",
       "  'source': \"ISLPED '21: Proceedings of the ACM/IEEE International Symposium on Low Power Electronics and Design\",\n",
       "  'abstract': '3D integration is becoming a cost-effective way to incorporate more CPU cores and memory to improve the performance of computing systems. Meanwhile, due to the higher power density, power delivery and thermal issues become more significant in multi-tier 3D ICs. In this paper, we explore and evaluate multiple design options for an Arm Neoverse-based 3D architecture focusing on power and thermals at 7nm process and sub-10μm pitch. Using a rapid voltage-drop and thermal analysis methodology, we model a system with a 32-core CPU layer and up to 4 layers of system-level caches, and quantify the trade-offs between performance, cost, voltage-drop, and temperature. A 3-layer configuration shows a good balance with 17% IPC gain and 17% lower cost, while incurring 15mV worse voltage drop and 8.5°C higher temperature compared with 2D. Our studies suggest that the co-optimization of system architecture, technology, and physical design is key for high-performance 3D systems.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISLPED52811.2021.9502481',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'SISA: Set-Centric Instruction Set Architecture for Graph Mining on Processing-in-Memory Systems',\n",
       "  'authors': \"['Maciej Besta', 'Raghavendra Kanakagiri', 'Grzegorz Kwasniewski', 'Rachata Ausavarungnirun', 'Jakub Beránek', 'Konstantinos Kanellopoulos', 'Kacper Janda', 'Zur Vonarburg-Shmaria', 'Lukas Gianinazzi', 'Ioana Stefan', 'Juan Gómez Luna', 'Jakub Golinowski', 'Marcin Copik', 'Lukas Kapp-Schwoerer', 'Salvatore Di Girolamo', 'Nils Blach', 'Marek Konieczny', 'Onur Mutlu', 'Torsten Hoefler']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"MICRO '21: MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture\",\n",
       "  'abstract': 'Simple graph algorithms such as PageRank have been the target of numerous hardware accelerators. Yet, there also exist much more complex graph mining algorithms for problems such as clustering or maximal clique listing. These algorithms are memory-bound and thus could be accelerated by hardware techniques such as Processing-in-Memory (PIM). However, they also come with non-straightforward parallelism and complicated memory access patterns. In this work, we address this problem with a simple yet surprisingly powerful observation: operations on sets of vertices, such as intersection or union, form a large part of many complex graph mining algorithms, and can offer rich and simple parallelism at multiple levels. This observation drives our cross-layer design, in which we (1) expose set operations using a novel programming paradigm, (2) express and execute these operations efficiently with carefully designed set-centric ISA extensions called SISA, and (3) use PIM to accelerate SISA instructions. The key design idea is to alleviate the bandwidth needs of SISA instructions by mapping set operations to two types of PIM: in-DRAM bulk bitwise computing for bitvectors representing high-degree vertices, and near-memory logic layers for integer arrays representing low-degree vertices. Set-centric SISA-enhanced algorithms are efficient and outperform hand-tuned baselines, offering more than 10 × speedup over the established Bron-Kerbosch algorithm for listing maximal cliques. We deliver more than 10 SISA set-centric algorithm formulations, illustrating SISA’s wide applicability.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3466752.3480133',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'ETEH: Unified Attention-Based End-to-End ASR and KWS Architecture',\n",
       "  'authors': \"['Gaofeng Cheng', 'Haoran Miao', 'Runyan Yang', 'Keqi Deng', 'Yonghong Yan']\",\n",
       "  'date': 'None',\n",
       "  'source': 'IEEE/ACM Transactions on Audio, Speech and Language Processing',\n",
       "  'abstract': 'Even though attention-based end-to-end (E2E) automatic speech recognition (ASR) models have been yielding state-of-the-art recognition accuracy, they still fall behind many of the ASR models deployed in the industry in some crucial functionalities such as online processing and precise timestamps generating. This weakness prevents attention-based E2E ASR models from being applied in several essential speech tasks, such as online speech recognition and keyword searching (KWS). In this paper, we describe our proposed unified attention-based E2E ASR and KWS architecture&#x2013;ETEH, which supports, in one model, both online and offline ASR decoding modes, thus allowing for precise and reliable KWS. &#x201C;ETE&#x201D; stands for attention-based E2E modeling, whereas &#x201C;H&#x201D; represents the hybrid gaussian mixture model and hidden Markov model (GMM-HMM). As a combination of both, ETEH is an attention-based E2E ASR architecture which utilizes the frame-wise time alignment (FTA) generated by GMM-HMM ASR models. This FTA is used to better the model in two ways: first, it helps the monotonic attentions of ETEH models to capture more accurate word time stamps, thus resulting in lower latency for online decoding; second, it helps ETEH models to provide accurate and reliable KWS results. Furthermore, we are able to combine both offline and online modes in one ETEH model and establish a concise system by adopt the universal training strategy. ETEH is functional and unique, and to the best of our knowledge, we can hardly find a comparable single attention-based E2E ASR system as the baseline. To evaluate ASR accuracy and latency for ETEH, we use our previously proposed monotonic truncated attention (MTA) based online CTC/attention (OCA) ASR models as baselines. Experimental results show that ETEH ASR models gain significant improvement in ASR latency compared to the baseline. To evaluate KWS performance, we compare ETEH models with CTC-based KWS models. Results demonstrate that our ETEH models achieve significantly better KWS performance compared to the CTC baselines.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/TASLP.2022.3161159',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'NASGuard: a novel accelerator architecture for robust neural architecture search (NAS) networks',\n",
       "  'authors': \"['Xingbin Wang', 'Boyan Zhao', 'Rui Hou', 'Amro Awad', 'Zhihong Tian', 'Dan Meng']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': \"Due to the wide deployment of deep learning applications in safety-critical systems, robust and secure execution of deep learning workloads is imperative. Adversarial examples, where the inputs are carefully designed to mislead the machine learning model is among the most challenging attacks to detect and defeat. The most dominant approach for defending against adversarial examples is to systematically create a network architecture that is sufficiently robust. Neural Architecture Search (NAS) has been heavily used as the de facto approach to design robust neural network models, by using the accuracy of detecting adversarial examples as a key metric of the neural network's robustness. While NAS has been proven effective in improving the robustness (and accuracy in general), the NAS-generated network models run noticeably slower on typical DNN accelerators than the hand-crafted networks, mainly because DNN accelerators are not optimized for robust NAS-generated models. In particular, the inherent multi-branch nature of NAS-generated networks causes unacceptable performance and energy overheads. To bridge the gap between the robustness and performance efficiency of deep learning applications, we need to rethink the design of AI accelerators to enable efficient execution of robust (auto-generated) neural networks. In this paper, we propose a novel hardware architecture, NASGuard, which enables efficient inference of robust NAS networks. NASGuard leverages a heuristic multi-branch mapping model to improve the efficiency of the underlying computing resources. Moreover, NASGuard addresses the load imbalance problem between the computation and memory-access tasks from multi-branch parallel computing. Finally, we propose a topology-aware performance prediction model for data prefetching, to fully exploit the temporal and spatial localities of robust NAS-generated architectures. We have implemented NASGuard with Verilog RTL. The evaluation results show that NASGuard achieves an average speedup of 1.74X over the baseline DNN accelerator.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00066',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Towards low-cost high-accuracy stochastic computing architecture for univariate functions: design and design space exploration',\n",
       "  'authors': \"['Kuncai Zhong', 'Zexi Li', 'Weikang Qian']\",\n",
       "  'date': 'March 2022',\n",
       "  'source': \"DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe\",\n",
       "  'abstract': 'Univariate functions are widely used. Several recent works propose to implement them by an unconventional computing paradigm, stochastic computing (SC). However, existing SC designs either have a high hardware cost due to the area-consuming randomizer or a low accuracy. In this work, we propose a low-cost high-accuracy SC architecture for univariate functions. It consists of only a single stochastic number generator and a minimum number of D flip-flops. We also apply three methods, random number source (RNS) negating, RNS scrambling, and input scrambling, to improve the accuracy of the architecture. To efficiently configure the architecture to achieve a high accuracy, we further propose a design space exploration algorithm. The experimental results show that compared to the conventional architecture, the area of the proposed architecture is reduced by up to 76%, while its accuracy is close to or sometimes even higher than that of the conventional architecture.',\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3539845.3539930',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Weaving a Faster Tor: A Multi-Threaded Relay Architecture for Improved Throughput',\n",
       "  'authors': \"['Steven Engler', 'Ian Goldberg']\",\n",
       "  'date': 'August 2021',\n",
       "  'source': \"ARES '21: Proceedings of the 16th International Conference on Availability, Reliability and Security\",\n",
       "  'abstract': 'The Tor anonymity network has millions of daily users and thousands of volunteer-run relays. Increasing the number of Tor users will enhance the privacy of not just new users, but also existing users by increasing their anonymity sets. However, growing the network further has several research and deployment challenges. One such challenge is supporting the increase in bandwidth required by additional users joining the network. While adding more Tor relays to the network would increase the total available bandwidth, it requires network architecture changes to reduce the impact of Tor’s growing directory documents. In order to increase the total available network bandwidth without needing to grow Tor’s directory documents, this work provides a multi-threaded relay architecture designed to improve the throughput of individual multi-core relays with available network capacity. We built an implementation of a subset of this new design on top of the standard Tor code base to demonstrate the potential throughput improvements of this architecture on both high- and low-performance hardware.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3465481.3465745',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Multi-Property Molecular Optimization using an Integrated Poly-Cycle Architecture',\n",
       "  'authors': \"['Guy Barshatski', 'Galia Nordon', 'Kira Radinsky']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"CIKM '21: Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management\",\n",
       "  'abstract': 'Molecular lead optimization is an important task of drug discovery focusing on generating molecules similar to a drug candidate but with enhanced properties. Most prior works focused on optimizing a single property. However, in real settings, we wish to find molecules that satisfy multiple constraints, e.g., potency and safety. Simultaneously optimizing these constraints was shown to be difficult, mostly due to the lack of training examples satisfying all constraints. In this work, we present a novel approach for multi-property optimization. Unlike prior approaches, that require a large training set of pairs of a lead molecule and an enhanced molecule, our approach is unpaired. Our architecture learns a transformation for each property optimization separately, while constraining the latent embedding space between all transformations. This allows generating a molecule which optimizes multiple properties simultaneously. We present a novel adaptive loss which balances the separate transformations and stabilizes the optimization process. We evaluate our method on optimizing for two properties: dopamine receptor (DRD2) and drug likeness (QED), and show our method outperforms previous state-of-the-art, especially when training examples satisfying all constraints are sparse.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3459637.3481938',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A Reference Architecture for Validating Security Across Multi-Cloud Computing Systems',\n",
       "  'authors': \"['Henry Edet']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"EASE '21: Proceedings of the 25th International Conference on Evaluation and Assessment in Software Engineering\",\n",
       "  'abstract': 'Correlative studies carried out by different experts have cited a number of impediments to the growth of cloud computing technology; at the top of the list is security and data privacy issues. More so, a systematic mapping study which was conducted at the start of this research, revealed that the most prevalent cloud security issues are a consequence of poor architecture. 73 percent of literature surveyed also revealed that frameworks and reference architectures are one of the most effective ways of preventing security and data privacy breaches within a cloud computing environment as these issues are addressed during the requirements phase, prior to deployment. [1], [14] This research seeks to explore a preventative approach to cloud security breaches through the use of reference architectures. Firstly, we investigate the main causes of cloud security breaches and then, we analyse existing reference architectures with the aim of designing a universal security framework usable across multi-cloud computing platforms.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3463274.3463345',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Ideological and political mobile learning system of computer professional courses based on J2EE architecture',\n",
       "  'authors': \"['Jing Luo', 'Mengyao Peng']\",\n",
       "  'date': 'September 2021',\n",
       "  'source': 'ICISCAE 2021: 2021 4th International Conference on Information Systems and Computer Aided Education',\n",
       "  'abstract': 'The core of the system is to feed back all kinds of search data according to the needs of users. However, the demand of Ideological and political learning obtained by the existing system is not clear enough, which makes the feedback logic of the system confused, which leads to the continuous decline of the stability of the system. In view of this problem, a mobile learning system of Ideological and political learning for computer specialty courses based on J2EE architecture is designed. In hardware design, the control chip is re selected to design carrier communication circuit. In software design, the demand for ideological and political learning is clarified, and the feedback logic of Ideological and political content of computer major course is established based on J2EE architecture. Through setting up data exchange function, all knowledge contents of Ideological and political course can be learned. The experimental results show that the hardware and software of the design system have high compatibility, and the feedback time and the number of crashes meet the design requirements. Thus, J2EE architecture can enhance the stability of mobile learning system.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3482632.3482665',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Architecture-Based Semantic Description Framework for Model Transformation',\n",
       "  'authors': \"['Jinkui Hou', 'Cong Xu', 'Yuyan Zhang']\",\n",
       "  'date': 'December 2021',\n",
       "  'source': \"NLPIR '21: Proceedings of the 2021 5th International Conference on Natural Language Processing and Information Retrieval\",\n",
       "  'abstract': 'In order to solve the problems of description and verification of semantic properties in model driven development, process algebra is introduced on the basis of extending typed category theory. A unified semantic description framework is established for the description and transformation of component-based software models, as well as the maintenance and verification of semantic properties in the process of model transformation. Category diagram is used to describe the semantics of architecture model, and typed morphism implies the dependency relationship between component objects, and typed functor is used to describe the mapping mechanism before and after model transformation. Application research shows that the framework well follows the essence and process requirements of model-driven development, and provides a new guidance framework for understanding, cognitive learning and promotion of software development research on the basis of model transformation.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3508230.3508241',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Acoustic Source Localization in the Circular Harmonic Domain Using Deep Learning Architecture',\n",
       "  'authors': \"['Kunkun SongGong', 'Wenwu Wang', 'Huawei Chen']\",\n",
       "  'date': 'None',\n",
       "  'source': 'IEEE/ACM Transactions on Audio, Speech and Language Processing',\n",
       "  'abstract': 'The problem of direction of arrival (DOA) estimation with a circular microphone array has been addressed with classical source localization methods, such as the model-based methods and the param etric methods. These methods have an advantage in estimating the DOAs in a blind manner, i.e. with no (or limited) prior knowledge about the sound sources. However, their performance tends to degrade rapidly in noisy and reverberant environments or in the presence of sensor array limitations, such as sensor gain and phase errors. In this paper, we present a new approach by leveraging the strength of a convolutional neural network (CNN)-based deep learning approach. In particular, we design new circular harmonic features that are frequency-invariant as inputs to the CNN architecture, so as to offer improvements in DOA estimation in unseen adverse environments and obtain good adaptation to array imperfections. To our knowledge, such a deep learning approach has not been used in the circular harmonic domain. Experiments performed on both simulated and real-data show that our method gives significantly better performance, than the recent baseline methods, in a variety of noise and reverberation levels, in terms of the accuracy of the DOA estimation.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/TASLP.2022.3190723',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Video Analytics Architecture with Metadata Event-Engine for Urban Safe Cities',\n",
       "  'authors': \"['David Eneko Ruiz de Gauna', 'Eider Irigoyen', 'Inaki Cejudo', 'Harbil Arregui', 'Peter Leskovsky', 'Oihana Otaegui']\",\n",
       "  'date': 'July 2021',\n",
       "  'source': \"ICCTA '21: Proceedings of the 2021 7th International Conference on Computer Technology Applications\",\n",
       "  'abstract': 'Intelligent video analysis from sources such as urban surveillance cameras is a prolific research area today. Multiple types of computer architectures offer a wide range of possibilities when addressing the needs of computer vision technologies. When it comes to real time processing for high level and complex event detections, however, some limitations may arise, such as the computing power in the edge or the cost of sending real time video to the cloud for running advanced algorithms. In this paper, we present a functional architecture of a complete video surveillance solution and we focus on the metadata-processing event engine which takes care of the high-level video processing that is decoupled from a low-level video analysis. The low-level video analysis running in the edge generates and publishes a flow of JSON messages structure containing the details of bounding boxes detected in each frame into an asynchronous messaging service. The metadata event engine is running in a remote cloud, far from the camera locations. We present the performance evaluation of this event engine under different circumstances simulating data coming simultaneously from multiple cameras, in order to study the best strategies when deploying and partitioning distributed processing tasks.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3477911.3477919',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Isadora: Automated Information Flow Property Generation for Hardware Designs',\n",
       "  'authors': \"['Calvin Deutschbein', 'Andres Meza', 'Francesco Restuccia', 'Ryan Kastner', 'Cynthia Sturton']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"ASHES '21: Proceedings of the 5th Workshop on Attacks and Solutions in Hardware Security\",\n",
       "  'abstract': 'Isadora is a methodology for creating information flow specifications of hardware designs. The methodology combines information flow tracking and specification mining to produce a set of information flow properties that are suitable for use during the security validation process, and which support a better understanding of the security posture of the design. Isadora is fully automated; the user provides only the design under consideration and a testbench and need not supply a threat model nor security specifications. We evaluate Isadora on a RISC-V processor plus two designs related to SoC access control. Isadora generates security properties that align with those suggested by the Common Weakness Enumerations (CWEs), and in the case of the SoC designs, align with the properties written manually by security experts.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3474376.3487286',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Automatic mapping and code optimization for OpenCL kernels on FT-matrix architecture (WIP paper)',\n",
       "  'authors': \"['Xiaolei Zhao', 'Mei Wen', 'Zhaoyun Chen', 'Yang Shi', 'Chunyuan Zhang']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': 'LCTES 2021: Proceedings of the 22nd ACM SIGPLAN/SIGBED International Conference on Languages, Compilers, and Tools for Embedded Systems',\n",
       "  'abstract': 'FT-Matrix is a typical vector-SIMD architecture that refines the cooperation between scalar and vector units. This approach is widely used in digital signal processing, high-performance computing, and artificial intelligence, among other fields. FT-Matrix currently adopts C vector extension as the main programming model, improving the utilization efficiency of SIMD by providing explicit vector extension API. Moreover, it is difficult to efficiently transplant parallel programs (OpenCL, CUDA) adopted by users. This paper proposes an automatic mapping and code optimization method for OpenCL kernels on FT-Matrix architecture. The proposed approach solves these challenges by means of work item coalescing, slicing and rotation, and instruction-level code optimization. Preliminary results show that our method can achieve high performance and good hardware utilization for OpenCL kernels, as well as decreasing the programming difficulty on FT-Matrix.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3461648.3463845',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'HyperParser: A High-Performance Parser Architecture for Next Generation Programmable Switch and SmartNIC',\n",
       "  'authors': \"['Huan Liu', 'Zhiliang Qiu', 'Weitao Pan', 'Jiajun Li', 'Jinjian Huang']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"APNet '21: Proceedings of the 5th Asia-Pacific Workshop on Networking\",\n",
       "  'abstract': 'Programmable switches and SmartNICs motivate the programmable network. ASIC is adopted in programmable switches to achieve high throughput, and FPGA-based SmartNIC is becoming increasingly popular. The programmable parser is a key element in programmable switches and SmartNICs, which can identify the protocol types and extract the relevant fields. The programmable parser for the next generation programmable switches and SmartNICs requires a significant improvement in PPAL (performance, power, area, and latency), which is quite challenging. According to the Ethernet roadmap, 800 Gbps and 1.6 Tbps are expected to be the future switch interface speeds after 2022, which leads to higher throughput of the parser. Meanwhile, the end of Dennard scaling and the slowdown of Moore’s Law result in limited power and area. Besides, the need for low-latency and low-jitter operations at the datacenter scale continues to grow.  Aforementioned requirements on PPAL inspire us to propose HyperParser, a high-performance parser architecture for next generation programmable switches and FPGA-based SmartNICs. The key innovation of HyperParser is the adoption of the butterfly network, which is widely used in cryptographic circuits. HyperParser supports ASIC and FPGA implementations, with low and deterministic latency. The PPAL of the ASIC implementation are 3.2-6.8 Tbps, 0.55 W, 2M gates, and 11.7 ns, and the PPAL of the FPGA implementation are 1.3-2.8 Tbps, 16.2 W, 43K LUTs, and 40 ns. The source code of HyperParser has been released on Github.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3469393.3469399',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Formal Modelling and Automated Trade-off Analysis of Enforcement Architectures for Cryptographic Access Control in the Cloud',\n",
       "  'authors': \"['Stefano Berlato', 'Roberto Carbone', 'Adam J. Lee', 'Silvio Ranise']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Privacy and Security',\n",
       "  'abstract': 'To facilitate the adoption of cloud by organizations, Cryptographic Access Control (CAC) is the obvious solution to control data sharing among users while preventing partially trusted Cloud Service Providers (CSP) from accessing sensitive data. Indeed, several CAC schemes have been proposed in the literature. Despite their differences, available solutions are based on a common set of entities—e.g., a data storage service or a proxy mediating the access of users to encrypted data—that operate in different (security) domains—e.g., on-premise or the CSP. However, the majority of these CAC schemes assumes a fixed assignment of entities to domains; this has security and usability implications that are not made explicit and can make inappropriate the use of a CAC scheme in certain scenarios with specific trust assumptions and requirements. For instance, assuming that the proxy runs at the premises of the organization avoids the vendor lock-in effect but may give rise to other security concerns (e.g., malicious insiders attackers).To the best of our knowledge, no previous work considers how to select the best possible architecture (i.e., the assignment of entities to domains) to deploy a CAC scheme for the trust assumptions and requirements of a given scenario. In this article, we propose a methodology to assist administrators in exploring different architectures for the enforcement of CAC schemes in a given scenario. We do this by identifying the possible architectures underlying the CAC schemes available in the literature and formalizing them in simple set theory. This allows us to reduce the problem of selecting the most suitable architectures satisfying a heterogeneous set of trust assumptions and requirements arising from the considered scenario to a decidable Multi-objective Combinatorial Optimization Problem (MOCOP) for which state-of-the-art solvers can be invoked. Finally, we show how we use the capability of solving the MOCOP to build a prototype tool assisting administrators to preliminarily perform a “What-if” analysis to explore the trade-offs among the various architectures and then use available standards and tools (such as TOSCA and Cloudify) for automated deployment in multiple CSPs.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3474056',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'StarFL: Hybrid Federated Learning Architecture for Smart Urban Computing',\n",
       "  'authors': \"['Anbu Huang', 'Yang Liu', 'Tianjian Chen', 'Yongkai Zhou', 'Quan Sun', 'Hongfeng Chai', 'Qiang Yang']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Intelligent Systems and Technology',\n",
       "  'abstract': 'From facial recognition to autonomous driving, Artificial Intelligence (AI) will transform the way we live and work over the next couple of decades. Existing AI approaches for urban computing suffer from various challenges, including dealing with synchronization and processing of vast amount of data generated from the edge devices, as well as the privacy and security of individual users, including their bio-metrics, locations, and itineraries. Traditional centralized-based approaches require data in each organization be uploaded to the central database, which may be prohibited by data protection acts, such as GDPR and CCPA. To decouple model training from the need to store the data in the cloud, a new training paradigm called Federated Learning (FL) is proposed. FL enables multiple devices to collaboratively learn a shared model while keeping the training data on devices locally, which can significantly mitigate privacy leakage risk. However, under urban computing scenarios, data are often communication-heavy, high-frequent, and asynchronized, posing new challenges to FL implementation. To handle these challenges, we propose a new hybrid federated learning architecture called StarFL. By combining with Trusted Execution Environment (TEE), Secure Multi-Party Computation (MPC), and (Beidou) satellites, StarFL enables safe key distribution, encryption, and decryption, and provides a verification mechanism for each participant to ensure the security of the local data. In addition, StarFL can provide accurate timestamp matching to facilitate synchronization of multiple clients. All these improvements make StarFL more applicable to the security-sensitive scenarios for the next generation of urban computing.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3467956',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Research on Distributed Edge Computing Architecture Scheme for Cable Status Awareness',\n",
       "  'authors': \"['Zehua Pan', 'Zhigang Ren', 'Wei Guo', 'Yekun Men', 'Zhao Yu', 'Ping Chen', 'Ming Ren', 'Ran Duan']\",\n",
       "  'date': 'May 2021',\n",
       "  'source': 'ICAIIS 2021: 2021 2nd International Conference on Artificial Intelligence and Information Systems',\n",
       "  'abstract': 'Power cables are important equipment in the power system. Comprehensive condition monitoring of the cables is the key to maintaining the safety and stability of the power system. Through the partial discharge (PD) multi-state perception method, various faults of the cable insulation can be detected in time, and mutual verification and supplementation can be achieved. The distributed sensing architecture can realize the comprehensive perception of the cable status while exerting the edge computing capabilities of the sensors, reducing the pressure of data transmission and processing. This paper proposes a distributed edge computing architecture scheme for cable state awareness, and on this basis, studies the architecture operation and data transmission methods.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3469213.3470663',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'The Demikernel Datapath OS Architecture for Microsecond-scale Datacenter Systems',\n",
       "  'authors': \"['Irene Zhang', 'Amanda Raybuck', 'Pratyush Patel', 'Kirk Olynyk', 'Jacob Nelson', 'Omar S. Navarro Leija', 'Ashlie Martinez', 'Jing Liu', 'Anna Kornfeld Simpson', 'Sujay Jayakar', 'Pedro Henrique Penna', 'Max Demoulin', 'Piali Choudhury', 'Anirudh Badam']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"SOSP '21: Proceedings of the ACM SIGOPS 28th Symposium on Operating Systems Principles\",\n",
       "  'abstract': \"Datacenter systems and I/O devices now run at single-digit microsecond latencies, requiring ns-scale operating systems. Traditional kernel-based operating systems impose an unaffordable overhead, so recent kernel-bypass OSes [73] and libraries [23] eliminate the OS kernel from the I/O datapath. However, none of these systems offer a general-purpose datapath OS replacement that meet the needs of μs-scale systems.' AB@This paper proposes Demikernel, a flexible datapath OS and architecture designed for heterogenous kernel-bypass devices and μs-scale datacenter systems. We build two prototype Demikernel OSes and show that minimal effort is needed to port existing μs-scale systems. Once ported, Demikernel lets applications run across heterogenous kernel-bypass devices with ns-scale overheads and no code changes.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3477132.3483569',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Design and Research of Web Crawler Based on Distributed Architecture',\n",
       "  'authors': \"['Lili Wang', 'Haoliang Wang']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': 'AIAM2021: 2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture',\n",
       "  'abstract': 'Internet data is abundance in content and diverse in organization. In order to automatically complete the process of collecting, analyzing, and storing large amounts of data and information on the web, a web crawler technology based on Hadoop distributed clusters is proposed. Using Nutch crawler framework, Hadoop distributed technology, and Zookeeper distributed coordination service framework, through the construction of distributed clusters, and high-performance Key-Value database Redis to store data, it verifies the feasibility of distributed crawlers. Through the analysis of data collection experiments, the comparison of multiple sets of experimental data between the distributed crawler based on Nutch and the traditional crawler shows that the crawler design of the distributed architecture is superior to the traditional crawler in terms of collection speed and efficiency.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3495018.3495061',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Discovering enterprise architecture developing a course with enterprise application software tools',\n",
       "  'authors': \"['Maria Weber', 'Kyle Chapman', 'John Buerck']\",\n",
       "  'date': 'None',\n",
       "  'source': 'Journal of Computing Sciences in Colleges',\n",
       "  'abstract': \"Information Technology (IT) and Digital Transformation (Dx) are shifting how companies function and embrace emerging technologies. Companies are reimagining how work can be done by adopting new technologies to existing or new infrastructure. Organizations need skilled IT professionals who can drive this transformation, leading to the establishment of a robust enterprise. Industry trends indicate a need to shift academic curricula in Information System (IS) programs. Enterprise Architecture (EA) is a course in a graduate IS program at Saint Louis University. The EA curriculum focuses on the alignment between IT and business. The course covers designing, planning, and implementing organizational changes to corporate architecture through standardization of principles, models, and procedures. The use of EA open-source tools in this course is innovative and ideal for building students' marketable skills and improving their university experience. This paper provides an overview of designing and delivering a gateway course in Enterprise Architecture and Infrastructure Systems using enterprise-class and open-source software tools as part of the Master of Information Systems.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3469567.3469569',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'IOT Data Storage Solution Based on Hybrid Blockchain Edge Architecture',\n",
       "  'authors': \"['Li Zhou', 'Jianhua Liu']\",\n",
       "  'date': 'September 2021',\n",
       "  'source': \"AIPR '21: Proceedings of the 2021 4th International Conference on Artificial Intelligence and Pattern Recognition\",\n",
       "  'abstract': 'In order to solve the problem of inefficient data storage and high latency in the case of a large number of IoT devices networked, for this purpose, this paper implements a data storage scheme for IoT devices based on blockchain and edge computing architecture, using an interstellar file system network formed by edge computing servers as nodes to achieve distributed localized storage of data and storing data hash addresses into the blockchain. Combined with smart contracts and an attribute-based access control framework, secure dynamic fine-grained access control of data and behavior tracking of visitors are achieved. Experiments show that the proposed scheme can effectively reduce data transmission latency, as well as lower latency request response and higher throughput, which also makes it more compatible with the current multi-user and multi-request access requirements in the IoT environment.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3488933.3489012',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A low-cost AR assistant component architecture for Warehouse Management Systems',\n",
       "  'authors': \"['Iakovos Stratigakis', 'Theodoros Amanatidis', 'Christina Volioti', 'George Kakarontzas', 'Thrasyvoulos Tsiatsos', 'Ioannis Stamelos', 'Charalampos Avratoglou', 'Apostolos Ampatzoglou', 'Alexander Chatzigeorgiou', 'Dimitris Folinas']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"PCI '21: Proceedings of the 25th Pan-Hellenic Conference on Informatics\",\n",
       "  'abstract': 'This work describes a research collaboration between universities and industry with the aim to provide a low-cost prototype based on Augmented Reality technologies, that assists with maintaining correct information in Warehouse Management Systems. The component interacts with the central server of an existing commercial WMS to provide up-to-date information on the actual state of the warehouse. The low-cost requirement restricts the solution to smartphones and other inexpensive equipment readily available, such as drones, as well as mostly Open Source Software. This requirement also introduces several interesting architectural issues that we discuss in this work. A prototype was built for the proposed architecture and several tests were carried out.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503823.3503854',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Large graph convolutional network training with GPU-oriented data communication architecture',\n",
       "  'authors': \"['Seung Won Min', 'Kun Wu', 'Sitao Huang', 'Mert Hidayetoğlu', 'Jinjun Xiong', 'Eiman Ebrahimi', 'Deming Chen', 'Wen-mei Hwu']\",\n",
       "  'date': 'None',\n",
       "  'source': 'Proceedings of the VLDB Endowment',\n",
       "  'abstract': 'Graph Convolutional Networks (GCNs) are increasingly adopted in large-scale graph-based recommender systems. Training GCN requires the minibatch generator traversing graphs and sampling the sparsely located neighboring nodes to obtain their features. Since real-world graphs often exceed the capacity of GPU memory, current GCN training systems keep the feature table in host memory and rely on the CPU to collect sparse features before sending them to the GPUs. This approach, however, puts tremendous pressure on host memory bandwidth and the CPU. This is because the CPU needs to (1) read sparse features from memory, (2) write features into memory as a dense format, and (3) transfer the features from memory to the GPUs.In this work, we propose a novel GPU-oriented data communication approach for GCN training, where GPU threads directly access sparse features in host memory through zero-copy accesses without much CPU help. By removing the CPU gathering stage, our method significantly reduces the consumption of the host resources and data access latency. We further present two important techniques to achieve high host memory access efficiency by the GPU: (1) automatic data access address alignment to maximize PCIe packet efficiency, and (2) asynchronous zero-copy access and kernel execution to fully overlap data transfer with training. We incorporate our method into PyTorch and evaluate its effectiveness using several graphs with sizes up to 111 million nodes and 1.6 billion edges. In a multi-GPU training setup, our method is 65--92% faster than the conventional data transfer method, and can even match the performance of all-in-GPU-memory training for some graphs that fit in GPU memory.',\n",
       "  'link': 'https://dl.acm.org/doi/10.14778/3476249.3476264',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'SecTULab: A Moodle-Integrated Secure Remote Access Architecture for Cyber Security Laboratories',\n",
       "  'authors': \"['Joachim Fabini', 'Alexander Hartl', 'Fares Meghdouri', 'Claudia Breitenfellner', 'Tanja Zseby']\",\n",
       "  'date': 'August 2021',\n",
       "  'source': \"ARES '21: Proceedings of the 16th International Conference on Availability, Reliability and Security\",\n",
       "  'abstract': 'The Covid-19 crisis has challenged cyber security teaching by creating the need for secure remote access to existing cyber security laboratory infrastructure. In this paper, we present requirements, architecture and key functionalities of a secure remote laboratory access solution that has been instantiated successfully for two existing laboratories at TU Wien. The proposed design prioritizes security and privacy aspects while integrating with existing Moodle eLearning platforms to leverage available authentication and group collaboration features. Performance evaluations of the prototype implementation for real cyber security classes support a first estimate of dimensioning and resources that must be provisioned when implementing the proposed secure remote laboratory access.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3465481.3470034',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Design of inpatient medical insurance management system based on three-tier architecture mode',\n",
       "  'authors': \"['Lianying Ge']\",\n",
       "  'date': 'September 2021',\n",
       "  'source': 'ICISCAE 2021: 2021 4th International Conference on Information Systems and Computer Aided Education',\n",
       "  'abstract': 'The informatization of medical insurance (medical insurance) plays a very important role in the reform of medical insurance. The field of medical insurance has its particularity. The regional difference and variability of medical insurance policy make the medical insurance system have strong scalability and maintainability. In the past, most medical insurance systems used C / S architecture. In this architecture, the client integrated user interface and business logic, and the business logic is the most easily changed. Three-tier architecture is a further extension of C/S architecture, which adopts layered idea and component technology. The three-tier architecture is logically divided into user layer, business layer and data layer. The user layer is responsible for interacting with users, the business layer is responsible for business logic, and the data layer is responsible for information access. This paper introduces the standard requirements of medical insurance management system, and adopts the three-tier structure model for system statistics.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3482632.3487551',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Failure sentinels: ubiquitous just-in-time intermittent computation via low-cost hardware support for voltage monitoring',\n",
       "  'authors': \"['Harrison Williams', 'Michael Moukarzel', 'Matthew Hicks']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': 'Energy harvesting systems support the deployment of low-power microcontrollers untethered by constant power sources or batteries, enabling long-lived deployments in a variety of applications previously limited by power or size constraints. However, the limitations of harvested energy mean that even the lowest-power microcontrollers operate intermittently---waiting for the harvester to slowly charge a buffer capacitor and rapidly discharging the capacitor to support a brief burst of computation. The challenges of the intermittent operation brought on by harvested energy drive a variety of hardware and software techniques that first enabled long-running computation, then focused on improving performance. Many of the most promising systems demand dynamic updates of available energy to inform checkpointing and mode decisions. Unfortunately, existing energy monitoring solutions based on analog circuits (e.g., analog-to-digital converters) are ill-matched for the task because their signal processing focus sacrifices power efficiency for increased performance---performance not required by current or future intermittent computation systems. This results in existing solutions consuming as much energy as the microcontroller, stealing energy from useful computation. To create a low-power energy monitoring solution that provides just enough performance for intermittent computation use cases, we design and implement Failure Sentinels, an on-chip, fully-digital energy monitor. Failure Sentinels leverages the predictable propagation delay response of digital logic gates to supply voltage fluctuations to measure available energy. Our design space exploration shows that Failure Sentinels provides 30--50mV of resolution at sample rates up to 10kHz, while consuming less than 2μA of current. Experiments show that Failure Sentinels increases the energy available for software computation by up to 77%, compared to current solutions. We also implement a RISC-V-based FPGA prototype that validates our design space exploration and shows the overheads of incorporating Failure Sentinels into a system-on-chip.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00058',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Design and Implementation of Smart Archives Information Service Architecture',\n",
       "  'authors': \"['Ying Luo']\",\n",
       "  'date': 'September 2021',\n",
       "  'source': 'ICISCAE 2021: 2021 4th International Conference on Information Systems and Computer Aided Education',\n",
       "  'abstract': 'None',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3482632.3484063',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'CGRA-EAM—Rapid Energy and Area Estimation for Coarse-grained Reconfigurable Architectures',\n",
       "  'authors': \"['Mark Wijtvliet', 'Henk Corporaal', 'Akash Kumar']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Reconfigurable Technology and Systems',\n",
       "  'abstract': 'Reconfigurable architectures are quickly gaining in popularity due to their flexibility and ability to provide high energy efficiency. However, reconfigurable systems allow for a huge design space. Iterative design space exploration (DSE) is often required to achieve good Pareto points with respect to some combination of performance, area, and/or energy. DSE tools depend on information about hardware characteristics in these aspects. These characteristics can be obtained from hardware synthesis and net-list simulation, but this is very time-consuming. Therefore, architecture models are common. This work introduces CGRA-EAM (Coarse-Grained Reconfigurable Architecture - Energy & Area Model), a model for energy and area estimation framework for coarse-grained reconfigurable architectures. The model is evaluated for the Blocks CGRA. The results demonstrate that the mean absolute percentage error is 15.5% and 2.1% for energy and area, respectively, while the model achieves a speedup of close to three orders of magnitude compared to synthesis.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3468874',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Exploiting architecture advances for sparse solvers in circuit simulation',\n",
       "  'authors': \"['Zhiyuan Yan', 'Biwei Xie', 'Xingquan Li', 'Yungang Bao']\",\n",
       "  'date': 'March 2022',\n",
       "  'source': \"DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe\",\n",
       "  'abstract': 'Sparse direct solvers provide vital functionality for a wide variety of scientific applications. The dominated part of the sparse direct solver, LU factorization, suffers a lot from the irregularity of sparse matrices. Meanwhile, the specific characteristics of sparse solvers in circuit simulation and unique sparse pattern of circuit matrices provide more design spaces and also great challenges. In this paper, we propose a sparse solver named FLU and re-examine the performance of LU factorization from the perspectives of vectorization, parallelization, and data locality. To improve vectorization efficiency and data locality, FLU introduces a register-level supernode computation method by delicately manipulating data movement. With alternating multiple columns computation, FLU further reduces the off-chip memory accesses greatly. Furthermore, we implement a fine-grained elimination tree based parallelization scheme to fully exploit task-level parallelism. Compared with PARDISO and NICSLU, experimental results show that FLU achieves a speedup up to 19.51× (3.86× on average) and 2.56× (1.66× on average) on Intel Xeon respectively.',\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3539845.3540043',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Towards a Blockchain-SDN Architecture for Secure and Trustworthy 5G Massive IoT Networks',\n",
       "  'authors': \"['Akram Hakiri', 'Behnam Dezfouli']\",\n",
       "  'date': 'April 2021',\n",
       "  'source': \"SDN-NFV Sec'21: Proceedings of the 2021 ACM International Workshop on Software Defined Networks &amp; Network Function Virtualization Security\",\n",
       "  'abstract': 'The emerging 5G mobile network is a prominent technology for addressing networking related challenges of Internet of Things (IoT). The forthcoming 5G is expected to allow low-power massive IoT devices to produce high volumes of data that can be transmitted over ultra-reliable, low-latency wireless communication services. However, IoT systems encounter several security and privacy issues to prevent unauthorized access to IoT nodes. To address these challenges, this paper introduces a novel blockchain-based architecture that leverages Software Defined Network (SDN) and Network Function Virtualization (NFV) for securing IoT transactions. A novel security appliance is introduced in a form of Virtualized Network Functions (VNFs) for improving the scalability and performance of IoT networks. Then, we introduce a novel consensus algorithm to detect and report suspected IoT nodes and mitigate malicious traffic. We evaluate and compare our proposed solution against three well-known consensus algorithms, i.e., Proof of Work (PoW), Proof of Elapsed Time (PoET), and Proof of Stake (PoS). We demonstrate that the proposed solution provides substantially lower latency and higher throughput as well as trustworthy IoT communication.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3445968.3452090',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'SMART: A Heterogeneous Scratchpad Memory Architecture for Superconductor SFQ-based Systolic CNN Accelerators',\n",
       "  'authors': \"['Farzaneh Zokaee', 'Lei Jiang']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"MICRO '21: MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture\",\n",
       "  'abstract': 'Ultra-fast & low-power superconductor single-flux-quantum (SFQ)-based CNN systolic accelerators are built to enhance the CNN inference throughput. However, shift-register (SHIFT)-based scratchpad memory (SPM) arrays prevent a SFQ CNN accelerator from exceeding 40% of its peak throughput, due to the lack of random access capability. This paper first documents our study of a variety of cryogenic memory technologies, including Vortex Transition Memory (VTM), Josephson-CMOS SRAM, MRAM, and Superconducting Nanowire Memory, during which we found that none of the aforementioned technologies made a SFQ CNN accelerator achieve high throughput, small area, and low power simultaneously. Second, we present a heterogeneous SPM architecture, SMART, composed of SHIFT arrays and a random access array to improve the inference throughput of a SFQ CNN systolic accelerator. Third, we propose a fast, low-power and dense pipelined random access CMOS-SFQ array by building SFQ passive-transmission-line-based H-Trees that connect CMOS sub-banks. Finally, we create an ILP-based compiler to deploy CNN models on SMART. Experimental results show that, with the same chip area overhead, compared to the latest SHIFT-based SFQ CNN accelerator, SMART improves the inference throughput by 3.9 × (2.2 ×), and reduces the inference energy by 86% (71%) when inferring a single image (a batch of images).',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3466752.3480041',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'SIEM Architecture for the Internet of Things and Smart City',\n",
       "  'authors': '[\\'Abdalrahman Hwoij\\', \"As\\'har Khamaiseh\", \\'Mohammad Ababneh\\']',\n",
       "  'date': 'April 2021',\n",
       "  'source': \"DATA'21: International Conference on Data Science, E-learning and Information Systems 2021\",\n",
       "  'abstract': \"The Internet of things (IoT) is a new technology that shapes the future of a world that is rapidly being invaded by smart devices connected to the Internet. Such technology has a great role in developing the idea of a smart city. A smart city is a city that takes advantage of existing infrastructure and integrates it with the Internet of things technology to improve the quality of life. Internet of Things (IoT) sensors are distributed geographically around the city to collect data from the environment (i.e.: streets, cars, traffic lights...etc.), process, and manage it to provide intelligent actionable information to citizens. All data transferred through networks of a smart city may be threatened and susceptible to illegal actions such as violation, stealing, and inappropriate use. These security threats affect the privacy and security of users; where hackers can get access to user's data and gain control of their smart homes, cars, medical devices and might even gain control over city traffic lights. All the above enforce the need to have a security system that continuously monitors and tracks all data logs to detect any suspicious activity. In this paper, we propose a Security Information and Event Management (SIEM) approach for smart cities by forwarding event logs generated by smart devices to a security operation center that works around the clock to detect security incidents and handle them. Such an approach aims to create a safe smart living environment.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3460620.3460747',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Analysis and Optimization Discussion on Control System Architecture of Electrochemical energy storage Power Station',\n",
       "  'authors': \"['Quan Hong', 'Jinbo Wu', 'Li Li', 'Shangfeng Xiong', 'Yusheng Gong', 'Zhihao Liu']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"ICITEE '21: Proceedings of the 4th International Conference on Information Technologies and Electrical Engineering\",\n",
       "  'abstract': 'With the continuous expansion of the scale of electrochemical energy storage power station connected to the grid, the demand for its unified dispatching control to participate in multi-scenario applications such as peak shaving, frequency modulation and dynamic reactive power support is also increasing. In this paper, combined with the above requirements, the shortcomings of the current mainstream technical route are analyzed, and the improvement direction is discussed. The system improvement scheme based on 4S architecture is proposed, which can be used as a reference for the improvement of the subsequent energy storage control system.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3513142.3513164',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Research on Financial Intelligence System Architecture and Metadata Management Based on Support Vector Machine',\n",
       "  'authors': \"['Yifei Bai']\",\n",
       "  'date': 'September 2021',\n",
       "  'source': 'ICISCAE 2021: 2021 4th International Conference on Information Systems and Computer Aided Education',\n",
       "  'abstract': \"As far as the present situation of financial market supervision system is concerned, there are some problems in financial statistics, such as lack of information and distortion of information, which have greatly affected the stable development of China's financial market system and caused some damage to the national economy. Aiming at the business and technical problems of bank financial risk management, this paper constructs the basic framework of financial intelligence system, and designs the corresponding data warehouse system architecture. Through the storage and analysis of financial system monitoring information, using distributed file system and parallel programming model, the management scheme of application cluster and virtual resources is proposed, and metadata is automatically extracted by using support vector machine (SVM) method. At the same time, through the difference analysis and importance analysis of metadata management, we can clearly know the importance of data dictionary and the impact of data dictionary changes.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3482632.3487532',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Research on the Overall Architecture Design of Project Management Information System Based on SOA',\n",
       "  'authors': \"['Kun Zhang', 'Xia Zhang']\",\n",
       "  'date': 'September 2021',\n",
       "  'source': 'ICISCAE 2021: 2021 4th International Conference on Information Systems and Computer Aided Education',\n",
       "  'abstract': 'Engineering project management system is an indispensable part of enterprises, and it is a key issue for the survival and development of construction enterprises. In this paper, the overall structure of the system is analyzed and designed in detail on the basis of following SOA architecture and its layered thought, and the design of each level is analyzed and explained in detail. Different from traditional management information systems, parallel management information systems interconnect virtual systems with real information systems, and integrate social factors such as human behavior and management system into overall consideration. The system has the characteristics of openness and flexible reconfiguration. To ensure the effective implementation of project construction, in order to comprehensively improve the project management ability.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3482632.3483996',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Cambricon-Q: a hybrid architecture for efficient training',\n",
       "  'authors': \"['Yongwei Zhao', 'Chang Liu', 'Zidong Du', 'Qi Guo', 'Xing Hu', 'Yimin Zhuang', 'Zhenxing Zhang', 'Xinkai Song', 'Wei Li', 'Xishan Zhang', 'Ling Li', 'Zhiwei Xu', 'Tianshi Chen']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': 'Deep neural network (DNN) training is notoriously time-consuming, and quantization is promising to improve the training efficiency with reduced bandwidth/storage requirements and computation costs. However, state-of-the-art quantized algorithms with negligible training accuracy loss, which require on-the-fly statistic-based quantization over a great amount of data (e.g., neurons and weights) and high-precision weight update, cannot be effectively deployed on existing DNN accelerators. To address this problem, we propose the first customized architecture for efficient quantized training with negligible accuracy loss, which is named as Cambricon-Q. Cambricon-Q features a hybrid architecture consisting of an ASIC acceleration core and a near-data-processing (NDP) engine. The acceleration core mainly targets at improving the efficiency of statistic-based quantization with specialized computing units for both statistical analysis (e.g., determining maximum) and data reformating, while the NDP engine avoids transferring the high-precision weights from the off-chip memory to the acceleration core. Experimental results show that on the evaluated benchmarks, Cambricon-Q improves the energy efficiency of DNN training by 6.41X and 1.62X, performance by 4.20X and 1.70X compared to GPU and TPU, respectively, with only ⩽ 0.4% accuracy degradation compared with full precision training.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00061',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Research on the Overall Architecture Design of Project Management Information System Based on SOA',\n",
       "  'authors': \"['Kun Zhang', 'Xia Zhang']\",\n",
       "  'date': 'September 2021',\n",
       "  'source': 'ICISCAE 2021: 2021 4th International Conference on Information Systems and Computer Aided Education',\n",
       "  'abstract': 'Engineering project management system is an indispensable part of enterprises, and it is a key issue for the survival and development of construction enterprises. In this paper, the overall structure of the system is analyzed and designed in detail on the basis of following SOA architecture and its layered thought, and the design of each level is analyzed and explained in detail. Different from traditional management information systems, parallel management information systems interconnect virtual systems with real information systems, and integrate social factors such as human behavior and management system into overall consideration. The system has the characteristics of openness and flexible reconfiguration. To ensure the effective implementation of project construction, in order to comprehensively improve the project management ability.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3482632.3483996',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A Flow-based Multi-agent Data Exfiltration Detection Architecture for Ultra-low Latency Networks',\n",
       "  'authors': \"['Rafael Salema Marques', 'Gregory Epiphaniou', 'Haider Al-Khateeb', 'Carsten Maple', 'Mohammad Hammoudeh', 'Paulo André Lima De Castro', 'Ali Dehghantanha', 'Kim Kwang Raymond Choo']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Internet Technology',\n",
       "  'abstract': 'Modern network infrastructures host converged applications that demand rapid elasticity of services, increased security, and ultra-fast reaction times. The Tactile Internet promises to facilitate the delivery of these services while enabling new economies of scale for high fidelity of machine-to-machine and human-to-machine interactions. Unavoidably, critical mission systems served by the Tactile Internet manifest high demands not only for high speed and reliable communications but equally, the ability to rapidly identify and mitigate threats and vulnerabilities. This article proposes a novel Multi-Agent Data Exfiltration Detector Architecture (MADEX), inspired by the mechanisms and features present in the human immune system. MADEX seeks to identify data exfiltration activities performed by evasive and stealthy malware that hides malicious traffic from an infected host in low-latency networks. Our approach uses cross-network traffic information collected by agents to effectively identify unknown illicit connections by an operating system subverted. MADEX does not require prior knowledge of the characteristics or behavior of the malicious code or a dedicated access to a knowledge repository. We tested the performance of MADEX in terms of its capacity to handle real-time data and the sensitivity of our algorithm’s classification when exposed to malicious traffic. Experimental evaluation results show that MADEX achieved 99.97% sensitivity, 98.78% accuracy, and an error rate of 1.21% when compared to its best rivals. We created a second version of MADEX, called MADEX level 2, that further improves its overall performance with a slight increase in computational complexity. We argue for the suitability of MADEX level 1 in non-critical environments, while MADEX level 2 can be used to avoid data exfiltration in critical mission systems. To the best of our knowledge, this is the first article in the literature that addresses the detection of rootkits real-time in an agnostic way using an artificial immune system approach while it satisfies strict latency requirements.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3419103',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Distributed Graph Processing System and Processing-in-memory Architecture with Precise Loop-carried Dependency Guarantee',\n",
       "  'authors': \"['Youwei Zhuo', 'Jingji Chen', 'Gengyu Rao', 'Qinyi Luo', 'Yanzhi Wang', 'Hailong Yang', 'Depei Qian', 'Xuehai Qian']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Computer Systems',\n",
       "  'abstract': 'To hide the complexity of the underlying system, graph processing frameworks ask programmers to specify graph computations in user-defined functions (UDFs) of graph-oriented programming model. Due to the nature of distributed execution, current frameworks cannot precisely enforce the semantics of UDFs, leading to unnecessary computation and communication. It exemplifies a gap between programming model and runtime execution. This article proposes novel graph processing frameworks for distributed system and Processing-in-memory (PIM) architecture that precisely enforces loop-carried dependency; i.e., when a condition is satisfied by a neighbor, all following neighbors can be skipped. Our approach instruments the UDFs to express the loop-carried dependency, then the distributed execution framework enforces the precise semantics by performing dependency propagation dynamically. Enforcing loop-carried dependency requires the sequential processing of the neighbors of each vertex distributed in different nodes. We propose to circulant scheduling in the framework to allow different nodes to process disjoint sets of edges/vertices in parallel while satisfying the sequential requirement. The technique achieves an excellent trade-off between precise semantics and parallelism—the benefits of eliminating unnecessary computation and communication offset the reduced parallelism. We implement a new distributed graph processing framework SympleGraph, and two variants of runtime systems—GraphS and GraphSR—for PIM-based graph processing architecture, which significantly outperform the state-of-the-art.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3453681',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Dynamic Scheduling Algorithm in Cyber Mimic Defense Architecture of Volunteer Computing',\n",
       "  'authors': \"['Qianmu Li', 'Shunmei Meng', 'Xiaonan Sang', 'Hanrui Zhang', 'Shoujin Wang', 'Ali Kashif Bashir', 'Keping Yu', 'Usman Tariq']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Internet Technology',\n",
       "  'abstract': 'Volunteer computing uses computers volunteered by the general public to do distributed scientific computing. Volunteer computing is being used in high-energy physics, molecular biology, medicine, astrophysics, climate study, and other areas. These projects have attained unprecedented computing power. However, with the development of information technology, the traditional defense system cannot deal with the unknown security problems of volunteer computing. At the same time, Cyber Mimic Defense (CMD) can defend the unknown attack behavior through its three characteristics: dynamic, heterogeneous, and redundant. As an important part of the CMD, the dynamic scheduling algorithm realizes the dynamic change of the service centralized executor, which can enusre the security and reliability of CMD of volunteer computing. Aiming at the problems of passive scheduling and large scheduling granularity existing in the existing scheduling algorithms, this article first proposes a scheduling algorithm based on time threshold and task threshold and realizes the dynamic randomness of mimic defense from two different dimensions; finally, combining time threshold and random threshold, a dynamic scheduling algorithm based on multi-level queue is proposed. The experiment shows that the dynamic scheduling algorithm based on multi-level queue can take both security and reliability into account, has better dynamic heterogeneous redundancy characteristics, and can effectively prevent the transformation rule of heterogeneous executors from being mastered by attackers.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3408291',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Point-X: A Spatial-Locality-Aware Architecture for Energy-Efficient Graph-Based Point-Cloud Deep Learning',\n",
       "  'authors': \"['Jie-Fang Zhang', 'Zhengya Zhang']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"MICRO '21: MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture\",\n",
       "  'abstract': 'Deep learning on point clouds has attracted increasing attention in the fields of 3D computer vision and robotics. In particular, graph-based point-cloud deep neural networks (DNNs) have demonstrated promising performance in 3D object classification and scene segmentation tasks. However, the scattered and irregular graph-structured data in a graph-based point-cloud DNN cannot be computed efficiently by existing SIMD architectures and accelerators. We present Point-X, an energy-efficient accelerator architecture that extracts and exploits the spatial locality in point cloud data for efficient processing. Point-X uses a clustering method to extract fine-grained and coarse-grained spatial locality from the input point cloud. The clustering maps the point cloud into distributed compute tiles to maximize intra-tile computational parallelism and minimize inter-tile data movement. Point-X employs a chain network-on-chip (NoC) to further reduce the NoC traffic and achieve up to 3.2 × speedup over a traditional mesh NoC. Point-X’s multi-mode dataflow can support all common operations in a graph-based point-cloud DNN, i.e., edge convolution, shared multi-layer perceptron, and fully-connected layers. Point-X is synthesized in a 28nm technology and it demonstrates a throughput of 1307.1 inference/s and an energy efficiency of 604.5 inference/J on the DGCNN workload. Compared to the Nvidia GTX-1080Ti GPU, Point-X shows 4.5 × and 342.9 × improvement in throughput and efficiency, respectively.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3466752.3480081',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A fresh look at the architecture and performance of contemporary isolation platforms',\n",
       "  'authors': \"['Vincent van Rijn', 'Jan S. Rellermeyer']\",\n",
       "  'date': 'December 2021',\n",
       "  'source': \"Middleware '21: Proceedings of the 22nd International Middleware Conference\",\n",
       "  'abstract': 'With the ever-increasing pervasiveness of the cloud computing paradigm, strong isolation guarantees and low performance overhead from isolation platforms are paramount. An ideal isolation platform offers both: an impermeable isolation boundary while imposing a negligible performance overhead. In this paper, we examine various isolation platforms (containers, secure containers, hypervisors, unikernels), and conduct a wide array of experiments to measure the performance overhead and degree of isolation offered by the platforms. We find that container platforms have the best, near-native, performance while the newly emerging secure containers suffer from various overheads. The highest degree of isolation is achieved by unikernels, closely followed by traditional containers.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3464298.3493404',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'PipeZK: accelerating zero-knowledge proof with a pipelined architecture',\n",
       "  'authors': \"['Ye Zhang', 'Shuo Wang', 'Xian Zhang', 'Jiangbin Dong', 'Xingzhong Mao', 'Fan Long', 'Cong Wang', 'Dong Zhou', 'Mingyu Gao', 'Guangyu Sun']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': 'Zero-knowledge proof (ZKP) is a promising cryptographic protocol for both computation integrity and privacy. It can be used in many privacy-preserving applications including verifiable cloud outsourcing and blockchains. The major obstacle of using ZKP in practice is its time-consuming step for proof generation, which consists of large-size polynomial computations and multi-scalar multiplications on elliptic curves. To efficiently and practically support ZKP in real-world applications, we propose PipeZK, a pipelined accelerator with two subsystems to handle the aforementioned two intensive compute tasks, respectively. The first subsystem uses a novel dataflow to decompose large kernels into smaller ones that execute on bandwidth-efficient hardware modules, with optimized off-chip memory accesses and on-chip compute resources. The second subsystem adopts a lightweight dynamic work dispatch mechanism to share the heavy processing units, with minimized resource underutilization and load imbalance. When evaluated in 28 nm, PipeZK can achieve 10x speedup on standard cryptographic benchmarks, and 5x on a widely-used cryptocurrency application, Zcash.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00040',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Research on the Architecture and Key Technologies of the Ubiquitous Customer Service Operating System for State Grid',\n",
       "  'authors': \"['Bin Xu', 'Xusheng Liu', 'Wei Zhao', 'Ziqian Li', 'Chenfei Wang', 'Qing Zhu']\",\n",
       "  'date': 'May 2021',\n",
       "  'source': 'ICAIIS 2021: 2021 2nd International Conference on Artificial Intelligence and Information Systems',\n",
       "  'abstract': 'Traditional customer service systems can hardly satisfy the high quality and efficiency requirements in the digital and intelligent era, to tackle this issue, advanced data and artificial intelligence technology are required to be integrated. Based on the ubiquitous operating system theory and take the State Grid customer service as a business scenario, this paper proposes an intelligent customer service platform. The platform takes the ubiquitous power customer service operating system as its core, manages and information multiplexes the massive data resources systematically to support customer service in a high-efficiency and intelligent manner. This paper studies the architecture of the platform and the key technologies involved, and implemented core applications at the industrial level. The paper also discusses the reliability of the system and proposed a reliability guarantee mechanism for customer service operating system. The demonstration system was verified in the actual production environment of the State Grid, and finally proved that this system can effectively improve the work efficiency of the customer service team and reduce operation and maintenance costs.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3469213.3470417',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Plant(e)tecture: Towards a Multispecies Media Architecture Framework for amplifying Plant Agencies',\n",
       "  'authors': \"['Hira Sheikh', 'Kavita Gonsalves', 'Marcus Foth']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': 'MAB20: Media Architecture Biennale 20',\n",
       "  'abstract': \"More-than-human media architecture is gaining increased attention as a response to the planet's environmental emergency and in turn allowing us to envision a more desirable future compared to tech-utopian smart cities. New types of digital technologies have emerged into the human mindscape, and with that a new potential for bridging human understanding and multispecies geographies. Emphasising the agency of plants, this paper attempts to answer the following research questions: how can twenty-first-century media technologies, such as media architecture, be used to better incorporate plant/flora perspectives? This paper begins by providing a review of multispecies ontology, exploration of plant agency and how media architecture can visualise and amplify plant agency based on Whitehead's Process Philosophy, Mark Hansen's Feed-Forward concept and Feenberg's technical agency framework. Hortum Machina B, terra0, and Elowan are explored as three examples of Plant(e)tecture where the multispecies actors “plants'' are coupled with technologies and analysed for agency and autonomy. Synthesising the three case studies, the paper discusses the role of media architecture in (i) enabling plant agency; (ii) engaging multispecies actors in autonomous decision-making, and; (iii) the creation of technology to amplify the agency of plants in a design process that transcends human sensibilities. The paper closes on the prospect of the multispecies-techno agency framework to enable human designers, makers, decision-makers and thinkers to move beyond human-centric determinism prevalent in media architecture. The framework offers new ways of thinking of the purpose of technical agency in multispecies assemblages.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3469410.3469419',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'ExRec: Experimental Framework for Reconfigurable Networks Based on Off-the-Shelf Hardware',\n",
       "  'authors': \"['Johannes Zerwas', 'Chen Avin', 'Stefan Schmid', 'Andreas Blenk']\",\n",
       "  'date': 'December 2021',\n",
       "  'source': \"ANCS '21: Proceedings of the Symposium on Architectures for Networking and Communications Systems\",\n",
       "  'abstract': 'In order to meet the increasingly stringent throughput and latency requirements in datacenter networks, several innovative network architectures based on reconfigurable optical topologies have been proposed. Examples include demand-oblivious reconfigurable topologies such as RotorNet (SIGCOMM 2017), Opera (NSDI 2020), and Sirius (SIGCOMM 2021), as well as demand-aware topologies such as ProjecToR (SIGCOMM 2016). All these architectures feature attractive performance properties using specific prototypes. However, reproducing these experiments is often difficult due to missing hardware and publicly available software. This paper presents a flexible framework for reconfigurable networks based on off-the-shelf hardware, which supports experimentation and reproducibility at a small scale. We describe how our framework, ExReC, can be instantiated with different configurations, allowing us to emulate existing architectures and to study their trade-offs. Finally, we demonstrate the application of our approach to different use cases and workloads, including distributed machine learning training.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3493425.3502748',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Practical and Scalable Security Verification of Secure Architectures',\n",
       "  'authors': \"['Tianwei Zhang', 'Jakub Szefer', 'Ruby B. Lee']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"HASP '21: Proceedings of the 10th International Workshop on Hardware and Architectural Support for Security and Privacy\",\n",
       "  'abstract': 'We present a new and practical framework for security verification of secure architectures. Specifically, we break the verification task into external verification and internal verification. External verification considers the external protocols, i.e. interactions between users, compute servers, network entities, etc. Meanwhile, internal verification considers the interactions between hardware and software components within each server. This verification framework is general-purpose and can be applied to a stand-alone server, or a large-scale distributed system. We evaluate our verification method on the CloudMonatt and HyperWall architectures as examples.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3505253.3505256',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'NeuroXplorer 1.0: An Extensible Framework for Architectural Exploration with Spiking Neural Networks',\n",
       "  'authors': \"['Adarsha Balaji', 'Shihao Song', 'Twisha Titirsha', 'Anup Das', 'Jeffrey Krichmar', 'Nikil Dutt', 'James Shackleford', 'Nagarajan Kandasamy', 'Francky Catthoor']\",\n",
       "  'date': 'July 2021',\n",
       "  'source': 'ICONS 2021: International Conference on Neuromorphic Systems 2021',\n",
       "  'abstract': 'Recently, both industry and academia have proposed many different neuromorphic architectures to execute applications that are designed with Spiking Neural Network (SNN). Consequently, there is a growing need for an extensible simulation framework that can perform architectural explorations with SNNs, including both platform-based design of today’s hardware, and hardware-software co-design and design-technology co-optimization of the future. We present NeuroXplorer, a fast and extensible framework that is based on a generalized template for modeling a neuromorphic architecture that can be infused with the specific details of a given hardware and/or technology. NeuroXplorer can perform both low-level cycle-accurate architectural simulations and high-level analysis with data-flow abstractions. NeuroXplorer’s optimization engine can incorporate hardware-oriented metrics such as energy, throughput, and latency, as well as SNN-oriented metrics such as inter-spike interval distortion and spike disorder, which directly impact SNN performance. We demonstrate the architectural exploration capabilities of NeuroXplorer through case studies with many state-of-the-art machine learning models.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3477145.3477156',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Architecture Design and Key Technology Research of Urban Intelligent Transportation Integrated Control System Based on Edge Cloud Cooperation Technology',\n",
       "  'authors': \"['Fenxin Zhang', 'Keshuang Tang', 'Huixin Zhang', 'Peng Zhang']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"CSAE '21: Proceedings of the 5th International Conference on Computer Science and Application Engineering\",\n",
       "  'abstract': 'With the development of vehicle-road collaborative technology and the popularity of unmanned vehicles, smart cars and smart roads have brought a wide range of impacts on urban intelligent transportation systems, which has made great changes in front-end equipment perception, front-end and back-end information interaction, and traffic collaborative command and control. This paper mainly studies the new application scenarios of urban intelligent transportation vehicle-road collaboration. With edge-cloud collaboration technology as the core, the container cloud technology is used to construct the system, which supports edge intelligent analysis, cloud big data fusion and multi-level deployment of system design theory, implementation methods and related collaborative technologies. Finally, the integrated management and control system of cloud, edge and end for urban intelligent transportation management is developed.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3487075.3487182',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Exploiting long-distance interactions and tolerating atom loss in neutral atom quantum architectures',\n",
       "  'authors': \"['Jonathan M. Baker', 'Andrew Litteken', 'Casey Duckering', 'Henry Hoffmann', 'Hannes Bernien', 'Frederic T. Chong']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': 'Quantum technologies currently struggle to scale beyond moderate scale prototypes and are unable to execute even reasonably sized programs due to prohibitive gate error rates or coherence times. Many software approaches rely on heavy compiler optimization to squeeze extra value from noisy machines but are fundamentally limited by hardware. Alone, these software approaches help to maximize the use of available hardware but cannot overcome the inherent limitations posed by the underlying technology. An alternative approach is to explore the use of new, though potentially less developed, technology as a path towards scalability. In this work we evaluate the advantages and disadvantages of a Neutral Atom (NA) architecture. NA systems offer several promising advantages such as long range interactions and native multiqubit gates which reduce communication overhead, overall gate count, and depth for compiled programs. Long range interactions, however, impede parallelism with restriction zones surrounding interacting qubit pairs. We extend current compiler methods to maximize the benefit of these advantages and minimize the cost. Furthermore, atoms in an NA device have the possibility to randomly be lost over the course of program execution which is extremely detrimental to total program execution time as atom arrays are slow to load. When the compiled program is no longer compatible with the underlying topology, we need a fast and efficient coping mechanism. We propose hardware and compiler methods to increase system resilience to atom loss dramatically reducing total computation time by circumventing complete reloads or full recompilation every cycle.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00069',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Design and practice of website group architecture and security based on data mining technology',\n",
       "  'authors': \"['Wei Zhao']\",\n",
       "  'date': 'December 2021',\n",
       "  'source': 'ICASIT 2021: 2021 International Conference on Aviation Safety and Information Technology',\n",
       "  'abstract': 'None',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3510858.3511423',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'AIoT Platform Design Based on Front and Rear End Separation Architecture for Smart Agricultural',\n",
       "  'authors': \"['Hang Li', 'Sufang Li', 'Jiguo Yu', 'Yubing Han', 'Anming Dong']\",\n",
       "  'date': 'January 2022',\n",
       "  'source': \"APIT '22: Proceedings of the 2022 4th Asia Pacific Information Technology Conference\",\n",
       "  'abstract': 'With the development of informatization, intelligence and precision of modern agriculture, i.e. there is a need for the integration of artificial intelligence (AI) with Internet of Things (IoT) systems, which is called AIoT (AI + IoT) systems. In this paper, we design an AIoT system for the smart agriculture based on the concept of front-rear end separation and the framework of MVVM (Model-View-View Model), through which it is possible to handle complex business logic and makes the integrating the AI algorithms much easier. Specifically, the system consists of a remote data service platform, the data collection terminals build on Raspberry Pi and the wireless data transmission using narrow-band Internet of Things (NB-IoT) modules. The data service platform is designed with the separated front-end and rear-end. The front-end is a web page constructed by the Vue.js and Element, while the rear-end business logic processing is constructed using the Python Django framework. The data interaction between the front and rear ends is realized through Axios. In such a way, the data in the front-end and the rear-end are decoupled, which makes it possible to improve the capability in dealing with complex data and makes it easy to carry out add-on development and extend new functions. Based on the data service platform, a series of basic application functions are integrated, including real-time data monitoring, historical data query, data visualization and abnormal data alerting, etc. Moreover, we integrate a deep-learning-based plant disease and pest detection algorithm in the propose system to show its scalability. In addition, the system also combines edge computing technology to improve the overall response efficiency of the system. The system has a convenient expansion interface and can be used as a basic development platform for various agricultural IoT applications, such as the soil environmental monitoring system and the intelligent disease and pest monitoring system, etc.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3512353.3512384',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Architecture and Implementation of Focus Crawler for Big Data Collection in Functional Sports Training',\n",
       "  'authors': \"['Weiwei Wang']\",\n",
       "  'date': 'September 2021',\n",
       "  'source': 'ICISCAE 2021: 2021 4th International Conference on Information Systems and Computer Aided Education',\n",
       "  'abstract': 'None',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3482632.3487513',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Research on the Evolution and Key Technologies of Mobile Communication Network Architecture in the Future',\n",
       "  'authors': \"['Li Pengcheng']\",\n",
       "  'date': 'May 2021',\n",
       "  'source': 'ICAIIS 2021: 2021 2nd International Conference on Artificial Intelligence and Information Systems',\n",
       "  'abstract': 'None',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3469213.3470341',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Flodam: cross-layer reliability analysis flow for complex hardware designs',\n",
       "  'authors': \"['Angeliki Kritikakou', 'Olivier Sentieys', 'Guillaume Hubert', 'Youri Helen', 'Jean-Francois Coulon', 'Patrice Deroux-Dauphin']\",\n",
       "  'date': 'March 2022',\n",
       "  'source': \"DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe\",\n",
       "  'abstract': 'Modern technologies make hardware designs more and more sensitive to radiation particles and related faults. As a result, analysing the behavior of a system under radiation-induced faults has become an essential part of the system design process. Existing approaches either focus on analysing the radiation impact at the lower hardware design layers, without further propagating any radiation-induced fault to the system execution, or analyse system reliability at higher hardware or application layers, based on fault models that are agnostic of the fabrication technology and the radiation environment. Flodam combines the benefits of existing approaches by providing a novel cross-layer reliability analysis from the semiconductor layer up to the application layer, able to quantify the risks of faults under a given context, taking into account the environmental conditions, the physical hardware design and the application under study.',\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3539845.3540039',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Snafu: an ultra-low-power, energy-minimal CGRA-generation framework and architecture',\n",
       "  'authors': \"['Graham Gobieski', 'Ahmet Oguz Atli', 'Kenneth Mai', 'Brandon Lucia', 'Nathan Beckmann']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': \"Ultra-low-power (ULP) devices are becoming pervasive, enabling many emerging sensing applications. Energy-efficiency is paramount in these applications, as efficiency determines device lifetime in battery-powered deployments and performance in energy-harvesting deployments. Unfortunately, existing designs fall short because ASICs' upfront costs are too high and prior ULP architectures are too inefficient or inflexible. We present SNAFU, the first framework to flexibly generate ULP coarse-grain reconfigurable arrays (CGRAs). SNAFU provides a standard interface for processing elements (PE), making it easy to integrate new types of PEs for new applications. Unlike prior high-performance, high-power CGRAs, SNAFU is designed from the ground up to minimize energy consumption while maximizing flexibility. SNAFU saves energy by configuring PEs and routers for a single operation to minimize switching activity; by minimizing buffering within the fabric; by implementing a statically routed, bufferless, multi-hop network; and by executing operations in-order to avoid expensive tag-token matching. We further present SNAFU-ARCH, a complete ULP system that integrates an instantiation of the SNAFU fabric alongside a scalar RISC-V core and memory. We implement SNAFU in RTL and evaluate it on an industrial sub-28 nm FinFET process across a suite of common sensing benchmarks. SNAFU-ARCH operates at <1mW, orders-of-magnitude less power than most prior CGRAs. SNAFU-ARCH uses 41% less energy and runs 4.4X faster than the prior state-of-the-art general-purpose ULP architecture. Moreover, we conduct three comprehensive case-studies to quantify the cost of programmability in SNAFU. We find that SNAFU-ARCH is close to ASIC designs built in the same technology, using just 2.6X more energy on average.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00084',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Design and Development Multi-Verification Protocols of A Digital Signed Document System Using Component-Based Architecture',\n",
       "  'authors': \"['Paniti Netinant', 'Siwaphat Boonbangyang', 'Meennapa Rukhiran']\",\n",
       "  'date': 'April 2021',\n",
       "  'source': \"ICEEG '21: Proceedings of the 5th International Conference on E-Commerce, E-Business and E-Government\",\n",
       "  'abstract': 'In the digital age, a transformation of a traditional business to an electronic organization becomes widely general. One of the resolutions is an adopted information technology to design and develop software supporting the whole process of organizations. End users enable to access and use digital services anywhere and anytime. Covid-19 has directed us to face a dramatic death of human life. Social distancing is an announcement of avoiding and touching unknown people, including colleagues, lecturers, and students. In this article, we aim to propose the design and development of a digitally signed document system. The system can handle documents of university processes to advance online forms significantly. By providing the digitally signed document system, a documented strategy can integrate users (e.g., general employees, executives, directors, and lecturers) to send electronic documents with proportional high security. The digital signature of user identification can perform via user identification and verification protocols in many methods. The design and development of software models are approached using rapid application development (RAD). RAD is the most compatible with project factors. The software functionaries of the digitally signed document system are decomposed into components for improving software quality. The interface components deliver indicators of a friendly environment of green information technology as well.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3466029.3466039',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'KallaxDB: A Table-less Hash-based Key-Value Store on Storage Hardware with Built-in Transparent Compression',\n",
       "  'authors': \"['Xubin Chen', 'Ning Zheng', 'Shukun Xu', 'Yifan Qiao', 'Yang Liu', 'Jiangpeng Li', 'Tong Zhang']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"DAMON '21: Proceedings of the 17th International Workshop on Data Management on New Hardware\",\n",
       "  'abstract': 'This paper studies the design of a key-value (KV) store that can take full advantage of modern storage hardware with built-in transparent compression capability. Many modern storage appliances/drives implement hardware-based data compression, transparent to OS and applications. Moreover, the growing deployment of hardware-based compression in Cloud infrastructure leads to the imminent arrival of Cloud-based storage hardware with built-in transparent compression. By decoupling the logical storage space utilization efficiency from the true physical storage usage, transparent compression allows data management software to purposely waste logical storage space in return for simpler data structures and algorithms, leading to lower implementation complexity and higher performance. This work proposes a table-less hash-based KV store, where the basic idea is to hash the key space directly onto the logical storage space without using a hash table at all. With a substantially simplified data structure, this approach is subject to significant logical storage space under-utilization, which can be seamlessly mitigated by storage hardware with transparent compression. This paper presents the basic KV store architecture, and develops mathematical formulations to assist its configuration and analysis. We implemented such a KV store KallaxDB and carried out experiments on a commercial SSD with built-in transparent compression. The results show that, while consuming very little memory resource, it compares favorably with the other modern KV stores in terms of throughput, latency, and CPU usage.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3465998.3466004',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'The Architecture and Security Design of Big Data Platform of the Physical Teaching Information System in Colleges and Universities',\n",
       "  'authors': \"['Caibao Han']\",\n",
       "  'date': 'September 2021',\n",
       "  'source': 'ICISCAE 2021: 2021 4th International Conference on Information Systems and Computer Aided Education',\n",
       "  'abstract': 'This paper constructs the logic structure of big data platform of physical teaching information system in Colleges and universities through Hadoop distributed system, which ensures the realization of management and monitoring function, data collection function, data storage and query function, data calculation function, security and management function. The security design is carried out from access control, data authorization and management, Hadoop security configuration to deal with the possible security problems of big data platform.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3482632.3483061',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'DEDD: Deep Encoder with Dual Decoder Architecture for Stability and Specificity Preserving Encoding and Translation of Embedding between Domains',\n",
       "  'authors': \"['Rajesh Ranjan', 'Debasmita Das', 'Ram Ganesh V', 'Yatin Katyal', 'Tanmoy Bhowmik']\",\n",
       "  'date': 'May 2021',\n",
       "  'source': \"CNIOT '21: Proceedings of the 2021 2nd International Conference on Computing, Networks and Internet of Things\",\n",
       "  'abstract': 'We propose a deep learning-based encoder with a dual decoder system to enrich the expressive power of embeddings pre-trained on two different corpora along with switching representation between domains. There are two scenarios: (a) Each of the corpora is pertaining to the different subject matter or topic of interests and (b) One corpus is a vast super-domain with generic and non-specific embeddings while the second one pertains to one specific sub-domain. In either case, the criterion for high-quality training would be to have enough common words between them. The mapping of contextual embeddings from both the corpus into the common latent space blends the semantic richness of both the corpus-specific learning while maintaining embedding stability. Furthermore, there is one dedicated decoder for either of the domains for generating the representation from common latent space. We evaluated our method for cross-learning between generalized GLOVE embedding and a very specialized skill-embedding developed by random-walk on a graph-based Skills Hierarchy. We demonstrate that our method preserves the stability of the generic embedding, the specificity of the skill domain as well as enriches the semantic representation of either domain through switching enabled by the encoder-to-duel-decoder path.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3468691.3468711',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Full-stack quantum computing systems in the NISQ era: algorithm-driven and hardware-aware compilation techniques',\n",
       "  'authors': \"['Medina Bandic', 'Sebastian Feld', 'Carmen G. Almudever']\",\n",
       "  'date': 'March 2022',\n",
       "  'source': \"DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe\",\n",
       "  'abstract': 'The progress in developing quantum hardware with functional quantum processors integrating tens of noisy qubits, together with the availability of near-term quantum algorithms has led to the release of the first quantum computers. These quantum computing systems already integrate different software and hardware components of the so-called \"full-stack\", bridging quantum applications to quantum devices. In this paper, we will provide an overview on current full-stack quantum computing systems. We will emphasize the need for tight co-design among adjacent layers as well as vertical cross-layer design to extract the most from noisy intermediate-scale quantum (NISQ) processors which are both error-prone and severely constrained in resources. As an example of co-design, we will focus on the development of hardware-aware and algorithm-driven compilation techniques.',\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3539845.3539847',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'SafeDM: a hardware diversity monitor for redundant execution on non-lockstepped cores',\n",
       "  'authors': \"['Francisco Bas', 'Pedro Benedicte', 'Sergi Alcaide', 'Guillem Cabo', 'Fabio Mazzocchetti', 'Jaume Abella']\",\n",
       "  'date': 'March 2022',\n",
       "  'source': \"DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe\",\n",
       "  'abstract': 'Computing systems in the safety domain, such as those in avionics or space, require specific safety measures related to the criticality of the deployment. A problem these systems face is that of transient failures in hardware. A solution commonly used to tackle potential failures is to introduce redundancy in these systems, for example 2 cores that execute the same program at the same time. However, redundancy does not solve all potential failures, such as Common Cause Failures (CCF), where a single fault affects both cores identically (e.g. a voltage droop). If both redundant cores have identical state when the fault occurs, then there may be a CCF since the fault can affect both cores in the same way. To avoid CCF it is critical to know that there is diversity in the execution amongst the redundant cores. In this paper we introduce SafeDM, a hardware Diversity Monitor that quantifies the diversity of each redundant processor to guarantee that CCF will not go unnoticed, and without needing to deploy lockstepped cores. SafeDM computes data and instruction diversity separately, using different techniques appropriate for each case. We integrate SafeDM in a RISC-V FPGA space MPSoC from Cobham Gaisler where SafeDM is proven effective with a large benchmark suite, incurring low area and power overheads. Overall, SafeDM is an effective hardware solution to quantify diversity in cores performing redundant execution.',\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3539845.3539933',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Odyssey: the impact of modern hardware on strongly-consistent replication protocols',\n",
       "  'authors': \"['Vasilis Gavrielatos', 'Antonios Katsarakis', 'Vijay Nagarajan']\",\n",
       "  'date': 'April 2021',\n",
       "  'source': \"EuroSys '21: Proceedings of the Sixteenth European Conference on Computer Systems\",\n",
       "  'abstract': \"Get/Put Key-Value Stores (KVSes) rely on replication protocols to enforce consistency and guarantee availability. Today's modern hardware, with manycore servers and RDMA-capable networks, challenges the conventional wisdom on protocol design. In this paper, we investigate the impact of modern hardware on the performance of strongly-consistent replication protocols. First, we create an informal taxonomy of replication protocols, based on which we carefully select 10 protocols for analysis. Secondly, we present Odyssey, a framework tailored towards protocol implementation for multi-threaded, RDMA-enabled, in-memory, replicated KVSes. We implement all 10 protocols over Odyssey, and perform the first apples-to-apples comparison of replication protocols over modern hardware. Our comparison characterizes the protocol design space, revealing the performance capabilities of different classes of protocols on modern hardware. Among other things, our results demonstrate that some of the protocols that were efficient in yesterday's hardware are not so today because they cannot take advantage of the abundant parallelism and fast networking present in modern hardware. Conversely, some protocols that were inefficient in yesterday's hardware are very attractive today. We distill our findings in a concise set of general guidelines and recommendations for protocol selection and design in the era of modern hardware.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3447786.3456240',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'ArchViMP – a Framework for Automatic Extraction of Concurrency-related Software Architectural Properties',\n",
       "  'authors': \"['Monireh Pourjafarian', 'Jasmin Jahic']\",\n",
       "  'date': 'August 2021',\n",
       "  'source': \"ICPP Workshops '21: 50th International Conference on Parallel Processing Workshop\",\n",
       "  'abstract': 'Concurrent multithreaded programs are more complex than sequential ones due to inter-dependencies of threads over shared memory. Because of these, software architects and developers quickly become overwhelmed when trying to design and manage concurrent software. Existing approaches that try to support architecture efforts in this domain rely on the visualization of concurrency-related properties of software to ease its understanding, but they fail because i) the abstractions they use do not capture information of architectural significance, and because ii) raw visualization of the interdependencies does not scale.  In this paper, we suggest a scalable solution that focuses on the abstraction of concurrency properties and their visualization using architectural views. Our framework for automatic extraction of concurrency-related architectural properties (ArchViMP) proposes i) a set of logical rules that abstract concurrency-related architecturally significant software properties and ii) a set of architectural views suitable for showing these concurrency properties.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3458744.3473349',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'On the Exploration and Optimization of High-Dimensional Architectural Design Space',\n",
       "  'authors': \"['Vincent Bode', 'Fariz Huseynli', 'Matrtin Schreiber', 'Carsten Trinitis', 'Martin Schulz']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"PERMAVOST '21: Proceedings of the 2021 on Performance EngineeRing, Modelling, Analysis, and VisualizatiOn STrategy\",\n",
       "  'abstract': \"The rise of heterogeneity in High-Performance Computing (HPC) architectures has caused a spike in the number of viable hardware solutions for different workloads. In order to take advantage of the increasing possibilities to influence how hardware can be tailored to boost software performance, collaboration between hardware manufacturers, computing centers and application developers must intensify with the goal of hardware-software co-design. To support the co-design effort, we need efficient methods to compare the performance of the many potential architectures running user-supplied applications. We present the High-Dimensional Exploration and Optimization Tool (HOT), a tool for visualizing and comparing software performance on CPU/GPU hybrid architectures. HOT is currently based on data acquired from Intel's Offload Advisor (I-OA) to model application performance, allowing us to extract performance predictions for existing/custom accelerator architectures. This eliminates the necessity of porting applications to different (parallel) programming models and also avoids benchmarking the application on target hardware. However, tools like I-OA allow users to tweak many hardware parameters, making it tedious to evaluate and compare results. HOT, therefore, focuses on visualizing these high-dimensional design spaces and assists the user in identifying suitable hardware configurations for given applications. Thus, users can gain rapid insights into how hardware/software influence each other in heterogeneous environments. We show the usage of HOT on several case studies. To determine the accuracy of collected performance data with I-OA, we analyze LULESH on different architectures. Next, we apply HOT to the synthetic benchmarks STREAM and 2MM to demonstrate the tool's visualization under these well-defined and known workloads, validating both the tool and its usage. Finally, we apply HOT to the real world code Gadget and the proxy application LULESH allowing us to easily identify their bottlenecks and optimize the choice of compute architecture for them.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3452412.3462754',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'LCCG: a locality-centric hardware accelerator for high throughput of concurrent graph processing',\n",
       "  'authors': \"['Jin Zhao', 'Yu Zhang', 'Xiaofei Liao', 'Ligang He', 'Bingsheng He', 'Hai Jin', 'Haikun Liu']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"SC '21: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis\",\n",
       "  'abstract': \"In modern data centers, massive concurrent graph processing jobs are being processed on large graphs. However, existing hardware/-software solutions suffer from irregular graph traversal and intense resource contention. In this paper, we propose LCCG, a <u>L</u>ocality-<u>C</u>entric programmable accelerator that augments the many-core processor for achieving higher throughput of <u>C</u>oncurrent <u>G</u>raph processing jobs. Specifically, we develop a novel topology-aware execution approach into the accelerator design to regularize the graph traversals for multiple jobs on-the-fly according to the graph topology, which is able to fully consolidate the graph data accesses from concurrent jobs. By reusing the same graph data among more jobs and coalescing the accesses of the vertices' states for these jobs, LCCG can improve the core utilization. We conduct extensive experiments on a simulated 64-core processor. The results show that LCCG improves the throughput of the cutting-edge software system by 11.3~23.9 times with only 0.5% additional area cost. Moreover, LCCG gains the speedups of 4.7~10.3, 5.5~13.2, and 3.8~8.4 times over state-of-the-art hardware graph processing accelerators (namely, HATS, Minnow, and PHI, respectively).\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3458817.3480854',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'TwinVisor: Hardware-isolated Confidential Virtual Machines for ARM',\n",
       "  'authors': \"['Dingji Li', 'Zeyu Mi', 'Yubin Xia', 'Binyu Zang', 'Haibo Chen', 'Haibing Guan']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"SOSP '21: Proceedings of the ACM SIGOPS 28th Symposium on Operating Systems Principles\",\n",
       "  'abstract': 'Confidential VM, which offers an isolated execution environment for cloud tenants with limited trust in the cloud provider, has recently been deployed in major clouds such as AWS and Azure. However, while ARM has become increasingly popular in cloud data centers, existing confidential VM designs mainly leverage specialized x86 hardware extensions (e.g., AMD SEV and Intel TDX) to isolate VMs upon a shared hypervisor. This paper proposes TwinVisor, the first system that enables the hardware-enforced isolation of confidential VMs on ARM platforms. TwinVisor takes advantage of the mature ARM TrustZone to run two isolated hypervisors, one in the secure world (called S-visor in this paper) and the other in the normal world (called N-visor), to support normal VMs and confidential VMs respectively. Instead of building a new S-visor from scratch, our design decouples protection from resource management, and reuses most functionalities of a full-fledged N-visor to minimize the size of S-visor. We have built two prototypes of TwinVisor: one on an official ARM simulator with S-EL2 enabled to validate functional correctness and the other on an ARM development board to evaluate performance. The S-visor comprises 5.8K LoCs while the N-visor introduces 906 LoC changes to KVM. According to our evaluation, TwinVisor can run unmodified VM images as confidential VMs while incurring less than 5% performance overhead for various real-world workloads on SMP VMs.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3477132.3483554',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Towards Extraction of Message-Based Communication in Mixed-Technology Architectures for Performance Model',\n",
       "  'authors': \"['Snigdha Singh', 'Yves Richard Kirschner', 'Anne Koziolek']\",\n",
       "  'date': 'April 2021',\n",
       "  'source': \"ICPE '21: Companion of the ACM/SPEC International Conference on Performance Engineering\",\n",
       "  'abstract': 'Software systems architected using multiple technologies are becoming popular. Many developers use these technologies as it offers high service quality which has often been optimized in terms of performance. In spite of the fact that performance is a key to the technology-mixed software applications, still there a little research on performance evaluation approaches explicitly considering the extraction of architecture for modelling and predicting performance. In this paper, we discuss the opportunities and challenges in applying existing architecture extraction approaches to support model-driven performance prediction for technology-mixed software. Further, we discuss how it can be extended to support a message-based system. We describe how various technologies deriving the architecture can be transformed to create the performance model. In order to realise the work, we used a case study from the energy system domain as an running example to support our arguments and observations throughout the paper.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3447545.3451201',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Tenet: A Neural Network Model Extraction Attack in Multi-core Architecture',\n",
       "  'authors': \"['Chengsi Gao', 'Bing Li', 'Ying Wang', 'Weiwei Chen', 'Lei Zhang']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"GLSVLSI '21: Proceedings of the 2021 on Great Lakes Symposium on VLSI\",\n",
       "  'abstract': 'As neural networks (NNs) are being widely deployed in many cloud-oriented systems for safety-critical tasks, the privacy and security of NNs become significant concerns to users in the cloud platform that shares the computation infrastructure such as memory resource. In this work, we observed that the memory timing channel in the shared memory of cloud multi-core architecture poses the risk of network model information leakage. Based on the observation, we propose a learning-based method to steal the model architecture of the NNs by exploiting the memory timing channel without any high-level privilege or physical access. We first trained an end-to-end measurement network offline to learn the relation between memory timing information and NNs model architecture. Then, we performed an online attack and reconstructed the target model using the prediction from the measurement network. We evaluated the proposed attack method on a multi-core architecture simulator. The experimental results show that our learning-based attack method can reconstruct the target model with high accuracy and improve the adversarial attack success rate by 42.4%.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3453688.3461512',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Towards trustworthy AI: safe-visor architecture for uncertified controllers in stochastic cyber-physical systems',\n",
       "  'authors': \"['Abolfazl Lavaei', 'Bingzhuo Zhong', 'Marco Caccamo', 'Majid Zamani']\",\n",
       "  'date': 'May 2021',\n",
       "  'source': \"CAADCPS '21: Proceedings of the Workshop on Computation-Aware Algorithmic Design for Cyber-Physical Systems\",\n",
       "  'abstract': 'Artificial intelligence-based (a.k.a. AI-based) controllers have received significant attentions in the past few years due to their broad applications in cyber-physical systems (CPSs) to accomplish complex control missions. However, guaranteeing safety and reliability of CPSs equipped with this kind of (uncertified) controllers is currently very challenging, which is of vital importance in many real-life safety-critical applications. To cope with this difficulty, we propose a Safe-visor architecture for sandboxing AI-based controllers in stochastic CPSs. The proposed framework contains (i) a history-based supervisor which checks inputs from the AI-based controller and makes compromise between functionality and safety of the system, and (ii) a safety advisor that provides fallback when the AI-based controller endangers the safety of the system. By employing this architecture, we provide formal probabilistic guarantees on the satisfaction of those classes of safety specifications which can be represented by the accepting languages of deterministic finite automata (DFA), while AI-based controllers can still be employed in the control loop even though they are not reliable.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3457335.3461705',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'FPRA: a fine-grained parallel RRAM architecture',\n",
       "  'authors': \"['Xiao Liu', 'Minxuan Zhou', 'Rachata Ausavarungnirun', 'Sean Eilert', 'Ameen Akel', 'Tajana Rosing', 'Vijaykrishnan Narayanan', 'Jishen Zhao']\",\n",
       "  'date': 'July 2021',\n",
       "  'source': \"ISLPED '21: Proceedings of the ACM/IEEE International Symposium on Low Power Electronics and Design\",\n",
       "  'abstract': 'Emerging resistive memory (RRAM) based crossbar array is a promising technology to accelerate neural network applications. RRAM-based CNN accelerators support a high-degree of intra-layer and inter-layer parallelism. The intra-layer parallelism duplicates kernels for each network layer while the inter-layer parallelism allows execution of each layer when a portion of input data is available. However, previously proposed RRAM-based accelerators do not leverage data sharing between duplicate kernels leading to significant idleness of crossbar arrays during inference. This shared data creates data dependencies that stall the processing of the next layer in the pipeline. To address these issues, we propose Fine-grained Parallel RRAM Architecture (FPRA), a novel architectural design, to improve parallelism for pipeline-enabled RRAM-based accelerators. FPRA addresses the data sharing issue with kernel batching and data sharing aware memory. Kernel batching rearranges the layout of the kernels and minimizes the data dependencies created by the input shared data. The data sharing aware memory uniformly buffers the input and output data for each layer, efficiently dispatching data to duplicate kernels while reducing the amount of data transferred between layers. We evaluate FPRA on eight popular image recognition CNN models with various configurations in a cycle-accurate simulator. We find that FPRA manages to achieve 2.0x average latency speedup, and 2.1x average throughput increase, as compared to the state-of-the-art RRAM-based accelerators.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISLPED52811.2021.9502474',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Taming the zoo: the unified GraphIt compiler framework for novel architectures',\n",
       "  'authors': \"['Ajay Brahmakshatriya', 'Emily Furst', 'Victor A. Ying', 'Claire Hsu', 'Changwan Hong', 'Max Ruttenberg', 'Yunming Zhang', 'Dai Cheol Jung', 'Dustin Richmond', 'Michael B. Taylor', 'Julian Shun', 'Mark Oskin', 'Daniel Sanchez', 'Saman Amarasinghe']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': 'We live in a new Cambrian Explosion of hardware devices. The end of conventional processor scaling has driven research and industry practice to explore a new generation of approaches. The old DNA of architecture design, including vectors, threads, shared or private memories, coherence or message passing, dataflow or von Neumann execution, are hybridized together in new and exciting ways. Each new architecture exposes a unique hardware-level API. Performance and energy efficiency are critically dependent on how well programs can use these APIs. One approach is to implement custom libraries for each new hardware architecture and application domain. A more scalable approach is to utilize a portable compiler infrastructure tailored to the application domain that makes it easy to generate efficient code for a diverse set of architectures with minimal porting effort. We propose the Unified GraphIt Compiler framework (UGC), which does exactly this for graph applications. UGC achieves portability with reasonable effort by decoupling the architecture-independent algorithm from the architecture-specific schedules and backends. We introduce a new domain-specific intermediate representation, GraphIR, that is key to this decoupling. GraphIR encodes high-level algorithm and optimization information needed for hardware-specific code generation, making it easy to develop different backends (GraphVMs) for diverse architectures, including CPUs, GPUs, and next-generation hardware such as Swarm and the HammerBlade manycore. We also build scheduling language extensions that make it easy to expose optimization decisions like load balancing strategies, blocking for locality, and other data structure choices. We evaluate UGC on five algorithms and 10 input graphs on these 4 distinct architectures and show that UGC enables implementing optimizations that can provide up to 53X speedup over programmer-generated straightforward implementations.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00041',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Technology-aware Router Architectures for On-Chip-Networks in Heterogeneous Technologies',\n",
       "  'authors': \"['Lennart Bamberg', 'Tushar Krishna', 'Jan Moritz Joseph']\",\n",
       "  'date': 'September 2021',\n",
       "  'source': \"NANOCOM '21: Proceedings of the Eight Annual ACM International Conference on Nanoscale Computing and Communication\",\n",
       "  'abstract': 'Heterogeneous 3D/2.5D stacking allows to tightly couple components that are ideally integrated into different technologies yielding advantages in nearly all design metrics. Massively parallel and scalable communication architectures between the components in such 3D ICs are commonly implemented through Networks-on-Chip (NoCs). This paper contributes a systematic approach to improve the efficiency of NoCs for these heterogeneous 3D ICs. The core idea is a heterogeneous co-design of the NoC routing algorithm and router micro-architecture. Thereby, the level of heterogeneity is derived from the physical implications of the different technologies. The proposed systematic approach enables a simultaneous improvement in the NoC power consumption, silicon footprint, and performance by 17 %, 45 %, and 52 %, respectively.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3477206.3477457',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Projecting Robot Navigation Paths: Hardware and Software for Projected AR',\n",
       "  'authors': \"['Zhao Han', 'Jenna Parrillo', 'Alexander Wilkinson', 'Holly A. Yanco', 'Tom Williams']\",\n",
       "  'date': 'March 2022',\n",
       "  'source': \"HRI '22: Proceedings of the 2022 ACM/IEEE International Conference on Human-Robot Interaction\",\n",
       "  'abstract': 'For mobile robots, mobile manipulators, and autonomous vehicles to safely navigate around populous places such as streets and warehouses, human observers must be able to understand their navigation intent. One way to enable such understanding is by visualizing this intent through projections onto the surrounding environment. But despite the demonstrated effectiveness of such projections, no open codebase with an integrated hardware setup exists. In this work, we detail the empirical evidence for the effectiveness of such directional projections, and share a robot-agnostic implementation of such projections, coded in C++ using the widely-used Robot Operating System (ROS) and rviz. Additionally, we demonstrate a hardware configuration for deploying this software, using a Fetch robot, and briefly summarize a full-scale user study that motivates this configuration. The code, configuration files (roslaunch and rviz files), and documentation are freely available on GitHub at https://github.com/umhan35/arrow_projection.',\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3523760.3523843',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Adversarial Attack Mitigation Approaches Using RRAM-Neuromorphic Architectures',\n",
       "  'authors': \"['Siddharth Barve', 'Sanket Shukla', 'Sai Manoj Pudukotai Dinakarrao', 'Rashmi Jha']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"GLSVLSI '21: Proceedings of the 2021 on Great Lakes Symposium on VLSI\",\n",
       "  'abstract': 'The rising trend and advancements in machine learning has resulted into its numerous applications in the field of computer vision, pattern recognition to providing security to hardware devices. Eventhough the proven achievements showcased by advancement in machine learning, one can exploit the vulnerabilities in those techniques by feeding adversaries. Adversarial samples are generated by well crafting and adding perturbations to the normal input samples. There exists majority of the software based adversarial attacks and defenses. In this paper, we demonstrate the effects of adversarial attacks on a reconfigurable RRAM-neuromorphic architecture with different learning algorithms and device characteristics. We also propose an integrated solution for mitigating the effects of the adversarial attack using the reconfigurable RRAM architecture.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3453688.3461757',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Online model swapping for architectural simulation',\n",
       "  'authors': \"['Patrick Lavin', 'Jeffrey Young', 'Richard Vuduc', 'Jonathan Beard']\",\n",
       "  'date': 'May 2021',\n",
       "  'source': \"CF '21: Proceedings of the 18th ACM International Conference on Computing Frontiers\",\n",
       "  'abstract': 'As systems and applications grow more complex, detailed computer architecture simulation takes an ever increasing amount of time. Longer simulation times result in slower design iterations which then force architects to use simpler models, such as spreadsheets, when they want to iterate quickly on a design. Simple models are not easy to work with though, as architects must rely on intuition to choose representative models, and the path from the simple models to a detailed hardware simulation is not always clear. In this work, we present a method of bridging the gap between simple and detailed simulation by monitoring simulation behavior online and automatically swapping out detailed models with simpler statistical approximations. We demonstrate the potential of our methodology by implementing it in the open-source simulator SVE-Cachesim to swap out the level one data cache (L1D) within a memory hierarchy. This proof of concept demonstrates that our technique can train simple models to match real program behavior in the L1D and can swap them in without destructive side-effects for the performance of downstream models. Our models introduce only 8% error in the overall cycle count, while being used for over 90% of the simulation and using models that require two to eight times less computation per cache access.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3457388.3458670',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'SwingNet: Ubiquitous Fine-Grained Swing Tracking Framework via Stochastic Neural Architecture Search and Adversarial Learning',\n",
       "  'authors': \"['Hong Jia', 'Jiawei Hu', 'Wen Hu']\",\n",
       "  'date': 'None',\n",
       "  'source': 'Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies',\n",
       "  'abstract': 'Sports analytics in the wild (i.e., ubiquitously) is a thriving industry. Swing tracking is a key feature in sports analytics. Therefore, a centimeter-level tracking resolution solution is required. Recent research has explored deep neural networks for sensor fusion to produce consistent swing-tracking performance. This is achieved by combining the advantages of two sensor modalities (IMUs and depth sensors) for golf swing tracking. Here, the IMUs are not affected by occlusion and can support high sampling rates. Meanwhile, depth sensors produce significantly more accurate motion measurements than those produced by IMUs. Nevertheless, this method can be further improved in terms of accuracy and lacking information for different domains (e.g., subjects, sports, and devices). Unfortunately, designing a deep neural network with good performance is time consuming and labor intensive, which is challenging when a network model is deployed to be used in new settings. To this end, we propose a network based on Neural Architecture Search (NAS), called SwingNet, which is a regression-based automatic generated deep neural network via stochastic neural network search. The proposed network aims to learn the swing tracking feature for better prediction automatically. Furthermore, SwingNet features a domain discriminator by using unsupervised learning and adversarial learning to ensure that it can be adaptive to unobserved domains. We implemented SwingNet prototypes with a smart wristband (IMU) and smartphone (depth sensor), which are ubiquitously available. They enable accurate sports analytics (e.g., coaching, tracking, analysis and assessment) in the wild.Our comprehensive experiment shows that SwingNet achieves less than 10 cm errors of swing tracking with a subject-independent model covering multiple sports (e.g., golf and tennis) and depth sensor hardware, which outperforms state-of-the-art approaches.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3478082',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': \"Architectural strategies for interoperability of software-intensive systems: practitioners' perspective\",\n",
       "  'authors': \"['Pedro Henrique Dias Valle', 'Lina Garcés', 'Elisa Yumi Nakagawa']\",\n",
       "  'date': 'March 2021',\n",
       "  'source': \"SAC '21: Proceedings of the 36th Annual ACM Symposium on Applied Computing\",\n",
       "  'abstract': \"Background: Currently, it is becoming increasingly common in the construction of more complex systems through the integration of existing and operational systems. To correctly construct these systems they must address interoperability requirements at different levels, i.e., technical, semantic, syntactic, and organizational. Several architectural strategies (i.e., architectural styles, patterns, and tactics) can be reused during such systems integration. However, there is a lack of evidence to orient which strategies and how they could be used during software integration practice. Objective: To investigate how architectural strategies have been used in practice to promote the interoperability of software-intensive systems. Method: We planned and executed an online survey with practitioners from different parts of the world, most of them with more than five years of experience in software integration. Results: We identified: (i) the main architectural strategies used in practice to promote different interoperability types in software-intensive systems; (ii) the difficulty level perceived by practitioners for using these strategies in real projects; (iii) the quality attributes more negatively impacted by these strategies; (iv) practitioners' suggestions to improve integration processes of software-intensive systems; (vi) common technologies used to integrate systems; (vii) and challenges perceived by practitioners during software-intensive systems integration. Conclusions: It is important to develop guidelines to assist practitioners in systematically selecting and applying the most suitable architectural strategies to their integration projects, analyzing the benefits and drawbacks of each strategy regarding other important quality attributes, such as security, reliability, and performance.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3412841.3442015',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Applying Architectural Patterns for Parallel Programming: Solving a Matrix Multiplication',\n",
       "  'authors': \"['Jorge L. Ortega-Arjona']\",\n",
       "  'date': 'July 2021',\n",
       "  'source': \"EuroPLoP'21: 26th European Conference on Pattern Languages of Programs\",\n",
       "  'abstract': 'The Architectural Patterns for Parallel Programming is a set of patterns along with a method for designing the coordination of parallel software systems. Their application takes as input: (a) the available parallel hardware platform, (b) the available parallel programming language, and (c) the analysis of the problem as an algorithm and data. This paper presents the application of the architectural patterns within the method for solving the Matrix Multiplication. The method takes information from the problem analysis, selects an architectural pattern for the coordination, and provides some elements about its implementation.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3489449.3490011',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Tangled: A Conventional Processor Integrating A Quantum-Inspired Coprocessor',\n",
       "  'authors': \"['Henry Dietz']\",\n",
       "  'date': 'August 2021',\n",
       "  'source': \"ICPP Workshops '21: 50th International Conference on Parallel Processing Workshop\",\n",
       "  'abstract': 'Quantum computers use quantum physics phenomena to create specialized hardware that can efficiently execute algorithms operating on entangled superposed data. That hardware must be attached to and controlled by a conventional host computer. However, it can be argued that the main benefit thus far has been from reformulating problems to make use of entangled superpositions rather than from use of exotic physics mechanisms to perform the computation – such reformulations often have produced more efficient algorithms for conventional computers. Parallel bit pattern computing does not simulate quantum computing, but provides a way to use non-quantum, bit-level, massively-parallel, SIMD hardware to efficiently execute a broad class of algorithms leveraging superposition and entanglement.  Just as quantum hardware needs a conventional host, so to does parallel bit pattern hardware. Thus, the current work presents Tangled: a simple proof-of-concept conventional processor design incorporating a tightly-coupled interface to an integrated parallel bit pattern co-processor (Qat). The feasibility of this type of interface between conventional and quantum-inspired computation was investigated by construction of an instruction set, building complete Verilog designs for pipelined implementations, and by observing the effectiveness of the interface in executing simple quantum-inspired algorithms involving operations on entangled, superposed, values.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3458744.3474044',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'DTQAtten: leveraging <u>d</u>ynamic <u>t</u>oken-based quantization for efficient <u>atten</u>tion architecture',\n",
       "  'authors': \"['Tao Yang', 'Dongyue Li', 'Zhuoran Song', 'Yilong Zhao', 'Fangxin Liu', 'Zongwu Wang', 'Zhezhi He', 'Li Jiang']\",\n",
       "  'date': 'March 2022',\n",
       "  'source': \"DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe\",\n",
       "  'abstract': 'Models based on the attention mechanism, i.e. transformers, have shown extraordinary performance in Natural Language Processing (NLP) tasks. However, their memory footprint, inference latency, and power consumption are still prohibitive for efficient inference at edge devices, even at data centers. To tackle this issue, we present an algorithm-architecture co-design with dynamic and mixed-precision quantization, DTQAtten. We present empirically that the tolerance to the noise varies from token to token in attention-based NLP models. This finding leads us to quantize different tokens with mixed levels of bits. Thus, we design a compression framework that (i) dynamically quantizes tokens while they are forwarded in the models and (ii) jointly determines the ratio of each precision. Moreover, due to the dynamic mixed-precision tokens caused by our framework, previous matrix-multiplication accelerators (e.g. systolic array) cannot effectively exploit the benefit of the compressed attention computation. We thus design our accelerator with the variable-speed systolic array (VSSA) and propose an effective optimization strategy to alleviate the pipeline-stall problem in VSSA without hardware overhead. We conduct experiments with existing attention-based NLP models, including BERT and GPT-2 on various language tasks. Our results show that DTQAtten outperforms the previous neural network accelerator Eyeriss by 13.12× in terms of speedup and 3.8× in terms of energy-saving. Compared with the state-of-the-art attention accelerator SpAtten, our DTQAtten achieves at least 2.65× speedup and 3.38× energy efficiency improvement.',\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3539845.3540016',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'BasicBlocker: ISA Redesign to Make Spectre-Immune CPUs Faster',\n",
       "  'authors': \"['Jan Philipp Thoma', 'Jakob Feldtkeller', 'Markus Krausz', 'Tim Güneysu', 'Daniel J. Bernstein']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"RAID '21: Proceedings of the 24th International Symposium on Research in Attacks, Intrusions and Defenses\",\n",
       "  'abstract': 'Recent research has revealed an ever-growing class of microarchitectural attacks that exploit speculative execution, a standard feature in modern processors. Proposed and deployed countermeasures involve a variety of compiler updates, firmware updates, and hardware updates. None of the deployed countermeasures have convincing security arguments, and many of them have already been broken.  The obvious way to simplify the analysis of speculative-execution attacks is to eliminate speculative execution. This is normally dismissed as being unacceptably expensive, but the underlying cost analyses consider only software written for current instruction-set architectures, so they do not rule out the possibility of a new instruction-set architecture providing acceptable performance without speculative execution. A new ISA requires compiler and hardware updates, but these are happening in any case.  This paper introduces BasicBlocker, a generic ISA modification that works for all common ISAs and that allows non-speculative CPUs to obtain most of the performance benefit that would have been provided by speculative execution. To demonstrate the feasibility of BasicBlocker, this paper defines a variant of the RISC-V ISA called BBRISC-V and provides a thorough evaluation on both a 5-stage in-order soft core and a superscalar out-of-order processor using an associated compiler and a variety of benchmarks.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3471621.3471857',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'BTMMU: an efficient and versatile cross-ISA memory virtualization',\n",
       "  'authors': \"['Kele Huang', 'Fuxin Zhang', 'Cun Li', 'Gen Niu', 'Junrong Wu', 'Tianyi Liu']\",\n",
       "  'date': 'April 2021',\n",
       "  'source': 'VEE 2021: Proceedings of the 17th ACM SIGPLAN/SIGOPS International Conference on Virtual Execution Environments',\n",
       "  'abstract': 'Full system dynamic binary translation (DBT) has many important applications, but it is typically much slower than the native host. One major overhead in full system DBT comes from cross-ISA memory virtualization, where multi-level memory address translation is needed to map guest virtual address into host physical address. Like the SoftMMU used in the popular open-source emulator QEMU, software-based memory virtualization solutions are not efficient. Meanwhile, mature techniques for same-ISA virtualization such as shadow page table or second level address translation are not directly applicable due to cross-ISA difficulties. Some previous studies achieved significant speedup by utilizing existing hardware (TLB or virtualization hardware) of the host. However, since the hardware is not designed with cross-ISA in mind, those solutions had some limitations that were hard to overcome. Most of them only supported guests with smaller virtual address space than the host. Some supported only guests with the same page size. And some did not support privileged memory accesses.  This paper proposes a new solution named BTMMU (Binary Translation Memory Management Unit). BTMMU composes of a low-cost hardware extension of host MMU, a kernel module and a patched QEMU version. BTMMU is able to solve most known limitations of previous hardware-assisted solutions and thus versatile enough for real deployments. Meanwhile, BTMMU achieves high efficiency by directly accessing guest address space, implementing shadow page table in kernel module, utilizing dedicated entrance for guest-related MMU exceptions and various software optimizations. Evaluations on SPEC CINT2006 benchmark suite and some real-world applications show that BTMMU achieves 1.40x and 1.36x speedup on IA32-to-MIPS64 and X86_64-to-MIPS64 configurations respectively when comparing with the base QEMU version. The result is compared to a representative previous work and shows its advantage.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3453933.3454015',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'TAMA: Turn-aware Mapping and Architecture – A Power-efficient Network-on-Chip Approach',\n",
       "  'authors': \"['Rashid Aligholipour', 'Mohammad Baharloo', 'Behnam Farzaneh', 'Meisam Abdollahi', 'Ahmad Khonsari']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Embedded Computing Systems',\n",
       "  'abstract': 'Nowadays, static power consumption in chip multiprocessor (CMP) is the most crucial concern of chip designers. Power-gating is an effective approach to mitigate static power consumption particularly in low utilization. Network-on-Chip (NoC) as the backbone of multi- and many-core chips has no exception. Previous state-of-the-art techniques in power-gating desire to decrease static power consumption alongside the lack of diminution in performance of NoC. However, maintaining the performance and utilization of the power-gating approach has not yet been addressed very well. In this article, we propose TAMA (Turn-Aware Mapping & Architecture) as an effective method to boost the performance of the TooT method that was only powering on a router during turning pass or packet injection. In other words, in the TooT method, straight and eject packets pass the router via a bypass route without powering on the router. By employing meta-heuristic approaches (Genetic and Ant Colony algorithms), we develop a specific application mapping that attempts to decrease the number of turns through interconnection networks. Accordingly, the average latency of packet transmission decreases due to fewer turns. Also, by powering on turn routers in advance with lightweight hardware, the latency of sending packets diminishes. The experimental results demonstrate that our proposed approach, i.e., TAMA achieves more than 13% reduction in packet latency of NoC in comparison with TooT. Besides the packet latency, the power consumption of TAMA is reduced by about 87% compared to the traditional approach.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3462700',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'RECOIN: A Low-Power Processing-in-ReRAM Architecture for Deformable Convolution',\n",
       "  'authors': \"['Cheng Chu', 'Fan Chen', 'Dawen Xu', 'Ying Wang']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"GLSVLSI '21: Proceedings of the 2021 on Great Lakes Symposium on VLSI\",\n",
       "  'abstract': 'The recent proposed Deformable Convolutional Networks (DCNs)greatly enhance the performance of conventional Convolutional Neural Networks (CNNs) on vision recognition tasks by allowing flexible input sampling during inference runtime. DCNs introduce an additional convolutional layer for adaptive sampling offset generation, followed by a bilinear interpolation (BLI) algorithm to integerize the generated non-integer offset values. Finally, a regular convolution is performed on the loaded input pixels. Compared with conventional CNNs, DCN demonstrated significantly increased computational complexity and irregular input-dependentmemory access patterns, making it a great challenge for deploying DCNs onto edge devices for real-time computer vision tasks. In this work, we propose RECOIN, a processing-in-memory (PIM) architecture, which supports DCN inference on resistive memory (ReRAM)crossbars, thus making the first DCN inference accelerator possible. We present a novel BLI processing engine that leverage both row-and column-oriented computation for in-situ BLI calculation. Amapping scheme and an address converter are particular designed to accommodate the intensive computation and irregular data access. We implement the DCN inference in a 4-stage pipeline and evaluate the effectiveness of RECOIN on six DCN models. Experimental results show RECOIN achieves respectively 225×and 17.4×improvement in energy efficiency compared to general-purpose CPU and GPU. Compared to two state-of-the-art ASIC accelerators, RECOIN achieve 26.8× and 20.4× speedup respectively.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3453688.3461480',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'AgEBO-tabular: joint neural architecture and hyperparameter search with autotuned data-parallel training for tabular data',\n",
       "  'authors': \"['Romain Égelé', 'Prasanna Balaprakash', 'Isabelle Guyon', 'Venkatram Vishwanath', 'Fangfang Xia', 'Rick Stevens', 'Zhengying Liu']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"SC '21: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis\",\n",
       "  'abstract': 'Developing high-performing predictive models for large tabular data sets is a challenging task. Neural architecture search (NAS) is an AutoML approach that generates and evaluates multiple neural networks with different architectures concurrently to automatically discover an high performing model. A key issue in NAS, particularly for large data sets, is the large computation time required to evaluate each generated architecture. While data-parallel training has the potential to address this issue, a straightforward approach can result in significant loss of accuracy. To that end, we develop AgEBO-Tabular, which combines Aging Evolution (AE) to search over neural architectures and asynchronous Bayesian optimization (BO) to search over hyperparameters to adapt data-parallel training. We evaluate the efficacy of our approach on two large predictive modeling tabular data sets from the Exascale Computing Project-CANcer Distributed Learning Environment (ECP-CANDLE).',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3458817.3476203',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'AgEBO-tabular: joint neural architecture and hyperparameter search with autotuned data-parallel training for tabular data',\n",
       "  'authors': \"['Romain Égelé', 'Prasanna Balaprakash', 'Isabelle Guyon', 'Venkatram Vishwanath', 'Fangfang Xia', 'Rick Stevens', 'Zhengying Liu']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"SC '21: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis\",\n",
       "  'abstract': 'Developing high-performing predictive models for large tabular data sets is a challenging task. Neural architecture search (NAS) is an AutoML approach that generates and evaluates multiple neural networks with different architectures concurrently to automatically discover an high performing model. A key issue in NAS, particularly for large data sets, is the large computation time required to evaluate each generated architecture. While data-parallel training has the potential to address this issue, a straightforward approach can result in significant loss of accuracy. To that end, we develop AgEBO-Tabular, which combines Aging Evolution (AE) to search over neural architectures and asynchronous Bayesian optimization (BO) to search over hyperparameters to adapt data-parallel training. We evaluate the efficacy of our approach on two large predictive modeling tabular data sets from the Exascale Computing Project-CANcer Distributed Learning Environment (ECP-CANDLE).',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3458817.3476203',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Gibbon: efficient co-exploration of NN model and processing-in-memory architecture',\n",
       "  'authors': \"['Hanbo Sun', 'Chenyu Wang', 'Zhenhua Zhu', 'Xuefei Ning', 'Guohao Dai', 'Huazhong Yang', 'Yu Wang']\",\n",
       "  'date': 'March 2022',\n",
       "  'source': \"DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe\",\n",
       "  'abstract': 'The memristor-based Processing-In-Memory (PIM) architectures have shown great potential to boost the computing energy efficiency of Neural Networks (NNs). Existing work concentrates on hardware architecture design and algorithm-hardware co-optimization, but neglects the non-negligible impact of the correlation between NN models and PIM architectures. To ensure high accuracy and energy efficiency, it is important to co-design the NN model and PIM architecture. However, on the one hand, the co-exploration space of NN model and PIM architecture is extremely tremendous, making searching for the optimal results difficult. On the other hand, during the co-exploration process, PIM simulators pose a heavy computational burden and runtime overhead for evaluation. To address these problems, in this paper, we propose an efficient co-exploration framework for the NN model and PIM architecture, named Gibbon. In Gibbon, we propose an evolutionary search algorithm with adaptive parameter priority, which focuses on subspace of high priority parameters and alleviates the problem of vast co-design space. Besides, we design a Recurrent Neural Network (RNN) based predictor for accuracy and hardware performances. It substitutes for a large part of the PIM simulator workload and reduces the long simulation time. Experimental results show that the proposed co-exploration framework can find better NN models and PIM architectures than existing studies in only seven GPU hours (8.4~41.3 × speedup). At the same time, Gibbon can improve the accuracy of co-design results by 10.7% and reduce the energy-delay-product by 6.48 × compared with existing work.',\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3539845.3540049',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'On resilience of security-oriented error detecting architectures against power attacks: a theoretical analysis',\n",
       "  'authors': \"['Osnat Keren', 'Ilia Polian']\",\n",
       "  'date': 'May 2021',\n",
       "  'source': \"CF '21: Proceedings of the 18th ACM International Conference on Computing Frontiers\",\n",
       "  'abstract': \"It has been previously shown that hardware implementation of fault attack countermeasures based on error-detecting codes (EDCs) can make the circuit more vulnerable to power analysis attacks. We revisit this finding and show that the hypothesis space can grow significantly when a state-of-the-art security-oriented robust EDC is properly crafted. We use the Roth-Karp decomposition as an analytical tool to prove that by a simple re-ordering of the EDC's bits, the number of extra bits needed to formulate the hypotheses becomes so large that power analysis (that tries to exploit additional information from the redundant bits) is rendered infeasible.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3457388.3458867',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Jupiter: a networked computing architecture',\n",
       "  'authors': \"['Pradipta Ghosh', 'Quynh Nguyen', 'Pranav K Sakulkar', 'Jason A. Tran', 'Aleksandra Knezevic', 'Jiatong Wang', 'Zhifeng Lin', 'Bhaskar Krishnamachari', 'Murali Annavaram', 'Salman Avestimehr']\",\n",
       "  'date': 'December 2021',\n",
       "  'source': \"UCC '21: Proceedings of the 14th IEEE/ACM International Conference on Utility and Cloud Computing Companion\",\n",
       "  'abstract': 'Modern latency-sensitive applications such as real-time multi-camera video analytics require networked computing to meet the time constraints. We present Jupiter, an open-source networked computing system that inputs a Directed Acyclic Graph (DAG)-based computational task graph to efficiently distribute the tasks among a set of networked compute nodes and orchestrates the execution thereafter. This Kubernetes container-orchestration-based system includes a range of profilers: network profilers, resource profilers, and execution time profilers; to support both centralized and decentralized scheduling algorithms. While centralized scheduling algorithms with global knowledge have been popular among the grid/cloud computing community, we argue that a distributed scheduling approach is better suited for networked computing due to lower communication and computation overhead in the face of network dynamics. We propose a new class of distributed scheduling algorithms called WAVE and show that despite using more localized knowledge, the WAVE algorithm can match the performance of a well-known centralized scheduling algorithm called Heterogeneous Earliest Finish Time (HEFT). To this, we present a set of real-world experiments on two separate testbeds: (1) a worldwide network of 90 cloud computers across eight cities and (2) a cluster of 30 Raspberry pi nodes.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3492323.3495630',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'η-LSTM: co-designing highly-efficient large LSTM training via exploiting memory-saving and architectural design opportunities',\n",
       "  'authors': \"['Xingyao Zhang', 'Haojun Xia', 'Donglin Zhuang', 'Hao Sun', 'Xin Fu', 'Michael B. Taylor', 'Shuaiwen Leon Song']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': 'Recently, the recurrent neural network, or its most popular type---the Long Short Term Memory (LSTM) network---has achieved great success in a broad spectrum of real-world application domains, such as autonomous driving, natural language processing, sentiment analysis, and epidemiology. Due to the complex features of the real-world tasks, current LSTM models become increasingly bigger and more complicated for enhancing the learning ability and prediction accuracy. However, through our in-depth characterization on the state-of-the-art general-purpose deep-learning accelerators, we observe that the LSTM training execution grows inefficient in terms of storage, performance, and energy consumption, under an increasing model size. With further algorithmic and architectural analysis, we identify the root cause for large LSTM training inefficiency: massive intermediate variables. To enable a highly-efficient LSTM training solution for the ever-growing model size, we exploit some unique memory-saving and performance improvement opportunities from the LSTM training procedure, and leverage them to propose the first cross-stack training solution, η-LSTM, for large LSTM models. η-LSTM comprises both software-level and hardware-level innovations that effectively lower the memory footprint upper-bound and excessive data movements during large LSTM training, while also drastically improving training performance and energy efficiency. Experimental results on six real-world large LSTM training benchmarks demonstrate that η-LSTM reduces the required memory footprint by an average of 57.5% (up to 75.8%) and brings down the data movements for weight matrices, activation data, and intermediate variables by 40.9%, 32.9%, and 80.0%, respectively. Furthermore, it outperforms the state-of-the-art GPU implementation for LSTM training by an average of 3.99× (up to 5.73×) on performance and 2.75× (up to 4.25x ) on energy. We hope this work can shed some light on how to design high logic utilization for future NPUs.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00051',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Toward systematic architectural design of near-term trapped ion quantum computers',\n",
       "  'authors': \"['Prakash Murali', 'Dripto M. Debroy', 'Kenneth R. Brown', 'Margaret Martonosi']\",\n",
       "  'date': 'March 2022',\n",
       "  'source': 'Communications of the ACM',\n",
       "  'abstract': \"Trapped ions (TIs) are a leading candidate for building Noisy Intermediate-Scale Quantum (NISQ) hardware. TI qubits have fundamental advantages over other technologies, featuring high qubit quality, coherence time, and qubit connectivity. However, current TI systems are small in size and typically use a single trap architecture, which has fundamental scalability limitations. To progress toward the next major milestone of 50--100 qubit TI devices, a modular architecture termed the Quantum Charge Coupled Device (QCCD) has been proposed. In a QCCD-based TI device, small traps are connected through ion shuttling. While the basic hardware components for such devices have been demonstrated, building a 50--100 qubit system is challenging because of a wide range of design possibilities for trap sizing, communication topology, and gate implementations and the need to match diverse application resource requirements.Toward realizing QCCD-based TI systems with 50--100 qubits, we perform an extensive application-driven architectural study evaluating the key design choices of trap sizing, communication topology, and operation implementation methods. To enable our study, we built a design toolflow, which takes a QCCD architecture's parameters as input, along with a set of applications and realistic hardware performance models. Our toolflow maps the applications onto the target device and simulates their execution to compute metrics such as application run time, reliability, and device noise rates. Using six applications and several hardware design points, we show that trap sizing and communication topology choices can impact application reliability by up to three orders of magnitude. Microarchitectural gate implementation choices influence reliability by another order of magnitude. From these studies, we provide concrete recommendations to tune these choices to achieve highly reliable and performant application executions. With industry and academic efforts underway to build TI devices with 50-100 qubits, our insights have the potential to influence QC hardware in the near future and accelerate the progress toward practical QC systems.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3511064',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Voltage-Based Covert Channels Using FPGAs',\n",
       "  'authors': \"['Dennis R. E. Gnad', 'Cong Dang Khoa Nguyen', 'Syed Hashim Gillani', 'Mehdi B. Tahoori']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Design Automation of Electronic Systems',\n",
       "  'abstract': 'Field Programmable Gate Arrays (FPGAs) are increasingly used in cloud applications and being integrated into Systems-on-Chip. For these systems, various side-channel attacks on cryptographic implementations have been reported, motivating one to apply proper countermeasures. Beyond cryptographic implementations, maliciously introduced covert channel receivers and transmitters can allow one to exfiltrate other secret information from the FPGA. In this article, we present a fast covert channel on FPGAs, which exploits the on-chip power distribution network. This can be achieved without any logical connection between the transmitter and receiver blocks. Compared to a recently published covert channel with an estimated 4.8\\xa0Mbit/s transmission speed, we show 8\\xa0Mbit/s transmission and reduced errors from around 3% to less than 0.003%. Furthermore, we demonstrate proper transmissions of word-size messages and test the channel in the presence of noise generated from other residing tenants’ modules in the FPGA. When we place and operate other co-tenant modules that require 85% of the total FPGA area, the error rate increases to 0.02%, depending on the platform and setup. This error rate is still reasonably low for a covert channel. Overall, the transmitter and receiver work with less than 3–5% FPGA LUT resources together. We also show the feasibility of other types of covert channel transmitters, in the form of synchronous circuits within the FPGA.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3460229',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Towards Trusted IoT Sensing Systems: Implementing PUF as Secure Key Generator for Root of Trust and Message Authentication Code',\n",
       "  'authors': \"['Kota Yoshida', 'Kuniyasu Suzaki', 'Takeshi Fujino']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"HASP '21: Proceedings of the 10th International Workshop on Hardware and Architectural Support for Security and Privacy\",\n",
       "  'abstract': 'Trustworthy sensor data is important for IoT sensing systems. As such, these systems need to guarantee that the sensor data is acquired by the correct device and has not been tampered with. However, IoT sensing systems can be quite complex and are often composed of multiple components, i.e., a main device and subordinate sensors. The main device is responsible for gathering and processing the data from the subordinate sensor and reports the result to a server. In order to guarantee data correctness, we introduce two types of physically unclonable function (PUF): one for the main device and one for the subordinate sensor. The main device has a trusted execution environment (TEE) for critical processing, and the correctness of the TEE is guaranteed by remote attestation based on a PUF. The subordinate sensor sends the sensor data to the main device with a message authentication code (MAC) based on a PUF. We implemented a trusted IoT sensing system using a RISC-V Keystone with a PRINCE Glitch PUF for the main device and a Raspberry Pi that simulates a CMOS image sensor PUF for the subordinate sensor.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3505253.3505258',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'SPARROW: a low-cost hardware/software co-designed SIMD microarchitecture for AI operations in space processors',\n",
       "  'authors': \"['Marc Solé Bonet', 'Leonidas Kosmidis']\",\n",
       "  'date': 'March 2022',\n",
       "  'source': \"DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe\",\n",
       "  'abstract': \"Recently there is an increasing interest in the use of artificial intelligence for on-board processing as indicated by the latest space missions, which cannot be satisfied by existing low-performance space-qualified processors. Although COTS AI accelerators can provide the required performance, they are not designed to meet space requirements. In this work, we co-design a low-cost SIMD micro-architecture integrated in a space qualified processor, which can significantly increase its performance. Our solution has no impact on the processor's 100 MHz frequency and consumes minimal area thanks to its innovative design compared to conventional vector micro-architectures. For the minimum configuration of our baseline space processor, our results indicate a performance boost of up to 9.3× for commonly used AI-related and image processing algorithms and 5.5× faster for a complex, space-relevant inference application with just 30% area increase.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3539845.3540112',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'No-FAT: architectural support for low overhead memory safety checks',\n",
       "  'authors': \"['Mohamed Tarek Ibn Ziad', 'Miguel A. Arroyo', 'Evgeny Manzhosov', 'Ryan Piersma', 'Simha Sethumadhavan']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': \"Memory safety continues to be a significant software reliability and security problem, and low overhead and low complexity hardware solutions have eluded computer designers. In this paper, we explore a pathway to deployable memory safety defenses. Our technique builds on a recent trend in software: the usage of binning memory allocators. We observe that if memory allocation sizes (e.g., malloc sizes) are made an architectural feature, then it is possible to overcome many of the thorny issues with traditional approaches to memory safety such as compatibility with unsecured software and significant performance degradation. We show that our architecture, No-FAT, incurs an overhead of 8% on SPEC CPU2017 benchmarks, and our VLSI measurements show low power and area overheads. Finally, as No-FAT's hardware is aware of the memory allocation sizes, it effectively mitigates certain speculative attacks (e.g., Spectre-V1) with no additional cost. When our solution is used for pre-deployment fuzz testing it can improve fuzz testing bandwidth by an order of magnitude compared to state-of-the-art approaches.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00076',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'How to exploit sparsity in RNNs on event-driven architectures',\n",
       "  'authors': \"['Jarno Brils', 'Luc Waeijen', 'Arash Pourtaherian']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"SCOPES '21: Proceedings of the 24th International Workshop on Software and Compilers for Embedded Systems\",\n",
       "  'abstract': 'Event-driven architectures have been shown to provide low-power, low-latency artificial neural network (ANN) inference. This is especially beneficial on Edge devices, particularly when combined with sparse execution. Recurrent neural networks (RNNs) are ANNs that emulate memory. Their recurrent connection enables the reuse of previous output for the generation of new output. However, when trying to use RNNs in a sparse context on event-driven architectures, novel challenges in synchronization and the usage of sparse data are encountered. In this work, these challenges are systematically analyzed, and mechanisms to overcome them are proposed. Experimental results of a monocular depth estimation use case on the NeuronFlow architecture show that sparsity in RNNs can be exploited effectively on event-driven architectures.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3493229.3493302',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'GraphAttack: Optimizing Data Supply for Graph Applications on In-Order Multicore Architectures',\n",
       "  'authors': \"['Aninda Manocha', 'Tyler Sorensen', 'Esin Tureci', 'Opeoluwa Matthews', 'Juan L. Aragón', 'Margaret Martonosi']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Architecture and Code Optimization',\n",
       "  'abstract': 'Graph structures are a natural representation of important and pervasive data. While graph applications have significant parallelism, their characteristic pointer indirect loads to neighbor data hinder scalability to large datasets on multicore systems. A scalable and efficient system must tolerate latency while leveraging data parallelism across millions of vertices. Modern Out-of-Order (OoO) cores inherently tolerate a fraction of long latencies, but become clogged when running severely memory-bound applications. Combined with large power/area footprints, this limits their parallel scaling potential and, consequently, the gains that existing software frameworks can achieve. Conversely, accelerator and memory hierarchy designs provide performant hardware specializations, but cannot support diverse application demands.To address these shortcomings, we present GraphAttack, a hardware-software data supply approach that accelerates graph applications on in-order multicore architectures. GraphAttack proposes compiler passes to (1)\\xa0identify idiomatic long-latency loads and (2)\\xa0slice programs along these loads into data Producer/ Consumer threads to map onto pairs of parallel cores. Each pair shares a communication queue; the Producer asynchronously issues long-latency loads, whose results are buffered in the queue and used by the Consumer. This scheme drastically increases memory-level parallelism (MLP) to mitigate latency bottlenecks. In equal-area comparisons, GraphAttack outperforms OoO cores, do-all parallelism, prefetching, and prior decoupling approaches, achieving a 2.87× speedup and 8.61× gain in energy efficiency across a range of graph applications. These improvements scale; GraphAttack achieves a 3× speedup over 64 parallel cores. Lastly, it has pragmatic design principles; it enhances in-order architectures that are gaining increasing open-source support.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3469846',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'DeepDive: An Integrative Algorithm/Architecture Co-Design for Deep Separable Convolutional Neural Networks',\n",
       "  'authors': \"['Mohammadreza Baharani', 'Ushma Sunil', 'Kaustubh Manohar', 'Steven Furgurson', 'Hamed Tabkhi']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"GLSVLSI '21: Proceedings of the 2021 on Great Lakes Symposium on VLSI\",\n",
       "  'abstract': \"Deep Separable Convolutional Neural Network (DSCNN) has become the emerging paradigm by offering modular networks with structural sparsity to achieve higher accuracy with relatively lower operations and parameters. However, there is a lack of customized architectures that can provide flexible solutions that fit the sparsity of the DSCNNs. This paper introduces DeepDive, a fully-functional vertical co-design framework, for power-efficient implementation of DSCNNs on edge FPGAs. DeepDive's architecture supports crucial heterogeneous Compute Units (CUs) to fully support DSCNNs with various convolutional operators interconnected with structural sparsity. It offers FPGA-aware training and online quantization combined with modular synthesizable C++ CUs, customized for DSCNNs. The execution results on Xilinx's ZCU102 FPGA board demonstrate 47.4 and 233.3 FPS/Watt for MobileNet-V2 and a compact version of EfficientNet, respectively, as two state-of-the-art depthwise separable CNNs. These comparisons showcase how DeepDive improves FPS/Watt by 2.2× and 1.51× over Jetson Nano high and low power modes, respectively. It also enhances FPS/Watt by about 2.27× and 37.25× over two other FPGA implementations.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3453688.3461485',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A uniform latency model for DNN accelerators with diverse architectures and dataflows',\n",
       "  'authors': \"['Linyan Mei', 'Huichu Liu', 'Tony Wu', 'H. Ekin Sumbul', 'Marian Verhelst', 'Edith Beigne']\",\n",
       "  'date': 'March 2022',\n",
       "  'source': \"DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe\",\n",
       "  'abstract': \"In the early design phase of a Deep Neural Network (DNN) acceleration system, fast energy and latency estimation are important to evaluate the optimality of different design candidates on algorithm, hardware, and algorithm-to-hardware mapping, given the gigantic design space. This work proposes a uniform intra-layer analytical latency model for DNN accelerators that can be used to evaluate diverse architectures and dataflows. It employs a 3-step approach to systematically estimate the latency breakdown of different system components, capture the operation state of each memory component, and identify stall-induced performance bottlenecks. To achieve high accuracy, different memory attributes, operands' memory sharing scenarios, as well as dataflow implications have been taken into account. Validation against an in-house taped-out accelerator across various DNN layers has shown an average latency model accuracy of 94.3%. To showcase the capability of the proposed model, we carry out 3 case studies to assess respectively the impact of mapping, workloads, and diverse hardware architectures on latency, driving design insights for algorithm-hardware-mapping co-optimization.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3539845.3539904',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'MG-Join: A Scalable Join for Massively Parallel Multi-GPU Architectures',\n",
       "  'authors': \"['Johns Paul', 'Shengliang Lu', 'Bingsheng He', 'Chiew Tong Lau']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data\",\n",
       "  'abstract': 'The recent scale-up of GPU hardware through the integration of multiple GPUs into a single machine and the introduction of higher bandwidth interconnects like NVLink 2.0 has enabled new opportunities of relational query processing on multiple GPUs. However, due to the unique characteristics of GPUs and the interconnects, existing hash join implementations spend up to 66% of their execution time moving the data between the GPUs and achieve lower than 50% utilization of the newer high bandwidth interconnects. This leads to extremely poor scalablity of hash join performance on multiple GPUs, which can be slower than the performance on a single GPU. In this paper, we propose MG-Join, a scalable partitioned hash join implementation on multiple GPUs of a single machine. In order to effectively improve the bandwidth utilization, we develop a novel multi-hop routing for cross-GPU communication that adaptively chooses the efficient route for each data flow to minimize congestion. Our experiments on the DGX-1 machine show that MG-Join helps significantly reduce the communication overhead and achieves up to 97% utilization of the bisection bandwidth of the interconnects, resulting in significantly better scalability. Overall, MG-Join outperforms the state-of-the-art hash join implementations by up to 2.5x. MG-Join further helps improve the overall performance of TPC-H queries by up to 4.5x over multi-GPU version of an open-source commercial GPU database Omnisci.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3448016.3457254',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Fifer: Practical Acceleration of Irregular Applications on Reconfigurable Architectures',\n",
       "  'authors': \"['Quan M. Nguyen', 'Daniel Sanchez']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"MICRO '21: MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture\",\n",
       "  'abstract': 'Coarse-grain reconfigurable arrays (CGRAs) can achieve much higher performance and efficiency than general-purpose cores, approaching the performance of a specialized design while retaining programmability. Unfortunately, CGRAs have so far only been effective on applications with regular compute patterns. However, many important workloads like graph analytics, sparse linear algebra, and databases, are irregular applications with unpredictable access patterns and control flow. Since CGRAs map computation statically to a spatial fabric of functional units, irregular memory accesses and control flow cause frequent stalls and load imbalance.  We present Fifer, an architecture and compilation technique that makes irregular applications efficient on CGRAs. Fifer first decouples irregular applications into a feed-forward network of pipeline stages. Each resulting stage is regular and can efficiently use the CGRA fabric. However, irregularity causes stages to have widely varying loads, resulting in high load imbalance if they execute spatially in a conventional CGRA. Fifer solves this by introducing dynamic temporal pipelining: it time-multiplexes multiple stages onto the same CGRA, and dynamically schedules stages to avoid load imbalance. Fifer makes time-multiplexing fast and cheap to quickly respond to load imbalance while retaining the efficiency and simplicity of a CGRA design. We show that Fifer improves performance by gmean 2.8 × (and up to 5.5 ×) over a conventional CGRA architecture (and by gmean 17 × over an out-of-order multicore) on a variety of challenging irregular applications.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3466752.3480048',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Albireo: energy-efficient acceleration of convolutional neural networks via silicon photonics',\n",
       "  'authors': \"['Kyle Shiflett', 'Avinash Karanth', 'Razvan Bunescu', 'Ahmed Louri']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': 'With the end of Dennard scaling, highly-parallel and specialized hardware accelerators have been proposed to improve the throughput and energy-efficiency of deep neural network (DNN) models for various applications. However, collective data movement primitives such as multicast and broadcast that are required for multiply-and-accumulate (MAC) computation in DNN models are expensive, and require excessive energy and latency when implemented with electrical networks. This consequently limits the scalability and performance of electronic hardware accelerators. Emerging technology such as silicon photonics can inherently provide efficient implementation of multicast and broadcast operations, making photonics more amenable to exploit parallelism within DNN models. Moreover, when coupled with other unique features such as low energy consumption, high channel capacity with wavelength-division multiplexing (WDM), and high speed, silicon photonics could potentially provide a viable technology for scaling DNN acceleration. In this paper, we propose Albireo, an analog photonic architecture for scaling DNN acceleration. By characterizing photonic devices such as microring resonators (MRRs) and Mach-Zehnder modulators (MZM) using photonic simulators, we develop realistic device models and outline their capability for system level acceleration. Using the device models, we develop an efficient broadcast combined with multicast data distribution by leveraging parameter sharing through unique WDM dot product processing. We evaluate the energy and throughput performance of Albireo on DNN models such as ResNet18, MobileNet and VGG16. When compared to current state-of-the-art electronic accelerators, Albireo increases throughput by 110 X, and improves energy-delay product (EDP) by an average of 74 X with current photonic devices. Furthermore, by considering moderate and aggressive photonic scaling, the proposed Albireo design shows that EDP can be reduced by at least 229 X.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00072',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Metaprogramming with combinators',\n",
       "  'authors': \"['Mahshid Shahmohammadian', 'Geoffrey Mainland']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': 'GPCE 2021: Proceedings of the 20th ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences',\n",
       "  'abstract': 'There are a wide array of methods for writing code generators. We advocate for a point in the design space, which we call metaprogramming with combinators, where programmers use (and write) combinator libraries that directly manipulate object language terms. The key language feature that makes this style of programming palatable is quasiquotation. Our approach leverages quasiquotation and other host language features to provide what is essentially a rich, well-typed macro language. Unlike other approaches, metaprogramming with combinators allows full control over generated code, thereby also providing full control over performance and resource usage. This control does not require sacrificing the ability to write high-level abstractions. We demonstrate metaprogramming with combinators through several code generators written in Haskell that produce VHDL targeted to Xilinx FPGAs.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3486609.3487198',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'CoPIM: a concurrency-aware PIM workload offloading architecture for graph applications',\n",
       "  'authors': \"['Liang Yan', 'Mingzhe Zhang', 'Rujia Wang', 'Xiaoming Chen', 'Xingqi Zou', 'Xiaoyang Lu', 'Yinhe Han', 'Xian-He Sun']\",\n",
       "  'date': 'July 2021',\n",
       "  'source': \"ISLPED '21: Proceedings of the ACM/IEEE International Symposium on Low Power Electronics and Design\",\n",
       "  'abstract': 'Processing-in-Memory (PIM) is considered a promising solution to improve the performance of graph-computing applications by minimizing the data movement between the host and memory. Which workload to offload and how to offload it to PIM logic determine whether the PIM architecture is well utilized. Offloading too much or too little workload from the host processor to the PIM side could hurt overall performance. On the other hand, the offloading granularity needs to be representative without losing generality. In this paper, we present CoPIM, a novel PIM workload offloading architecture that can dynamically determine which portion of the graph workload can benefit more from PIM-side computation. CoPIM focuses on the loop code blocks of graph applications and evaluates the necessity of offloading based on a concurrent memory access model. We also provide detailed architectural designs to support the offloading. In this way, CoPIM reduces the size of offloading instructions and also improves the overall performance with less energy consumption. The experimental results show that compared with other state-of-the-art PIM workload offloading frameworks, CoPIM achieves a speedup by the geometric mean of 19.5% and 11.4% than PEI and GraphPIM, respectively. On the other hand, CoPIM also reduces the un-core energy consumption by 6.8% and 6.5% on average over PEI and GraphPIM, respectively.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISLPED52811.2021.9502483',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Scaling of multi-core quantum architectures: a communications-aware structured gap analysis',\n",
       "  'authors': \"['Santiago Rodrigo', 'Medina Bandic', 'Sergi Abadal', 'Hans van Someren', 'Eduard Alarcón', 'Carmen G. Almudéver']\",\n",
       "  'date': 'May 2021',\n",
       "  'source': \"CF '21: Proceedings of the 18th ACM International Conference on Computing Frontiers\",\n",
       "  'abstract': 'In the quest of large-scale quantum computers, multi-core distributed architectures are considered a compelling alternative to be explored. A crucial aspect in such approach is the stringent demand on communication among cores when qubits need to interact, which conditions the scalability potential of these architectures. In this work, we address the question of how the cost of the communication among cores impacts on the viability of the quantum multi-core approach. Methodologically, we consider a design space in which architectural variables (number of cores, number of qubits per core), application variables for several quantum benchmarks (number of qubits, number of gates, percentage of two-qubit gates) and inter-core communication latency are swept along with the definition of a figure of merit. This approach yields both a qualitative understanding of trends in the design space and companion dimensioning guidelines for the architecture, including optimal points, as well as quantitative answers to the question of beyond which communication performance levels the multi-core architecture pays off. Our results allow to determine the thresholds for inter-core communication latency in order for multi-core architectures to outperform single-core quantum processors.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3457388.3458674',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'multiPULPly: A Multiplication Engine for Accelerating Neural Networks on Ultra-low-power Architectures',\n",
       "  'authors': \"['Adi Eliahu', 'Ronny Ronen', 'Pierre-Emmanuel Gaillardon', 'Shahar Kvatinsky']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Journal on Emerging Technologies in Computing Systems',\n",
       "  'abstract': 'Computationally intensive neural network applications often need to run on resource-limited low-power devices. Numerous hardware accelerators have been developed to speed up the performance of neural network applications and reduce power consumption; however, most focus on data centers and full-fledged systems. Acceleration in ultra-low-power systems has been only partially addressed. In this article, we present multiPULPly, an accelerator that integrates memristive technologies within standard low-power CMOS technology, to accelerate multiplication in neural network inference on ultra-low-power systems. This accelerator was designated for PULP, an open-source microcontroller system that uses low-power RISC-V processors. Memristors were integrated into the accelerator to enable power consumption only when the memory is active, to continue the task with no context-restoring overhead, and to enable highly parallel analog multiplication. To reduce the energy consumption, we propose novel dataflows that handle common multiplication scenarios and are tailored for our architecture. The accelerator was tested on FPGA and achieved a peak energy efficiency of 19.5 TOPS/W, outperforming state-of-the-art accelerators by 1.5× to 4.5×.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3432815',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Eva-CAM: a circuit/architecture-level evaluation tool for general content addressable memories',\n",
       "  'authors': \"['Liu Liu', 'Mohammad Mehdi Sharifi', 'Ramin Rajaei', 'Arman Kazemi', 'Kai Ni', 'Xunzhao Yin', 'Michael Niemier', 'Xiaobo Sharon Hu']\",\n",
       "  'date': 'March 2022',\n",
       "  'source': \"DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe\",\n",
       "  'abstract': 'Content addressable memories (CAMs), a special-purpose in-memory computing (IMC) unit, support parallel searches directly in memory. There are growing interests in CAMs for data-intensive applications such as machine learning and bioinformatics. The design space for CAMs is rapidly expanding. In addition to traditional ternary CAMs (TCAMs), analog CAM (ACAM) and multi-bit CAM (MCAM) designs based on various non-volatile memory (NVM) devices have been recently introduced and may offer higher density, better energy efficiency, and non-volatility. Furthermore, aside from the widely-used exact match based search, CAM-based approximate matches have been proposed to further extend the utility of CAMs to new application spaces. For this memory architecture, evaluating different CAM design options for a given application is becoming more challenging. This paper presents Eva-CAM, a circuit/architecture-level modeling and evaluation tool for CAMs. Eva-CAM supports TCAM, ACAM, and MCAM designs implemented in non-volatile memories, for both exact and approximate match types. It also allows for the exploration of CAM array structures and sensing circuits. Eva-CAM has been validated with HSPICE simulation results and chip measurements. A comprehensive case study is described for FeFET CAM design space exploration.',\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3539845.3540123',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'BOSS: bandwidth-optimized search accelerator for storage-class memory',\n",
       "  'authors': \"['Jun Heo', 'Seung Yul Lee', 'Sunhong Min', 'Yeonhong Park', 'Sung Jun Jung', 'Tae Jun Ham', 'Jae W. Lee']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': 'Search is one of the most popular and important web services. The inverted index is the standard data structure adopted by most full-text search engines. Recently, custom hardware accelerators for inverted index search have emerged to demonstrate much higher throughput than the conventional CPU or GPU. However, less attention has been paid to addressing the memory capacity pressure with inverted index. The conventional DDRx DRAM memory system significantly increases the system cost to make a terabyte-scale main memory. Instead, a shared memory pool composed of storage-class memory (SCM) devices is a promising alternative for scaling memory capacity at a much lower cost. However, this SCM-based pooled memory poses new challenges caused by the limited bandwidth of both SCM devices and the shared interconnect to the host CPU. Thus, we propose BOSS, the first near-data processing (NDP) architecture for inverted index search on SCM-based pooled memory, which maintains high throughput of query processing in this bandwidth-constrained environment. BOSS mitigates the impact of low bandwidth of SCM devices by employing early-termination search algorithms, reducing the footprint of intermediate data, and introducing a programmable decompression module that can select the best compression scheme for a given inverted index. Furthermore, BOSS includes a top-k selection module in hardware to substantially reduce the host-accelerator bandwidth consumption. Compared to Apache Lucene, a production-grade search engine library, running on 8 CPU cores, BOSS achieves a geomean speedup of 8.1x on various complex query types, while reducing the average energy consumption by 189x.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00030',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'ClickTrain: efficient and accurate end-to-end deep learning training via fine-grained architecture-preserving pruning',\n",
       "  'authors': \"['Chengming Zhang', 'Geng Yuan', 'Wei Niu', 'Jiannan Tian', 'Sian Jin', 'Donglin Zhuang', 'Zhe Jiang', 'Yanzhi Wang', 'Bin Ren', 'Shuaiwen Leon Song', 'Dingwen Tao']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ICS '21: Proceedings of the ACM International Conference on Supercomputing\",\n",
       "  'abstract': 'Convolutional neural networks (CNNs) are becoming increasingly deeper, wider, and non-linear because of the growing demand on prediction accuracy and analysis quality. The wide and deep CNNs, however, require a large amount of computing resources and processing time. Many previous works have studied model pruning to improve inference performance, but little work has been done for effectively reducing training cost. In this paper, we propose ClickTrain: an efficient and accurate end-to-end training and pruning framework for CNNs. Different from the existing pruning-during-training work, ClickTrain provides higher model accuracy and compression ratio via fine-grained architecture-preserving pruning. By leveraging pattern-based pruning with our proposed novel accurate weight importance estimation, dynamic pattern generation and selection, and compiler-assisted computation optimizations, ClickTrain generates highly accurate and fast pruned CNN models for direct deployment without any time overhead, compared with the baseline training. ClickTrain also reduces the end-to-end time cost of the state-of-the-art pruning-after-training method by up to 2.3x with comparable accuracy and compression ratio. Moreover, compared with the state-of-the-art pruning-during-training approach, ClickTrain provides significant improvements both accuracy and compression ratio on the tested CNN models and datasets, under similar limited training time.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3447818.3459988',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'coxHE: a software-hardware co-design framework for FPGA acceleration of homomorphic computation',\n",
       "  'authors': \"['Mingqin Han', 'Yilan Zhu', 'Qian Lou', 'Zimeng Zhou', 'Shanqing Guo', 'Lei Ju']\",\n",
       "  'date': 'March 2022',\n",
       "  'source': \"DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe\",\n",
       "  'abstract': 'Data privacy becomes a crucial concern in the AI and big data era. Fully homomorphic encryption (FHE) is a promising data privacy protection technique where the entire computation is performed on encrypted data. However, the dramatic increase of the computation workload restrains the usage of FHE for the real-world applications. In this paper, we propose an FPFA accelerator design framework for CKKS-based HE. While the KeySwitch operations are the primary performance bottleneck of FHE computation, we propose a low latency design of KeySwitch module with reduced intra-operation data dependency. Compared with the state-of-the-art FPGA based key-switch implementation that is based on Verilog, the proposed high-level synthesis (HLS) based design reduces the operation latency by 40%. Furthermore, we propose an automated design space exploration framework which generates optimal encryption parameters and accelerators for a given application kernel and the target FPGA device. Experimental results for a set of real HE application kernels on different FPGA devices show that our HLS-based flexible design framework produces substantially better accelerator design compared with a fixed-parameter HE accelerator in terms of security, approximation error, and overall performance.',\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3539845.3540161',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'μNAS: Constrained Neural Architecture Search for Microcontrollers',\n",
       "  'authors': \"['Edgar Liberis', 'Łukasz Dudziak', 'Nicholas D. Lane']\",\n",
       "  'date': 'April 2021',\n",
       "  'source': \"EuroMLSys '21: Proceedings of the 1st Workshop on Machine Learning and Systems\",\n",
       "  'abstract': 'IoT devices are powered by microcontroller units (MCUs) which are extremely resource-scarce: a typical MCU may have an underpowered processor and around 64 KB of memory and persistent storage. Designing neural networks for such a platform requires an intricate balance between keeping high predictive performance (accuracy) while achieving low memory and storage usage and inference latency. This is extremely challenging to achieve manually, so in this work, we build a neural architecture search (NAS) system, called μNAS, to automate the design of such small-yet-powerful MCU-level networks. μNAS explicitly targets the three primary aspects of resource scarcity of MCUs: the size of RAM, persistent storage and processor speed. μNAS represents a significant advance in resource-efficient models, especially for \"mid-tier\" MCUs with memory requirements ranging from 0.5 KB to 64 KB. We show that on a variety of image classification datasets μNAS is able to (a) improve top-1 classification accuracy by up to 4.8%, or (b) reduce memory footprint by 4-13×, or (c) reduce the number of multiply-accumulate operations by at least 2×, compared to existing MCU specialist literature and resource-efficient models. μNAS is freely available for download at https://github.com/eliberis/uNAS',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3437984.3458836',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'AStitch: enabling a new multi-dimensional optimization space for memory-intensive ML training and inference on modern SIMT architectures',\n",
       "  'authors': \"['Zhen Zheng', 'Xuanda Yang', 'Pengzhan Zhao', 'Guoping Long', 'Kai Zhu', 'Feiwen Zhu', 'Wenyi Zhao', 'Xiaoyong Liu', 'Jun Yang', 'Jidong Zhai', 'Shuaiwen Leon Song', 'Wei Lin']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': \"This work reveals that memory-intensive computation is a rising performance-critical factor in recent machine learning models. Due to a unique set of new challenges, existing ML optimizing compilers cannot perform efficient fusion under complex two-level dependencies combined with just-in-time demand. They face the dilemma of either performing costly fusion due to heavy redundant computation, or skipping fusion which results in massive number of kernels. Furthermore, they often suffer from low parallelism due to the lack of support for real-world production workloads with irregular tensor shapes. To address these rising challenges, we propose AStitch, a machine learning optimizing compiler that opens a new multi-dimensional optimization space for memory-intensive ML computations. It systematically abstracts four operator-stitching schemes while considering multi-dimensional optimization objectives, tackles complex computation graph dependencies with novel hierarchical data reuse, and efficiently processes various tensor shapes via adaptive thread mapping. Finally, AStitch provides just-in-time support incorporating our proposed optimizations for both ML training and inference. Although AStitch serves as a stand-alone compiler engine that is portable to any version of TensorFlow, its basic ideas can be generally applied to other ML frameworks and optimization compilers. Experimental results show that AStitch can achieve an average of 1.84x speedup (up to 2.73x) over the state-of-the-art Google's XLA solution across five production workloads. We also deploy AStitch onto a production cluster for ML workloads with thousands of GPUs. The system has been in operation for more than 10 months and saves about 20,000 GPU hours for 70,000 tasks per week.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503222.3507723',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Trident: Harnessing Architectural Resources for All Page Sizes in x86 Processors',\n",
       "  'authors': \"['Venkat Sri Sai Ram', 'Ashish Panwar', 'Arkaprava Basu']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"MICRO '21: MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture\",\n",
       "  'abstract': 'Intel and AMD processors have long supported more than one large page sizes – 1GB and 2MB, to reduce address translation overheads for applications with large memory footprints. However, previous works on large pages have primarily focused on 2MB pages, partly due to a lack of evidence on the usefulness of 1GB pages to real-world applications. Consequently, micro-architectural resources devoted to 1GB pages have gone underutilized for a decade.  We quantitatively demonstrate where 1GB pages can be valuable, especially when employed in conjunction with 2MB pages. Unfortunately, the lack of application-transparent dynamic allocation of 1GB pages is to blame for the under-utilization of 1GB pages on today’s systems. Toward this, we design and implement Trident in Linux to fully harness micro-architectural resources devoted for all page sizes in the current x86 hardware by transparently allocating 1GB, 2MB, and 4KB pages as suitable at runtime. Trident speeds up eight memory-intensive applications by 18%, on average, over Linux’s use of 2MB pages. We then propose Tridentpv, an extension to Trident that virtualizes 1GB pages via copy-less promotion and compaction in the guest OS. Overall, this paper shows that adequate software enablement brings practical relevance to even GB-sized pages, and motivates micro-architects to continue enhancing hardware support for all large page sizes.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3466752.3480062',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Improving on the Experience of Hand-Assembling Programs for Application-Specific Architectures',\n",
       "  'authors': \"['Ian Piumarta']\",\n",
       "  'date': 'March 2021',\n",
       "  'source': \"Programming '21: Companion Proceedings of the 5th International Conference on the Art, Science, and Engineering of Programming\",\n",
       "  'abstract': 'Creating an application-specific processor is an effective and popular way to solve many problems in embedded hardware design using FPGAs, ASICs, or custom silicon. Programming these processors is complicated by the lack of toolchain support for creating the necessary binary code as part of hardware design, implementation, and evaluation. Hardware developers who cannot create their own ad-hoc assembler are left to hand-assemble their code into binary instructions which is both painful and error prone. We present a tool that supports the rapid creation of assemblers for application-specific processors. A single language is used to specify both instruction formats as collections of bit fields and the instantiation of those formats into sequences of binary instructions as a single, homogeneous activity that is designed to be as familiar and accessible to hardware designers as possible. The output from the tool can be used directly by hardware synthesis tools to initialise the program memory of an application-specific processor.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3464432.3464434',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'SpZip: architectural support for effective data compression in irregular applications',\n",
       "  'authors': \"['Yifan Yang', 'Joel S. Emer', 'Daniel Sanchez']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': 'Irregular applications, such as graph analytics and sparse linear algebra, exhibit frequent indirect, data-dependent accesses to single or short sequences of elements that cause high main memory traffic and limit performance. Data compression is a promising way to accelerate irregular applications by reducing memory traffic. However, software compression adds substantial overheads, and prior hardware compression techniques work poorly on the complex access patterns of irregular applications. We present SpZip, an architectural approach that makes data compression practical for irregular algorithms. SpZip accelerates the traversal, decompression, and compression of the data structures used by irregular applications. In addition, these activities run in a decoupled fashion, hiding both memory access and decompression latencies. To support the wide range of access patterns in these applications, SpZip is programmable, and uses a novel Dataflow Configuration Language to specify programs that traverse and generate compressed data. Our SpZip implementation leverages dataflow execution and time-multiplexing to implement programmability cheaply. We evaluate SpZip on a simulated multicore system running a broad set of graph and linear algebra algorithms. SpZip outperforms prior state-of-the art software-only (hardware-accelerated) systems by gmean 3.0X (1.5X) and reduces memory traffic by 1.7X (1.4X). These benefits stem from both reducing data movement due to compression, and offloading expensive traversal and (de)compression operations.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00087',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Architectural Patterns for Integrating AI Technology into Safety-Critical Systems',\n",
       "  'authors': \"['Georg Macher', 'Matthias Seidl', 'Maid Dzambic', 'Jürgen Dobaj']\",\n",
       "  'date': 'July 2021',\n",
       "  'source': \"EuroPLoP'21: 26th European Conference on Pattern Languages of Programs\",\n",
       "  'abstract': 'Artificial Intelligence (AI) is widely acknowledged as one of the most disruptive technologies driving the digital transformation of industries, enterprises, and societies in the 21st century. Advances in computing speed, algorithmic improvements, and access to a vast amount of data contributed to the adaption of AI in many different domains. Due to the outstanding performance, AI technology is increasingly integrated into safety-critical applications. However, the established safety engineering processes and practices have been only successfully applied in conventional model-based system development and no commonly agreed approaches for integrating AI technology are available yet. This work presents two architectural patterns that can support designers and engineers in the conception of safety-critical AI-enhanced cyber-physical system (CPS) applications. The first pattern addresses the problem of integrating AI capabilities into safety-critical functions. The second pattern deals with architectural approaches to integrate AI technologies for monitoring and learning system-specific behavior at runtime.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3489449.3490014',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Analysing and extending privacy patterns with architectural context',\n",
       "  'authors': \"['Su Yen Chia', 'Xiwei Xu', 'Hye-Young Paik', 'Liming Zhu']\",\n",
       "  'date': 'March 2021',\n",
       "  'source': \"SAC '21: Proceedings of the 36th Annual ACM Symposium on Applied Computing\",\n",
       "  'abstract': 'Privacy is now an increasingly important software quality. Software architects and developers should consider privacy from the early stages of system design to prevent privacy breaches. Both industry and academia have proposed privacy patterns as reusable design solutions to address common privacy problems. However, from the system development perspective, the existing privacy patterns do not provide architectural context to assist software design for privacy. More specifically, the current privacy patterns lack proper analysis with regards to privacy properties - the well-established software traits relating to privacy (e.g., unlinkability, identifiability). Furthermore, the impacts of privacy patterns on other quality attributes such as performance are yet to be investigated. Our paper aims to provide guidance to software architects and developers for considering privacy patterns, by adding new perspectives to the existing privacy patterns. First, we provide a new structural and interaction view of the patterns by relating privacy regulation contexts. Then, we analyse the patterns in architectural contexts and map available privacy-preserving techniques for implementing each privacy pattern. We also give an analysis of privacy patterns with regard to their impact on privacy properties, and the trade-off between privacy and other quality attributes.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3412841.3442014',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Efficient Application of Tensor Core Units for Convolving Images',\n",
       "  'authors': \"['Stefan Groth', 'Jürgen Teich', 'Frank Hannig']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"SCOPES '21: Proceedings of the 24th International Workshop on Software and Compilers for Embedded Systems\",\n",
       "  'abstract': 'Tensor Core Units (TCUs) are a domain-specific architecture capable of executing small Matrix Multiply-Accumulates (MMAs) in a single clock cycle, showing significant performance improvements over other optimized implementations. When Convolutional Neural Networks (CNNs) are accelerated using TCUs, the layout of the input image is transformed to allow large amounts of filters to be applied to an image using a single large matrix-matrix multiplication. However, there are applications in other domains that only require a small number of filters. To accommodate such applications, we first show the inappropriateness of this standard technique of transforming the data layout. Subsequently, we propose an approach that uses TCUs to convolve one filter with an image. Further, we introduce several optimizations of this method. Finally, we evaluate the performance of our approach and its optimizations by comparing it to code generated using a state-of-the-art image processing language.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3493229.3493305',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'FSA: fronthaul slicing architecture for 5G using dataplane programmable switches',\n",
       "  'authors': \"['Nishant Budhdev', 'Raj Joshi', 'Pravein Govindan Kannan', 'Mun Choon Chan', 'Tulika Mitra']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"MobiCom '21: Proceedings of the 27th Annual International Conference on Mobile Computing and Networking\",\n",
       "  'abstract': \"5G networks are gaining pace in development and deployment in recent years. One of 5G's key objective is to support a variety of use cases with different Service Level Objectives (SLOs). Slicing is a key part of 5G that allows operators to provide a tailored set of resources to different use cases in order to meet their SLOs. Existing works focus on slicing in the frontend or the C-RAN. However, slicing is missing in the fronthaul network that connects the frontend to the C-RAN. This leads to over-provisioning in the fronthaul and the C-RAN, and also limits the scalability of the network. In this paper, we design and implement Fronthaul Slicing Architecture (FSA), which to the best of our knowledge, is the first slicing architecture for the fronthaul network. FSA runs in the switch dataplane and uses information from the wireless schedule to identify the slice of a fronthaul data packet at line-rate. It enables multipoint-to-multipoint routing as well as packet prioritization to provide multiplexing gains in the fronthaul and the C-RAN, making the system more scalable. Our testbed evaluation using scaled-up LTE traces shows that FSA can support accurate multipoint-to-multipoint routing for 80 Gbps of fronthaul traffic. Further, the slice-aware packet scheduling enabled by FSA's packet prioritization reduces the 95th percentile Flowlet Completion Times (FCT) of latency-sensitive traffic by up to 4 times.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3447993.3483247',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Simba: scaling deep-learning inference with chiplet-based architecture',\n",
       "  'authors': \"['Yakun Sophia Shao', 'Jason Cemons', 'Rangharajan Venkatesan', 'Brian Zimmer', 'Matthew Fojtik', 'Nan Jiang', 'Ben Keller', 'Alicia Klinefelter', 'Nathaniel Pinckney', 'Priyanka Raina', 'Stephen G. Tell', 'Yanqing Zhang', 'William J. Dally', 'Joel Emer', 'C. Thomas Gray', 'Brucek Khailany', 'Stephen W. Keckler']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': 'Communications of the ACM',\n",
       "  'abstract': 'Package-level integration using multi-chip-modules (MCMs) is a promising approach for building large-scale systems. Compared to a large monolithic die, an MCM combines many smaller chiplets into a larger system, substantially reducing fabrication and design costs. Current MCMs typically only contain a handful of coarse-grained large chiplets due to the high area, performance, and energy overheads associated with inter-chiplet communication. This work investigates and quantifies the costs and benefits of using MCMs with finegrained chiplets for deep learning inference, an application domain with large compute and on-chip storage requirements. To evaluate the approach, we architected, implemented, fabricated, and tested Simba, a 36-chiplet prototype MCM system for deep-learning inference. Each chiplet achieves 4 TOPS peak performance, and the 36-chiplet MCM package achieves up to 128 TOPS and up to 6.1 TOPS/W. The MCM is configurable to support a flexible mapping of DNN layers to the distributed compute and storage units. To mitigate inter-chiplet communication overheads, we introduce three tiling optimizations that improve data locality. These optimizations achieve up to 16% speedup compared to the baseline layer mapping. Our evaluation shows that Simba can process 1988 images/s running ResNet-50 with a batch size of one, delivering an inference latency of 0.50 ms.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3460227',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'GoSPA: an energy-efficient high-performance globally optimized sparse convolutional neural network accelerator',\n",
       "  'authors': \"['Chunhua Deng', 'Yang Sui', 'Siyu Liao', 'Xuehai Qian', 'Bo Yuan']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': 'The co-existence of activation sparsity and model sparsity in convolutional neural network (CNN) models makes sparsity-aware CNN hardware designs very attractive. The existing sparse CNN accelerators utilize intersection operation to search and identify the key positions of the matched entries between two sparse vectors, and hence avoid unnecessary computations. However, these state-of-the-art designs still suffer from three major architecture-level drawbacks, including 1) hardware cost for the intersection operation is high; 2) frequent stalls of computation phase due to strong data dependency between intersection and computation phases; and 3) unnecessary data transfer incurred by the explicit intersection operation. By leveraging the knowledge of the complete sparse 2-D convolution, this paper proposes two key ideas that overcome all of the three drawbacks. First, an implicit on-the-fly intersection is proposed to realize the optimal solution for intersection between one static stream and one dynamic stream, which is the case for sparse neural network inference. Second, by leveraging the global computation structure of 2--D convolution, we propose a specialized computation reordering to ensure that the activation is only transferred if necessary and only once. Based on these two key ideas, we develop GoSPA, an energy-efficient high-performance Globally Optimized SParse CNN Accelerator. GoSPA is implemented with CMOS 28nm technology. Compared with the state-of-the-art sparse CNN architecture, GoSPA achieves average 1.38X, 1.28X, 1.23X, 1.17X, 1.21X and 1.28X speedup on AlexNet, VGG, GoogLeNet, MobileNet, ResNet and ResNeXt workloads, respectively. Also, GoSPA achieves 5.38X, 4.96X, 4.79X, 5.02X, 4.86X and 2.06X energy efficiency improvement on AlexNet, VGG, GoogLeNet, MobileNet, ResNet and ResNeXt, respectively. In more comprehensive comparison including DRAM access, GoSPA also shows significant performance improvement over the existing designs.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00090',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A Novel Hybrid Cache Coherence with Global Snooping for Many-core Architectures',\n",
       "  'authors': \"['Sri Harsha Gade', 'Sujay Deb']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Design Automation of Electronic Systems',\n",
       "  'abstract': 'Cache coherence ensures correctness of cached data in multi-core processors. Traditional implementations of existing protocols make them unscalable for many core architectures. While snoopy coherence requires unscalable ordered networks, directory coherence is weighed down by high area and energy overheads. In this work, we propose Wireless-enabled Share-aware Hybrid (WiSH) to provide scalable coherence in many core processors. WiSH implements a novel Snoopy over Directory protocol using on-chip wireless links and hierarchical, clustered Network-on-Chip to achieve low-overhead and highly efficient coherence. A local directory protocol maintains coherence within a cluster of cores, while coherence among such clusters is achieved through global snoopy protocol. The ordered network for global snooping is provided through low-latency and low-energy broadcast wireless links. The overheads are further reduced through share-aware cache segmentation to eliminate coherence for private blocks. Evaluations show that WiSH reduces traffic by \\\\(\\\\) and runtime by \\\\(\\\\), while requiring \\\\(\\\\) smaller storage and \\\\(\\\\) lower energy as compared to existing hierarchical and hybrid coherence protocols. Owing to its modularity, WiSH provides highly efficient and scalable coherence for many core processors.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3462775',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'On the Application of Computer CAD Drawing Software in Civil Engineering Architectural Design and Structural Design',\n",
       "  'authors': \"['Xu Yao']\",\n",
       "  'date': 'August 2021',\n",
       "  'source': \"ICIMTECH 21: <italic toggle='yes'>Retracted on September 15, 2021</italic>The Sixth International Conference on Information Management and Technology\",\n",
       "  'abstract': 'NOTICE OF RETRACTION: While investigating potential publication-related misconduct in connection with the ICIMTech 2021 Conference Proceedings, serious concerns were raised that cast doubt on the integrity of the peer-review process and all papers published in the Proceedings of this Conference. The integrity of the entire Conference has been called into question. As a result, of its investigation, ACM has decided to retract the Entire Conference Proceedings and all related papers from the ACM Digital Library.None of the papers from this Proceeding should be cited in the literature because of the questionable integrity of the peer review process for this Conference.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3465631.3465816',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Towards Learning-Based Architectures for Sensor Impact Evaluation in Building Controls',\n",
       "  'authors': \"['Saptarshi Bhattacharya', 'Himanshu Sharma', 'Veronica Adetola']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"e-Energy '21: Proceedings of the Twelfth ACM International Conference on Future Energy Systems\",\n",
       "  'abstract': 'Advanced control algorithms for building systems have significant potential in reducing energy consumption while optimizing thermal comfort. Success of such algorithms is critically contingent on several different types of sensor systems, which are deployed for continuous monitoring, identification and estimation of several important building states, such as temperatures, humidity, air quality, power consumption and occupancy. Nonidealities in these sensors can lead to significant performance degradation of the control functionalities, thereby causing unwanted sub-optimal building operation. In this paper, we provide a simulation example with a high-fidelity building model, for a particular use-case of advanced optimization-based control in buildings, i.e., occupancy-based controls. We show how imperfections in occupancy sensing can degrade performance. Subsequently, we discuss a novel learning-based framework to efficiently evaluate the impact of sensor nonidealities for building systems, in context of advanced control algorithms.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3447555.3466591',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Architectural Jewels of Lublin: A Modern Computerized Board Game in Cultural Heritage Education',\n",
       "  'authors': \"['Jerzy Montusiewicz', 'Marek Milosz']\",\n",
       "  'date': 'None',\n",
       "  'source': 'Journal on Computing and Cultural Heritage ',\n",
       "  'abstract': \"Lublin is a city located in the eastern part of Poland, which is an important place of cultural heritage, being the venue where the Polish-Lithuanian Union was signed 450 years ago in 2019. This article presents “Architectural Jewels of Lublin,” a computerized serious board game for two players. The aim of the game is to collect points for recognizing models of the city's historic architectural objects and their correct position on the board. They represent the landmarks of the historic Old Town quarter. Another point of the game is to answer questions about the cultural heritage of Lublin. 3D models of historic buildings were initially designed manually and then 3D printed in FFF (Fused Filament Fabrication) technology. The correct location of the object on the board is identified by sensors working in the RFID (Radio-Frequency IDentification) technology supported by two microcontrollers of an Arduino platform, which were connected to the software managing the whole game shown on a tablet monitor. The game is used both to promote Lublin at numerous cyclical cultural and science popularization events, and during conferences and seminars organized for circles representing cultural heritage from Poland and abroad. It is aimed at presenting a way to integrate many different contemporary digital technologies that can serve education in the area of cultural heritage. The game, in contrast to popular games using VR and AR technologies, combines in an interesting way physical and digital space using modern computer technologies. The research carried out on the participants of the game has shown its high effectiveness in raising the historical awareness of its participants, as well as the players’ positive attitude toward the game.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3446978',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Exploring Efficient Architectures on Remote In-Memory NVM over RDMA',\n",
       "  'authors': \"['Qingfeng Zhuge', 'Hao Zhang', 'Edwin Hsing-Mean Sha', 'Rui Xu', 'Jun Liu', 'Shengyu Zhang']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Embedded Computing Systems',\n",
       "  'abstract': 'Efficiently accessing remote file data remains a challenging problem for data processing systems. Development of technologies in non-volatile dual in-line memory modules (NVDIMMs), in-memory file systems, and RDMA networks provide new opportunities towards solving the problem of remote data access. A general understanding about NVDIMMs, such as Intel Optane DC Persistent Memory (DCPM), is that they expand main memory capacity with a cost of multiple times lower performance than DRAM. With an in-depth exploration presented in this paper, however, we show an interesting finding that the potential of NVDIMMs for high-performance, remote in-memory accesses can be revealed through careful design. We explore multiple architectural structures for accessing remote NVDIMMs in a real system using Optane DCPM, and compare the performance of various structures. Experiments are conducted to show significant performance gaps among different ways of using NVDIMMs as memory address space accessible through RDMA interface. Furthermore, we design and implement a prototype of user-level, in-memory file system, RIMFS, in the device DAX mode on Optane DCPM. By comparing against the DAX-supported Linux file system, Ext4-DAX, we show that the performance of remote reads on RIMFS over RDMA is 11.44 \\\\(\\\\) higher than that on a remote Ext4-DAX on average. The experimental results also show that the performance of remote accesses on RIMFS is maintained on a heavily loaded data server with CPU utilization as high as 90%, while the performance of remote reads on Ext4-DAX is significantly reduced by 49.3%, and the performance of local reads on Ext4-DAX is even more significantly reduced by 90.1%. The performance comparisons of writes exhibit the same trends.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3477004',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'RaPiD: AI accelerator for ultra-low precision training and inference',\n",
       "  'authors': \"['Swagath Venkataramani', 'Vijayalakshmi Srinivasan', 'Wei Wang', 'Sanchari Sen', 'Jintao Zhang', 'Ankur Agrawal', 'Monodeep Kar', 'Shubham Jain', 'Alberto Mannari', 'Hoang Tran', 'Yulong Li', 'Eri Ogawa', 'Kazuaki Ishizaki', 'Hiroshi Inoue', 'Marcel Schaal', 'Mauricio Serrano', 'Jungwook Choi', 'Xiao Sun', 'Naigang Wang', 'Chia-Yu Chen', 'Allison Allain', 'James Bonano', 'Nianzheng Cao', 'Robert Casatuta', 'Matthew Cohen', 'Bruce Fleischer', 'Michael Guillorn', 'Howard Haynie', 'Jinwook Jung', 'Mingu Kang', 'Kyu-hyoun Kim', 'Siyu Koswatta', 'Saekyu Lee', 'Martin Lutz', 'Silvia Mueller', 'Jinwook Oh', 'Ashish Ranjan', 'Zhibin Ren', 'Scot Rider', 'Kerstin Schelm', 'Michael Scheuermann', 'Joel Silberman', 'Jie Yang', 'Vidhi Zalani', 'Xin Zhang', 'Ching Zhou', 'Matt Ziegler', 'Vinay Shah', 'Moriyoshi Ohara', 'Pong-Fei Lu', 'Brian Curran', 'Sunil Shukla', 'Leland Chang', 'Kailash Gopalakrishnan']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': 'The growing prevalence and computational demands of Artificial Intelligence (AI) workloads has led to widespread use of hardware accelerators in their execution. Scaling the performance of AI accelerators across generations is pivotal to their success in commercial deployments. The intrinsic error-resilient nature of AI workloads present a unique opportunity for performance/energy improvement through precision scaling. Motivated by the recent algorithmic advances in precision scaling for inference and training, we designed RAPID1, a 4-core AI accelerator chip supporting a spectrum of precisions, namely, 16 and 8-bit floating-point and 4 and 2-bit fixed-point. The 36mm2 RAPID chip fabricated in 7nm EUV technology delivers a peak 3.5 TFLOPS/W in HFP8 mode and 16.5 TOPS/W in INT4 mode at nominal voltage. Using a performance model calibrated to within 1% of the measurement results, we evaluated DNN inference using 4-bit fixed-point representation for a 4-core 1 RAPID chip system and DNN training using 8-bit floating point representation for a 768 TFLOPs AI system comprising 4 32-core RAPID chips. Our results show INT4 inference for batch size of 1 achieves 3 - 13.5 (average 7) TOPS/W and FP8 training for a mini-batch of 512 achieves a sustained 102 - 588 (average 203) TFLOPS across a wide range of applications.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00021',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'DANA: Dimension-Adaptive Neural Architecture for Multivariate Sensor Data',\n",
       "  'authors': \"['Mohammad Malekzadeh', 'Richard Clegg', 'Andrea Cavallaro', 'Hamed Haddadi']\",\n",
       "  'date': 'None',\n",
       "  'source': 'Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies',\n",
       "  'abstract': 'Motion sensors embedded in wearable and mobile devices allow for dynamic selection of sensor streams and sampling rates, enabling several applications, such as power management and data-sharing control. While deep neural networks (DNNs) achieve competitive accuracy in sensor data classification, DNN architectures generally process incoming data from a fixed set of sensors with a fixed sampling rate, and changes in the dimensions of their inputs cause considerable accuracy loss, unnecessary computations, or failure in operation. To address these limitations, we introduce a dimension-adaptive pooling (DAP) layer that makes DNNs flexible and more robust to changes in sensor availability and in sampling rate. DAP operates on convolutional filter maps of variable dimensions and produces an input of fixed dimensions suitable for feedforward and recurrent layers. Further, we propose a dimension-adaptive training (DAT) procedure for enabling DNNs that use DAP to better generalize over the set of feasible data dimensions at inference time. DAT comprises the random selection of dimensions during the forward passes and optimization with accumulated gradients of several backward passes. Combining DAP and DAT, we show how to transform existing non-adaptive DNNs into a Dimension-Adaptive Neural Architecture (DANA), while keeping the same number of parameters. Compared to existing approaches, our solution provides better average classification accuracy over the range of possible data dimensions at inference time and does not require up-sampling or imputation, thus reducing unnecessary computations. Experimental results on seven datasets (four benchmark real-world datasets for human activity recognition and three synthetic datasets) show that DANA prevents significant losses in classification accuracy of the state-of-the-art DNNs and, compared to baselines, it better captures correlated patterns in sensor data under dynamic sensor availability and varying sampling rates.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3478074',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Nap: Persistent Memory Indexes for NUMA Architectures',\n",
       "  'authors': \"['Qing Wang', 'Youyou Lu', 'Junru Li', 'Minhui Xie', 'Jiwu Shu']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Storage',\n",
       "  'abstract': 'We present Nap, a black-box approach that converts concurrent persistent memory (PM) indexes into non-uniform memory access (NUMA)-aware counterparts. Based on the observation that real-world workloads always feature skewed access patterns, Nap introduces a NUMA-aware layer (NAL) on the top of existing concurrent PM indexes, and steers accesses to hot items to this layer. The NAL maintains (1) per-node partial views in PM for serving insert/update/delete operations with failure atomicity and (2) a global view in DRAM for serving lookup operations. The NAL eliminates remote PM accesses to hot items without inducing extra local PM accesses. Moreover, to handle dynamic workloads, Nap adopts a fast NAL switch mechanism. We convert five state-of-the-art PM indexes using Nap. Evaluation on a four-node machine with Optane DC Persistent Memory shows that Nap can improve the throughput by up to 2.3× and 1.56× under write-intensive and read-intensive workloads, respectively.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3507922',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Network Performance Influences of Software-defined Networks on Micro-service Architectures',\n",
       "  'authors': \"['Axel Busch', 'Martin Kammerer']\",\n",
       "  'date': 'April 2021',\n",
       "  'source': \"ICPE '21: Proceedings of the ACM/SPEC International Conference on Performance Engineering\",\n",
       "  'abstract': 'Modern business applications are increasingly developed as micro-services and deployed in the cloud. Due to many components involved micro-services need a flexible and high-performance network infrastructure. To ensure highly available and high performance applications, operators are increasingly relying on cloud service platforms such as the OpenShift Container Platform on Z. In such environments modern software-defined network technologies such as Open vSwitch (OVS) are used. However, the impact of their architecture on network performance has not yet been sufficiently researched although networking performance is particularly critical for the quality of the service. In this paper, we analyse the impact of the OVS pipeline and selected OVS operations in detail. We define different scenarios used in the industry and analyse the performance of different OVS configurations using an IBM z14 mainframe system. Our analysis showed the OVS pipeline and its operations can affect network performance by up to factor 3. Our results show that even the use of virtual switches such as OVS, network performance can be significantly improved by optimizing the OVS pipeline architecture.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3427921.3450236',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'On Using Blockchains for Beyond Visual Line of Sight (BVLOS) Drones Operation: An Architectural Study',\n",
       "  'authors': \"['Tahina Ralitera', 'Onder Gurcan']\",\n",
       "  'date': 'January 2022',\n",
       "  'source': 'DroneSE and RAPIDO: System Engineering for constrained embedded systems',\n",
       "  'abstract': 'Beyond Visual Line of Sight operation enables drones to surpass the limits imposed by the reach and constraints of the eyes of their operator. It extends their range and, as such, productivity and profitability. Drones operating BVLOS include a variety of highly sensitive assets and information that could be subject to unintentional or intentional security vulnerabilities. As a solution, blockchain-based services could enable secure and trustworthy exchange and storage of related data. They also allow for traceability of exchanges and synchronization with other nodes in the network. In this context, we develop simulation models to evaluate different blockchain-based solutions before implementing them in real systems. However, most of these blockchain-based approaches focus on the network and the protocol aspects of drone systems. Few studies focus on the architectural level of on-chip compute platforms of drones. Based on this observation, the contribution of this paper is twofold : (1) a generic blockchain-based service architecture for on-chip compute platforms of drones in a simulated environment, and (2) an illustration of the proposed generic architecture in a simulated environment for authentication.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3522784.3522794',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'An Archive of Interfaces: Exploring the Potential of Emulation for Software Research, Pedagogy, and Design',\n",
       "  'authors': \"['Daniel Cardoso-Llach', 'Eric Kaltman', 'Emek Erdolu', 'Zachary Furste']\",\n",
       "  'date': 'None',\n",
       "  'source': 'Proceedings of the ACM on Human-Computer Interaction',\n",
       "  'abstract': 'This paper explores the potential of distributed emulation networks to support research and pedagogy into historical and sociotechnical aspects of software. Emulation is a type of virtualization that re-creates the conditions for a piece of legacy software to operate on a modern system. The paper first offers a review of Computer-Supported Cooperative Work (CSCW), Human-Computer Interaction (HCI), and Science and Technology Studies (STS) literature engaging with software as historical and sociotechnical artifacts, and with emulation as a vehicle of scholarly inquiry. It then documents the novel use of software emulations as a pedagogical resource and research tool for legacy software systems analysis. This is accomplished through the integration of the Emulation as a Service Infrastructure (EaaSI) distributed emulation network into a university-level course focusing on computer-aided design (CAD). The paper offers a detailed case study of a pedagogical experience oriented to incorporate emulations into software research and learning. It shows how emulations allow for close, user-centered analyses of software systems that highlight both their historical evolution and core interaction concepts, and how they shape the work practices of their users.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3476035',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Improving Barnes-Hut t-SNE Algorithm\\xa0in Modern GPU Architectures with Random Forest KNN and Simulated Wide-Warp',\n",
       "  'authors': \"['Bruno Henrique Meyer', 'Aurora Trinidad Ramirez Pozo', 'Wagner M. Nunan Zola']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Journal on Emerging Technologies in Computing Systems',\n",
       "  'abstract': 'The t-Distributed Stochastic Neighbor Embedding (t-SNE) is a widely used technique for dimensionality reduction but is limited by its scalability when applied to large datasets. Recently, BH-tSNE was proposed; this is a successful approximation that transforms a step of the original algorithm into an N-Body simulation problem that can be solved by a modified Barnes-Hut algorithm. However, this improvement still has limitations to process large data volumes (millions of records). Late studies, such as t-SNE-CUDA, have used GPUs to implement highly parallel BH-tSNE. In this research we have developed a new GPU BH-tSNE implementation that produces the embedding of multidimensional data points into three-dimensional space. We examine scalability issues in two of the most expensive steps of GPU BH-tSNE by using efficient memory access strategies , recent acceleration techniques , and a new approach to compute the KNN graph structure used in BH-tSNE with GPU. Our design allows up to 460% faster execution when compared to the t-SNE-CUDA implementation. Although our SIMD acceleration techniques were used in a modern GPU setup, we have also verified a potential for applications in the context of multi-core processors.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3447779',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Sustainable Security for the Internet of Things Using Artificial Intelligence Architectures',\n",
       "  'authors': \"['Celestine Iwendi', 'Saif Ur Rehman', 'Abdul Rehman Javed', 'Suleman Khan', 'Gautam Srivastava']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Internet Technology',\n",
       "  'abstract': 'In this digital age, human dependency on technology in various fields has been increasing tremendously. Torrential amounts of different electronic products are being manufactured daily for everyday use. With this advancement in the world of Internet technology, cybersecurity of software and hardware systems are now prerequisites for major business’ operations. Every technology on the market has multiple vulnerabilities that are exploited by hackers and cyber-criminals daily to manipulate data sometimes for malicious purposes. In any system, the Intrusion Detection System (IDS) is a fundamental component for ensuring the security of devices from digital attacks. Recognition of new developing digital threats is getting harder for existing IDS. Furthermore, advanced frameworks are required for IDS to function both efficiently and effectively. The commonly observed cyber-attacks in the business domain include minor attacks used for stealing private data. This article presents a deep learning methodology for detecting cyber-attacks on the Internet of Things using a Long Short Term Networks classifier. Our extensive experimental testing show an Accuracy of 99.09%, F1-score of 99.46%, and Recall of 99.51%, respectively. A detailed metric representing our results in tabular form was used to compare how our model was better than other state-of-the-art models in detecting cyber-attacks with proficiency.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3448614',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Review on architectures of aquaponic systems based on the Internet of Things and artificial intelligence: Comparative study.',\n",
       "  'authors': \"['Khaoula Taji', 'Rachida Ait Abdelouahid', 'Ibtissame Ezzahoui', 'Abdelaziz Marzak']\",\n",
       "  'date': 'April 2021',\n",
       "  'source': \"NISS '21: Proceedings of the 4th International Conference on Networking, Information Systems &amp; Security\",\n",
       "  'abstract': \"The aquaponics system is an agricultural production technique that combines aquaculture with hydroponics and reduces the environmental impact by reducing the need for water and nutrients. New technologies such as the Internet of Things and artificial intelligence can be an efficient and reliable application for this purpose. They can help to implement this system in urban areas and to produce organic food. The main objective of this article is to present a comparative study between different existing aquaponic architectures according to different factors such as Security, Interoperability, Renewable energy usability, Cost, and so one, and various technologies used based on Internet of Things and artificial intelligence approaches such as sensors, actuators, gateway, communication technology, storage system, user interface etc., as well as the advantages and limitations of each proposed solution. This paper is an opening towards a new contribution that will be based on an interoperable, secure, scalable, low-cost, fully self-powered, flexible, reliable, intelligent and generic IoT architecture that meets the requirements of aquaponics. This new contribution will also make it possible to optimize the critical parameters in aquaponics, namely water quality, pH, air temperature, humidity, CO2, etc., and predict the state of the system's health to improve its productivity, thanks to artificial intelligence.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3454127.3457625',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Dynamic Partitioned Scheduling of Real-Time DAG Tasks on ARM big.LITTLE Architectures*',\n",
       "  'authors': \"['Agostino Mascitti', 'Tommaso Cucinotta']\",\n",
       "  'date': 'April 2021',\n",
       "  'source': \"RTNS '21: Proceedings of the 29th International Conference on Real-Time Networks and Systems\",\n",
       "  'abstract': 'This paper evaluates the combination of a Directed Acyclic Graph (DAG) task splitting technique already proposed in the literature and the state-of-the-art, energy-aware version of the well-known CBS server (BL-CBS), which dynamically partitions and schedules real-time task sets in an energy-efficient way on multi-core platforms based on the ARM big.LITTLE architecture. The approach is designed to be used with any DAG in a transparent way as an on-line and adaptive scheduler supporting “open” systems. The approach is validated and evaluated through the open-source RTSim simulator, which has been extended integrating an energy model of the ODROID-XU3 board and the code-base needed to perform the DAG task decomposition and scheduling. Simulations on randomly generated DAGs show that the approach leads to promising results.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3453417.3453442',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Gem5-X: A Many-core Heterogeneous Simulation Platform for Architectural Exploration and Optimization',\n",
       "  'authors': \"['Yasir Mahmood Qureshi', 'William Andrew Simon', 'Marina Zapater', 'Katzalin Olcoz', 'David Atienza']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Architecture and Code Optimization',\n",
       "  'abstract': 'The increasing adoption of smart systems in our daily life has led to the development of new applications with varying performance and energy constraints, and suitable computing architectures need to be developed for these new applications. In this article, we present gem5-X, a system-level simulation framework, based on gem-5, for architectural exploration of heterogeneous many-core systems. To demonstrate the capabilities of gem5-X, real-time video analytics is used as a case-study. It is composed of two kernels, namely, video encoding and image classification using convolutional neural networks (CNNs). First, we explore through gem5-X the benefits of latest 3D high bandwidth memory (HBM2) in different architectural configurations. Then, using a two-step exploration methodology, we develop a new optimized clustered-heterogeneous architecture with HBM2 in gem5-X for video analytics application. In this proposed clustered-heterogeneous architecture, ARMv8 in-order cluster with in-cache computing engine executes the video encoding kernel, giving 20% performance and 54% energy benefits compared to baseline ARM in-order and Out-of-Order systems, respectively. Furthermore, thanks to gem5-X, we conclude that ARM Out-of-Order clusters with HBM2 are the best choice to run visual recognition using CNNs, as they outperform DDR4-based system by up to 30% both in terms of performance and energy savings.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3461662',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Fast simulation of future 128-bit architectures',\n",
       "  'authors': \"['Fabien Portas', 'Frédéric Pétrot']\",\n",
       "  'date': 'March 2022',\n",
       "  'source': \"DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe\",\n",
       "  'abstract': 'Whether 128-bit architectures will some day hit the market or not is an open question. There is however a trend towards that direction: virtual addresses grew from 34 to 48 bits in 1999 and then to 57 bits in 2019. The impact of a virtually infinite addressable space on software is hard to predict, but it will most likely be major. Simulation tools are therefore needed to support research and experimentation for tooling and software. In this paper, we present the implementation of the 128-bit extension of the RISC-V architecture in the QEMU functional simulator and report first performance evaluations. On our limited set of programs, simulation is slowed down by a factor of at worst 5 compared to 64-bit simulation, making the tool still usable for executing large software codes.',\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3539845.3540109',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Designing calibration and expressivity-efficient instruction sets for quantum computing',\n",
       "  'authors': \"['Lingling Lao', 'Prakash Murali', 'Margaret Martonosi', 'Dan Browne']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': 'Near-term quantum computing (QC) systems have limited qubit counts, high gate (instruction) error rates, and typically support a minimal instruction set having one type of two-qubit gate (2Q). To reduce program instruction counts and improve application expressivity, vendors have proposed, and shown proof-of-concept demonstrations of richer instruction sets such as XY gates (Rigetti) and fSim gates (Google). These instruction sets comprise of families of 2Q gate types parameterized by continuous qubit rotation angles. That is, it allows a large set of different physical operations to be realized on the qubits, based on the input angles. However, having such a large number of gate types is problematic because each gate type has to be calibrated periodically, across the full system, to obtain high fidelity implementations. This results in substantial recurring calibration overheads even on current systems which use only a few gate types. Our work aims to navigate this tradeoff between application expressivity and calibration overhead, and identify what instructions vendors should implement to get the best expressivity with acceptable calibration time. Studying this tradeoff is challenging because of the diversity in QC application requirements, the need to optimize applications for widely different hardware gate types and noise variations across gate types. Therefore, our work develops NuOp, a flexible compilation pass based on numerical optimization, to efficiently decompose application operations into arbitrary hardware gate types. Using NuOp and four important quantum applications, we study the instruction set proposals of Rigetti and Google, with realistic noise simulations and a calibration model. Our experiments show that implementing 4--8 types of 2Q gates is sufficient to attain nearly the same expressivity as a full continuous gate family, while reducing the calibration overhead by two orders of magnitude. With several vendors proposing rich gate families as means to higher fidelity, our work has potential to provide valuable instruction set design guidance for near-term QC systems.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00071',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'On the Performance of Deep Learning in the Full Edge and the Full Cloud Architectures',\n",
       "  'authors': \"['Tajeddine Benbarrad', 'Marouane Salhaoui', 'Mounir Arioua']\",\n",
       "  'date': 'April 2021',\n",
       "  'source': \"NISS '21: Proceedings of the 4th International Conference on Networking, Information Systems &amp; Security\",\n",
       "  'abstract': 'Deep learning today surpasses various machine learning approaches in performance and is widely used for variety of different tasks. Deep learning has increased accuracy compared to other approaches for tasks like language translation and image recognition. However, training a deep learning model on a large dataset is a challenging and expensive task that can be time consuming and require large computational resources. Therefore, Different architectures have been proposed for the implementation of deep learning models in machine vision systems to deal with this problem. Currently, the application of deep learning in the cloud is the most common and typical method. Nevertheless, the challenge of having to move the data from where it is generated to a cloud data center so that it can be used to prepare and develop machine learning models represents a major limitation of this approach. As a result, it is becoming increasingly important to consider moving aspects of deep learning to the edge, instead of the cloud, especially with the rapid increase in data volumes and the growing need to act in real time. From this perspective, a comparative study between the full edge and the full cloud architectures based on the performance of the deep learning models implemented in both architectures is elaborated. The results of this study lead us to specify the strengths of both the cloud and the edge for deploying deep learning models, and to choose the optimal architecture to deal with the rapid increase in data volumes and the growing need for real-time action.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3454127.3457632',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'RingCNN: exploiting algebraically-sparse ring tensors for energy-efficient CNN-based computational imaging',\n",
       "  'authors': \"['Chao-Tsung Huang']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': 'In the era of artificial intelligence, convolutional neural networks (CNNs) are emerging as a powerful technique for computational imaging. They have shown superior quality for reconstructing fine textures from badly-distorted images and have potential to bring next-generation cameras and displays to our daily life. However, CNNs demand intensive computing power for generating high-resolution videos and defy conventional sparsity techniques when rendering dense details. Therefore, finding new possibilities in regular sparsity is crucial to enable large-scale deployment of CNN-based computational imaging. In this paper, we consider a fundamental but yet well-explored approach---algebraic sparsity---for energy-efficient CNN acceleration. We propose to build CNN models based on ring algebra that defines multiplication, addition, and non-linearity for n-tuples properly. Then the essential sparsity will immediately follow, e.g. n-times reduction for the number of real-valued weights. We define and unify several variants of ring algebras into a modeling framework, RingCNN, and make comparisons in terms of image quality and hardware complexity. On top of that, we further devise a novel ring algebra which minimizes complexity with component-wise product and achieves the best quality using directional ReLU. Finally, we design an accelerator, eRingCNN, to accommodate to the proposed ring algebra, in particular with regular ring-convolution arrays for efficient inference and on-the-fly directional ReLU blocks for fixed-point computation. We implement two configurations, n = 2 and 4 (50% and 75% sparsity), with 40 nm technology to support advanced denoising and super-resolution at up to 4K UHD 30 fps. Layout results show that they can deliver equivalent 41 TOPS using 3.76 W and 2.22 W, respectively. Compared to the real-valued counterpart, our ring convolution engines for n = 2 achieve 2.00 x energy efficiency and 2.08 x area efficiency with similar or even better image quality. With n = 4, the efficiency gains of energy and area are further increased to 3.84X and 3.77X with only 0.11 dB drop of peak signal-to-noise ratio (PSNR). The results show that RingCNN exhibits great architectural advantages for providing near-maximum hardware efficiencies and graceful quality degradation simultaneously.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00089',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Accelerating Transform Algorithm\\xa0Implementation for Efficient Intra Coding of 8K UHD Videos',\n",
       "  'authors': \"['Yang Guo', 'Wei Gao', 'Siwei Ma', 'Ge Li']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Multimedia Computing, Communications, and Applications',\n",
       "  'abstract': 'Real-time ultra-high-definition (UHD) video applications have attracted much attention, where the encoder side urgently demands the high-throughput two-dimensional (2D) transform hardware implementation for the latest video coding standards. This article proposes an effective acceleration method for transform algorithm in UHD intra coding based on the third generation of audio video coding standard (AVS3). First, by conducting detailed statistical analysis, we devise an efficient hardware-friendly transform algorithm that can reduce running cycles and resource consumption remarkably. Second, to implement multiplierless computation for saving resources and power, a series of shift-and-add unit (SAU) hardwares are investigated to have much less adoptions of shifters and adders than the existing methods. Third, different types of hardware acceleration methods, including calculation pipelining, logical-loop unrolling, and module-level parallelism, are designed to efficaciously support the data-intensive high frame-rate 8K UHD video coding. Finally, due to the scarcity of 8K video sources, we also provide a new dataset for the performance verification. Experimental results demonstrate that our proposed method can effectively fulfill the real-time 8K intra encoding at beyond 60 fps, with very negligible loss on rate-distortion (R-D) performance, which is averagely 0.98% Bjontegaard-Delta Bit-Rate (BD-BR).',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3507970',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A Pattern for a Secure Actuator Node',\n",
       "  'authors': \"['Cristian Orellana', 'Hernán Astudillo', 'Eduardo B. Fernandez']\",\n",
       "  'date': 'July 2021',\n",
       "  'source': \"EuroPLoP'21: 26th European Conference on Pattern Languages of Programs\",\n",
       "  'abstract': 'IoT devices are increasingly present in our lives, and today constitute a strong pillar for designing smart systems that perceive the environment and intervene in it according to predefined rules. This ability to interact with the environment has historically been the target of cyberattacks, causing erratic behavior in these systems, which brings serious consequences. This work introduces Secure Actuator Node, a pattern that addresses the design of a secure actuator that considers the identification of threats and their respective countermeasures to mitigate or eliminate these types of security issues.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3489449.3490007',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Efficient-Grad: Efficient Training Deep Convolutional Neural Networks on Edge Devices with Gradient Optimizations',\n",
       "  'authors': \"['Ziyang Hong', 'C. Patrick Yue']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Embedded Computing Systems',\n",
       "  'abstract': 'With the prospering of mobile devices, the distributed learning approach, enabling model training with decentralized data, has attracted great interest from researchers. However, the lack of training capability for edge devices significantly limits the energy efficiency of distributed learning in real life. This article describes Efficient-Grad, an algorithm-hardware co-design approach for training deep convolutional neural networks, which improves both throughput and energy saving during model training, with negligible validation accuracy loss.The key to Efficient-Grad is its exploitation of two observations. Firstly, the sparsity has potential for not only activation and weight, but gradients and the asymmetry residing in the gradients for the conventional back propagation (BP). Secondly, a dedicated hardware architecture for sparsity utilization and efficient data movement can be optimized to support the Efficient-Grad algorithm in a scalable manner. To the best of our knowledge, Efficient-Grad is the first approach that successfully adopts a feedback-alignment (FA)-based gradient optimization scheme for deep convolutional neural network training, which leads to its superiority in terms of energy efficiency. We present case studies to demonstrate that the Efficient-Grad design outperforms the prior arts by 3.72x in terms of energy efficiency.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3504034',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'SparseP: Towards Efficient Sparse Matrix Vector Multiplication on Real Processing-In-Memory Architectures',\n",
       "  'authors': \"['Christina Giannoula', 'Ivan Fernandez', 'Juan Gómez Luna', 'Nectarios Koziris', 'Georgios Goumas', 'Onur Mutlu']\",\n",
       "  'date': 'None',\n",
       "  'source': 'Proceedings of the ACM on Measurement and Analysis of Computing Systems',\n",
       "  'abstract': 'Several manufacturers have already started to commercialize near-bank Processing-In-Memory (PIM) architectures, after decades of research efforts. Near-bank PIM architectures place simple cores close to DRAM banks. Recent research demonstrates that they can yield significant performance and energy improvements in parallel applications by alleviating data access costs. Real PIM systems can provide high levels of parallelism, large aggregate memory bandwidth and low memory access latency, thereby being a good fit to accelerate the Sparse Matrix Vector Multiplication (SpMV) kernel. SpMV has been characterized as one of the most significant and thoroughly studied scientific computation kernels. It is primarily a memory-bound kernel with intensive memory accesses due its algorithmic nature, the compressed matrix format used, and the sparsity patterns of the input matrices given. This paper provides the first comprehensive analysis of SpMV on a real-world PIM architecture, and presents SparseP, the first SpMV library for real PIM architectures. We make three key contributions. First, we implement a wide variety of software strategies on SpMV for a multithreaded PIM core, including (1) various compressed matrix formats, (2) load balancing schemes across parallel threads and (3) synchronization approaches, and characterize the computational limits of a single multithreaded PIM core. Second, we design various load balancing schemes across multiple PIM cores, and two types of data partitioning techniques to execute SpMV on thousands of PIM cores: (1) 1D-partitioned kernels to perform the complete SpMV computation only using PIM cores, and (2) 2D-partitioned kernels to strive a balance between computation and data transfer costs to PIM-enabled memory. Third, we compare SpMV execution on a real-world PIM system with 2528 PIM cores to an Intel Xeon CPU and an NVIDIA Tesla V100 GPU to study the performance and energy efficiency of various devices, i.e., both memory-centric PIM systems and conventional processor-centric CPU/GPU systems, for the SpMV kernel. SparseP software package provides 25 SpMV kernels for real PIM systems supporting the four most widely used compressed matrix formats, i.e., CSR, COO, BCSR and BCOO, and a wide range of data types. SparseP is publicly and freely available at https://github.com/CMU-SAFARI/SparseP. Our extensive evaluation using 26 matrices with various sparsity patterns provides new insights and recommendations for software designers and hardware architects to efficiently accelerate the SpMV kernel on real PIM systems.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3508041',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Branchless Code Generation for Modern Processor Architectures',\n",
       "  'authors': \"['Alexandros Angelou', 'Antonios Dadaliaris', 'Michael Dossis', 'Georgios Dimitriou']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"PCI '21: Proceedings of the 25th Pan-Hellenic Conference on Informatics\",\n",
       "  'abstract': 'Compilers apply transformations to the code they compile in order to make it run faster without changing its behavior. This process is called code optimization. Modern compilers apply many different passes of code optimization to ensure maximum runtime performance and efficiency, at the rather negligible expense of larger compilation times. This study focuses on a particular optimization, called branchless optimization, which eliminates code branches by utilizing different data transformation techniques that have the same effect. Such techniques are explored on their implementation on the LLVM IR and MIPS and partly ARM assembly, and ranked based on their runtime efficiency. Moreover, the stages of implementing the optimization transformation are explored, as well as different instruction set features that some CPU architectures provide that can be used to increase the efficiency of the optimization.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503823.3503879',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'NanoTransport: A Low-Latency, Programmable Transport Layer for NICs',\n",
       "  'authors': \"['Serhat Arslan', 'Stephen Ibanez', 'Alex Mallery', 'Changhoon Kim', 'Nick McKeown']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"SOSR '21: Proceedings of the ACM SIGCOMM Symposium on SDN Research (SOSR)\",\n",
       "  'abstract': 'Transport protocols can be implemented in NIC (Network Interface Card) hardware to increase throughput, reduce latency and free up CPU cycles. If the ideal transport protocol were known, the optimal implementation would be simple: bake it into fixed-function hardware. But transport layer protocols are still evolving, with innovative new algorithms proposed every year. A recent study proposed Tonic, a Verilog-programmable transport layer in hardware. We build on this work to propose a new programmable hardware transport layer architecture, called nanoTransport, optimized for the extremely low-latency message-based RPCs (Remote Procedure Calls) that dominate large, modern distributed data center applications. NanoTransport is programmed using the P4 language, making it easy to modify existing (or create entirely new) transport protocols in hardware. We identify common events and primitive operations, allowing for a streamlined, modular, programmable pipeline, including packetization, reassembly, timeouts and packet generation, all to be expressed by the programmer. We evaluate our nanoTransport prototype by programming it to run the reliable message-based transport protocols NDP and Homa, as well as a hybrid variant. Our FPGA prototype - implemented in Chisel and running on the Firesim simulator - exposes P4-programmable pipelines and is designed to run in an ASIC at 200Gb/s with each packet processed end-to-end in less than 10ns (including message reassembly).',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3482898.3483365',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Domain Decomposition Preconditioners for Unstructured Network Problems in Parallel Vector Architectures',\n",
       "  'authors': \"['Daniel Adrian Maldonado', 'Michel Schanen', 'François Pacaud', 'Mihai Anitescu']\",\n",
       "  'date': 'August 2021',\n",
       "  'source': \"ICPP Workshops '21: 50th International Conference on Parallel Processing Workshop\",\n",
       "  'abstract': 'In this paper we present our experience implementing domain decomposition preconditioners on vector architectures. In particular, we will focus on the solution of unstructured network equations arising from electrical power systems by preconditioning iterative algorithms with the Additive Schwarz Method (ASM). The implementation will be carried out using the Julia programming language, which allows for easy prototyping and interfacing with GPU architectures thanks to its multiple dispatch features. In our experiments, we will show the trade-off between device throughput and convergence of the iterative algorithm as the size of the domain varies, and determine optimal fronts of computational performance.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3458744.3473363',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Fast and Scalable Sparse Triangular Solver for Multi-GPU Based HPC Architectures',\n",
       "  'authors': \"['CHENHAO XIE', 'Jieyang Chen', 'Jesun Firoz', 'Jiajia Li', 'Shuaiwen Leon Song', 'Kevin Barker', 'Mark Raugas', 'Ang Li']\",\n",
       "  'date': 'August 2021',\n",
       "  'source': \"ICPP '21: Proceedings of the 50th International Conference on Parallel Processing\",\n",
       "  'abstract': 'Designing efficient and scalable sparse linear algebra kernels on modern multi-GPU based HPC systems is a challenging task due to significant irregular memory references and workload imbalance across GPUs. These challenges are particularly compounded in the case of Sparse Triangular Solver (SpTRSV), which introduces additional complexity of two-dimensional computation dependencies among subsequent computation steps. Dependency information may need to be exchanged and shared among GPUs, thus warranting for efficient memory allocation, data partitioning, and workload distribution as well as fine-grained communication and synchronization support. In this work, we focus on designing algorithm for SpTRSV in a single-node, multi-GPU setting. We demonstrate that directly adopting unified memory can adversely affect the performance of SpTRSV on multi-GPU architectures, despite linking via fast interconnect like NVLinks and NVSwitches. Alternatively, we employ the latest NVSHMEM technology based on Partitioned Global Address Space programming model to enable efficient fine-grained communication and drastic synchronization overhead reduction. Furthermore, to handle workload imbalance, we propose a malleable task-pool execution model which can further enhance the utilization of GPUs. By applying these techniques, our experiments on the NVIDIA multi-GPU supernode V100-DGX-1 and DGX-2 systems demonstrate that our design can achieve an average of 3.53 × (up to 9.86 ×) speedup on a DGX-1 system and 3.66 × (up to 9.64 ×) speedup on a DGX-2 system with four GPUs over the Unified-Memory design. The comprehensive sensitivity and scalability studies also show that the proposed zero-copy SpTRSV is able to fully utilize the computing and communication resources of the multi-GPU systems.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3472456.3472478',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Application of VR technology in architectural decoration engineering technology',\n",
       "  'authors': \"['Ran Yin']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': 'AIAM2021: 2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture',\n",
       "  'abstract': 'With the rapid development of social economy and the increasing complexity of architectural decoration projects, there is an urgent need to introduce new technologies to improve work efficiency and solve practical problems. Nowadays, the emerging VR technology, a computer-aided technology, has been widely used in actual engineering projects and has proven to greatly improve work efficiency. This article will discuss the specific application and operation of VR technology in architectural decoration engineering technology, analyze the application advantages of VR technology and look forward to its application prospects in architectural decoration engineering technology.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3495018.3501213',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'FlexMiner: a pattern-aware accelerator for graph pattern mining',\n",
       "  'authors': \"['Xuhao Chen', 'Tianhao Huang', 'Shuotao Xu', 'Thomas Bourgeat', 'Chanwoo Chung', 'Arvind']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': 'Graph pattern mining (GPM) is a class of algorithms widely used in many real-world applications in bio-medicine, e-commerce, security, social sciences, etc. GPM is a computationally intensive problem with an enormous amount of coarse-grain parallelism and therefore, attractive for hardware acceleration. Unfortunately, existing GPM accelerators have not used the best known algorithms and optimizations, and thus offer questionable benefits over software implementations. We present FlexMiner, a software/hardware co-designed GPM accelerator that improves the efficiency without compromising the generality or productivity of state-of-the-art software GPM frameworks. FlexMiner exploits massive amount of coarse-grain parallelism in GPM by deploying a large number of specialized processing elements. For efficient searches, the FlexMiner hardware accepts pattern-specific execution plans, which are generated automatically by the FlexMiner compiler from the given pat-tern(s). To avoid repetitive computation on neighborhood connectivity, we provide dedicated on-chip storage to memoize reusable connectivity information in a connectivity map (c-map) which is implemented with low-cost yet high-throughput hardware. The on-chip memories in FlexMiner are managed dynamically using heuristics derived by the compiler, and thus are fully utilized. We have evaluated FlexMiner with 4 GPM applications on a wide range of real-world graphs. Our cycle-accurate simulation shows that FlexMiner with 64 PEs achieves 10.6× speedup on average over the state-of-the-art software system executing 20 threads on a 10-core Intel CPU.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00052',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Audio-Frequency Induction Loops (AFILs) as a Design Materialfor Architectural Interactivity: An Illustrated Guide',\n",
       "  'authors': \"['Eleni Economidou', 'Bart Hengeveld', 'Moritz Kubesch', 'Alina Krischkowsky', 'Martin Murer', 'Manfred Tscheligi']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"DIS '21: Proceedings of the 2021 ACM Designing Interactive Systems Conference\",\n",
       "  'abstract': 'Audio-frequency induction loops (AFILs) are commonly used as an assistive listening technology for hard-of-hearing individuals. They generate an electromagnetic field proportional to a sound source receivable by hearing aids. Our interactive system, the Sound of Space, is based on AFILs that generate a multi-dimensional soundscape in space. Cochlear implant (CI) listeners and hearing-aids wearers can experience the soundscape through bodily movement, whereas hearing individuals can experience it via a corresponding tangible device. While typical AFIL installations transmit a single sound source, in our interactive system we implement overlapping loops and their interference to locate multiple synchronised audio sources (i.e., corresponding electromagnetic fields) in space. The designed system is installed permanently in an integrative school for deaf, hard-of-hearing and hearing students and teachers. In this pictorial, we illustrate our design and implementation process and contribute our learnings of using AFILs as a design material for architectural interactivity.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3461778.3462070',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A study of work distribution and contention in database primitives on heterogeneous CPU/GPU architectures',\n",
       "  'authors': \"['Michael Gowanlock', 'Zane Fink', 'Ben Karsin', 'Jordan Wright']\",\n",
       "  'date': 'March 2021',\n",
       "  'source': \"SAC '21: Proceedings of the 36th Annual ACM Symposium on Applied Computing\",\n",
       "  'abstract': 'Graphics Processing Units (GPUs) provide very high on-card memory bandwidth which can be exploited to address data-intensive workloads. To maximize algorithm throughput, it is important to concurrently utilize both the CPU and GPU to carry out database queries. We select data-intensive algorithms that are common in databases and data analytic applications including: (i) scan; (ii) batched predecessor searches; (iii) multiway merging; and, (iv) partitioning. For each algorithm, we examine the performance of parallel CPU/GPU-only, and hybrid CPU/GPU approaches. There are several challenges to combining the CPU and GPU for query processing, including distributing work between architectures. We demonstrate that despite being able to accurately split the work between the CPU and GPU, contention for memory bandwidth is a major limiting factor for hybrid CPU/GPU data-intensive algorithms. We employ performance models that allow us to explore several research questions. We find that while hybrid data-intensive algorithms may be limited by contention, these algorithms are more robust to workload characteristics; therefore, they are preferable to CPU/GPU-only approaches. We also find that hybrid algorithms achieve good performance when there is low memory contention between the CPU and GPU, such that the GPU can perform its operations without significantly reducing CPU throughput.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3412841.3441913',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Blockchain Technology as A Guarantee of Transparency in The Supply Chain of Commercial Enterprises',\n",
       "  'authors': \"['Jonathan Lenge', 'Kaninda Musumbu', 'Geisla Wanuku', 'Pascal Sungu']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"ASSE' 22: 2022 3rd Asia Service Sciences and Software Engineering Conference\",\n",
       "  'abstract': 'Traceability has become an essential element of supply chain management, especially in safety-sensitive sectors such as food, pharmaceuticals, etc. Upstream (manufacturers, producers, etc.) and downstream (distributors, wholesalers, etc.) supply chain actors need to store and process traceability information in order to provide proof of regulatory compliance to national authorities as well as the most demanding customers. Studies have shown that developing countries, especially on the African continent, are particularly prone to the commercialization of counterfeit products and the use of diverted consumables, thus reducing the confidence in the various products and services offered by commercial enterprises, not only towards consumers but also towards companies and other actors of the concerned sectors. Therefore, the main objective of this paper is to examine the characteristics and functionalities of blockchain technology and to identify blockchain-based solutions to address consumer product traceability issues, as well as to highlight the benefits of implementing blockchain-based traceability systems by proposing a blockchain architecture to ensure traceability and transparency.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3523181.3523182',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Achieving High In Situ Training Accuracy and Energy Efficiency with Analog Non-Volatile Synaptic Devices',\n",
       "  'authors': \"['Shanshi Huang', 'Xiaoyu Sun', 'Xiaochen Peng', 'Hongwu Jiang', 'Shimeng Yu']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Design Automation of Electronic Systems',\n",
       "  'abstract': 'On-device embedded artificial intelligence prefers the adaptive learning capability when deployed in the field, and thus in situ training is required. The compute-in-memory approach, which exploits the analog computation within the memory array, is a promising solution for deep neural network (DNN) on-chip acceleration. Emerging non-volatile memories are of great interest, serving as analog synapses due to their multilevel programmability. However, the asymmetry and nonlinearity in the conductance tuning remain grand challenges for achieving high in situ training accuracy. In addition, analog-to-digital converters at the edge of the memory array introduce quantization errors. In this work, we present an algorithm-hardware co-optimization to overcome these challenges. We incorporate the device/circuit non-ideal effects into the DNN propagation and weight update steps. By introducing the adaptive “momentum” in the weight update rule, in situ training accuracy on CIFAR-10 could approach its software baseline even under severe asymmetry/nonlinearity and analog-to-digital converter quantization error. The hardware performance of the on-chip training architecture and the overhead for adding “momentum” are also evaluated. By optimizing the backpropagation dataflow, 23.59 TOPS/W training energy efficiency (12× improvement compared to naïve dataflow) is achieved. The circuits that handle “momentum” introduce only 4.2% energy overhead. Our results show great potential and more relaxed requirements that enable emerging non-volatile memories for DNN acceleration on the embedded artificial intelligence platforms.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3500929',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'First Step Towards μNet: Open-Access Aquatic Testbeds and Robotic Ecosystem',\n",
       "  'authors': \"['Scott Mayberry', 'Junkai Wang', 'Qiuyang Tao', 'Fumin Zhang', 'Aijun Song', 'Xiaoyan Hong', 'Shuai Dong', 'Connor Webb', 'Dmitrii Dugaev', 'Zheng Peng']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"WUWNet '21: Proceedings of the 15th International Conference on Underwater Networks &amp; Systems\",\n",
       "  'abstract': 'Aquatic-based research requires a special set of facilities, expertise, and supporting personnel, all of which are costly and often not accessible to small research groups. A shared research infrastructure, particularly testbeds and open-access software, would lower the barrier to entry, decrease implementation time, and mitigate the risk of failure in harsh underwater environments. We have made the first step towards developing μNet, an open-access aquatic testbed and robotic ecosystem addressing the need for shared infrastructure. Our contributions include the initial steps towards an indoor testbed, an outdoor testbed, and an open-access software suite.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3491315.3491322',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'FCOSMask: Fully Convolutional One-Stage Face Mask Wearing Detection Based on MobileNetV3',\n",
       "  'authors': \"['Yang Yu', 'Jie Lu', 'Chao Huang', 'Bo Xiao']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"CSAE '21: Proceedings of the 5th International Conference on Computer Science and Application Engineering\",\n",
       "  'abstract': 'Wearing masks correctly in public is one major self-prevention method against the worldwide Coronavirus disease 2019 (COVID-19). This paper proposes FCOSMask, a fully convolutional one-stage face mask wearing detector based on the lightweight network, for emergency epidemic control and long-term epidemic prevention work. MobileNetV3 is applied as the backbone network to reduce computational overhead. Thus, complex calculation related to anchor boxes is avoided in the anchor-free method, and Complete Intersection over Union (CIoU) loss is selected as the bounding box regression loss function to speed up model convergence. Experiments show that compared to other anchor-based methods, detection speed of FCOSMask is improved around 3 to 4 times on self-established datasets and mean average precision (mAP) achieves 92.4%, which meets the accuracy and real-time requirements of the face mask wearing detection task in most public areas. Finally, a Web-based face mask wearing system is developed that can support public epidemic prevention and control management.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3487075.3487078',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'FINGERS: exploiting fine-grained parallelism in graph mining accelerators',\n",
       "  'authors': \"['Qihang Chen', 'Boyu Tian', 'Mingyu Gao']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'Graph mining is an emerging application of high importance and also with high complexity, thus requiring efficient hardware acceleration. Current accelerator designs only utilize coarse-grained parallelism, leaving large room for further optimizations. Our key insight is to fully exploit fine-grained parallelism to overcome the existing issues of hardware underutilization, inefficient resource provision, and limited single-thread performance under imbalanced loads. Targeting pattern-aware graph mining algorithms, we first comprehensively identify and analyze the abundant fine-grained parallelism at the branch, set, and segment levels during search tree exploration and set operations. We then propose a novel graph mining accelerator, FINGERS, which effectively exploits these multiple levels of fine-grained parallelism to achieve significant performance improvements. FINGERS mainly enhances the design of each single processing element with parallel compute units for set operations, and efficient techniques for task scheduling, load balancing, and data aggregation. FINGERS outperforms the state-of-the-art design by 2.8× on average and up to 8.9× with the same chip area. We also demonstrate that different patterns and different graphs exhibit drastically different parallelism opportunities, justifying the necessity of exploiting all levels of fine-grained parallelism in FINGERS.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503222.3507730',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'MT-DLA: An Efficient Multi-Task Deep Learning Accelerator Design',\n",
       "  'authors': \"['Mengdi Wang', 'Bing Li', 'Ying Wang', 'Cheng Liu', 'Xiaohan Ma', 'Xiandong Zhao', 'Lei Zhang']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"GLSVLSI '21: Proceedings of the 2021 on Great Lakes Symposium on VLSI\",\n",
       "  'abstract': 'Multi-task learning systems are commonly adopted in many real-world AI applications such as intelligent robots and self-driving vehicles. Instead of improving single-network performance, this work proposes a specialized Multi-Task Deep Learning Accelerator architecture, MT-DLA, to improve the performance of concurrent networks by exploiting the shared feature and parameters across these models. It is shown in our evaluation with realistic multi-task workloads, MT-DLA dramatically eliminates the memory and computation overhead caused by the shared parameters, activations and computation result. In the experiments with real-world multi-task learning workloads, MT-DLA brings about 1.4x-7.0x energy efficiency boost when compared to the baseline neural network accelerator without multi-task support.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3453688.3461514',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A full-stack search technique for domain optimized deep learning accelerators',\n",
       "  'authors': \"['Dan Zhang', 'Safeen Huda', 'Ebrahim Songhori', 'Kartik Prabhu', 'Quoc Le', 'Anna Goldie', 'Azalia Mirhoseini']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'The rapidly-changing deep learning landscape presents a unique opportunity for building inference accelerators optimized for specific datacenter-scale workloads. We propose Full-stack Accelerator Search Technique (FAST), a hardware accelerator search framework that defines a broad optimization environment covering key design decisions within the hardware-software stack, including hardware datapath, software scheduling, and compiler passes such as operation fusion and tensor padding. In this paper, we analyze bottlenecks in state-of-the-art vision and natural language processing (NLP) models, including EfficientNet and BERT, and use FAST to design accelerators capable of addressing these bottlenecks. FAST-generated accelerators optimized for single workloads improve Perf/TDP by 3.7× on average across all benchmarks compared to TPU-v3. A FAST-generated accelerator optimized for serving a suite of workloads improves Perf/TDP by 2.4× on average compared to TPU-v3. Our return on investment analysis shows that FAST-generated accelerators can potentially be practical for moderate-sized datacenter deployments.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503222.3507767',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A Flexible Multichannel EEG Artifact Identification Processor using Depthwise-Separable Convolutional Neural Networks',\n",
       "  'authors': \"['Mohit Khatwani', 'Hasib-Al Rashid', 'Hirenkumar Paneliya', 'Mark Horton', 'Nicholas Waytowich', 'W. David Hairston', 'Tinoosh Mohsenin']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Journal on Emerging Technologies in Computing Systems',\n",
       "  'abstract': 'This article presents an energy-efficient and flexible multichannel Electroencephalogram (EEG) artifact identification network and its hardware using depthwise and separable convolutional neural networks. EEG signals are recordings of the brain activities. EEG recordings that are not originated from cerebral activities are termed artifacts. Our proposed model does not need expert knowledge for feature extraction or pre-processing of EEG data and has a very efficient architecture implementable on mobile devices. The proposed network can be reconfigured for any number of EEG channel and artifact classes. Experiments were done with the proposed model with the goal of maximizing the identification accuracy while minimizing the weight parameters and required number of operations. Our proposed network achieves 93.14% classification accuracy using an EEG dataset collected by 64-channel BioSemi ActiveTwo headsets, averaged across 17 patients and 10 artifact classes. Our hardware architecture is fully parameterized with number of input channels, filters, depth, and data bit-width. The number of processing engines (PE) in the proposed hardware can vary between 1 to 16, providing different latency, throughput, power, and energy efficiency measurements. We implement our custom hardware architecture on Xilinx FPGA (Artix-7), which on average consumes 1.4 to 4.7 mJ dynamic energy with different PE configurations. Energy consumption is further reduced by 16.7× implementing on application-specified integrated circuit at the post layout level in 65-nm CMOS technology. Our FPGA implementation is 1.7 × to 5.15 × higher in energy efficiency than some previous works. Moreover, our Application-Specified Integrated Circuit implementation is also 8.47 × to 25.79 × higher in energy efficiency compared to previous works. We also demonstrated that the proposed network is reconfigurable to detect artifacts from another EEG dataset collected in our lab by a 14-channel Emotiv EPOC+ headset and achieved 93.5% accuracy for eye blink artifact detection.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3427471',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Efficient and scalable core multiplexing with M³v',\n",
       "  'authors': \"['Nils Asmussen', 'Sebastian Haas', 'Carsten Weinhold', 'Till Miemietz', 'Michael Roitzsch']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'The M³ system (ASPLOS\\xa0’16) proposed a hardware/software co-design that simplifies integration between general-purpose cores and special-purpose accelerators, allowing users to easily utilize them in a unified manner. M³ is a tiled architecture, whose tiles (cores and accelerators) are partitioned between applications, such that each tile is dedicated to its own application. The M³x system (ATC\\xa0’19) extended M³ by trading off some isolation to enable coarse-grained multiplexing of tiles among multiple applications. With M³x, if source tile t₁ runs code of application p and sends a message m to destination tile t₂ while t₂ is currently not associated with p, then m is forwarded to the right place through a “slow path”, via some special OS tile. In this paper, we present M³v, which extends M³x by further trading off some isolation between applications to support “fast path” communication that does not require the said OS tile’s involvement. Thus, with M³v, a tile can be efficiently multiplexed between applications provided it is a general-purpose core. M³v achieves this goal by 1)\\xa0adding a local multiplexer to each such core, and by 2)\\xa0virtualizing the core’s hardware component responsible for cross-tile communications. We prototype M³v using RISC-V cores on an FPGA platform and show that it significantly outperforms M³x and may achieve competitive performance to Linux.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503222.3507741',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'XJoin: Portable, parallel hash join across diverse XPU architectures with oneAPI',\n",
       "  'authors': \"['Eugenio Marinelli', 'Raja Appuswamy']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"DAMON '21: Proceedings of the 17th International Workshop on Data Management on New Hardware\",\n",
       "  'abstract': 'Modern server hardware is increasingly heterogeneous with a diverse mix of XPU architectures deployed across CPU, GPU, and FPGAs. However, till date, database developers have had to rely on either proprietary, architecture-specific solutions (like CUDA), or low-level, cross-architecture solutions that complicate development (like OpenCL). The lack of portable parallelism caused by the absence of a common high-level programming framework is one of the main reasons preventing a wider adoption of XPUs by database systems. In this paper, we take the first steps towards solving this problem using oneAPI-a cross-industry effort for developing an open, standards-based unified programming model that extends standard C++ to provide portable parallelism across diverse processor architectures. In particular, we port a recently-proposed, highly-optimized, GPU-based hash join algorithm from CUDA to Data Parallel C++ (DPC++). We then execute the hash join on multicore CPUs, integrated GPUs (Intel GEN9), and discrete GPUs (Intel DG1 and NVIDIA GeForce) without changing a single line of kernel code to demonstrate that DPC++ enables portable parallelism. We compare the performance of DPC++ kernels with hand-optimized CUDA kernels and model-based theoretical performance bounds to demonstrate the performance-portability trade off in using DPC++.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3465998.3466012',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Warehouse-scale video acceleration: co-design and deployment in the wild',\n",
       "  'authors': \"['Parthasarathy Ranganathan', 'Daniel Stodolsky', 'Jeff Calow', 'Jeremy Dorfman', 'Marisabel Guevara', 'Clinton Wills Smullen IV', 'Aki Kuusela', 'Raghu Balasubramanian', 'Sandeep Bhatia', 'Prakash Chauhan', 'Anna Cheung', 'In Suk Chong', 'Niranjani Dasharathi', 'Jia Feng', 'Brian Fosco', 'Samuel Foss', 'Ben Gelb', 'Sara J. Gwin', 'Yoshiaki Hase', 'Da-ke He', 'C. Richard Ho', 'Roy W. Huffman Jr.', 'Elisha Indupalli', 'Indira Jayaram', 'Poonacha Kongetira', 'Cho Mon Kyaw', 'Aaron Laursen', 'Yuan Li', 'Fong Lou', 'Kyle A. Lucke', 'JP Maaninen', 'Ramon Macias', 'Maire Mahony', 'David Alexander Munday', 'Srikanth Muroor', 'Narayana Penukonda', 'Eric Perkins-Argueta', 'Devin Persaud', 'Alex Ramirez', 'Ville-Mikko Rautio', 'Yolanda Ripley', 'Amir Salek', 'Sathish Sekar', 'Sergey N. Sokolov', 'Rob Springer', 'Don Stark', 'Mercedes Tan', 'Mark S. Wachsler', 'Andrew C. Walton', 'David A. Wickeraad', 'Alvin Wijaya', 'Hon Kwan Wu']\",\n",
       "  'date': 'April 2021',\n",
       "  'source': \"ASPLOS '21: Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'Video sharing (e.g., YouTube, Vimeo, Facebook, TikTok) accounts for the majority of internet traffic, and video processing is also foundational to several other key workloads (video conferencing, virtual/augmented reality, cloud gaming, video in Internet-of-Things devices, etc.). The importance of these workloads motivates larger video processing infrastructures and – with the slowing of Moore’s law – specialized hardware accelerators to deliver more computing at higher efficiencies. This paper describes the design and deployment, at scale, of a new accelerator targeted at warehouse-scale video transcoding. We present our hardware design including a new accelerator building block – the video coding unit (VCU) – and discuss key design trade-offs for balanced systems at data center scale and co-designing accelerators with large-scale distributed software systems. We evaluate these accelerators “in the wild\" serving live data center jobs, demonstrating 20-33x improved efficiency over our prior well-tuned non-accelerated baseline. Our design also enables effective adaptation to changing bottlenecks and improved failure management, and new workload capabilities not otherwise possible with prior systems. To the best of our knowledge, this is the first work to discuss video acceleration at scale in large warehouse-scale environments.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3445814.3446723',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'E-BATCH: Energy-Efficient and High-Throughput RNN Batching',\n",
       "  'authors': \"['Franyell Silfa', 'Jose Maria Arnau', 'Antonio González']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Architecture and Code Optimization',\n",
       "  'abstract': 'Recurrent Neural Network (RNN) inference exhibits low hardware utilization due to the strict data dependencies across time-steps. Batching multiple requests can increase throughput. However, RNN batching requires a large amount of padding since the batched input sequences may vastly differ in length. Schemes that dynamically update the batch every few time-steps avoid padding. However, they require executing different RNN layers in a short time span, decreasing energy efficiency. Hence, we propose E-BATCH, a low-latency and energy-efficient batching scheme tailored to RNN accelerators. It consists of a runtime system and effective hardware support. The runtime concatenates multiple sequences to create large batches, resulting in substantial energy savings. Furthermore, the accelerator notifies it when the evaluation of an input sequence is done. Hence, a new input sequence can be immediately added to a batch, thus largely reducing the amount of padding. E-BATCH dynamically controls the number of time-steps evaluated per batch to achieve the best trade-off between latency and energy efficiency for the given hardware platform. We evaluate E-BATCH on top of E-PUR and TPU. E-BATCH improves throughput by 1.8× and energy efficiency by 3.6× in E-PUR, whereas in TPU, it improves throughput by 2.1× and energy efficiency by 1.6×, over the state-of-the-art.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3499757',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Instruments of Vision: Eye-Tracking and Robotics as an Embodied Interface',\n",
       "  'authors': \"['Irem Bugdayci', 'Anne-Heloise Dautel', 'Robert Wuss', 'Ruairi Glynn']\",\n",
       "  'date': 'None',\n",
       "  'source': 'Proceedings of the ACM on Computer Graphics and Interactive Techniques',\n",
       "  'abstract': 'In the age of ubiquitous visual technologies and systems, our perceptive apparatuses are constantly challenged, adapted, and shaped by instruments and machines, rendering the observing body as an active site of knowledge. Your Eye\\'s Motion by Luna is an interactive installation that uses real-time eye-tracking to control a robotic creature named Luna (Figure 1). Materializing eye movements through a wondrous spectacle of light, motion, and color, the observer becomes conscious of her gaze enacted and extended by a robotic counterpart. Building on a diverse set of theories and understandings of vision from the fields of cybernetics, visual studies, embodied mind, and more, the project explores how our perceptual apparatuses and bodies are reconfigured in relation to machines and the environment to afford new ways of seeing. Once we see how observing bodies accommodate feedback from actions to cognition, we can uncover the embodied and affective potential of eye movement as an interface for robotics. The curiosity of Luna invests in this potential, articulating a unity between our embodied percepts and machinic environments to create a \"vision machine.\"',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3465618',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Deploying Multi-tenant FPGAs within Linux-based Cloud Infrastructure',\n",
       "  'authors': \"['Joel Mandebi Mbongue', 'Danielle Tchuinkou Kwadjo', 'Alex Shuping', 'Christophe Bobda']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Reconfigurable Technology and Systems',\n",
       "  'abstract': 'Cloud deployments now increasingly exploit Field-Programmable Gate Array (FPGA) accelerators as part of virtual instances. While cloud FPGAs are still essentially single-tenant, the growing demand for efficient hardware acceleration paves the way to FPGA multi-tenancy. It then becomes necessary to explore architectures, design flows, and resource management features that aim at exposing multi-tenant FPGAs to the cloud users. In this article, we discuss a hardware/software architecture that supports provisioning space-shared FPGAs in Kernel-based Virtual Machine (KVM) clouds. The proposed hardware/software architecture introduces an FPGA organization that improves hardware consolidation and support hardware elasticity with minimal data movement overhead. It also relies on VirtIO to decrease communication latency between hardware and software domains. Prototyping the proposed architecture with a Virtex UltraScale+ FPGA demonstrated near specification maximum frequency for on-chip data movement and high throughput in virtual instance access to hardware accelerators. We demonstrate similar performance compared to single-tenant deployment while increasing FPGA utilization, which is one of the goals of virtualization. Overall, our FPGA design achieved about 2× higher maximum frequency than the state of the art and a bandwidth reaching up to 28 Gbps on 32-bit data width.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3474058',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Demystifying the system vulnerability stack: transient fault effects across the layers',\n",
       "  'authors': \"['George Papadimitriou', 'Dimitris Gizopoulos']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': 'In this paper, we revisit the system vulnerability stack for transient faults. We reveal severe pitfalls in widely used vulnerability measurement approaches, which separate the hardware and the software layers. We rely on microarchitecture level fault injection to derive very tight full-system vulnerability measurements. For our architectural and microarchitectural measurements, we employ GeFIN, a state-of-the-art fault injector built on top of the gem5 simulator, while for software level measurements we employ the LLFI fault injector. Analyzing two different Arm ISAs and two different microarchitectures for each ISA, we quantify the sources and the magnitude of error of architecture and software level vulnerability evaluation methods, which aim to reproduce the effects of hardware faults. We show that widely applied methodologies for system resilience evaluation fail to capture important fault manifestation and propagation aspects and lead to misleading findings, which report opposite vulnerability results than a comprehensive cross-layer analysis. To justify the validity of our findings we employ a state-of-the-art software-based fault tolerance technique and evaluate its impact at all layers through a case study. Our evaluation shows that although higher-level methods can report significant vulnerability improvements (up to 3.8x vulnerability reduction), the actual cross-layer vulnerability of the protected system can be degraded (increased) by up to 30% for the selected benchmarks. Our analysis firmly suggests that only accurate methodologies for full-system vulnerability evaluation of a microprocessor can guide informed transient faults protection decisions either at the hardware or at the software layer.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00075',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'TENET: a framework for modeling tensor dataflow based on relation-centric notation',\n",
       "  'authors': \"['Liqiang Lu', 'Naiqing Guan', 'Yuyue Wang', 'Liancheng Jia', 'Zizhang Luo', 'Jieming Yin', 'Jason Cong', 'Yun Liang']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': 'Accelerating tensor applications on spatial architectures provides high performance and energy-efficiency, but requires accurate performance models for evaluating various dataflow alternatives. Such modeling relies on the notation of tensor dataflow and the formulation of performance metrics. Recent proposed compute-centric and data-centric notations describe the dataflow using imperative directives. However, these two notations are less expressive and thus lead to limited optimization opportunities and inaccurate performance models. In this paper, we propose a framework TENET that models hardware dataflow of tensor applications. We start by introducing a relation-centric notation, which formally describes the hardware dataflow for tensor computation. The relation-centric notation specifies the hardware dataflow, PE interconnection, and data assignment in a uniform manner using relations. The relation-centric notation is more expressive than the compute-centric and data-centric notations by using more sophisticated affine transformations. Another advantage of relation-centric notation is that it inherently supports accurate metrics estimation, including data reuse, bandwidth, latency, and energy. TENET computes each performance metric by counting the relations using integer set structures and operators. Overall, TENET achieves 37.4% and 51.4% latency reduction for CONV and GEMM kernels compared with the state-of-the-art data-centric notation by identifying more sophisticated hardware dataflows.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00062',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A novel network fabric for efficient spatio-temporal reduction in flexible DNN accelerators',\n",
       "  'authors': \"['Francisco Muñoz-Martínez', 'José L. Abellán', 'Manuel E. Acacio', 'Tushar Krishna']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"NOCS '21: Proceedings of the 15th IEEE/ACM International Symposium on Networks-on-Chip\",\n",
       "  'abstract': 'Increasing deployment of Deep Neural Networks (DNNs) in a myriad of applications, has recently fueled interest in the development of specific accelerator architectures capable of meeting their stringent performance and energy consumption requirements. DNN accelerators use three separate NoCs within the accelerator, namely distribution, multiplier and reduction networks (or DN, MN and RN, respectively) between the global buffer(s) and compute units (multipliers/adders). These NoCs enable data delivery, and more importantly, on-chip reuse of operands and outputs to minimize the expensive off-chip memory accesses. Among them, the RN, used to generate and reduce the partial sums produced during DNN processing, is what implies the largest fraction of chip area (25% of the total chip area in some cases) and power dissipation (38% of the total chip power budget), thus representing a first-order driver of the energy efficiency of the accelerator. RNs can be orchestrated to exploit a Temporal, Spatial or Spatio-Temporal reduction dataflow. Among these, the latter is the one that has shown superior performance. However, as we demonstrate in this work, a state-of-the-art implementation of the Spatio-Temporal reduction dataflow, based on the addition of Accumulators (Ac) to the RN (i.e. RN+Ac strategy), can result into significant area and energy expenses. To cope with this important issue, we propose STIFT (that stands for Spatio-Temporal Integrated Folding Tree) that implements the Spatio-Temporal reduction dataflow entirely on the RN hardware substrate (i.e. without the need of the extra accumulators). STIFT results into significant area and power savings regarding the more complex RN+Ac strategy, at the same time its performance advantage is preserved.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3479876.3481602',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'FORMS: fine-grained polarized ReRAM-based in-situ computation for mixed-signal DNN accelerator',\n",
       "  'authors': \"['Geng Yuan', 'Payman Behnam', 'Zhengang Li', 'Ali Shafiee', 'Sheng Lin', 'Xiaolong Ma', 'Hang Liu', 'Xuehai Qian', 'Mahdi Nazm Bojnordi', 'Yanzhi Wang', 'Caiwen Ding']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': 'Recent work demonstrated the promise of using resistive random access memory (ReRAM) as an emerging technology to perform inherently parallel analog domain in-situ matrix-vector multiplication---the intensive and key computation in deep neural networks (DNNs). One key problem is the weights that are signed values. However, in a ReRAM crossbar, weights are stored as conductance of the crossbar cells, and the in-situ computation assumes all cells on each crossbar column are of the same sign. The current architectures either use two ReRAM crossbars for positive and negative weights (PRIME), or add an offset to weights so that all values become positive (ISAAC). Neither solution is ideal: they either double the cost of crossbars, or incur extra offset circuity. To better address this problem, we propose FORMS, a fine-grained ReRAM-based DNN accelerator with algorithm/hardware co-design. Instead of trying to represent the positive/negative weights, our key design principle is to enforce exactly what is assumed in the in-situ computation---ensuring that all weights in the same column of a crossbar have the same sign. It naturally avoids the cost of an additional crossbar. Such polarized weights can be nicely generated using alternating direction method of multipliers (ADMM) regularized optimization during the DNN training, which can exactly enforce certain patterns in DNN weights. To achieve high accuracy, we divide the crossbar into logical sub-arrays and only enforce this property within the fine-grained sub-array columns. Crucially, the small sub-arrays provides a unique opportunity for input zero-skipping, which can significantly avoid unnecessary computations and reduce computation time. At the same time, it also makes the hardware much easier to implement and is less susceptible to non-idealities and noise than coarse-grained architectures. Putting all together, with the same optimized DNN models, FORMS achieves 1.50× and 1.93× throughput improvement in terms of and GOPs/sxmm2 and GOPs/W compared to ISAAC, and 1.12× ~ 2.4× speed up in terms of frame per second over optimized ISAAC with almost the same power/area cost. Interestingly, FORMS optimization framework can even speed up the original ISAAC from 10.7× up to 377.9×, reflecting the importance of software/hardware co-design optimizations.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00029',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'S2Dedup: SGX-enabled secure deduplication',\n",
       "  'authors': \"['Mariana Miranda', 'Tânia Esteves', 'Bernardo Portela', 'João Paulo']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"SYSTOR '21: Proceedings of the 14th ACM International Conference on Systems and Storage\",\n",
       "  'abstract': \"Secure deduplication allows removing duplicate content at third-party storage services while preserving the privacy of users' data. However, current solutions are built with strict designs that cannot be adapted to storage service and applications with different security and performance requirements. We present S2Dedup, a trusted hardware-based privacy-preserving deduplication system designed to support multiple security schemes that enable different levels of performance, security guarantees and space savings. An in-depth evaluation shows these trade-offs for the distinct Intel SGX-based secure schemes supported by our prototype. Moreover, we propose a novel Epoch and Exact Frequency scheme that prevents frequency analysis leakage attacks present in current deterministic approaches for secure deduplication while maintaining similar performance and space savings to state-of-the-art approaches.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3456727.3463773',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Sextans: A Streaming Accelerator for General-Purpose Sparse-Matrix Dense-Matrix Multiplication',\n",
       "  'authors': \"['Linghao Song', 'Yuze Chi', 'Atefeh Sohrabizadeh', 'Young-kyu Choi', 'Jason Lau', 'Jason Cong']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"FPGA '22: Proceedings of the 2022 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays\",\n",
       "  'abstract': 'Sparse-Matrix Dense-Matrix multiplication (SpMM) is the key operator for a wide range of applications including scientific computing, graph processing, and deep learning. Architecting accelerators for SpMM is faced with three challenges - (1) the random memory accessing and unbalanced load in processing because of random distribution of elements in sparse matrices, (2) inefficient data handling of the large matrices which can not be fit on-chip, and (3) a non-general-purpose accelerator design where one accelerator can only process a fixed-size problem. In this paper, we present Sextans, an accelerator for general-purpose SpMM processing. Sextans accelerator features (1) fast random access using on-chip memory, (2) streaming access to off-chip large matrices, (3) PE-aware non-zero scheduling for balanced workload with an II=1 pipeline, and (4) hardware flexibility to enable prototyping the hardware once to support SpMMs of different size as a general-purpose accelerator. We leverage high bandwidth memory (HBM) for the efficient accessing of both sparse and dense matrices. In the evaluation, we present an FPGA prototype Sextans which is executable on a Xilinx U280 HBM FPGA board and a projected prototype Sextans-P with higher bandwidth competitive to V100 and more frequency optimization. We conduct a comprehensive evaluation on 1,400 SpMMs on a wide range of sparse matrices including 50 matrices from SNAP and 150 from SuiteSparse. We compare Sextans with NVIDIA K80 and V100 GPUs. Sextans achieves a 2.50x geomean speedup over K80 GPU, and Sextans-P achieves a 1.14x geomean speedup over V100 GPU (4.94x over K80). The code is available at https://github.com/linghaosong/Sextans.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3490422.3502357',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'NN-baton: DNN workload orchestration and chiplet granularity exploration for multichip accelerators',\n",
       "  'authors': \"['Zhanhong Tan', 'Hongyu Cai', 'Runpei Dong', 'Kaisheng Ma']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': 'The revolution of machine learning poses an unprecedented demand for computation resources, urging more transistors on a single monolithic chip, which is not sustainable in the Post-Moore era. The multichip integration with small functional dies, called chiplets, can reduce the manufacturing cost, improve the fabrication yield, and achieve die-level reuse for different system scales. DNN workload mapping and hardware design space exploration on such multichip systems are critical, but missing in the current stage. This work provides a hierarchical and analytical framework to describe the DNN mapping on a multichip accelerator and analyze the communication overhead. Based on this framework, we propose an automatic tool called NN-Baton with a pre-design flow and a post-design flow. The pre-design flow aims to guide the chiplet granularity exploration with given area and performance budgets for the target workload. The post-design flow focuses on the workload orchestration on different computation levels - package, chiplet, and core - in the hierarchy. Compared to Simba, NN-Baton generates mapping strategies that save 22.5%~44% energy under the same computation and memory configurations. The architecture exploration demonstrates that area is a decisive factor for the chiplet granularity. For a 2048-MAC system under a 2 mm2 chiplet area constraint, the 4-chiplet implementation with 4 cores and 16 lanes of 8-size vector-MAC is always the top-pick computation allocation across several benchmarks. In contrast, the optimal memory allocation policy in the hierarchy typically depends on the neural network models.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00083',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'DOTA: detect and omit weak attentions for scalable transformer acceleration',\n",
       "  'authors': \"['Zheng Qu', 'Liu Liu', 'Fengbin Tu', 'Zhaodong Chen', 'Yufei Ding', 'Yuan Xie']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'Transformer Neural Networks have demonstrated leading performance in many applications spanning over language understanding, image processing, and generative modeling. Despite the impressive performance, long-sequence Transformer processing is expensive due to quadratic computation complexity and memory consumption of self-attention. In this paper, we present DOTA, an algorithm-architecture co-design that effectively addresses the challenges of scalable Transformer inference. Based on the insight that not all connections in an attention graph are equally important, we propose to jointly optimize a lightweight Detector with the Transformer model to accurately detect and omit weak connections during runtime. Furthermore, we design a specialized system architecture for end-to-end Transformer acceleration using the proposed attention detection mechanism. Experiments on a wide range of benchmarks demonstrate the superior performance of DOTA over other solutions. In summary, DOTA achieves 152.6x and 4.5x performance speedup and orders of magnitude energy-efficiency improvements over GPU and customized hardware, respectively.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503222.3507738',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Accelerating On-Chip Training with Ferroelectric-Based Hybrid Precision Synapse',\n",
       "  'authors': \"['Yandong Luo', 'Panni Wang', 'Shimeng Yu']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Journal on Emerging Technologies in Computing Systems',\n",
       "  'abstract': 'In this article, we propose a hardware accelerator design using ferroelectric transistor (FeFET)-based hybrid precision synapse (HPS) for deep neural network (DNN) on-chip training. The drain erase scheme for FeFET programming is incorporated for both FeFET HPS design and FeFET buffer design. By using drain erase, high-density FeFET buffers can be integrated onchip to store the intermediate input-output activations and gradients, which reduces the energy consuming off-chip DRAM access. Architectural evaluation results show that the energy efficiency could be improved by 1.2× ∼ 2.1×, 3.9× ∼ 6.0× compared to the other HPS-based designs and emerging non-volatile memory baselines, respectively. The chip area is reduced by 19% ∼ 36% compared with designs using SRAM on-chip buffer even though the capacity of FeFET buffer is increased. Besides, by utilizing drain erase scheme for FeFET programming, the chip area is reduced by 11% ∼ 28.5% compared with the designs using body erase scheme.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3473461',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A RISC-V in-network accelerator for flexible high-performance low-power packet processing',\n",
       "  'authors': \"['Salvatore Di Girolamo', 'Andreas Kurth', 'Alexandru Calotoiu', 'Thomas Benz', 'Timo Schneider', 'Jakub Beránek', 'Luca Benini', 'Torsten Hoefler']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': 'The capacity of offloading data and control tasks to the network is becoming increasingly important, especially if we consider the faster growth of network speed when compared to CPU frequencies. In-network compute alleviates the host CPU load by running tasks directly in the network, enabling additional computation/communication overlap and potentially improving overall application performance. However, sustaining bandwidths provided by next-generation networks, e.g., 400 Gbit/s, can become a challenge. sPIN is a programming model for in-NIC compute, where users specify handler functions that are executed on the NIC, for each incoming packet belonging to a given message or flow. It enables a CUDA-like acceleration, where the NIC is equipped with lightweight processing elements that process network packets in parallel. We investigate the architectural specialties that a sPIN NIC should provide to enable high-performance, low-power, and flexible packet processing. We introduce PsPIN, a first open-source sPIN implementation, based on a multi-cluster RISC-V architecture and designed according to the identified architectural specialties. We investigate the performance of PsPIN with cycle-accurate simulations, showing that it can process packets at 400 Gbit/s for several use cases, introducing minimal latencies (26 ns for 64 B packets) and occupying a total area of 18.5 mm2 (22 nm FDSOI).',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00079',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Patterns for Blockchain-Based Payment Applications',\n",
       "  'authors': \"['Qinghua Lu', 'Xiwei Xu', 'H.M.N. Dilum Bandara', 'Shiping Chen', 'Liming Zhu']\",\n",
       "  'date': 'July 2021',\n",
       "  'source': \"EuroPLoP'21: 26th European Conference on Pattern Languages of Programs\",\n",
       "  'abstract': 'As the killer application of blockchain technology, blockchain-based payments have attracted extensive attention ranging from hobbyists to corporates to regulatory bodies. Blockchain facilitates fast, secure, and cross-border payments without the need for intermediaries such as banks. Because blockchain technology is still emerging, systematically organised knowledge providing a holistic and comprehensive view on designing payment applications that use blockchain is yet to be established. If such knowledge could be established in the form of a set of blockchain-specific patterns, architects could use those patterns in designing a payment application that leverages blockchain. Therefore, in this paper, we first identify a token’s lifecycle and then present 12 patterns that cover critical aspects in enabling the state transitions of a token in blockchain-based payment applications. The lifecycle and the annotated patterns provide a payment-focused systematic view of system interactions and a guide to effective use of the patterns.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3489449.3490006',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Enhancing the Security of FPGA-SoCs via the Usage of ARM TrustZone and a Hybrid-TPM',\n",
       "  'authors': \"['Mathieu Gross', 'Konrad Hohentanner', 'Stefan Wiehler', 'Georg Sigl']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Reconfigurable Technology and Systems',\n",
       "  'abstract': 'Isolated execution is a concept commonly used for increasing the security of a computer system. In the embedded world, ARM TrustZone technology enables this goal and is currently used on mobile devices for applications such as secure payment or biometric authentication. In this work, we investigate the security benefits achievable through the usage of ARM TrustZone on FPGA-SoCs. We first adapt Microsoft’s implementation of a firmware Trusted Platform Module (fTPM) running inside ARM TrustZone for the Zynq UltraScale+ platform. This adaptation consists in integrating hardware accelerators available on the device to fTPM’s implementation and to enhance fTPM with an entropy source derived from on-chip SRAM start-up patterns. With our approach, we transform a software implementation of a TPM into a hybrid hardware/software design that could address some of the security drawbacks of the original implementation while keeping its flexibility. To demonstrate the security gains obtained via the usage of ARM TrustZone and our hybrid-TPM on FPGA-SoCs, we propose a framework that combines them for enabling a secure remote bitstream loading. The approach consists in preventing the insecure usages of a bitstream reconfiguration interface that are made possible by the manufacturer and to integrate the interface inside a Trusted Execution Environment.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3472959',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'FleetRec: Large-Scale Recommendation Inference on Hybrid GPU-FPGA Clusters',\n",
       "  'authors': \"['Wenqi Jiang', 'Zhenhao He', 'Shuai Zhang', 'Kai Zeng', 'Liang Feng', 'Jiansong Zhang', 'Tongxuan Liu', 'Yong Li', 'Jingren Zhou', 'Ce Zhang', 'Gustavo Alonso']\",\n",
       "  'date': 'August 2021',\n",
       "  'source': \"KDD '21: Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining\",\n",
       "  'abstract': 'We present FleetRec, a high-performance and scalable recommendation inference system within tight latency constraints. FleetRec takes advantage of heterogeneous hardware including GPUs and the latest FPGAs equipped with high-bandwidth memory. By disaggregating computation and memory to different types of hardware and bridging their connections by high-speed network, FleetRec gains the best of both worlds, and can naturally scale out by adding nodes to the cluster. Experiments on three production models up to 114 GB show that FleetRec outperforms optimized CPU baseline by more than one order of magnitude in terms of throughput while achieving significantly lower latency.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3447548.3467139',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'When climate meets machine learning: edge to cloud ML energy efficiency',\n",
       "  'authors': \"['Diana Marculescu']\",\n",
       "  'date': 'July 2021',\n",
       "  'source': \"ISLPED '21: Proceedings of the ACM/IEEE International Symposium on Low Power Electronics and Design\",\n",
       "  'abstract': 'A large portion of current cloud and edge workloads feature Machine Learning (ML) tasks, thereby requiring a deep understanding of their energy efficiency. While the holy grail for judging the quality of a ML model has largely been testing accuracy, and only recently its resource usage, neither of these metrics translate directly to energy efficiency, runtime, or mobile device battery lifetime. This work uncovers the need for building accurate, platform-specific power and latency models for ML and efficient hardware-aware ML design methodologies, thus allowing machine learners and hardware designers to identify not just the best accuracy ML model configuration, but also those that satisfy given hardware constraints.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISLPED52811.2021.9502472',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Mapping Computations in Heterogeneous Multicore Systems with Statistical Regression on Program Inputs',\n",
       "  'authors': \"['Junio Cezar Ribeiro Da Silva', 'Lorena Leão', 'Vinicius Petrucci', 'Abdoulaye Gamatié', 'Fernando Magno Quintão Pereira']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Embedded Computing Systems',\n",
       "  'abstract': 'A hardware configuration is a set of processors and their frequency levels in a multicore heterogeneous system. This article presents a compiler-based technique to match functions with hardware configurations. Such a technique consists of using multivariate linear regression to associate function arguments with particular hardware configurations. By showing that this classification space tends to be convex in practice, this article demonstrates that linear regression is not only an efficient tool to map computations to heterogeneous hardware, but also an effective one. To demonstrate the viability of multivariate linear regression as a way to perform adaptive compilation for heterogeneous architectures, we have implemented our ideas onto the Soot Java bytecode analyzer. Code that we produce can predict the best configuration for a large class of Java and Scala benchmarks running on an Odroid XU4 big.LITTLE board; hence, outperforming prior techniques such as ARM’s GTS and CHOAMP, a recently released static program scheduler.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3478288',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Programming and Synthesis for Software-defined FPGA Acceleration: Status and Future Prospects',\n",
       "  'authors': \"['Yi-Hsiang Lai', 'Ecenur Ustun', 'Shaojie Xiang', 'Zhenman Fang', 'Hongbo Rong', 'Zhiru Zhang']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Reconfigurable Technology and Systems',\n",
       "  'abstract': 'FPGA-based accelerators are increasingly popular across a broad range of applications, because they offer massive parallelism, high energy efficiency, and great flexibility for customizations. However, difficulties in programming and integrating FPGAs have hindered their widespread adoption. Since the mid 2000s, there has been extensive research and development toward making FPGAs accessible to software-inclined developers, besides hardware specialists. Many programming models and automated synthesis tools, such as high-level synthesis, have been proposed to tackle this grand challenge. In this survey, we describe the progression and future prospects of the ongoing journey in significantly improving the software programmability of FPGAs. We first provide a taxonomy of the essential techniques for building a high-performance FPGA accelerator, which requires customizations of the compute engines, memory hierarchy, and data representations. We then summarize a rich spectrum of work on programming abstractions and optimizing compilers that provide different trade-offs between performance and productivity. Finally, we highlight several additional challenges and opportunities that deserve extra attention by the community to bring FPGA-based computing to the masses.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3469660',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Rethinking Embedded Blocks for Machine Learning Applications',\n",
       "  'authors': \"['Seyedramin Rasoulinezhad', 'Esther Roorda', 'Steve Wilton', 'Philip H. W. Leong', 'David Boland']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Reconfigurable Technology and Systems',\n",
       "  'abstract': 'The underlying goal of FPGA architecture research is to devise flexible substrates that implement a wide variety of circuits efficiently. Contemporary FPGA architectures have been optimized to support networking, signal processing, and image processing applications through high-precision digital signal processing (DSP) blocks. The recent emergence of machine learning has created a new set of demands characterized by: (1) higher computational density and (2) low precision arithmetic requirements. With the goal of exploring this new design space in a methodical manner, we first propose a problem formulation involving computing nested loops over multiply-accumulate (MAC) operations, which covers many basic linear algebra primitives and standard deep neural network (DNN) kernels. A quantitative methodology for deriving efficient coarse-grained compute block architectures from benchmarks is then proposed together with a family of new embedded blocks, called MLBlocks. An MLBlock instance includes several multiply-accumulate units connected via a flexible routing, where each configuration performs a few parallel dot-products in a systolic array fashion. This architecture is parameterized with support for different data movements, reuse, and precisions, utilizing a columnar arrangement that is compatible with existing FPGA architectures. On synthetic benchmarks, we demonstrate that for 8-bit arithmetic, MLBlocks offer 6× improved performance over the commercial Xilinx DSP48E2 architecture with smaller area and delay; and for time-multiplexed 16-bit arithmetic, achieves 2× higher performance per area with the same area and frequency. All source codes and data, along with documents to reproduce all the results in this article, are available at  http://github.com/raminrasoulinezhad/MLBlocks.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3491234',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'The Programmable Data Plane: Abstractions, Architectures, Algorithms, and Applications',\n",
       "  'authors': \"['Oliver Michel', 'Roberto Bifulco', 'Gábor Rétvári', 'Stefan Schmid']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Computing Surveys',\n",
       "  'abstract': 'Programmable data plane technologies enable the systematic reconfiguration of the low-level processing steps applied to network packets and are key drivers toward realizing the next generation of network services and applications. This survey presents recent trends and issues in the design and implementation of programmable network devices, focusing on prominent abstractions, architectures, algorithms, and applications proposed, debated, and realized over the past years. We elaborate on the trends that led to the emergence of this technology and highlight the most important pointers from the literature, casting different taxonomies for the field, and identifying avenues for future research.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3447868',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'BISWSRBS: A Winograd-based CNN Accelerator with a Fine-grained Regular Sparsity Pattern and Mixed Precision Quantization',\n",
       "  'authors': \"['Tao Yang', 'Zhezhi He', 'Tengchuan Kou', 'Qingzheng Li', 'Qi Han', 'Haibao Yu', 'Fangxin Liu', 'Yun Liang', 'Li Jiang']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Reconfigurable Technology and Systems',\n",
       "  'abstract': 'Field-programmable Gate Array\\xa0(FPGA) is a high-performance computing platform for Convolution Neural Networks\\xa0(CNNs) inference. Winograd algorithm, weight pruning, and quantization are widely adopted to reduce the storage and arithmetic overhead of CNNs on FPGAs. Recent studies strive to prune the weights in the Winograd domain, however, resulting in irregular sparse patterns and leading to low parallelism and reduced utilization of resources. Besides, there are few works to discuss a suitable quantization scheme for Winograd.In this article, we propose a regular sparse pruning pattern in the Winograd-based CNN, namely, Sub-row-balanced Sparsity\\xa0(SRBS) pattern, to overcome the challenge of the irregular sparse pattern. Then, we develop a two-step hardware co-optimization approach to improve the model accuracy using the SRBS pattern. Based on the pruned model, we implement a mixed precision quantization to further reduce the computational complexity of bit operations. Finally, we design an FPGA accelerator that takes both the advantage of the SRBS pattern to eliminate low-parallelism computation and the irregular memory accesses, as well as the mixed precision quantization to get a layer-wise bit width. Experimental results on VGG16/VGG-nagadomi with CIFAR-10 and ResNet-18/34/50 with ImageNet show up to 11.8×/8.67× and 8.17×/8.31×/10.6× speedup, 12.74×/9.19× and 8.75×/8.81×/11.1× energy efficiency improvement, respectively, compared with the state-of-the-art dense Winograd accelerator\\xa0[20] with negligible loss of model accuracy. We also show that our design has 4.11× speedup compared with the state-of-the-art sparse Winograd accelerator [19] on VGG16.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3467476',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'EffiCSense: an architectural pathfinding framework for energy-constrained sensor applications',\n",
       "  'authors': \"['Jonah Van Assche', 'Ruben Helsen', 'Georges Gielen']\",\n",
       "  'date': 'March 2022',\n",
       "  'source': \"DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe\",\n",
       "  'abstract': 'This paper introduces EffiCSense, an architectural pathfinding framework for mixed-signal sensor front-ends for both regular and compressive sensing systems. Since sensing systems are often energy constrained, finding a suitable architecture can be a long iterative process between high-level modeling and circuit design. We present a Simulink-based framework that allows for architectural pathfinding with high-level functional models while also including power consumption models of the different circuit blocks. This allows to directly model the impact of design specifications on power consumption and speeds up the overall design process significantly. Both architectures with and without compressive sensing can be handled. The framework is demonstrated for the processing of EEG signals for epilepsy detection, comparing solutions with and without analog compressive sensing. Simulations show that using the compression, an optimal design can be found that is estimated to be 3.6 times more power-efficient compared to a system without compression, consuming 2.44μW for a detection accuracy of 99.3%.',\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3539845.3539888',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'PHiLIP on the HiL: Automated Multi-Platform OS Testing With External Reference Devices',\n",
       "  'authors': \"['Kevin Weiss', 'Michel Rottleuthner', 'Thomas C. Schmidt', 'Matthias Wählisch']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Embedded Computing Systems',\n",
       "  'abstract': 'Developing an operating systems (OSs) for low-end embedded devices requires continuous adaptation to new hardware architectures and components, while serviceability of features needs to be assured for each individual platform under tight resource constraints. It is challenging to design a versatile and accurate heterogeneous test environment that is agile enough to cover a continuous evolution of the code base and platforms. This mission is even more challenging when organized in an agile open-source community process with many contributors such as for the RIOT OS. Hardware in the Loop (HiL) testing and Continuous Integration (CI) are automatable approaches to verify functionality, prevent regressions, and improve the overall quality at development speed in large community projects.In this paper, we present PHiLIP (Primitive Hardware in the Loop Integration Product), an open-source external reference device together with tools that validate the system software while it controls hardware and interprets physical signals. Instead of focusing on a specific test setting, PHiLIP takes the approach of a tool-assisted agile HiL test process, designed for continuous evolution and deployment cycles. We explain its design, describe how it supports HiL tests, evaluate performance metrics, and report on practical experiences of employing PHiLIP in an automated CI test infrastructure. Our initial deployment comprises 22 unique platforms, each of which executes 98 peripheral tests every night. PHiLIP allows for easy extension of low-cost, adaptive testing infrastructures but serves testing techniques and tools to a much wider range of applications.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3477040',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': \"Don't forget the I/O when allocating your LLC\",\n",
       "  'authors': \"['Yifan Yuan', 'Mohammad Alian', 'Yipeng Wang', 'Ren Wang', 'Ilia Kurakin', 'Charlie Tai', 'Nam Sung Kim']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': \"In modern server CPUs, last-level cache (LLC) is a critical hardware resource that exerts significant influence on the performance of the workloads, and how to manage LLC is a key to the performance isolation and QoS in the cloud with multi-tenancy. In this paper, we argue that in addition to CPU cores, high-speed I/O is also important for LLC management. This is because of an Intel architectural innovation - Data Direct I/O (DDIO) - that directly injects the inbound I/O traffic to (part of) the LLC instead of the main memory. We summarize two problems caused by DDIO and show that (1) the default DDIO configuration may not always achieve optimal performance, (2) DDIO can decrease the performance of non-I/O workloads that share LLC with it by as high as 32%. We then present IAT, the first LLC management mechanism that treats the I/O as the first-class citizen. IAT monitors and analyzes the performance of the core/LLC/DDIO using CPU's hardware performance counters and adaptively adjusts the number of LLC ways for DDIO or the tenants that demand more LLC capacity. In addition, IAT dynamically chooses the tenants that share its LLC resource with DDIO to minimize the performance interference by both the tenants and the I/O. Our experiments with multiple microbenchmarks and real-world applications demonstrate that with minimal overhead, IAT can effectively and stably reduce the performance degradation caused by DDIO.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00018',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Parallel Computing of Graph-based Functions in ReRAM',\n",
       "  'authors': \"['Saman Froehlich', 'Saeideh Shirinzadeh', 'Rolf Drechsler']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Journal on Emerging Technologies in Computing Systems',\n",
       "  'abstract': 'Resistive Random Access Memory\\xa0(ReRAM) is an emerging non-volatile memory technology. Besides its low power consumption and its high scalability, its inherent computation capabilities make ReRAM especially interesting for future computer architectures. Merging computations into the memory is a promising solution for overcoming the memory bottleneck. To perform computations in ReRAM, efficient synthesis strategies for Boolean functions have to be developed. In this article, we give a thorough presentation of how to employ parallel computing capabilities of ReRAM for the synthesis of functions given state-of-the-art graph-based representations AIGs or BDDs. Additionally, we introduce a new graph-based representation called m-And-Inverter Graph (m-AIGs), which allows us to fully exploit the computing capabilities of ReRAM. In the simulations, we show that our proposed approaches outperform state-of-the art synthesis strategies, and we show the superiority of m-AIGs over the standard AIG representation for ReRAM-based synthesis.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3453163',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': \"Opening pandora's box: a systematic study of new ways microarchitecture can leak private data\",\n",
       "  'authors': \"['Jose Rodrigo Sanchez Vicarte', 'Pradyumna Shome', 'Nandeeka Nayak', 'Caroline Trippel', 'Adam Morrison', 'David Kohlbrenner', 'Christopher W. Fletcher']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': \"Microarchitectural attacks have plunged Computer Architecture into a security crisis. Yet, as the slowing of Moore's law justifies the use of ever more exotic microarchitecture, it is likely we have only seen the tip of the iceberg. To better anticipate this security crisis, this paper performs a systematic security-centric analysis of the Computer Architecture literature. Our rationale is that when implementing current and future processors, microarchitects will (quite reasonably) look to previously-proposed ideas. Our study uncovers seven classes of microarchitectural optimization with novel security implications, proposes a conceptual framework through which to study them and demonstrates several proofs-of-concept to show their efficacy. The optimizations we study range from those that leak as much privacy as Spectre/Meltdown (but without exploiting speculative execution) to those that otherwise undermine security-critical programs in a variety of ways. Many have storied histories---ranging from industry patents to media/3rd party speculation regarding current implementation status to recent renewed interest in the academic community. This paper's goal is to perform an early (hopefully not too late) analysis to inform their development moving forward.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00035',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'ZeRØ: <u>ze</u>ro-overhead <u>r</u>esilient <u>o</u>peration under pointer integrity attacks',\n",
       "  'authors': \"['Mohamed Tarek Ibn Ziad', 'Miguel A. Arroyo', 'Evgeny Manzhosov', 'Simha Sethumadhavan']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': \"A large class of today's systems require high levels of availability and security. Unfortunately, state-of-the-art security solutions tend to induce crashes and raise exceptions when under attack, trading off availability for security. In this work, we propose ZeRØ, a pointer integrity mechanism that can continue program execution even when under attack. ZeRØ proposes unique memory instructions and a novel metadata encoding scheme to protect code and data pointers. The combination of instructions and metadata allows ZeRØ to avoid explicitly tagging every word in memory, eliminating performance overheads. Moreover, ZeRØ is a deterministic security primitive that requires minor microarchitectural changes. We show that ZeRØ is better than commercially available state-of-the-art hardware primitives, e.g., ARM's Pointer Authentication (PAC), by a significant margin. ZeRØ incurs zero performance overheads on the SPEC CPU2017 benchmarks, and our VLSI measurements show low power and area overheads.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00082',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Energy Efficient Error Resilient Multiplier Using Low-power Compressors',\n",
       "  'authors': \"['Skandha Deepsita S', 'Dhayala Kumar M', 'Noor Mahammad SK']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Design Automation of Electronic Systems',\n",
       "  'abstract': 'The approximate hardware design can save huge energy at the cost of errors incurred in the design. This article proposes the approximate algorithm for low-power compressors, utilized to build approximate multiplier with low energy and acceptable error profiles. This article presents two design approaches (DA1 and DA2) for higher bit size approximate multipliers. The proposed multiplier of DA1 have no propagation of carry signal from LSB to MSB, resulted in a very high-speed design. The increment in delay, power, and energy are not exponential with increment of multiplier size (n) for DA1 multiplier. It can be observed that the maximum combinations lie in the threshold Error Distance of 5% of the maximum value possible for any particular multiplier of size n. The proposed 4-bit DA1 multiplier consumes only 1.3 fJ of energy, which is 87.9%, 78%, 94%, 67.5%, and 58.9% less when compared to M1, M2, LxA, MxA, accurate designs respectively. The DA2 approach is recursive method, i.e., n-bit multiplier built with n/2-bit sub-multipliers. The proposed 8-bit multiplication has 92% energy savings with Mean Relative Error Distance (MRED) of 0.3 for the DA1 approach and at least 11% to 40% of energy savings with MRED of 0.08 for the DA2 approach. The proposed multipliers are employed in the image processing algorithm of DCT, and the quality is evaluated. The standard PSNR metric is 55 dB for less approximation and 35 dB for maximum approximation.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3488837',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'ABC-DIMM: alleviating the bottleneck of communication in DIMM-based near-memory processing with inter-DIMM broadcast',\n",
       "  'authors': \"['Weiyi Sun', 'Zhaoshi Li', 'Shouyi Yin', 'Shaojun Wei', 'Leibo Liu']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': 'Near-Memory Processing (NMP) systems that integrate accelerators within DIMM (Dual-Inline Memory Module) buffer chips potentially provide high performance with relatively low design and manufacturing costs. However, an inevitable communication bottleneck arises when considering the main memory bus among peer DIMMs and the host CPU. This communication bottleneck roots in the bus-based nature and the limited point-to-point communication pattern of the main memory system. The aggregated memory bandwidth of DIMM-based NMP scales with the number of DIMMs. When the number of DIMMs in a channel scales up, the per-DIMM point-to-point communication bandwidth scales down, whereas the computation resources and local memory bandwidth per DIMM stay the same. For many important sparse data-intensive workloads like graph applications and sparse tensor algebra, we identify that communication among DIMMs and the host CPU easily dominates their processing procedure in previous DIMM-based NMP systems, which severely bottlenecks their performance. To tackle this challenge, we propose that inter-DIMM broadcast should be implemented and utilized in the main memory system of DIMM-based NMP. On the hardware side, the main memory bus naturally scales out with broadcast, where per-DIMM effective bandwidth of broadcast remains the same as the number of DIMMs grows. On the software side, many sparse applications can be implemented in a form such that broadcasts dominate their communication. Based on these ideas, we design <u>ABC-DIMM</u>, which Alleviates the Bottleneck of Communication in DIMM-based NMP, consisting of integral broadcast mechanisms and Broadcast-Process programming framework, with minimized modifications to commodity software-hardware stack. Our evaluation shows that ABC-DIMM offers an 8.33X geo-mean speedup over a 16-core CPU baseline, and outperforms two NMP baselines by 2.59X and 2.93X on average.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00027',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'HP-GNN: Generating High Throughput GNN Training Implementation on CPU-FPGA Heterogeneous Platform',\n",
       "  'authors': \"['Yi-Chien Lin', 'Bingyi Zhang', 'Viktor Prasanna']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"FPGA '22: Proceedings of the 2022 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays\",\n",
       "  'abstract': 'Graph Neural Networks (GNNs) have shown great success in many applications such as recommendation systems, molecular property prediction, traffic prediction, etc. Recently, CPU-FPGA heterogeneous platforms have been used to accelerate many applications by exploiting customizable data path and abundant user-controllable on-chip memory resources of FPGAs. Yet, accelerating and deploying GNN training on such platforms requires not only expertise in hardware design but also substantial development efforts. We propose HP-GNN, a novel framework that generates high throughput GNN training implementations on a given CPU-FPGA platform that can benefit both application developers and machine learning researchers. HP-GNN takes GNN training algorithms, GNN models as the inputs, and automatically performs hardware mapping onto the target CPU-FPGA platform. HP-GNN consists of: (1) data layout and internal representation that reduce the memory traffic and random memory accesses; (2) optimized hardware templates that support various GNN models; (3) a design space exploration engine for automatic hardware mapping; (4) high-level application programming interfaces (APIs) that allow users to specify GNN training with only a handful of lines of code. To evaluate HP-GNN, we experiment with two well-known sampling-based GNN training algorithms and two GNN models. For each training algorithm and model, HP-GNN generates implementation on a state-of-the-art CPU-FPGA platform. Compared with CPU-only and CPU-GPU platforms, experimental results show that the generated implementations achieve 55.67x and 2.17x speedup on the average, respectively. Compared with the state-of-the-art GNN training implementations, HP-GNN achieves up to 4.45x speedup.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3490422.3502359',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'High-Performance Match-Action Table Updates from within Programmable Software Data Planes',\n",
       "  'authors': \"['Manuel Simon', 'Henning Stubbe', 'Dominik Scholz', 'Sebastian Gallenmüller', 'Georg Carle']\",\n",
       "  'date': 'December 2021',\n",
       "  'source': \"ANCS '21: Proceedings of the Symposium on Architectures for Networking and Communications Systems\",\n",
       "  'abstract': \"For long, P4's mantra was that table entries could only be updated by the control plane. With the ongoing Portable NIC Architecture (PNA) standardization efforts, this is changing. In fact, PNA presumably includes explicit methods for table updates from within the data planes. Now, it is onto manufacturers and developers to integrate and use this mechanism in future P4 data planes. This would enable novel and improved applications, e.g., requiring means for maintaining state. We present our implementation of flexible match-action tables for the DPDK-based t4p4s target. We discuss different approaches for table updates from within the data plane and challenges that arise when operating at line rate. Further, we analyze the data consistency of our enhanced table structures in a multi-core scenario and model the memory overhead for state management purposes.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3493425.3502759',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'FILM-QNN: Efficient FPGA Acceleration of Deep Neural Networks with Intra-Layer, Mixed-Precision Quantization',\n",
       "  'authors': \"['Mengshu Sun', 'Zhengang Li', 'Alec Lu', 'Yanyu Li', 'Sung-En Chang', 'Xiaolong Ma', 'Xue Lin', 'Zhenman Fang']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"FPGA '22: Proceedings of the 2022 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays\",\n",
       "  'abstract': 'With the trend to deploy Deep Neural Network (DNN) inference models on edge devices with limited resources, quantization techniques have been widely used to reduce on-chip storage and improve computation throughput. However, existing DNN quantization work deploying quantization below 8-bit may be either suffering from evident accuracy loss or facing a big gap between the theoretical improvement of computation throughput and the practical inference speedup. In this work, we propose a general framework, called FILM-QNN, to quantize and accelerate multiple DNN models across different embedded FPGA devices. First, we propose the novel intra-layer, mixed-precision quantization algorithm that assigns different precisions onto the filters of each layer. The candidate precision levels and assignment granularity are determined from our empirical study with the capability of preserving accuracy and improving hardware parallelism. Second, we apply multiple optimization techniques for the FPGA accelerator architecture in support of quantized computations, including DSP packing, weight reordering, and data packing, to enhance the overall throughput with the available resources. Moreover, a comprehensive resource model is developed to balance the allocation of FPGA computation resources (LUTs and DSPs) as well as data transfer and on-chip storage resources (BRAMs) to accelerate the computations in mixed precisions within each layer. Finally, to improve the portability of FILM-QNN, we implement it using Vivado High-Level Synthesis (HLS) on Xilinx PYNQ-Z2 and ZCU102 FPGA boards. Our experimental results of ResNet-18, ResNet-50, and MobileNet-V2 demonstrate that the implementations with intra-layer, mixed-precision (95% of 4-bit weights and 5% of 8-bit weights, and all 5-bit activations) can achieve comparable accuracy (70.47%, 77.25%, and 65.67% for the three models) as the 8-bit (and 32-bit) versions and comparable throughput (214.8 FPS, 109.1 FPS, and 537.9 FPS on ZCU102) as the 4-bit designs.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3490422.3502364',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'IChannels: exploiting current management mechanisms to create covert channels in modern processors',\n",
       "  'authors': \"['Jawad Haj-Yahya', 'Jeremie S. Kim', 'A. Giray Yağlikçi', 'Ivan Puddu', 'Lois Orosa', 'Juan Gómez Luna', 'Mohammed Alser', 'Onur Mutlu']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': \"To operate efficiently across a wide range of workloads with varying power requirements, a modern processor applies different current management mechanisms, which briefly throttle instruction execution while they adjust voltage and frequency to accommodate for power-hungry instructions (PHIs) in the instruction stream. Doing so 1) reduces the power consumption of non-PHI instructions in typical workloads and 2) optimizes system voltage regulators' cost and area for the common use case while limiting current consumption when executing PHIs. However, these mechanisms may compromise a system's confidentiality guarantees. In particular, we observe that multi-level side-effects of throttling mechanisms, due to PHI-related current management mechanisms, can be detected by two different software contexts (i.e., sender and receiver) running on 1) the same hardware thread, 2) co-located Simultaneous Multi-Threading (SMT) threads, and 3) different physical cores. Based on these new observations on current management mechanisms, we develop a new set of covert channels, IChannels, and demonstrate them in real modern Intel processors (which span more than 70% of the entire client and server processor market). Our analysis shows that IChannels provides more than 24X the channel capacity of state-of-the-art power management covert channels. We propose practical and effective mitigations to each covert channel in IChannels by leveraging the insights we gain through a rigorous characterization of real systems.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00081',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'AIgean: An Open Framework for Deploying Machine Learning on Heterogeneous Clusters',\n",
       "  'authors': \"['Naif Tarafdar', 'Giuseppe Di Guglielmo', 'Philip C. Harris', 'Jeffrey D. Krupa', 'Vladimir Loncar', 'Dylan S. Rankin', 'Nhan Tran', 'Zhenbin Wu', 'Qianfeng Shen', 'Paul Chow']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Reconfigurable Technology and Systems',\n",
       "  'abstract': 'AIgean, pronounced like the sea, is an open framework to build and deploy machine learning (ML) algorithms on a heterogeneous cluster of devices (CPUs and FPGAs). We leverage two open source projects: Galapagos, for multi-FPGA deployment, and hls4ml, for generating ML kernels synthesizable using Vivado HLS. AIgean provides a full end-to-end multi-FPGA/CPU implementation of a neural network. The user supplies a high-level neural network description, and our tool flow is responsible for the synthesizing of the individual layers, partitioning layers across different nodes, as well as the bridging and routing required for these layers to communicate. If the user is an expert in a particular domain and would like to tinker with the implementation details of the neural network, we define a flexible implementation stack for ML that includes the layers of Algorithms, Cluster Deployment & Communication, and Hardware. This allows the user to modify specific layers of abstraction without having to worry about components outside of their area of expertise, highlighting the modularity of AIgean. We demonstrate the effectiveness of AIgean with two use cases: an autoencoder, and ResNet-50 running across 10 and 12 FPGAs. AIgean leverages the FPGA’s strength in low-latency computing, as our implementations target batch-1 implementations.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3482854',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Accelerated seeding for genome sequence alignment with enumerated radix trees',\n",
       "  'authors': \"['Arun Subramaniyan', 'Jack Wadden', 'Kush Goliya', 'Nathan Ozog', 'Xiao Wu', 'Satish Narayanasamy', 'David Blaauw', 'Reetuparna Das']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': 'Read alignment is a time-consuming step in genome sequencing analysis. The most widely used software for read alignment, BWA-MEM, and the recently published faster version BWA-MEM2 are based on the seed-and-extend paradigm for read alignment. The seeding step of read alignment is a major bottleneck contributing ~40% to the overall execution time of BWA-MEM2 when aligning whole human genome reads from the Platinum Genomes dataset. This is because both BWA-MEM and BWA-MEM2 use a compressed index structure called the FMD-Index, which results in high bandwidth requirements, primarily due to its character-by-character processing of reads. For instance, to seed each read (101 DNA base-pairs stored in 37.8 bytes), the FMD-Index solution in BWA-MEM2 requires ~68.5 KB of index data. We propose a novel indexing data structure named Enumerated Radix Tree (ERT) and design a custom seeding accelerator based on it. ERT improves bandwidth efficiency of BWA-MEM2 by 4.5X while guaranteeing 100% identical output to the original software, and still fitting in 64 GB DRAM. Overall, the proposed seeding accelerator implemented on AWS F1 FPGA (f1.4xlarge) improves seeding throughput of BWA-MEM2 by 3.3X. When combined with seed-extension accelerators, we observe a 2.1X improvement in overall read alignment throughput over BWA-MEM2. The software implementation of ERT is integrated into BWA-MEM2 (ert branch: https://github.com/bwa-mem2/bwa-mem2/tree/ert) and is open sourced for the benefit of the research community.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00038',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'QUAC-TRNG: high-throughput true random number generation using quadruple row activation in commodity DRAM chips',\n",
       "  'authors': \"['Ataberk Olgun', 'Minesh Patel', 'A. Giray Yağlikçi', 'Haocong Luo', 'Jeremie S. Kim', 'F. Nisa Bostanci', 'Nandita Vijaykumar', 'Oğuz Ergin', 'Onur Mutlu']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': 'True random number generators (TRNG) sample random physical processes to create large amounts of random numbers for various use cases, including security-critical cryptographic primitives, scientific simulations, machine learning applications, and even recreational entertainment. Unfortunately, not every computing system is equipped with dedicated TRNG hardware, limiting the application space and security guarantees for such systems. To open the application space and enable security guarantees for the overwhelming majority of computing systems that do not necessarily have dedicated TRNG hardware (e.g., processing-in-memory systems), we develop QUAC-TRNG, a new high-throughput TRNG that can be fully implemented in commodity DRAM chips, which are key components in most modern systems. QUAC-TRNG exploits the new observation that a carefully-engineered sequence of DRAM commands activates four consecutive DRAM rows in rapid succession. This QUadruple ACtivation (QUAC) causes the bitline sense amplifiers to non-deterministically converge to random values when we activate four rows that store conflicting data because the net deviation in bitline voltage fails to meet reliable sensing margins. We experimentally demonstrate that QUAC reliably generates random values across 136 commodity DDR4 DRAM chips from one major DRAM manufacturer. We describe how to develop an effective TRNG (QUAC-TRNG) based on QUAC. We evaluate the quality of our TRNG using the commonly-used NIST statistical test suite for randomness and find that QUAC-TRNG successfully passes each test. Our experimental evaluations show that QUAC-TRNG reliably generates true random numbers with a throughput of 3.44 Gb/s (per DRAM channel), outperforming the state-of-the-art DRAM-based TRNG by 15.08X and 1.41X for basic and throughput-optimized versions, respectively. We show that QUAC-TRNG utilizes DRAM bandwidth better than the state-of-the-art, achieving up to 2.03X the throughput of a throughput-optimized baseline when scaling bus frequencies to 12 GT/s.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00078',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'The Future of FPGA Acceleration in Datacenters and the Cloud',\n",
       "  'authors': \"['Christophe Bobda', 'Joel Mandebi Mbongue', 'Paul Chow', 'Mohammad Ewais', 'Naif Tarafdar', 'Juan Camilo Vega', 'Ken Eguro', 'Dirk Koch', 'Suranga Handagala', 'Miriam Leeser', 'Martin Herbordt', 'Hafsah Shahzad', 'Peter Hofste', 'Burkhard Ringlein', 'Jakub Szefer', 'Ahmed Sanaullah', 'Russell Tessier']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Reconfigurable Technology and Systems',\n",
       "  'abstract': 'In this article, we survey existing academic and commercial efforts to provide Field-Programmable Gate Array (FPGA) acceleration in datacenters and the cloud. The goal is a critical review of existing systems and a discussion of their evolution from single workstations with PCI-attached FPGAs in the early days of reconfigurable computing to the integration of FPGA farms in large-scale computing infrastructures. From the lessons learned, we discuss the future of FPGAs in datacenters and the cloud and assess the challenges likely to be encountered along the way. The article explores current architectures and discusses scalability and abstractions supported by operating systems, middleware, and virtualization. Hardware and software security becomes critical when infrastructure is shared among tenants with disparate backgrounds. We review the vulnerabilities of current systems and possible attack scenarios and discuss mitigation strategies, some of which impact FPGA architecture and technology. The viability of these architectures for popular applications is reviewed, with a particular focus on deep learning and scientific computing. This work draws from workshop discussions, panel sessions including the participation of experts in the reconfigurable computing field, and private discussions among these experts. These interactions have harmonized the terminology, taxonomy, and the important topics covered in this manuscript.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3506713',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Understanding host network stack overheads',\n",
       "  'authors': \"['Qizhe Cai', 'Shubham Chaudhary', 'Midhul Vuppalapati', 'Jaehyun Hwang', 'Rachit Agarwal']\",\n",
       "  'date': 'August 2021',\n",
       "  'source': \"SIGCOMM '21: Proceedings of the 2021 ACM SIGCOMM 2021 Conference\",\n",
       "  'abstract': 'Traditional end-host network stacks are struggling to keep up with rapidly increasing datacenter access link bandwidths due to their unsustainable CPU overheads. Motivated by this, our community is exploring a multitude of solutions for future network stacks: from Linux kernel optimizations to partial hardware offload to clean-slate userspace stacks to specialized host network hardware. The design space explored by these solutions would benefit from a detailed understanding of CPU inefficiencies in existing network stacks. This paper presents measurement and insights for Linux kernel network stack performance for 100Gbps access link bandwidths. Our study reveals that such high bandwidth links, coupled with relatively stagnant technology trends for other host resources (e.g., CPU speeds and capacity, cache sizes, NIC buffer sizes, etc.), mark a fundamental shift in host network stack bottlenecks. For instance, we find that a single core is no longer able to process packets at line rate, with data copy from kernel to application buffers at the receiver becoming the core performance bottleneck. In addition, increase in bandwidth-delay products have outpaced the increase in cache sizes, resulting in inefficient DMA pipeline between the NIC and the CPU. Finally, we find that traditional loosely-coupled design of network stack and CPU schedulers in existing operating systems becomes a limiting factor in scaling network stack performance across cores. Based on insights from our study, we discuss implications to design of future operating systems, network protocols, and host hardware.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3452296.3472888',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'HW-FlowQ: A Multi-Abstraction Level HW-CNN Co-design Quantization Methodology',\n",
       "  'authors': \"['Nael Fasfous', 'Manoj Rohit Vemparala', 'Alexander Frickenstein', 'Emanuele Valpreda', 'Driton Salihu', 'Nguyen Anh Vu Doan', 'Christian Unger', 'Naveen Shankar Nagaraja', 'Maurizio Martina', 'Walter Stechele']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Embedded Computing Systems',\n",
       "  'abstract': 'Model compression through quantization is commonly applied to convolutional neural networks (CNNs) deployed on compute and memory-constrained embedded platforms. Different layers of the CNN can have varying degrees of numerical precision for both weights and activations, resulting in a large search space. Together with the hardware (HW) design space, the challenge of finding the globally optimal HW-CNN combination for a given application becomes daunting. To this end, we propose HW-FlowQ, a systematic approach that enables the co-design of the target hardware platform and the compressed CNN model through quantization. The search space is viewed at three levels of abstraction, allowing for an iterative approach for narrowing down the solution space before reaching a high-fidelity CNN hardware modeling tool, capable of capturing the effects of mixed-precision quantization strategies on different hardware architectures (processing unit counts, memory levels, cost models, dataflows) and two types of computation engines (bit-parallel vectorized, bit-serial). To combine both worlds, a multi-objective non-dominated sorting genetic algorithm (NSGA-II) is leveraged to establish a Pareto-optimal set of quantization strategies for the target HW-metrics at each abstraction level. HW-FlowQ detects optima in a discrete search space and maximizes the task-related accuracy of the underlying CNN while minimizing hardware-related costs. The Pareto-front approach keeps the design space open to a range of non-dominated solutions before refining the design to a more detailed level of abstraction. With equivalent prediction accuracy, we improve the energy and latency by 20% and 45% respectively for ResNet56 compared to existing mixed-precision search methods.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3476997',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A Novel Memory Management for RISC-V Enclaves',\n",
       "  'authors': \"['Haonan Li', 'Weijie Huang', 'Mingde Ren', 'Hongyi Lu', 'Zhenyu Ning', 'Heming Cui', 'Fengwei Zhang']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"HASP '21: Proceedings of the 10th International Workshop on Hardware and Architectural Support for Security and Privacy\",\n",
       "  'abstract': 'Trusted Execution Environment (TEE) is a popular technology to protect sensitive data and programs. Recent TEEs have proposed the concept of enclaves to execute code processing sensitive data, which cannot be tampered with even by a malicious OS. However, due to hardware limitations and security requirements, existing TEE architectures usually offer limited memory management, such as dynamic memory allocation, defragmentation, etc. In this paper, we present Ashman—a novel software-based memory management extension of TEE on RISC-V, including dynamic memory allocation, migration, and defragmentation. We integrate Ashman into a self-designed TEE and evaluate the performance on a real-world development board. Experimental results have shown that Ashman provides memory management functions similar to native user applications while ensuring enclave security without modifying hardware.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3505253.3505257',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Hetero-ViTAL: a virtualization stack for heterogeneous FPGA clusters',\n",
       "  'authors': \"['Yue Zha', 'Jing Li']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': 'With field-programmable gate arrays (FPGAs) being widely deployed into data centers, an efficient virtualization support is required to fully unleash the potential of cloud FPGAs. Nevertheless, existing FPGA virtualization solutions only support a homogeneous FPGA cluster comprising identical FPGA devices. Representative work such as ViTAL provides sufficient system support for scale-out acceleration and improves the overall resource utilization through a fine-grained spatial sharing. While these existing solutions (including ViTAL) can efficiently virtualize a homogeneous cluster, it is hard to extend them to virtualizing a heterogeneous cluster which comprises multiple types of FPGAs. We expect the future cloud FPGAs are likely to be more heterogeneous due to hardware rolling upgrade. In this paper, we rethink FPGA virtualization from ground up and propose HETERO-VITAL to virtualize heterogeneous FPGA clusters. We identify the conflicting requirements of runtime management and offline compilation when designing the abstraction for a heterogeneous cluster, which is also the fundamental reason why the single-level abstraction as proposed in ViTAL (and other prior works) cannot be trivially extended to the heterogeneous case. To decouple these conflicting requirements, we provide a two-level system abstraction in HETERO-VITAL. Specifically, the high-level abstraction is FPGA-agnostic and provides a simple and homogeneous view of the FPGA resources to simplify the runtime management. On the contrary, the low-level abstraction is FPGA-specific and exposes sufficient spatial resource constraints to the compilation framework to ensure the mapping quality. Rather than simply adding a layer on top of the single-level abstraction as proposed in ViTAL and other prior work, we judiciously determine how much hardware details should be exposed at each level to balance the management complexity, mapping quality and compilation cost. We then develop a compilation framework to map applications onto this two-level abstraction with several optimization techniques to further improve the mapping quality. We also provide a runtime management policy to alleviate the fragmentation issue, which becomes more severe in a heterogeneous cluster due to the distinct resource capacities of diverse FPGAs. We evaluate HETERO-VITAL on a custom-built FPGA cluster and demonstrate its effectiveness using machine learning and image processing applications. Results show that HETERO-VITAL reduces the average response time (a critical metric for QoS) by 79.2% for a heterogeneous cluster compared to the non-virtualized baseline. When virtualizing a homogeneous cluster, HETERO-VITAL also reduces the average response time by 42.0% compared with ViTAL due to a better system design.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00044',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A Survey of Network-on-Chip Security Attacks and Countermeasures',\n",
       "  'authors': \"['Subodha Charles', 'Prabhat Mishra']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Computing Surveys',\n",
       "  'abstract': 'With the advances of chip manufacturing technologies, computer architects have been able to integrate an increasing number of processors and other heterogeneous components on the same chip. Network-on-Chip (NoC) is widely employed by multicore System-on-Chip (SoC) architectures to cater to their communication requirements. NoC has received significant attention from both attackers and defenders. The increased usage of NoC and its distributed nature across the chip has made it a focal point of potential security attacks. Due to its prime location in the SoC coupled with connectivity with various components, NoC can be effectively utilized to implement security countermeasures to protect the SoC from potential attacks. There is a wide variety of existing literature on NoC security attacks and countermeasures. In this article, we provide a comprehensive survey of security vulnerabilities in NoC-based SoC architectures and discuss relevant countermeasures.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3450964',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'ACE-GCN: A Fast Data-driven FPGA Accelerator for GCN Embedding',\n",
       "  'authors': \"['José Romero Hung', 'Chao Li', 'Pengyu Wang', 'Chuanming Shao', 'Jinyang Guo', 'Jing Wang', 'Guoyong Shi']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Reconfigurable Technology and Systems',\n",
       "  'abstract': 'ACE-GCN is a fast and resource/energy-efficient FPGA accelerator for graph convolutional embedding under data-driven and in-place processing conditions. Our accelerator exploits the inherent power law distribution and high sparsity commonly exhibited by real-world graphs datasets. Contrary to other hardware implementations of GCN, on which traditional optimization techniques are employed to bypass the problem of dataset sparsity, our architecture is designed to take advantage of this very same situation. We propose and implement an innovative acceleration approach supported by our “implicit-processing-by-association” concept, in conjunction with a dataset-customized convolutional operator. The computational relief and consequential acceleration effect arise from the possibility of replacing rather complex convolutional operations for a faster embedding result estimation. Based on a computationally inexpensive and super-expedited similarity calculation, our accelerator is able to decide from the automatic embedding estimation or the unavoidable direct convolution operation. Evaluations demonstrate that our approach presents excellent applicability and competitive acceleration value. Depending on the dataset and efficiency level at the target, between 23× and 4,930× PyG baseline, coming close to AWB-GCN by 46% to 81% on smaller datasets and noticeable surpassing AWB-GCN for larger datasets and with controllable accuracy loss levels. We further demonstrate the unique hardware optimization characteristics of our approach and discuss its multi-processing potentiality.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3470536',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A Forward Error Compensation Approach for Fault Resilient Deep Neural Network Accelerator Design',\n",
       "  'authors': \"['Wenye Liu', 'Chip-Hong Chang']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"ASHES '21: Proceedings of the 5th Workshop on Attacks and Solutions in Hardware Security\",\n",
       "  'abstract': 'Deep learning accelerator is a key enabler of a variety of safety-critical applications such as self-driving car and video surveillance. However, recently reported hardware-oriented attack vectors, e.g., fault injection attacks, have extended the threats on deployed deep neural network (DNN) systems beyond the software attack boundary by input data perturbation. Existing fault mitigation schemes including data masking, zeroing-on-error and circuit level time-borrowing techniques exploit the noise-tolerance of neural network models to resist random and sparse errors. Such noise tolerant-based schemes are not sufficiently effective to suppress intensive transient errors if a DNN accelerator is blasted with malicious and deliberate faults. In this paper, we conduct comprehensive investigations on reported resilient designs and propose a more robust countermeasure to fault injection attacks. The proposed design utilizes shadow flip flops for error detection and lightweight circuit for timely error correction. Our forward error compensation scheme rectifies the incorrect partial sum of the multiply-accumulation operation by estimating the difference between the correct and error-inflicted computation. The difference is added back to the final accumulated result at a later cycle without stalling the execution pipeline. We implemented our proposed design and the existing fault-mitigation schemes on the same Intel FPGA-based DNN accelerator to demonstrate its substantially enhanced resiliency against deliberate fault attacks on two popular DNN models, ResNet50 and VGG16, trained with ImageNet.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3474376.3487281',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Accelerating DNNs inference with predictive layer fusion',\n",
       "  'authors': \"['MohammadHossein Olyaiy', 'Christopher Ng', 'Mieszko Lis']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ICS '21: Proceedings of the ACM International Conference on Supercomputing\",\n",
       "  'abstract': 'Many modern convolutional neural neworks (CNNs) rely on bottleneck block structures where the activation tensor is mapped between higher dimensions using an intermediate low dimension, and convolved with depthwise feature filters rather than multi-channel filters. Because most of the computation lies in computing the large dimensional tensors, however, such networks cannot be scaled without significant computation costs. In this paper, we show how \\\\emph{fusing} the layers inside these blocks can dramatically reduce the multiplication count (by 6--20x) at the cost of extra additions. ReLU nonlinearities are predicted dynamically, and only the activations that survive ReLU contribute to directly compute the output of the block. We also propose FusioNet, a CNN architecture optimized for fusion, as well as ARCHON, a novel accelerator design with a dataflow optimized for fused networks. When FusioNet is executed on the proposed accelerator, it yields up to 5.8x faster inference compared to compact networks executed on a dense DNN accelerator, and 2.1x faster inference compared to the same networks when pruned and executed on a sparse DNN accelerator.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3447818.3460378',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Maya: using formal control to obfuscate power side channels',\n",
       "  'authors': \"['Raghavendra Pradyumna Pothukuchi', 'Sweta Yamini Pothukuchi', 'Petros G. Voulgaris', 'Alexander Schwing', 'Josep Torrellas']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': 'The security of computers is at risk because of information leaking through their power consumption. Attackers can use advanced signal measurement and analysis to recover sensitive data from this side channel. To address this problem, this paper presents Maya, a simple and effective defense against power side channels. The idea is to use formal control to re-shape the power dissipated by a computer in an application-transparent manner---preventing attackers from learning any information about the applications that are running. With formal control, a controller can reliably keep power close to a desired target function even when runtime conditions change unpredictably. By selecting the target function intelligently, the controller can make power to follow any desired shape, appearing to carry activity information which, in reality, is unrelated to the application. Maya can be implemented in privileged software, firmware, or simple hardware. In this paper, we implement Maya on three machines using privileged threads only, and show its effectiveness and ease of deployment. Maya has already thwarted a newly-developed remote power attack.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00074',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'N3H-Core: Neuron-designed Neural Network Accelerator via FPGA-based Heterogeneous Computing Cores',\n",
       "  'authors': \"['Yu Gong', 'Zhihan Xu', 'Zhezhi He', 'Weifeng Zhang', 'Xiaobing Tu', 'Xiaoyao Liang', 'Li Jiang']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"FPGA '22: Proceedings of the 2022 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays\",\n",
       "  'abstract': 'Accelerating the neural network inference by FPGA has emerged as a popular option, since the reconfigurability and high performance computing capability of FPGA intrinsically satisfies the computation demand of the fast-evolving neural algorithms. However, the popular neural accelerators on FPGA (e.g., Xilinx DPU) mainly utilize the DSP resources for constructing their processing units, while the rich LUT resources are not well exploited. Via the software-hardware co-design approach, in this work, we develop an FPGA-based heterogeneous computing system for neural network acceleration. From the hardware perspective, the proposed accelerator consists of DSP- and LUT-based GEneral Matrix-Multiplication (GEMM) computing cores, which forms the entire computing system in a heterogeneous fashion. The DSP- and LUT-based GEMM cores are computed w.r.t a unified Instruction Set Architecture (ISA) and unified buffers. Along the data flow of the neural network inference path, the computation of the convolution/fully-connected layer is split into two portions, handled by the DSP- and LUT-based GEMM cores asynchronously. From the software perspective, we mathematically and systematically model the latency and resource utilization of the proposed heterogeneous accelerator, regarding varying system design configurations. Through leveraging the reinforcement learning technique, we construct a framework to achieve end-to-end selection and optimization of the design specification of target heterogeneous accelerator, including workload split strategy, mixed-precision quantization scheme, and resource allocation of DSP- and LUT-core. In virtue of the proposed design framework and heterogeneous computing system, our design outperforms the state-of-the-art Mix&Match design with latency reduced by 1.12-1.32x with higher inference accuracy. The N3H-core is open-sourced at: https://github.com/elliothe/N3H_Core.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3490422.3502367',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A Modular End-to-End Framework for Secure Firmware Updates on Embedded Systems',\n",
       "  'authors': \"['Solon Falas', 'Charalambos Konstantinou', 'Maria K. Michael']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Journal on Emerging Technologies in Computing Systems',\n",
       "  'abstract': 'Firmware refers to device read-only resident code which includes microcode and macro-instruction-level routines. For Internet-of-Things (IoT) devices without an operating system, firmware includes all the necessary instructions on how such embedded systems operate and communicate. Thus, firmware updates are essential parts of device functionality. They provide the ability to patch vulnerabilities, address operational issues, and improve device reliability and performance during the lifetime of the system. This process, however, is often exploited by attackers in order to inject malicious firmware code into the embedded device. In this article, we present a framework for secure firmware updates on embedded systems. This approach is based on hardware primitives and cryptographic modules, and it can be deployed in environments where communication channels might be insecure. The implementation of the framework is flexible, as it can be adapted in regards to the IoT device’s available hardware resources and constraints. Our security analysis shows that our framework is resilient to a variety of attack vectors. The experimental setup demonstrates the feasibility of the approach. By implementing a variety of test cases on FPGA, we demonstrate the adaptability and performance of the framework. Experiments indicate that the update procedure for a 1183-kB firmware image could be achieved, in a secure manner, under 1.73 seconds.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3460234',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Design of Distributed Reconfigurable Robotics Systems with ReconROS',\n",
       "  'authors': \"['Christian Lienen', 'Marco Platzner']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Reconfigurable Technology and Systems',\n",
       "  'abstract': 'Robotics applications process large amounts of data in real time and require compute platforms that provide high performance and energy efficiency. FPGAs are well suited for many of these applications, but there is a reluctance in the robotics community to use hardware acceleration due to increased design complexity and a lack of consistent programming models across the software/hardware boundary. In this article, we present ReconROS, a framework that integrates the widely used robot operating system (ROS) with ReconOS, which features multithreaded programming of hardware and software threads for reconfigurable computers. This unique combination gives ROS 2 developers the flexibility to transparently accelerate parts of their robotics applications in hardware. We elaborate on the architecture and the design flow for ReconROS and report on a set of experiments that underline the feasibility and flexibility of our approach.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3494571',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A Conditionally Chaotic Physically Unclonable Function Design Framework with High Reliability',\n",
       "  'authors': \"['Saranyu Chattopadhyay', 'Pranesh Santikellur', 'Rajat Subhra Chakraborty', 'Jimson Mathew', 'Marco Ottavi']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Design Automation of Electronic Systems',\n",
       "  'abstract': 'Physically Unclonable Function (PUF) circuits are promising low-overhead hardware security primitives, but are often gravely susceptible to machine learning–based modeling attacks. Recently, chaotic PUF circuits have been proposed that show greater robustness to modeling attacks. However, they often suffer from unacceptable overhead, and their analog components are susceptible to low reliability. In this article, we propose the concept of a conditionally chaotic PUF that enhances the reliability of the analog components of a chaotic PUF circuit to a level at par with their digital counterparts. A conditionally chaotic PUF has two modes of operation: bistable and chaotic, and switching between these two modes is conveniently achieved by setting a mode-control bit (at a secret position) in an applied input challenge. We exemplify our PUF design framework for two different PUF variants—the CMOS Arbiter PUF and a previously proposed hybrid CMOS-memristor PUF, combined with a hardware realization of the Lorenz system as the chaotic component. Through detailed circuit simulation and modeling attack experiments, we demonstrate that the proposed PUF circuits are highly robust to modeling and cryptanalytic attacks, without degrading the reliability of the original PUF that was combined with the chaotic circuit, and incurs acceptable hardware footprint.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3460004',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Birds of a Feather Flock Together: Scaling RDMA RPCs with Flock',\n",
       "  'authors': \"['Sumit Kumar Monga', 'Sanidhya Kashyap', 'Changwoo Min']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"SOSP '21: Proceedings of the ACM SIGOPS 28th Symposium on Operating Systems Principles\",\n",
       "  'abstract': 'RDMA-capable networks are gaining traction with datacenter deployments due to their high throughput, low latency, CPU efficiency, and advanced features, such as remote memory operations. However, efficiently utilizing RDMA capability in a common setting of high fan-in, fan-out asymmetric network topology is challenging. For instance, using RDMA programming features comes at the cost of connection scalability, which does not scale with increasing cluster size. To address that, several works forgo some RDMA features by only focusing on conventional RPC APIs. In this work, we strive to exploit the full capability of RDMA, while scaling the number of connections regardless of the cluster size. We present Flock, a communication framework for RDMA networks that uses hardware provided reliable connection. Using a partially shared model, Flock departs from the conventional RDMA design by enabling connection sharing among threads, which provides significant performance improvements contrary to the widely held belief that connection sharing deteriorates performance. At its core, Flock uses a connection handle abstraction for connection multiplexing; a new coalescing-based synchronization approach for efficient network utilization; and a load-control mechanism for connections with symbiotic send-recv scheduling, which reduces the synchronization overheads associated with connection sharing along with ensuring fair utilization of network connections. We demonstrate the benefits for a distributed transaction processing system and an in-memory index, where it outperforms other RPC systems by up to 88% and 50%, respectively, with significant reductions in median and tail latency.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3477132.3483576',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'The benefits of general-purpose on-NIC memory',\n",
       "  'authors': \"['Boris Pismenny', 'Liran Liss', 'Adam Morrison', 'Dan Tsafrir']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'We propose to use the small, newly available on-NIC memory (\"nicmem\") to keep pace with the rapidly increasing performance of NICs. We motivate our proposal by accelerating two types of workload classes: NFV and key-value stores. As NFV workloads frequently operate on headers---rather than data---of incoming packets, we introduce a new packet-processing architecture that splits between the two, keeping the data on nicmem when possible and thus reducing PCIe traffic, memory bandwidth, and CPU processing time. Our approach consequently shortens NFV latency by up to 23% and increases its throughput by up to 19%. Similarly, because key-value stores commonly exhibit skewed distributions, we introduce a new network stack mechanism that lets applications keep frequently accessed items on nicmem. Our design shortens memcached latency by up to 43% and increases its throughput by up to 80%.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503222.3507711',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'PolyGraph: exposing the value of flexibility for graph processing accelerators',\n",
       "  'authors': \"['Vidushi Dadu', 'Sihao Liu', 'Tony Nowatzki']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': 'Because of the importance of graph workloads and the limitations of CPUs/GPUs, many graph processing accelerators have been proposed. The basic approach of prior accelerators is to focus on a single graph algorithm variant (eg. bulk-synchronous + slicing). While helpful for specialization, this leaves performance potential from flexibility on the table and also complicates understanding the relationship between graph types, workloads, algorithms, and specialization. In this work, we explore the value of flexibility in graph processing accelerators. First, we identify a taxonomy of key algorithm variants. Then we develop a template architecture (PolyGraph) that is flexible across these variants while being able to modularly integrate specialization features for each. Overall we find that flexibility in graph acceleration is critical. If only one variant can be supported, asynchronous-updates/priority-vertex-scheduling/graph-slicing is the best design, achieving 1.93X speedup over the best-performing accelerator, GraphPulse. However, static flexibility per-workload can further improve performance by 2.71X. With dynamic flexibility per-phase, performance further improves by up to 50%.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00053',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Designing and Building communIT',\n",
       "  'authors': \"['Carlos Henrique Araujo de Aguiar', 'Keith Green', 'Trevor Pinch', 'Gilly Leshed', 'Kevin Guo', 'Yeolim Jo']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': 'MAB20: Media Architecture Biennale 20',\n",
       "  'abstract': 'Many subgroups in the US remain marginalized from, misunderstood by, or invisible to the larger communities they reside in. Technologies supporting community building, more generally, have focused on apps, but these apps can fall short of making visible and heard subgroups such as the LGTBQ+, immigrant, and black populations. In response to this shortcoming, we report on the design iterations and an early evaluation of communIT—an interactive artifact for making visible and heard subgroups towards building community. To inform the design of communIT, we conducted in our lab a design studio study (N=57), a co-design activity with a to-scale prototype (N= 12), and a co-design activity with a full-scale prototype (N=28). This paper offers a design exemplar of a large-scale, cyber-physical artifact that might support groups in shaping their identities, practices, and roles in the larger community.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3469410.3469411',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Design of AID and Monitoring Function Based on Intelligent Vision',\n",
       "  'authors': \"['Yang Yang', 'Jianglong Fu', 'Jianguang Zhao', 'Juan Hao', 'Haoyue Sun', 'Xiaohui Qin']\",\n",
       "  'date': 'December 2021',\n",
       "  'source': 'ICASIT 2021: 2021 International Conference on Aviation Safety and Information Technology',\n",
       "  'abstract': 'Event detection system is more and more used in road monitoring. This paper proposes a traffic multi state recognition system based on intelligent vision recognition technology. The gray change interval of frame difference is used to update the image background, and the pixel change control rule is introduced as the core software algorithm. Combined with traffic state monitoring data source, multi state traffic events such as congestion, pedestrian and parking are detected. The actual system test shows that the system can timely and accurately detect related traffic events, and has high detection accuracy. As an important part of intelligent transportation system, traffic incident automatic detection (AID) system plays an important role in avoiding traffic accidents, handling and controlling.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3510858.3511360',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Lessons Learned from Blockchain Applications of Trusted Execution Environments and Implications for Future Research',\n",
       "  'authors': \"['Karanjai Rabimba', 'Lei Xu', 'Lin Chen', 'Fengwei Zhang', 'Zhimin Gao', 'Weidong Shi']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"HASP '21: Proceedings of the 10th International Workshop on Hardware and Architectural Support for Security and Privacy\",\n",
       "  'abstract': 'Modern computer systems tend to rely on large trusted computing bases (TCBs) for operations. To address the TCB bloating problem, hardware vendors have developed mechanisms to enable or facilitate the creation of a trusted execution environment (TEE) in which critical software applications can execute securely in an isolated environment. Even under the circumstance that a host OS is compromised by an adversary, key security properties such as confidentiality and integrity of the software inside the TEEs can be guaranteed. The promise of integrity and security has driven developers to adopt it for use cases involving access control, PKS, IoT among other things. Among these applications include blockchain-related use cases. The usage of the TEEs doesn’t come without its own implementation challenges and potential pitfalls. In this paper, we examine the assumptions, security models, and operational environments of the proposed TEE use cases of blockchain-based applications. The exercise and analysis help the hardware TEE research community to identify some open challenges and opportunities for research and rethink the design of hardware TEEs in general.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3505253.3505259',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Jetstream2: Accelerating cloud computing via Jetstream',\n",
       "  'authors': \"['David Y. Hancock', 'Jeremy Fischer', 'John Michael Lowe', 'Winona Snapp-Childs', 'Marlon Pierce', 'Suresh Marru', 'J. Eric Coulter', 'Matthew Vaughn', 'Brian Beck', 'Nirav Merchant', 'Edwin Skidmore', 'Gwen Jacobs']\",\n",
       "  'date': 'July 2021',\n",
       "  'source': \"PEARC '21: Practice and Experience in Advanced Research Computing\",\n",
       "  'abstract': 'Jetstream2 will be a category I production cloud resource that is part of the National Science Foundation’s Innovative HPC Program. The project’s aim is to accelerate science and engineering by providing “on-demand” programmable infrastructure built around a core system at Indiana University and four regional sites. Jetstream2 is an evolution of the Jetstream platform, which functions primarily as an Infrastructure-as-a-Service cloud. The lessons learned in cloud architecture, distributed storage, and container orchestration have inspired changes in both hardware and software for Jetstream2. These lessons have wide implications as institutions converge HPC and cloud technology while building on prior work when deploying their own cloud environments. Jetstream2’s next-generation hardware, robust open-source software, and enhanced virtualization will provide a significant platform to further cloud adoption within the US research and education communities.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3437359.3465565',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'BiSon-e: a lightweight and high-performance accelerator for narrow integer linear algebra computing on the edge',\n",
       "  'authors': \"['Enrico Reggiani', 'Cristóbal Ramírez Lazo', 'Roger Figueras Bagué', 'Adrián Cristal', 'Mauro Olivieri', 'Osman Sabri Unsal']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'Linear algebra computational kernels based on byte and sub-byte integer data formats are at the base of many classes of applications, ranging from Deep Learning to Pattern Matching. Porting the computation of these applications from cloud to edge and mobile devices would enable significant improvements in terms of security, safety, and energy efficiency. However, despite their low memory and energy demands, their intrinsically high computational intensity makes the execution of these workloads challenging on highly resource-constrained devices. In this paper, we present BiSon-e, a novel RISC-V based architecture that accelerates linear algebra kernels based on narrow integer computations on edge processors by performing Single Instruction Multiple Data (SIMD) operations on off-the-shelf scalar Functional Units (FUs). Our novel architecture is built upon the binary segmentation technique, which allows to significantly reduce the memory footprint and the arithmetic intensity of linear algebra kernels requiring narrow data sizes. We integrate BiSon-e into a complete System-on-Chip (SoC) based on RISC-V, synthesized and Place&Routed in 65nm and 22nm technologies, introducing a negligible 0.07% area overhead with respect to the baseline architecture. Our experimental evaluation shows that, when computing the Convolution and Fully-Connected layers of the AlexNet and VGG-16 Convolutional Neural Networks (CNNs) with 8-, 4-, and 2-bit, our solution gains up to 5.6×, 13.9× and 24× in execution time compared to the scalar implementation of a single RISC-V core, and improves the energy efficiency of string matching tasks by 5× when compared to a RISC-V-based Vector Processing Unit (VPU).',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503222.3507746',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Dual-side sparse tensor core',\n",
       "  'authors': \"['Yang Wang', 'Chen Zhang', 'Zhiqiang Xie', 'Cong Guo', 'Yunxin Liu', 'Jingwen Leng']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': \"Leveraging sparsity in deep neural network (DNN) models is promising for accelerating model inference. Yet existing GPUs can only leverage the sparsity from weights but not activations, which are dynamic, unpredictable, and hence challenging to exploit. In this work, we propose a novel architecture to efficiently harness the dual-side sparsity (i.e., weight and activation sparsity). We take a systematic approach to understand the (dis)advantages of previous sparsity-related architectures and propose a novel, unexplored paradigm that combines outer-product computation primitive and bitmap-based encoding format. We demonstrate the feasibility of our design with minimal changes to the existing production-scale inner-product-based Tensor Core. We propose a set of novel ISA extensions and co-design the matrix-matrix multiplication and convolution algorithms, which are the two dominant computation patterns in today's DNN models, to exploit our new dual-side sparse Tensor Core. Our evaluation shows that our design can fully unleash the dualside DNN sparsity and improve the performance by up to one order of magnitude with small hardware overhead.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00088',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Design Cognition while using digital tools: A Distributed Cognition Approach',\n",
       "  'authors': \"['Vriddhi Vriddhi', 'Joy Sen', 'Aneesha Sharma']\",\n",
       "  'date': 'September 2021',\n",
       "  'source': \"ICDTE '21: Proceedings of the 5th International Conference on Digital Technology in Education\",\n",
       "  'abstract': 'The use of digital tools in the conventional architecture design thinking process which derives its basis from sketching is followed in many colleges in India. Various shortcomings due to the integration of digital tools to the manual design process have been enumerated during the past 30 years. Digital tools provide affordances different from the manual sketching design process, the effects of which can be understood by adopting a distributed cognition approach. The paper builds on design cognition research while using externalization tools in the design process. It does so by developing a theoretical framework derived from distributed cognition and an understanding of visual thinking processes from design literature. The paper utilizes the distributed cognition framework by Zhang and Norman, to arrive at resultant affordances of externalization tools in design. The same is then utilized for a protocol study which was coded for its visual thinking components and other relevant codes. The same protocol study was also coded for ideation flow analysis. The findings pointed towards compromised visual thinking and reduced ideation while utilizing digital tools in quick conceptualization.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3488466.3488491',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Software Hint-Driven Data Management for Hybrid Memory in Mobile Systems',\n",
       "  'authors': \"['Fei Wen', 'Mian Qin', 'Paul Gratz', 'Narasimha Reddy']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Embedded Computing Systems',\n",
       "  'abstract': 'Hybrid memory systems, comprised of emerging non-volatile memory (NVM) and DRAM, have been proposed to address the growing memory demand of current mobile applications. Recently emerging NVM technologies, such as phase-change memories (PCM), memristor, and 3D XPoint, have higher capacity density, minimal static power consumption and lower cost per GB. However, NVM has longer access latency and limited write endurance as opposed to DRAM. The different characteristics of distinct memory classes render a new challenge for memory system design.Ideally, pages should be placed or migrated between the two types of memories according to the data objects’ access properties. Prior system software approaches exploit the program information from OS but at the cost of high software latency incurred by related kernel processes. Hardware approaches can avoid these latencies, however, hardware’s vision is constrained to a short time window of recent memory requests, due to the limited on-chip resources.In this work, we propose OpenMem: a hardware-software cooperative approach that combines the execution time advantages of pure hardware approaches with the data object properties in a global scope. First, we built a hardware-based memory manager unit (HMMU) that can learn the short-term access patterns by online profiling, and execute data migration efficiently. Then, we built a heap memory manager for the heterogeneous memory systems that allows the programmer to directly customize each data object’s allocation to a favorable memory device within the presumed object life cycle. With the programmer’s hints guiding the data placement at allocation time, data objects with similar properties will be congregated to reduce unnecessary page migrations.We implemented the whole system on the FPGA board with embedded ARM processors. In testing under a set of benchmark applications from SPEC 2017 and PARSEC, experimental results show that OpenMem reduces 44.6% energy consumption with only a 16% performance degradation compared to the all-DRAM memory system. The amount of writes to the NVM is reduced by 14% versus the HMMU-only, extending the NVM device lifetime.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3494536',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Approximate Constant-Coefficient Multiplication Using Hybrid Binary-Unary Computing for FPGAs',\n",
       "  'authors': \"['S. Rasoul Faraji', 'Pierre Abillama', 'Kia Bazargan']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Reconfigurable Technology and Systems',\n",
       "  'abstract': 'Multipliers are used in virtually all Digital Signal Processing (DSP) applications such as image and video processing. Multiplier efficiency has a direct impact on the overall performance of such applications, especially when real-time processing is needed, as in 4K video processing, or where hardware resources are limited, as in mobile and IoT devices. We propose a novel, low-cost, low energy, and high-speed approximate constant coefficient multiplier (CCM) using a hybrid binary-unary encoding method. The proposed method implements a CCM using simple routing networks with no logic gates in the unary domain, which results in more efficient multipliers compared to Xilinx LogiCORE IP CCMs and table-based KCM CCMs (Flopoco) on average. We evaluate the proposed multipliers on 2-D discrete cosine transform algorithm as a common DSP module. Post-routing FPGA results show that the proposed multipliers can improve the {area, area × delay, power consumption, and energy-delay product} of a 2-D discrete cosine transform on average by {30%, 33%, 30%, 31%}. Moreover, the throughput of the proposed 2-D discrete cosine transform is on average 5% more than that of the binary architecture implemented using table-based KCM CCMs. We will show that our method has fewer routability issues compared to binary implementations when implementing a DCT core.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3494570',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'DTA-PUF: Dynamic Timing-aware Physical Unclonable Function for Resource-constrained Devices',\n",
       "  'authors': \"['Ioannis Tsiokanos', 'Jack Miskelly', 'Chongyan Gu', 'Maire O’neill', 'Georgios Karakonstantis']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Journal on Emerging Technologies in Computing Systems',\n",
       "  'abstract': 'In recent years, physical unclonable functions (PUFs) have gained a lot of attention as mechanisms for hardware-rooted device authentication. While the majority of the previously proposed PUFs derive entropy using dedicated circuitry, software PUFs achieve this from existing circuitry in a system. Such software-derived designs are highly desirable for low-power embedded systems as they require no hardware overhead. However, these software PUFs induce considerable processing overheads that hinder their adoption in resource-constrained devices. In this article, we propose DTA-PUF, a novel, software PUF design that exploits the instruction- and data-dependent dynamic timing behaviour of pipelined cores to provide a reliable challenge-response mechanism without requiring any extra hardware. DTA-PUF accepts sequences of instructions as an input challenge and produces an output response based on the manifested timing errors under specific over-clocked settings. To lower the required processing effort, we systematically select instruction sequences that maximise error-rate. The application to a post-layout pipelined floating-point unit, which is implemented in 45 nm process technology, demonstrates the effectiveness and practicability of our PUF design. Finally, DTA-PUF requires up to 50× fewer instructions than existing software processor PUF designs, limiting processing costs and resulting in up to 26% power savings.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3434281',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A cost-effective entangling prefetcher for instructions',\n",
       "  'authors': \"['Alberto Ros', 'Alexandra Jimborean']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': 'Prefetching instructions in the instruction cache is a fundamental technique for designing high-performance computers. There are three key properties to consider when designing an efficient and effective prefetcher: timeliness, coverage, and accuracy. Timeliness is essential, as bringing instructions too early increases the risk of the instructions being evicted from the cache before their use and requesting them too late can lead to the instructions arriving after they are demanded. Coverage is important to reduce the number of instruction cache misses and accuracy to ensure that the prefetcher does not pollute the cache or interacts negatively with the other hardware mechanisms. This paper presents the Entangling Prefetcher for Instructions that entangles instructions to maximize timeliness. The prefetcher works by finding which instruction should trigger the prefetch for a subsequent instruction, accounting for the latency of each cache miss. The prefetcher is carefully adjusted to account for both coverage and accuracy. Our evaluation shows that with 40KB of storage, Entangling can increase performance up to 23%, outperforming state-of-the-art prefetchers.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00017',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Dvé: improving DRAM reliability and performance on-demand via coherent replication',\n",
       "  'authors': \"['Adarsh Patil', 'Vijay Nagarajan', 'Rajeev Balasubramonian', 'Nicolai Oswald']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': \"As technologies continue to shrink, memory system failure rates have increased, demanding support for stronger forms of reliability. In this work, we take inspiration from the two-tier approach that decouples correction from detection and explore a novel extrapolation. We propose Dvé, a hardware-driven replication mechanism where data blocks are replicated in 2 different sockets across a cache-coherent NUMA system. Each data block is also accompanied by a code with strong error detection capabilities so that when an error is detected, correction is performed using the replica. Such an organization has the advantage of offering two independent points of access to data which enables: (a) strong error correction that can recover from a range of faults affecting any of the components in the memory, upto and including the memory controller, and (b) higher performance by providing another nearer point of memory access. Dvé realizes both of these benefits via Coherent Replication, a technique that builds on top of existing cache coherence protocols for not only keeping the replicas in sync for reliability, but also to provide coherent access to the replicas during fault-free operation for performance. Dvé can flexibly provide these benefits on-demand by simply using the provisioned memory capacity which, as reported in recent studies, is often underutilized in today's systems. Thus, Dvé introduces a unique design point that offers higher reliability and performance for workloads that do not require the entire memory capacity.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00048',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Rebooting virtual memory with midgard',\n",
       "  'authors': \"['Siddharth Gupta', 'Atri Bhattacharyya', 'Yunho Oh', 'Abhishek Bhattacharjee', 'Babak Falsafi', 'Mathias Payer']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': \"Computer systems designers are building cache hierarchies with higher capacity to capture the ever-increasing working sets of modern workloads. Cache hierarchies with higher capacity improve system performance but shift the performance bottleneck to address translation. We propose Midgard, an intermediate address space between the virtual and the physical address spaces, to mitigate address translation overheads without program-level changes. Midgard leverages the operating system concept of virtual memory areas (VMAs) to realize a single Midgard address space where VMAs of all processes can be uniquely mapped. The Midgard address space serves as the namespace for all data in a coherence domain and the cache hierarchy. Because real-world workloads use far fewer VMAs than pages to represent their virtual address space, virtual to Midgard translation is achieved with hardware structures that are much smaller than TLB hierarchies. Costlier Midgard to physical address translations are needed only on LLC misses, which become much less frequent with larger caches. As a consequence, Midgard shows that instead of amplifying address translation overheads, memory hierarchies with large caches can reduce address translation overheads. Our evaluation shows that Midgard achieves only 5% higher address translation overhead as compared to traditional TLB hierarchies for 4KB pages when using a 16MB aggregate LLC. Midgard also breaks even with traditional TLB hierarchies for 2MB pages when using a 256MB aggregate LLC. For cache hierarchies with higher capacity, Midgard's address translation overhead drops to near zero as secondary and tertiary data working sets fit in the LLC, while traditional TLBs suffer even higher degrees of address translation overhead.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00047',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'SPACE: locality-aware processing in heterogeneous memory for personalized recommendations',\n",
       "  'authors': \"['Hongju Kal', 'Seokmin Lee', 'Gun Ko', 'Won Woo Ro']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': 'Personalized recommendation systems have become a major AI application in modern data centers. The main challenges in processing personalized recommendation inferences are the large memory footprint and high bandwidth requirement of embedding layers. To overcome the capacity limit and bandwidth congestion of on-chip memory, near memory processing (NMP) can be a promising solution. Recent work on accelerating personalized recommendations proposes a DIMM-based NMP design to solve the bandwidth problem and increases memory capacity. The performance of NMP is determined by the internal bandwidth and the prior DIMM-based approach utilizes more DIMMs to achieve higher operation throughput. However, extending the number of DIMMs could eventually lead to significant power consumption due to inefficient scaling. We propose SPACE, a novel heterogeneous memory architecture, which is efficient in terms of performance and energy. SPACE exploits a compute-capable 3D-stacked DRAM with DIMMs for personalized recommendations. Prior to designing the proposed system, we give a quantitative analysis of the user/item interactions and define the two localities: gather locality and reduction locality. In gather operations, we find only a small proportion of items are highly-accessed by users, and we call this gather locality. Also, we define reduction locality as the reusability of the gathered items in reduction operations. Based on the gather locality, SPACE allocates highly-accessed embedding items to the 3D-stacked DRAM to achieve the maximum bandwidth. Subsequently, by exploiting reduction locality, we utilize the remaining space of the 3D-stacked DRAM to store and reuse repeated partial sums, thereby minimizing the required number of element-wise reduction operations. As a result, the evaluation shows that SPACE achieves 3.2X performance improvement and 56% energy saving over the previous DIMM-based NMPs leveraging 3D-stacked DRAM with a 1/8 size of DIMMs. Also, compared to the state-of-the-art DRAM cache designs with the same NMP configuration, SPACE achieves an average 32.7% of performance improvement.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00059',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Porting Deep Spiking Q-Networks to neuromorphic chip Loihi',\n",
       "  'authors': \"['Mahmoud Akl', 'Yulia Sandamirskaya', 'Florian Walter', 'Alois Knoll']\",\n",
       "  'date': 'July 2021',\n",
       "  'source': 'ICONS 2021: International Conference on Neuromorphic Systems 2021',\n",
       "  'abstract': 'Deep neural networks (DNNs) set the benchmark in many tasks in perception and control. Spiking versions of DNNs, implemented on neuromorphic hardware can enable orders of magnitude lower power consumption and low latency during network use. In this paper, we explore behavior and generalization capability of spiking, quantized spiking, and hardware implementation of deep Q-networks in two classical reinforcement learning tasks. We found that spiking neural networks have slightly decreased performance compared to non-spiking network, but we can avoid performance degradation from quantization and in-chip implementation. We conclude that since hardware implementation leads to lower power consumption and low latency, neuromorphic approach is a promising avenue for deep Q-learning. Furthermore, online learning, enabled in neuromorphic chips, can be used to compensate for the performance decrease in environments with parameter variations.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3477145.3477159',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'TimeCache: using time to eliminate cache side channels when sharing software',\n",
       "  'authors': \"['Divya Ojha', 'Sandhya Dwarkadas']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': 'Timing side channels have been used to extract cryptographic keys and sensitive documents even from trusted enclaves. Specifically, cache side channels created by reuse of shared code or data in the memory hierarchy have been exploited by several known attacks, e.g., evict+reload for recovering an RSA key and Spectre variants for leaking speculatively loaded data. In this paper, we present TimeCache, a cache design that incorporates knowledge of prior cache line access to eliminate cache side channels due to reuse of shared software (code and data). Our goal is to retain the benefits of a shared cache of allowing each process access to the entire cache and of cache occupancy by a single copy of shared software. We achieve our goal by implementing per-process cache line visibility so that the processes do not benefit from cached data brought in by another process until they have incurred a corresponding miss penalty. Our design achieves low overhead by using a novel combination of timestamps and a hardware design to allow efficient parallel comparisons of the timestamps. The solution works at all the cache levels without the need to limit the number of security domains, and defends against an attacker process running on the same core, on a another hyperthread, or on another core. Our implementation in the gem5 simulator demonstrates that the system is able to defend against RSA key extraction. We evaluate performance using SPEC2006 and PARSEC and observe the overhead of TimeCache to be 1.13% on average. Delay due to first access misses adds the majority of the overhead, with the security context bookkeeping incurred at the time of a context switch contributing 0.02% of the 1.13%.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00037',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'GraphPEG: Accelerating Graph Processing on GPUs',\n",
       "  'authors': \"['Yashuai Lü', 'Hui Guo', 'Libo Huang', 'Qi Yu', 'Li Shen', 'Nong Xiao', 'Zhiying Wang']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Architecture and Code Optimization',\n",
       "  'abstract': 'Due to massive thread-level parallelism, GPUs have become an attractive platform for accelerating large-scale data parallel computations, such as graph processing. However, achieving high performance for graph processing with GPUs is non-trivial. Processing graphs on GPUs introduces several problems, such as load imbalance, low utilization of hardware unit, and memory divergence. Although previous work has proposed several software strategies to optimize graph processing on GPUs, there are several issues beyond the capability of software techniques to address.In this article, we present GraphPEG, a graph processing engine for efficient graph processing on GPUs. Inspired by the observation that many graph algorithms have a common pattern on graph traversal, GraphPEG improves the performance of graph processing by coupling automatic edge gathering with fine-grain work distribution. GraphPEG can also adapt to various input graph datasets and simplify the software design of graph processing with hardware-assisted graph traversal. Simulation results show that, in comparison with two representative highly efficient GPU graph processing software framework Gunrock and SEP-Graph, GraphPEG improves graph processing throughput by 2.8× and 2.5× on average, and up to 7.3× and 7.0× for six graph algorithm benchmarks on six graph datasets, with marginal hardware cost.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3450440',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Scalable Phylogeny Reconstruction with Disaggregated Near-memory Processing',\n",
       "  'authors': \"['Nikolaos Alachiotis', 'Panagiotis Skrimponis', 'Manolis Pissadakis', 'Dionisios Pnevmatikatos']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Reconfigurable Technology and Systems',\n",
       "  'abstract': 'Disaggregated computer architectures eliminate resource fragmentation in next-generation datacenters by enabling virtual machines to employ resources such as CPUs, memory, and accelerators that are physically located on different servers. While this paves the way for highly compute- and/or memory-intensive applications to potentially deploy all CPUs and/or memory resources in a datacenter, it poses a major challenge to the efficient deployment of hardware accelerators: input/output data can reside on different servers than the ones hosting accelerator resources, thereby requiring time- and energy-consuming remote data transfers that diminish the gains of hardware acceleration. Targeting a disaggregated datacenter architecture similar to the IBM dReDBox disaggregated datacenter prototype, the present work explores the potential of deploying custom acceleration units adjacently to the disaggregated-memory controller on memory bricks (in dReDBox terminology), which is implemented on FPGA technology, to reduce data movement and improve performance and energy efficiency when reconstructing large phylogenies (evolutionary relationships among organisms). A fundamental computational kernel is the Phylogenetic Likelihood Function (PLF), which dominates the total execution time (up to 95%) of widely used maximum-likelihood methods. Numerous efforts to boost PLF performance over the years focused on accelerating computation; since the PLF is a data-intensive, memory-bound operation, performance remains limited by data movement, and memory disaggregation only exacerbates the problem. We describe two near-memory processing models, one that addresses the problem of workload distribution to memory bricks, which is particularly tailored toward larger genomes (e.g., plants and mammals), and one that reduces overall memory requirements through memory-side data interpolation transparently to the application, thereby allowing the phylogeny size to scale to a larger number of organisms without requiring additional memory.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3484983',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'IntroSpectre: a pre-silicon framework for discovery and analysis of transient execution vulnerabilities',\n",
       "  'authors': \"['Moein Ghaniyoun', 'Kristin Barber', 'Yinqian Zhang', 'Radu Teodorescu']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': 'Transient execution vulnerabilities originate in the extensive speculation implemented in modern high-performance microprocessors. Identifying all possible vulnerabilities in complex designs is very challenging. One of the challenges stems from the lack of visibility into the transient micro-architectural state of the processor. Prior work has used covert channels to identify data leakage from transient state, which limits the systematic discovery of all potential leakage sources. This paper presents INTROSPECTRE, a pre-silicon framework for early discovery of transient execution vulnerabilities. INTROSPECTRE addresses the lack of visibility into the micro-architectural processor state by integrating into the register transfer level (RTL) design flow, gaining full access to the internal state of the processor. Full visibility into the processor state enables INTROSPECTRE to perform a systematic leakage analysis that includes all micro-architectural structures, allowing it to identify potential leakage that may not be reachable with known side channels. We implement INTROSPECTRE on an RTL simulator and use it to perform transient leakage analysis on the RISC-V BOOM processor. We identify multiple transient leakage scenarios, most of which had not been highlighted on this processor design before.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00073',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'SIAM: Chiplet-based Scalable In-Memory Acceleration with Mesh for Deep Neural Networks',\n",
       "  'authors': \"['Gokul Krishnan', 'Sumit K. Mandal', 'Manvitha Pannala', 'Chaitali Chakrabarti', 'Jae-Sun Seo', 'Umit Y. Ogras', 'Yu Cao']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Embedded Computing Systems',\n",
       "  'abstract': 'In-memory computing (IMC) on a monolithic chip for deep learning faces dramatic challenges on area, yield, and on-chip interconnection cost due to the ever-increasing model sizes. 2.5D integration or chiplet-based architectures interconnect multiple small chips (i.e., chiplets) to form a large computing system, presenting a feasible solution beyond a monolithic IMC architecture to accelerate large deep learning models. This paper presents a new benchmarking simulator, SIAM, to evaluate the performance of chiplet-based IMC architectures and explore the potential of such a paradigm shift in IMC architecture design. SIAM integrates device, circuit, architecture, network-on-chip (NoC), network-on-package (NoP), and DRAM access models to realize an end-to-end system. SIAM is scalable in its support of a wide range of deep neural networks (DNNs), customizable to various network structures and configurations, and capable of efficient design space exploration. We demonstrate the flexibility, scalability, and simulation speed of SIAM by benchmarking different state-of-the-art DNNs with CIFAR-10, CIFAR-100, and ImageNet datasets. We further calibrate the simulation results with a published silicon result, SIMBA. The chiplet-based IMC architecture obtained through SIAM shows 130\\\\(\\\\) and 72\\\\(\\\\) improvement in energy-efficiency for ResNet-50 on the ImageNet dataset compared to Nvidia V100 and T4 GPUs.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3476999',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Ripple: profile-guided instruction cache replacement for data center applications',\n",
       "  'authors': \"['Tanvir Ahmed Khan', 'Dexin Zhang', 'Akshitha Sriraman', 'Joseph Devietti', 'Gilles Pokam', 'Heiner Litz', 'Baris Kasikci']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': 'Modern data center applications exhibit deep software stacks, resulting in large instruction footprints that frequently cause instruction cache misses degrading performance, cost, and energy efficiency. Although numerous mechanisms have been proposed to mitigate instruction cache misses, they still fall short of ideal cache behavior, and furthermore, introduce significant hardware overheads. We first investigate why existing I-cache miss mitigation mechanisms achieve sub-optimal performance for data center applications. We find that widely-studied instruction prefetchers fall short due to wasteful prefetch-induced cache line evictions that are not handled by existing replacement policies. Existing replacement policies are unable to mitigate wasteful evictions since they lack complete knowledge of a data center application\\'s complex program behavior. To make existing replacement policies aware of these eviction-inducing program behaviors, we propose Ripple, a novel software-only technique that profiles programs and uses program context to inform the underlying replacement policy about efficient replacement decisions. Ripple carefully identifies program contexts that lead to I-cache misses and sparingly injects \"cache line eviction\" instructions in suitable program locations at link time. We evaluate Ripple using nine popular data center applications and demonstrate that Ripple enables any replacement policy to achieve speedup that is closer to that of an ideal I-cache. Specifically, Ripple achieves an average performance improvement of 1.6% (up to 2.13%) over prior work due to a mean 19% (up to 28.6%) I-cache miss reduction.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00063',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Confidential serverless made efficient with <u>p</u>lug-<u>in</u> <u>e</u>nclaves',\n",
       "  'authors': \"['Mingyu Li', 'Yubin Xia', 'Haibo Chen']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': 'Serverless computing has become a fact of life on modern clouds. A serverless function may process sensitive data from clients. Protecting such a function against untrusted clouds using hardware enclave is attractive for user privacy. In this work, we run existing serverless applications in SGX enclave, and observe that the performance degradation can be as high as 5.6X to even 422.6X. Our investigation identifies these slowdowns are related to architectural features, mainly from page-wise enclave initialization. Leveraging insights from our overhead analysis, we revisit SGX hardware design and make minimal modification to its enclave model. We extend SGX with a new primitive---region-wise plugin enclaves that can be mapped into existing enclaves to reuse attested common states amongst functions. By remapping plugin enclaves, an enclave allows in-situ processing to avoid expensive data movement in a function chain. Experiments show that our design reduces the enclave function latency by 94.74--99.57%, and boosts the autoscaling throughput by 19-179X.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00032',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Sunder: Enabling Low-Overhead and Scalable Near-Data Pattern Matching Acceleration',\n",
       "  'authors': \"['Elaheh Sadredini', 'Reza Rahimi', 'Mohsen Imani', 'Kevin Skadron']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"MICRO '21: MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture\",\n",
       "  'abstract': 'Automata processing is an efficient computation model for regular expressions and other forms of sophisticated pattern matching. The demand for high-throughput and real-time pattern matching in many applications, including network intrusion detection and spam filters, has motivated several in-memory architectures for automata processing. Existing in-memory architectures focus on accelerating the pattern-matching kernel, but either fail to support a practical reporting solution or optimistically assume that the reporting stage is not the performance bottleneck. However, gathering and processing the reports can be the major bottleneck, especially when the reporting frequency is high. Moreover, all the existing in-memory architectures work with a fixed processing rate (mostly 8-bit/cycle), and they do not adjust the input consumption rate based on the properties of the applications, which can lead to throughput and capacity loss.  To address these issues, we present Sunder, an in-SRAM pattern matching architecture, to processes a reconfigurable number of nibbles (4-bit symbols) in parallel, instead of fixed-rate processing, by adopting an algorithm/architecture methodology to perform hardware-aware transformations. Inspired by prior work, we transform the commonly-used 8-bit processing to nibble-processing (4-bit processing) to reduce hardware requirements exponentially and achieve higher information density. This frees up space for storing reporting data in place, which significantly eliminates host communication and reporting overhead. Our proposed reporting architecture supports in-place report summarization and provides an easy access mechanism to read the reporting data. As a result, Sunder enables a low-overhead, high-performance, and flexible in-memory pattern-matching and reporting solution. Our results confirm that Sunder reporting architecture has zero performance overhead for 95% of the applications and incurs only 2% additional hardware overhead.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3466752.3480934',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'HASCO: towards agile <u>ha</u>rdware and <u>s</u>oftware <u>co</u>-design for tensor computation',\n",
       "  'authors': \"['Qingcheng Xiao', 'Size Zheng', 'Bingzhe Wu', 'Pengcheng Xu', 'Xuehai Qian', 'Yun Liang']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': 'Tensor computations overwhelm traditional general-purpose computing devices due to the large amounts of data and operations of the computations. They call for a holistic solution composed of both hardware acceleration and software mapping. Hardware/software (HW/SW) co-design optimizes the hardware and software in concert and produces high-quality solutions. There are two main challenges in the co-design flow. First, multiple methods exist to partition tensor computation and have different impacts on performance and energy efficiency. Besides, the hardware part must be implemented by the intrinsic functions of spatial accelerators. It is hard for programmers to identify and analyze the partitioning methods manually. Second, the overall design space composed of HW/SW partitioning, hardware optimization, and software optimization is huge. The design space needs to be efficiently explored. To this end, we propose an agile co-design approach HASCO that provides an efficient HW/SW solution to dense tensor computation. We use tensor syntax trees as the unified IR, based on which we develop a two-step approach to identify partitioning methods. For each method, HASCO explores the hardware and software design spaces. We propose different algorithms for the explorations, as they have distinct objectives and evaluation costs. Concretely, we develop a multi-objective Bayesian optimization algorithm to explore hardware optimization. For software optimization, we use heuristic and Q-learning algorithms. Experiments demonstrate that HASCO achieves a 1.25X to 1.44X latency reduction through HW/SW co-design compared with developing the hardware and software separately.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00086',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'On the TOCTOU Problem in Remote Attestation',\n",
       "  'authors': \"['Ivan De Oliveira Nunes', 'Sashidhar Jakkamsetti', 'Norrathep Rattanavipanon', 'Gene Tsudik']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"CCS '21: Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security\",\n",
       "  'abstract': \"Much attention has been devoted to verifying software integrity of remote embedded (IoT) devices. Many techniques, with different assumptions and security guarantees, have been proposed under the common umbrella of so-called Remote Attestation (RA). Aside from executable's integrity verification, RA serves as a foundation for many security services, such as proofs of memory erasure, system reset, software update, and verification of runtime properties. Prior RA techniques verify the remote device's binary at the time when RA functionality is executed, thus providing no information about the device's binary before current RA execution or between consecutive RA executions. This implies that presence of transient malware (in the form of modified binary) may be undetected. In other words, if transient malware infects a device (by modifying its binary), performs its nefarious tasks, and erases itself before the next attestation, its temporary presence will not be detected. This important problem, called Time-Of-Check-Time-Of-Use ( TOCTOU ), is well-known in the research literature and remains unaddressed in the context of hybrid RA. In this work, we propose Remote Attestation with TOCTOU Avoidance (RATA): a provably secure approach to address the RA TOCTOU problem. With RATA, even malware that erases itself before execution of the next RA, can not hide its ephemeral presence. RATA targets hybrid RA architectures, which are aimed at low-end embedded devices. We present two alternative techniques - RATA A and RATA B - suitable for devices with and without real-time clocks, respectively. Each is shown to be secure and accompanied by a publicly available and formally verified implementation. Our evaluation demonstrates low hardware overhead of both techniques. Compared with current hybrid RA architectures - that offer no TOCTOU protection - RATA incurs no extra runtime overhead. In fact, it substantially reduces the time complexity of RA computations: from linear to constant time.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3460120.3484532',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Implementation and acceleration scheme of Heart sound classification Algorithm based on SOC-FPGA',\n",
       "  'authors': \"['Guozheng Li', 'Hongbo Yang', 'Tao Guo', 'Weilain Wang']\",\n",
       "  'date': 'January 2022',\n",
       "  'source': 'BIC 2022: 2022 2nd International Conference on Bioinformatics and Intelligent Computing',\n",
       "  'abstract': 'ABSTRACT-Widespread screening of congenital heart disease is of time-consuming, labor-consuming, and difficult for rural\\xa0doctors to master the skill of cardiac auscultation. A kind of machine-assisted diagnosis method was put forwarded in this paper to solve the above problems. In which a heart sound classification algorithm and acceleration plan of CNN was implemented on a small scale SoC-FPGA chip with fewer resources. In this method, heart sounds were denoised and segmented into cardio cycles first. Then STFT transformation was done for time-frequency feature extraction. The time-frequency features were used to train the CNN model to extract network model parameters. In hardware implementation, the parallelism of CNN was corresponding to FPGA parallel hardware. In order to make acceleration of the algorithm, loop unrolling, fixed-point of model parameter, and reducing global memory access were done. The experimental results show that the classification speed is 3.13 times as much as one of CPU at the same conditions with the classification accuracy not any dropping significantly. It provides an offline solution for the machine-assisted screening of congenital heart disease.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3523286.3524551',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'FlexDriver: a network driver for your accelerator',\n",
       "  'authors': \"['Haggai Eran', 'Maxim Fudim', 'Gabi Malka', 'Gal Shalom', 'Noam Cohen', 'Amit Hermony', 'Dotan Levi', 'Liran Liss', 'Mark Silberstein']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'We propose a new system design for connecting hardware and FPGA accelerators to the network, allowing the accelerator to directly control commodity Network Interface Cards (NICs) without using the CPU. This enables us to solve the key challenge of leveraging existing NIC hardware offloads such as virtualization, tunneling, and RDMA for accelerator networking. Our approach supports a diverse set of use cases, from direct network access for disaggregated accelerators to inline-acceleration of the network stack, all without the complex networking logic in the accelerator.  To demonstrate the feasibility of this approach, we build FlexDriver (FLD), an on-accelerator hardware module that implements a NIC data-plane driver. Our main technical contribution is a mechanism that compresses the NIC control structures by two orders of magnitude, allowing FLD to achieve high networking scalability with low die area cost and no bandwidth interference with the accelerator logic.  The prototype for NVIDIA Innova-2 FPGA SmartNICs showcases our design’s utility for three different accelerators: a disaggregated LTE cipher, an IP-defragmentation inline accelerator, and an IoT cryptographic-token authentication offload. These accelerators reach 25 Gbps line rate and leverage the NIC for RDMA processing, VXLAN tunneling, and traffic shaping without CPU involvement.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503222.3507776',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Research on High Performance Transmission Technology of DC Based on Network Awareness',\n",
       "  'authors': \"['Yanwei Wang', 'Cheng Huang', 'Jiaheng Fan', 'Le Yang', 'Hongwei Kan', 'Gaoming Cao']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"CSAE '21: Proceedings of the 5th International Conference on Computer Science and Application Engineering\",\n",
       "  'abstract': 'In recent years, RDMA over Converged Ethernet (RoCE) provides high performance data transmission for Data center (DC). RoCE has excellent performance in lossless network, but when the network environment is unstable, the transmission performance of RoCE will decline rapidly. This paper proposes a DC network transmission technology based on network awareness. By monitoring the network status, the network status can be updated in real time. The transmission mechanism can be adjusted dynamically. In order to support the transmission in harsh environment, this paper constructs DC-TCP transmission based on Data Plane Development Kit (DPDK) TCP and DC-RoCE transmission based on RoCE, and designs a high-performance DC-TCP / DC-RoCE fusion technology framework. The simulation results show that the DC network transmission technology based on network awareness significantly improves the transmission capacity of the DC in complex environment.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3487075.3487153',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Domain Isolation in FPGA-Accelerated Cloud and Data Center Applications',\n",
       "  'authors': \"['Joel Mandebi Mbongue', 'Sujan Kumar Saha', 'Christophe Bobda']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"GLSVLSI '21: Proceedings of the 2021 on Great Lakes Symposium on VLSI\",\n",
       "  'abstract': 'Cloud and data center applications increasingly leverage FPGAs because of their performance/watt benefits and flexibility advantages over traditional processing cores such as CPUs and GPUs. As the rising demand for hardware acceleration gradually leads to FPGA multi-tenancy in the cloud, there are rising concerns about the security challenges posed by FPGA virtualization. Exposing space-shared FPGAs to multiple cloud tenants may compromise the confidentiality, integrity, and availability of FPGA-accelerated applications. In this work, we present a hardware/software architecture for domain isolation in FPGA-accelerated clouds and data centers with a focus on software-based attacks aiming at unauthorized access and information leakage. Our proposed architecture implements Mandatory Access Control security policies from software down to the hardware accelerators on FPGA. Our experiments demonstrate that the proposed architecture protects against such attacks with minimal area and communication overhead.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3453688.3461527',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Energy efficiency boost in the AI-infused POWER10 processor',\n",
       "  'authors': \"['Brian W. Thompto', 'Dung Q. Nguyen', 'José E. Moreira', 'Ramon Bertran', 'Hans Jacobson', 'Richard J. Eickemeyer', 'Rahul M. Rao', 'Michael Goulet', 'Marcy Byers', 'Christopher J. Gonzalez', 'Karthik Swaminathan', 'Nagu R. Dhanwada', 'Silvia M. Müller', 'Andreas Wagner', 'Satish Kumar Sadasivam', 'Robert K. Montoye', 'William J. Starke', 'Christian G. Zoellin', 'Michael S. Floyd', 'Jeffrey Stuecheli', 'Nandhini Chandramoorthy', 'John-David Wellman', 'Alper Buyuktosunoglu', 'Matthias Pflanz', 'Balaram Sinharoy', 'Pradip Bose']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': 'We present the novel micro-architectural features, supported by an innovative and novel pre-silicon methodology in the design of POWER10. The resulting projected energy efficiency boost over POWER9 is 2.6x at core level (for SPECint) and up to 3x at socket level. In addition, a new feature supporting inline AI acceleration was added to the POWER ISA and incorporated into the POWER10 processor core design. The resulting boost in SIMD/AI socket performance is projected to be up to 10x for FP32 and 21x for INT8 models of ResNet-50 and BERT-Large. In this paper, we describe the novel methodology deployed and used not only to obtain these efficiency boosts for traditional workloads, but also to infuse AI/ML/HPC capability directly into the POWER10 core.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00012',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Design and Implementation of FPGA-based High-speed Printing Printhead Controller',\n",
       "  'authors': \"['Haoran Wen', 'Feng Li', 'Shijie Zhou']\",\n",
       "  'date': 'May 2021',\n",
       "  'source': 'ICFEICT 2021: International Conference on Frontiers of Electronics, Information and Computation Technologies',\n",
       "  'abstract': 'This paper presents a new FPGA-based control system for high-speed digital printing printheads. System we proposed is dedicated to freeing general-purpose processors from the heavy burden of controlling printhead, and instead using FPGA hardware acceleration to achieve a more efficient, stable control system. The main work of this paper includes: (1) changing the traditional printing image processing algorithm suitable for general-purpose processors to make it a hardware-oriented algorithm that can be executed by FPGA in parallel; (2) Splitting data processing unit and timing control unit of the system, reducing their coupling, increasing the reliability, and also bringing higher maintainability and expandability to the system. After simulation tests, our system is able to achieve the expected functions, and the upper limit of system throughput can be improved by about 8 times compared with traditional methods.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3474198.3478155',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Research on DC Network Transmission Handover Technology Based on User Mode Sharing',\n",
       "  'authors': \"['Cheng Huang', 'Yanwei Wang', 'Jiaheng Fan', 'Le Yang', 'Junkai Liu', 'Hongwei Kan']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"CSAE '21: Proceedings of the 5th International Conference on Computer Science and Application Engineering\",\n",
       "  'abstract': 'RDMA over Converged Ethernet (RoCE) and Data Plane Development Kit (DPDK) TCP are both high-performance transmission technologies for Data center (DC), and they are often mixed due to different characteristics. When the application scenario or network status changes, the application needs to be switched from one communication mode to another. The current method is to stop one mode of communication and then re-establish another mode of network connection, which is less efficient. This paper constructs a DC network transmission handover technology based on user mode sharing, which includes: user mode integrated driver, end-to-end transmission handover, and multi-mode synchronous caching technology. User mode integration driver transfers the lower-level authority of the Ethernet card and RoCE upwards. On this basis, technologies such as end-to-end transmission handover and multi-mode synchronous cache can be established. The end-to-end transmission handover improves the overall performance and reduces the performance loss of switching streams one by one. Multi-mode synchronous cache technology applies synchronous processing technology to the user-mode driven technology, which reduces the performance loss caused by cache retransmission during handover. The simulation experiment verifies that the new technology has the characteristics of low latency in various switching scenarios.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3487075.3487154',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Networked Answer to \"Life, The Universe, and Everything\"',\n",
       "  'authors': \"['Giles Babich', 'Keith Bengston', 'Andrew Bolin', 'John Bunton', 'Yuqing Chen', 'Grant Hampson', 'David Humphrey', 'Guillaume Jourjon']\",\n",
       "  'date': 'December 2021',\n",
       "  'source': \"ANCS '21: Proceedings of the Symposium on Architectures for Networking and Communications Systems\",\n",
       "  'abstract': 'In the last few years, Input/Output (I/O) bandwidth limitation of legacy computer architectures forced us to reconsider where and how to store and compute data across a large range of applications. This shift has been made possible with the concurrent development of both smartNICs and programmable switches with a common programming language (P4), and the advent of attached High Bandwidth Memory within smartNICs/FPGAs. Recently, proposals to use this kind of technology have emerged to tackle computer science related issues such as fast consensus algorithm in the network, network accelerated key-value stores, machine learning, or data-center data aggregation. In this paper, we introduce a novel architecture that leverages these advancements to potentially accelerate and improve the processing of radio-astronomy Digital Signal Processing (DSP), such as correlators or beamformers, at unprecedented continuous rates in what we have called the \"Atomic COTS\" design. We give an overview of this new type of architecture to accelerate digital signal processing, leveraging programmable switches and HBM capable FPGAs. We also discuss how to handle radio astronomy data streams to pre-process this stream of data for astronomy science products such as pulsar timing and search. Finally, we illustrate, using a proof of concept, how we can process emulated data from the Square Kilometer Array (SKA) project to time pulsars.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3493425.3502770',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Cache abstraction for data race detection in heterogeneous systems with non-coherent accelerators',\n",
       "  'authors': \"['May Young', 'Alan J. Hu', 'Guy G. F. Lemieux']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': 'LCTES 2021: Proceedings of the 22nd ACM SIGPLAN/SIGBED International Conference on Languages, Compilers, and Tools for Embedded Systems',\n",
       "  'abstract': 'Embedded systems are becoming increasingly complex and heterogeneous, featuring multiple processor cores (which might themselves be heterogeneous) as well as specialized hardware accelerators, all accessing shared memory. Many accelerators are non-coherent (i.e., do not support hardware cache coherence) because it reduces hardware complexity, cost, and power consumption, while potentially offering superior performance. However, the disadvantage of non-coherence is that the software must explicitly synchronize between accelerators and processors, and this synchronization is notoriously error-prone.   We propose an analysis technique to find data races in software for heterogeneous systems that include non-coherent accelerators. Our approach builds on classical results for data race detection, but the challenge turns out to be analyzing cache behavior rather than the behavior of the non-coherent accelerators. Accordingly, our central contribution is a novel, sound (data-race-preserving) abstraction of cache behavior. We prove our abstraction sound, and then to demonstrate the precision of our abstraction, we implement it in a simple dynamic race detector for a system with a processor and a massively parallel accelerator provided by a commercial FPGA-based accelerator vendor. On eleven software examples provided by the vendor, the tool had zero false positives and was able to detect previously unknown data races in 2 of the 11 examples.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3461648.3463856',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'NDS: N-Dimensional Storage',\n",
       "  'authors': \"['Yu-Chia Liu', 'Hung-Wei Tseng']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"MICRO '21: MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture\",\n",
       "  'abstract': 'Demands for efficient computing among applications that use high-dimensional datasets have led to multi-dimensional computers—computers that leverage heterogeneous processors/accelerators offering various processing models to support multi-dimensional compute kernels. Yet the front-end for these processors/accelerators is inefficient, as memory/storage systems often expose only entrenched linear-space abstractions to an application, and they often ignore the benefits of modern memory/storage systems, such as support for multi-dimensionality through different types of parallel access.  This paper presents N-Dimensional Storage (NDS), a novel, multi-dimensional memory/storage system that fulfills the demands of modern hardware accelerators and applications. NDS abstracts memory arrays as native storage that applications can use to describe data locations and uses coordinates in any application-defined multi-dimensional space, thereby avoiding the software overhead associated with data-object transformations. NDS gauges the application demand underlying memory-device architectures in order to intelligently determine the physical data layout that maximizes access bandwidth and minimizes the overhead of presenting objects for arbitrary applications.  This paper demonstrates an efficient architecture in supporting NDS. We evaluate a set of linear/tensor algebra workloads along with graph and data-mining algorithms on custom-built systems using each architecture. Our result shows a 5.73 × speedup with appropriate architectural support.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3466752.3480122',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'In-sensor classification with boosted race trees',\n",
       "  'authors': \"['Georgios Tzimpragos', 'Advait Madhavan', 'Dilip Vasudevan', 'Dmitri Strukov', 'Timothy Sherwood']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': 'Communications of the ACM',\n",
       "  'abstract': 'When extremely low-energy processing is required, the choice of data representation makes a tremendous difference. Each representation (e.g., frequency domain, residue coded, and log-scale) embodies a different set of tradeoffs based on the algebraic operations that are either easy or hard to perform in that domain. We demonstrate the potential of a novel form of encoding, race logic, in which information is represented as the delay in the arrival of a signal. Under this encoding, the ways in which signal delays interact and interfere with one another define the operation of the system. Observations of the relative delays (for example, the outcome of races between signals) define the output of the computation. Interestingly, completely standard hardware logic elements can be repurposed to this end and the resulting embedded systems have the potential to be extremely energy efficient. To realize this potential in a practical design, we demonstrate two different approaches to the creation of programmable tree-based ensemble classifiers in an extended set of race logic primitives; we explore the trade-offs inherent to their operation across sensor, hardware architecture, and algorithm; and we compare the resulting designs against traditional state-of-the-art hardware techniques.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3460223',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Cloud Building Block Chip for Creating FPGA and ASIC Clouds',\n",
       "  'authors': \"['Atakan Doğan', 'Kemal Ebcioğlu']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Reconfigurable Technology and Systems',\n",
       "  'abstract': 'Hardware-accelerated cloud computing systems based on FPGA chips (FPGA cloud) or ASIC chips (ASIC cloud) have emerged as a new technology trend for power-efficient acceleration of various software applications. However, the operating systems and hypervisors currently used in cloud computing will lead to power, performance, and scalability problems in an exascale cloud computing environment. Consequently, the present study proposes a parallel hardware hypervisor system that is implemented entirely in special-purpose hardware, and that virtualizes application-specific multi-chip supercomputers, to enable virtual supercomputers to share available FPGA and ASIC resources in a cloud system. In addition to the virtualization of multi-chip supercomputers, the system’s other unique features include simultaneous migration of multiple communicating hardware tasks, and on-demand increase or decrease of hardware resources allocated to a virtual supercomputer. Partitioning the flat hardware design of the proposed hypervisor system into multiple partitions and applying the chip unioning technique to its partitions, the present study introduces a cloud building block chip that can be used to create FPGA or ASIC clouds as well. Single-chip and multi-chip verification studies have been done to verify the functional correctness of the hypervisor system, which consumes only a fraction of (10%) hardware resources.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3466822',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': \"How to Analyze, Preserve, and Communicate Leonardo's Drawing? A Solution to Visualize in RTR Fine Art Graphics Established from “the Best Sense”\",\n",
       "  'authors': \"['Fabrizio Ivan Apollonio', 'Riccardo Foschi', 'Marco Gaiani', 'Simone Garagnani']\",\n",
       "  'date': 'None',\n",
       "  'source': 'Journal on Computing and Cultural Heritage ',\n",
       "  'abstract': \"Original hand drawings by Leonardo are astonishing collections of knowledge, superb representations of the artist's way of working, which proves the technical and cultural peak of the Renaissance era. However, due to their delicate and fragile nature, they are hard to manipulate and compulsory to preserve. To overcome this problem we developed, in a 10-year-long research program, a complete workflow to produce a system able to replace, investigate, describe and communicate ancient fine drawings through what Leonardo calls “the best sense” (i.e., the view), the so-called ISLe (InSightLeonardo). The outcoming visualization app is targeted to a wide audience made of museum visitors and, most importantly, art historians, scholars, conservators, and restorers. This article describes a specific feature of the workflow: the appearance modeling with the aim of an accurate Real-Time Rendering (RTR) visualization. This development is based on the direct observation of five among the most known Leonardo da Vinci's drawings, spanning his entire activity as a draftsman, and it is the result of an accurate analysis of drawing materials used by Leonardo, in which peculiarities of materials are digitally reproduced at the various scales exploiting solutions that favor the accuracy of perceived reproduction instead of the fidelity to the physical model and their ability to be efficiently implemented over a standard GPU-accelerated RTR pipeline. Results of the development are exemplified on five of Leonardo's drawings and multiple evaluations of the results, subjective and objective, are illustrated, aiming to assess potential and critical issues of the application.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3433606',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'GenStore: a high-performance in-storage processing system for genome sequence analysis',\n",
       "  'authors': \"['Nika Mansouri Ghiasi', 'Jisung Park', 'Harun Mustafa', 'Jeremie Kim', 'Ataberk Olgun', 'Arvid Gollwitzer', 'Damla Senol Cali', 'Can Firtina', 'Haiyu Mao', 'Nour Almadhoun Alserr', 'Rachata Ausavarungnirun', 'Nandita Vijaykumar', 'Mohammed Alser', 'Onur Mutlu']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'Read mapping is a fundamental step in many genomics applications. It is used to identify potential matches and differences between fragments (called reads) of a sequenced genome and an already known genome (called a reference genome). Read mapping is costly because it needs to perform approximate string matching (ASM) on large amounts of data. To address the computational challenges in genome analysis, many prior works propose various approaches such as accurate filters that select the reads within a dataset of genomic reads (called a read set) that must undergo expensive computation, efficient heuristics, and hardware acceleration. While effective at reducing the amount of expensive computation, all such approaches still require the costly movement of a large amount of data from storage to the rest of the system, which can significantly lower the end-to-end performance of read mapping in conventional and emerging genomics systems.  We propose GenStore, the first in-storage processing system designed for genome sequence analysis that greatly reduces both data movement and computational overheads of genome sequence analysis by exploiting low-cost and accurate in-storage filters. GenStore leverages hardware/software co-design to address the challenges of in-storage processing, supporting reads with 1)\\xa0different properties such as read lengths and error rates, which highly depend on the sequencing technology, and 2)\\xa0different degrees of genetic variation compared to the reference genome, which highly depends on the genomes that are being compared. Through rigorous analysis of read mapping processes of reads with different properties and degrees of genetic variation, we meticulously design low-cost hardware accelerators and data/computation flows inside a NAND flash-based solid-state drive (SSD). Our evaluation using a wide range of real genomic datasets shows that GenStore, when implemented in three modern NAND flash-based SSDs, significantly improves the read mapping performance of state-of-the-art software (hardware) baselines by 2.07-6.05× (1.52-3.32×) for read sets with high similarity to the reference genome and 1.45-33.63× (2.70-19.2×) for read sets with low similarity to the reference genome.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503222.3507702',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'ReuseTracker: Fast Yet Accurate Multicore Reuse Distance Analyzer',\n",
       "  'authors': \"['Muhammad Aditya Sasongko', 'Milind Chabbi', 'Mandana Bagheri Marzijarani', 'Didem Unat']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Architecture and Code Optimization',\n",
       "  'abstract': 'One widely used metric that measures data locality is reuse distance—the number of unique memory locations that are accessed between two consecutive accesses to a particular memory location. State-of-the-art techniques that measure reuse distance in parallel applications rely on simulators or binary instrumentation tools that incur large performance and memory overheads. Moreover, the existing sampling-based tools are limited to measuring reuse distances of a single thread and discard interactions among threads in multi-threaded programs. In this work, we propose ReuseTracker—a fast and accurate reuse distance analyzer that leverages existing hardware features in commodity CPUs. ReuseTracker is designed for multi-threaded programs and takes cache-coherence effects into account. By utilizing hardware features like performance monitoring units and debug registers, ReuseTracker can accurately profile reuse distance in parallel applications with much lower overheads than existing tools. It introduces only 2.9× runtime and 2.8× memory overheads. Our tool achieves 92% accuracy when verified against a newly developed configurable benchmark that can generate a variety of different reuse distance patterns. We demonstrate the tool’s functionality with two use-case scenarios using PARSEC, Rodinia, and Synchrobench benchmark suites where ReuseTracker guides code refactoring in these benchmarks by detecting spatial reuses in shared caches that are also false sharing and successfully predicts whether some benchmarks in these suites can benefit from adjacent cache line prefetch optimization.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3484199',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Cohmeleon: Learning-Based Orchestration of Accelerator Coherence in Heterogeneous SoCs',\n",
       "  'authors': \"['Joseph Zuckerman', 'Davide Giri', 'Jihye Kwon', 'Paolo Mantovani', 'Luca P. Carloni']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"MICRO '21: MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture\",\n",
       "  'abstract': 'One of the most critical aspects of integrating loosely-coupled accelerators in heterogeneous SoC architectures is orchestrating their interactions with the memory hierarchy, especially in terms of navigating the various cache-coherence options: from accelerators accessing off-chip memory directly, bypassing the cache hierarchy, to accelerators having their own private cache. By running real-size applications on FPGA-based prototypes of many-accelerator multi-core SoCs, we show that the best cache-coherence mode for a given accelerator varies at runtime, depending on the accelerator’s characteristics, the workload size, and the overall SoC status.  Cohmeleon applies reinforcement learning to select the best coherence mode for each accelerator dynamically at runtime, as opposed to statically at design time. It makes these selections adaptively, by continuously observing the system and measuring its performance. Cohmeleon is accelerator-agnostic, architecture-independent, and it requires minimal hardware support. Cohmeleon is also transparent to application programmers and has a negligible software overhead. FPGA-based experiments show that our runtime approach offers, on average, a 38% speedup with a 66% reduction of off-chip memory accesses compared to state-of-the-art design-time approaches. Moreover, it can match runtime solutions that are manually tuned for the target architecture.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3466752.3480065',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Multiple-tasks on multiple-devices (MTMD): exploiting concurrency in heterogeneous managed runtimes',\n",
       "  'authors': \"['Michail Papadimitriou', 'Eleni Markou', 'Juan Fumero', 'Athanasios Stratikopoulos', 'Florin Blanaru', 'Christos Kotselidis']\",\n",
       "  'date': 'April 2021',\n",
       "  'source': 'VEE 2021: Proceedings of the 17th ACM SIGPLAN/SIGOPS International Conference on Virtual Execution Environments',\n",
       "  'abstract': 'Modern commodity devices are nowadays equipped with a plethora of heterogeneous devices serving different purposes. Being able to exploit such heterogeneous hardware accelerators to their full potential is of paramount importance in the pursuit of higher performance and energy efficiency. Towards these objectives, the reduction of idle time of each device as well as the concurrent program execution across different accelerators can lead to better scalability within the computing platform.  In this work, we propose a novel approach for enabling a Java-based heterogeneous managed runtime to automatically and efficiently deploy multiple tasks on multiple devices. We extend TornadoVM with parallel execution of bytecode interpreters to dynamically and concurrently manage and execute arbitrary tasks across multiple OpenCL-compatible devices. In addition, in order to achieve an efficient device-task allocation, we employ a machine learning approach with a multiple-classification architecture of Extra-Trees-Classifiers. Our proposed solution has been evaluated over a suite of 12 applications split into three different groups. Our experimental results showcase performance improvements up 83% compared to all tasks running on the single best device, while reaching up to 91% of the oracle performance.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3453933.3454019',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Vector runahead',\n",
       "  'authors': \"['Ajeya Naithani', 'Sam Ainsworth', 'Timothy M. Jones', 'Lieven Eeckhout']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': 'The memory wall places a significant limit on performance for many modern workloads. These applications feature complex chains of dependent, indirect memory accesses, which cannot be picked up by even the most advanced microarchitectural prefetchers. The result is that current out-of-order superscalar processors spend the majority of their time stalled. While it is possible to build special-purpose architectures to exploit the fundamental memory-level parallelism, a microarchitectural technique to automatically improve their performance in conventional processors has remained elusive. Runahead execution is a tempting proposition for hiding latency in program execution. However, to achieve high memory-level parallelism, a standard runahead execution skips ahead of cache misses. In modern workloads, this means it only prefetches the first cache-missing load in each dependent chain. We argue that this is not a fundamental limitation. If runahead were instead to stall on cache misses to generate dependent chain loads, then it could regain performance if it could stall on many at once. With this insight, we present Vector Runahead, a technique that prefetches entire load chains and speculatively reorders scalar operations from multiple loop iterations into vector format to bring in many independent loads at once. Vectorization of the runahead instruction stream increases the effective fetch/decode bandwidth with reduced resource requirements, to achieve high degrees of memory-level parallelism at a much faster rate. Across a variety of memory-latency-bound indirect workloads, Vector Runahead achieves a 1.79X performance speedup on a large out-of-order superscalar system, significantly improving on state-of-the-art techniques.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00024',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'BurstZ+: Eliminating The Communication Bottleneck of Scientific Computing Accelerators via Accelerated Compression',\n",
       "  'authors': \"['Gongjin Sun', 'Seongyoung Kang', 'Sang-Woo Jun']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Reconfigurable Technology and Systems',\n",
       "  'abstract': 'We present BurstZ+, an accelerator platform that eliminates the communication bottleneck between PCIe-attached scientific computing accelerators and their host servers, via hardware-optimized compression. While accelerators such as GPUs and FPGAs provide enormous computing capabilities, their effectiveness quickly deteriorates once data is larger than its on-board memory capacity, and performance becomes limited by the communication bandwidth of moving data between the host memory and accelerator. Compression has not been very useful in solving this issue due to performance and efficiency issues of compressing floating point numbers, which scientific data often consists of. BurstZ+ is an FPGA-based prototype accelerator platform which addresses the bandwidth issue via a class of novel hardware-optimized floating point compression algorithm called ZFP-V. We demonstrate that BurstZ+ can completely remove the host-side communication bottleneck for accelerators, using multiple stencil kernels with a wide range of operational intensities. Evaluated against hand-optimized implementations of kernel accelerators of the same architecture, our single-pipeline BurstZ+ prototype outperforms an accelerator without compression by almost 4×, and even an accelerator with enough memory for the entire dataset by over 2×. Furthermore, the projected performance of BurstZ+ on a future, faster FPGA scales to almost 7× that of the same accelerator without compression, whose performance is still limited by the PCIe bandwidth.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3476831',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'BlockMaestro: enabling programmer-transparent task-based execution in GPU systems',\n",
       "  'authors': \"['AmirAli Abdolrashidi', 'Hodjat Asghari Esfeden', 'Ali Jahanshahi', 'Kaustubh Singh', 'Nael Abu-Ghazaleh', 'Daniel Wong']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': 'As modern GPU workloads grow in size and complexity, there is an ever-increasing demand for GPU computational power. Emerging workloads contain hundreds or thousands of GPU kernel launches, which incur high overheads, and exhibit data-dependent behavior between kernels, which requires synchronization, leading to GPU under-utilization. Task-based execution models have been proposed to solve these issues, but they require significant programmer effort to port applications to proprietary task-based programming models in order to specify tasks and task dependencies. To address this need, we propose BlockMaestro, a software-hardware solution that combines command queue reordering, kernel-launch-time static analysis, and runtime hardware support to dynamically identify and resolve thread-block level data dependencies between kernels. Through static analysis of memory access patterns at kernel-launch-time, BlockMaestro can extract inter-kernel thread block-level data dependencies. BlockMaestro also introduces kernel pre-launching to reduce the kernel launch overheads experienced by multiple dependent kernels. Correctness is enforced by dynamically resolving thread block-level data dependency at runtime through hardware support. BlockMaestro achieves an average speedup of 51.76% (up to 2.92x) on data-dependent benchmarks, and requires minimal hardware overhead.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00034',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'HTNN: deep learning in heterogeneous transform domains with sparse-orthogonal weights',\n",
       "  'authors': \"['Yu Chen', 'Bowen Liu', 'Pierre Abillama', 'Hun-Seok Kim']\",\n",
       "  'date': 'July 2021',\n",
       "  'source': \"ISLPED '21: Proceedings of the ACM/IEEE International Symposium on Low Power Electronics and Design\",\n",
       "  'abstract': 'Convolutional neural networks (CNNs) achieved great success on various tasks in recent years. Their applications to low power and low cost hardware platforms, however, have been often limited due to extensive complexity of convolution layers. We present a new class of transform domain deep neural networks (DNNs), where convolution operations are replaced by element-wise multiplications in heterogeneous transform domains. To further reduce the network complexity, we propose a framework to learn sparse-orthogonal weights in heterogeneous transform domains co-optimized with a hardware-efficient accelerator architecture to minimize the overhead of handling sparse weights. Furthermore, sparse-orthogonal weights are non-uniformly quantized with canonical-signed-digit (CSD) representations to substitute multiplications with simpler additions. The proposed approach reduces the complexity by a factor of 4.9 -- 6.8 x without compromising the DNN accuracy compared to equivalent CNNs that employ sparse (pruned) weights. The code is available at https://github.com/unchenyu/HTNN.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISLPED52811.2021.9502477',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'REDUCT: keep it close, keep it cool!: efficient scaling of DNN inference on multi-core CPUs with near-cache compute',\n",
       "  'authors': \"['Anant V. Nori', 'Rahul Bera', 'Shankar Balachandran', 'Joydeep Rakshit', 'Om J. Omer', 'Avishaii Abuhatzera', 'Belliappa Kuttanna', 'Sreenivas Subramoney']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': 'Deep Neural Networks (DNN) are used in a variety of applications and services. With the evolving nature of DNNs, the race to build optimal hardware (both in datacenter and edge) continues. General purpose multi-core CPUs offer unique attractive advantages for DNN inference at both datacenter [60] and edge [71]. Most of the CPU pipeline design complexity is targeted towards optimizing general-purpose single thread performance, and is overkill for relatively simpler, but still hugely important, data parallel DNN inference workloads. Addressing this disparity efficiently can enable both raw performance scaling and overall performance/Watt improvements for multi-core CPU DNN inference. We present REDUCT, where we build innovative solutions that bypass traditional CPU resources which impact DNN inference power and limit its performance. Fundamentally, REDUCT\\'s \"Keep it close\" policy enables consecutive pieces of work to be executed close to each other. REDUCT enables instruction delivery/decode close to execution and instruction execution close to data. Simple ISA extensions encode the fixed-iteration count loop-y workload behavior enabling an effective bypass of many power-hungry front-end stages of the wide Out-of-Order (OoO) CPU pipeline. Per core performance scales efficiently by distributing lightweight tensor compute near all caches in a multi-level cache hierarchy. This maximizes the cumulative utilization of the existing architectural bandwidth resources in the system and minimizes movement of data. Across a number of DNN models, REDUCT achieves a 2.3X increase in convolution performance/Watt with a 2X to 3.94X scaling in raw performance. Similarly, REDUCT achieves a 1.8X increase in inner-product performance/Watt with 2.8X scaling in performance. REDUCT performance/power scaling is achieved with no increase to cache capacity or bandwidth and a mere 2.63% increase in area. Crucially, REDUCT operates entirely within the CPU programming and memory model, simplifying software development, while achieving performance similar to or better than state-of-the-art Domain Specific Accelerators (DSA) for DNN inference, providing fresh design choices in the AI era.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00022',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A Framework for Reproducible Data Plane Performance Modeling',\n",
       "  'authors': \"['Dominik Scholz', 'Hasanin Harkous', 'Sebastian Gallenmüller', 'Henning Stubbe', 'Max Helm', 'Benedikt Jaeger', 'Nemanja Deric', 'Endri Goshi', 'Zikai Zhou', 'Wolfgang Kellerer', 'Georg Carle']\",\n",
       "  'date': 'December 2021',\n",
       "  'source': \"ANCS '21: Proceedings of the Symposium on Architectures for Networking and Communications Systems\",\n",
       "  'abstract': 'Languages for programming data planes like P4 sparked a plethora of new applications in the data plane. The dynamic, evolving environment makes it challenging to understand what performance can be expected when running a program in a specific data plane target. However, knowing this is crucial for network operators when upgrading their networks. We present a framework for the reproducible analysis and modeling of P4 program components. By defining and generating precise specifications of the experiments, we separate fully auto-generated components from testbed- or target-specific parts. Measurement results are used to derive performance models automatically. These can then be used to compare the measured with the theoretical performance, or to model the cost of entire paths through the data plane. In two case studies, we use our framework to discover and model selected behavior for a DPDK-based software target and for the NFP-4000 SmartNIC platform.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3493425.3502756',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Survey of Transient Execution Attacks and Their Mitigations',\n",
       "  'authors': \"['Wenjie Xiong', 'Jakub Szefer']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Computing Surveys',\n",
       "  'abstract': 'Transient execution attacks, also known as speculative execution attacks, have drawn much interest in the last few years as they can cause critical data leakage. Since the first disclosure of Spectre and Meltdown attacks in January 2018, a number of new transient execution attack types have been demonstrated targeting different processors. A transient execution attack consists of two main components: transient execution itself and a covert channel that is used to actually exfiltrate the information.Transient execution is a result of the fundamental features of modern processors that are designed to boost performance and efficiency, while covert channels are unintended information leakage channels that result from temporal and spatial sharing of the micro-architectural components. Given the severity of the transient execution attacks, they have motivated computer architects in both industry and academia to rethink the design of the processors and to propose hardware defenses. To help understand the transient execution attacks, this survey summarizes the phases of the attacks and the security boundaries across which the information is leaked in different attacks.This survey further analyzes the causes of transient execution as well as the different types of covert channels and presents a taxonomy of the attacks based on the causes and types. This survey in addition presents metrics for comparing different aspects of the transient execution attacks and uses them to evaluate the feasibility of the different attacks. This survey especially considers both existing attacks and potential new attacks suggested by our analysis. This survey finishes by discussing different mitigations that have so far been proposed at the micro-architecture level and discusses their benefits and limitations.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3442479',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'SCALPEL: Exploring the Limits of Tag-enforced Compartmentalization',\n",
       "  'authors': \"['Nick Roessler', 'André DeHon']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Journal on Emerging Technologies in Computing Systems',\n",
       "  'abstract': 'We present Secure Compartments Automatically Learned and Protected by Execution using Lightweight metadata (SCALPEL), a tool for automatically deriving compartmentalization policies and lowering them to a tagged architecture for hardware-accelerated enforcement. SCALPEL allows a designer to explore high-quality points in the privilege-reduction vs. performance overhead tradeoff space using analysis tools and a detailed knowledge of the target architecture to make best use of the available hardware. SCALPEL automatically implements hundreds of compartmentalization strategies across the privilege-performance tradeoff space, all without manual tagging or code restructuring. SCALPEL uses two novel optimizations for achieving highly performant policies: the first is an algorithm for packing policies into working sets of rules for favorable rule cache characteristics, and the second is a rule prefetching system that allows it to exploit the highly predictable nature of compartmentalization rules. To create policies, SCALPEL introduces a quantitative privilege metric (the Overprivilege Ratio) that is used to drive its algorithmic compartment generation. We implement SCALPEL on a FreeRTOS stack and target a tag-extended RISC-V core. Our results show that SCALPEL-created policies can reduce overprivilege by orders of magnitude with hundreds of logical compartments while imposing low overheads (<5%).',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3461673',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Zero inclusion victim: isolating core caches from inclusive last-level cache evictions',\n",
       "  'authors': \"['Mainak Chaudhuri']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': 'The most widely used last-level cache (LLC) architecture in the microprocessors has been the inclusive LLC design. The popularity of the inclusive design stems from the bandwidth optimization and simplification it offers to the implementation of the cache coherence protocols. However, inclusive LLCs have always been associated with the curse of inclusion victims. An inclusion victim is a block that must be forcefully replaced from the inner levels of the cache hierarchy when the copy of the block is replaced from the inclusive LLC. This tight coupling between the LLC victims and the inner-level cache contents leads to three major drawbacks. First, live inclusion victims can lead to severe performance degradation depending on the LLC replacement policies. Second, a process can victimize the blocks of another process in an LLC shared by multiple cores and this can be exploited to leak information through well-known eviction-based timing side-channels. An inclusive LLC makes these channels much less noisy due to the presence of inclusion victims which allow the malicious processes to control the contents of the per-core private caches through LLC evictions. Third, to reduce the impact of the aforementioned two drawbacks, the inner-level caches, particularly the mid-level cache in a three-level inclusive cache hierarchy, must be kept small even if a larger mid-level cache could have been beneficial in the absence of inclusion victims. We observe that inclusion victims are not fundamental to the inclusion property, but arise due to the way the contents of an inclusive LLC are managed. Motivated by this observation, we introduce a fundamentally new inclusive LLC design named the Zero Inclusion Victim (ZIV) LLC that guarantees freedom from inclusion victims while retaining all advantages of an inclusive LLC. This is the first inclusive LLC design proposal to offer such a guarantee, thereby completely isolating the core caches from LLC evictions. We observe that the root cause of inclusion victims is the constraint that an LLC victim must be chosen from the set pointed to by the set indexing function. The ZIV LLC relaxes this constraint only when necessary by efficiently and minimally enabling a global victim selection scheme in the inclusive LLC to avoid generation of inclusion victims. Detailed simulations conducted with a chip-multiprocessor model using multi-programmed and multi-threaded workloads show that the ZIV LLC gracefully supports large mid-level caches (e.g., half the size of the LLC) and delivers performance close to a non-inclusive LLC for different classes of LLC replacement policies. We also show that the ZIV LLC comfortably outperforms the existing related proposals and its performance lead grows with increasing mid-level cache capacity.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00015',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Specifying with Interface and Trait Abstractions in Abstract State Machines: A Controlled Experiment',\n",
       "  'authors': \"['Philipp Paulweber', 'Georg Simhandl', 'Uwe Zdun']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Software Engineering and Methodology',\n",
       "  'abstract': 'Abstract State Machine (ASM) theory is a well-known state-based formal method. As in other state-based formal methods, the proposed specification languages for ASMs still lack easy-to-comprehend abstractions to express structural and behavioral aspects of specifications. Our goal is to investigate object-oriented abstractions such as interfaces and traits for ASM-based specification languages. We report on a controlled experiment with 98 participants to study the specification efficiency and effectiveness in which participants needed to comprehend an informal specification as problem (stimulus) in form of a textual description and express a corresponding solution in form of a textual ASM specification using either interface or trait syntax extensions. The study was carried out with a completely randomized design and one alternative (interface or trait) per experimental group. The results indicate that specification effectiveness of the traits experiment group shows a better performance compared to the interfaces experiment group, but specification efficiency shows no statistically significant differences. To the best of our knowledge, this is the first empirical study studying the specification effectiveness and efficiency of object-oriented abstractions in the context of formal methods.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3450968',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Connection Pruning for Deep Spiking Neural Networks with On-Chip Learning',\n",
       "  'authors': \"['Thao N. N. Nguyen', 'Bharadwaj Veeravalli', 'Xuanyao Fong']\",\n",
       "  'date': 'July 2021',\n",
       "  'source': 'ICONS 2021: International Conference on Neuromorphic Systems 2021',\n",
       "  'abstract': 'Long training time hinders the potential of the deep, large-scale Spiking Neural Network (SNN) with the on-chip learning capability to be realized on the embedded systems hardware. Our work proposes a novel connection pruning approach that can be applied during the on-chip Spike Timing Dependent Plasticity (STDP)-based learning to optimize the learning time and the network connectivity of the deep SNN. We applied our approach to a deep SNN with the Time To First Spike (TTFS) coding and has successfully achieved 2.1x speed-up and 64% energy savings in the on-chip learning and reduced the network connectivity by 92.83%, without incurring any accuracy loss. Moreover, the connectivity reduction results in 2.83x speed-up and 78.24% energy savings in the inference. Evaluation of our proposed approach on the Field Programmable Gate Array (FPGA) platform revealed 0.56% power overhead was needed to implement the pruning algorithm.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3477145.3477157',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'I see dead μops: leaking secrets via Intel/AMD micro-op caches',\n",
       "  'authors': \"['Xida Ren', 'Logan Moody', 'Mohammadkazem Taram', 'Matthew Jordan', 'Dean M. Tullsen', 'Ashish Venkat']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': 'Modern Intel, AMD, and ARM processors translate complex instructions into simpler internal micro-ops that are then cached in a dedicated on-chip structure called the micro-op cache. This work presents an in-depth characterization study of the micro-op cache, reverse-engineering many undocumented features, and further describes attacks that exploit the micro-op cache as a timing channel to transmit secret information. In particular, this paper describes three attacks - (1) a same thread cross-domain attack that leaks secrets across the user-kernel boundary, (2) a cross-SMT thread attack that transmits secrets across two SMT threads via the micro-op cache, and (3) transient execution attacks that have the ability to leak an unauthorized secret accessed along a misspeculated path, even before the transient instruction is dispatched to execution, breaking several existing invisible speculation and fencing-based solutions that mitigate Spectre.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00036',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'xDNN: Inference for Deep Convolutional Neural Networks',\n",
       "  'authors': '[\"Paolo D\\'Alberto\", \\'Victor Wu\\', \\'Aaron Ng\\', \\'Rahul Nimaiyar\\', \\'Elliott Delaye\\', \\'Ashish Sirasao\\']',\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Reconfigurable Technology and Systems',\n",
       "  'abstract': 'We present xDNN, an end-to-end system for deep-learning inference based on a family of specialized hardware processors synthesized on Field-Programmable Gate Array (FPGAs) and Convolution Neural Networks (CNN). We present a design optimized for low latency, high throughput, and high compute efficiency with no batching. The design is scalable and a parametric function of the number of multiply-accumulate units, on-chip memory hierarchy, and numerical precision. The design can produce a scale-down processor for embedded devices, replicated to produce more cores for larger devices, or resized to optimize efficiency. On Xilinx Virtex Ultrascale+ VU13P FPGA, we achieve 800 MHz that is close to the Digital Signal Processing maximum frequency and above 80% efficiency of on-chip compute resources.On top of our processor family, we present a runtime system enabling the execution of different networks for different input sizes (i.e., from 224× 224 to 2048× 1024). We present a compiler that reads CNNs from native frameworks (i.e., MXNet, Caffe, Keras, and Tensorflow), optimizes them, generates codes, and provides performance estimates. The compiler combines quantization information from the native environment and optimizations to feed the runtime with code as efficient as any hardware expert could write. We present tools partitioning a CNN into subgraphs for the division of work to CPU cores and FPGAs. Notice that the software will not change when or if the FPGA design becomes an ASIC, making our work vertical and not just a proof-of-concept FPGA project.We show experimental results for accuracy, latency, and power for several networks: In summary, we can achieve up to 4 times higher throughput, 3 times better power efficiency than the GPUs, and up to 20 times higher throughput than the latest CPUs. To our knowledge, we provide solutions faster than any previous FPGA-based solutions and comparable to any other top-of-the-shelves solutions.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3473334',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'DFI: The Data Flow Interface for High-Speed Networks',\n",
       "  'authors': \"['Lasse Thostrup', 'Jan Skrzypczak', 'Matthias Jasny', 'Tobias Ziegler', 'Carsten Binnig']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data\",\n",
       "  'abstract': 'In this paper, we propose the Data Flow Interface (DFI) as a way to make it easier for data processing systems to exploit high-speed networks without the need to deal with the complexity of RDMA. By lifting the level of abstraction, DFI factors out much of the complexity of network communication and makes it easier for developers to declaratively express how data should be efficiently routed to accomplish a given distributed data processing task. As we show in our experiments, DFI is able to support a wide variety of data-centric applications with high performance at a low complexity for the applications.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3448016.3452816',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'First-person Cinematographic Videogames: Game Model, Authoring Environment, and Potential for Creating Affection for Places',\n",
       "  'authors': \"['Ivan BlečIć', 'Sara Cuccu', 'Filippo Andrea Fanni', 'Vittoria Frau', 'Riccardo Macis', 'Valeria Saiu', 'Martina Senis', 'Lucio Davide Spano', 'Alessandro Tola']\",\n",
       "  'date': 'None',\n",
       "  'source': 'Journal on Computing and Cultural Heritage ',\n",
       "  'abstract': \"We present and explore the fruitfulness of “first-person cinematographic videogames,” a game model we have devised for the promotion of cultural, environmental, and territorial heritage. To support and foster the development of these type of games, we have developed a Web-based user-friendly authoring environment, extensively presented in the article. While employing standard first-person point-and-click game mechanics, the game model's distinctive feature is that the game environment is not based on a digital reconstruction (3D model) of the real-world settings but on cinematographic techniques combining videos and photos of existing places, integrating videoclips of mostly practical effects to obtain the interactivity typical of the first-person point-and-click adventure games. Our goal with such a game model is to mobilise mechanisms of engendering affection for real-world places when they become settings of the game world, arousing in the player forms of affection, attachment, and desire to visit them.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3446977',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Temporal and SFQ pulse-streams encoding for area-efficient superconducting accelerators',\n",
       "  'authors': \"['Patricia Gonzalez-Guerrero', 'Meriam Gay Bautista', 'Darren Lyles', 'George Michelogiannakis']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'Superconducting technology is a prime candidate for the future of computing. However, current superconducting prototypes are limited to small-scale examples due to stringent area constraints and complex architectures inspired from voltage-level encoding in CMOS; this is at odds with the ps-wide Single Quantum Flux (SFQ) pulses used in superconductors to carry information. In this work, we propose a wave-pipelined Unary SFQ (U-SFQ) architecture that leverages the advantages of two data representations: pulse-streams and Race Logic (RL). We introduce novel building blocks such as multipliers, adders, and memory cells, which leverage the natural properties of SFQ pulses to mitigate area constraints. We then design and simulate three popular hardware accelerators: i) a Processing Element (PE), typically used in spatial architectures; ii) A dot-product-unit (DPU), one of the most popular accelerators in artificial neural networks and digital signal processing (DSP); and iii) A Finite Impulse Response (FIR) filter, a popular and computationally demanding DSP accelerator. The proposed U-SFQ building blocks require up to 200× fewer JJs compared to their SFQ binary counterparts, exposing an area-delay trade-off. This work mitigates the stringent area constraints of superconducting technology.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503222.3507765',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'CoSA: scheduling by <u>c</u>onstrained <u>o</u>ptimization for <u>s</u>patial <u>a</u>ccelerators',\n",
       "  'authors': \"['Qijing Huang', 'Minwoo Kang', 'Grace Dinh', 'Thomas Norell', 'Aravind Kalaiah', 'James Demmel', 'John Wawrzynek', 'Yakun Sophia Shao']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': \"Recent advances in Deep Neural Networks (DNNs) have led to active development of specialized DNN accelerators, many of which feature a large number of processing elements laid out spatially, together with a multi-level memory hierarchy and flexible interconnect. While DNN accelerators can take advantage of data reuse and achieve high peak throughput, they also expose a large number of runtime parameters to the programmers who need to explicitly manage how computation is scheduled both spatially and temporally. In fact, different scheduling choices can lead to wide variations in performance and efficiency, motivating the need for a fast and efficient search strategy to navigate the vast scheduling space. To address this challenge, we present CoSA, a constrained-optimization-based approach for scheduling DNN accelerators. As opposed to existing approaches that either rely on designers' heuristics or iterative methods to navigate the search space, CoSA expresses scheduling decisions as a constrained-optimization problem that can be deterministically solved using mathematical optimization techniques. Specifically, CoSA leverages the regularities in DNN operators and hardware to formulate the DNN scheduling space into a mixed-integer programming (MIP) problem with algorithmic and architectural constraints, which can be solved to automatically generate a highly efficient schedule in one shot. We demonstrate that CoSA-generated schedules significantly outperform state-of-the-art approaches by a geometric mean of up to 2.5X across a wide range of DNN networks while improving the time-to-solution by 90X.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00050',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Xar-trek: run-time execution migration among FPGAs and heterogeneous-ISA CPUs',\n",
       "  'authors': \"['Edson Horta', 'Ho-Ren Chuang', 'Naarayanan Rao VSathish', 'Cesar Philippidis', 'Antonio Barbalace', 'Pierre Olivier', 'Binoy Ravindran']\",\n",
       "  'date': 'December 2021',\n",
       "  'source': \"Middleware '21: Proceedings of the 22nd International Middleware Conference\",\n",
       "  'abstract': \"Datacenter servers are increasingly heterogeneous: from x86 host CPUs, to ARM or RISC-V CPUs in NICs/SSDs, to FPGAs. Previous works have demonstrated that migrating application execution at run-time across heterogeneous-ISA CPUs can yield significant performance and energy gains, with relatively little programmer effort. However, FPGAs have often been overlooked in that context: hardware acceleration using FPGAs involves statically implementing select application functions, which prohibits dynamic and transparent migration. We present Xar-Trek, a new compiler and run-time software framework that overcomes this limitation. Xar-Trek compiles an application for several CPU ISAs and select application functions for acceleration on an FPGA, allowing execution migration between heterogeneous-ISA CPUs and FPGAs at run-time. Xar-Trek's run-time monitors server workloads and migrates application functions to an FPGA or to heterogeneous-ISA CPUs based on a scheduling policy. We develop a heuristic policy that uses application workload profiles to make scheduling decisions. Our evaluations conducted on a system with x86-64 server CPUs, ARM64 server CPUs, and an Alveo accelerator card reveal 88%-l% performance gains over no-migration baselines.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3464298.3493388',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'CryoGuard: a near refresh-free robust DRAM design for cryogenic computing',\n",
       "  'authors': \"['Gyu-Hyeon Lee', 'Seongmin Na', 'Ilkwon Byun', 'Dongmoon Min', 'Jangwoo Kim']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': 'Cryogenic computing, which runs a computer device at an extremely low temperature, is highly promising thanks to the significant reduction of the wire latency and leakage current. A recently proposed cryogenic DRAM design achieved the promising performance improvement, but it also reveals that it must reduce the DRAM\\'s dynamic power to overcome the huge cooling cost at 77 K. Therefore, researchers now target to reduce the cryogenic DRAM\\'s refresh power by utilizing its significantly increased retention time driven by the reduced leakage current. To achieve the goal, however, architects should first answer many fundamental questions regarding the reliability and then design a refresh-free, but still robust cryogenic DRAM by utilizing the analysis result. In this work, we propose a near refresh-free, but robust cryogenic DRAM (NRFC-DRAM), which can almost eliminate its refresh overhead while ensuring reliable operations at 77 K. For the purpose, we first evaluate various DRAM samples of multiple vendors by conducting a thorough analysis to accurately estimate the cryogenic DRAM\\'s retention time and reliability. Our analysis identifies a new critical challenge such that reducing DRAM\\'s refresh rate can make the memory highly unreliable because normal memory operations can now appear as row-hammer attacks at 77 K. Therefore, NRFC-DRAM requires a cost-effective, cryogenic-friendly protection mechanism against the new row-hammer-like \"faults\" at 77 K. To resolve the challenge, we present CryoGuard, our cryogenic-friendly row-hammer protection method to ensure the NRFC-DRAM\\'s reliable operations at 77 K. With CryoGuard applied, NRFC-DRAM reduces the overall power consumption by 25.9% even with its cooling cost included, whereas the existing cryogenic DRAM fails to reduce the power consumption.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00056',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'SnapperGPS: Algorithms for Energy-Efficient Low-Cost Location Estimation Using GNSS Signal Snapshots',\n",
       "  'authors': \"['Jonas Beuchert', 'Alex Rogers']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"SenSys '21: Proceedings of the 19th ACM Conference on Embedded Networked Sensor Systems\",\n",
       "  'abstract': 'Snapshot GNSS is a more energy-efficient approach to location estimation than traditional GNSS positioning methods. This is beneficial for applications with long deployments on battery such as wildlife tracking. However, only a few snapshot GNSS implementations have been presented so far and all have disadvantages. Most significantly, they typically require the GNSS signals to be captured with a certain minimum resolution, which demands complex receiver hardware capable of capturing multi-bit data at sampling rates of 16 MHz and more. By contrast, we develop fast algorithms that reliably estimate locations from twelve-millisecond signals that are sampled at just 4 MHz and quantised with only a single bit per sample. This allows us to build a snapshot receiver at an unmatched low cost of $14, which can acquire one position per hour for a year. On a challenging public dataset with thousands of snapshots from real-world scenarios, our system achieves 97% reliability and 11 m median accuracy, comparable to existing solutions with more complex and expensive hardware and higher energy consumption. We provide an open implementation of the algorithms as well as a public web service for cloud-based location estimation from low-quality GNSS signal snapshots.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3485730.3485931',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'PacketMill: toward per-Core 100-Gbps networking',\n",
       "  'authors': \"['Alireza Farshin', 'Tom Barbette', 'Amir Roozbeh', 'Gerald Q. Maguire Jr.', 'Dejan Kostić']\",\n",
       "  'date': 'April 2021',\n",
       "  'source': \"ASPLOS '21: Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'We present PacketMill, a system for optimizing software packet processing, which (i) introduces a new model to efficiently manage packet metadata and (ii) employs code-optimization techniques to better utilize commodity hardware. PacketMill grinds the whole packet processing stack, from the high-level network function configuration file to the low-level userspace network (specifically DPDK) drivers, to mitigate inefficiencies and produce a customized binary for a given network function. Our evaluation results show that PacketMill increases throughput (up to 36.4 Gbps -- 70%) & reduces latency (up to 101 us -- 28%) and enables nontrivial packet processing (e.g., router) at ~100 Gbps, when new packets arrive >10× faster than main memory access times, while using only one processing core.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3445814.3446724',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'CODIC: a low-cost substrate for enabling custom in-DRAM functionalities and optimizations',\n",
       "  'authors': \"['Lois Orosa', 'Yaohua Wang', 'Mohammad Sadrosadati', 'Jeremie S. Kim', 'Minesh Patel', 'Ivan Puddu', 'Haocong Luo', 'Kaveh Razavi', 'Juan Gómez-Luna', 'Hasan Hassan', 'Nika Mansouri-Ghiasi', 'Saugata Ghose', 'Onur Mutlu']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': 'DRAM is the dominant main memory technology used in modern computing systems. Computing systems implement a memory controller that interfaces with DRAM via DRAM commands. DRAM executes the given commands using internal components (e.g., access transistors, sense amplifiers) that are orchestrated by DRAM internal timings, which are fixed for each DRAM command. Unfortunately, the use of fixed internal timings limits the types of operations that DRAM can perform and hinders the implementation of new functionalities and custom mechanisms that improve DRAM reliability, performance and energy. To overcome these limitations, we propose enabling programmable DRAM internal timings for controlling in-DRAM components. To this end, we design CODIC, a new low-cost DRAM substrate that enables fine-grained control over four previously fixed internal DRAM timings that are key to many DRAM operations. We implement CODIC with only minimal changes to the DRAM chip and the DDRx interface. To demonstrate the potential of CODIC, we propose two new CODIC-based security mechanisms that outperform state-of-the-art mechanisms in several ways: (1) a new DRAM Physical Unclonable Function (PUF) that is more robust and has significantly higher throughput than state-of-the-art DRAM PUFs, and (2) the first cold boot attack prevention mechanism that does not introduce any performance or energy overheads at runtime.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00045',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Building an Internet Router with P4Pi',\n",
       "  'authors': \"['Radostin Stoyanov', 'Adam Wolnikowski', 'Robert Soulé', 'Sándor Laki', 'Noa Zilberman']\",\n",
       "  'date': 'December 2021',\n",
       "  'source': \"ANCS '21: Proceedings of the Symposium on Architectures for Networking and Communications Systems\",\n",
       "  'abstract': 'Building an Internet Router is a popular, hands-on project used to teach computer networks. However, there is currently no hardware target that allows students to develop the project in P4 without incurring significant cost or encountering FPGA knowledge barriers. This paper presents P4Pi as a target for the Building an Internet Router project. P4Pi is a platform for developing, testing, and evaluating P4 programs on a Raspberry Pi device. We describe the architecture of the router project on P4Pi, and discuss the practical aspects of running it as a class project. The P4Pi-based router project is low-cost and easy to adopt, enabling students to focus on their P4 programming skills and to evaluate their designs on a physical target through interoperability tests with their colleagues.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3493425.3502762',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'NASPipe: high performance and reproducible pipeline parallel supernet training via causal synchronous parallelism',\n",
       "  'authors': \"['Shixiong Zhao', 'Fanxin Li', 'Xusheng Chen', 'Tianxiang Shen', 'Li Chen', 'Sen Wang', 'Nicholas Zhang', 'Cheng Li', 'Heming Cui']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'Supernet training, a prevalent and important paradigm in Neural Architecture Search, embeds the whole DNN architecture search space into one monolithic supernet, iteratively activates a subset of the supernet (i.e., a subnet) for fitting each batch of data, and searches a high-quality subnet which meets specific requirements. Although training subnets in parallel on multiple GPUs is desirable for acceleration, there inherently exists a race hazard that concurrent subnets may access the same DNN layers. Existing systems support neither efficiently parallelizing subnets’ training executions, nor resolving the race hazard deterministically, leading to unreproducible training procedures and potentiallly non-trivial accuracy loss.   We present NASPipe, the first high-performance and reproducible distributed supernet training system via causal synchronous parallel (CSP) pipeline scheduling abstraction: NASPipe partitions a supernet across GPUs and concurrently executes multiple generated sub-tasks (subnets) in a pipelined manner; meanwhile, it oversees the correlations between the subnets and deterministically resolves any causal dependency caused by subnets’ layer sharing. To obtain high performance, NASPipe’s CSP scheduler exploits the fact that the larger a supernet spans, the fewer dependencies manifest between chronologically close subnets; therefore, it aggressively schedules the subnets with larger chronological orders into execution, only if they are not causally dependent on unfinished precedent subnets. Moreover, to relieve the excessive GPU memory burden for holding the whole supernet’s parameters, NASPipe uses a context switch technique that stashes the whole supernet in CPU memory, precisely predicts the subnets’ schedule, and pre-fetches/evicts a subnet before/after its execution. The evaluation shows that NASPipe is the only system that retains supernet training reproducibility, while achieving a comparable and even higher performance (up to 7.8X) compared to three recent pipeline training systems (e.g., GPipe).',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503222.3507735',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Understanding and improving model-driven IoT systems through accompanying digital twins',\n",
       "  'authors': \"['Jörg Christian Kirchhof', 'Lukas Malcher', 'Bernhard Rumpe']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': 'GPCE 2021: Proceedings of the 20th ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences',\n",
       "  'abstract': 'Developers questioning why their system behaves differently than expected often have to rely on time-consuming and error-prone manual analysis of log files. Understanding the behavior of Internet of Things (IoT) applications is a challenging task because they are not only inherently hard-to-trace distributed systems, but their integration with the environment via sensors adds another layer of complexity. Related work proposes to record data during the execution of the system, which can later be replayed to analyze the system. We apply the model-driven development approach to this idea and leverage digital twins to collect the required data. We enable developers to replay and analyze the system’s executions by applying model-to-model transformations. These transformations instrument component and connector (C&C) architecture models with components that reproduce the system’s environment based on the data recorded by the system’s digital twin. We validate and evaluate the feasibility of our approach using a heating, ventilation, and air conditioning (HVAC) case study. By facilitating the reproduction of the system’s behavior, our method lowers the barrier to understanding the behavior of model-driven IoT systems.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3486609.3487210',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'NASPipe: high performance and reproducible pipeline parallel supernet training via causal synchronous parallelism',\n",
       "  'authors': \"['Shixiong Zhao', 'Fanxin Li', 'Xusheng Chen', 'Tianxiang Shen', 'Li Chen', 'Sen Wang', 'Nicholas Zhang', 'Cheng Li', 'Heming Cui']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'Supernet training, a prevalent and important paradigm in Neural Architecture Search, embeds the whole DNN architecture search space into one monolithic supernet, iteratively activates a subset of the supernet (i.e., a subnet) for fitting each batch of data, and searches a high-quality subnet which meets specific requirements. Although training subnets in parallel on multiple GPUs is desirable for acceleration, there inherently exists a race hazard that concurrent subnets may access the same DNN layers. Existing systems support neither efficiently parallelizing subnets’ training executions, nor resolving the race hazard deterministically, leading to unreproducible training procedures and potentiallly non-trivial accuracy loss.   We present NASPipe, the first high-performance and reproducible distributed supernet training system via causal synchronous parallel (CSP) pipeline scheduling abstraction: NASPipe partitions a supernet across GPUs and concurrently executes multiple generated sub-tasks (subnets) in a pipelined manner; meanwhile, it oversees the correlations between the subnets and deterministically resolves any causal dependency caused by subnets’ layer sharing. To obtain high performance, NASPipe’s CSP scheduler exploits the fact that the larger a supernet spans, the fewer dependencies manifest between chronologically close subnets; therefore, it aggressively schedules the subnets with larger chronological orders into execution, only if they are not causally dependent on unfinished precedent subnets. Moreover, to relieve the excessive GPU memory burden for holding the whole supernet’s parameters, NASPipe uses a context switch technique that stashes the whole supernet in CPU memory, precisely predicts the subnets’ schedule, and pre-fetches/evicts a subnet before/after its execution. The evaluation shows that NASPipe is the only system that retains supernet training reproducibility, while achieving a comparable and even higher performance (up to 7.8X) compared to three recent pipeline training systems (e.g., GPipe).',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503222.3507735',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'MemOReL: A <u>Mem</u>ory-oriented <u>O</u>ptimization Approach to <u>Re</u>inforcement <u>L</u>earning on FPGA-based Embedded Systems',\n",
       "  'authors': \"['Siva Satyendra Sahoo', 'Akhil Raj Baranwal', 'Salim Ullah', 'Akash Kumar']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"GLSVLSI '21: Proceedings of the 2021 on Great Lakes Symposium on VLSI\",\n",
       "  'abstract': 'Reinforcement Learning (RL) represents the machine learning method that has come closest to showing human-like learning. While Deep RL is becoming increasingly popular for complex applications such as AI-based gaming, it has a high implementation cost in terms of both power and latency. Q-Learning, on the other hand, is a much simpler method that makes it more feasible for implementation on resource-constrained embedded systems for control and navigation. However, the optimal policy search in Q-Learning is a compute-intensive and inherently sequential process and a software-only implementation may not be able to satisfy the latency and throughput constraints of such applications. To this end, we propose a novel accelerator design with multiple design trade-offs for implementing Q-Learning on FPGA-based SoCs. Specifically, we analyze the various stages of the Epsilon-Greedy algorithm for RL and propose a novel microarchitecture that reduces the latency by optimizing the memory access during each iteration. Consequently, we present multiple designs that provide varying trade-offs between performance, power dissipation, and resource utilization of the accelerator. With the proposed approach, we report considerable improvement in throughput with lower resource utilization over state-of-the-art design implementations.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3453688.3461533',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Coalescent computing',\n",
       "  'authors': \"['Kyle Hale']\",\n",
       "  'date': 'August 2021',\n",
       "  'source': \"APSys '21: Proceedings of the 12th ACM SIGOPS Asia-Pacific Workshop on Systems\",\n",
       "  'abstract': \"As computational infrastructure extends to the edge, it will increasingly offer the same fine-grained resource provisioning mechanisms used in large-scale cloud datacenters, and advances in low-latency, wireless networking technology will allow service providers to blur the distinction between local and remote resources for commodity computing. From the users' perspectives, their devices will no longer have fixed computational power, but rather will appear to have flexible computational capabilities that vary subject to the shared, disaggregated edge resources available in their physical proximity. System software will transparently leverage these ephemeral resources to provide a better end-user experience. We discuss key systems challenges to enabling such tightly-coupled, disaggregated, and ephemeral infrastructure provisioning, advocate for more research in the area, and outline possible paths forward.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3476886.3477503',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Accuracy and Resiliency of Analog Compute-in-Memory Inference Engines',\n",
       "  'authors': \"['Zhe Wan', 'Tianyi Wang', 'Yiming Zhou', 'Subramanian S. Iyer', 'Vwani P. Roychowdhury']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Journal on Emerging Technologies in Computing Systems',\n",
       "  'abstract': 'Recently, analog compute-in-memory (CIM) architectures based on emerging analog non-volatile memory (NVM) technologies have been explored for deep neural networks (DNNs) to improve scalability, speed, and energy efficiency. Such architectures, however, leverage charge conservation, an operation with infinite resolution, and thus are susceptible to errors. Thus, the inherent stochasticity in any analog NVM used to execute DNNs, will compromise performance. Several reports have demonstrated the use of analog NVM for CIM in a limited scale. It is unclear whether the uncertainties in computations will prohibit large-scale DNNs. To explore this critical issue of scalability, this article first presents a simulation framework to evaluate the feasibility of large-scale DNNs based on CIM architecture and analog NVM. Simulation results show that DNNs trained for high-precision digital computing engines are not resilient against the uncertainty of the analog NVM devices. To avoid such catastrophic failures, this article introduces the analog bi-scale representation for the DNN, and the Hessian-aware Stochastic Gradient Descent training algorithm to enhance the inference accuracy of trained DNNs. As a result of such enhancements, DNNs such as Wide ResNets for CIFAR-100 image recognition problem are demonstrated to have significant performance improvements in accuracy without adding cost to the inference hardware.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3502721',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Efficient LLVM-based dynamic binary translation',\n",
       "  'authors': \"['Alexis Engelke', 'Dominik Okwieka', 'Martin Schulz']\",\n",
       "  'date': 'April 2021',\n",
       "  'source': 'VEE 2021: Proceedings of the 17th ACM SIGPLAN/SIGOPS International Conference on Virtual Execution Environments',\n",
       "  'abstract': 'Emulation of other or newer processor architectures is necessary for a wide variety of use cases, from ensuring compatibility to offering a vehicle for computer architecture research. This problem is usually approached using dynamic binary translation, where machine code is translated, on the fly, to the host architecture during program execution. Existing systems, like QEMU, usually focus on translation performance rather than the overall program execution, and extensions, like HQEMU, are limited by their underlying implementation. Conversely, performance-focused systems are typically used for binary instrumentation. E.g., DynamoRIO reuses original instructions where possible, while Instrew utilizes the LLVM compiler infrastructure, but only supports same-architecture code generation.   In this short paper, we generalize Instrew to support different guest and host architectures by refactoring the lifter and by implementing target-independent optimizations to re-use host hardware features for emulated code. We demonstrate this flexibility by adding support for RISC-V as guest architecture and AArch64 as host architecture. Our performance results on SPEC CPU2017 show significant improvements compared to QEMU, HQEMU as well as the original Instrew.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3453933.3454022',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'PERI: A Configurable Posit Enabled RISC-V Core',\n",
       "  'authors': \"['Sugandha Tiwari', 'Neel Gala', 'Chester Rebeiro', 'V. Kamakoti']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Architecture and Code Optimization',\n",
       "  'abstract': 'Owing to the failure of Dennard’s scaling, the past decade has seen a steep growth of prominent new paradigms leveraging opportunities in computer architecture. Two technologies of interest are Posit and RISC-V. Posit was introduced in mid-2017 as a viable alternative to IEEE-754, and RISC-V provides a commercial-grade open source Instruction Set Architecture (ISA). In this article, we bring these two technologies together and propose a Configurable Posit Enabled RISC-V Core called PERI.The article provides insights on how the Single-Precision Floating Point (“F”) extension of RISC-V can be leveraged to support posit arithmetic. We also present the implementation details of a parameterized and feature-complete posit Floating Point Unit (FPU). The configurability and the parameterization features of this unit generate optimal hardware, which caters to the accuracy and energy/area tradeoffs imposed by the applications, a feature not possible with IEEE-754 implementation. The posit FPU has been integrated with the RISC-V compliant SHAKTI C-class core as an execution unit. To further leverage the potential of posit, we enhance our posit FPU to support two different exponent sizes (with posit-size being 32-bits), thereby enabling multiple-precision at runtime. To enable the compilation and execution of C programs on PERI, we have made minimal modifications to the GNU C Compiler (GCC), targeting the “F” extension of the RISC-V. We compare posit with IEEE-754 in terms of hardware area, application accuracy, and runtime. We also present an alternate methodology of integrating the posit FPU with the RISC-V core as an accelerator using the custom opcode space of RISC-V.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3446210',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Client-optimized algorithms and acceleration for encrypted compute offloading',\n",
       "  'authors': \"['McKenzie van der Hagen', 'Brandon Lucia']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'Homomorphic Encryption (HE) enables secure cloud offload processing on encrypted data. HE schemes are limited in the complexity and type of operations they can perform, motivating client-aided implementations that distribute computation between client (unencrypted) and server (encrypted). Prior client-aided systems optimize server performance, ignoring client costs: client-aided models put encryption and decryption on the critical path and require communicating large ciphertexts. We introduce Client-aided HE for Opaque Compute Offloading (CHOCO), a client-optimized system for encrypted offload processing. CHOCO reduces ciphertext size, reducing communication and computing costs through HE parameter minimization and through “rotational redundancy”, a new HE algorithm optimization. We present Client-aided HE for Opaque Compute Offloading Through Accelerated Cryptographic Operations (CHOCO-TACO), an accelerator for HE encryption and decryption, making client-aided HE feasible for even resource-constrained clients. CHOCO supports two popular HE schemes (BFV and CKKS) and several applications, including DNNs, PageRank, KNN, and K-Means. CHOCO reduces communication by up to 2948× over prior work. With CHOCO-TACO client enc-/decryption is up to 1094× faster and uses up to 648× less energy.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503222.3507737',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Efficient multi-GPU shared memory via automatic optimization of fine-grained transfers',\n",
       "  'authors': \"['Harini Muthukrishnan', 'David Nellans', 'Daniel Lustig', 'Jeffrey A. Fessler', 'Thomas F. Wenisch']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': \"Despite continuing research into inter-GPU communication mechanisms, extracting performance from multi-GPU systems remains a significant challenge. Inter-GPU communication via bulk DMA-based transfers exposes data transfer latency on the GPU's critical execution path because these large transfers are logically interleaved between compute kernels. Conversely, fine-grained peer-to-peer memory accesses during kernel execution lead to memory stalls that can exceed the GPUs' ability to cover these operations via multi-threading. Worse yet, these sub-cacheline transfers are highly inefficient on current inter-GPU interconnects. To remedy these issues, we propose PROACT, a system enabling remote memory transfers with the programmability and pipeline advantages of peer-to-peer stores, while achieving interconnect efficiency that rivals bulk DMA transfers. Combining compile-time instrumentation with fine-grain tracking of data block readiness within each GPU, PROACT enables interconnect-friendly data transfers while hiding the transfer latency via pipelining during kernel execution. This work describes both hardware and software implementations of PROACT and demonstrates the effectiveness of a PROACT software prototype on three generations of GPU hardware and interconnects. Achieving near-ideal interconnect efficiency, PROACT realizes a mean speedup of 3.0X over single-GPU performance for 4-GPU systems, capturing 83% of available performance opportunity. On a 16-GPU NVIDIA DGX-2 system, we demonstrate an 11.0X average strong-scaling speedup over single-GPU performance, 5.3X better than a bulk DMA-based approach.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00020',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Reproducing Spectre Attack with gem5: How To Do It Right?',\n",
       "  'authors': \"['Pierre Ayoub', 'Clémentine Maurice']\",\n",
       "  'date': 'April 2021',\n",
       "  'source': \"EuroSec '21: Proceedings of the 14th European Workshop on Systems Security\",\n",
       "  'abstract': 'As processors become more and more complex due to performance optimizations and energy savings, new attack surfaces raise. We know that the micro-architecture of a processor leaks some information into the architectural domain. Moreover, some mechanisms like speculative execution can be exploited to execute malicious instructions. As a consequence, it allows a process to spy another process or to steal data. These attacks are consequences of fundamental design issues, thus they are complicated to fix and reproduce. Simulation would be of a great help for scientific research for microarchitectural security, but it also leads to new challenges. We try to address the first challenges to demonstrate that simulation could be useful in research and an interesting technique to develop in the future.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3447852.3458715',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'MC-DeF: Creating Customized CGRAs for Dataflow Applications',\n",
       "  'authors': \"['George Charitopoulos', 'Dionisios N. Pnevmatikatos', 'Georgi Gaydadjiev']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Architecture and Code Optimization',\n",
       "  'abstract': 'Executing complex scientific applications on Coarse-Grain Reconfigurable Arrays (CGRAs) promises improvements in execution time and/or energy consumption compared to optimized software implementations or even fully customized hardware solutions. Typical CGRA architectures contain of multiple instances of the same compute module that consist of simple and general hardware units such as ALUs, simple processors. However, generality in the cell contents, while convenient for serving a wide variety of applications, penalizes performance and energy efficiency. To that end, a few proposed CGRAs use custom logic tailored to a particular application’s specific characteristics in the compute module. This approach, while much more efficient, restricts the versatility of the array. To date, versatility at hardware speeds is only supported with Field programmable gate arrays (FPGAs), that are reconfigurable at a very fine grain.This work proposes MC-DeF, a novel Mixed-CGRA Definition Framework targeting a Mixed-CGRA architecture that leverages the advantages of CGRAs by utilizing a customized cell array, and those of FPGAs by incorporating a separate LUT array used for adaptability. The framework presented aims to develop a complete CGRA architecture. First, a cell structure and functionality definition phase creates highly customized application/domain specific CGRA cells. Then, mapping and routing phases define the CGRA connectivity and cell-LUT array transactions. Finally, an energy and area estimation phase presents the user with area occupancy and energy consumption estimations of the final design. MC-DeF uses novel algorithms and cost functions driven by user defined metrics, threshold values, and area/energy restrictions. The benefits of our framework, besides creating fast and efficient CGRA designs, include design space exploration capabilities offered to the user.The validity of the presented framework is demonstrated by evaluating and creating CGRA designs of nine applications. Additionally, we provide comparisons of MC-DeF with state-of-the-art related works, and show that MC-DeF offers competitive performance (in terms of internal bandwidth and processing throughput) even compared against much larger designs, and requires fewer physical resources to achieve this level of performance. Finally, MC-DeF is able to better utilize the underlying FPGA fabric and achieves the best efficiency (measured in LUT/GOPs).',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3447970',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Improving Performance-Power-Programmability in Space Avionics with Edge Devices: VBN on Myriad2 SoC',\n",
       "  'authors': \"['Vasileios Leon', 'George Lentaris', 'Evangelos Petrongonas', 'Dimitrios Soudris', 'Gianluca Furano', 'Antonis Tavoularis', 'David Moloney']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Embedded Computing Systems',\n",
       "  'abstract': 'The advent of powerful edge devices and AI algorithms has already revolutionized many terrestrial applications; however, for both technical and historical reasons, the space industry is still striving to adopt these key enabling technologies in new mission concepts. In this context, the current work evaluates an heterogeneous multi-core system-on-chip processor for use on-board future spacecraft to support novel, computationally demanding digital signal processors and AI functionalities. Given the importance of low power consumption in satellites, we consider the Intel Movidius Myriad2 system-on-chip and focus on SW development and performance aspects. We design a methodology and framework to accommodate efficient partitioning, mapping, parallelization, code optimization, and tuning of complex algorithms. Furthermore, we propose an avionics architecture combining this commercial off-the-shelf chip with a field programmable gate array device to facilitate, among others, interfacing with traditional space instruments via SpaceWire transcoding. We prototype our architecture in the lab targeting vision-based navigation tasks. We implement a representative computer vision pipeline to track the 6D pose of ENVISAT using megapixel images during hypothetical spacecraft proximity operations. Overall, we achieve 2.6 to 4.9 FPS with only 0.8 to 1.1 W on Myriad2, i.e., 10-fold acceleration versus modern rad-hard processors. Based on the results, we assess various benefits of utilizing Myriad2 instead of conventional field programmable gate arrays and CPUs.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3440885',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'NxTF: An API and Compiler for Deep Spiking Neural Networks on Intel Loihi',\n",
       "  'authors': \"['Bodo Rueckauer', 'Connor Bybee', 'Ralf Goettsche', 'Yashwardhan Singh', 'Joyesh Mishra', 'Andreas Wild']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Journal on Emerging Technologies in Computing Systems',\n",
       "  'abstract': 'Spiking Neural Networks (SNNs) is a promising paradigm for efficient event-driven processing of spatio-temporally sparse data streams. Spiking Neural Networks (SNNs) have inspired the design of and can take advantage of the emerging class of neuromorphic processors like Intel Loihi. These novel hardware architectures expose a variety of constraints that affect firmware, compiler, and algorithm development alike. To enable rapid and flexible development of SNN algorithms on Loihi, we developed NxTF: a programming interface derived from Keras and compiler optimized for mapping deep convolutional SNNs to the multi-core Intel Loihi architecture. We evaluate NxTF on Deep Neural Networks (DNNs) trained directly on spikes as well as models converted from traditional DNNs, processing both sparse event-based and dense frame-based datasets. Further, we assess the effectiveness of the compiler to distribute models across a large number of cores and to compress models by exploiting Loihi’s weight-sharing features. Finally, we evaluate model accuracy, energy, and time-to-solution compared to other architectures. The compiler achieves near-optimal resource utilization of 80% across 16 Loihi chips for a 28-layer, 4M parameter MobileNet model with input size 128×128. In addition, we report the lowest error rate of 8.52% for the CIFAR-10 dataset on neuromorphic hardware, using an off-the-shelf MobileNet.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3501770',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Experimental Analysis and Design Guidelines for Microphone Virtualization in Automotive Scenarios',\n",
       "  'authors': \"['Alessandro Opinto', 'Marco Martalò', 'Alessandro Costalunga', 'Nicolò Strozzi', 'Carlo Tripodi', 'Riccardo Raheli']\",\n",
       "  'date': 'None',\n",
       "  'source': 'IEEE/ACM Transactions on Audio, Speech and Language Processing',\n",
       "  'abstract': 'In this article, a performance analysis on the estimation of the so-called observation filter for the Virtual Microphone Technique (VMT) in a realistic automotive environment is presented. A performance comparison between adaptive and fixed observation filter estimation methods, namely Least Mean Square (LMS) and Minimum Mean Square Error (MMSE), respectively, was carried on. Two different experimental setups were implemented on a popular B-segment car. Eight microphones were placed at the monitoring and virtual positions in order to sense environmental acoustic noise propagating within the cabin of the car running at variable speed on a smooth asphalt. Our experimental results show that a large spectral coherence between monitoring and virtual microphone signals indicates a potentially effective and relatively wide-band virtual microphone signal reconstruction. The fixed observation filter estimation method achieves better performance than the adaptive one, guaranteeing remarkable broadband estimation accuracy. Moreover, for each considered setup, design guidelines are proposed to obtain a good trade-off between estimation accuracy and material costs.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/TASLP.2022.3190727',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Research on Ultrasonic High-resolution Imaging Technology for the Reinforced Concrete',\n",
       "  'authors': \"['Lulu Ge', 'Hua Huang', 'Zhigang Wang', 'Dexiu Dong', 'Haitao Wang', 'Qiufeng Li']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': 'AIAM2021: 2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture',\n",
       "  'abstract': 'None',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3495018.3495055',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Armed Cats: Formal Concurrency Modelling at Arm',\n",
       "  'authors': \"['Jade Alglave', 'Will Deacon', 'Richard Grisenthwaite', 'Antoine Hacquard', 'Luc Maranget']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Programming Languages and Systems',\n",
       "  'abstract': 'We report on the process for formal concurrency modelling at Arm. An initial formal consistency model of the Arm achitecture, written in the cat language, was published and upstreamed to the herd+diy tool suite in 2017. Since then, we have extended the original model with extra features, for example, mixed-size accesses, and produced two provably equivalent alternative formulations.In this article, we present a comprehensive review of work done at Arm on the consistency model. Along the way, we also show that our principle for handling mixed-size accesses applies to x86: We confirm this via vast experimental campaigns. We also show that our alternative formulations are applicable to any model phrased in a style similar to the one chosen by Arm.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3458926',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Pinned loads: taming speculative loads in secure processors',\n",
       "  'authors': \"['Zirui Neil Zhao', 'Houxiang Ji', 'Adam Morrison', 'Darko Marinov', 'Josep Torrellas']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'In security frameworks for speculative execution, an instruction is said to reach its Visibility Point (VP) when it is no longer vulnerable to pipeline squashes. Before a potentially leaky instruction reaches its VP, it has to stall—unless a defense scheme such as invisible speculation provides protection. Unfortunately, either stalling or protecting the execution of pre-VP instructions typically has a performance cost.  One way to attain low-overhead safe execution is to develop techniques that speed-up the advance of the VP from older to younger instructions. In this paper, we propose one such technique. We find that the progress of the VP for loads is mostly impeded by waiting until no memory consistency violations (MCVs) are possible. Hence, our technique, called , tries to make loads invulnerable to MCVs as early as possible—a process we call pinning the loads in the pipeline. The result is faster VP progress and a reduction in the execution overhead of defense schemes. In this paper, we describe the hardware needed by , and two possible designs with different tradeoffs between hardware requirements and performance. Our evaluation shows that is very effective: extending three popular defense schemes against speculative execution attacks with reduces their average execution overhead on SPEC17 and on SPLASH2/PARSEC applications by about 50%. For example, on SPEC17, the execution overhead of the three defense schemes decreases from to , from to , and from to .',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503222.3507724',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'PMNet: in-network data persistence',\n",
       "  'authors': \"['Korakit Seemakhupt', 'Sihang Liu', 'Yasas Senevirathne', 'Muhammad Shahbaz', 'Samira Khan']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': \"To guarantee data persistence, storage workloads (such as key-value stores and databases) typically use a synchronous protocol that places the network and server stack latency on the critical path of request processing. The use of the fast and byte-addressable persistent memory (PM) has helped mitigate the storage overhead of the server stack; yet, networking is still a dominant factor in the end-to-end latency of request processing. Emerging programmable network devices can reduce network latency by moving parts of the applications' compute into the network (e.g., caching results for read requests); however, for update requests, the client still has to stall on the server to commit the updates, persistently. In this work, we introduce in-network data persistence that extends the data-persistence domain from servers to the network, and present PMNet, a programmable data plane (e.g., switch or NIC) with PM for persisting data in the network. PMNet logs incoming update requests and acknowledges clients directly without having them wait on the server to commit the request. In case of a failure, the logged requests act as redo logs for the server to recover. We implement PMNet on an FPGA and evaluate its performance using common PM workloads, including key-value stores and PM-backed applications. Our evaluation shows that PMNet can improve the throughput of update requests by 4.31X on average, and the 99th-percentile tail latency by 3.23X.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00068',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'YODA: A Pedagogical Tool for Teaching Systems Concepts',\n",
       "  'authors': \"['Apan Qasem']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': 'SIGCSE 2022: Proceedings of the 53rd ACM Technical Symposium on Computer Science Education - Volume 1',\n",
       "  'abstract': 'Computer science undergraduates often struggle in hardware-oriented courses like Computer Organization and Computer Architecture. Active learning instruments can improve student performance in these classes. Regrettably, few tools exist today to support the creation of active learning teaching material for such courses. This paper describes YODA, a pedagogical tool for creating active learning content to help teach systems concepts. At the core, YODA is a collection of functional simulators embedded into a custom Jupyter kernel. YODA produces notebooks that allow students to learn about a system through guided interaction and observation. We have been using YODA at our home institution for two years and have seen significant improvement in student learning outcomes.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3478431.3499322',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Practical Model Checking on FPGAs',\n",
       "  'authors': \"['Shenghsun Cho', 'Mrunal Patel', 'Michael Ferdman', 'Peter Milder']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Reconfigurable Technology and Systems',\n",
       "  'abstract': 'Software verification is an important stage of the software development process, particularly for mission-critical systems. As the traditional methodology of using unit tests falls short of verifying complex software, developers are increasingly relying on formal verification methods, such as explicit state model checking, to automatically verify that the software functions properly. However, due to the ever-increasing complexity of software designs, model checking cannot be performed in a reasonable amount of time when running on general-purpose cores, leading to the exploration of hardware-accelerated model checking. FPGAs have been demonstrated to be promising verification accelerators, exhibiting nearly three orders of magnitude speedup over software. Unfortunately, the “FPGA programmability wall,” particularly the long synthesis and place-and-route times, block the general adoption of FPGAs for model checking.To address this problem, we designed a runtime-programmable pipeline specifically for model checkers on FPGAs to minimize the “preparation time” before a model can be checked. Our design of the successor state generator and the state validator modules enables FPGA-acceleration of model checking without incurring the time-consuming FPGA implementation stages, reducing the preparation time before checking a model from hours to less than a minute, while incurring only a 26% execution time overhead compared to model-specific implementations.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3448272',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Branchboozle: a side-channel within a hidden pattern history table of modern branch prediction units',\n",
       "  'authors': \"['Andrés R. Hernández C.', 'Wonjun Lee', 'Wei-Ming Lin']\",\n",
       "  'date': 'March 2021',\n",
       "  'source': \"SAC '21: Proceedings of the 36th Annual ACM Symposium on Applied Computing\",\n",
       "  'abstract': \"We present Branchboozle, a side-channel that can be configured on top of a hidden Pattern History Table (PHT) now found in modern Branch Prediction Units (BPUs). In a similar fashion to known BranchScope attacks, Branchboozle works by closely monitoring the directional predictions issued by the BPU, i.e., whether a branch is predicted Taken or Non-Taken. However, our attack exclusively focuses on analyzing the predictions issued by a secondary, mostly-undocumented 3-bit PHT, whereas BranchScope attacks manipulate the predictions issued by a textbook-like 2-bit PHT. This work describes how Branchboozle can configure an extremely robust covert-channel among independent processes that works even across the physical threads of an execution core with Simultaneous Multi-Threading technology. Additionally, we demonstrate that branches protected by Intel's Software Guard eXtensions are also vulnerable to our attack setting. Finally, we illustrate how Branchboozle can potentiate transient execution attacks dependent on branch direction misprediction, i.e., Spectre Variant 1.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3412841.3442035',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Sieve: scalable in-situ DRAM-based accelerator designs for massively parallel k-mer matching',\n",
       "  'authors': \"['Lingxi Wu', 'Rasool Sharifi', 'Marzieh Lenjani', 'Kevin Skadron', 'Ashish Venkat']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': 'The rapid influx of biosequence data, coupled with the stagnation of the processing power of modern computing systems, highlights the critical need for exploring high-performance accelerators that can meet the ever-increasing throughput demands of modern bioinformatics applications. This work argues that processing in memory (PIM) is an effective solution to enhance the performance of k-mer matching, a critical bottleneck stage in standard bioinformatics pipelines, that is characterized by random access patterns and low computational intensity. This work proposes three DRAM-based in-situ k-mer matching accelerator designs (one optimized for area, one optimized for throughput, and one that strikes a balance between hardware cost and performance), dubbed Sieve, that leverage a novel data mapping scheme to allow for simultaneous comparisons of millions of DNA base pairs, lightweight matching circuitry for fast pattern matching, and an early termination mechanism that prunes unnecessary DRAM row activation to reduce latency and save energy. Evaluation of Sieve using state-of-the-art workloads with real-world datasets shows that the most aggressive design provides an average of 326x/32x speedup and 74X/48x energy savings over multi-core-CPU/GPU baselines for k-mer matching.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00028',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'STEAM for all: New Computational Thinking Curricula in Spanish Formal Secondary Education',\n",
       "  'authors': \"['Rocio Garcia-Robles', 'Santiago Fernández-Cabaleiro']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': 'ARTECH 2021: 10th International Conference on Digital and Interactive Arts',\n",
       "  'abstract': 'In this article, the authors introduce the new educational curricula for Computing and Robotics in Andalusia [1], that will be offered as an optional subject for more than three hundred thousand students during 2021/2022 academic year and so on. This curriculum offers young students, aged 12-15 years old, with the opportunity to gain a better understanding on how our digital world works, as well as, developing a set of skills known as computational thinking. This term refers to a way of understanding and solving problems, in any discipline, with the help of computers.ç',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3483529.3483662',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Mahjong: A Generic Framework for Network Data Plane Verification',\n",
       "  'authors': \"['Yifan Li', 'Chengjun Jia', 'Xiaohe Hu', 'Jun Li']\",\n",
       "  'date': 'December 2021',\n",
       "  'source': \"ANCS '21: Proceedings of the Symposium on Architectures for Networking and Communications Systems\",\n",
       "  'abstract': 'Existing network data plane verification approaches check network correctness with various models and algorithms. With respect to a specific scenario, it is hard to judge which network model provides sufficient functionality and suitable performance, because existing verification approaches are implemented with different languages and evaluated against different datasets on different hardware platforms in their papers. A network operator usually has to try out a number of complex verification approaches to find the best one for her/his network and intents. Mahjong has a modular system architecture, a unified input format, and three classic verification tools built-in. Leveraging its well-defined partition interfaces and straight-forward configuration file, not only existing approaches can be refactored and merged into Mahjong, new approaches can also be introduced and evaluated with ease.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3493425.3502755',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'NNStreamer: efficient and Agile† development of on-device AI systems',\n",
       "  'authors': \"['MyungJoo Ham', 'Jijoong Moon', 'Geunsik Lim', 'Jaeyun Jung', 'Hyoungjoo Ahn', 'Wook Song', 'Sangjung Woo', 'Parichay Kapoor', 'Dongju Chae', 'Gichan Jang', 'Yongjoo Ahn', 'Jihoon Lee']\",\n",
       "  'date': 'May 2021',\n",
       "  'source': \"ICSE-SEIP '21: Proceedings of the 43rd International Conference on Software Engineering: Software Engineering in Practice\",\n",
       "  'abstract': 'We propose NNStreamer, a software system that handles neural networks as filters of stream pipelines, applying the stream processing paradigm to deep neural network applications. A new trend with the wide-spread of deep neural network applications is on-device AI. It is to process neural networks on mobile devices or edge/IoT devices instead of cloud servers. Emerging privacy issues, data transmission costs, and operational costs signify the need for on-device AI, especially if we deploy a massive number of devices. NNStreamer efficiently handles neural networks with complex data stream pipelines on devices, significantly improving the overall performance with minimal effort. Besides, NNStreamer simplifies implementations and allows reusing off-the-shelf media filters directly, which reduces developmental costs significantly. We are already deploying NNStreamer for a wide range of products and platforms, including the Galaxy series and various consumer electronic devices. The experimental results suggest a reduction in developmental costs and enhanced performance of pipeline architectures and NNStreamer. It is an open-source project incubated by Linux Foundation AI & Data, available to the public and applicable to various hardware and software platforms.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ICSE-SEIP52600.2021.00029',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'The Forward Slice Core: A High-Performance, Yet Low-Complexity Microarchitecture',\n",
       "  'authors': \"['Kartik Lakshminarasimhan', 'Ajeya Naithani', 'Josué Feliu', 'Lieven Eeckhout']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Architecture and Code Optimization',\n",
       "  'abstract': 'Superscalar out-of-order cores deliver high performance at the cost of increased complexity and power budget. In-order cores, in contrast, are less complex and have a smaller power budget, but offer low performance. A processor architecture should ideally provide high performance in a power- and cost-efficient manner. Recently proposed slice-out-of-order (sOoO) cores identify backward slices of memory operations which they execute out-of-order with respect to the rest of the dynamic instruction stream for increased instruction-level and memory-hierarchy parallelism. Unfortunately, constructing backward slices is imprecise and hardware-inefficient, leaving performance on the table.In this article, we propose Forward Slice Core (FSC), a novel core microarchitecture that builds on a stall-on-use in-order core and extracts more instruction-level and memory-hierarchy parallelism than slice-out-of-order cores. FSC does so by identifying and steering forward slices (rather than backward slices) to dedicated in-order FIFO queues. Moreover, FSC puts load-consumers that depend on L1 D-cache misses on the side to enable younger independent load-consumers to execute faster. Finally, FSC eliminates the need for dynamic memory disambiguation by replicating store-address instructions across queues. Considering 3-wide pipeline configurations, we find that FSC improves performance by 27.1%, 21.1%, and 14.6% on average compared to Freeway, the state-of-the-art sOoO core, across SPEC CPU2017, GAP, and DaCapo, respectively, while at the same time incurring reduced hardware complexity. Compared to an OoO core, FSC reduces power consumption by 61.3% and chip area by 47%, providing a microarchitecture with high performance at low complexity.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3499424',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Enzian: an open, general, CPU/FPGA platform for systems software research',\n",
       "  'authors': \"['David Cock', 'Abishek Ramdas', 'Daniel Schwyn', 'Michael Giardino', 'Adam Turowski', 'Zhenhao He', 'Nora Hossle', 'Dario Korolija', 'Melissa Licciardello', 'Kristina Martsenko', 'Reto Achermann', 'Gustavo Alonso', 'Timothy Roscoe']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'Hybrid computing platforms, comprising CPU cores and FPGA logic, are increasingly used for accelerating data-intensive workloads in cloud deployments, and are a growing topic of interest in systems research. However, from a research perspective, existing hardware platforms are limited: they are often optimized for concrete, narrow use-cases and, therefore lack the flexibility needed to explore other applications and configurations.   We show that a research group can design and build a more general, open, and affordable hardware platform for hybrid systems research. The platform, Enzian, is capable of duplicating the functionality of existing CPU/FPGA systems with comparable performance but in an open, flexible system. It couples a large FPGA with a server-class CPU in an asymmetric cache-coherent NUMA system. Enzian also enables research not possible with existing hybrid platforms, through explicit access to coherence messages, extensive thermal and power instrumentation, and an open, programmable baseboard management processor.   Enzian is already being used in multiple projects, is open source (both hardware and software), and available for remote use. We present the design principles of Enzian, the challenges in building it, and evaluate it with a range of existing research use-cases alongside other, more specialized platforms, as well as demonstrating research not possible on existing platforms.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503222.3507742',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'F1: A Fast and Programmable Accelerator for Fully Homomorphic Encryption',\n",
       "  'authors': \"['Nikola Samardzic', 'Axel Feldmann', 'Aleksandar Krastev', 'Srinivas Devadas', 'Ronald Dreslinski', 'Christopher Peikert', 'Daniel Sanchez']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"MICRO '21: MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture\",\n",
       "  'abstract': 'Fully Homomorphic Encryption (FHE) allows computing on encrypted data, enabling secure offloading of computation to untrusted servers. Though it provides ideal security, FHE is expensive when executed in software, 4 to 5 orders of magnitude slower than computing on unencrypted data. These overheads are a major barrier to FHE’s widespread adoption.  We present F1, the first FHE accelerator that is programmable, i.e., capable of executing full FHE programs. F1 builds on an in-depth architectural analysis of the characteristics of FHE computations that reveals acceleration opportunities. F1 is a wide-vector processor with novel functional units deeply specialized to FHE primitives, such as modular arithmetic, number-theoretic transforms, and structured permutations. This organization provides so much compute throughput that data movement becomes the key bottleneck. Thus, F1 is primarily designed to minimize data movement. Hardware provides an explicitly managed memory hierarchy and mechanisms to decouple data movement from execution. A novel compiler leverages these mechanisms to maximize reuse and schedule off-chip and on-chip data movement.  We evaluate F1 using cycle-accurate simulation and RTL synthesis. F1 is the first system to accelerate complete FHE programs, and outperforms state-of-the-art software implementations by gmean 5,400 × and by up to 17,000 ×. These speedups counter most of FHE’s overheads and enable new applications, like real-time private deep learning in the cloud.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3466752.3480070',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Non-Intrusive Distributed Tracing of Wireless IoT Devices with the FlockLab\\xa02 Testbed',\n",
       "  'authors': \"['Roman Trüb', 'Reto Da Forno', 'Lukas Daschinger', 'Andreas Biri', 'Jan Beutel', 'Lothar Thiele']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Internet of Things',\n",
       "  'abstract': 'Testbeds for wireless IoT devices facilitate testing and validation of distributed target nodes. A testbed usually provides methods to control, observe, and log the execution of the software. However, most of the methods used for tracing the execution require code instrumentation and change essential properties of the observed system. Methods that are non-intrusive are typically not applicable in a distributed fashion due to a lack of time synchronization or necessary hardware/software support. In this article, we present a tracing system for validating time-critical software running on multiple distributed wireless devices that does not require code instrumentation, is non-intrusive and is designed to trace the distributed state of an entire network. For this purpose, we make use of the on-chip debug and trace hardware that is part of most modern microcontrollers. We introduce a testbed architecture as well as models and methods that accurately synchronize the timestamps of observations collected by distributed observers. In a case study, we demonstrate how the tracing system can be applied to observe the distributed state of a flooding-based low-power communication protocol for wireless sensor networks. The presented non-intrusive tracing system is implemented as a service of the publicly accessible open source FlockLab\\xa02 testbed.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3480248',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Superconducting computing with alternating logic elements',\n",
       "  'authors': \"['Georgios Tzimpragos', 'Jennifer Volk', 'Alex Wynn', 'James E. Smith', 'Timothy Sherwood']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': 'Although superconducting single flux quantum (SFQ) technologies offer the potential for low-latency operation with energy dissipation of the order of attojoules per gate, their inherently pulse-driven nature and stateful cells have led to designs in which every logic gate is clocked. This means that clocked buffers must be added to equalize logic path lengths, and every gate becomes a pipeline stage. We propose a different approach, where gates are clock-free and synchronous designs have a conventional look-and-feel. Despite being clock-free, however, the gates are state machines by nature. To properly manage these state machines, the logical clock cycle is composed of two synchronous alternating phases: the first of which implements the desired function, and the second of which returns the state machines to the ground state. Moreover, to address the challenges associated with the asynchronous implementation of Boolean NOT operations in pulse-based systems, values are represented as unordered binary codes - in particular, dual-rail codes. With unordered codes, AND and OR operations are functionally complete. We demonstrate that our new approach, xSFQ, with its dual-rail construction and alternating clock phases, along with \"double-pumped\" logical latches and a timing optimization through latch decomposition, is capable of implementing arbitrary digital designs without gate-level pipelining and the overheads that come with it. We evaluate energy-delay tradeoffs enabled by this approach through a mix of detailed analog circuit modeling, pulse-level discrete-event simulation, and high-level pipeline efficiency analysis. The resulting systems are shown to deliver energy-delay product (EDP) gains over conventional SFQ even with pipeline hazard ratios (HR) below 1%. For hazard ratios equal to 15% and 20% and a design resembling a RISC-V RV32I core (excluding the cost of interlock logic), xSFQ achieves 22x and 31x EDP savings, respectively.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00057',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Large-scale graph processing on FPGAs with caches for thousands of simultaneous misses',\n",
       "  'authors': \"['Mikhail Asiatici', 'Paolo Ienne']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': 'Efficient large-scale graph processing is crucial to many disciplines. Yet, while graph algorithms naturally expose massive parallelism opportunities, their performance is limited by the memory system because of irregular memory accesses. State-of-the-art FPGA graph processors, such as ForeGraph and FabGraph, address the memory issues by using scratchpads and regularly streaming edges from DRAM, but then they end up wasting bandwidth on unneeded data. Yet, where classic caches and scratchpads fail to deliver, FPGAs make powerful unorthodox solutions possible. In this paper, we resort to extreme nonblocking caches that handle tens of thousands of outstanding read misses. They significantly increase the ability of memory systems to coalesce multiple accelerator accesses into fewer DRAM memory requests; essentially, when latency is not the primary concern, they bring the advantages expected from a very large cache at a fraction of the cost. We prove our point with an adaptable graph accelerator running on Amazon AWS f1; our implementation takes into account all practical aspects of such a design, including the challenges involved when working with modern multidie FPGAs. Running classic algorithms (PageRank, SCC, and SSSP) on large graphs, we achieve 3X geometric mean speedup compared to state-of-the-art FPGA accelerators, 1.1--5.8X higher bandwidth efficiency and 3.0--15.3X better power efficiency than multicore CPUs, and we support much larger graphs than the state-of-the-art on GPUs.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00054',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Context-Based Interface Prototyping: Understanding the Effect of Prototype Representation on User Feedback',\n",
       "  'authors': \"['Marius Hoggenmüller', 'Martin Tomitsch', 'Luke Hespanhol', 'Tram Thi Minh Tran', 'Stewart Worrall', 'Eduardo Nebot']\",\n",
       "  'date': 'May 2021',\n",
       "  'source': \"CHI '21: Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems\",\n",
       "  'abstract': 'The rise of autonomous systems in cities, such as automated vehicles (AVs), requires new approaches for prototyping and evaluating how people interact with those systems through context-based user interfaces, such as external human-machine interfaces (eHMIs). In this paper, we present a comparative study of three prototype representations (real-world VR, computer-generated VR, real-world video) of an eHMI in a mixed-methods study with 42 participants. Quantitative results show that while the real-world VR representation results in higher sense of presence, no significant differences in user experience and trust towards the AV itself were found. However, interview data shows that participants focused on different experiential and perceptual aspects in each of the prototype representations. These differences are linked to spatial awareness and perceived realism of the AV behaviour and its context, affecting in turn how participants assess trust and the eHMI. The paper offers guidelines for prototyping and evaluating context-based interfaces through simulations.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3411764.3445159',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Exploiting page table locality for agile TLB prefetching',\n",
       "  'authors': \"['Georgios Vavouliotis', 'Lluc Alvarez', 'Vasileios Karakostas', 'Konstantinos Nikas', 'Nectarios Koziris', 'Daniel A. Jiménez', 'Marc Casas']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': 'Frequent Translation Lookaside Buffer (TLB) misses incur high performance and energy costs due to page walks required for fetching the corresponding address translations. Prefetching page table entries (PTEs) ahead of demand TLB accesses can mitigate the address translation performance bottleneck, but each prefetch requires traversing the page table, triggering additional accesses to the memory hierarchy. Therefore, TLB prefetching is a costly technique that may undermine performance when the prefetches are not accurate. In this paper we exploit the locality in the last level of the page table to reduce the cost and enhance the effectiveness of TLB prefetching by fetching cache-line adjacent PTEs \"for free\". We propose Sampling-Based Free TLB Prefetching (SBFP), a dynamic scheme that predicts the usefulness of these \"free\" PTEs and prefetches only the ones most likely to prevent TLB misses. We demonstrate that combining SBFP with novel and state-of-the-art TLB prefetchers significantly improves miss coverage and reduces most memory accesses due to page walks. Moreover, we propose Agile TLB Prefetcher (ATP), a novel composite TLB prefetcher particularly designed to maximize the benefits of SBFP. ATP efficiently combines three low-cost TLB prefetchers and disables TLB prefetching for those execution phases that do not benefit from it. Unlike state-of-the-art TLB prefetchers that correlate patterns with only one feature (e.g., strides, PC, distances), ATP correlates patterns with multiple features and dynamically enables the most appropriate TLB prefetcher per TLB miss. To alleviate the address translation performance bottleneck, we propose a unified solution that combines ATP and SBFP. Across an extensive set of industrial workloads provided by Qualcomm, ATP coupled with SBFP improves geometric speedup by 16.2%, and eliminates on average 37% of the memory references due to page walks. Considering the SPEC CPU 2006 and SPEC CPU 2017 benchmark suites, ATP with SBFP increases geometric speedup by 11.1%, and eliminates page walk memory references by 26%. Applied to big data workloads (GAP suite, XSBench), ATP with SBFP yields a geometric speedup of 11.8% while reducing page walk memory references by 5%. Over the best state-of-the-art TLB prefetcher for each benchmark suite, ATP with SBFP achieves speedups of 8.7%, 3.4%, and 4.2% for the Qualcomm, SPEC, and GAP+XSBench workloads, respectively.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00016',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'LILLIPUT: a lightweight low-latency lookup-table decoder for near-term Quantum error correction',\n",
       "  'authors': \"['Poulami Das', 'Aditya Locharla', 'Cody Jones']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'The error rates of quantum devices are orders of magnitude higher than what is needed to run most quantum applications. To close this gap, Quantum Error Correction (QEC) encodes logical qubits and distributes information using several physical qubits. By periodically executing a syndrome extraction circuit on the logical qubits, information about errors (called syndrome) is extracted while running programs. A decoder uses these syndromes to identify and correct errors in real time, which is necessary to prevent accumulation of errors. Unfortunately, software decoders are slow and hardware decoders are fast but less accurate. Thus, almost all QEC studies so far have relied on offline decoding.  To enable real-time decoding in near-term QEC, we propose LILLIPUT– a Lightweight Low Latency Look-Up Table decoder. LILLIPUT consists of two parts– First, it translates syndromes into error detection events that index into a Look-Up Table (LUT) whose entry provides the error information in real-time. Second, it programs the LUTs with error assignments for all possible error events by running a software decoder offline. LILLIPUT tolerates an error on any operation in the quantum hardware, including gates and measurements, and the number of tolerated errors grows with the size of the code. LILLIPUT utilizes less than 7% logic on off-the-shelf FPGAs enabling practical adoption, as FPGAs are already used to design the control and readout circuits in existing systems. LILLIPUT incurs a latency of a few nanoseconds and enables real-time decoding. We also propose Compressed LUTs (CLUTs) to reduce the memory required by LILLIPUT. By exploiting the fact that not all error events are equally likely and only storing data for the most probable error events, CLUTs reduce the memory needed by up-to 107x (from 148 MB to 1.38 MB) without degrading the accuracy.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503222.3507707',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Streamline: a fast, flushless cache covert-channel attack by enabling asynchronous collusion',\n",
       "  'authors': \"['Gururaj Saileshwar', 'Christopher W. Fletcher', 'Moinuddin Qureshi']\",\n",
       "  'date': 'April 2021',\n",
       "  'source': \"ASPLOS '21: Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'Covert-channel attacks exploit contention on shared hardware resources such as processor caches to transmit information between colluding processes on the same system. In recent years, covert channels leveraging cacheline-flush instructions, such as Flush+Reload and Flush+Flush, have emerged as the fastest cross-core attacks. However, current attacks are limited in their applicability and bit-rate not due to any fundamental hardware limitations, but due to their protocol design requiring flush instructions and tight synchronization between sender and receiver, where both processes synchronize every bit-period to maintain low error-rates.   In this paper, we present Streamline, a flush-less covert-channel attack faster than all prior known attacks. The key insight behind the higher channel bandwidth is asynchronous communication. Streamline communicates over a sequence of shared addresses (larger than the cache size), where the sender can move to the next address after transmitting each bit without waiting for the receiver. Furthermore, it ensures that addresses accessed by the sender are preserved in the cache until the receiver has accessed them. Finally, by the time the sender accesses the entire sequence and wraps around, the cache-thrashing property ensures that the previously transmitted addresses are automatically evicted from the cache without any cacheline flushes, which ensures functional correctness while simultaneously improving channel bandwidth. To orchestrate Streamline on a real system, we overcome multiple challenges, such as circumventing hardware optimizations (prefetching and replacement policy), and ensuring that the sender and receiver have similar execution rates. We demonstrate Streamline on an Intel Skylake CPU and show that it achieves a bit-rate of 1801 KB/s, which is 3x to 3.6x faster than the previous fastest Take-a-Way (588 KB/s) and Flush+Flush (496 KB/s) attacks, at comparable error rates. Unlike prior attacks, Streamline only relies on generic properties of caches and is applicable to processors of all ISAs (x86, ARM, etc.) and micro-architectures (Intel, AMD, etc.).',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3445814.3446742',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Software-defined address mapping: a case on 3D memory',\n",
       "  'authors': \"['Jialiang Zhang', 'Michael Swift', 'Jing (Jane) Li']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': '3D-stacking memory such as High-Bandwidth Memory (HBM) and Hybrid Memory Cube (HMC) provides orders of magnitude more bandwidth and significantly increased channel-level parallelism (CLP) due to its new parallel memory architecture. However, it is challenging to fully exploit the abundant CLP for performance as the bandwidth utilization is highly dependent on address mapping in the memory controller. Unfortunately, CLP is very sensitive to a program’s data access pattern, which is not made available to OS/hardware by existing mechanisms.  In this work, we address these challenges with software-defined address mapping (SDAM) that, for the first time, enables user program to obtain a direct control of the low-level memory hardware in a more intelligent and fine-grained manner. In particular, we develop new mechanisms that can effectively communicate a program’s data access properties to the OS and hardware and to use it to control data placement in hardware. To guarantee correctness and reduce overhead in storage and performance, we extend Linux kernel and C-language memory allocators to support multiple address mappings. For advanced system optimization, we develop machine learning methods that can automatically identify access patterns of major variables in a program and cluster these with similar access patterns to reduce the overhead for SDAM. We demonstrate the benefits of our design on real system prototype, comprising (1) a RISC-V processor, near memory accelerators and HBM modules using Xilinx FPGA platform, and (2) modified Linux and glibc. Our evaluation on standard CPU benchmarks and data-intensive benchmarks (for both CPU and accelerators) demonstrates 1.41×, 1.84× speedup on CPU and 2.58× on near memory accelerators in our system with SDAM compared to a baseline system that uses a fixed address mapping.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503222.3507774',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A compiler infrastructure for accelerator generators',\n",
       "  'authors': \"['Rachit Nigam', 'Samuel Thomas', 'Zhijing Li', 'Adrian Sampson']\",\n",
       "  'date': 'April 2021',\n",
       "  'source': \"ASPLOS '21: Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'We present Calyx, a new intermediate language (IL) for compiling high-level programs into hardware designs. Calyx combines a hardware-like structural language with a software-like control flow representation with loops and conditionals. This split representation enables a new class of hardware-focused optimizations that require both structural and control flow information which are crucial for high-level programming models for hardware design. The Calyx compiler lowers control flow constructs using finite-state machines and generates synthesizable hardware descriptions.  We have implemented Calyx in an optimizing compiler that translates high-level programs to hardware. We demonstrate Calyx using two DSL-to-RTL compilers, a systolic array generator and one for a recent imperative accelerator language, and compare them to equivalent designs generated using high-level synthesis (HLS). The systolic arrays are 4.6× faster and 1.11× larger on average than HLS implementations, and the HLS-like imperative language compiler is within a few factors of a highly optimized commercial HLS toolchain. We also describe three optimizations implemented in the Calyx compiler.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3445814.3446712',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Execution dependence extension (EDE): isa support for eliminating fences',\n",
       "  'authors': \"['Thomas Shull', 'Ilias Vougioukas', 'Nikos Nikoleris', 'Wendy Elsasser', 'Josep Torrellas']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': \"Fence instructions are a coarse-grained mechanism to enforce the order of instruction execution in an out-of-order pipeline. They are an overkill for cases when only one instruction must wait for the completion of one other instruction. For example, this is the case when performing undo logging in Non-Volatile Memory (NVM) systems: while the update of a variable needs to wait until the corresponding undo log entry is persisted, all other instructions can be reordered. Unfortunately, current ISAs do not provide a way to describe such an execution dependence between two instructions that have no register or memory dependences. As a result, programmers must place fences, which unnecessarily serialize many unrelated instructions. To remedy this limitation, we propose an ISA extension capable of describing these execution dependences. We call the proposal Execution Dependence Extension (EDE), and add it to Arm's AArch64 ISA. We also present two hardware realizations of EDE that enforce execution dependences at different stages of the pipeline: one in the issue queue (IQ) and another in the write buffer (WB). We implement IQ and WB in a simulator and test them with several NVM applications. Overall, by using EDE with IQ and WB rather than fences, we attain average workload speedups of 18% and 26%, respectively.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00043',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'CoolEdge: hotspot-relievable warm water cooling for energy-efficient edge datacenters',\n",
       "  'authors': \"['Qiangyu Pei', 'Shutong Chen', 'Qixia Zhang', 'Xinhui Zhu', 'Fangming Liu', 'Ziyang Jia', 'Yishuo Wang', 'Yongjie Yuan']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'As the computing frontier drifts to the edge, edge datacenters play a crucial role in supporting various real-time applications. Different from cloud datacenters, the requirements of proximity to end-users, high density, and heterogeneity, present new challenges to cool the edge datacenters efficiently. Although warm water cooling has become a promising cooling technique for this infrastructure, the one-size-fits-all cooling control would lower the cooling efficiency considerably because of the severe thermal imbalance across servers, hardware, and even inside one hardware component in an edge datacenter. In this work, we propose CoolEdge, a hotspot-relievable warm water cooling system for improving the cooling efficiency and saving costs of edge datacenters. Specifically, through the elaborate design of water circulations, CoolEdge can dynamically adjust the water temperature and flow rate for each heterogeneous hardware component to eliminate the hardware-level hotspots. By redesigning cold plates, CoolEdge can quickly disperse the chip-level hotspots without manual intervention. We further quantify the power saving achieved by the warm water cooling theoretically, and propose a custom-designed cooling solution to decide an appropriate water temperature and flow rate periodically. Based on a hardware prototype and real-world traces from SURFsara, the evaluation results show that CoolEdge reduces the cooling energy by 81.81% and 71.92%, respectively, compared with conventional and state-of-the-art water cooling systems.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503222.3507713',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Data Structures for a Generic Software System using the Composite Design Pattern',\n",
       "  'authors': \"['Stefan Nadschläger', 'Daniel Hofer', 'Josef Küng', 'Markus Jäger']\",\n",
       "  'date': 'July 2021',\n",
       "  'source': \"EuroPLoP'21: 26th European Conference on Pattern Languages of Programs\",\n",
       "  'abstract': 'A well-designed generic software system can be reused in many contexts by efficiently handling variability. In this paper, data structures and their application in a software architecture is presented that apply the basic idea of the Composite design pattern to keep it as simple as possible, but also as generic as possible. The application of these data structures is shown throughout a layered architecture so that software developers can follow and apply the concepts. The benefit of such an architecture is that it (1) only makes use of familiar concepts, and it is easy to read and understand by knowing especially one design pattern and (2) results in a generic software system, usable in different domains.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3489449.3489972',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'PF-DRAM: a precharge-free DRAM structure',\n",
       "  'authors': \"['Nezam Rohbani', 'Sina Darabi', 'Hamid Sarbazi-Azad']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': 'Although DRAM capacity and bandwidth have increased sharply by the advances in technology and standards, its latency and energy per access have remained almost constant in recent generations. The main portion of DRAM power/energy is dissipated by Read, Write, and Refresh operations, all initiated by a Precharge phase. Precharge phase not only imposes a large amount of energy consumption, but also increases the delay of closing a row in a memory block to open another one. By reduction of row-hit rate in recent workloads, especially in multi-core systems, precharge rate increases which exacerbates DRAM power dissipation and access latency. This work proposes a novel DRAM structure, called Precharge-Free DRAM (PF-DRAM), that eliminates the Precharge phase of DRAM. PF-DRAM uses the charge on bitlines from the previous Activation phase, as the starting point for the next Activation. The difference between PF-DRAM and conventional DRAM structure is limited to precharge and equalizer circuitry and simple modifications in sense amplifier, which are all limited to subarray level. PF-DRAM is compatible with the mainstream JEDEC memory standards like DDRx and HBM, with minimum modifications in memory controller. Furthermore, almost all of the previously proposed power/energy reduction techniques in DRAM are still applicable to PF-DRAM for further improvement. Our experimental results on a 8 GB memory system running SPEC CPU2017 and PAR-SEC 2.1 workloads show an average of 35.3% memory power consumption reduction (up to 54.2%) achieved by the system using PF-DRAM with respect to the system using conventional DRAM. Moreover, the overall performance is improved by 8.6%, in average (up to 24.3%). According to our analysis, all such improvements are achieved for less than 9% area overhead.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00019',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Time-optimal Qubit mapping',\n",
       "  'authors': \"['Chi Zhang', 'Ari B. Hayes', 'Longfei Qiu', 'Yuwei Jin', 'Yanhao Chen', 'Eddy Z. Zhang']\",\n",
       "  'date': 'April 2021',\n",
       "  'source': \"ASPLOS '21: Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'Rapid progress in the physical implementation of quantum computers gave birth to multiple recent quantum machines implemented with superconducting technology. In these NISQ machines, each qubit is physically connected to a bounded number of neighbors. This limitation prevents most quantum programs from being directly executed on quantum devices. A compiler is required for converting a quantum program to a hardware-compliant circuit, in particular, making each two-qubit gate executable by mapping the two logical qubits to two physical qubits with a link between them. To solve this problem, existing studies focus on inserting SWAP gates to dynamically remap logical qubits to physical qubits. However, most of the schemes lack the consideration of time-optimality of generated quantum circuits, or are achieving time-optimality with certain constraints. In this work, we propose a theoretically time-optimal SWAP insertion scheme for the qubit mapping problem. Our model can also be extended to practical heuristic algorithms. We present exact analysis results by using our model for quantum programs with recurring execution patterns. We have for the first time discovered an optimal qubit mapping pattern for quantum fourier transformation (QFT) on 2D nearest neighbor architecture. We also present a scalable extension of our theoretical model that can be used to solve qubit mapping for large quantum circuits.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3445814.3446706',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Speculative vectorisation with selective replay',\n",
       "  'authors': \"['Peng Sun', 'Giacomo Gabrielli', 'Timothy M. Jones']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': \"While industry continues to develop SIMD vector ISAs by providing new instructions and wider data-paths, modern SIMD architectures still rely on the programmer or compiler to transform code to vector form only when it is safe. Limitations in the power of a compiler's memory alias analysis and the presence of infrequent memory data dependences mean that whole regions of code cannot be safely vectorised without risking changing the semantics of the application, restricting the available performance. We present a new SIMD architecture to address this issue, which relies on speculation to identify and catch memory-dependence violations that occur during vector execution. Once identified, only those SIMD lanes that have used erroneous data are replayed; other lanes, both older and younger, keep the results of their latest execution. We use the compiler to mark loops with possible cross-iteration dependences and safely vectorise them by executing on our architecture, termed selective-replay vectorisation (SRV). Evaluating on a range of general-purpose and HPC benchmarks gives an average loop speedup of 2.9X, and up to 5.3X in the best case, over already-vectorised code. This leads to a whole-program speedup of up to 1.19X (average 1.06X) over already-vectorised applications.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00026',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Boosting the Restoring Performance of Deduplication Data by Classifying Backup Metadata',\n",
       "  'authors': \"['Ru Yang', 'Yuhui Deng', 'Yi Zhou', 'Ping Huang']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM/IMS Transactions on Data Science',\n",
       "  'abstract': 'Restoring data is the main purpose of data backup in storage systems. The fragmentation issue, caused by physically scattering logically continuous data across a variety of disk locations, poses a negative impact on the restoring performance of a deduplication system. Rewriting algorithms are used to alleviate the fragmentation problem by improving the restoring speed of a deduplication system. However, rewriting methods give birth to a big sacrifice in terms of deduplication ratio, leading to a huge storage space waste. Furthermore, traditional backup approaches treat file metadata and chunk metadata as the same, which causes frequent on-disk metadata accesses. In this article, we start by analyzing storage characteristics of backup metadata. An intriguing finding shows that with 10 million files, the file metadata merely takes up approximately 340 MB. Motivated by this finding, we propose a Classified-Metadata based Restoring method (CMR) that classifies backup metadata into file metadata and chunk metadata. Because the file metadata merely takes up a meager amount of space, CMR maintains all file metadata in memory, whereas chunk metadata are aggressively prefetched to memory in a greedy manner. A deduplication system with CMR in place exhibits three salient features: (i) It avoids rewriting algorithms’ additional overhead by reducing the number of disk reads in a restoring process, (ii) it increases the restoring throughput without sacrificing the deduplication ratio, and (iii) it thoroughly leverages the hardware resources to boost the restoring performance. To quantitatively evaluate the performance of CMR, we compare our CMR against two state-of-the-art approaches, namely, a history-aware rewriting method (HAR) and a context-based rewriting scheme (CAP). The experimental results show that compared to HAR and CAP, CMR reduces the restoring time by 27.2% and 29.3%, respectively. Moreover, the deduplication ratio is improved by 1.91% and 4.36%, respectively.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3437261',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Automated HW/SW co-design for edge AI: state, challenges and steps ahead',\n",
       "  'authors': \"['Oliver Bringmann', 'Wolfgang Ecker', 'Ingo Feldner', 'Adrian Frischknecht', 'Christoph Gerum', 'Timo Hämäläinen', 'Muhammad Abdullah Hanif', 'Michael J. Klaiber', 'Daniel Mueller-Gritschneder', 'Paul Palomero Bernardo', 'Sebastian Prebeck', 'Muhammad Shafique']\",\n",
       "  'date': 'September 2021',\n",
       "  'source': \"CODES/ISSS '21: Proceedings of the 2021 International Conference on Hardware/Software Codesign and System Synthesis\",\n",
       "  'abstract': 'Gigantic rates of data production in the era of Big Data, Internet of Thing (IoT), and Smart Cyber Physical Systems (CPS) pose incessantly escalating demands for massive data processing, storage, and transmission while continuously interacting with the physical world using edge sensors and actuators. For IoT systems, there is now a strong trend to move the intelligence from the cloud to the edge or the extreme edge (known as TinyML). Yet, this shift to edge AI systems requires to design powerful machine learning systems under very strict resource constraints. This poses a difficult design task that needs to take the complete system stack from machine learning algorithm, to model optimization and compression, to software implementation, to hardware platform and ML accelerator design into account. This paper discusses the open research challenges to achieve such a holistic Design Space Exploration for a HW/SW Co-design for Edge AI Systems and discusses the current state with three currently developed flows: one design flow for systems with tightly-coupled accelerator architectures based on RISC-V, one approach using loosely-coupled, application-specific accelerators as well as one framework that integrates software and hardware optimization techniques to built efficient Deep Neural Network (DNN) systems.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3478684.3479261',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A pattern for a Secure IoT Thing',\n",
       "  'authors': \"['Eduardo B. Fernandez', 'Hernan Astudillo', 'Cristian Orellana']\",\n",
       "  'date': 'July 2021',\n",
       "  'source': \"EuroPLoP'21: 26th European Conference on Pattern Languages of Programs\",\n",
       "  'abstract': 'IoT systems are very complex systems with an extensive attack surface, which makes them susceptible to a large variety of threats. It is important that every component in an IoT architecture be secure, any weak point can allow an adversary to penetrate the system. We present here a pattern to add security to an IoT thing, where a “thing” is an entity that has an identity, some intelligence, is able to communicate with other entities, and is connected to the internet. We presented earlier a pattern for a Secure IoT Architecture, the Secure IoT Thing pattern complements that pattern by securing one of its main components. This pattern is an addition to a set of patterns focusing on the security of IoT ecosystems.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3489449.3489988',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'NASA: accelerating neural network design with a NAS processor',\n",
       "  'authors': \"['Xiaohan Ma', 'Chang Si', 'Ying Wang', 'Cheng Liu', 'Lei Zhang']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': 'Neural network search (NAS) projects a promising direction to automate the design process of efficient and powerful neural network architectures. Nevertheless, the NAS techniques have to dynamically generate a large number of candidate neural networks, and iteratively train and evaluate these on-line generated network architectures, thus they are extremely time-consuming even when deployed on large GPU clusters, which dramatically hinders the adoption of NAS. Though recently there are many specialized architectures proposed to accelerate the training or inference of neural networks, we observe that existing neural network accelerators are typically targeted at static neural network architectures, and they are not suitable to accelerate the evaluation of the dynamical neural network candidates evolving during the NAS process, which cannot be deployed onto current accelerators via the off-line compilation. To enable rapid and energy-efficient NAS in compact singlechip solutions, we propose NASA, a specialized architecture for one-shot based NAS acceleration. It is able to generate, schedule, and evaluate the candidate neural network architectures for the target machine learning workload with high speed, significantly alleviating the processing bottleneck of one-shot NAS. Motivated by the observation that there are considerable computation sharing opportunities among the different neural network candidates generated in one-shot NAS, NASA is equipped with an on-chip network fusion unit to remove the redundant computation during the network mapping stage. In addition, the NASA accelerator can partition and re-schedule the candidate neural network architectures at fine-granularity to maximize the chance of data reuse and improve the utilization of the accelerator arrays integrated to accelerate network evaluation. According to our experiments on multiple one-shot NAS tasks, NASA achieves 33.52X performance speedup and 214.33X energy consumption reduction on average when compared to a CPU-GPU system.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00067',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A real-time experimentation platform for sub-6 GHz and millimeter-wave MIMO systems',\n",
       "  'authors': \"['Jesus O. Lacruz', 'Rafael Ruiz Ortiz', 'Joerg Widmer']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"MobiSys '21: Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services\",\n",
       "  'abstract': \"The performance of wireless communication systems is evolving rapidly, making it difficult to build experimentation platforms that meet the hardware requirements of new standards. The bandwidth of current systems ranges from 160 MHz for IEEE 802.11ac/ax to 2 GHz for Millimeter-Wave (mm-wave) IEEE 802.11ad/ay, and they support up to 8 spatial MIMO streams. Mobile 5G and beyond systems have a similarly diverse set of requirements. To address this, we propose a highly configurable wireless platform that meets such requirements and is both affordable and scalable. It is implemented on a single state-of-the-art FPGA board that can be configured from 4x4 mm-wave MIMO with 2 GHz channels to 8x8 MIMO with 160 MHz channels in sub-6 GHz bands. In addition, multi-band operation will play an important role in future wireless networks and our platform supports mixed configurations with simultaneous use of mm-wave and sub-6 GHz. Finally, the platform supports real-time operation, e.g., for closed-loop MIMO beam training with low-latency, by implementing suitable hardware/software accelerators. We demonstrate the platform's performance in a wide range of experiments. The platform is provided as open-source to build a community to use and extend it.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3458864.3466868',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Learning from the Past: Efficient High-level Synthesis Design Space Exploration for FPGAs',\n",
       "  'authors': \"['Zi Wang', 'Benjamin Carrion Schafer']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Design Automation of Electronic Systems',\n",
       "  'abstract': 'The quest to democratize the use of Field-Programmable Gate Arrays (FPGAs) has given High-Level Synthesis (HLS) the final push to be widely accepted with FPGA vendors strongly supporting this VLSI design methodology to expand the FPGA user base. HLS takes as input an untimed behavioral description and generates efficient RTL (Verilog or VHDL). One major advantage of HLS is that it allows us to generate a variety of different micro-architectures from the same behavioral description by simply specifying different combination of synthesis options. In particular, commercial HLS tools make extensive use of synthesize directives in the form pragmas. This strength is also a weakness as it forces HLS users to fully understand how these synthesis options work and how they interact to efficiently set them to get a hardware implementation with the desired characteristics. Luckily, this process can be automated. Unfortunately, the search space grows supra-linearly with the number of synthesis options. To address this, this work proposes an automatic synthesis option tuner dedicated for FPGAs. We have explored a larger number of behavioral descriptions targeting ASICs and FPGAs and found out that due to the internal structure of the FPGA a large number of synthesis options combinations never lead to a Pareto-optimal design and, hence, the search space can be drastically reduced. Moreover, we make use of large database of DSE results that we have generated since we started working in this field to further accelerate the exploration process. For this, we use a technique based on perceptual hashing that allows our proposed explorer to recognize similar program structures in the new description to be explored and match them with structures in our database. This allows us to directly retrieve the pragma settings that lead to Pareto-optimal configurations. Experimental results show that the search space can be accelerated substantially while leading to finding most of the Pareto-optimal designs.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3495531',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Polyhedral-Based Compilation Framework for In-Memory Neural Network Accelerators',\n",
       "  'authors': \"['Jianhui Han', 'Xiang Fei', 'Zhaolin Li', 'Youhui Zhang']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Journal on Emerging Technologies in Computing Systems',\n",
       "  'abstract': 'Memristor-based processing-in-memory architecture is a promising solution to the memory bottleneck in the neural network (NN) processing. A major challenge for the programmability of such architectures is the automatic compilation of high-level NN workloads, from various operators to the memristor-based hardware that may provide programming interfaces with different granularities. This article proposes a source-to-source compilation framework for such memristor-based NN accelerators, which can conduct automatic detection and mapping of multiple NN operators based on the flexible and rich representation capability of the polyhedral model. In contrast to previous studies, it implements support for pipeline generation to exploit the parallelism in the NN loads to leverage hardware resources for higher efficiency. The evaluation based on synthetic kernels and NN benchmarks demonstrates that the proposed framework can reliably detect and map the target operators. Case studies on typical memristor-based architectures also show its generality over various architectural designs. The evaluation further demonstrates that compared with existing polyhedral-based compilation frameworks that do not support the pipelined execution, the performance can upgrade by an order of magnitude with the pipelined execution, which emphasizes the necessity of our improvement.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3469847',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'COSMO: Computing with Stochastic Numbers in Memory',\n",
       "  'authors': \"['Saransh Gupta', 'Mohsen Imani', 'Joonseop Sim', 'Andrew Huang', 'Fan Wu', 'Jaeyoung Kang', 'Yeseong Kim', 'Tajana Šimunić Rosing']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Journal on Emerging Technologies in Computing Systems',\n",
       "  'abstract': 'Stochastic computing (SC) reduces the complexity of computation by representing numbers with long streams of independent bits. However, increasing performance in SC comes with either an increase in area or a loss in accuracy. Processing in memory (PIM) computes data in-place while having high memory density and supporting bit-parallel operations with low energy consumption. In this article, we propose COSMO, an architecture for computing with stochastic numbers in memory, which enables SC in memory. The proposed architecture is general and can be used for a wide range of applications. It is a highly dense and parallel architecture that supports most SC encodings and operations in memory. It maximizes the performance and energy efficiency of SC by introducing several innovations: (i) in-memory parallel stochastic number generation, (ii) efficient implication-based logic in memory, (iii) novel memory bit line segmenting, (iv) a new memory-compatible SC addition operation, and (v) enabling flexible block allocation. To show the generality and efficiency of our stochastic architecture, we implement image processing, deep neural networks (DNNs), and hyperdimensional (HD) computing on the proposed hardware. Our evaluations show that running DNN inference on COSMO is 141× faster and 80× more energy efficient as compared to GPU.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3484731',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Learning from the Past: Efficient High-level Synthesis Design Space Exploration for FPGAs',\n",
       "  'authors': \"['Zi Wang', 'Benjamin Carrion Schafer']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Design Automation of Electronic Systems',\n",
       "  'abstract': 'The quest to democratize the use of Field-Programmable Gate Arrays (FPGAs) has given High-Level Synthesis (HLS) the final push to be widely accepted with FPGA vendors strongly supporting this VLSI design methodology to expand the FPGA user base. HLS takes as input an untimed behavioral description and generates efficient RTL (Verilog or VHDL). One major advantage of HLS is that it allows us to generate a variety of different micro-architectures from the same behavioral description by simply specifying different combination of synthesis options. In particular, commercial HLS tools make extensive use of synthesize directives in the form pragmas. This strength is also a weakness as it forces HLS users to fully understand how these synthesis options work and how they interact to efficiently set them to get a hardware implementation with the desired characteristics. Luckily, this process can be automated. Unfortunately, the search space grows supra-linearly with the number of synthesis options. To address this, this work proposes an automatic synthesis option tuner dedicated for FPGAs. We have explored a larger number of behavioral descriptions targeting ASICs and FPGAs and found out that due to the internal structure of the FPGA a large number of synthesis options combinations never lead to a Pareto-optimal design and, hence, the search space can be drastically reduced. Moreover, we make use of large database of DSE results that we have generated since we started working in this field to further accelerate the exploration process. For this, we use a technique based on perceptual hashing that allows our proposed explorer to recognize similar program structures in the new description to be explored and match them with structures in our database. This allows us to directly retrieve the pragma settings that lead to Pareto-optimal configurations. Experimental results show that the search space can be accelerated substantially while leading to finding most of the Pareto-optimal designs.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3495531',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'The Uncertainty of Side-channel Analysis: A Way to Leverage from Heuristics',\n",
       "  'authors': \"['Unai Rioja', 'Servio Paguada', 'Lejla Batina', 'Igor Armendariz']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Journal on Emerging Technologies in Computing Systems',\n",
       "  'abstract': 'Performing a comprehensive side-channel analysis evaluation of small embedded devices is a process known for its variability and complexity. In real-world experimental setups, the results are largely influenced by a huge amount of parameters, some of which are not easily adjusted without trial and error and are heavily relying on the experience of professional security analysts. In this article, we advocate the usage of an existing statistical methodology called Six Sigma (6\\\\(\\\\)) for side-channel analysis optimization. This well-known methodology is commonly used in other industrial fields, such as production and quality engineering, to reduce the variability of industrial processes. We propose a customized Six Sigma methodology, which allows even a less-experienced security analysis to select optimal values for the different variables that are critical for the side-channel analysis procedure. Moreover, we show how our methodology helps in improving different phases in the side-channel analysis process.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3446997',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'LION: real-time I/O transfer control for massively parallel processor arrays',\n",
       "  'authors': \"['Dominik Walter', 'Jürgen Teich']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"MEMOCODE '21: Proceedings of the 19th ACM-IEEE International Conference on Formal Methods and Models for System Design\",\n",
       "  'abstract': 'The performance of many accelerator architectures depends on the communication with external memory. During execution, new I/O data is continuously fetched forth and back to memory. This data exchange is very often performance-critical and a careful orchestration thus vital. To satisfy the I/O demand for accelerators of loop nests, it was shown that the individual reads and writes can be merged into larger blocks, which are subsequently transferred by a single DMA transfer. Furthermore, the order in which such DMA transfers must be issued, was shown to be reducible to a real-time task scheduling problem to be solved at run time. Rather than just concepts, we investigate in this paper efficient algorithms, data structures and their implementation in hardware of such a programmable Loop I/O Controller architecture called LION that only needs to be synthesized once for each processor array size and I/O buffer configuration, thus supporting a large class of processor arrays. Based on a proposed heap-based priority queue, LION is able to issue every 6 cycles a new DMA request to a memory bus. Even on a simple FPGA prototype running at just 200 MHz, this allows for more than 33 million DMA requests to be issued per second. Since the execution time of a typical DMA request is in general at least one order of magnitude longer, we can conclude that this rate is sufficient to fully utilize a given memory interface. Finally, we present implementations on FPGA and also 22nm FDX ASIC showing that the overall overhead of a LION typically amounts to less than 5% of an overall processor array design.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3487212.3487349',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': '“An Error Occurred!” - Trust Repair With Virtual Robot Using Levels of Mistake Explanation',\n",
       "  'authors': \"['Kasper Hald', 'Katharina Weitz', 'Elisabeth André', 'Matthias Rehm']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"HAI '21: Proceedings of the 9th International Conference on Human-Agent Interaction\",\n",
       "  'abstract': 'Human-robot collaboration in industrial settings is an expanding research field in robotics. When working together, robot mistakes are an important factor to decrease trust and therefore interferes with cooperation. It is unclear whether explanations help to restore human-robot trust after a mistake. In our study, we investigate whether system explanations as a trust-repairing action after a robot makes a mistake in a collaborative task is helpful. Our pilot study revealed that users are more interested in solutions to errors than they are in just why the error happened. Therefore, in our main study, we evaluated three levels of mistake explanations (no explanation, explanation, and explanation with solution) after a robot in VR made a mistake in executing a shared objective. After testing with 30 participants we found that the robot making a mistake significantly affects trust toward the robot, compared to it completing the task successfully. While participants found the explanations helpful to trust or distrust the robot, the levels of the explanation did not lead to an increase in trust towards the robot after a mistake. In addition, we found no significant impact of explanations on self-efficacy and the emotional state of the participants. Our results show that explanations alone are not sufficient to increase human-computer trust after robot mistakes.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3472307.3484170',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Quantifying server memory frequency margin and using it to improve performance in HPC systems',\n",
       "  'authors': \"['Da Zhang', 'Gagandeep Panwar', 'Jagadish B. Kotra', 'Nathan DeBardeleben', 'Sean Blanchard', 'Xun Jian']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': \"To maintain strong reliability, memory manufacturers label server memories at much slower data rates than the highest data rates at which they can still operate correctly for most (e.g., 99.999%+ of) accesses; we refer to the gap between these two data rates as memory frequency margin. While many prior works have studied memory latency margins in a different context of consumer memories, none has publicly studied memory frequency margin (either for consumer or server memories). To close this knowledge gap in the public domain, we perform the first public study to characterize frequency margins in commodity server memory modules. Through our large-scale study, we find that under standard voltage and cooling, they can operate 27% faster, on average, without error(s) for 99.999%+ of accesses even at high temperatures. The current practice of conservatively operating server memory is far from ideal; it slows down 99.999%+ of accesses to benefit the <0.001% of accesses that would be erroneous at a faster data rate. An ideal system should only pay this reliability tax for the <0.001% of accesses that actually need it. Towards unleashing ideal performance, our second contribution is performing the first exploration on exploiting server memory frequency margin to maximize performance. We focus on High-Performance Computing (HPC) systems, where performance is paramount. We propose exploiting HPC systems' abundant free memory in the common case to store copies of every data block and operate the copies unreliably fast to speedup common-case accesses; we use the safely-operated original blocks for recovery when the unsafely-operated copies become corrupted. We refer to our idea as Heterogeneously-accessed Dual Module Redundancy (Hetero-DMR). Hetero-DMR improves node-level performance by 18%, on average across two CPU memory hierarchies and six HPC benchmark suites, while weighted by different frequency margins and different levels of memory utilization. We also use a real system to emulate the speedup of Hetero-DMR over a conventional system; it closely matches simulation. Our system-wide simulations show applying Hetero-DMR to an HPC system provides 1.4x average speedup on job turnaround time. To facilitate adoption, Hetero-DMR also rigorously preserves system reliability and works for commodity DIMMs and CPU-memory interfaces.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00064',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'SARA: scaling a reconfigurable dataflow accelerator',\n",
       "  'authors': \"['Yaqi Zhang', 'Nathan Zhang', 'Tian Zhao', 'Matt Vilim', 'Muhammad Shahbaz', 'Kunle Olukotun']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': 'The need for speed in modern data-intensive workloads and the rise of \"dark silicon\" in the semiconductor industry are pushing for larger, faster, and more energy and area-efficient architectures, such as Reconfigurable Dataflow Accelerators (RDAs). Nevertheless, challenges remain in developing mechanisms to effectively utilize the compute power of these large-scale RDAs. To address these challenges, we present SARA, a compiler that employs a novel mapping strategy to efficiently utilize large-scale RDAs. Starting from a single-threaded imperative abstraction, SARA spatially maps a program onto RDA\\'s distributed resources, exploiting dataflow parallelism within and across hyperblocks to saturate the compute throughput of an RDA. SARA introduces (a) compiler-managed memory consistency (CMMC), a control paradigm that hierarchically pipelines a nested and data-dependent control-flow graph onto a dataflow architecture, and (b) a compilation flow that decomposes the program graph across distributed heterogeneous resources to hide low-level RDA constraints from programmers. Our evaluation shows that SARA achieves close to perfect performance scaling on a recently proposed RDA-Plasticine. Over a mix of deep-learning, graph-processing, and streaming applications, SARA achieves a 1.9X geo-mean speedup over a Tesla V100 GPU using only 12% of the silicon area.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00085',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Cost-efficient overclocking in immersion-cooled datacenters',\n",
       "  'authors': \"['Majid Jalili', 'Ioannis Manousakis', 'Íñigo Goiri', 'Pulkit A. Misra', 'Ashish Raniwala', 'Husam Alissa', 'Bharath Ramakrishnan', 'Phillip Tuma', 'Christian Belady', 'Marcus Fontoura', 'Ricardo Bianchini']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': 'Cloud providers typically use air-based solutions for cooling servers in datacenters. However, increasing transistor counts and the end of Dennard scaling will result in chips with thermal design power that exceeds the capabilities of air cooling in the near future. Consequently, providers have started to explore liquid cooling solutions (e.g., cold plates, immersion cooling) for the most power-hungry workloads. By keeping the servers cooler, these new solutions enable providers to operate server components beyond the normal frequency range (i.e., overclocking them) all the time. Still, providers must tradeoff the increase in performance via overclocking with its higher power draw and any component reliability implications. In this paper, we argue that two-phase immersion cooling (2PIC) is the most promising technology, and build three prototype 2PIC tanks. Given the benefits of 2PIC, we characterize the impact of overclocking on performance, power, and reliability. Moreover, we propose several new scenarios for taking advantage of overclocking in cloud platforms, including oversubscribing servers and virtual machine (VM) auto-scaling. For the auto-scaling scenario, we build a system that leverages overclocking for either hiding the latency of VM creation or postponing the VM creations in the hopes of not needing them. Using realistic cloud workloads running on a tank prototype, we show that overclocking can improve performance by 20%, increase VM packing density by 20%, and improve tail latency in auto-scaling scenarios by 54%. The combination of 2PIC and overclocking can reduce platform cost by up to 13% compared to air cooling.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00055',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Pioneering chiplet technology and design for the AMD EPYC™ and Ryzen™ processor families',\n",
       "  'authors': \"['Samuel Naffziger', 'Noah Beck', 'Thomas Burd', 'Kevin Lepak', 'Gabriel H. Loh', 'Mahesh Subramony', 'Sean White']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': 'For decades, Moore\\'s Law has delivered the ability to integrate an exponentially increasing number of devices in the same silicon area at a roughly constant cost. This has enabled tremendous levels of integration, where the capabilities of computer systems that previously occupied entire rooms can now fit on a single integrated circuit. In recent times, the steady drum beat of Moore\\'s Law has started to slow down. Whereas device density historically doubled every 18--24 months, the rate of recent silicon process advancements has declined. While improvements in device scaling continue, albeit at a reduced pace, the industry is simultaneously observing increases in manufacturing costs. In response, the industry is now seeing a trend toward reversing direction on the traditional march toward more integration. Instead, multiple industry and academic groups are advocating that systems on chips (SoCs) be \"disintegrated\" into multiple smaller \"chiplets.\" This paper details the technology challenges that motivated AMD to use chiplets, the technical solutions we developed for our products, and how we expanded the use of chiplets from individual processors to multiple product families.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00014',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'SmartIO: Zero-overhead Device Sharing through PCIe Networking',\n",
       "  'authors': \"['Jonas Markussen', 'Lars Bjørlykke Kristiansen', 'Pål Halvorsen', 'Halvor Kielland-Gyrud', 'Håkon Kvale Stensland', 'Carsten Griwodz']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Computer Systems',\n",
       "  'abstract': 'The large variety of compute-heavy and data-driven applications accelerate the need for a distributed I/O solution that enables cost-effective scaling of resources between networked hosts. For example, in a cluster system, different machines may have various devices available at different times, but moving workloads to remote units over the network is often costly and introduces large overheads compared to accessing local resources. To facilitate I/O disaggregation and device sharing among hosts connected using Peripheral Component Interconnect Express (PCIe) non-transparent bridges, we present SmartIO. NVMes, GPUs, network adapters, or any other standard PCIe device may be borrowed and accessed directly, as if they were local to the remote machines. We provide capabilities beyond existing disaggregation solutions by combining traditional I/O with distributed shared-memory functionality, allowing devices to become part of the same global address space as cluster applications. Software is entirely removed from the data path, and simultaneous sharing of a device among application processes running on remote hosts is enabled. Our experimental results show that I/O devices can be shared with remote hosts, achieving native PCIe performance. Thus, compared to existing device distribution mechanisms, SmartIO provides more efficient, low-cost resource sharing, increasing the overall system performance.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3462545',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Computing Utilization Enhancement for Chiplet-based Homogeneous Processing-in-Memory Deep Learning Processors',\n",
       "  'authors': \"['Bo Jiao', 'Haozhe Zhu', 'Jinshan Zhang', 'Shunli Wang', 'Xiaoyang Kang', 'Lihua Zhang', 'Mingyu Wang', 'Chixiao Chen']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"GLSVLSI '21: Proceedings of the 2021 on Great Lakes Symposium on VLSI\",\n",
       "  'abstract': 'This paper presents a design strategy of chiplet-based processing-in-memory systems for deep neural network applications. Monolithic silicon chips are area and power limited, failing to catch the recent rapid growth of deep learning algorithms. The paper first demonstrates a straightforward layer-wise method that partitions the workload of a monolithic accelerator to a multi-chiplet pipeline. A quantitative analysis shows that the straightforward separation degrades the overall utilization of computing resources due to the reduced on-chiplet memory size, thus introducing a higher memory wall. A tile interleaving strategy is proposed to overcome such degradation. This strategy can segment one layer to different chiplets which maximizes the computing utilization. To facilitate the strategy, the modification of the chiplet system hardware is also discussed. To validate the proposed strategy, a nine-chiplet processing-in-memory system is evaluated with a custom-designed object detection network. Each chiplet can achieve a peak performance of 204.8GOPS at a 100-MHz rate. The peak performance of the overall system is 1.711TOPS, where no off-chip memory access is needed. By the tile interleaving strategy, the utilization is improved from 53.9 to 92.8',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3453688.3461499',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Design and evaluation frameworks for advanced RISC-based ternary processor',\n",
       "  'authors': \"['Dongyun Kam', 'Jung Gyu Min', 'Jongho Yoon', 'Sunmean Kim', 'Seokhyeong Kang', 'Youngjoo Lee']\",\n",
       "  'date': 'March 2022',\n",
       "  'source': \"DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe\",\n",
       "  'abstract': 'In this paper, we introduce the design and verification frameworks for developing a fully-functional emerging ternary processor. Based on the existing compiling environments for binary processors, for the given ternary instructions, the software-level framework provides an efficient way to convert the given programs to the ternary assembly codes. We also present a hardware-level framework to rapidly evaluate the performance of a ternary processor implemented in arbitrary design technology. As a case study, the fully-functional 9-trit advanced RISC-based ternary (ART-9) core is newly developed by using the proposed frameworks. Utilizing 24 custom ternary instructions, the 5-stage ART-9 prototype architecture is successfully verified by a number of test programs including dhrystone benchmark in a ternary domain, achieving the processing efficiency of 57.8 DMIPS/W and 3.06×106 DMIPS/W in the FPGA-level ternary-logic emulations and the emerging CNTFET ternary gates, respectively.',\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3539845.3540092',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'VELTAIR: towards high-performance multi-tenant deep learning services via adaptive compilation and scheduling',\n",
       "  'authors': \"['Zihan Liu', 'Jingwen Leng', 'Zhihui Zhang', 'Quan Chen', 'Chao Li', 'Minyi Guo']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'Deep learning (DL) models have achieved great success in many application domains. As such, many industrial companies such as Google and Facebook have acknowledged the importance of multi-tenant DL services. Although the multi-tenant service has been studied in conventional workloads, it is not been deeply studied on deep learning service, especially on general-purpose hardware.  In this work, we systematically analyze the opportunities and challenges of providing multi-tenant deep learning services on the general-purpose CPU architecture from the aspects of scheduling granularity and code generation. We propose an adaptive granularity scheduling scheme to both guarantee resource usage efficiency and reduce the scheduling conflict rate. We also propose an adaptive compilation strategy, by which we can dynamically and intelligently pick a program with proper exclusive and shared resource usage to reduce overall interference-induced performance loss. Compared to the existing works, our design can serve more requests under the same QoS target in various scenarios (e.g., +71%, +62%, +45% for light, medium, and heavy workloads, respectively), and reduce the averaged query latency by 50%.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503222.3507752',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Supporting legacy libraries on non-volatile memory: a user-transparent approach',\n",
       "  'authors': \"['Chencheng Ye', 'Yuanchao Xu', 'Xipeng Shen', 'Xiaofei Liao', 'Hai Jin', 'Yan Solihin']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': 'As mainstream computing is poised to embrace the advent of byte-addressable non-volatile memory (NVM), an important roadblock has remained largely unnoticed, support of legacy libraries on NVM. Libraries underpin modern software everywhere. As current NVM programming interfaces all designate special types and constructs for NVM objects and references, legacy libraries, being incompatible with these data types, will face major obstacles for working with future applications written for NVM. This paper introduces a simple approach to mitigating the issue. The novel approach centers around user-transparent persistent reference, a new concept that allows programmers to reference a persistent object in the same way as reference a normal (volatile) object. The paper presents the implementation of the concept, carefully examines its soundness, and describes compiler and simple architecture support for keeping performance overheads very low.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00042',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Toward Evaluating High-Level Synthesis Portability and Performance between Intel and Xilinx FPGAs',\n",
       "  'authors': \"['Anthony M Cabrera', 'Aaron R Young', 'Jacob Lambert', 'Zhili Xiao', 'Amy An', 'Seyong Lee', 'Zheming Jin', 'Jungwon Kim', 'Jeremy Buhler', 'Roger D Chamberlain', 'Jeffrey S Vetter']\",\n",
       "  'date': 'April 2021',\n",
       "  'source': \"IWOCL'21: International Workshop on OpenCL\",\n",
       "  'abstract': 'Offloading computation from a CPU to a hardware accelerator is becoming a more common solution for improving performance because traditional gains enabled by Moore’s law and Dennard scaling have slowed. GPUs are often used as hardware accelerators, but field-programmable gate arrays (FPGAs) are gaining traction. FPGAs are beneficial because they allow hardware specific to a particular application to be created. However, they are notoriously difficult to program. To this end, two of the main FPGA manufacturers, Intel and Xilinx, have created tools and frameworks that enable the use of higher level languages to design FPGA hardware. Although Xilinx kernels can be designed by using C/C++, both Intel and Xilinx support the use of OpenCL C to architect FPGA hardware. However, not much is known about the portability and performance between these two device families other than the fact that it is theoretically possible to synthesize a kernel meant for Intel to Xilinx and vice versa.  In this work, we evaluate the portability and performance of Intel and Xilinx kernels. We use OpenCL C implementations of a subset of the Rodinia benchmarking suite that were designed for an Intel FPGA and make the necessary modifications to create synthesizable OpenCL C kernels for a Xilinx FPGA. We find that the difficulty of porting certain kernel optimizations varies, depending on the construct. Once the minimum amount of modifications is made to create synthesizable hardware for the Xilinx platform, more nontrivial work is needed to improve performance. However, we find that constructs that are known to be performant for an FPGA should improve performance regardless of the platform; the difficulty comes in deciding how to invoke certain kernel optimizations while also abiding by the constraints enforced by a given platform’s hardware compiler.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3456669.3456699',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Bitwise Neural Network Acceleration Using Silicon Photonics',\n",
       "  'authors': \"['Kyle Shiflett', 'Avinash Karanth', 'Ahmed Louri', 'Razvan Bunescu']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"GLSVLSI '21: Proceedings of the 2021 on Great Lakes Symposium on VLSI\",\n",
       "  'abstract': 'Hardware accelerators provide significant speedup and improve energy efficiency for several demanding deep neural network (DNN) applications. DNNs have several hidden layers that perform concurrent matrix-vector multiplications (MVMs) between the network weights and input features. As MVMs are critical to the performance of DNNs, previous research has optimized the performance and energy efficiency of MVMs at both the architecture and algorithm levels. In this paper, we propose to use emerging silicon photonics technology to improve parallelism, speed and overall efficiency with the goal of providing real-time inference and fast training of neural nets. We use microring resonators (MRRs) and Mach-Zehnder interferometers (MZIs) to design two versions (all-optical and partial-optical) of hybrid matrix multiplications for DNNs. Our results indicate that our partial optical design gave the best performance in both energy efficiency and latency, with a reduction of 33.1% for energy-delay product (EDP) with conservative estimates and a 76.4% reduction for EDP with aggressive estimates.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3453688.3461515',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A Case For Intra-rack Resource Disaggregation in HPC',\n",
       "  'authors': \"['George Michelogiannakis', 'Benjamin Klenk', 'Brandon Cook', 'Min Yee Teh', 'Madeleine Glick', 'Larry Dennison', 'Keren Bergman', 'John Shalf']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Architecture and Code Optimization',\n",
       "  'abstract': 'The expected halt of traditional technology scaling is motivating increased heterogeneity in high-performance computing (HPC) systems with the emergence of numerous specialized accelerators. As heterogeneity increases, so does the risk of underutilizing expensive hardware resources if we preserve today’s rigid node configuration and reservation strategies. This has sparked interest in resource disaggregation to enable finer-grain allocation of hardware resources to applications. However, there is currently no data-driven study of what range of disaggregation is appropriate in HPC. To that end, we perform a detailed analysis of key metrics sampled in NERSC’s Cori, a production HPC system that executes a diverse open-science HPC workload. In addition, we profile a variety of deep-learning applications to represent an emerging workload. We show that for a rack (cabinet) configuration and applications similar to Cori, a central processing unit with intra-rack disaggregation has a 99.5% probability to find all resources it requires inside its rack. In addition, ideal intra-rack resource disaggregation in Cori could reduce memory and NIC resources by 5.36% to 69.01% and still satisfy the worst-case average rack utilization.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3514245',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'TaskStream: accelerating task-parallel workloads by recovering program structure',\n",
       "  'authors': \"['Vidushi Dadu', 'Tony Nowatzki']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'Reconfigurable accelerators, like CGRAs and dataflow architectures, have come to prominence for addressing data-processing problems. However, they are largely limited to workloads with regular parallelism, precluding their applicability to prevalent task-parallel workloads. Reconfigurable architectures and task parallelism seem to be at odds, as the former requires repetitive and simple program structure, and the latter breaks program structure to create small, individually scheduled program units.   Our insight is that if tasks and their potential for communication structure are first-class primitives in the hardware, it is possible to recover program structure with extremely low overhead. We propose a task execution model for accelerators called TaskStream, which annotates task dependences with information sufficient to recover inter-task structure. TaskStream enables work-aware load balancing, recovery of pipelined inter-task dependences, and recovery of inter-task read sharing through multicasting.   We apply TaskStream to a reconfigurable dataflow architecture, creating a seamless hierarchical dataflow model for task-parallel workloads. We compare our accelerator, Delta, with an equivalent static-parallel design. Overall, we find that our execution model can improve performance by 2.2× with only 3.6% area overhead, while alleviating the programming burden of managing task distribution.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503222.3507706',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Unlimited vector extension with data streaming support',\n",
       "  'authors': \"['Joao Mario Domingos', 'Nuno Neves', 'Nuno Roma', 'Pedro Tomás']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': 'Unlimited vector extension (UVE) is a novel instruction set architecture extension that takes streaming and SIMD processing together into the modern computing scenario. It aims to overcome the shortcomings of state-of-the-art scalable vector extensions by adding data streaming as a way to simultaneously reduce the overheads associated with loop control and memory access indexing, as well as with memory access latency. This is achieved through a new set of instructions that pre-configure the loop memory access patterns. These attain accurate and timely data prefetching on predictable access patterns, such as in multidimensional arrays or in indirect memory access patterns. Each of the configured data streams is associated to a general-purpose vector register, which is then used to interface with the streams. In particular, iterating over a given stream is simply achieved by reading/writing to the corresponding input/output stream, as the data is instantly consumed/produced. To evaluate the proposed UVE, a proof-of-concept gem5 implementation was integrated in an out-of-order processor model, based on the ARM Cortex-A76, thus taking into consideration the typical speculative and out-of-order execution paradigms found in high-performance computing processors. The evaluation was carried out with a set of representative kernels, by assessing the number of executed instructions, its impact on the memory bus and its overall performance. Compared to other state-of-the-art solutions, such as the upcoming ARM Scalable Vector Extension (SVE), the obtained results show that the proposed extension attains average performance speedups over 2.4 x for the same processor configuration, including vector length.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00025',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Memory-Aware Functional IR for Higher-Level Synthesis of Accelerators',\n",
       "  'authors': \"['Christof Schlaak', 'Tzung-Han Juang', 'Christophe Dubach']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Architecture and Code Optimization',\n",
       "  'abstract': 'Specialized accelerators deliver orders of a magnitude of higher performance than general-purpose processors. The ever-changing nature of modern workloads is pushing the adoption of Field Programmable Gate Arrays (FPGAs) as the substrate of choice. However, FPGAs are hard to program directly using Hardware Description Languages (HDLs). Even modern high-level HDLs, e.g., Spatial and Chisel, still require hardware expertise.This article adopts functional programming concepts to provide a hardware-agnostic higher-level programming abstraction. During synthesis, these abstractions are mechanically lowered into a functional Intermediate Representation (IR) that defines a specific hardware design point. This novel IR expresses different forms of parallelism and standard memory features such as asynchronous off-chip memories or synchronous on-chip buffers. Exposing such features at the IR level is essential for achieving high performance.The viability of this approach is demonstrated on two stencil computations and by exploring the optimization space of matrix-matrix multiplication. Starting from a high-level representation for these algorithms, our compiler produces low-level VHSIC Hardware Description Language (VHDL) code automatically. Several design points are evaluated on an Intel Arria\\xa010 FPGA, demonstrating the ability of the IR to exploit different hardware features. This article also shows that the designs produced are competitive with highly tuned OpenCL implementations and outperform hardware-agnostic OpenCL code.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3501768',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A Comprehensive Analysis of Low-Impact Computations in Deep Learning Workloads',\n",
       "  'authors': \"['Hengyi Li', 'Zhichen Wang', 'Xuebin Yue', 'Wenwen Wang', 'Tomiyama Hiroyuki', 'Lin Meng']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"GLSVLSI '21: Proceedings of the 2021 on Great Lakes Symposium on VLSI\",\n",
       "  'abstract': 'Deep Neural Networks (DNNs) have achieved great successes in various machine learning tasks involving a wide range of domains. Though there are multiple hardware platforms available, such as GPUs, CPUs, FPGAs, and etc, CPUs are still preferred choices for machine learning applications, especially in low-power and resource-constrained computation environments such as embedded systems. However, the power and performance efficiency become critical issues in such computation environments when applying DNN techniques. An attractive optimization to DNNs is to remove redundant computations to enhance the execution efficiency. To this end, this paper conducts extensive experiments and analyses on popular state-of-the-art deep learning models. The experimental results include the numbers of instructions, branches, branch prediction misses, cache misses, and etc, during the execution of the models. Besides, we also investigate the performance and sparsity of each layer in the models. Based on the analysis results, this paper also proposes an instruction-level optimization, which achieves the performance improvement ranging from 10.26% to 28.0% for certain convolution layers.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3453688.3461747',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'The deployment of FPGA Based on Network in Ultra-large-scale Data Center',\n",
       "  'authors': \"['Qianqian Zhao', 'Hongwei Kan', 'Yanwei Wang', 'Dongdong Su', 'Kefeng Zhu', 'Le Yang']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"EITCE '21: Proceedings of the 2021 5th International Conference on Electronic Information Technology and Computer Engineering\",\n",
       "  'abstract': 'With the increasing acceptance of cloud computing in various fields, Field Programmable Gate Arrays(FPGA), as one of the core computing power due to its programmable, low power consumption and low characteristics, has been widely deployed in data center. Therefore, how to deploy and use FPGA devices in the cloud computing system of ultra large-scale data center has become a research topic for many units. Traditionally, the deployment of FPGA in the data center is directly inserted in the PCIE slot of server, which belongs to the FPGA-CPU binding method, and The number of FPGA boards supported by single host is limited by the server slot. We propose to change the CPU-FPGA mode by decoupling the FPGA from the CPU and connecting the FPGA to the data center network as an independent resource. This solution solves the scalability problem of FPGA deployment, making it easier for FPGA to deploy and schedule on a large scale. Based on the above FPGA, this paper proposes a solution for large-scale deployment of FPGA devices in kubernetes platform. The scheme adopts container based and kubernetes container deployment management technology. The experimental results proved that the prototype has good throughput, lower communication latency between cards and better scalability compared with the deployment in the way of PCIE.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3501409.3501596',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'PiMulator: a fast and flexible processing-in-memory emulation platform',\n",
       "  'authors': \"['Sergiu Mosanu', 'Mohammad Nazmus Sakib', 'Tommy Tracy', 'Ersin Cukurtas', 'Alif Ahmed', 'Preslav Ivanov', 'Samira Khan', 'Kevin Skadron', 'Mircea Stan']\",\n",
       "  'date': 'March 2022',\n",
       "  'source': \"DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe\",\n",
       "  'abstract': \"Motivated by the memory wall problem, researchers propose many new Processing-in-Memory (PiM) architectures to bring computation closer to data. However, evaluating the performance of these emerging architectures involves using a myriad of tools, including circuit simulators, behavioral RTL or software simulation models, hardware approximations, etc. It is challenging to mimic both software and hardware aspects of a PiM architecture using the currently available tools with high performance and fidelity. Until and unless actual products that include PiM become available, the next best thing is to emulate various hardware PiM solutions on FPGA fabric and boards. This paper presents a modular, parameterizable, FPGA synthesizable soft PiM model suitable for prototyping and rapid evaluation of Processing-in-Memory architectures. The PiM model is implemented in System Verilog and allows users to generate any desired memory configuration on the FPGA fabric with complete control over the structure and distribution of the PiM logic units. Moreover, the model is compatible with the LiteX framework, which provides a high degree of usability and compatibility with the FPGA and RISC-V ecosystem. Thus, the framework enables architects to easily prototype, emulate and evaluate a wide range of emerging PiM architectures and designs. We demonstrate strategies to model several pioneering bitwise-PiM architectures and provide detailed benchmark performance results that demonstrate the platform's ability to facilitate design space exploration. We observe an emulation vs. simulation weighted-average speedup of 28× when running a memory benchmark workload. The model can utilize 100% BRAM and only 1% FF and LUT of an Alveo U280 FPGA board. The project is entirely open-source.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3539845.3540186',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A Unified FPGA Virtualization Framework for General-Purpose Deep Neural Networks in the Cloud',\n",
       "  'authors': \"['Shulin Zeng', 'Guohao Dai', 'Hanbo Sun', 'Jun Liu', 'Shiyao Li', 'Guangjun Ge', 'Kai Zhong', 'Kaiyuan Guo', 'Yu Wang', 'Huazhong Yang']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Reconfigurable Technology and Systems',\n",
       "  'abstract': 'INFerence-as-a-Service (INFaaS) has become a primary workload in the cloud. However, existing FPGA-based Deep Neural Network (DNN) accelerators are mainly optimized for the fastest speed of a single task, while the multi-tenancy of INFaaS has not been explored yet. As the demand for INFaaS keeps growing, simply increasing the number of FPGA-based DNN accelerators is not cost-effective, while merely sharing these single-task optimized DNN accelerators in a time-division multiplexing way could lead to poor isolation and high-performance loss for INFaaS. On the other hand, current cloud-based DNN accelerators have excessive compilation overhead, especially when scaling out to multi-FPGA systems for multi-tenant sharing, leading to unacceptable compilation costs for both offline deployment and online reconfiguration. Therefore, it is far from providing efficient and flexible FPGA virtualization for public and private cloud scenarios.Aiming to solve these problems, we propose a unified virtualization framework for general-purpose deep neural networks in the cloud, enabling multi-tenant sharing for both the Convolution Neural Network (CNN), and the Recurrent Neural Network (RNN) accelerators on a single FPGA. The isolation is enabled by introducing a two-level instruction dispatch module and a multi-core based hardware resources pool. Such designs provide isolated and runtime-programmable hardware resources, which further leads to performance isolation for multi-tenant sharing. On the other hand, to overcome the heavy re-compilation overheads, a tiling-based instruction frame package design and a two-stage static-dynamic compilation, are proposed. Only the lightweight runtime information is re-compiled with ∼1 ms overhead, thus guaranteeing the private cloud’s performance. Finally, the extensive experimental results show that the proposed virtualized solutions achieve up to 3.12× and 6.18× higher throughput in the private cloud compared with the static CNN and RNN baseline designs, respectively.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3480170',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'How to Shrink My FPGAs — Optimizing Tile Interfaces and the Configuration Logic in FABulous FPGA Fabrics',\n",
       "  'authors': \"['King Lok Chung', 'Nguyen Dao', 'Jing Yu', 'Dirk Koch']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"FPGA '22: Proceedings of the 2022 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays\",\n",
       "  'abstract': 'Commercial FPGAs from major vendors are extensively optimized, and fabrics use many hand-crafted custom cells, including switch matrix multiplexers and configuration memory cells. The physical design optimizations commonly improve area, latency (=speed), and power consumption together. This paper is dedicated to improving the physical implementation of FPGA tiles and the configuration storage in SRAM FPGAs. This paper proposes to remap configuration bits and interface wires to implement tightly packed tiles. Using the FABulous FPGA framework, we show that our optimizations are virtually for free but can save over 20% in area and improve latency at the same time. We will evaluate our approach in different scenarios by changing the available metal layers or the requested channel capacity. Our optimizations consider all tiles and we propose a flow that resolves dependencies between the CLBs and other tiles. Moreover, we will show that frame-based reconfiguration is, in almost all cases, better than shift register configuration.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3490422.3502371',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Mining Evidences of Internet of Robotic Things (IoRT) Software from Open Source Projects',\n",
       "  'authors': \"['Michel Albonico', 'Adair Rohling', 'Juliano Santos', 'Paulo Varela']\",\n",
       "  'date': 'September 2021',\n",
       "  'source': \"SBCARS '21: Proceedings of the 15th Brazilian Symposium on Software Components, Architectures, and Reuse\",\n",
       "  'abstract': 'The current world scenario is heading to contactless technologies, where robots are in the center. These systems usually benefit from Internet of Things (IoT) sensing, being named Internet of Robotics Things (IoRT) systems. Developing IoRT software naturally involves high levels of complexity, which may be softened with well-established architectural evidence. In this paper, we aim at mining IoRT software architectural evidence from open source IoRT software repositories. For this, we (i) extract a dataset from GitHub repositories containing real open-source IoRT systems, (ii) mine relevant information from those repositories, (iii) and compile a catalog of architectural software characteristics. The catalog from our study can then be used by practitioners architects.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3483899.3483900',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Athena: high-performance sparse tensor contraction sequence on heterogeneous memory',\n",
       "  'authors': \"['Jiawen Liu', 'Dong Li', 'Roberto Gioiosa', 'Jiajia Li']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ICS '21: Proceedings of the ACM International Conference on Supercomputing\",\n",
       "  'abstract': 'Sparse tensor contraction sequence has been widely employed in many fields, such as chemistry and physics. However, how to efficiently implement the sequence faces multiple challenges, such as redundant computations and memory operations, massive memory consumption, and inefficient utilization of hardware. To address the above challenges, we introduce Athena, a high-performance framework for SpTC sequences. Athena introduces new data structures, leverages emerging Optane-based heterogeneous memory (HM) architecture, and adopts stage parallelism. In particular, Athena introduces shared hash table-represented sparse accumulator to eliminate unnecessary input processing and data migration; Athena uses a novel data-semantic guided dynamic migration solution to make the best use of the Optane-based HM for high performance; Athena also co-runs execution phases with different characteristics to enable high hardware utilization. Evaluating with 12 datasets, we show that Athena brings 327-7362× speedup over the state-of-the-art SpTC algorithm. With the dynamic data placement guided by data semantics, Athena brings performance improvement on Optane-based HM over a state-of-the-art software-based data management solution, a hardware-based data management solution, and PMM-only by 1.58×, 1.82×, and 2.34× respectively. Athena also showcases its effectiveness in quantum chemistry and physics scenarios.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3447818.3460355',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Orchestrated trios: compiling for efficient communication in Quantum programs with 3-Qubit gates',\n",
       "  'authors': \"['Casey Duckering', 'Jonathan M. Baker', 'Andrew Litteken', 'Frederic T. Chong']\",\n",
       "  'date': 'April 2021',\n",
       "  'source': \"ASPLOS '21: Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'Current quantum computers are especially error prone and require high levels of optimization to reduce operation counts and maximize the probability the compiled program will succeed. These computers only support operations decomposed into one- and two-qubit gates and only two-qubit gates between physically connected pairs of qubits. Typical compilers first decompose operations, then route data to connected qubits. We propose a new compiler structure, Orchestrated Trios, that first decomposes to the three-qubit Toffoli, routes the inputs of the higher-level Toffoli operations to groups of nearby qubits, then finishes decomposition to hardware-supported gates.   This significantly reduces communication overhead by giving the routing pass access to the higher-level structure of the circuit instead of discarding it. A second benefit is the ability to now select an architecture-tuned Toffoli decomposition such as the 8-CNOT Toffoli for the specific hardware qubits now known after the routing pass. We perform real experiments on IBM Johannesburg showing an average 35% decrease in two-qubit gate count and 23% increase in success rate of a single Toffoli over Qiskit. We additionally compile many near-term benchmark algorithms showing an average 344% increase in (or 4.44x) simulated success rate on the Johannesburg architecture and compare with other architecture types.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3445814.3446718',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'RealSWATT: Remote Software-based Attestation for Embedded Devices under Realtime Constraints',\n",
       "  'authors': \"['Sebastian Surminski', 'Christian Niesler', 'Ferdinand Brasser', 'Lucas Davi', 'Ahmad-Reza Sadeghi']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"CCS '21: Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security\",\n",
       "  'abstract': \"Smart factories, critical infrastructures, and medical devices largely rely on embedded systems that need to satisfy realtime constraints to complete crucial tasks. Recent studies and reports have revealed that many of these devices suffer from crucial vulnerabilities that can be exploited with fatal consequences. Despite the security and safety-critical role of these devices, they often do not feature state-of-the-art security mechanisms. Moreover, since realtime systems have strict timing requirements, integrating new security mechanisms is not a viable option as they often influence the device's runtime behavior. One solution is to offload security enhancements to a remote instance, the so-called remote attestation. We present RealSWATT, the first software-based remote attestation system for realtime embedded devices. Remote attestation is a powerful security service that allows a party to verify the correct functionality of an untrusted remote device. In contrast to previous remote attestation approaches for realtime systems, RealSWATT does neither require custom hardware extensions nor trusted computing components. It is designed to work within real-world IoT networks, connected through Wi-Fi. RealSWATT leverages a dedicated processor core for remote attestation and provides the required timing guarantees without hardware extensions. We implement RealSWATT on the popular ESP32 microcontroller, and we evaluate it on a real-world medical device with realtime constraints. To demonstrate its applicability, we furthermore integrate RealSWATT into a framework for off-the-shelf IoT devices and apply it to a smart plug, a smoke detector, and a smart light bulb.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3460120.3484788',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Cerebros: Evading the RPC Tax in Datacenters',\n",
       "  'authors': \"['Arash Pourhabibi', 'Mark Sutherland', 'Alexandros Daglis', 'Babak Falsafi']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"MICRO '21: MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture\",\n",
       "  'abstract': 'The emerging paradigm of microservices decomposes online services into fine-grained software modules frequently communicating over the datacenter network, often using Remote Procedure Calls\\xa0(RPCs). Ongoing advancements in the network stack have exposed the RPC layer itself as a bottleneck, that we show accounts for 40–90% of a microservice’s total execution cycles. We break down the underlying modules that comprise production RPC layers and demonstrate, based on prior evidence, that CPUs can only expect limited improvements for such tasks, mandating a shift to hardware to remove the RPC layer as a limiter of microservice performance. Although recently proposed accelerators can efficiently handle a portion of the RPC layer, their overall benefit is limited by unnecessary CPU involvement, which occurs because the accelerators are architected as co-processors under the CPU’s control. Instead, we show that conclusively removing the RPC layer bottleneck requires all of the RPC layer’s modules to be executed by a NIC-attached hardware accelerator. We introduce Cerebros, a dedicated RPC processor that executes the Apache Thrift RPC layer and acts as an intermediary stage between the NIC and the microservice running on the CPU. Our evaluation using the DeathStarBench microservice suite shows that Cerebros reduces the CPU cycles spent in the RPC layer by 37–64 ×, yielding a 1.8–14 × reduction in total cycles expended per microservice request.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3466752.3480055',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'PMEM-spec: persistent memory speculation (strict persistency can trump relaxed persistency)',\n",
       "  'authors': \"['Jungi Jeong', 'Changhee Jung']\",\n",
       "  'date': 'April 2021',\n",
       "  'source': \"ASPLOS '21: Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'Persistency models define the persist-order that controls the order in which stores update persistent memory (PM). As with memory consistency, the relaxed persistency models provide better performance than the strict ones by relaxing the ordering constraints. To support such relaxed persistency models, previous studies resort to APIs for annotating the persist-order in program and hardware implementations for enforcing the programmer-specified order. However, these approaches to supporting relaxed persistency impose costly burdens on both architects and programmers. In light of this, the goal of this study is to demonstrate that the strict persistency model can outperform the relaxed models with significantly less hardware complexity and programming difficulty. To achieve that, this paper presents PMEM-Spec that speculatively allows any PM accesses without stalling or buffering, detecting their ordering violation (e.g., misspeculation for PM loads and stores). PMEM-Spec treats misspeculation as power failure and thus leverages failure-atomic transactions to recover from misspeculation by aborting and restarting them purposely. Since the ordering violation rarely occurs, PMEM-Spec can accelerate persistent memory accesses without significant misspeculation penalty. Experimental results show that PMEM-Spec outperforms two epoch-based persistency models with Intel X86 ISA and the state-of-the-art hardware support by 27.2% and 10.6%, respectively.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3445814.3446698',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'I-GCN: A Graph Convolutional Network Accelerator with Runtime Locality Enhancement through Islandization',\n",
       "  'authors': \"['Tong Geng', 'Chunshu Wu', 'Yongan Zhang', 'Cheng Tan', 'Chenhao Xie', 'Haoran You', 'Martin Herbordt', 'Yingyan Lin', 'Ang Li']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"MICRO '21: MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture\",\n",
       "  'abstract': 'Graph Convolutional Networks (GCNs) have drawn tremendous attention in the past three years. Compared with other deep learning modalities, high-performance hardware acceleration of GCNs is as critical but even more challenging. The hurdles arise from the poor data locality and redundant computation due to the large size, high sparsity, and irregular non-zero distribution of real-world graphs.  In this paper we propose a novel hardware accelerator for GCN inference, called I-GCN, that significantly improves data locality and reduces unnecessary computation. The mechanism is a new online graph restructuring algorithm we refer to as islandization. The proposed algorithm finds clusters of nodes with strong internal but weak external connections. The islandization process yields two major benefits. First, by processing islands rather than individual nodes, there is better on-chip data reuse and fewer off-chip memory accesses. Second, there is less redundant computation as aggregation for common/shared neighbors in an island can be reused. The parallel search, identification, and leverage of graph islands are all handled purely in hardware at runtime working in an incremental pipeline. This is done without any preprocessing of the graph data or adjustment of the GCN model structure. Experimental results show that I-GCN can significantly reduce off-chip accesses and prune 38% of aggregation operations, leading to performance speedups over CPUs, GPUs, the prior art GCN accelerators of 5549 ×, 403 ×, and 5.7 × on average, respectively.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3466752.3480113',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Optimizing Storage Performance with Calibrated Interrupts',\n",
       "  'authors': \"['Amy Tai', 'Igor Smolyar', 'Michael Wei', 'Dan Tsafrir']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Storage',\n",
       "  'abstract': 'After request completion, an I/O device must decide whether to minimize latency by immediately firing an interrupt or to optimize for throughput by delaying the interrupt, anticipating that more requests will complete soon and help amortize the interrupt cost. Devices employ adaptive interrupt coalescing heuristics that try to balance between these opposing goals. Unfortunately, because devices lack the semantic information about which I/O requests are latency-sensitive, these heuristics can sometimes lead to disastrous results.Instead, we propose addressing the root cause of the heuristics problem by allowing software to explicitly specify to the device if submitted requests are latency-sensitive. The device then “calibrates” its interrupts to completions of latency-sensitive requests. We focus on NVMe storage devices and show that it is natural to express these semantics in the kernel and the application and only requires a modest two-bit change to the device interface. Calibrated interrupts increase throughput by up to 35%, reduce CPU consumption by as much as 30%, and achieve up to 37% lower latency when interrupts are coalesced.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3505139',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Ax-BxP: Approximate Blocked Computation for Precision-reconfigurable Deep Neural Network Acceleration',\n",
       "  'authors': \"['Reena Elangovan', 'Shubham Jain', 'Anand Raghunathan']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Design Automation of Electronic Systems',\n",
       "  'abstract': 'Precision scaling has emerged as a popular technique to optimize the compute and storage requirements of Deep Neural Networks (DNNs). Efforts toward creating ultra-low-precision (sub-8-bit) DNNs for efficient inference suggest that the minimum precision required to achieve a given network-level accuracy varies considerably across networks, and even across layers within a network. This translates to a need to support variable precision computation in DNN hardware. Previous proposals for precision-reconfigurable hardware, such as bit-serial architectures, incur high overheads, significantly diminishing the benefits of lower precision. We propose Ax-BxP, a method for approximate blocked computation wherein each multiply-accumulate operation is performed block-wise (a block is a group of bits), facilitating re-configurability at the granularity of blocks. Further, approximations are introduced by only performing a subset of the required block-wise computations to realize precision re-configurability with high efficiency. We design a DNN accelerator that embodies approximate blocked computation and propose a method to determine a suitable approximation configuration for any given DNN. For the AlexNet, ResNet50, and MobileNetV2 DNNs, Ax-BxP achieves improvement in system energy and performance, respectively, over an 8-bit fixed-point (FxP8) baseline, with minimal loss (<1% on average) in classification accuracy. Further, by varying the approximation configurations at a finer granularity across layers and data-structures within a DNN, we achieve improvement in system energy and performance, respectively.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3492733',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Democratizing cellular access with CellBricks',\n",
       "  'authors': \"['Zhihong Luo', 'Silvery Fu', 'Mark Theis', 'Shaddi Hasan', 'Sylvia Ratnasamy', 'Scott Shenker']\",\n",
       "  'date': 'August 2021',\n",
       "  'source': \"SIGCOMM '21: Proceedings of the 2021 ACM SIGCOMM 2021 Conference\",\n",
       "  'abstract': 'Markets in which competition thrives are good for both consumers and innovation but, unfortunately, competition is not thriving in the increasingly important cellular market. We propose CellBricks, a novel cellular architecture that lowers the barrier to entry for new operators by enabling users to consume access on-demand from any available cellular operator — small or large, trusted or untrusted. CellBricks achieves this by moving support for mobility and user management (authentication and billing) out of the network and into end hosts. These changes, we believe, bring valuable benefits beyond enabling competition: they lead to a cellular infrastructure that is simpler and more efficient. We design, build, and evaluate CellBricks, showing that its benefits come at little-to-no cost in performance, with application performance overhead between -1.6% to 3.1% of that achieved by current cellular infrastructure.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3452296.3473336',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Quick-Div: Rethinking Integer Divider Design for FPGA-based Soft-processors',\n",
       "  'authors': \"['Eric Matthews', 'Alec Lu', 'Zhenman Fang', 'Lesley Shannon']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Reconfigurable Technology and Systems',\n",
       "  'abstract': 'In today’s FPGA-based soft-processors, one of the slowest instructions is integer division. Compared to the low single-digit latency of other arithmetic operations, the fixed 32-cycle latency of radix-2 division is substantially longer. Given that today’s soft-processors typically only implement radix-2 division—if they support hardware division at all—there is significant potential to improve the performance of integer dividers.In this work, we present a set of high-performance, data-dependent, variable-latency integer dividers for FPGA-based soft-processors that we call Quick-Div. We compare them to various radix-N dividers and provide a thorough analysis in terms of latency and resource usage. In addition, we analyze the frequency scaling for such divider designs when (1) treated as a stand-alone unit and (2) integrated as part of a high-performance soft-processor. Moreover, we provide additional theoretical analysis of different dividers’ behaviour and develop a new better-performing Quick-Div variant, called Quick-radix-4. Experimental results show that our Quick-radix-4 design can achieve up to 6.8× better performance and 6.1× better performance-per-LUT over the radix-2 divider for applications such as random number generation. Even in cases where division operations constitute as little as 1% of all executed instructions, Quick-radix-4 provides a performance uplift of 16% compared to the radix-2 divider.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3502492',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A Cost-Efficient Platform Design for Distributed UAV Swarm Research',\n",
       "  'authors': \"['Zhongxuan Cai', 'Xuefeng Chang', 'Minglong Li']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"AISS '21: Proceedings of the 3rd International Conference on Advanced Information Science and System\",\n",
       "  'abstract': 'Unmanned Aerial Vehicles (UAVs) have been attracting more and more attention in research and education. Specifically, Swarm intelligence is a promising future technology of UAVs and the frontier of multi-agent system research. It has the characteristics of low individual cost, strong system flexibility and robustness, and has great potential in many tasks. However, due to the constraints of research conditions and cost, most of the current researches on large-scale swarm UAVs are carried out in the simulation environment. Building a low-cost open-source software and hardware platform for swarm UAVs is an important basis for promoting researches on swarm UAVs and multi-agent systems. In this paper, we propose a design of a UAV platform with common cost-efficient hardware and a rich open-source software ecosystem, and provide a software solution for swarm robots based on the open-source robot operating system ROS. These software packages support the rapid programming development of swarm behaviors and different communication topology. Experiments have been conducted for typical UAV tasks like flocking and formation, indicating the effectiveness of the proposed platform.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503047.3503070',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Intersection Prediction for Accelerated GPU Ray Tracing',\n",
       "  'authors': \"['Lufei Liu', 'Wesley Chang', 'Francois Demoullin', 'Yuan Hsi Chou', 'Mohammadreza Saed', 'David Pankratz', 'Tyler Nowicki', 'Tor M. Aamodt']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"MICRO '21: MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture\",\n",
       "  'abstract': 'Ray tracing has been used for years in motion picture to generate photorealistic images while faster raster-based shading techniques have been preferred for video games to meet real-time requirements. However, recent Graphics Processing Units (GPUs) incorporate hardware accelerator units designed for ray tracing. These accelerator units target the process of traversing hierarchical tree data structures used to test for ray-object intersections. Distinct rays following similar paths through these structures execute many redundant ray-box intersection tests. We propose a ray intersection predictor that speculatively elides redundant operations during this process and proceeds directly to test primitives that the ray is likely to intersect. A key aspect of our predictor strategy involves identifying hash functions that preserve enough spatial information to identify redundant traversals. We explore how to integrate our ray prediction strategy into existing GPU pipelines along with improving the predictor effectiveness by predicting nodes higher in the tree as well as regrouping and scheduling traversal operations in a low cost, judicious manner. On a mobile class GPU with a ray tracing accelerator unit, we find the addition of a 5.5KB predictor per streaming multiprocessor improves performance for ambient occlusion workloads by a geometric mean of 26%.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3466752.3480097',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Machine Learning–enabled Scalable Performance Prediction of Scientific Codes',\n",
       "  'authors': \"['Gopinath Chennupati', 'Nandakishore Santhi', 'Phill Romero', 'Stephan Eidenbenz']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Modeling and Computer Simulation',\n",
       "  'abstract': 'Hardware architectures become increasingly complex as the compute capabilities grow to exascale. We present the Analytical Memory Model with Pipelines (AMMP) of the Performance Prediction Toolkit (PPT). PPT-AMMP takes high-level source code and hardware architecture parameters as input and predicts runtime of that code on the target hardware platform, which is defined in the input parameters. PPT-AMMP transforms the code to an (architecture-independent) intermediate representation, then (i) analyzes the basic block structure of the code, (ii) processes architecture-independent virtual memory access patterns that it uses to build memory reuse distance distribution models for each basic block, and (iii) runs detailed basic-block level simulations to determine hardware pipeline usage.PPT-AMMP uses machine learning and regression techniques to build the prediction models based on small instances of the input code, then integrates into a higher-order discrete-event simulation model of PPT running on Simian PDES engine. We validate PPT-AMMP on four standard computational physics benchmarks and present a use case of hardware parameter sensitivity analysis to identify bottleneck hardware resources on different code inputs. We further extend PPT-AMMP to predict the performance of a scientific application code, namely, the radiation transport mini-app SNAP. To this end, we analyze multi-variate regression models that accurately predict the reuse profiles and the basic block counts. We validate predicted SNAP runtimes against actual measured times.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3450264',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A spiking network model for semantic representation and replay-based association acquisition',\n",
       "  'authors': \"['Brian Robinson', 'Adam Polevoy', 'Sean McDaniel', 'Will Coon', 'Clara Scholl', 'Mark McLean', 'Erik Johnson']\",\n",
       "  'date': 'July 2021',\n",
       "  'source': 'ICONS 2021: International Conference on Neuromorphic Systems 2021',\n",
       "  'abstract': 'The ability to form and store several types of associations between representations of natural images is an area of ongoing research in artificial deep neural networks, which may be informed by biologically-inspired computational models. It is hypothesized that replay of sensory stimuli through cortical-hippocampal connections is responsible for training associations between events, as a powerful form of associative learning. While models of associative memories and sensory processing have been studied extensively, there is a potential for spiking models encompassing sensory processing, reasoning over associations, and learning representations which has not been previously demonstrated. Such networks would be suitable for reasoning and learning from visual data on neuromorphic hardware. In this work, we demonstrate a novel visual reasoning network capable of representing semantic relationships and learning new associations through replay-based association with spiking models using natural images. This is demonstrated through associations of natural images from Tiny Imagenet with a knowledge graph derived from WordNet, and we show that relations in the knowledge graph can be accurately traversed for multiple sequential queries. We also demonstrate learning of a novel association after replayed presentations of natural images. This represents a novel capability for machine learning and reasoning with spiking neural networks which may be amenable to neuromorphic hardware.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3477145.3477259',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'NumaPerf: predictive NUMA profiling',\n",
       "  'authors': \"['Xin Zhao', 'Jin Zhou', 'Hui Guan', 'Wei Wang', 'Xu Liu', 'Tongping Liu']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ICS '21: Proceedings of the ACM International Conference on Supercomputing\",\n",
       "  'abstract': 'It is extremely challenging to achieve optimal performance of parallel applications on a NUMA architecture, which necessitates the assistance of profiling tools. However, existing NUMA-profiling tools share some similar shortcomings, such as portability, effectiveness, and helpfulness issues. This paper proposes a novel profiling tool–NumaPerf–that overcomes these issues. NumaPerf aims to identify potential performance issues for any NUMA architecture, instead of only on the current hardware. To achieve this, NumaPerf focuses on memory sharing patterns between threads, instead of real remote accesses. NumaPerf further detects potential thread migrations and load imbalance issues that could significantly affect the performance but are omitted by existing profilers. NumaPerf also identifies cache coherence issues separately that may require different fix strategies. Based on our extensive evaluation, NumaPerf can identify more performance issues than any existing tool, while fixing them leads to significant performance speedup.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3447818.3460361',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'ArkDB: A Key-Value Engine for Scalable Cloud Storage Services',\n",
       "  'authors': \"['Zhu Pang', 'Qingda Lu', 'Shuo Chen', 'Rui Wang', 'Yikang Xu', 'Jiesheng Wu']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data\",\n",
       "  'abstract': \"Persistent key-value stores play a crucial role in enabling internet-scale services. At Alibaba Cloud, scale-out cloud storage services including Object Storage Service, File Storage Service and Tablestore are built on distributed key-value stores. Key challenges in the design of the underlying key-value engine for these services lie in utilization of disaggregated storage, supporting write and range query-heavy workloads, and balancing of scalability, availability and resource usage. This paper presents ArkDB, a key-value engine designed to address these challenges by combining advantages of both LSM tree and Bw-tree, and leveraging advances in hardware technologies. Built on top of Pangu, an append-only distributed file system, ArkDB's innovations include shrinkable page mapping table, clear separation of system and user states for fast recovery, write amplification reduction, efficient garbage collection and lightweight partition split and merge. Experimental results demonstrate ArkDB's improvements over existing designs. Compared with Bw-tree, ArkDB efficiently stabilizes the mapping table size despite continuous write working set growth. Compared with RocksDB, an LSM tree-based key-value engine, ArkDB increases ingestion throughput by 2.16x, while reducing write amplification by 3.1x. It outperforms RocksDB by 52% and 37% respectively on a write-heavy workload and a range query-intensive workload of the Yahoo! Cloud Serving Benchmark. Experiments running in Tablestore in a cluster environment further demonstrate ArkDB's performance on Pangu and its efficient partition split/merge support.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3448016.3457553',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'RAP-NoC: Reliability Assessment of Photonic Network-on-Chips, A simulator',\n",
       "  'authors': \"['Meisam Abdollahi', 'Mohammad Baharloo', 'Fateme Shokouhinia', 'Masoumeh Ebrahimi']\",\n",
       "  'date': 'September 2021',\n",
       "  'source': \"NANOCOM '21: Proceedings of the Eight Annual ACM International Conference on Nanoscale Computing and Communication\",\n",
       "  'abstract': 'Nowadays, optical network-on-chip is accepted as a promising alternative solution for traditional electrical interconnects due to lower transmission delay and power consumption as well as considerable high data bandwidth. However, silicon photonics struggles with some particular challenges that threaten the reliability of the data transmission process. The most important challenges can be considered as temperature fluctuation, process variation, aging, crosstalk noise, and insertion loss. Although several attempts have been made to investigate the effect of these issues on the reliability of optical network-on-chip, none of them modeled the reliability of photonic network-on-chip in a system-level approach based on basic element failure rate. In this paper, an analytical model-based simulator, called Reliability Assessment of Photonic Network-on-Chips (RAP-NoC), is proposed to evaluate the reliability of different 2D optical network-on-chip architectures and data traffic. The experimental results show that, in general, Mesh topology is more reliable than Torus considering the same size. Increasing the reliability of Microring Resonator (MR) has a more significant impact on the reliability of an optical router rather than a network.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3477206.3477455',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Architecting for Artificial Intelligence with Emerging Nanotechnology',\n",
       "  'authors': \"['Sourabh Kulkarni', 'Sachin Bhat', 'Csaba Andras Moritz']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Journal on Emerging Technologies in Computing Systems',\n",
       "  'abstract': 'Artificial Intelligence is becoming ubiquitous in products and services that we use daily. Although the domain of AI has seen substantial improvements over recent years, its effectiveness is limited by the capabilities of current computing technology. Recently, there have been several architectural innovations for AI using emerging nanotechnology. These architectures implement mathematical computations of AI with circuits that utilize physical behavior of nanodevices purpose-built for such computations. This approach leads to a much greater efficiency vs. software algorithms running on von Neumann processors or CMOS architectures, which emulate the operations with transistor circuits. In this article, we provide a comprehensive survey of these architectural directions and categorize them based on their contributions. Furthermore, we discuss the potential offered by these directions with real-world examples. We also discuss major challenges and opportunities in this field.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3445977',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'EdgeBERT: Sentence-Level Energy Optimizations for Latency-Aware Multi-Task NLP Inference',\n",
       "  'authors': \"['Thierry Tambe', 'Coleman Hooper', 'Lillian Pentecost', 'Tianyu Jia', 'En-Yu Yang', 'Marco Donato', 'Victor Sanh', 'Paul Whatmough', 'Alexander M. Rush', 'David Brooks', 'Gu-Yeon Wei']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"MICRO '21: MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture\",\n",
       "  'abstract': 'Transformer-based language models such as BERT provide significant accuracy improvement to a multitude of natural language processing (NLP) tasks. However, their hefty computational and memory demands make them challenging to deploy to resource-constrained edge platforms with strict latency requirements.  We present EdgeBERT, an in-depth algorithm-hardware co-design for latency-aware energy optimizations for multi-task NLP. EdgeBERT employs entropy-based early exit predication in order to perform dynamic voltage-frequency scaling (DVFS), at a sentence granularity, for minimal energy consumption while adhering to a prescribed target latency. Computation and memory footprint overheads are further alleviated by employing a calibrated combination of adaptive attention span, selective network pruning, and floating-point quantization.  Furthermore, in order to maximize the synergistic benefits of these algorithms in always-on and intermediate edge computing settings, we specialize a 12nm scalable hardware accelerator system, integrating a fast-switching low-dropout voltage regulator (LDO), an all-digital phase-locked loop (ADPLL), as well as, high-density embedded non-volatile memories (eNVMs) wherein the sparse floating-point bit encodings of the shared multi-task parameters are carefully stored. Altogether, latency-aware multi-task NLP inference acceleration on the EdgeBERT hardware system generates up to 7 ×, 2.5 ×, and 53 × lower energy compared to the conventional inference without early stopping, the latency-unbounded early exit approach, and CUDA adaptations on an Nvidia Jetson Tegra X2 mobile GPU, respectively.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3466752.3480095',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'DOSAGE: generating domain-specific accelerators for resource-constrained computing',\n",
       "  'authors': \"['Ankur Limaye', 'Tosiron Adegbija']\",\n",
       "  'date': 'July 2021',\n",
       "  'source': \"ISLPED '21: Proceedings of the ACM/IEEE International Symposium on Low Power Electronics and Design\",\n",
       "  'abstract': \"Integrating low-overhead domain-specific accelerators with low-energy general-purpose processors can improve the processors' performance efficiency in resource-constrained systems (e.g., embedded systems). However, current function-based approaches for designing domain-specific accelerators require substantial programmer efforts for hardware/software partitioning and program modifications to access the best available hardware accelerators. This paper presents DOSAGE, an LLVM compiler-based methodology to generate domain-specific accelerators for resource-constrained computing systems. Given a set of applications, DOSAGE automatically identifies and ranks the recurrent and similar code blocks that would benefit the most from hardware acceleration, based on the code blocks' composition. We illustrate the benefits of the proposed approach using a case study that involves generating domain-specific accelerators for a diverse set of healthcare applications and evaluate the accelerators via FPGA-based prototyping. Compared to a base low-resource RISC-V processor, DOSAGE accelerators improved the system's performance and energy by 24.85% and 8.54%, respectively. Furthermore, compared to a state-of-the-art function-based accelerator generation approach, DOSAGE eliminated the function-level granularity constraint of the generation process and reduced the number of required accelerators---and, in effect, the interfacing overhead---by 33.33%, while achieving equal or better program coverage and performance/energy results.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISLPED52811.2021.9502501',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Towards approximate computing for achieving energy vs. accuracy trade-offs',\n",
       "  'authors': \"['Aleksandr Ometov', 'Jari Nurmi']\",\n",
       "  'date': 'March 2022',\n",
       "  'source': \"DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe\",\n",
       "  'abstract': 'Despite the recent advances in semiconductor technology and energy-aware system design, the overall energy consumption of computing and communication systems is rapidly growing. On the one hand, the pervasiveness of these technologies everywhere in the form of mobile devices, cyber-physical embedded systems, sensor networks, wearables, social media and context-awareness, intelligent machines, broadband cellular networks, Cloud computing, and Internet of Things (IoT) has drastically increased the demand for computing and communications. On the other hand, the user expectations on features and battery life of online devices are increasing all the time, and it creates another incentive for finding good trade-offs between performance and energy consumption. One of the opportunities to address this growing demand is to utilize an Approximate Computing approach through software and hardware design. The APROPOS project aims at finding the balance between accuracy and energy consumption, and this short paper provides an initial overview of the corresponding roadmap, as the project is still in the initial stage.',\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3539845.3540001',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'LADDER: Architecting Content and Location-aware Writes for Crossbar Resistive Memories',\n",
       "  'authors': \"['Md Hafizul Islam Chowdhuryy', 'Muhammad Rashedul Haq Rashed', 'Amro Awad', 'Rickard Ewetz', 'Fan Yao']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"MICRO '21: MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture\",\n",
       "  'abstract': 'Resistive memories (ReRAM) organized in the form of crossbars are promising for main memory integration. While offering high cell density, crossbar-based ReRAMs suffer from variable write latency requirement for RESET operations due to the varying impact of IR drop, which jointly depends on the data pattern of the crossbar and the location of target cells being RESET. The exacerbated worst-case RESET latencies can significantly limit system performance.  In this paper, we propose LADDER, an effective and low-cost processor-side framework that performs writes with variable latency by exploiting both content and location dependencies. To enable content awareness, LADDER incorporates a novel scheme that maintains metadata for per-row data pattern (i.e., number of 1’s) in memory, and performs efficient metadata management and caching through the memory controller. LADDER does not require hardware changes to the ReRAM chip. We design several optimizations that further boost the performance of LADDER, including LRS-metadata estimation that eliminates stale memory block reads, intra-line bit-level shifting that reduces the worst-case LRS-counter values and multi-granularity LRS-metadata design that optimizes the number of counters to maintain. We evaluate the efficacy of LADDER using 16 single- and multi-programmed workloads. Our results show that LADDER exhibits on average 46% performance improvement as compared to a baseline scheme and up to 33% over state-of-the-art designs. Furthermore, LADDER achieves 28.8% average dynamic memory energy saving compared to the existing architecture schemes and has less than 3% impact on device lifetime.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3466752.3480054',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Flynn’s Reconciliation: Automating the Register Cache Idiom for Cross-accelerator Programming',\n",
       "  'authors': \"['Daniel Thuerck', 'Nicolas Weber', 'Roberto Bifulco']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Architecture and Code Optimization',\n",
       "  'abstract': 'A large portion of the recent performance increase in the High Performance Computing (HPC) and Machine Learning (ML) domains is fueled by accelerator cards. Many popular ML frameworks support accelerators by organizing computations as a computational graph over a set of highly optimized, batched general-purpose kernels. While this approach simplifies the kernels’ implementation for each individual accelerator, the increasing heterogeneity among accelerator architectures for HPC complicates the creation of portable and extensible libraries of such kernels. Therefore, using a generalization of the CUDA community’s warp register cache programming idiom, we propose a new programming idiom (CoRe) and a virtual architecture model (PIRCH), abstracting over SIMD and SIMT paradigms. We define and automate the mapping process from a single source to PIRCH’s intermediate representation and develop backends that issue code for three different architectures: Intel AVX512, NVIDIA GPUs, and NEC SX-Aurora. Code generated by our source-to-source compiler for batched kernels, borG, competes favorably with vendor-tuned libraries and is up to 2× faster than hand-tuned kernels across architectures.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3458357',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Byte-Select Compression',\n",
       "  'authors': \"['Matthew Tomei', 'Shomit Das', 'Mohammad Seyedzadeh', 'Philip Bedoukian', 'Bradford Beckmann', 'Rakesh Kumar', 'David Wood']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Architecture and Code Optimization',\n",
       "  'abstract': 'Cache-block compression is a highly effective technique for both reducing accesses to lower levels in the memory hierarchy (cache compression) and minimizing data transfers (link compression). While many effective cache-block compression algorithms have been proposed, the design of these algorithms is largely ad hoc and manual and relies on human recognition of patterns. In this article, we take an entirely different approach. We introduce a class of “byte-select” compression algorithms, as well as an automated methodology for generating compression algorithms in this class. We argue that, based on upper bounds within the class, the study of this class of byte-select algorithms has potential to yield algorithms with better performance than existing cache-block compression algorithms. The upper bound we establish on the compression ratio is 2X that of any existing algorithm. We then offer a generalized representation of a subset of byte-select compression algorithms and search through the resulting space guided by a set of training data traces. Using this automated process, we find efficient and effective algorithms for various hardware applications. We find that the resulting algorithms exploit novel patterns that can inform future algorithm designs. The generated byte-select algorithms are evaluated against a separate set of traces and evaluations show that Byte-Select has a 23% higher compression ratio on average. While no previous algorithm performs best for all our data sets which include CPU and GPU applications, our generated algorithms do. Using an automated hardware generator for these algorithms, we show that their decompression and compression latency is one and two cycles respectively, much lower than any existing algorithm with a competitive compression ratio.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3462209',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Quingo: A Programming Framework for Heterogeneous Quantum-Classical Computing with NISQ Features',\n",
       "  'authors': \"['X. Fu', 'Jintao Yu', 'Xing Su', 'Hanru Jiang', 'Hua Wu', 'Fucheng Cheng', 'Xi Deng', 'Jinrong Zhang', 'Lei Jin', 'Yihang Yang', 'Le Xu', 'Chunchao Hu', 'Anqi Huang', 'Guangyao Huang', 'Xiaogang Qiang', 'Mingtang Deng', 'Ping Xu', 'Weixia Xu', 'Wanwei Liu', 'Yu Zhang', 'Yuxin Deng', 'Junjie Wu', 'Yuan Feng']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Quantum Computing',\n",
       "  'abstract': 'The increasing control complexity of Noisy Intermediate-Scale Quantum (NISQ) systems underlines the necessity of integrating quantum hardware with quantum software. While mapping heterogeneous quantum-classical computing (HQCC) algorithms to NISQ hardware for execution, we observed a few dissatisfactions in quantum programming languages (QPLs), including difficult mapping to hardware, limited expressiveness, and counter-intuitive code. In addition, noisy qubits require repeatedly performed quantum experiments, which explicitly operate low-level configurations, such as pulses and timing of operations. This requirement is beyond the scope or capability of most existing QPLs.We summarize three execution models to depict the quantum-classical interaction of existing QPLs. Based on the refined HQCC model, we propose the Quingo framework to integrate and manage quantum-classical software and hardware to provide the programmability over HQCC applications and map them to NISQ hardware. We propose a six-phase quantum program life-cycle model matching the refined HQCC model, which is implemented by a runtime system. We also propose the Quingo programming language, an external domain-specific language highlighting timer-based timing control and opaque operation definition, which can be used to describe quantum experiments. We believe the Quingo framework could contribute to the clarification of key techniques in the design of future HQCC systems.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3483528',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'OAuth 2.0-based authentication solution for FPGA-enabled cloud computing',\n",
       "  'authors': \"['Semih Ince', 'David Espes', 'Guy Gogniat', 'Julien Lallet', 'Renaud Santoro']\",\n",
       "  'date': 'December 2021',\n",
       "  'source': \"UCC '21: Proceedings of the 14th IEEE/ACM International Conference on Utility and Cloud Computing Companion\",\n",
       "  'abstract': 'FPGA-enabled cloud computing is getting more and more common as cloud providers offer hardware accelerated solutions. In this context, clients need confidential remote computing. However Intellectual Properties and data are being used and communicated. So current security models require the client to trust the cloud provider blindly by disclosing sensitive information. In addition, the lack of strong authentication and access control mechanisms, for both the client and the provided FPGA in current solutions, is a major security drawback. To enhance security measures and privacy between the client, the cloud provider and the FPGA, an additional entity needs to be introduced: the trusted authority. Its role is to authenticate the client-FPGA pair and isolate them from the cloud provider. With our novel OAuth 2.0-based access delegation solution for FPGA-accelerated clouds, a remote confidential FPGA environment with a token-based access can be created for the client. Our solution allows to manage and securely allocate heterogeneous resource pools with enhanced privacy & confidentiality for the client. Our formal analysis shows that our protocol adds a very small latency which is suitable for real-time application.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3492323.3495635',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': \"One Glitch to Rule Them All: Fault Injection Attacks Against AMD's Secure Encrypted Virtualization\",\n",
       "  'authors': \"['Robert Buhren', 'Hans-Niklas Jacob', 'Thilo Krachenfels', 'Jean-Pierre Seifert']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"CCS '21: Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security\",\n",
       "  'abstract': \"AMD Secure Encrypted Virtualization (SEV) offers protection mechanisms for virtual machines in untrusted environments through memory and register encryption. To separate security-sensitive operations from software executing on the main x86 cores, SEV leverages the AMD Secure Processor (AMD-SP). This paper introduces a new approach to attack SEV-protected virtual machines (VMs) by targeting the AMD-SP. We present a voltage glitching attack that allows an attacker to execute custom payloads on the AMD-SPs of all microarchitectures that support SEV currently on the market (Zen 1, Zen 2, and Zen 3). The presented methods allow us to deploy a custom SEV firmware on the AMD-SP, which enables an adversary to decrypt a VM's memory. Furthermore, using our approach, we can extract endorsement keys of SEV-enabled CPUs, which allows us to fake attestation reports or to pose as a valid target for VM migration without requiring physical access to the target host. Moreover, we reverse-engineered the Versioned Chip Endorsement Key (VCEK) mechanism introduced with SEV Secure Nested Paging (SEV-SNP). The VCEK binds the endorsement keys to the firmware version of TCB components relevant for SEV. Building on the ability to extract the endorsement keys, we show how to derive valid VCEKs for arbitrary firmware versions. With our findings, we prove that SEV cannot adequately protect confidential data in cloud environments from insider attackers, such as rogue administrators, on currently available CPUs.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3460120.3484779',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Metron: High-performance NFV Service Chaining Even in the Presence of Blackboxes',\n",
       "  'authors': \"['Georgios P. Katsikas', 'Tom Barbette', 'Dejan Kostić', 'JR. Gerald Q. Maguire', 'Rebecca Steinert']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Computer Systems',\n",
       "  'abstract': 'Deployment of 100Gigabit Ethernet (GbE) links challenges the packet processing limits of commodity hardware used for Network Functions Virtualization (NFV). Moreover, realizing chained network functions (i.e., service chains) necessitates the use of multiple CPU cores, or even multiple servers, to process packets from such high speed links.Our system Metron jointly exploits the underlying network and commodity servers’ resources: (i) to offload part of the packet processing logic to the network, (ii) \\xa0by using smart tagging to setup and exploit the affinity of traffic classes, and (iii) \\xa0by using tag-based hardware dispatching to carry out the remaining packet processing at the speed of the servers’ cores, with zero inter-core communication. Moreover, Metron transparently integrates, manages, and load balances proprietary “blackboxes” together with Metron service chains.Metron realizes stateful network functions at the speed of 100GbE network cards on a single server, while elastically and rapidly adapting to changing workload volumes. Our experiments demonstrate that Metron service chains can coexist with heterogeneous blackboxes, while still leveraging Metron’s accurate dispatching and load balancing. In summary, Metron has (i) \\xa02.75–8× better efficiency, up to (ii) \\xa04.7× lower latency, and (iii) \\xa07.8× higher throughput than OpenBox, a state-of-the-art NFV system.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3465628',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'FlexFilt: Towards Flexible Instruction Filtering for Security',\n",
       "  'authors': \"['Leila Delshadtehrani', 'Sadullah Canakci', 'William Blair', 'Manuel Egele', 'Ajay Joshi']\",\n",
       "  'date': 'December 2021',\n",
       "  'source': \"ACSAC '21: Proceedings of the 37th Annual Computer Security Applications Conference\",\n",
       "  'abstract': 'As the complexity of software applications increases, there has been a growing demand for intra-process memory isolation. The commercially available intra-process memory isolation mechanisms in modern processors, e.g., Intel’s memory protection keys, trade-off between efficiency and security guarantees. Recently, researchers have tended to leverage the features with low security guarantees for intra-process memory isolation. Subsequently, they have relied on binary scanning and runtime binary rewriting to prevent the execution of unsafe instructions, which improves the security guarantees. Such intra-process memory isolation mechanisms are not the only security solutions that have to prevent the execution of unsafe instructions in untrusted parts of the code. In fact, we identify a similar requirement in a variety of other security solutions. Although binary scanning and runtime binary rewriting approaches can be leveraged to address this requirement, it is challenging to efficiently implement these approaches.  In this paper, we propose an efficient and flexible hardware-assisted feature for runtime filtering of user-specified instructions. This flexible feature, called FlexFilt, assists with securing various isolation-based mechanisms. FlexFilt enables the software developer to create up to 16 instruction domains, where each instruction domain can be configured to filter the execution of user-specified instructions. In addition to filtering unprivileged instructions, FlexFilt is capable of filtering privileged instructions. To illustrate the effectiveness of FlexFilt compared to binary scanning approaches, we measure the overhead caused by scanning the JIT compiled code while browsing various webpages. We demonstrate the feasibility of FlexFilt by implementing our design on the RISC-V Rocket core, providing the Linux kernel support for it, and prototyping our full design on an FPGA.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3485832.3488019',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Leaky buddies: cross-component covert channels on integrated CPU-GPU systems',\n",
       "  'authors': \"['Sankha Baran Dutta', 'Hoda Naghibijouybari', 'Nael Abu-Ghazaleh', 'Andres Marquez', 'Kevin Barker']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': \"Graphics Processing Units (GPUs) are ubiquitous components used across the range of today's computing platforms, from phones and tablets, through personal computers, to high-end server class platforms. With the increasing importance of graphics and video workloads, recent processors are shipped with GPU devices that are integrated on the same chip. Integrated GPUs share some resources with the CPU and as a result, there is a potential for microarchitectural attacks from the GPU to the CPU or vice versa. We consider the potential for covert channel attacks that arise either from shared microarchitectural components (such as caches) or through shared contention domains (e.g., shared buses). We illustrate these two types of channels by developing two reliable covert channel attacks. The first covert channel uses the shared LLC cache in Intel's integrated GPU architectures. The second is a contention based channel targeting the ring bus connecting the CPU and GPU to the LLC. This is the first demonstrated microarchitectural attack crossing the component boundary (GPU to CPU or vice versa). Cross-component channels introduce a number of new challenges that we had to overcome since they occur across heterogeneous components that use different computation models and are interconnected using asymmetric memory hierarchies. We also exploit GPU parallelism to increase the bandwidth of the communication, even without relying on a common clock. The LLC based channel achieves a bandwidth of 120 kbps with a low error rate of 2%, while the contention based channel delivers up to 400 kbps with a 0.8% error rate. We also demonstrate a proof-of-concept prime-and-probe side channel attack that probes the full LLC from the GPU.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00080',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Deployment and scalability of an inter-domain multi-path routing infrastructure',\n",
       "  'authors': \"['Cyrill Krähenbühl', 'Seyedali Tabaeiaghdaei', 'Christelle Gloor', 'Jonghoon Kwon', 'Adrian Perrig', 'David Hausheer', 'Dominik Roos']\",\n",
       "  'date': 'December 2021',\n",
       "  'source': \"CoNEXT '21: Proceedings of the 17th International Conference on emerging Networking EXperiments and Technologies\",\n",
       "  'abstract': \"Path aware networking (PAN) is a promising approach that enables endpoints to participate in end-to-end path selection. PAN unlocks numerous benefits, such as fast failover after link failures, application-based path selection and optimization, and native interdomain multi-path. The utility of PAN hinges on the availability of a large number of high-quality path options. In an inter-domain context, two core questions arise. Can we deploy such an architecture natively in today's Internet infrastructure without creating an overlay relying on BGP? Can we build a scalable multi-path routing system that provides a large number of high-quality paths? We first report on the real-world native deployment of the SCION next-generation architecture, providing a usable PAN infrastructure operating in parallel to today's Internet. We then analyze the scalability of the architecture in an Internet-scale topology. Finally, we introduce a new routing approach to further improve scalability.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3485983.3494862',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Faster enclave transitions for IO-intensive network applications',\n",
       "  'authors': \"['Jakob Svenningsson', 'Nicolae Paladi', 'Arash Vahidi']\",\n",
       "  'date': 'August 2021',\n",
       "  'source': \"SPIN '21: Proceedings of the ACM SIGCOMM 2021 Workshop on Secure Programmable network INfrastructure\",\n",
       "  'abstract': 'Process-based confidential computing enclaves such as Intel SGX have been proposed for protecting the confidentiality and integrity of network applications, without the overhead of virtualization. However, these solutions introduce other types of overhead, particularly the cost transitioning in and out of an enclave context. This makes the use of enclaves impractical for running IO-intensive applications, such as network packet processing. We build on earlier approaches to improve the IO performance of workloads in Intel SGX enclaves and propose the HotCall-Bundler library that helps reduce the cost of individual single enclave transitions and the total number of enclave transitions in trusted applications running in Intel SGX enclaves. We describe the implementation of the HotCall-Bundler library, evaluate its performance and demonstrate its practicality using the case study of Open vSwitch, a widely used software switch implementation.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3472873.3472879',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Distance-aware Approximate Nanophotonic Interconnect',\n",
       "  'authors': \"['Jaechul Lee', 'Cédric Killian', 'Sebastien Le Beux', 'Daniel Chillet']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Design Automation of Electronic Systems',\n",
       "  'abstract': 'The energy consumption of manycore architectures is dominated by data movement, which calls for energy-efficient and high-bandwidth interconnects. To overcome the bandwidth limitation of electrical interconnects, integrated optics appear as a promising technology. However, it suffers from high power overhead related to low laser efficiency, which calls for the use of techniques and methods to improve its energy costs. Besides, approximate computing is emerging as an efficient method to reduce energy consumption and improve execution speed of embedded computing systems. It relies on allowing accuracy reduction on data at the cost of tolerable application output error. In this context, the work presented in this article exploits both features by defining approximate communications for error-tolerant applications. We propose a method to design realistic and scalable nanophotonic interconnect supporting approximate data transmission and power adaption according to the communication distance to improve the energy efficiency. For this purpose, the data can be sent by mixing low optical power signal and truncation for the Least Significant Bits (LSB) of the floating-point numbers, while the overall power is adapted according to the communication distance. We define two ranges of communications, short and long, which require only four power levels. This reduces area and power overhead to control the laser output power. A transmission model allows estimating the laser power according to the targeted BER and the number of truncated bits, while the optical network interface allows configuring, at runtime, the number of approximated and truncated bits and the laser output powers. We explore the energy efficiency provided by each communication scheme, and we investigate the error resilience of the benchmarks over several approximation and truncation schemes. The simulation results of ApproxBench applications show that, compared to an interconnect involving only robust communications, approximations in the optical transmission led to up to 53% laser power reduction with a limited degradation at the application level with less than 9% of output error. Finally, we show that our solution is scalable and leads to 10% reduction in the total energy consumption, 35× reduction in the laser driver size, and 10× reduction in the laser controller compared to state-of-the-art solution.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3484309',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'RFClock: timing, phase and frequency synchronization for distributed wireless networks',\n",
       "  'authors': \"['Kubra Alemdar', 'Divashree Varshney', 'Subhramoy Mohanti', 'Ufuk Muncuk', 'Kaushik Chowdhury']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"MobiCom '21: Proceedings of the 27th Annual International Conference on Mobile Computing and Networking\",\n",
       "  'abstract': \"Emerging applications like distributed coordinated beamforming (DCB), intelligent reflector arrays, and networked robotic devices will transform wireless applications. However, for systems-centric work on these topics, the research community must first overcome the hurdle of implementing fine-grained, over-the-air timing synchronization, which is critical for any coordinated operation. To address this gap, this paper presents an open-source design and implementation of 'RFClock' that provides timing, frequency and phase synchronization for software defined radios (SDRs). It shows how RFClock can be used for a practical, 5-node DCB application without modifying existing physical/link layer protocols. By utilizing a leader-follower architecture, RFClock-leader allows follower clocks to synchronize with mean offset under 0.107Hz, and then corrects the time/phase alignment to be within a 5ns deviation. RFClock is designed to operate in generalized environments: as standalone unit, it generates a 10MHz/1PPS signal reference suitable for most commercial-off-the-shelf (COTS) SDRs today; it does not require custom protocol-specific headers or messaging; and it is robust to interference through a frequency-agile operation. Using RFClock for DCB, we verify significant increase in channel gain and low BER in a range of [0 -- 10--3] for different modulation schemes. We also demonstrate performance that is similar to a popular wired solution and significant improvement over a GPS-based solution, while delivering this functionality at a fractional price/power point.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3447993.3448623',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Durable Queues: The Second Amendment',\n",
       "  'authors': \"['Gal Sela', 'Erez Petrank']\",\n",
       "  'date': 'July 2021',\n",
       "  'source': \"SPAA '21: Proceedings of the 33rd ACM Symposium on Parallelism in Algorithms and Architectures\",\n",
       "  'abstract': 'We consider durable data structures for non-volatile main memory, such as the new Intel Optane memory architecture. Substantial recent work has concentrated on making concurrent data structures durable with low overhead, by adding a minimal number of blocking persist operations (i.e., flushes and fences). In this work we show that focusing on minimizing the number of persist instructions is important, but not enough. We show that access to flushed content is of high cost due to cache invalidation in current architectures. Given this finding, we present a design of the queue data structure that properly takes care of minimizing blocking persist operations as well as minimizing access to flushed content. The proposed design outperforms state-of-the-art durable queues.   We start by providing a durable version of the Michael Scott queue (MSQ ). We amend MSQ by adding a minimal number of persist instructions, fewer than in available durable queues, and meeting the theoretical lower bound on the number of blocking persist operations. We then proceed with a second amendment to this design, that eliminates accesses to flushed data. Evaluation shows that the second amendment yields substantial performance improvement, outperforming the state of the art and demonstrating the importance of reduced accesses to flushed content. The presented queues are durably linearizable and lock-free. Finally, we discuss the theoretical optimal number of accesses to flushed content.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3409964.3461791',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Efficiently detecting concurrency bugs in persistent memory programs',\n",
       "  'authors': \"['Zhangyu Chen', 'Yu Hua', 'Yongle Zhang', 'Luochangqi Ding']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'Due to the salient DRAM-comparable performance, TB-scale capacity, and non-volatility, persistent memory (PM) provides new opportunities for large-scale in-memory computing with instant crash recovery. However, programming PM systems is error-prone due to the existence of crash-consistency bugs, which are challenging to diagnose especially with concurrent programming widely adopted in PM applications to exploit hardware parallelism. Existing bug detection tools for DRAM-based concurrency issues cannot detect PM crash-consistency bugs because they are oblivious to PM operations and PM consistency. On the other hand, existing PM-specific debugging tools only focus on sequential PM programs and cannot effectively detect crash-consistency issues hidden in concurrent executions.   In order to effectively detect crash-consistency bugs that only manifest in concurrent executions, we propose PMRace, the first PM-specific concurrency bug detection tool. We identify and define two new types of concurrent crash-consistency bugs: PM Inter-thread Inconsistency and PM Synchronization Inconsistency. In particular, PMRace adopts PM-aware and coverage-guided fuzz testing to explore PM program executions. For PM Inter-thread Inconsistency, which denotes the data inconsistency hidden in thread interleavings, PMRace performs PM-aware interleaving exploration and thread scheduling to drive the execution towards executions that reveal such inconsistencies. For PM Synchronization Inconsistency between persisted synchronization variables and program data, PMRace identifies the inconsistency during interleaving exploration. The post-failure validation reduces the false positives that come from custom crash recovery mechanisms. PMRace has found 14 bugs (10 new bugs) in real-world concurrent PM systems including PM-version memcached.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503222.3507755',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Task-RM: A Resource Manager for Energy Reduction in Task-Parallel Applications under Quality of Service Constraints',\n",
       "  'authors': \"['M. Waqar Azhar', 'Miquel Pericàs', 'Per Stenström']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Architecture and Code Optimization',\n",
       "  'abstract': 'Improving energy efficiency is an important goal of computer system design. This article focuses on a general model of task-parallel applications under quality-of-service requirements on the completion time. Our technique, called Task-RM, exploits the variance in task execution-times and imbalance between tasks to allocate just enough resources in terms of voltage-frequency and core-allocation so that the application completes before the deadline. Moreover, we provide a solution that can harness additional energy savings with the availability of additional processors. We observe that, for the proposed run-time resource manager to allocate resources, it requires specification of the soft deadlines to the tasks. This is accomplished by analyzing the energy-saving scenarios offline and by providing Task-RM with the performance requirements of the tasks. The evaluation shows an energy saving of 33% compared to race-to-idle and 22% compared to dynamic slack allocation (DSA) with an overhead of less than 1%.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3494537',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Horizontal Side-Channel Vulnerabilities of Post-Quantum Key Exchange and Encapsulation Protocols',\n",
       "  'authors': \"['Furkan Aydin', 'Aydin Aysu', 'Mohit Tiwari', 'Andreas Gerstlauer', 'Michael Orshansky']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Embedded Computing Systems',\n",
       "  'abstract': 'Key exchange protocols and key encapsulation mechanisms establish secret keys to communicate digital information confidentially over public channels. Lattice-based cryptography variants of these protocols are promising alternatives given their quantum-cryptanalysis resistance and implementation efficiency. Although lattice cryptosystems can be mathematically secure, their implementations have shown side-channel vulnerabilities. But such attacks largely presume collecting multiple measurements under a fixed key, leaving the more dangerous single-trace attacks unexplored. This article demonstrates successful single-trace power side-channel attacks on lattice-based key exchange and encapsulation protocols. Our attack targets both hardware and software implementations of matrix multiplications used in lattice cryptosystems. The crux of our idea is to apply a horizontal attack that makes hypotheses on several intermediate values within a single execution all relating to the same secret, and to combine their correlations for accurately estimating the secret key. We illustrate that the design of protocols combined with the nature of lattice arithmetic enables our attack. Since a straightforward attack suffers from false positives, we demonstrate a novel extend-and-prune procedure to recover the key by following the sequence of intermediate updates during multiplication. We analyzed two protocols, Frodo and FrodoKEM, and reveal that they are vulnerable to our attack. We implement both stand-alone hardware and RISC-V based software realizations and test the effectiveness of the proposed attack by using concrete parameters of these protocols on physical platforms with real measurements. We show that the proposed attack can estimate secret keys from a single power measurement with over 99% success rate.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3476799',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Federated Learning for Internet of Things',\n",
       "  'authors': \"['Tuo Zhang', 'Chaoyang He', 'Tianhao Ma', 'Lei Gao', 'Mark Ma', 'Salman Avestimehr']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"SenSys '21: Proceedings of the 19th ACM Conference on Embedded Networked Sensor Systems\",\n",
       "  'abstract': 'Federated learning can be a promising solution for enabling IoT cybersecurity (i.e., anomaly detection in the IoT environment) while preserving data privacy and mitigating the high communication/storage overhead (e.g., high-frequency data from time-series sensors) of centralized over-the-cloud approaches. In this paper, to further push forward this direction with a comprehensive study in both algorithm and system design, we build FedIoT platform that contains FedDetect algorithm for on-device anomaly data detection and a system design for realistic evaluation of federated learning on IoT devices. Furthermore, the proposed FedDetect learning framework improves the performance by utilizing a local adaptive optimizer (e.g., Adam) and a cross-round learning rate scheduler. In a network of realistic IoT devices (Raspberry PI), we evaluate FedIoT platform and FedDetect algorithm in both model and system performance. Our results demonstrate the efficacy of federated learning in detecting a wider range of attack types occurred at multiple devices. The system efficiency analysis indicates that both end-to-end training time and memory cost are affordable and promising for resource-constrained IoT devices. The source code is publicly available at https://github.com/FedML-AI/FedIoT.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3485730.3493444',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Chimera: A Low-power Reconfigurable Platform for Internet of Things',\n",
       "  'authors': \"['Emekcan Aras', 'Stéphane Delbruel', 'Fan Yang', 'Wouter Joosen', 'Danny Hughes']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Internet of Things',\n",
       "  'abstract': 'The Internet of Things (IoT) is being deployed in an ever-growing range of applications, from industrial monitoring to smart buildings to wearable devices. Each of these applications has specific computational requirements arising from their networking, system security, and edge analytics functionality. This diversity in requirements motivates the need for adaptable end-devices, which can be re-configured and re-used throughout their lifetime to handle computation-intensive tasks without sacrificing battery lifetime. To tackle this problem, this article presents Chimera, a low-power platform for research and experimentation with reconfigurable hardware for the IoT end-devices. Chimera achieves flexibility and re-usability through an architecture based on a Flash Field Programmable Gate Array (FPGA) with a reconfigurable software stack that enables over-the-air hardware and software evolution at runtime. This adaptability enables low-cost hardware/software upgrades on the end-devices and an increased ability to handle computationally-intensive tasks. This article describes the design of the Chimera hardware platform and software stack, evaluates it through three application scenarios, and reviews the factors that have thus far prevented FPGAs from being utilized in IoT end-devices.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3440995',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'IoT in the Wild: An expedition of discovery for remote monitoring.',\n",
       "  'authors': \"['Graham Coulby', 'Adrian K Clear', 'Oliver Jones', 'Alan Godfrey']\",\n",
       "  'date': 'September 2021',\n",
       "  'source': \"UbiComp/ISWC '21 Adjunct: Adjunct Proceedings of the 2021 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2021 ACM International Symposium on Wearable Computers\",\n",
       "  'abstract': 'Free-living assessment and remote monitoring is important for healthcare researchers. Moving research beyond the laboratory provides habitual environments for remote assessment that allows research to remain agile even when facing uncontrollable external factors e.g., the SARS-COV-2 pandemic. Emergent technologies have the potential to make this form of assessment feasible by providing accessible and affordable mechanisms for conducting free-living research. This paper presents findings from a study that was halted due to the pandemic, but this work highlighted a series of challenges that may present themselves to researchers conducting similar work. By transparently reporting the challenges and solutions rather than just methods, it is hoped that the lessons learned from this study could provide researchers with greater awareness in future studies.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3460418.3479364',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Vision Paper: Towards Software-Defined Video Analytics with Cross-Camera Collaboration',\n",
       "  'authors': \"['Juheon Yi', 'Chulhong Min', 'Fahim Kawsar']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"SenSys '21: Proceedings of the 19th ACM Conference on Embedded Networked Sensor Systems\",\n",
       "  'abstract': 'Video cameras are becoming ubiquitous in our daily lives. With the recent advancement of Artificial Intelligence (AI), live video analytics are enabling various useful services, including traffic monitoring and campus surveillance. However, current video analytics systems are highly limited in leveraging the enormous opportunities of the deployed cameras due to (i) centralized processing architecture (i.e., cameras are treated as dumb streaming-only sensors), (ii) hard-coded analytics capabilities from tightly coupled hardware and software, (iii) isolated and fragmented camera deployment from different service providers, and (iv) independent processing of camera streams without any collaboration. In this paper, we envision a full-fledged system for software-defined video analytics with cross-camera collaboration that overcomes the aforementioned limitations. We illustrate its detailed system architecture, carefully analyze the key system requirements with representative app scenarios, and derive potential research issues along with a summary of the status quo of existing works.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3485730.3493453',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Enabling Passive Backscatter Tag Localization Without Active Receivers',\n",
       "  'authors': \"['Abeer Ahmad', 'Xiao Sha', 'Milutin Stanaćević', 'Akshay Athalye', 'Petar M. Djurić', 'Samir R. Das']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"SenSys '21: Proceedings of the 19th ACM Conference on Embedded Networked Sensor Systems\",\n",
       "  'abstract': 'Backscattering tags transmit passively without an on-board active radio transmitter. Almost all present-day backscatter systems, however, rely on active radio receivers. This presents a significant scalability, power and cost challenge for backscatter systems. To overcome this barrier, recent research has empowered these passive tags with the ability to reliably receive backscatter signals from other tags. This forms the building block of passive networks wherein tags talk to each other without an active radio on either the transmit or receive side. For wider functionality, accurate localization of such tags is critical. All known backscatter tag localization techniques rely on active receivers for measuring and characterizing the received signal. As a result, they cannot be directly applied to passive tag-to-tag networks. This paper overcomes the gap by developing a localization technique for such passive networks based on a novel method for phase-based ranging in passive receivers. This method allows pairs of passive tags to collaboratively determine the inter-tag channel phase while effectively minimizing the effects of multipath and noise in the surrounding environment. Building on this, we develop a localization technique that benefits from large link diversity uniquely available in a passive tag-to-tag network. We evaluate the performance of our techniques with extensive micro-benchmarking experiments in an indoor environment using fabricated prototypes of tag hardware. We show that our phase-based ranging performs similar to active receivers, providing median 1D ranging error <1 cm and median localization error also <1 cm. Benefiting from the large-scale link diversity our localization technique outperforms several state-of-the-art techniques that use active receivers.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3485730.3485950',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'FULL-W2V: fully exploiting data reuse for W2V on GPU-accelerated systems',\n",
       "  'authors': \"['Thomas Randall', 'Tyler Allen', 'Rong Ge']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ICS '21: Proceedings of the ACM International Conference on Supercomputing\",\n",
       "  'abstract': 'Word2Vec remains one of the highly-impactful innovations in the field of Natural Language Processing (NLP) that represents latent grammatical and syntactical information in human text with dense vectors in a low dimension. Word2Vec has high computational cost due to the algorithm’s inherent sequentiality, intensive memory accesses, and the large vocabularies it represents. While prior studies have investigated technologies to explore parallelism and improve memory system performance, they struggle to effectively gain throughput on powerful GPUs. We identify memory data access and latency as the primary bottleneck in prior works on GPUs, which prevents highly optimized kernels from attaining the architecture’s peak performance. We present a novel algorithm, FULL-W2V, which maximally exploits the opportunities for data reuse in the W2V algorithm and leverages GPU architecture and resources to reduce access to low memory levels and improve temporal locality. FULL-W2V is capable of reducing accesses to GPU global memory significantly, e.g., by more than 89%, compared to prior state-of-the-art GPU implementations, resulting in significant performance improvement that scales across successive hardware generations. Our prototype implementation achieves 2.97X speedup when ported from Nvidia Pascal P100 to Volta V100 cards, and outperforms the state-of-the-art by 5.72X on V100 cards with the same embedding quality. In-depth analysis indicates that the reduction of memory accesses through register and shared memory caching and high-throughput shared memory reduction leads to a significantly improved arithmetic intensity. FULL-W2V can potentially benefit many applications in NLP and other domains.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3447818.3460373',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Leveraging Automatic High-Level Synthesis Resource Sharing to Maximize Dynamical Voltage Overscaling with Error Control',\n",
       "  'authors': \"['Prattay Chowdhury', 'Benjamin Carrion Schafer']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Design Automation of Electronic Systems',\n",
       "  'abstract': 'Approximate Computing has emerged as an alternative way to further reduce the power consumption of integrated circuits (ICs) by trading off errors at the output with simpler, more efficient logic. So far the main approaches in approximate computing have been to simplify the hardware circuit by pruning the circuit until the maximum error threshold is met. One of the critical issues, though, is the training data used to prune the circuit. The output error can significantly exceed the maximum error if the final workload does not match the training data. Thus, most previous work typically assumes that training data matches with the workload data distribution. In this work, we present a method that dynamically overscales the supply voltage based on different workload distribution at runtime. This allows to adaptively select the supply voltage that leads to the largest power savings while ensuring that the error will never exceed the maximum error threshold. This approach also allows restoring of the original error-free circuit if no matching workload distribution is found. The proposed method also leverages the ability of High-Level Synthesis (HLS) to automatically generate circuits with different properties by setting different synthesis constraints to maximize the available timing slack and, hence, maximize the power savings. Experimental results show that our proposed method works very well, saving on average 47.08% of power as compared to the exact output circuit and 20.25% more than a traditional approximation method.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3473909',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'CryoWire: wire-driven microarchitecture designs for cryogenic computing',\n",
       "  'authors': \"['Dongmoon Min', 'Yujin Chung', 'Ilkwon Byun', 'Junpyo Kim', 'Jangwoo Kim']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'Cryogenic computing, which runs a computer device at an extremely low temperature, is promising thanks to its significant reduction of wire resistance as well as leakage current. Recent studies on cryogenic computing have focused on various architectural units including the main memory, cache, and CPU core running at 77K. However, little research has been conducted to fully exploit the fast cryogenic wires, even though the slow wires are becoming more serious performance bottleneck in modern processors. In this paper, we propose a CPU microarchitecture which extensively exploits the fast wires at 77K. For this goal, we first introduce our validated cryogenic-performance models for the CPU pipeline and network on chip (NoC), whose performance can be significantly limited by the slow wires. Next, based on the analysis with the models, we architect CryoSP and CryoBus as our pipeline and NoC designs to fully exploit the fast wires. Our evaluation shows that our cryogenic computer equipped with both microarchitectures achieves 3.82 times higher system-level performance compared to the conventional computer system thanks to the 96% higher clock frequency of CryoSP and five times lower NoC latency of CryoBus.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503222.3507749',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'AUTO-PRUNE: automated DNN pruning and mapping for ReRAM-based accelerator',\n",
       "  'authors': \"['Siling Yang', 'Weijian Chen', 'Xuechen Zhang', 'Shuibing He', 'Yanlong Yin', 'Xian-He Sun']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ICS '21: Proceedings of the ACM International Conference on Supercomputing\",\n",
       "  'abstract': 'Emergent ReRAM-based accelerators support in-memory computation to accelerate deep neural network (DNN) inference. Weight matrix pruning of DNNs is a widely used technique to reduce the size of DNN models, thereby reducing the resource and energy consumption of ReRAM-based accelerators. However, conventional works on weight matrix pruning for ReRAM-based accelerators have three major issues. First, they use heuristics or rules from domain experts to prune the weights, leading to suboptimal pruning policies. Second, they mostly focus on improving compression ratio, thus may not meet accuracy constraints. Third, they ignore direct feedback of hardware. In this paper, we introduce an automated DNN pruning and mapping framework, named AUTO-PRUNE. It leverages reinforcement learning (RL) to automatically determine the pruning policy considering the constraint of accuracy loss. The reward function of RL agents is designed using hardware’s direct feedback (i.e., accuracy and compression rate of occupied crossbars). The function directs the search of the pruning ratio of each layer for a global optimum considering the characteristics of individual layers of DNN models. Then AUTO-PRUNE maps the pruned weight matrices to crossbars to store only nontrivial elements. Finally, to avoid the dislocation problem, we design a new data-path in ReRAM-based accelerators to correctly index and feed input to matrix-vector computation leveraging the mechanism of operation units. Experimental results show that, compared to the state-of-the-art work, AUTO-PRUNE achieves up to 3.3X compression rate, 3.1X area efficiency, and 3.3X energy efficiency with a similar or even higher accuracy.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3447818.3460366',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Temporal State Machines: Using Temporal Memory to Stitch Time-based Graph Computations',\n",
       "  'authors': \"['Advait Madhavan', 'Matthew W. Daniels', 'Mark D. Stiles']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Journal on Emerging Technologies in Computing Systems',\n",
       "  'abstract': 'Race logic, an arrival-time-coded logic family, has demonstrated energy and performance improvements for applications ranging from dynamic programming to machine learning. However, the various ad hoc mappings of algorithms into hardware rely on researcher ingenuity and result in custom architectures that are difficult to systematize. We propose to associate race logic with the mathematical field of tropical algebra, enabling a more methodical approach toward building temporal circuits. This association between the mathematical primitives of tropical algebra and generalized race logic computations guides the design of temporally coded tropical circuits. It also serves as a framework for expressing high-level timing-based algorithms. This abstraction, when combined with temporal memory, allows for the systematic exploration of race logic–based temporal architectures by making it possible to partition feed-forward computations into stages and organize them into a state machine. We leverage analog memristor-based temporal memories to design such a state machine that operates purely on time-coded wavefronts. We implement a version of Dijkstra’s algorithm to evaluate this temporal state machine. This demonstration shows the promise of expanding the expressibility of temporal computing to enable it to deliver significant energy and throughput advantages.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3451214',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Morphy: Software Defined Charge Storage for the IoT',\n",
       "  'authors': \"['Fan Yang', 'Ashok Samraj Thangarajan', 'Sam Michiels', 'Wouter Joosen', 'Danny Hughes']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"SenSys '21: Proceedings of the 19th ACM Conference on Embedded Networked Sensor Systems\",\n",
       "  'abstract': 'Recent innovations in energy harvesting promise extended operational life and reduced maintenance costs for the next generation of Internet of Things (IoT) platforms. However, energy management in these platforms remains problematic due to dynamism in energy supply and demand, inefficiency in storing and converting energy and a lack of per-task charge isolation. This paper tackles this problem by proposing a software defined charge storage module called Morphy, which combines a polymorphic capacitor array with intelligent power management software. Morphy delivers energy to application tasks in a flexible, efficient, and isolated manner. Morphy provides two software extensions to the Operating System scheduler: the energy semaphore blocks the execution of tasks until sufficient charge is available to safely run them, and the energy watchdog monitors and mitigates energy management bugs. We have realized a prototype of Morphy with the hardware form factor of a standard 9V (PP3) battery package and a software library that integrates with the FreeRTOS scheduler. Our evaluation shows that, in comparison to standard energy storage and management approaches, our prototype reaches an operational voltage more quickly, sustains operation longer in the case of power failure and effectively isolates charge storage for dedicated tasks with minimal compute, memory and energy overhead.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3485730.3485947',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'SPARSE: Spatially Aware LFI Resilient State Machine Encoding',\n",
       "  'authors': \"['Choudhury Muhtadi', 'Tajik Shahin', 'Domenic Forte']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"HASP '21: Proceedings of the 10th International Workshop on Hardware and Architectural Support for Security and Privacy\",\n",
       "  'abstract': 'As finite state machines (FSMs) control the behavior of sequential circuits, they can be a target for attacks. With laser-based fault injection (LFI), an adversary may attain unauthorized access to sensitive states by altering the values of individual state flip-flops (FFs). Although standard error correction/detection techniques improve FSM resiliency, all states and FFs of an FSM are assumed equally critical to protect, incurring significant overhead. In this paper, we introduce a novel spatial vulnerability metric to aid the security analysis, which precisely manifests the susceptibility of FSM designs to LFI based on state FF sensitivity and placement. A novel encoding and spatially aware physical design framework (SPARSE) are then proposed that co-optimize the FSM encoding and state FF placement to minimize LFI susceptibility. SPARSE’s encoding uses the minimum number of FFs by placing security-sensitive FFs a sufficient distance apart from other FFs. SPARSE is demonstrated on 5 benchmarks using commercial CAD tools and outperforms other FSM encoding schemes in terms of security, area, and PDP.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3505253.3505254',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'You Only Traverse Twice: A YOTT Placement, Routing, and Timing Approach for CGRAs',\n",
       "  'authors': \"['Michael Canesche', 'Westerley Carvalho', 'Lucas Reis', 'Matheus Oliveira', 'Salles Magalhães', 'Peter Jamieson', 'Jaugusto M. Nacif', 'Ricardo Ferreira']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Embedded Computing Systems',\n",
       "  'abstract': 'Coarse-grained reconfigurable architecture (CGRA) mapping involves three main steps: placement, routing, and timing. The mapping is an NP-complete problem, and a common strategy is to decouple this process into its independent steps. This work focuses on the placement step, and its aim is to propose a technique that is both reasonably fast and leads to high-performance solutions. Furthermore, a near-optimal placement simplifies the following routing and timing steps. Exact solutions cannot find placements in a reasonable execution time as input designs increase in size. Heuristic solutions include meta-heuristics, such as Simulated Annealing (SA) and fast and straightforward greedy heuristics based on graph traversal. However, as these approaches are probabilistic and have a large design space, it is not easy to provide both run-time efficiency and good solution quality. We propose a graph traversal heuristic that provides the best of both: high-quality placements similar to SA and the execution time of graph traversal approaches. Our placement introduces novel ideas based on “you only traverse twice” (YOTT) approach that performs a two-step graph traversal. The first traversal generates annotated data to guide the second step, which greedily performs the placement, node per node, aided by the annotated data and target architecture constraints. We introduce three new concepts to implement this technique: I/O and reconvergence annotation, degree matching, and look-ahead placement. Our analysis of this approach explores the placement execution time/quality trade-offs. We point out insights on how to analyze graph properties during dataflow mapping. Our results show that YOTT is 60.6\\\\(\\\\), 9.7 \\\\(\\\\), and 2.3\\\\(\\\\) faster than a high-quality SA, bounding box SA VPR, and multi-single traversal placements, respectively. Furthermore, YOTT reduces the average wire length and the maximal FIFO size (additional timing requirement on CGRAs) to avoid delay mismatches in fully pipelined architectures.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3477038',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'An experimental framework for improving the performance of BFT consensus for future permissioned blockchains',\n",
       "  'authors': \"['Man-Kit Sit', 'Manuel Bravo', 'Zsolt István']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"DEBS '21: Proceedings of the 15th ACM International Conference on Distributed and Event-based Systems\",\n",
       "  'abstract': 'Permissioned Blockchains are increasingly considered in enterprise use-cases, many of which do not require geo-distribution, or even disallow it due to legislation. Examples include countrywide networks, such as Alastria, or those deployed using cloud-based platforms such as IBM Blockchain Platform. We expect these blockchains to eventually run in environments with high bandwidth and low latency modern networks, as well as with advanced programmable hardware accelerators. Even though there is renewed interest in BFT consensus algorithms with various proposals targeting Permissioned Blockchains, related work does not optimize for fast networks and does not incorporate hardware accelerators - we make the case that doing so will pay off in the long run. To this end, we re-implemented the seminal PBFT algorithm in a way that allows us to measure different configurations of the protocol. Through this we explore the benefits of various common optimization strategies and show that the protocol is unlikely to saturate more than 10Gbps networks without relying on specialized hardware-based offloading. Based on the experimental results, we discuss two concrete ways in which the cost of consensus in Permissioned Blockchains could be reduced in high-speed networking environments, namely, offloading to SmartNICs and implementing the protocol on standalone FPGAs.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3465480.3466922',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Ginkgo: A Modern Linear Operator Algebra Framework for High Performance Computing',\n",
       "  'authors': \"['Hartwig Anzt', 'Terry Cojean', 'Goran Flegar', 'Fritz Göbel', 'Thomas Grützmacher', 'Pratik Nayak', 'Tobias Ribizel', 'Yuhsiang Mike Tsai', 'Enrique S. Quintana-Ortí']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Mathematical Software',\n",
       "  'abstract': 'In this article, we present Ginkgo, a modern C++ math library for scientific high performance computing. While classical linear algebra libraries act on matrix and vector objects, Ginkgo’s design principle abstracts all functionality as “linear operators,” motivating the notation of a “linear operator algebra library.” Ginkgo’s current focus is oriented toward providing sparse linear algebra functionality for high performance graphics processing unit (GPU) architectures, but given the library design, this focus can be easily extended to accommodate other algorithms and hardware architectures. We introduce this sophisticated software architecture that separates core algorithms from architecture-specific backends and provide details on extensibility and sustainability measures. We also demonstrate Ginkgo’s usability by providing examples on how to use its functionality inside the MFEM and deal.ii finite element ecosystems. Finally, we offer a practical demonstration of Ginkgo’s high performance on state-of-the-art GPU architectures.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3480935',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Enhancing the Scalability of Multi-FPGA Stencil Computations via Highly Optimized HDL Components',\n",
       "  'authors': \"['Enrico Reggiani', 'Emanuele Del Sozzo', 'Davide Conficconi', 'Giuseppe Natale', 'Carlo Moroni', 'Marco D. Santambrogio']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Reconfigurable Technology and Systems',\n",
       "  'abstract': 'Stencil-based algorithms are a relevant class of computational kernels in high-performance systems, as they appear in a plethora of fields, from image processing to seismic simulations, from numerical methods to physical modeling. Among the various incarnations of stencil-based computations, Iterative Stencil Loops (ISLs) and Convolutional Neural Networks (CNNs) represent two well-known examples of kernels belonging to the stencil class. Indeed, ISLs apply the same stencil several times until convergence, while CNN layers leverage stencils to extract features from an image. The computationally intensive essence of ISLs, CNNs, and in general stencil-based workloads, requires solutions able to produce efficient implementations in terms of throughput and power efficiency. In this context, FPGAs are ideal candidates for such workloads, as they allow design architectures tailored to the stencil regular computational pattern. Moreover, the ever-growing need for performance enhancement leads FPGA-based architectures to scale to multiple devices to benefit from a distributed acceleration. For this reason, we propose a library of HDL components to effectively compute ISLs and CNNs inference on FPGA, along with a scalable multi-FPGA architecture, based on custom PCB interconnects. Our solution eases the design flow and guarantees both scalability and performance competitive with state-of-the-art works.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3461478',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': \"Ten lessons from three generations shaped Google's TPUv4i\",\n",
       "  'authors': \"['Norman P. Jouppi', 'Doe Hyun Yoon', 'Matthew Ashcraft', 'Mark Gottscho', 'Thomas B. Jablin', 'George Kurian', 'James Laudon', 'Sheng Li', 'Peter Ma', 'Xiaoyu Ma', 'Thomas Norrie', 'Nishant Patil', 'Sushma Prasad', 'Cliff Young', 'Zongwei Zhou', 'David Patterson']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': 'Google deployed several TPU generations since 2015, teaching us lessons that changed our views: semi-conductor technology advances unequally; compiler compatibility trumps binary compatibility, especially for VLIW domain-specific architectures (DSA); target total cost of ownership vs initial cost; support multi-tenancy; deep neural networks (DNN) grow 1.5X annually; DNN advances evolve workloads; some inference tasks require floating point; inference DSAs need air-cooling; apps limit latency, not batch size; and backwards ML compatibility helps deploy DNNs quickly. These lessons molded TPUv4i, an inference DSA deployed since 2020.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00010',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Short Paper: Device- and Locality-Specific Fingerprinting of Shared NISQ Quantum Computers',\n",
       "  'authors': \"['Allen Mi', 'Shuwen Deng', 'Jakub Szefer']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"HASP '21: Proceedings of the 10th International Workshop on Hardware and Architectural Support for Security and Privacy\",\n",
       "  'abstract': 'Fingerprinting of quantum computer devices is a new threat that poses a challenge to shared, cloud-based quantum computers. Fingerprinting can allow adversaries to map quantum computer infrastructures, uniquely identify cloud-based devices which otherwise have no public identifiers, and it can assist other adversarial attacks. This work shows idle tomography-based fingerprinting method based on crosstalk-induced errors in NISQ quantum computers. The device- and locality-specific fingerprinting results show prediction accuracy values of 99.1% and 95.3%, respectively.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3505253.3505261',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'L2D2: low latency distributed downlink for LEO satellites',\n",
       "  'authors': \"['Deepak Vasisht', 'Jayanth Shenoy', 'Ranveer Chandra']\",\n",
       "  'date': 'August 2021',\n",
       "  'source': \"SIGCOMM '21: Proceedings of the 2021 ACM SIGCOMM 2021 Conference\",\n",
       "  'abstract': \"Large constellations of Low Earth Orbit satellites promise to provide near real-time high-resolution Earth imagery. Yet, getting this large amount of data back to Earth is challenging because of their low orbits and fast motion through space. Centralized architectures with few multi-million dollar ground stations incur large hour-level data download latency and are hard to scale. We propose a geographically distributed ground station design, L2D2, that uses low-cost commodity hardware to offer low latency robust downlink. L2D2 is the first system to use a hybrid ground station model, where only a subset of ground stations are uplink-capable. We design new algorithms for scheduling and rate adaptation that enable low latency and high robustness despite the limitations of the receive-only ground stations. We evaluate L2D2 through a combination of trace-driven simulations and real-world satellite-ground station measurements. Our results demonstrate that L2D2's geographically distributed design can reduce data downlink latency from 90 minutes to 21 minutes.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3452296.3472932',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'CRISP: critical slice prefetching',\n",
       "  'authors': \"['Heiner Litz', 'Grant Ayers', 'Parthasarathy Ranganathan']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'The high access latency of DRAM continues to be a performance challenge for contemporary microprocessor systems. Prefetching is a well-established technique to address this problem, however, existing implemented designs fail to provide any performance benefits in the presence of irregular memory access patterns. The hardware complexity of prior techniques that can predict irregular memory accesses such as runahead execution has proven untenable for implementation in real hardware. We propose a lightweight mechanism to hide the high latency of irregular memory access patterns by leveraging criticality-based scheduling. In particular, our technique executes delinquent loads and their load slices as early as possible, hiding a significant fraction of their latency. Furthermore, we observe that the latency induced by branch mispredictions and other high latency instructions can be hidden with a similar approach. Our proposal only requires minimal hardware modifications by performing memory access classification, load and branch slice extraction, as well as priority analysis exclusively in software. As a result, our technique is feasible to implement, introducing only a simple new instruction prefix while requiring minimal modifications of the instruction scheduler. Our technique increases the IPC of memory-latency-bound applications by up to 38% and by 8.4% on average.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503222.3507745',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'AGAPE: anomaly detection with generative adversarial network for improved performance, energy, and security in manycore systems',\n",
       "  'authors': \"['Ke Wang', 'Hao Zheng', 'Yuan Li', 'Jiajun Li', 'Ahmed Louri']\",\n",
       "  'date': 'March 2022',\n",
       "  'source': \"DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe\",\n",
       "  'abstract': 'The security of manycore systems has become increasingly critical. In system-on-chips (SoCs), Hardware Trojans (HTs) manipulate the functionalities of the routing components to saturate the on-chip network, degrade performance, and result in the leakage of sensitive data. Existing HT detection techniques, including runtime monitoring and state-of-the-art learning-based methods, are unable to timely and accurately identify the implanted HTs, due to the increasingly dynamic and complex nature of on-chip communication behaviors. We propose AGAPE, a novel Generative Adversarial Network (GAN)-based anomaly detection and mitigation method against HTs for secured on-chip communication. AGAPE learns the distribution of the multivariate time series of a number of NoC attributes captured by on-chip sensors under both HT-free and HT-infected working conditions. The proposed GAN can learn the potential latent interactions among different runtime attributes concurrently, accurately distinguish abnormal attacked situations from normal SoC behaviors, and identify the type and location of the implanted HTs. Using the detection results, we apply the most suitable protection techniques to each type of detected HTs instead of simply isolating the entire HT-infected router, with the aim to mitigate security threats as well as reducing performance loss. Simulation results show that AGAPE enhances the HT detection accuracy by 19%, reduces network latency and power consumption by 39% and 30%, respectively, as compared to state-of-the-art security designs.',\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3539845.3540045',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Speculative taint tracking (STT): a comprehensive protection for speculatively accessed data',\n",
       "  'authors': \"['Jiyong Yu', 'Mengjia Yan', 'Artem Khyzha', 'Adam Morrison', 'Josep Torrellas', 'Christopher W. Fletcher']\",\n",
       "  'date': 'December 2021',\n",
       "  'source': 'Communications of the ACM',\n",
       "  'abstract': 'Speculative execution attacks present an enormous security threat, capable of reading arbitrary program data under malicious speculation, and later exfiltrating that data over microarchitectural covert channels. This paper proposes speculative taint tracking (STT), a high security and high performance hardware mechanism to block these attacks. The main idea is that it is safe to execute and selectively forward the results of speculative instructions that read secrets, as long as we can prove that the forwarded results do not reach potential covert channels. The technical core of the paper is a new abstraction to help identify all micro-architectural covert channels, and an architecture to quickly identify when a covert channel is no longer a threat. We further conduct a detailed formal analysis on the scheme in a companion document. When evaluated on SPEC06 workloads, STT incurs 8.5% or 14.5% performance overhead relative to an insecure machine.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3491201',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'GenSeq&#x002B;: A Scalable High-Performance Accelerator for Genome Sequencing',\n",
       "  'authors': \"['Chao Wang', 'Lei Gong', 'Shiming Lei', 'Haijie Fang', 'Xi Li', 'Aili Wang', 'Xuehai Zhou']\",\n",
       "  'date': 'None',\n",
       "  'source': 'IEEE/ACM Transactions on Computational Biology and Bioinformatics',\n",
       "  'abstract': 'Genome sequencing is one of the most challenging problems in computational biology and bioinformatics. As a traditional algorithm, the string match meets a challenge with the development of the massive volume of data because of gene sequencing. Surveys show that there will be a huge amount of short read segments during the process of gene sequencing and the need for a highly efficient is urgent. As a classic fast and exact single pattern matching algorithm, Knuth-Morris-Pratt (KMP) algorithm has been demonstrated in network security and computational biology. However, with the increasing amount of data in the modern society, it becomes increasingly important and essential to provide a High-performance implementation of KMP algorithm. In this article, we implement a scalable KMP accelerator based on FPGA, named GeneKMP. The accelerator is composed of different computing units to achieve a pipelined organization for higher throughput with satisfying scalability. A novel programming model is provided to alleviate the burden of the high-level programmers. We provide a greedy-based partitioning algorithm for the software/hardware design paradigms. Experimental results on the state-of-the-art Xilinx FPGA hardware prototype show that our accelerator can achieve up to a promising speedup with insignificant hardware cost and power consumption.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/TCBB.2019.2947059',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A Threat Model Analysis of a Mobile Agent-based system on Raspberry Pi',\n",
       "  'authors': \"['Iván García Aguilar', 'Antonio Muñoz Gallego']\",\n",
       "  'date': 'August 2021',\n",
       "  'source': \"ARES '21: Proceedings of the 16th International Conference on Availability, Reliability and Security\",\n",
       "  'abstract': 'Security is considered one of the critical points in any computer system. Nowadays, a multitude of protocols and computer models are appearing along with new attacks increasing the need to develop solutions. This work focuses on the protection of the agent as well as the information it processes in a distributed environment throughout the network. Mobile agents move between various network-enabled platforms to process the information they manage. To simulate an environment based on the Internet of Things (IoT), a scheme has been presented which details the necessary steps to be carried out by the agent to perform the migration.  Today it was proved that there is no infallible solution that guarantees the security of the whole system. However, the importance of security mechanisms to reduce and/or mitigate security threats is fundamental. This work is a study based on a mobile agent-based approach that travels from host to host. A review of different threats to this particular model is presented. Throughout this work a detailed study is presented based on the migration protocol of the agents, which will be determined by using modeling tools such as Microsoft Modeling Tool (MMT) used in this case, to discover and detail each of the threats presented by this protocol. Additionally, an alternative as a solution according to a protocol that runs thanks to the implementation of hardware elements is proposed, which makes use of a TPM, thus determining which threats are mitigated or solved by implementing such hardware in conjunction with the protocol developed for this purpose.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3465481.3470064',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'CacheRewinder: revoking speculative cache updates exploiting write-back buffer',\n",
       "  'authors': \"['Jongmin Lee', 'Junyeon Lee', 'Taeweon Suh', 'Gunjae Koo']\",\n",
       "  'date': 'March 2022',\n",
       "  'source': \"DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe\",\n",
       "  'abstract': 'Transient execution attacks are critical security threats since those attacks exploit speculative execution which is an essential architectural solution that can improve the performance of out-of-order processors significantly. Such attacks change cache state by accessing secret data during speculative executions, then the attackers leak the secret information exploiting cache timing side-channels. Even though software patches against transient execution attacks have been proposed, the software solutions significantly slow down the performance of a system. In this paper, we propose CacheRewinder, an efficient hardware-based defense mechanism against transient execution attacks. CacheRewinder prevents leakage of secret information by revoking the cache updates done by speculative executions. To restore the cache state efficiently, CacheRewinder exploits the underutilized write-back buffer space as the temporary storage for victimized cache blocks evicted during speculative executions. Hence, when speculation fails CacheRewinder can quickly restore the cache state using the victim blocks held in the write-back buffer. Our evaluation exhibits CacheRewinder can effectively defend against transient execution attacks. The performance overhead by CacheRewinder is only 0.6%, which is negligible compared to the unprotected baseline processor. CacheRewinder also requires minimal storage cost since it exploits unused writeback buffer entries as storage for evicted cache blocks.',\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3539845.3539965',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'SILVerIn: Systematic Integrity Verification of Printed Circuit Board Using JTAG Infrastructure',\n",
       "  'authors': \"['Shubhra Deb Paul', 'Swarup Bhunia']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Journal on Emerging Technologies in Computing Systems',\n",
       "  'abstract': 'A printed circuit board (PCB) provides necessary mechanical support to an electronic system and acts as a platform for connecting electronic components. Counterfeiting and in-field tampering of PCBs have become significant security concerns in the semiconductor industry as a result of increasing untrusted entities in the supply chain. These counterfeit components may result in performance degradation, profit reduction, and reputation risk for the manufacturers. While Integrated Circuit (IC) level authentication using physical unclonable functions (PUFs) has been widely investigated, countermeasures at the PCB level are scarce. These approaches either suffer from significant overhead issues, or opportunistic counterfeiters can breach them like clockwork. Besides, they cannot be extended to system-level (both chip and PCB together), and their applications are also limited to a specific purpose (i.e., either counterfeiting or tampering). In this article, we introduce SILVerIn, a novel systematic approach to verify the authenticity of all chips used in a PCB as well as the board for combating attacks such as counterfeiting, cloning, and in-field malicious modifications. We develop this approach by utilizing the existing boundary scan architecture (BSA) of modern ICs and PCBs. As a result, its implementation comes at a negligible (∼0.5%) hardware overhead. SILVerIn\\xa0is integrated into a PCB design during the manufacturing phase. We implement our technique on a custom hardware platform consisting of an FPGA and a microcontroller. We incorporate the industry-standard JTAG (Joint Test Action Group) interface to transmit test data into the BSA and perform hands-on measurement of supply current at both chip and PCB levels on 20 boards. We reconstruct these current values to digital signatures that exhibit high uniqueness, robustness, and randomness features. Our approach manifests strong reproducibility of signatures at different supply voltage levels, even with a low-resolution measurement setup. SILVerIn\\xa0also demonstrates a high resilience against machine learning-based modeling attacks, with an average prediction accuracy of ∼51%. Finally, we conduct intentional alteration experiments by replacing the on-board FPGA to replicate the scenario of PCB tampering, and the results indicate successful detection of in-field modifications in a PCB.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3460232',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Defining an open source CAD workflow for experimental music and media arts',\n",
       "  'authors': \"['Nicolò Merendino', 'Antonio Rodà']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': 'ARTECH 2021: 10th International Conference on Digital and Interactive Arts',\n",
       "  'abstract': 'The practice of designing and building instruments, interfaces and hardware in general, became a crucial part of contemporary audio and media arts productions. This task could benefit from the high performance tools offered by state of the art Open source Computer Aided Design (CAD). Although these applications have reached a good level of maturity, their use in the artistic field is still not so widespread, due to an initial barrier probably caused by a lack of accessible documentation and best practices.  This article aims to analyse and experiment a variety of open source Computer Aided Design (CAD) applications with the goal of further spread the use of open source CAD software among media artists, designers,and researchers in the field of STEAM (Science, Technology, Engineering, Art, Mathematics) applications[17]. Following a research through design approach, we will provide up date guidelines regarding how to design every aspect of an electronic music interface using exclusively open source software. To represent the various topics of our research we defined a hypothetical electronic device, and the workflow will be illustrated by describing and analysing all the steps that is necessary to cover in order to to bring such object from a breadboard overloaded with wires and components to a more stable and reliable prototype.  Open source software can play an important role in terms of democratization and long term sustainability of many initiatives [7], and this article aims to help a vast number of workers and researchers in the field of sound and media art to embrace those virtuous software in their artistic practices.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3483529.3483715',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Satori: efficient and fair resource partitioning by sacrificing short-term benefits for long-term gains',\n",
       "  'authors': \"['Rohan Basu Roy', 'Tirthak Patel', 'Devesh Tiwari']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': 'Multi-core architectures have enabled data centers to increasingly co-locate multiple jobs to improve resource utilization and lower the operational cost. Unfortunately, naively co-locating multiple jobs may lead to only a modest increase in system throughput. Worse, some users may observe proportionally higher performance degradation compared to other users co-located on the same physical multi-core system. SATORI is a novel strategy to partition multi-core architectural resources to achieve two conflicting goals simultaneously: increasing system throughput and achieving fairness among the co-located jobs.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00031',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': '(Mis)managed: A Novel TLB-based Covert Channel on GPUs',\n",
       "  'authors': \"['Ajay Nayak', 'Pratheek B.', 'Vinod Ganapathy', 'Arkaprava Basu']\",\n",
       "  'date': 'May 2021',\n",
       "  'source': \"ASIA CCS '21: Proceedings of the 2021 ACM Asia Conference on Computer and Communications Security\",\n",
       "  'abstract': \"GPUs are now commonly available in most modern computing platforms. They are increasingly being adopted in cloud platforms and data centers due to their immense computing capability. In response to this growth in usage, manufacturers continuously try to improve GPU hardware by adding new features. However, this increase in usage and the addition of utility-improving features can create new, unexpected attack channels. In this paper, we show that two such features-unified virtual memory (UVM) and multi-process service (MPS)-primarily introduced to improve the programmability and efficiency of GPU kernels have an unexpected consequence-that of creating a novel covert-timing channel via the GPU's translation lookaside buffer (TLB) hierarchy. To enable this covert channel, we first perform experiments to understand the characteristics of TLBs present on a GPU. The use of UVM allows fine-grained management of translations, and helps us discover several idiosyncrasies of the TLB hierarchy, such as three-levels of TLB, coalesced entries. We use this newly-acquired understanding to demonstrate a novel covert channel via the shared TLB. We then leverage MPS to increase the bandwidth of this channel by 40×. Finally, we demonstrate the channel's utility by leaking data from a GPU-accelerated database application.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3433210.3453077',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'HEART: Hybrid Memory and Energy-Aware Real-Time Scheduling for Multi-Processor Systems',\n",
       "  'authors': \"['Mario Günzel', 'Christian Hakert', 'Kuan-Hsun Chen', 'Jian-Jia Chen']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Embedded Computing Systems',\n",
       "  'abstract': 'Dynamic power management (DPM) reduces the power consumption of a computing system when it idles, by switching the system into a low power state for hibernation. When all processors in the system share the same component, e.g., a shared memory, powering off this component during hibernation is only possible when all processors idle at the same time. For a real-time system, the schedulability property has to be guaranteed on every processor, especially if idle intervals are considered to be actively introduced.In this work, we consider real-time systems with hybrid shared-memory architectures, which consist of shared volatile memory (VM) and non-volatile memory (NVM). Energy-efficient execution is achieved by applying DPM to turn off all memories during the hibernation mode. Towards this, we first explore the hybrid memory architectures and suggest a task model, which features configurable hibernation overheads. We propose a multi-processor procrastination algorithm (HEART), based on partitioned earliest-deadline-first (pEDF) scheduling. Our algorithm facilitates reducing the energy consumption by actively enlarging the hibernation time. It enforces all processors to idle simultaneously without violating the schedulability condition, such that the system can enter the hibernation state, where shared memories are turned off. Throughout extensive evaluation of HEART, we demonstrate (1) the increase in potential hibernation time, respectively the decrease in energy consumption, and (2)\\xa0that our algorithm is not only more general but also has better performance than the state of the art with respect to energy efficiency in most cases.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3477019',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Maximizing Persistent Memory Bandwidth Utilization for OLAP Workloads',\n",
       "  'authors': \"['Björn Daase', 'Lars Jonas Bollmeier', 'Lawrence Benson', 'Tilmann Rabl']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data\",\n",
       "  'abstract': \"Modern database systems for online analytical processing (OLAP) typically rely on in-memory processing. Keeping all active data in DRAM severely limits the data capacity and makes larger deployments much more expensive than disk-based alternatives. Byte-addressable persistent memory (PMEM) is an emerging storage technology that bridges the gap between slow-but-cheap SSDs and fast-but-expensive DRAM. Thus, research and industry have identified it as a promising alternative to pure in-memory data warehouses. However, recent work shows that PMEM's performance is strongly dependent on access patterns and does not always yield good results when simply treated like DRAM. To characterize PMEM's behavior in OLAP workloads, we systematically evaluate PMEM on a large, multi-socket server commonly used for OLAP workloads. Our evaluation shows that PMEM can be treated like DRAM for most read access but must be used differently when writing. To support our findings, we run the Star Schema Benchmark on PMEM and DRAM. We show that PMEM is suitable for large, read-heavy OLAP workloads with an average query runtime slowdown of 1.66x compared to DRAM. Following our evaluation, we present 7 best practices on how to maximize PMEM's bandwidth utilization in future system designs.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3448016.3457292',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Yashme: detecting persistency races',\n",
       "  'authors': \"['Hamed Gorjiara', 'Guoqing Harry Xu', 'Brian Demsky']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'Persistent memory (PM) or Non-Volatile Random-Access Memory (NVRAM) hardware such as Intel’s Optane memory product promises to transform how programs store and manipulate information. Ensuring that persistent memory programs are crash consistent is a major challenge. We present a novel class of crash consistency bugs for persistent memory programs, which we call persistency races. Persistency races can cause non-atomic stores to be made partially persistent. Persistency races arise due to the interaction of standard compiler optimizations with persistent memory semantics.   We present Yashme, the first detector for persistency races. A major challenge is that in order to detect persistency races, the execution must crash in a very narrow window between a store with a persistency race and its corresponding cache flush operation, making it challenging for naïve techniques to be effective. Yashme overcomes this challenge with a novel technique for detecting races in executions that are prefixes of the pre-crash execution. This technique enables Yashme to effectively find persistency races even if the injected crashes do not fall into that window. We have evaluated Yashme on a range of persistent memory benchmarks and have found 26 real persistency races that have never been reported before.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503222.3507766',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Short Paper: A Quantum Circuit Obfuscation Methodology for Security and Privacy',\n",
       "  'authors': \"['Aakarshitha Suresh', 'Abdullah Ash Saki', 'Mahababul Alam', 'Rasit Onur Topaloglu', 'Swaroop Ghosh']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"HASP '21: Proceedings of the 10th International Workshop on Hardware and Architectural Support for Security and Privacy\",\n",
       "  'abstract': 'In the Noisy Intermediate-Scale Quantum (NISQ) realm, efficient quantum circuit compilation is critical to ensure successful computation. Several third-party compilers are improving the compilation times and depth/gate counts. Untrusted third parties or a particular version of a trusted compiler may allow an attacker to steal, clone, and/or reverse engineer the quantum circuit. We propose to obfuscate quantum circuits by employing dummy CNOT gates to prevent such threats. If the adversary clones the obfuscated design, he/she will get faulty results. We propose a metric-based dummy gate insertion process to ensure maximum corruption of functionality measured using Total Variation Distance (TVD) and validated using IBM’s noisy simulators. Our metric guided dummy gate insertion process achieves TVD of up to 28.83%, and performs 10.14% better than the average TVD and performs within 12.45% of the best obtainable TVD for the test benchmarks. The removal of dummy gates by the designer post-compilation to restore functionality as well as other finer details have been addressed.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3505253.3505260',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Towards All-optical Stochastic Computing Using Photonic Crystal Nanocavities',\n",
       "  'authors': \"['Hassnaa El-Derhalli', 'Léa Constans', 'Sébastien Le Beux', 'Alfredo De Rossi', 'Fabrice Raineri', 'Sofiène Tahar']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Journal on Emerging Technologies in Computing Systems',\n",
       "  'abstract': 'Stochastic computing allows a drastic reduction in hardware complexity using serial processing of bit streams. While the induced high computing latency can be overcome using integrated optics technology, the design of realistic optical stochastic computing architectures calls for energy efficient switching devices. Photonics Crystal (PhC) nanocavities are\\xa0μm2\\xa0scale\\xa0devices offering 100fJ switching operation under picoseconds-scale switching speed. Fabrication process allows controlling the Quality factor of each nanocavity resonance, leading to opportunities to implement architectures involving cascaded gates and multi-wavelength signaling.\\xa0In this paper, we investigate the design of cascaded gates architecture using nanocavities in the context of stochastic computing.\\xa0We\\xa0propose a transmission model considering key nanocavity device parameters, such as Quality factors, resonance wavelength, and switching efficiency. The model is calibrated with experimental measurements.\\xa0We propose the design of XOR gate and multiplexer. We illustrate the use of the gates to design an edge detection filter. System-level exploration of laser power, bit-stream length and bit-error rate is carried out for the processing of gray-scale images. The results show that the proposed architecture leads to 8.5nJ/pixel energy consumption and 512ns/pixel processing time.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3484871',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Dynamic Reliability Management in Neuromorphic Computing',\n",
       "  'authors': \"['Shihao Song', 'Jui Hanamshet', 'Adarsha Balaji', 'Anup Das', 'Jeffrey L. Krichmar', 'Nikil D. Dutt', 'Nagarajan Kandasamy', 'Francky Catthoor']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Journal on Emerging Technologies in Computing Systems',\n",
       "  'abstract': 'Neuromorphic computing systems execute machine learning tasks designed with spiking neural networks. These systems are embracing non-volatile memory to implement high-density and low-energy synaptic storage. Elevated voltages and currents needed to operate non-volatile memories cause aging of CMOS-based transistors in each neuron and synapse circuit in the hardware, drifting the transistor’s parameters from their nominal values. If these circuits are used continuously for too long, the parameter drifts cannot be reversed, resulting in permanent degradation of circuit performance over time, eventually leading to hardware faults. Aggressive device scaling increases power density and temperature, which further accelerates the aging, challenging the reliable operation of neuromorphic systems. Existing reliability-oriented techniques periodically de-stress all neuron and synapse circuits in the hardware at fixed intervals, assuming worst-case operating conditions, without actually tracking their aging at run-time. To de-stress these circuits, normal operation must be interrupted, which introduces latency in spike generation and propagation, impacting the inter-spike interval and hence, performance (e.g., accuracy). We observe that in contrast to long-term aging, which permanently damages the hardware, short-term aging in scaled CMOS transistors is mostly due to bias temperature instability. The latter is heavily workload-dependent and, more importantly, partially reversible. We propose a new architectural technique to mitigate the aging-related reliability problems in neuromorphic systems by designing an intelligent run-time manager (NCRTM), which dynamically de-stresses neuron and synapse circuits in response to the short-term aging in their CMOS transistors during the execution of machine learning workloads, with the objective of meeting a reliability target. NCRTM de-stresses these circuits only when it is absolutely necessary to do so, otherwise reducing the performance impact by scheduling de-stress operations off the critical path. We evaluate NCRTM with state-of-the-art machine learning workloads on a neuromorphic hardware. Our results demonstrate that NCRTM significantly improves the reliability of neuromorphic hardware, with marginal impact on performance.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3462330',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Spiking Neural Networks in Spintronic Computational RAM',\n",
       "  'authors': \"['Hüsrev Cılasun', 'Salonik Resch', 'Zamshed I. Chowdhury', 'Erin Olson', 'Masoud Zabihi', 'Zhengyang Zhao', 'Thomas Peterson', 'Keshab K. Parhi', 'Jian-Ping Wang', 'Sachin S. Sapatnekar', 'Ulya R. Karpuzcu']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Architecture and Code Optimization',\n",
       "  'abstract': 'Spiking Neural Networks (SNNs) represent a biologically inspired computation model capable of emulating neural computation in human brain and brain-like structures. The main promise is very low energy consumption. Classic Von Neumann architecture based SNN accelerators in hardware, however, often fall short of addressing demanding computation and data transfer requirements efficiently at scale. In this article, we propose a promising alternative to overcome scalability limitations, based on a network of in-memory SNN accelerators, which can reduce the energy consumption by up to 150.25= when compared to a representative ASIC solution. The significant reduction in energy comes from two key aspects of the hardware design to minimize data communication overheads: (1) each node represents an in-memory SNN accelerator based on a spintronic Computational RAM array, and (2) a novel, De Bruijn graph based architecture establishes the SNN array connectivity.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3475963',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A pluggable vector unit for RISC-V vector extension',\n",
       "  'authors': \"['Vincenzo Maisto', 'Alessandro Cilardo']\",\n",
       "  'date': 'March 2022',\n",
       "  'source': \"DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe\",\n",
       "  'abstract': 'Vector extensions have become increasingly important for accelerating data-parallel applications in areas like multimedia, data-streaming, and Machine Learning. This interactive presentation introduces a microarchitectural design of a vector unit compliant with the RISC-V vector extension v1.0. While we targeted a specific core for demonstration, CVA6, our architecture is designed so as to ensure extensibility, maintainability, and re-usability in other cores. Furthermore, as a distinctive feature, we support speculative execution and precise vector traps. The paper provides an overview of the main motivation, design choices, and implementation details, followed by a qualitative and quantitative discussion of the results collected from the synthesis of the extended CVA6 RISC-V core.',\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3539845.3540113',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Systolic-Array Deep-Learning Acceleration Exploring Pattern-Indexed Coordinate-Assisted Sparsity for Real-Time On-Device Speech Processing',\n",
       "  'authors': \"['Shiwei Liu', 'Zihao Zhao', 'Yanhong Wang', 'Qiaosha Zou', 'Yiyun Zhang', 'C- J. Richard Shi']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"GLSVLSI '21: Proceedings of the 2021 on Great Lakes Symposium on VLSI\",\n",
       "  'abstract': 'This paper presents a hardware-software co-design for efficient sparse deep neural networks (DNNs) implementation in a regular systolic array for real-time on-device speech processing. The weight pruning format, exploring pattern-based coordinate-assisted (PICA) sparsity, expands the pattern-based pruning into both convolutional neural networks (CNNs) and recurrent neural networks (RNNs). It reduces the index storage overhead as well as avoids accuracy degradation. The proposed systolic accelerator leverages the intrinsic data reuse and locality to accommodate the PICA-based sparsity without using complex data distribution networks. It also supports DNNs with different topologies. By reducing the model size by 16x, PICA sparsification reduces 6.02x index storage overhead while still achieving 20.7% WER in TIMIT dataset. For the pruned WaveNet and LSTM, the accelerator achieves 0.62 and 2.69 TOPS/W energy efficiency, 1.7x to 10x higher than the state-of-the-art.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3453688.3461530',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'SRAM has no chill: exploiting power domain separation to steal on-chip secrets',\n",
       "  'authors': \"['Jubayer Mahmod', 'Matthew Hicks']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': \"The abundance of embedded systems and smart devices increases the risk of physical memory disclosure attacks. One such classic non-invasive attack exploits dynamic RAM's temperature-dependent ability to retain information across power cycles---known as a cold boot attack. When exposed to low temperatures, DRAM cells preserve their state for a short time without power, mimicking non-volatile memories in that time frame. Attackers exploit this physical phenomenon to gain access to a system's secrets, leading to data theft from encrypted storage. To prevent cold boot attacks, programmers hide secrets on-chip in Static Random-Access Memory (SRAM); by construction, on-chip SRAM is isolated from external probing and has little intrinsic capacitance, making it robust against cold boot attacks.   While it is the case that SRAM protects against traditional cold boot attacks, we show that there is another way to retain information in on-chip SRAM across power cycles and software changes. This paper presents Volt Boot, an attack that demonstrates a vulnerability of on-chip volatile memories due to the physical separation common to modern system-on-chip power distribution networks. Volt Boot leverages asymmetrical power states (e.g., on vs. off) to force SRAM state retention across power cycles, eliminating the need for traditional cold boot attack enablers, such as low-temperature or intrinsic data retention time. Using several modern ARM Cortex-A devices, we demonstrate the effectiveness of the attack in caches, registers, and iRAMs. Unlike other forms of SRAM data retention attacks, Volt Boot retrieves data with 100% accuracy---without any complex post-processing.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503222.3507710',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Invenio: Communication Affinity Computation for Low-Latency Microservices',\n",
       "  'authors': \"['Amit Sheoran', 'Sonia Fahmy', 'Puneet Sharma', 'Navin Modi']\",\n",
       "  'date': 'December 2021',\n",
       "  'source': \"ANCS '21: Proceedings of the Symposium on Architectures for Networking and Communications Systems\",\n",
       "  'abstract': \"Microservices enable rapid service deployment and scaling. Integrating poorly-understood microservice components into Service Function Chains (SFCs) or graphs limits a provider's control over service delivery latency, however. Orchestration frameworks currently instantiate and place myriads of microservice components without knowing the impact of placement decisions on latency. In this paper, we explore challenges that service providers encounter in managing complex SFCs, and propose Invenio to empower providers to effectively place microservices without prior knowledge of service functionality. Invenio correlates user actions with procedure messages in network traces, and computes procedural affinity of communication among microservices for each user action. The procedural affinity values can then be used to make placement decisions to meet latency constraints of individual user actions. Our experiments with two microservice-based cellular network implementations demonstrate that placement with Invenio-computed affinity values significantly reduces failures by bounding message processing latency, resulting in up to 21% performance gain compared to message count-based placement algorithms, and up to 51% gain over default placement.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3493425.3502750',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Trusted-DNN: A TrustZone-based Adaptive Isolation Strategy for Deep Neural Networks',\n",
       "  'authors': \"['Zhuang Liu', 'Ye Lu', 'Xueshuo Xie', 'Yaozheng Fang', 'Zhaolong Jian', 'Tao Li']\",\n",
       "  'date': 'July 2021',\n",
       "  'source': \"ACM TURC '21: Proceedings of the ACM Turing Award Celebration Conference - China\",\n",
       "  'abstract': 'Deep neural network (DNN) models have been widely deployed on embedded and mobile devices in lots of application fields such as health care, face recognition, driver assistance, etc. These applications usually require privacy or trusted computing protection. However, diverse hardware resources, various transport protocols, and limited computation and storage capacity make it challenging for traditional embedded systems to provide complex security protection mechanism oriented DNN models. To meet the challenges, we propose Trusted-DNN, a TrustZone-based adaptive isolation strategy for DNN models. We first design a normal pattern to exploit TrustZone technology to provide overall protection for running DNNs. To deploy arbitrary DNN models into TrustZone, we then develop a dynamic model partition method, which makes our strategy easily adaptive to various DNN models and devices. Finally, we employ several optimization techniques to reduce the inference latency of Trusted-DNN models. We perform AlexNet on OP-TEE, which is a TrustZone-based secure operating system, based on a Raspberry Pi 3B+ board. The extensive experimental results highlight that the optimized Trusted-DNN can reduce memory footprint by up to 98% compared with the ordinary program and Trusted-DNN only increase inference latency by 22.8%. Our code is available at https://gitee.com/PaintZero/alexnet-tee.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3472634.3472652',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Rhythmic pixel regions: multi-resolution visual sensing system towards high-precision visual computing at low power',\n",
       "  'authors': \"['Venkatesh Kodukula', 'Alexander Shearer', 'Van Nguyen', 'Srinivas Lingutla', 'Yifei Liu', 'Robert LiKamWa']\",\n",
       "  'date': 'April 2021',\n",
       "  'source': \"ASPLOS '21: Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'High spatiotemporal resolution can offer high precision for vision applications, which is particularly useful to capture the nuances of visual features, such as for augmented reality. Unfortunately, capturing and processing high spatiotemporal visual frames generates energy-expensive memory traffic. On the other hand, low resolution frames can reduce pixel memory throughput, but reduce also the opportunities of high-precision visual sensing. However, our intuition is that not all parts of the scene need to be captured at a uniform resolution. Selectively and opportunistically reducing resolution for different regions of image frames can yield high-precision visual computing at energy-efficient memory data rates.   To this end, we develop a visual sensing pipeline architecture that flexibly allows application developers to dynamically adapt the spatial resolution and update rate of different \"rhythmic pixel regions\" in the scene. We develop a system that ingests pixel streams from commercial image sensors with their standard raster-scan pixel read-out patterns, but only encodes relevant pixels prior to storing them in the memory. We also present streaming hardware to decode the stored rhythmic pixel region stream into traditional frame-based representations to feed into standard computer vision algorithms. We integrate our encoding and decoding hardware modules into existing video pipelines. On top of this, we develop runtime support allowing developers to flexibly specify the region labels. Evaluating our system on a Xilinx FPGA platform over three vision workloads shows 43-64% reduction in interface traffic and memory footprint, while providing controllable task accuracy.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3445814.3446737',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'High-Performance Sparse Linear Algebra on HBM-Equipped FPGAs Using HLS: A Case Study on SpMV',\n",
       "  'authors': \"['Yixiao Du', 'Yuwei Hu', 'Zhongchun Zhou', 'Zhiru Zhang']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"FPGA '22: Proceedings of the 2022 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays\",\n",
       "  'abstract': 'Sparse linear algebra operators are memory bound due to low compute to memory access ratio and irregular data access patterns. The exceptional bandwidth improvement provided by the emerging high-bandwidth memory (HBM) technologies, coupled with the ability of FPGAs to customize the memory hierarchy and compute engines, brings the potential to significantly boost the performance of sparse linear algebra operators. In this paper we identify four challenges when developing high-performance sparse linear algebra accelerators on HBM-equipped FPGAs --- low HBM bandwidth utilization with conventional sparse storage, limited on-chip memory capacity being the bottleneck when scaling to multiple HBM channels, low compute occupancy due to bank conflicts and inter-iteration carried dependencies, and timing closure on multi-die heterogeneous fabrics. We conduct an in-depth case study on sparse matrix-vector multiplication (SpMV) to explore techniques that tackle the four challenges. These techniques include (1) a customized sparse matrix format tailored for HBMs, (2) a scalable on-chip buffer design that combines replication and banking, (3) best practices of using HLS to implement hardware modules that dynamically resolve bank conflicts and carried dependencies for achieving high compute occupancy, and (4) a split-kernel design methodology for frequency optimization. Using the techniques, we demonstrate HiSparse, a high-performance SpMV accelerator on a multi-die HBM-equipped FPGA device. We evaluated HiSparse on a variety of matrix datasets. The results show that HiSparse achieves a high frequency and delivers promising speedup with increased bandwidth efficiency when compared to prior arts on CPUs, GPUs, and FPGAs. HiSparse is available at https://github.com/cornell-zhang/HiSparse.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3490422.3502368',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'KickTree: A Recursive Algorithmic Scheme for Packet Classification with Bounded Worst-Case Performance',\n",
       "  'authors': \"['Yao Xin', 'Yuxi Liu', 'Wenjun Li', 'Ruyi Yao', 'Yang Xu', 'Yi Wang']\",\n",
       "  'date': 'December 2021',\n",
       "  'source': \"ANCS '21: Proceedings of the Symposium on Architectures for Networking and Communications Systems\",\n",
       "  'abstract': 'As a promising alternative to TCAM-based solutions for packet classification, FPGA has received increasing attention. Although extensive research has been conducted in this area, existing FPGA-based packet classifiers cannot satisfy the burgeoning needs from OpenFlow, which demands large-scale rule sets and frequent rule updates. As a recently proposed hardware-specific approach, TabTree avoids rule replication and supports dynamic rule update. However, it still faces problems of unbalanced rule subset partition, unevenly distributed subtrees and excessive TSS leaf nodes when implemented on FPGA. In this paper, we propose a hardware-friendly packet classification approach called KickTree, which is elaborated by considering hardware properties. To take advantage of intrinsic parallelism of FPGA, KickTree adopts multiple balanced decision trees which can run simultaneously. The bit selection is more flexible which breaks the restriction of rule subset. Moreover, each subset size is strictly limited, leading to bounded and evenly-distributed',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3493425.3502752',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A Composable Monitoring System for Heterogeneous Embedded Platforms',\n",
       "  'authors': \"['Giacomo Valente', 'Tiziana Fanni', 'Carlo Sau', 'Tania Di Mascio', 'Luigi Pomante', 'Francesca Palumbo']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Embedded Computing Systems',\n",
       "  'abstract': 'Advanced computations on embedded devices are nowadays a must in any application field. Often, to cope with such a need, embedded systems designers leverage on complex heterogeneous reconfigurable platforms that offer high performance, thanks to the possibility of specializing/customizing some computing elements on board, and are usually flexible enough to be optimized at runtime. In this context, monitoring the system has gained increasing interest. Ideally, monitoring systems should be non-intrusive, serve several purposes, and provide aggregated information about the behavior of the different system components. However, current literature is not close to such ideality: For example, existing monitoring systems lack in being applicable to modern heterogeneous platforms. This work presents a hardware monitoring system that is intended to be minimally invasive on system performance and resources, composable, and capable of providing to the user homogeneous observability and transparent access to the different components of a heterogeneous computing platform, so system metrics can be easily computed from the aggregation of the collected information. Building on a previous work, this article is primarily focused on the extension of an existing hardware monitoring system to cover also specialized coprocessing units, and the assessment is done on a Xilinx FPGA-based System on Programmable Chip. Different explorations are presented to explain the level of customizability of the proposed hardware monitoring system, the tradeoffs available to the user, and the benefits with respect to standard de facto monitoring support made available by the targeted FPGA vendor.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3461647',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Flare: flexible in-network allreduce',\n",
       "  'authors': \"['Daniele De Sensi', 'Salvatore Di Girolamo', 'Saleh Ashkboos', 'Shigang Li', 'Torsten Hoefler']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"SC '21: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis\",\n",
       "  'abstract': 'The allreduce operation is one of the most commonly used communication routines in distributed applications. To improve its bandwidth and to reduce network traffic, this operation can be accelerated by offloading it to network switches, that aggregate the data received from the hosts, and send them back the aggregated result. However, existing solutions provide limited customization opportunities and might provide suboptimal performance when dealing with custom operators and data types, with sparse data, or when reproducibility of the aggregation is a concern. To deal with these problems, in this work we design a flexible programmable switch by using as a building block PsPIN, a RISC-V architecture implementing the sPIN programming model. We then design, model, and analyze different algorithms for executing the aggregation on this architecture, showing performance improvements compared to state-of-the-art approaches.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3458817.3476178',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Specializing FGPU for Persistent Deep Learning',\n",
       "  'authors': \"['Rui Ma', 'Jia-Ching Hsu', 'Tian Tan', 'Eriko Nurvitadhi', 'David Sheffield', 'Rob Pelt', 'Martin Langhammer', 'Jaewoong Sim', 'Aravind Dasu', 'Derek Chiou']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Reconfigurable Technology and Systems',\n",
       "  'abstract': 'Overlay architectures are a good way to enable fast development and debug on FPGAs at the expense of potentially limited performance compared to fully customized FPGA designs. When used in concert with hand-tuned FPGA solutions, performant overlay architectures can improve time-to-solution and thus overall productivity of FPGA solutions. This work tunes and specializes FGPU, an open source OpenCL-programmable GPU overlay for FPGAs. We demonstrate that our persistent deep learning (PDL)-FGPU architecture maintains the ease-of-programming and generality of GPU programming while achieving high performance from specialization for the persistent deep learning domain. We also propose an easy method to specialize for other domains. PDL-FGPU includes new instructions, along with micro-architecture and compiler enhancements. We evaluate both the FGPU baseline and the proposed PDL-FGPU on a modern high-end Intel Stratix 10 2800 FPGA in simulation running persistent DL applications (RNN, GRU, LSTM), and non-DL applications to demonstrate generality. PDL-FGPU requires 1.4–3× more ALMs, 4.4–6.4× more M20ks, and 1–9.5× more DSPs than baseline, but improves performance by 56–693× for PDL applications with an average 23.1% degradation on non-PDL applications. We integrated the PDL-FGPU overlay into Intel OPAE to measure real-world performance/power and demonstrate that PDL-FGPU is only 4.0–10.4× slower than the Nvidia V100.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3457886',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Analyzing Security Vulnerabilities Induced by High-level Synthesis',\n",
       "  'authors': \"['Nitin Pundir', 'Sohrab Aftabjahani', 'Rosario Cammarota', 'Mark Tehranipoor', 'Farimah Farahmandi']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Journal on Emerging Technologies in Computing Systems',\n",
       "  'abstract': 'High-level synthesis (HLS) is essential to map the high-level language (HLL) description (e.g., in C/C++) of hardware design to the corresponding Register Transfer Level (RTL) to produce hardware-independent design specifications with reduced design complexity for ASICs and FPGAs. Adopting HLS is crucial for industrial and government applications to lower development costs, verification efforts, and time-to-market. Current research practices focus on optimizing HLS for performance, power, and area constraints. However, the literature does not include an analysis of the security implications carried through HLS-generated RTL translations (e.g., from an untimed high-level sequential specification to a fully scheduled implementation). This article demonstrates the evidence of security vulnerabilities that emerge during the HLS translation of a high-level description of system-on-chip (SoC) intellectual properties to their corresponding RTL. The evidence provided in this manuscript highlights the need for (a) guidelines for high-level programmers to prevent these security issues at the design time and (b) automated HLS verification solutions that cover security in their optimization flow.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3492345',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'On the role of system software in energy management of neuromorphic computing',\n",
       "  'authors': \"['Twisha Titirsha', 'Shihao Song', 'Adarsha Balaji', 'Anup Das']\",\n",
       "  'date': 'May 2021',\n",
       "  'source': \"CF '21: Proceedings of the 18th ACM International Conference on Computing Frontiers\",\n",
       "  'abstract': 'Neuromorphic computing systems such as DYNAPs and Loihi have recently been introduced to the computing community to improve performance and energy efficiency of machine learning programs, especially those that are implemented using Spiking Neural Network (SNN). The role of a system software for neuromorphic systems is to cluster a large machine learning model (e.g., with many neurons and synapses) and map these clusters to the computing resources of the hardware. In this work, we formulate the energy consumption of a neuromorphic hardware, considering the power consumed by neurons and synapses, and the energy consumed in communicating spikes on the interconnect. Based on such formulation, we first evaluate the role of a system software in managing the energy consumption of neuromorphic systems. Next, we formulate a simple heuristic-based mapping approach to place the neurons and synapses onto the computing resources to reduce energy consumption. We evaluate our approach with 10 machine learning applications and demonstrate that the proposed mapping approach leads to a significant reduction of energy consumption of neuromorphic computing systems.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3457388.3458664',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Two ICS Security Datasets and Anomaly Detection Contest on the HIL-based Augmented ICS Testbed',\n",
       "  'authors': \"['Hyeok-Ki Shin', 'Woomyo Lee', 'Jeong-Han Yun', 'Byung-Gil Min']\",\n",
       "  'date': 'August 2021',\n",
       "  'source': \"CSET '21: Proceedings of the 14th Cyber Security Experimentation and Test Workshop\",\n",
       "  'abstract': 'Security datasets with various operating characteristics and abnormal situations of industrial control system (ICS) are essential to develop artificial intelligence (AI)-based control system security technology. In this study, we built a hardware-in-the-loop (HIL)-based augmented ICS (HAI) testbed and developed ICS security datasets. Here, we introduce the second dataset (HAI 21.03), which was developed with the user feedback of the first released version (HAI 20.07). All HAI datasets are publicly available at https://github.com/icsdataset/hai. HAI 21.03 was expanded by adding data points and normal/attack scenarios to HAI 20.07. We also held an AI-based anomaly detection contest (HAICon 2020) utilizing the HAI datasets developed so far, giving many AI researchers an opportunity to discuss and share ideas for ICS anomaly detection research. This paper presents the results of the HAICon 2020. The results of the top teams in the competition can be used as a performance comparison criterion when using HAI 21.03.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3474718.3474719',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Reservoir Computing Using Networks of CMOS Logic Gates',\n",
       "  'authors': \"['Heidi Komkov', 'Liam Pocher', 'Alessandro Restelli', 'Brian Hunt', 'Daniel Lanthrop']\",\n",
       "  'date': 'July 2021',\n",
       "  'source': 'ICONS 2021: International Conference on Neuromorphic Systems 2021',\n",
       "  'abstract': 'Reservoir computing is a brain-inspired architecture for machine learning, capable of rapid time series processing. A recurrent neural network called a reservoir is created, and a simple trained readout map is applied to the reservoir state. A real-world dynamical system at the edge of criticality can be used as a reservoir for information processing in place of a software model. In this paper we test the dynamics of a reservoir comprised of discrete digital logic chips on a printed circuit board. The logic gates run freely without a clock, exhibiting complex behavior that expands an input into a higher dimensional representation. By testing these circuits in a dataset-agnostic manner, we identify promising configurations for machine learning. We demonstrate that the reservoir circuit substantially improves the accuracy of a simple classifier on a noisy waveform classification machine learning task.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3477145.3477163',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'An instruction-level power and energy model for the rocket chip generator',\n",
       "  'authors': \"['Zhiping Wang', 'W. Rhett Davis']\",\n",
       "  'date': 'July 2021',\n",
       "  'source': \"ISLPED '21: Proceedings of the ACM/IEEE International Symposium on Low Power Electronics and Design\",\n",
       "  'abstract': 'As digital systems become more power and energy constrained, the need for optimizing these quantities early in the design process grows ever more important. Fast and accurate power and energy models are needed for complex hardware blocks, such as processor cores, in order to optimize systems that contain these blocks. Today accurate energy/power estimation can be achieved only after physical design is complete, which is too late to affect the system architecture. This paper demonstrates the development of a fast instruction-level model for the Rocket Chip Generator to facilitate power- and energy-efficient software optimization. We first discuss an event-based power modeling methodology which is the foundation of our model and is compatible with emerging power- and energy-modeling standards such as IEEE-2416. Detailed energy characterization for basic events is explained along with an evaluation of a model with and without cache-fill events. The validation results show that the proposed instruction-level power model achieves less than 3% error on simple C program benchmarks.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISLPED52811.2021.9502485',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Quantifying the design-space tradeoffs in autonomous drones',\n",
       "  'authors': \"['Ramyad Hadidi', 'Bahar Asgari', 'Sam Jijina', 'Adriana Amyette', 'Nima Shoghi', 'Hyesoon Kim']\",\n",
       "  'date': 'April 2021',\n",
       "  'source': \"ASPLOS '21: Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'With fully autonomous flight capabilities coupled with user-specific applications, drones, in particular quadcopter drones, are becoming prevalent solutions in myriad commercial and research contexts. However, autonomous drones must operate within constraints and design considerations that are quite different from any other compute-based agent. At any given time, a drone must arbitrate among its limited compute, energy, and electromechanical resources. Despite huge technological advances in this area, each of these problems has been approached in isolation and drone systems design-space tradeoffs are largely unknown. To address this knowledge gap, we formalize the fundamental drone subsystems and find how computations impact this design space. We present a design-space exploration of autonomous drone systems and quantify how we can provide productive solutions. As an example, we study widely used simultaneous localization and mapping (SLAM) on various platforms and demonstrate that optimizing SLAM on FPGA is more fruitful for the drones. Finally, to address the lack of publicly available experimental drones, we release our open-source drone that is customizable across the hardware-software stack.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3445814.3446721',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'SKT: a one-pass multi-sketch data analytics accelerator',\n",
       "  'authors': \"['Monica Chiosa', 'Thomas B. Preußer', 'Gustavo Alonso']\",\n",
       "  'date': 'None',\n",
       "  'source': 'Proceedings of the VLDB Endowment',\n",
       "  'abstract': \"Data analysts often need to characterize a data stream as a first step to its further processing. Some of the initial insights to be gained include, e.g., the cardinality of the data set and its frequency distribution. Such information is typically extracted by using sketch algorithms, now widely employed to process very large data sets in manageable space and in a single pass over the data. Often, analysts need more than one parameter to characterize the stream. However, computing multiple sketches becomes expensive even when using high-end CPUs. Exploiting the increasing adoption of hardware accelerators, this paper proposes SKT, an FPGA-based accelerator that can compute several sketches along with basic statistics (average, max, min, etc.) in a single pass over the data. SKT has been designed to characterize a data set by calculating its cardinality, its second frequency moment, and its frequency distribution. The design processes data streams coming either from PCIe or TCP/IP, and it is built to fit emerging cloud service architectures, such as Microsoft's Catapult or Amazon's AQUA. The paper explores the trade-offs of designing sketch algorithms on a spatial architecture and how to combine several sketch algorithms into a single design. The empirical evaluation shows how SKT on an FPGA offers a significant performance gain over high-end, server-class CPUs.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.14778/3476249.3476287',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Research on the Development of Key Technologies of Tactical Edge Cloud',\n",
       "  'authors': \"['Sicong Yu', 'Huiji Zheng', 'Yang Fan', 'Caihong Ma']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"ICISE '21: Proceedings of the 6th International Conference on Information Systems Engineering\",\n",
       "  'abstract': 'In view of the current situation that the traditional centralized cloud service architecture cannot meet the high requirements of tactical edge for data processing and storage, this paper introduces an emerging tactical cloud service model——tactical edge cloud, and summarizes the status quo of technology development around two key aspects: tactical edge cloud architecture design and computing offloading, and finally related problems to be solved are put forward to provide reference for the development of cloud service architecture of tactical edge.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503928.3508348',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Listen to Your Heart: Evaluation of the Cardiologic Ecosystem',\n",
       "  'authors': \"['Endres Puschner', 'Christoph Saatjohann', 'Markus Willing', 'Christian Dresen', 'Julia Köbe', 'Benjamin Rath', 'Christof Paar', 'Lars Eckardt', 'Uwe Haverkamp', 'Sebastian Schinzel']\",\n",
       "  'date': 'August 2021',\n",
       "  'source': \"ARES '21: Proceedings of the 16th International Conference on Availability, Reliability and Security\",\n",
       "  'abstract': 'Modern implantable cardiologic devices communicate via radio frequency techniques and nearby gateways to a backend server on the internet. Those implanted devices, gateways, and servers form an ecosystem of proprietary hardware and protocols that process sensitive medical data and is often vital for patients’ health.  This paper analyzes the security of this Ecosystem, from technical gateway aspects, via the programmer, to configure the implanted device, up to the processing of personal medical data from large cardiological device producers. Based on a real-world attacker model, we evaluated different devices and found several severe vulnerabilities. Furthermore, we could purchase a fully functional programmer for implantable cardiological devices, allowing us to re-program such devices or even induce electric shocks on untampered implanted devices.  Additionally, we sent several Art. 15 and Art. 20 GDPR inquiries to manufacturers of implantable cardiologic devices, revealing non-conforming processes and a lack of awareness about patients’ rights and companies’ obligations. This, and the fact that many vulnerabilities are still to be found after many vulnerability disclosures in recent years, present a worrying security state of the whole ecosystem.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3465481.3465753',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Loopback strategy for in-vehicle network processing in automotive gateway network on chip',\n",
       "  'authors': \"['Angela Gonzalez Mariño', 'Francesc Fons', 'Zhang Haigang', 'Juan Manuel Moreno Arostegui']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"NoCArc '21: Proceedings of the 14th International Workshop on Network on Chip Architectures\",\n",
       "  'abstract': 'In this work, authors introduce an innovative loopback strategy for In-Vehicle Network (IVN) processing in automotive gateway (GW) Network on Chip. The new proposed architecture is fully HW centric, and allows performing any IVN processing algorithms without intervention from the CPU. In essence, the loopback strategy allows for adapting the number of stages in the pipeline of the processing stage by betting on the centralization of the processing resources and recirculating frames from output to input when further stages are needed. It permits even to select to which stage to send them depending on the processing required, optimizing thus the number of stages traversed by frames and consequently reducing latency. The processing unit is built as a stack of parallel tasks with the required interconnection resources that allow performing any processing over any frame, and to handle several frames in parallel. With this architecture, the GW data path is fully adaptable per frame, optimizing latency and Quality of Service, allowing for fulfilling the high demanding requirements of future IVNs.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3477231.3490429',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Getting to the HART of the Matter: An Evaluation of Real-World Safety System OT/IT Interfaces, Attacks, and Countermeasures',\n",
       "  'authors': \"['Laura Tinnel', 'Mike Cochrane']\",\n",
       "  'date': 'August 2021',\n",
       "  'source': \"CSET '21: Proceedings of the 14th Cyber Security Experimentation and Test Workshop\",\n",
       "  'abstract': 'This paper discusses our experience evaluating attack paths and security controls in commonly used, real-world ICS safety system architectures. Specifically, we sought to determine if an SIS-mediated architecture could provide better protection against unauthorized and malicious safety instrument configuration changes than could a MUX-mediated architecture.  An assessment question-driven approach was layered on top of standard penetration assessment methods. Test cases were planned around the questions and a sample set of vendor products typically used in the oil and gas sector. Four systems were composed from different product subsets and were assessed using the test cases. We analyzed results from the four assessments to illuminate issues that existed regardless of system composition.  Analysis revealed recurring vulnerabilities that exist in all safety systems due to issues in the design of safety instruments and the HART protocol. We found that device-native hardware write-protections provide the best defense, followed by SIS write protections. We concluded that, when using SIS security controls, an SIS-mediated system can protect against unauthorized device reconfigurations better than can a MUX-based system. When SIS security controls are not used, there is no added security benefit.  We present lessons learned for ICS stakeholders and for people who are interested in conducting this kind of evaluation.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3474718.3474726',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Fragments of the Past: Curating Peer Support with Perpetrators of Domestic Violence',\n",
       "  'authors': \"['Rosanna Bellini', 'Alexander Wilson', 'Jan David Smeddinck']\",\n",
       "  'date': 'May 2021',\n",
       "  'source': \"CHI '21: Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems\",\n",
       "  'abstract': 'There is growing evidence that digital peer-support networks can have a positive influence on behaviour change and wellbeing outcomes for people who harm themselves and others. However, making and sustaining such networks are subject to ethical and pragmatic challenges, particularly for perpetrators of domestic violence whom pose unique risks when brought together. In this work we report on a ten-month study where we worked with six support workers and eighteen perpetrators in the design and deployment of Fragments of the Past; a socio-material system that connects audio messages with tangible artefacts. We share how crafting digitally-augmented artefacts - ‘fragments’ - of experiences of desisting from violence can translate messages for motivation and rapport between peers, without subjecting the process to risks inherent with direct inter-personal communication. These insights provide the basis for practical considerations for future network design with challenging populations.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3411764.3445611',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'SOSP: A SmartNIC-based Offloading Framework for Cloud Storage Pooling',\n",
       "  'authors': \"['Yan Mu', 'Kehan Yao', 'Yang Li', 'Zhiqiang Li', 'Tao Sun', 'Lu Lu', 'Jian He', 'Mingfei Huang']\",\n",
       "  'date': 'January 2022',\n",
       "  'source': \"icWCSN '22: Proceedings of the 2022 9th International Conference on Wireless Communication and Sensor Networks\",\n",
       "  'abstract': \"As Moore's Law is gradually reaching its limitation, traditional CPU-centric computing architecture cannot meet the ever growing computational requirements, especially in large distributed data centers. There is a growing consensus in the industry that the future architecture is a data-centric fabric which can better integrate the function of computation, storage and network. In order to reduce the pressure on CPUs in data centers and further facilitate the transformation of the entire architecture towards higher flexibility and efficiency, this paper presents a SmartNIC-based offloading framework for storage pooling (SOSP) by leveraging the functionality of SmartNICs to offload both the local and remote storage services from CPUs. Our experiments show that the SmartNIC-based solution SOSP can increase the I/O random Read/Write operation performance by around 20-25% and 13-15% respectively, and at the meantime, free up host CPU cores by saving 16.7% virtual cores compared to the solution without using SmartNICs.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3514105.3514110',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A Real-Time Deep Learning OFDM Receiver',\n",
       "  'authors': \"['Stefan Brennsteiner', 'Tughrul Arslan', 'John Thompson', 'Andrew McCormick']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Reconfigurable Technology and Systems',\n",
       "  'abstract': 'Machine learning in the physical layer of communication systems holds the potential to improve performance and simplify design methodology. Many algorithms have been proposed; however, the model complexity is often unfeasible for real-time deployment. The real-time processing capability of these systems has not been proven yet. In this work, we propose a novel, less complex, fully connected neural network to perform channel estimation and signal detection in an orthogonal frequency division multiplexing system. The memory requirement, which is often the bottleneck for fully connected neural networks, is reduced by ≈ 27 times by applying known compression techniques in a three-step training process. Extensive experiments were performed for pruning and quantizing the weights of the neural network detector. Additionally, Huffman encoding was used on the weights to further reduce memory requirements. Based on this approach, we propose the first field-programmable gate array based, real-time capable neural network accelerator, specifically designed to accelerate the orthogonal frequency division multiplexing detector workload. The accelerator is synthesized for a Xilinx RFSoC field-programmable gate array, uses small-batch processing to increase throughput, efficiently supports branching neural networks, and implements superscalar Huffman decoders.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3494049',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Demystifying memory access patterns of FPGA-based graph processing accelerators',\n",
       "  'authors': \"['Jonas Dann', 'Daniel Ritter', 'Holger Fröning']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"GRADES-NDA '21: Proceedings of the 4th ACM SIGMOD Joint International Workshop on Graph Data Management Experiences &amp; Systems (GRADES) and Network Data Analytics (NDA)\",\n",
       "  'abstract': 'Recent advances in reprogrammable hardware (e. g., FPGAs) and memory technology (e. g., DDR4, HBM) promise to solve performance problems inherent to graph processing like irregular memory access patterns on traditional hardware (e. g., CPU). While several of these graph accelerators were proposed in recent years, it remains difficult to assess their performance and compare them on common graph workloads and accelerator platforms, due to few open source implementations and excessive implementation effort. In this work, we build on a simulation environment for graph processing accelerators, to make several existing accelerator approaches comparable. This allows us to study relevant performance dimensions such as partitioning schemes and memory technology, among others. The evaluation yields insights into the strengths and weaknesses of current graph processing accelerators along these dimensions, and features a novel in-depth comparison.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3461837.3464512',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A Guideline on Pseudorandom Number Generation (PRNG) in the IoT',\n",
       "  'authors': \"['Peter Kietzmann', 'Thomas C. Schmidt', 'Matthias Wählisch']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Computing Surveys',\n",
       "  'abstract': 'Random numbers are an essential input to many functions on the Internet of Things (IoT). Common use cases of randomness range from low-level packet transmission to advanced algorithms of artificial intelligence as well as security and trust, which heavily rely on unpredictable random sources. In the constrained IoT, though, unpredictable random sources are a challenging desire due to limited resources, deterministic real-time operations, and frequent lack of a user interface.In this article, we revisit the generation of randomness from the perspective of an IoT operating system (OS) that needs to support general purpose or crypto-secure random numbers. We analyze the potential attack surface, derive common requirements, and discuss the potentials and shortcomings of current IoT OSs. A systematic evaluation of current IoT hardware components and popular software generators based on well-established test suits and on experiments for measuring performance give rise to a set of clear recommendations on how to build such a random subsystem and which generators to use.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3453159',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Efficient Fruit and Vegetable Classification and Counting for Retail Applications Using Deep Learning',\n",
       "  'authors': \"['Kirill Bogomasov', 'Stefan Conrad']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"ICAAI '21: Proceedings of the 5th International Conference on Advances in Artificial Intelligence\",\n",
       "  'abstract': 'The process of manual classification and counting of fruits and vegetables, from the moment the customer places items on the conveyor belt to their weighing by the cashier on the checkout scale is time consuming and may be burdensome for cashiers, who need to look up or remember the identification code for each product. Not any more: We built a real-life application, which is capable of doing both tasks simultaneously. The presented research is focused on a case that is attractive for its practical applications, in which data is expanded by product weight information. We approach the problem as that of estimating the object count as a classification task and evade the more resource consuming object detection. We introduce a new hybrid architecture which is an ensemble of EfficientNet [31] for image classification and a Decision Tree [3] for object counting based on weight and previous classification result. The trained architecture provides accurate object count and requires fewer resources and less time than current object detection architectures. The proposed architecture accomplishes a counting accuracy of around 80% and an inference time of 0.2 sec. per image. It is a good candidate for handling huge amount of visual information involving fast processing on a CPU.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3505711.3505720',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Tolerating Defects in Low-Power Neural Network Accelerators Via Retraining-Free Weight Approximation',\n",
       "  'authors': \"['Fateme S. Hosseini', 'Fanruo Meng', 'Chengmo Yang', 'Wujie Wen', 'Rosario Cammarota']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Embedded Computing Systems',\n",
       "  'abstract': 'Hardware accelerators are essential to the accommodation of ever-increasing Deep Neural Network (DNN) workloads on the resource-constrained embedded devices. While accelerators facilitate fast and energy-efficient DNN operations, their accuracy is threatened by faults in their on-chip and off-chip memories, where millions of DNN weights are held. The use of emerging Non-Volatile Memories (NVM) further exposes DNN accelerators to a non-negligible rate of permanent defects due to immature fabrication, limited endurance, and aging. To tolerate defects in NVM-based DNN accelerators, previous work either requires extra redundancy in hardware or performs defect-aware retraining, imposing significant overhead. In comparison, this paper proposes a set of algorithms that exploit the flexibility in setting the fault-free bits in weight memory to effectively approximate weight values, so as to mitigate defect-induced accuracy drop. These algorithms can be applied as a one-step solution when loading the weights to embedded devices. They only require trivial hardware support and impose negligible run-time overhead. Experiments on popular DNN models show that the proposed techniques successfully boost inference accuracy even in the face of elevated defect rates in the weight memory.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3477016',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'High-throughput Near-Memory Processing on CNNs with 3D HBM-like Memory',\n",
       "  'authors': \"['Naebeom Park', 'Sungju Ryu', 'Jaeha Kung', 'Jae-Joon Kim']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Design Automation of Electronic Systems',\n",
       "  'abstract': 'This article discusses the high-performance near-memory neural network (NN) accelerator architecture utilizing the logic die in three-dimensional (3D) High Bandwidth Memory– (HBM) like memory. As most of the previously reported 3D memory-based near-memory NN accelerator designs used the Hybrid Memory Cube (HMC) memory, we first focus on identifying the key differences between HBM and HMC in terms of near-memory NN accelerator design. One of the major differences between the two 3D memories is that HBM has the centralized through-silicon-via (TSV) channels while HMC has distributed TSV channels for separate vaults. Based on the observation, we introduce the Round-Robin Data Fetching and Groupwise Broadcast schemes to exploit the centralized TSV channels for improvement of the data feeding rate for the processing elements. Using synthesized designs in a 28-nm CMOS technology, performance and energy consumption of the proposed architectures with various dataflow models are evaluated. Experimental results show that the proposed schemes reduce the runtime by 16.4–39.3% on average and the energy consumption by 2.1–5.1% on average compared to conventional data fetching schemes.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3460971',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Automation of Domain-specific FPGA-IP Generation and Test',\n",
       "  'authors': \"['Yuya Nakazato', 'Motoki Amagasaki', 'Qian Zhao', 'Masahiro Iida', 'Morihiro Kuga']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"HEART '21: Proceedings of the 11th International Symposium on Highly Efficient Accelerators and Reconfigurable Technologies\",\n",
       "  'abstract': 'Multi-access edge computing (MEC) devices that perform processing between the edge and cloud are becoming important in the Internet of Things infrastructure. MEC devices are designed to reduce the load on the edge devices, ensure real-time performance, and reduce the communication traffic between the edge and cloud. In this paper, to enable high-performance and low-power hardware-accelerated processing for different application domains in MEC devices, we propose an automated flow for domain-specific field-programmable gate array intellectual property core (FPGA-IP) generation and testing. First, we perform logic cell exploration using a target user application to find the optimal scalable logic module (SLM) structure, and use the optimal SLM instead of a lookup table to reduce the logic area. Second, we perform routing and FPGA array exploration to determine other FPGA-IP architecture parameters. Finally, the proposed flow uses the explored parameters to automatically generate the entire FPGA-IP and LSI test bitstreams. In a case study, we optimized an FPGA-IP for a differential privacy encryption circuit using the proposed flow. We implemented and evaluated the FPGA-IP with a 55nm TEG chip design. Furthermore, the simulation-based LSI test showed that 100% of the stuck-at faults in the routing paths of the FPGA-IP were detected.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3468044.3468048',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Extending C++ for Heterogeneous Quantum-Classical Computing',\n",
       "  'authors': \"['Alexander Mccaskey', 'Thien Nguyen', 'Anthony Santana', 'Daniel Claudino', 'Tyler Kharazi', 'Hal Finkel']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Quantum Computing',\n",
       "  'abstract': 'We present qcor—a language extension to C++ and compiler implementation that enables heterogeneous quantum-classical programming, compilation, and execution in a single-source context. Our work provides a first-of-its-kind C++ compiler enabling high-level quantum kernel (function) expression in a quantum-language agnostic manner, as well as a hardware-agnostic, retargetable compiler workflow targeting a number of physical and virtual quantum computing backends. qcor leverages novel Clang plugin interfaces and builds upon the XACC system-level quantum programming framework to provide a state-of-the-art integration mechanism for quantum-classical compilation that leverages the best from the community at-large. qcor translates quantum kernels ultimately to the XACC intermediate representation, and provides user-extensible hooks for quantum compilation routines like circuit optimization, analysis, and placement. This work details the overall architecture and compiler workflow for qcor, and provides a number of illuminating programming examples demonstrating its utility for near-term variational tasks, quantum algorithm expression, and feed-forward error correction schemes.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3462670',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Design Automation for Tree-based Nearest Neighborhood–aware Placement of High-speed Cellular Automata on FPGA with Scan Path Insertion',\n",
       "  'authors': \"['Ayan Palchaudhuri', 'Sandeep Sharma', 'Anindya Sundar Dhar']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Design Automation of Electronic Systems',\n",
       "  'abstract': 'Cellular Automata (CA) is attractive for high-speed VLSI implementation due to modularity, cascadability, and locality of interconnections confined to neighboring logic cells. However, this outcome is not easily transferable to tree-structured CA, since the neighbors having half and double the index value of the current CA cell under question can be sufficiently distanced apart on the FPGA floor. Challenges to meet throughput requirements, seamlessly translate algorithmic modifications for changing application specifications to gate level architectures and to address reliability challenges of semiconductor chips are ever increasing. Thus, a proper design framework assisting automation of synthesizable, delay-optimized VLSI architecture descriptions facilitating testability is desirable. In this article, we have automated the generation of hardware description of tree-structured CA that includes a built-in scan path realized with zero area and delay overhead. The scan path facilitates seeding the CA, state modification, and fault localization on the FPGA fabric. Three placement algorithms were proposed to ensure maximum physical adjacency amongst neighboring CA cells, arranged in a multi-columnar fashion on the FPGA grid. Our proposed architectures outperform implementations arising out of standard placers and behavioral designs, existing tree mapping strategies, and state-of-the-art FPGA centric error detection architectures in area and speed.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3446206',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'UNO: virtualizing and unifying nonlinear operations for emerging neural networks',\n",
       "  'authors': \"['Di Wu', 'Jingjie Li', 'Setareh Behroozi', 'Younghyun Kim', 'Joshua San Miguel']\",\n",
       "  'date': 'July 2021',\n",
       "  'source': \"ISLPED '21: Proceedings of the ACM/IEEE International Symposium on Low Power Electronics and Design\",\n",
       "  'abstract': 'Linear multiply-accumulate (MAC) operations have been the main focus of prior efforts in improving the energy efficiency of neural network inference due to their dominant contribution to energy consumption in traditional models. On the other hand, nonlinear operations, such as division, exponentiation, and logarithm, that are becoming increasingly significant in emerging neural network models, have been largely underexplored. In this paper, we propose UNO, a low-area, low-energy processing element that virtualizes the Taylor approximation of nonlinear operations on top of off-the-shelf linear MAC units already present in inference hardware. Such virtualization approximates multiple nonlinear operations in a unified, MAC-compatible manner to achieve dynamic run-time accuracy-energy scaling. Compared to the baseline, our scheme reduces the energy consumption by up to 38.4% for individual operations and increases the energy efficiency by up to 274.5% for emerging neural network models with negligible inference loss.',\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3489049.3489051',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Fast Key-Value Lookups with Node Tracker',\n",
       "  'authors': \"['Mustafa Cavus', 'Mohammed Shatnawi', 'Resit Sendag', 'Augustus K. Uht']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Architecture and Code Optimization',\n",
       "  'abstract': 'Lookup operations for in-memory databases are heavily memory bound, because they often rely on pointer-chasing linked data structure traversals. They also have many branches that are hard-to-predict due to random key lookups. In this study, we show that although cache misses are the primary bottleneck for these applications, without a method for eliminating the branch mispredictions only a small fraction of the performance benefit is achieved through prefetching alone. We propose the Node Tracker (NT), a novel programmable prefetcher/pre-execution unit that is highly effective in exploiting inter key-lookup parallelism to improve single-thread performance. We extend NT with branch outcome streaming (BOS) to reduce branch mispredictions and show that this achieves an extra 3× speedup. Finally, we evaluate the NT as a pre-execution unit and demonstrate that we can further improve the performance in both single- and multi-threaded execution modes. Our results show that, on average, NT improves single-thread performance by 4.1× when used as a prefetcher; 11.9× as a prefetcher with BOS; 14.9× as a pre-execution unit and 18.8× as a pre-execution unit with BOS. Finally, with 24 cores of the latter version, we achieve a speedup of 203× and 11× over the single-core and 24-core baselines, respectively.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3452099',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Provable Advantages for Graph Algorithms in Spiking Neural Networks',\n",
       "  'authors': \"['James B. Aimone', 'Yang Ho', 'Ojas Parekh', 'Cynthia A. Phillips', 'Ali Pinar', 'William Severa', 'Yipu Wang']\",\n",
       "  'date': 'July 2021',\n",
       "  'source': \"SPAA '21: Proceedings of the 33rd ACM Symposium on Parallelism in Algorithms and Architectures\",\n",
       "  'abstract': 'We present a theoretical framework for designing and assessing the performance of algorithms executing in networks consisting of spiking artificial neurons. Although spiking neural networks (SNNs) are capable of general-purpose computation, few algorithmic results with rigorous asymptotic performance analysis are known. SNNs are exceptionally well-motivated practically, as neuromorphic computing systems with 100 million spiking neurons are available, and systems with a billion neurons are anticipated in the next few years. Beyond massive parallelism and scalability, neuromorphic computing systems offer energy consumption orders of magnitude lower than conventional high-performance computing systems. We employ our framework to design and analyze neuromorphic graph algorithms, focusing on shortest path problems. Our neuromorphic algorithms are message-passing algorithms relying critically on data movement for computation, and we develop data-movement lower bounds for conventional algorithms. A fair and rigorous comparison with conventional algorithms and architectures is challenging but paramount. We prove a polynomial-factor advantage even when we assume an SNN consisting of a simple grid-like network of neurons. To the best of our knowledge, this is one of the first examples of a provable asymptotic computational advantage for neuromorphic computing.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3409964.3461813',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'UNO: virtualizing and unifying nonlinear operations for emerging neural networks',\n",
       "  'authors': \"['Di Wu', 'Jingjie Li', 'Setareh Behroozi', 'Younghyun Kim', 'Joshua San Miguel']\",\n",
       "  'date': 'July 2021',\n",
       "  'source': \"ISLPED '21: Proceedings of the ACM/IEEE International Symposium on Low Power Electronics and Design\",\n",
       "  'abstract': 'Linear multiply-accumulate (MAC) operations have been the main focus of prior efforts in improving the energy efficiency of neural network inference due to their dominant contribution to energy consumption in traditional models. On the other hand, nonlinear operations, such as division, exponentiation, and logarithm, that are becoming increasingly significant in emerging neural network models, have been largely underexplored. In this paper, we propose UNO, a low-area, low-energy processing element that virtualizes the Taylor approximation of nonlinear operations on top of off-the-shelf linear MAC units already present in inference hardware. Such virtualization approximates multiple nonlinear operations in a unified, MAC-compatible manner to achieve dynamic run-time accuracy-energy scaling. Compared to the baseline, our scheme reduces the energy consumption by up to 38.4% for individual operations and increases the energy efficiency by up to 274.5% for emerging neural network models with negligible inference loss.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISLPED52811.2021.9502473',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'CAMeleon: Reconfigurable B(T)CAM in Computational RAM',\n",
       "  'authors': \"['Zamshed I. Chowdhury', 'Salonik Resch', 'Hüsrev Cilasun', 'Zhengyang Zhao', 'Masoud Zabihi', 'Sachin S. Sapatnekar', 'Jian-Ping Wang', 'Ulya R. Karpuzcu']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"GLSVLSI '21: Proceedings of the 2021 on Great Lakes Symposium on VLSI\",\n",
       "  'abstract': 'Embedded/edge computing comes with a very stringent hardware resource (area) budget and a need for extreme energy efficiency. This motivates repurposing, i.e., reconfiguring hardware resources on demand, where the overhead of reconfiguration itself is subject to the very same tight budgets in area and energy efficiency. Numerous applications running on resource constrained environments such as wearable devices and Internet-of-Things incorporate CAM (Content Addressable Memory) as a key computational building block. In this paper we present CAMeleon -- a novel energy-efficient compute substrate which can seamlessly be reconfigured to perform CAM operations in addition to logic and memory functions. CAMeleon has a similar level of latency to conventional CAM designs based on SRAM and emerging memory technologies (such as STT-MTJ, ReRAM and PCM), however, performs CAM operations more energy-efficiently, consumes less area, and can support traditional logic and memory functions beyond CAM operations on demand thanks to its reconfigurability.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3453688.3461507',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Learning to Train CNNs on Faulty ReRAM-based Manycore Accelerators',\n",
       "  'authors': \"['Biresh Kumar Joardar', 'Janardhan Rao Doppa', 'Hai Li', 'Krishnendu Chakrabarty', 'Partha Pratim Pande']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Embedded Computing Systems',\n",
       "  'abstract': 'The growing popularity of convolutional neural networks (CNNs) has led to the search for efficient computational platforms to accelerate CNN training. Resistive random-access memory (ReRAM)-based manycore architectures offer a promising alternative to commonly used GPU-based platforms for training CNNs. However, due to the immature fabrication process and limited write endurance, ReRAMs suffer from different types of faults. This makes training of CNNs challenging as weights are misrepresented when they are mapped to faulty ReRAM cells. This results in unstable training, leading to unacceptably low accuracy for the trained model. Due to the distributed nature of the mapping of the individual bits of a weight to different ReRAM cells, faulty weights often lead to exploding gradients. This in turn introduces a positive feedback in the training loop, resulting in extremely large and unstable weights. In this paper, we propose a lightweight and reliable CNN training methodology using weight clipping to prevent this phenomenon and enable training even in the presence of many faults. Weight clipping prevents large weights from destabilizing CNN training and provides the backpropagation algorithm with the opportunity to compensate for the weights mapped to faulty cells. The proposed methodology achieves near-GPU accuracy without introducing significant area or performance overheads. Experimental evaluation indicates that weight clipping enables the successful training of CNNs in the presence of faults, while also reducing training time by 4X on average compared to a conventional GPU platform. Moreover, we also demonstrate that weight clipping outperforms a recently proposed error correction code (ECC)-based method when training is carried out using faulty ReRAMs.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3476986',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Ghost routing to enable oblivious computation on memory-centric networks',\n",
       "  'authors': \"['Yeonju Ro', 'Seongwook Jin', 'Jaehyuk Huh', 'John Kim']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': 'With offloading of data to the cloud, ensuring privacy and securing data has become more important. However, encrypting data alone is insufficient as the memory address itself can leak sensitive information. In this work, we exploit packetized memory interface to provide secure memory access and support oblivious computation in a system with multiple memory modules interconnected with a multi-hop, memory-centric network. While the memory address can be encrypted with a packetized memory interface, simply encrypting the address does not provide full oblivious computation since coarse-grain memory access patterns can be leaked. In this work, we first propose a scalable encryption microarchitecture with source-based routing where the packet is only encrypted once at source and latency overhead in intermediate routers is minimized. We then define secure routing in memory-centric networks to enable oblivious computation such that memory access patterns across the memory modules are completely obfuscated. We explore different naive secure routing algorithms to ensure oblivious computation but they come with high performance overhead. To minimize performance overhead, we propose ghost packets that replace dummy packets with existing network traffic. We also propose Ghost routing that batches multiple ghost packets together to minimize bandwidth loss from naive secure routing while exploiting random routing.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00077',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': '55nm CMOS analog circuit implementation of LIF and STDP functions for low-power SNNs',\n",
       "  'authors': \"['Zhitao Yang', 'Zhujiang Han', 'Yucong Huang', 'Terry Tao Ye']\",\n",
       "  'date': 'July 2021',\n",
       "  'source': \"ISLPED '21: Proceedings of the ACM/IEEE International Symposium on Low Power Electronics and Design\",\n",
       "  'abstract': 'Spiking neural networks (SNNs) demonstrate great potentials to achieve low-power computation for AI applications. SNN uses spike trains, instead of binary bit-steams to encode input and output information, therefore, analog implementation of SNN will have more advantages than digital implementation in terms of power consumption and hardware overheads. Leaky Integrate-and-Fire (LIF) and Spike Timing Dependent Plasticity (STDP) models are the two fundamental mechanisms of SNN operation. In this paper, we propose a 55nm analog CMOS implementation of the LIF and STDP functions. Testing results demonstrate that the circuit can closely imitate the behavior of the LIF and STDP mechanisms, while demanding a much lower power consumption (around 1nJ per spike with the pulse width of 0.5ms). The proposed LIF and STDP circuits can be used as building blocks to construct a complete SNN architecture.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISLPED52811.2021.9502497',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Impact of On-chip Interconnect on In-memory Acceleration of Deep Neural Networks',\n",
       "  'authors': \"['Gokul Krishnan', 'Sumit K. Mandal', 'Chaitali Chakrabarti', 'Jae-Sun Seo', 'Umit Y. Ogras', 'Yu Cao']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Journal on Emerging Technologies in Computing Systems',\n",
       "  'abstract': 'With the widespread use of Deep Neural Networks (DNNs), machine learning algorithms have evolved in two diverse directions—one with ever-increasing connection density for better accuracy and the other with more compact sizing for energy efficiency. The increase in connection density increases on-chip data movement, which makes efficient on-chip communication a critical function of the DNN accelerator. The contribution of this work is threefold. First, we illustrate that the point-to-point (P2P)-based interconnect is incapable of handling a high volume of on-chip data movement for DNNs. Second, we evaluate P2P and network-on-chip (NoC) interconnect (with a regular topology such as a mesh) for SRAM- and ReRAM-based in-memory computing (IMC) architectures for a range of DNNs. This analysis shows the necessity for the optimal interconnect choice for an IMC DNN accelerator. Finally, we perform an experimental evaluation for different DNNs to empirically obtain the performance of the IMC architecture with both NoC-tree and NoC-mesh. We conclude that, at the tile level, NoC-tree is appropriate for compact DNNs employed at the edge, and NoC-mesh is necessary to accelerate DNNs with high connection density. Furthermore, we propose a technique to determine the optimal choice of interconnect for any given DNN. In this technique, we use analytical models of NoC to evaluate end-to-end communication latency of any given DNN. We demonstrate that the interconnect optimization in the IMC architecture results in up to 6 × improvement in energy-delay-area product for VGG-19 inference compared to the state-of-the-art ReRAM-based IMC architectures.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3460233',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'EXAMINER: automatically locating inconsistent instructions between real devices and CPU emulators for ARM',\n",
       "  'authors': \"['Muhui Jiang', 'Tianyi Xu', 'Yajin Zhou', 'Yufeng Hu', 'Ming Zhong', 'Lei Wu', 'Xiapu Luo', 'Kui Ren']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'Emulators are widely used to build dynamic analysis frameworks due to its fine-grained tracing capability, full system monitoring functionality, and scalability of running on different operating systems and architectures. However, whether emulators are consistent with real devices is unknown. To understand this problem, we aim to automatically locate inconsistent instructions, which behave differently between emulators and real devices.   We target the ARM architecture, which provides machine-readable specifications. Based on the specification, we propose a sufficient test case generator by designing and implementing the first symbolic execution engine for the ARM architecture specification language (ASL). We generate 2,774,649 representative instruction streams and conduct differential testing between four ARM real devices in different architecture versions (i.e., ARMv5, ARMv6, ARMv7, and ARMv8) and three state-of-the-art emulators (i.e., QEMU, Unicorn, and Angr). We locate a huge number of inconsistent instruction streams (171,858 for QEMU, 223,264 for unicorn, and 120,169 for Angr). We find that undefined implementation in ARM manual and bugs of emulators are the major causes of inconsistencies. Furthermore, we discover 12 bugs, which influence commonly used instructions (e.g., BLX). With the inconsistent instructions, we build three security applications and demonstrate the capability of these instructions on detecting emulators, anti-emulation, and anti-fuzzing.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503222.3507736',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Monolithically Integrating Non-Volatile Main Memory over the Last-Level Cache',\n",
       "  'authors': \"['Candace Walden', 'Devesh Singh', 'Meenatchi Jagasivamani', 'Shang Li', 'Luyi Kang', 'Mehdi Asnaashari', 'Sylvain Dubois', 'Bruce Jacob', 'Donald Yeung']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Architecture and Code Optimization',\n",
       "  'abstract': 'Many emerging non-volatile memories are compatible with CMOS logic, potentially enabling their integration into a CPU’s die. This article investigates such monolithically integrated CPU–main memory chips. We exploit non-volatile memories employing 3D crosspoint subarrays, such as resistive RAM (ReRAM), and integrate them over the CPU’s last-level cache (LLC). The regular structure of cache arrays enables co-design of the LLC and ReRAM main memory for area efficiency. We also develop a streamlined LLC/main memory interface that employs a single shared internal interconnect for both the cache and main memory arrays, and uses a unified controller to service both LLC and main memory requests.We apply our monolithic design ideas to a many-core CPU by integrating 3D ReRAM over each core’s LLC slice. We find that co-design of the LLC and ReRAM saves 27% of the total LLC–main memory area at the expense of slight increases in delay and energy. The streamlined LLC/main memory interface saves an additional 12% in area. Our simulation results show monolithic integration of CPU and main memory improves performance by 5.3× and 1.7× over HBM2 DRAM for several graph and streaming kernels, respectively. It also reduces the memory system’s energy by 6.0× and 1.7×, respectively. Moreover, we show that the area savings of co-design permits the CPU to have 23% more cores and main memory, and that streamlining the LLC/main memory interface incurs a small 4% performance penalty.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3462632',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'ShEF: shielded enclaves for cloud FPGAs',\n",
       "  'authors': \"['Mark Zhao', 'Mingyu Gao', 'Christos Kozyrakis']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': \"FPGAs are now used in public clouds to accelerate a wide range of applications, including many that operate on sensitive data such as financial and medical records. We present ShEF, a trusted execution environment (TEE) for cloud-based reconfigurable accelerators. ShEF is independent from CPU-based TEEs and allows secure execution under a threat model where the adversary can control all software running on the CPU connected to the FPGA, has physical access to the FPGA, and can compromise the FPGA interface logic of the cloud provider. ShEF provides a secure boot and remote attestation process that relies solely on existing FPGA mechanisms for root of trust. It also includes a Shield component that provides secure access to data while the accelerator is in use. The Shield is highly customizable and extensible, allowing users to craft a bespoke security solution that fits their accelerator's memory access patterns, bandwidth, and security requirements at minimum performance and area overheads. We describe a prototype implementation of ShEF for existing cloud FPGAs, map ShEF to a performant and secure storage application, and measure the performance benefits of customizable security using five additional accelerators.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503222.3507733',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'CrypTag: Thwarting Physical and Logical Memory Vulnerabilities using Cryptographically Colored Memory',\n",
       "  'authors': \"['Pascal Nasahl', 'Robert Schilling', 'Mario Werner', 'Jan Hoogerbrugge', 'Marcel Medwed', 'Stefan Mangard']\",\n",
       "  'date': 'May 2021',\n",
       "  'source': \"ASIA CCS '21: Proceedings of the 2021 ACM Asia Conference on Computer and Communications Security\",\n",
       "  'abstract': 'Memory vulnerabilities are a major threat to many computing systems. To effectively thwart spatial and temporal memory vulnerabilities, full logical memory safety is required. However, current mitigation techniques for memory safety are either too expensive or trade security against efficiency. One promising attempt to detect memory safety vulnerabilities in hardware is memory coloring, a security policy deployed on top of tagged memory architectures. However, due to the memory storage and bandwidth overhead of large tags, commodity tagged memory architectures usually only provide small tag sizes, thus limiting their use for security applications. Irrespective of logical memory safety, physical memory safety is a necessity in hostile environments prevalent for modern cloud computing and IoT devices. Architectures from Intel and AMD already implement transparent memory encryption to maintain confidentiality and integrity of all off-chip data. Surprisingly, the combination of both, logical and physical memory safety, has not yet been extensively studied in previous research, and a naive combination of both security strategies would accumulate both overheads. In this paper, we propose CrypTag, an efficient hardware/software co-design mitigating a large class of logical memory safety issues and providing full physical memory safety. At its core, CrypTag utilizes a transparent memory encryption engine not only for physical memory safety, but also for memory coloring at hardly any additional costs. The design avoids any overhead for tag storage by embedding memory colors in the upper bits of a pointer and using these bits as an additional input for the memory encryption. A custom compiler extension automatically leverages CrypTag to detect logical memory safety issues for commodity programs and is fully backward compatible. For evaluating the design, we extended a RISC-V processor with memory encryption with CrypTag. Furthermore, we developed a LLVM-based toolchain automatically protecting all dynamic, local, and global data. Our evaluation shows a hardware overhead of less than 1% and an average runtime overhead between 1.5% and 6.1% for thwarting logical memory safety vulnerabilities on a system already featuring memory encryption. Enhancing a system with memory encryption typically induces a runtime overhead between 5% and 109.8% for commercial and open-source encryption units.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3433210.3453684',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'EPEX: Processor Verification by Equivalent Program Execution',\n",
       "  'authors': \"['Lucas Klemmer', 'Daniel Große']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"GLSVLSI '21: Proceedings of the 2021 on Great Lakes Symposium on VLSI\",\n",
       "  'abstract': 'Verifying processors has been and still is a major challenge. Therefore, intensive research has led to advanced verification solutions ranging from ISS-based reference models, (cross-level) simulation down to formal verification at the RTL. During the verification of the processor implementation at the Instruction Set Architecture (ISA) level, test stimuli, i.e. test programs are needed. They are either created manually or with the aid of sophisticated test program generators. However, significant effort is required to produce thorough test programs. In this paper, we devise a novel approach for processor verification by Equivalent Program EXecution (EPEX). Our approach is based on a new form of equivalence checking Instead of comparing the architectural states of two models which execute the same program P, we derive a second, but equivalent program P^ from P (wrt. to a formal ISA model), and check that executing P and P^ will produce equal architectural states on the same design. We show that EPEX can easily be used in a simulation-based verification environment and broadens existing tests automatically. In a RISC-V case study using different core configurations of the well-known VexRiscv core, we demonstrate the bug-finding capabilities of EPEX.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3453688.3461497',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'HeteroGen: transpiling C to heterogeneous HLS code with automated test generation and program repair',\n",
       "  'authors': \"['Qian Zhang', 'Jiyuan Wang', 'Guoqing Harry Xu', 'Miryung Kim']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'Despite the trend of incorporating heterogeneity and specialization in hardware, the development of heterogeneous applications is limited to a handful of engineers with deep hardware expertise. We propose HeteroGen that takes C/C++ code as input and automatically generates an HLS version with test behavior preservation and better performance. Key to the success of HeteroGen is adapting the idea of search-based program repair to the heterogeneous computing domain, while addressing two technical challenges. First, the turn-around time of HLS compilation and simulation is much longer than the usual C/C++ compilation and execution time; therefore, HeteroGen applies pattern-oriented program edits guided by common fix patterns and their dependences. Second, behavior and performance checking requires testing, but test cases are often unavailable. Thus, HeteroGen auto-generates test inputs suitable for checking C to HLS-C conversion errors, while providing high branch coverage for the original C code.   An evaluation of HeteroGen shows that it produces an HLS-compatible version for nine out of ten real-world heterogeneous applications fully automatically, applying up to 438 lines of edits to produce an HLS version 1.63x faster than the original version.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503222.3507748',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Enabling pipeline parallelism in heterogeneous managed runtime environments via batch processing',\n",
       "  'authors': \"['Florin Blanaru', 'Athanasios Stratikopoulos', 'Juan Fumero', 'Christos Kotselidis']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': 'VEE 2022: Proceedings of the 18th ACM SIGPLAN/SIGOPS International Conference on Virtual Execution Environments',\n",
       "  'abstract': 'During the last decade, managed runtime systems have been constantly evolving to become capable of exploiting underlying hardware accelerators, such as GPUs and FPGAs. Regardless of the programming language and their corresponding runtime systems, the majority of the work has been focusing on the compiler front trying to tackle the challenging task of how to enable just-in-time compilation and execution of arbitrary code segments on various accelerators. Besides this challenging task, another important aspect that defines both functional correctness and performance of managed runtime systems is that of automatic memory management. Although automatic memory management improves productivity by abstracting away memory allocation and maintenance, it hinders the capability of using specific memory regions, such as pinned memory, in order to perform data transfer times between the CPU and hardware accelerators.   In this paper, we introduce and evaluate a series of memory optimizations specifically tailored for heterogeneous managed runtime systems. In particular, we propose: (i) transparent and automatic \"parallel batch processing\" for overlapping data transfers and computation between the host and hardware accelerators in order to enable pipeline parallelism, and (ii) \"off-heap pinned memory\" in combination with parallel batch processing in order to increase the performance of data transfers without posing any on-heap overheads. These two techniques have been implemented in the context of the state-of-the-art open-source TornadoVM and their combination can lead up to 2.5x end-to-end performance speedup against sequential batch processing.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3516807.3516821',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Prepare: Power-Aware Approximate Real-time Task Scheduling for Energy-Adaptive QoS Maximization',\n",
       "  'authors': \"['Shounak Chakraborty', 'Sangeet Saha', 'Magnus Själander', 'Klaus Mcdonald-Maier']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Embedded Computing Systems',\n",
       "  'abstract': 'Achieving high result-accuracy in approximate computing (AC) based real-time applications without violating power constraints of the underlying hardware is a challenging problem. Execution of such AC real-time tasks can be divided into the execution of the mandatory part to obtain a result of acceptable quality, followed by a partial/complete execution of the optional part to improve accuracy of the initially obtained result within the given time-limit. However, enhancing result-accuracy at the cost of increased execution length might lead to deadline violations with higher energy usage. We propose Prepare, a novel hybrid offline-online approximate real-time task-scheduling approach, that first schedules AC-based tasks and determines operational processing speeds for each individual task constrained by system-wide power limit, deadline, and task-dependency. At runtime, by employing fine-grained DVFS, the energy-adaptive processing speed governing mechanism of Prepare reduces processing speed during each last level cache miss induced stall and scales up the processing speed once the stall finishes to a higher value than the predetermined one. To ensure on-chip thermal safety, this higher processing speed is maintained only for a short time-span after each stall, however, this reduces execution times of the individual task and generates slacks. Prepare exploits the slacks either to enhance result-accuracy of the tasks, or to improve thermal and energy efficiency of the underlying hardware, or both. With a 70 - 80% workload, Prepare offers 75% result-accuracy with its constrained scheduling, which is enhanced by 5.3% for our benchmark based evaluation of the online energy-adaptive mechanism on a 4-core based homogeneous chip multi-processor, while meeting the deadline constraint. Overall, while maintaining runtime thermal safety, Prepare reduces peak temperature by up to 8.6 °C for our baseline system. Our empirical evaluation shows that constrained scheduling of Prepare outperforms a state-of-the-art scheduling policy, whereas our runtime energy-adaptive mechanism surpasses two current DVFS based thermal management techniques.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3476993',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Enabling compute-communication overlap in distributed deep learning training platforms',\n",
       "  'authors': \"['Saeed Rashidi', 'Matthew Denton', 'Srinivas Sridharan', 'Sudarshan Srinivasan', 'Amoghavarsha Suresh', 'Jade Nie', 'Tushar Krishna']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': \"Deep Learning (DL) training platforms are built by interconnecting multiple DL accelerators (e.g., GPU/TPU) via fast, customized interconnects with 100s of gigabytes (GBs) of bandwidth. However, as we identify in this work, driving this bandwidth is quite challenging. This is because there is a pernicious balance between using the accelerator's compute and memory for both DL computations and communication. This work makes two key contributions. First, via real system measurements and detailed modeling, we provide an understanding of compute and memory bandwidth demands for DL compute and comms. Second, we propose a novel DL collective communication accelerator called Accelerator Collectives Engine (ACE) that sits alongside the compute and networking engines at the accelerator endpoint. ACE frees up the endpoint's compute and memory resources for DL compute, which in turn reduces the required memory BW by 3.5X on average to drive the same network BW compared to state-of-the-art baselines. For modern DL workloads and different network sizes, ACE, on average, increases the effective network bandwidth utilization by 1.44X (up to 2.67X), resulting in an average of 1.41X (up to 1.51X), 1.12X (up to 1.17X), and 1.13X (up to 1.19X) speedup in iteration time for ResNet-50, GNMT and DLRM when compared to the best baseline configuration, respectively.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00049',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'CLU: A Near-Memory Accelerator Exploiting the Parallelism in Convolutional Neural Networks',\n",
       "  'authors': \"['Palash Das', 'Hemangee K. Kapoor']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Journal on Emerging Technologies in Computing Systems',\n",
       "  'abstract': 'Convolutional/Deep Neural Networks (CNNs/DNNs) are rapidly growing workloads for the emerging AI-based systems. The gap between the processing speed and the memory-access latency in multi-core systems affects the performance and energy efficiency of the CNN/DNN tasks. This article aims to alleviate this gap by providing a simple and yet efficient near-memory accelerator-based system that expedites the CNN inference. Towards this goal, we first design an efficient parallel algorithm to accelerate CNN/DNN tasks. The data is partitioned across the multiple memory channels (vaults) to assist in the execution of the parallel algorithm. Second, we design a hardware unit, namely the convolutional logic unit (CLU), which implements the parallel algorithm. To optimize the inference, the CLU is designed, and it works in three phases for layer-wise processing of data. Last, to harness the benefits of near-memory processing (NMP), we integrate homogeneous CLUs on the logic layer of the 3D memory, specifically the Hybrid Memory Cube (HMC). The combined effect of these results in a high-performing and energy-efficient system for CNNs/DNNs. The proposed system achieves a substantial gain in the performance and energy reduction compared to multi-core CPU- and GPU-based systems with a minimal area overhead of 2.37%.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3427472',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Enabling energy-efficient DNN training on hybrid GPU-FPGA accelerators',\n",
       "  'authors': \"['Xin He', 'Jiawen Liu', 'Zhen Xie', 'Hao Chen', 'Guoyang Chen', 'Weifeng Zhang', 'Dong Li']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ICS '21: Proceedings of the ACM International Conference on Supercomputing\",\n",
       "  'abstract': 'DNN training consumes orders of magnitude more energy than inference and requires innovative use of accelerators to improve energy-efficiency. However, despite having complementary features, GPUs and FPGAs have been mostly used independently for the entire training process, thus neglecting the opportunity in assigning individual but distinct operations to the most suitable hardware. In this paper, we take the initiative to explore new opportunities and viable solutions in enabling energy-efficient DNN training on hybrid accelerators. To overcome fundamental challenges including avoiding training throughput loss, enabling fast design space exploration, and efficient scheduling, we propose a comprehensive framework, Hype-training, that utilizes a combination of offline characterization, performance modeling, and online scheduling of individual operations. Experimental tests using NVIDIA V100 GPUs and Intel Stratix 10 FPGAs show that, Hype-training is able to exploit a mixture of GPUs and FPGAs at a fine granularity to achieve significant energy reduction, by 44.3% on average and up to 59.7%, without any loss in training throughput. Hype-training can also enforce power caps more effectively than state-of-the-art power management mechanisms on GPUs.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3447818.3460371',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A resource-efficient spiking neural network accelerator supporting emerging neural encoding',\n",
       "  'authors': \"['Daniel Gerlinghoff', 'Zhehui Wang', 'Xiaozhe Gu', 'Rick Siow Mong Goh', 'Tao Luo']\",\n",
       "  'date': 'March 2022',\n",
       "  'source': \"DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe\",\n",
       "  'abstract': 'Spiking neural networks (SNNs) recently gained momentum due to their low-power multiplication-free computing and the closer resemblance of biological processes in the nervous system of humans. However, SNNs require very long spike trains (up to 1000) to reach an accuracy similar to their artificial neural network (ANN) counterparts for large models, which offsets efficiency and inhibits its application to low-power systems for real-world use cases. To alleviate this problem, emerging neural encoding schemes are proposed to shorten the spike train while maintaining the high accuracy. However, current accelerators for SNN cannot well support the emerging encoding schemes. In this work, we present a novel hardware architecture that can efficiently support SNN with emerging neural encoding. Our implementation features energy and area efficient processing units with increased parallelism and reduced memory accesses. We verified the accelerator on FPGA and achieve 25% and 90% improvement over previous work in power consumption and latency, respectively. At the same time, high area efficiency allows us to scale for large neural network models. To the best of our knowledge, this is the first work to deploy the large neural network model VGG on physical FPGA-based neuromorphic hardware.',\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3539845.3539873',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Bridging the Gap between RTL and Software Fault Injection',\n",
       "  'authors': \"['J. Laurent', 'C. Deleuze', 'F. Pebay-Peyroula', 'V. Beroulle']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Journal on Emerging Technologies in Computing Systems',\n",
       "  'abstract': \"Protecting programs against hardware fault injection requires accurate software fault models. However, typical models, such as the instruction skip, do not take into account the microarchitecture specificities of a processor. We propose in this article an approach to study the relation between faults at the Register Transfer Level (RTL) and faults at the software level. The goal is twofold: accurately model RTL faults at the software level and materialize software fault models to actual RTL injections. These goals lead to a better understanding of a system's security against hardware fault injection, which is important to design effective and cost-efficient countermeasures. Our approach is based on the comparison between results from RTL simulations and software injections (using a program mutation tool). Various analyses are included in this article to give insight on the relevance of software fault models, such as the computation of a coverage and fidelity metric, and to link software fault models to hardware RTL descriptions. These analyses are applied on various single-bit and multiple-bit injection campaigns to study the faulty behaviors of a RISC-V processor.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3446214',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Dolmen: FPGA swarm for safety and liveness verification',\n",
       "  'authors': \"['Emilien Fournier', 'Ciprian Teodorov', 'Loïc Lagadec']\",\n",
       "  'date': 'March 2022',\n",
       "  'source': \"DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe\",\n",
       "  'abstract': 'To ensure correctness of critical systems, swarm verification produces proofs of failure on systems too large to be verified using model-checking. Recent research efforts exploit both intrinsic parallelism and low-latency on-chip memory offered by FPGAs to achieve 3 orders of magnitude speedups over software. However, these approaches are limited to safety verification that encodes only what the system should not do. Liveness properties express what the system should do, and are widely used in the verification of operating systems, distributed systems, and communication protocols. Both safety and liveness properties are of paramount importance to ensure systems correctness. This paper presents Dolmen, the first FPGA implementation of a swarm verification engine that supports both safety and liveness properties. Dolmen features a deeply pipelined verification core, along with a scalable architecture to allow high-frequency synthesis on large FPGAs. Our experimental results, on a Xilinx Virtex Ultrascale+ FPGA, show that the Dolmen architecture can achieve up to 4 orders of magnitude speedups compared to software model-checking.',\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3539845.3540176',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Invisible bits: hiding secret messages in SRAM’s analog domain',\n",
       "  'authors': \"['Jubayer Mahmod', 'Matthew Hicks']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': \"Electronic devices are increasingly the subject of inspection by authorities. While encryption hides secret messages, it does not hide the transmission of those secret messages---in fact, it calls attention to them. Thus, an adversary, seeing encrypted data, turns to coercion to extract the credentials required to reveal the secret message. Steganographic techniques hide secret messages in plain sight, providing the user with plausible deniability, removing the threat of coercion.   This paper unveils Invisible Bits a new steganographic technique that hides secret messages in the analog domain of Static Random Access Memory (SRAM) embedded within a computing device. Unlike other memory technologies, the power-on state of SRAM reveals the analog-domain properties of its individual cells. We show how to quickly and systematically change the analog-domain properties of SRAM cells to encode data in the analog domain and how to reveal those changes by capturing SRAM's power-on state. Experiments with commercial devices show that Invisible Bits provides over 90% capacity---two orders-of-magnitude more than previous on-chip steganographic approaches, while retaining device functionality---even when the device undergoes subsequent normal operation or is shelved for months. Experiments also show that adversaries cannot differentiate between devices with encoded messages and those without. Lastly, we show how to layer encryption and error correction on top of our message encoding scheme in an end-to-end demonstration.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503222.3507756',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Design space for scaling-in general purpose computing within the DDR DRAM hierarchy for map-reduce workloads',\n",
       "  'authors': \"['Siddhartha Balakrishna Rai', 'Anand Sivasubramaniam', 'Adithya Kumar', 'Prasanna Venkatesh Rengasamy', 'Vijaykrishnan Narayanan', 'Ameen Akel', 'Sean Eilert']\",\n",
       "  'date': 'May 2021',\n",
       "  'source': \"CF '21: Proceedings of the 18th ACM International Conference on Computing Frontiers\",\n",
       "  'abstract': 'This paper conducts a design space exploration of placing general purpose RISCV cores within the DDR DRAM hierarchy to boost the performance of important data analytics applications in the datacenter. We investigate the hardware (where? how many? how to interface?) and software (how to place data? how to map computations?) choices for placing these cores within the rank, chip, and bank of the DIMM slots to take advantage of the locality vs. parallelism trade-offs. We use the popular MapReduce paradigm, normally used to scale out workloads across servers, to scale in these workloads into the DDR DRAM hierarchy. We evaluate the design space using diverse off-the-shelf Apache Spark Workloads to show the pros-and-cons of different hardware placement and software mapping strategies. Results show that bank-level RISCV cores can provide tremendous speedup (up to 363X) for the offload-able parts of these applications, amounting to 14X speedup overall in some applications. Even in the non-amenable applications, we get at least 31% performance boost for the entire application. To realize this, we incur an area overhead of 4% at the bank level, and increase in temperature of < 4°C over the chip averaged over all applications.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3457388.3458661',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'NVOverlay: enabling efficient and scalable high-frequency snapshotting to NVM',\n",
       "  'authors': \"['Ziqi Wang', 'Chul-Hwan Choo', 'Michael A. Kozuch', 'Todd C. Mowry', 'Gennady Pekhimenko', 'Vivek Seshadri', 'Dimitrios Skarlatos']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': 'The ability to capture frequent (per millisecond) persistent snapshots to NVM would enable a number of compelling use cases. Unfortunately, existing NVM snapshotting techniques suffer from a combination of persistence barrier stalls, write amplification to NVM, and/or lack of scalability beyond a single socket. In this paper, we present NVOverlay, which is a scalable and efficient technique for capturing frequent persistent snapshots to NVM such that they can be randomly accessed later. NVOverlay uses Coherent Snapshot Tracking to efficiently track changes to memory (since the previous snapshot) across multi-socket parallel systems, and it uses Multi-snapshot NVM Mapping to store these snapshots to NVM while avoiding excessive write amplification. Our experiments demonstrate that NVOverlay successfully hides the overhead of capturing these snapshots while reducing write amplification by 29%-47% compared with state-of-the-art logging-based snapshotting techniques.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00046',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A one-for-all and o(v log(v ))-cost solution for parallel merge style operations on sorted key-value arrays',\n",
       "  'authors': \"['Bangyan Wang', 'Lei Deng', 'Fei Sun', 'Guohao Dai', 'Liu Liu', 'Yu Wang', 'Yuan Xie']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'The processing of sorted key-value arrays using a “merge style operation (MSO)” is a very basic and important problem in domains like scientific computing, deep learning, database, graph analysis, sorting, set-operation etc. MSOs dominate the execution time in some important applications like SpGEMM and graph mining. For example, sparse vector addition as an MSO takes up to 98% execution time in SpGEMM in our experiment. For this reason, accelerating MSOs on CPU, GPU, and accelerators using parallel execution has been extensively studied but the solutions in prior work have three major limitations. (1) They treat different MSOs as isolated problems using incompatible methods and an unified solution is still lacking. (2) They do not have the flexibility to support variable key/value sizes and value calculations in the runtime given a fixed hardware design. (3) They require a quadratic hardware cost (O(V2)) for given parallelism V in most cases.  To address above three limitations, we make the following efforts. (1) We present a one-for-all solution to support all interested MSOs based on a unified abstraction model “restricted zip machine (RZM)”. (2) We propose a set of composable and parallel primitives for RZM to provide the flexibility to support variable key/value sizes and value calculations. (3) We provide the hardware design to implement the proposed primitives using only O(Vlog(V)) resource. With the above techniques, a flexible and efficient solution for MSOs has been built. Our design can be used either as a drop-in replacement of the merge unit in prior accelerators to reduce the cost from O(V2) to O(Vlog(V)), or as an extension to the SIMD ISA of CPU and GPU. In our evaluation on CPU, when V=16 (512-bit SIMD, 32-bit element), we achieve significant speedup on a range of representative kernels including set operations (8.4×), database joins (7.3×), sparse vector/matrix/tensor addition/multiplication on real/complex numbers (6.5×), merge sort (8.0× over scalar, 3.4× over the state-of-the-art SIMD), and SpGEMM (4.4× over the best one in the baseline collection).',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503222.3507728',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Exploiting Different Levels of Parallelism in the Quantum Control Microarchitecture for Superconducting Qubits',\n",
       "  'authors': \"['Mengyu Zhang', 'Lei Xie', 'Zhenxing Zhang', 'Qiaonian Yu', 'Guanglei Xi', 'Hualiang Zhang', 'Fuming Liu', 'Yarui Zheng', 'Yicong Zheng', 'Shengyu Zhang']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"MICRO '21: MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture\",\n",
       "  'abstract': 'As current Noisy Intermediate Scale Quantum (NISQ) devices suffer from decoherence errors, any delay in the instruction execution of quantum control microarchitecture can lead to the loss of quantum information and incorrect computation results. Hence, it is crucial for the control microarchitecture to issue quantum operations to the Quantum Processing Unit (QPU) in time. As in classical microarchitecture, parallelism in quantum programs needs to be exploited for speedup. However, three challenges emerge in the quantum scenario: 1) the quantum feedback control can introduce significant pipeline stall latency; 2) timing control is required for all quantum operations; 3) QPU requires a deterministic operation supply to prevent the accumulation of quantum errors.  In this paper, we propose a novel control microarchitecture design to exploit Circuit Level Parallelism (CLP) and Quantum Operation Level Parallelism (QOLP). Firstly, we develop a Multiprocessor architecture to exploit CLP, which supports dynamic scheduling of different sub-circuits. This architecture can handle parallel feedback control and minimize the potential overhead that disrupts the timing control. Secondly, we propose a Quantum Superscalar approach that exploits QOLP by efficiently executing massive quantum instructions in parallel. Both methods issue quantum operations to QPU deterministically. In the benchmark test of a Shor syndrome measurement, a six-core implementation of our proposal achieves up to 2.59 × speedup compared with a single core. For various canonical quantum computing algorithms, our superscalar approach achieves an average of 4.04 × improvement over a baseline design. Finally, We perform a simultaneous randomized benchmarking (simRB) experiment on a real QPU using the proposed microarchitecture for validation.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3466752.3480116',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Distance-in-time versus distance-in-space',\n",
       "  'authors': \"['Mahmut Taylan Kandemir', 'Xulong Tang', 'Hui Zhao', 'Jihyun Ryoo', 'Mustafa Karakoy']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': 'PLDI 2021: Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation',\n",
       "  'abstract': 'Cache behavior is one of the major factors that influence the performance of applications. Most of the existing compiler techniques that target cache memories focus exclusively on reducing data reuse distances in time (DIT). However, current manycore systems employ distributed on-chip caches that are connected using an on-chip network. As a result, a reused data element/block needs to travel over this on-chip network, and the distance to be traveled -- reuse distance in space (DIS) -- can be as influential in dictating application performance as reuse DIT. This paper represents the first attempt at defining a compiler framework that accommodates both DIT and DIS. Specifically, it first classifies data reuses into four groups: G1: (low DIT, low DIS), G2: (high DIT, low DIS), G3: (low DIT, high DIS), and G4: (high DIT, high DIS). Then, observing that reuses in G1 represent the ideal case and there is nothing much to be done in computations in G4, it proposes a \"reuse transfer\" strategy that transfers select reuses between G2 and G3, eventually, transforming each reuse to either G1 or G4. Finally, it evaluates the proposed strategy using a set of 10 multithreaded applications. The collected results reveal that the proposed strategy reduces parallel execution times of the tested applications between 19.3% and 33.3%.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3453483.3454069',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'TCN mapping optimization for ultra-low power time-series edge inference',\n",
       "  'authors': \"['Alessio Burrello', 'Alberto Dequino', 'Daniele Jahier Pagliari', 'Francesco Conti', 'Marcello Zanghieri', 'Enrico Macii', 'Luca Benini', 'Massimo Poncino']\",\n",
       "  'date': 'July 2021',\n",
       "  'source': \"ISLPED '21: Proceedings of the ACM/IEEE International Symposium on Low Power Electronics and Design\",\n",
       "  'abstract': 'Temporal Convolutional Networks (TCNs) are emerging lightweight Deep Learning models for Time Series analysis. We introduce an automated exploration approach and a library of optimized kernels to map TCNs on Parallel Ultra-Low Power (PULP) microcontrollers. Our approach minimizes latency and energy by exploiting a layer tiling optimizer to jointly find the tiling dimensions and select among alternative implementations of the causal and dilated 1D-convolution operations at the core of TCNs. We benchmark our approach on a commercial PULP device, achieving up to 103X lower latency and 20.3X lower energy than the Cube-AI toolkit executed on the STM32L4 and from 2.9X to 26.6X lower energy compared to commercial closed-source and academic open-source approaches on the same hardware target.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISLPED52811.2021.9502494',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Toward accurate platform-aware performance modeling for deep neural networks',\n",
       "  'authors': \"['Chuan-Chi Wang', 'Ying-Chiao Liao', 'Ming-Chang Kao', 'Wen-Yew Liang', 'Shih-Hao Hung']\",\n",
       "  'date': 'March 2021',\n",
       "  'source': 'ACM SIGAPP Applied Computing Review',\n",
       "  'abstract': 'In this paper, we provide a fine-grain machine learning-based method, PerfNetV2, which improves the accuracy of our previous work for modeling the neural network performance on a variety of GPU accelerators. Given an application, the proposed method can be used to predict the inference time and training time of the convolutional neural networks used in the application, which enables the system developer to optimize the performance by choosing the neural networks and/or incorporating the hardware accelerators to deliver satisfactory results in time. Furthermore, the proposed method is capable of predicting the performance of an unseen or non-existing device, e.g. a new GPU which has a higher operating frequency with less processor cores, but more memory capacity. This allows a system developer to quickly search the hardware design space and/or fine-tune the system configuration. Compared to the previous works, PerfNetV2 delivers more accurate results by modeling detailed host-accelerator interactions in executing the full neural networks and improving the architecture of the machine learning model used in the predictor. Our case studies show that PerfNetV2 yields a mean absolute percentage error within 13.1% on LeNet, AlexNet, and VGG16 on NVIDIA GTX-1080Ti, while the error rate on a previous work published in ICBD 2018 could be as large as 200%.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3477133.3477137',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Automatic Code Generation and Optimization of Large-scale Stencil Computation on Many-core Processors',\n",
       "  'authors': \"['Mingzhen Li', 'Yi Liu', 'Hailong Yang', 'Yongmin Hu', 'Qingxiao Sun', 'Bangduo Chen', 'Xin You', 'Xiaoyan Liu', 'Zhongzhi Luan', 'Depei Qian']\",\n",
       "  'date': 'August 2021',\n",
       "  'source': \"ICPP '21: Proceedings of the 50th International Conference on Parallel Processing\",\n",
       "  'abstract': 'Stencil computation is an indispensable building block of many scientific applications and is widely used by the numerical solvers of partial differential equations (PDEs). Due to the complex computation patterns of different stencils and the various hardware targets (e.g., many-core processors), many domain-specific languages (DSLs) have been proposed to optimize stencil computation. However, existing stencil DSLs mostly focus on the performance optimizations on homogeneous many-core processors such as CPUs and GPUs, and fail to embrace emerging heterogeneous many-core processors such as Sunway. In addition, few of them can support expressing stencil with multiple time dependencies and optimizations from both spatial and temporal dimensions. Moreover, most stencil DSLs are unable to generate codes that can run efficiently in large scale, which limits their practical applicability. In this paper, we propose MSC, a new stencil DSL designed to express stencil computation in both spatial and temporal dimensions. It can generate high-performance stencil codes for large-scale execution on emerging many-core processors. Specially, we design several optimization primitives for improving parallelism and data locality, and a communication library for efficient halo exchange in large scale execution. The experiment results show that our MSC achieves better performance compared to the state-of-the-art stencil DSLs.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3472456.3473517',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Mind mappings: enabling efficient algorithm-accelerator mapping space search',\n",
       "  'authors': \"['Kartik Hegde', 'Po-An Tsai', 'Sitao Huang', 'Vikas Chandra', 'Angshuman Parashar', 'Christopher W. Fletcher']\",\n",
       "  'date': 'April 2021',\n",
       "  'source': \"ASPLOS '21: Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'Modern day computing increasingly relies on specialization to satiate growing performance and efficiency requirements. A core challenge in designing such specialized hardware architectures is how to perform mapping space search, i.e., search for an optimal mapping from algorithm to hardware. Prior work shows that choosing an inefficient mapping can lead to multiplicative-factor efficiency overheads. Additionally, the search space is not only large but also non-convex and non-smooth, precluding advanced search techniques. As a result, previous works are forced to implement mapping space search using expert choices or sub-optimal search heuristics.  This work proposes Mind Mappings, a novel gradient-based search method for algorithm-accelerator mapping space search. The key idea is to derive a smooth, differentiable approximation to the otherwise non-smooth, non-convex search space. With a smooth, differentiable approximation, we can leverage efficient gradient-based search algorithms to find high-quality mappings. We extensively compare Mind Mappings to black-box optimization schemes used in prior work. When tasked to find mappings for two important workloads (CNN and MTTKRP), Mind Mapping finds mappings that achieve an average 1.40×, 1.76×, and 1.29×\\xa0(when run for a fixed number of steps) and 3.16×, 4.19×, and 2.90×\\xa0(when run for a fixed amount of time) better energy-delay product (EDP) relative to Simulated Annealing, Genetic Algorithms and Reinforcement Learning, respectively. Meanwhile, Mind Mappings returns mappings with only 5.32× higher EDP than a possibly unachievable theoretical lower-bound, indicating proximity to the global optima.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3445814.3446762',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Aion: Enabling Open Systems through Strong Availability Guarantees for Enclaves',\n",
       "  'authors': \"['Fritz Alder', 'Jo Van Bulck', 'Frank Piessens', 'Jan Tobias Mühlberg']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"CCS '21: Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security\",\n",
       "  'abstract': 'Embedded Trusted Execution Environments (TEEs) can provide strong security for software in the IoT or in critical control systems. Approaches to combine this security with real-time and availability guarantees are currently missing. In this paper we present Aion, a configurable security architecture that provides a notion of guaranteed real-time execution for dynamically loaded enclaves. We implement preemptive multitasking and restricted atomicity on top of strong enclave software isolation and attestation. Our approach allows the hardware to enforce confidentiality and integrity protections, while a decoupled small enclaved scheduler software component can enforce availability and guarantee strict deadlines of a bounded number of protected applications, without necessarily introducing a notion of priorities amongst these applications. We implement a prototype on a light-weight TEE processor and provide a case study. Our implementation can guarantee that protected applications can handle interrupts and make progress with deterministic activation latencies, even in the presence of a strong adversary with arbitrary code execution capabilities.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3460120.3484782',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Evaluation of large scale RoI mining applications in edge computing environments',\n",
       "  'authors': \"['Loris Belcastro', 'Alberto Falcone', 'Alfredo Garro', 'Fabrizio Marozzo']\",\n",
       "  'date': 'September 2021',\n",
       "  'source': \"DS-RT '21: Proceedings of the 2021 IEEE/ACM 25th International Symposium on Distributed Simulation and Real Time Applications\",\n",
       "  'abstract': \"Researchers and leading IT companies are increasingly proposing hybrid cloud/edge solutions, which allow to move part of the workload from the cloud to the edge nodes, by reducing the network traffic and energy consumption, but also getting low latency responses near to real time. This paper proposes a novel hybrid cloud/edge architecture for efficiently extracting Regions-of-Interest (RoI) in a large scale urban computing environment, where a huge amount of geotagged data are generated and collected through users's mobile devices. The proposal is organized in two parts: (i) a modeling part that defines the hybrid cloud/edge architecture capable of managing a large number of devices; (ii) a simulation part in which different design choices are evaluated to improve the performance of RoI mining algorithms in terms of processing time, network delay, task failure and computing resource utilization. Several experiments have been carried out to evaluate the performance of the proposed architecture starting from different configurations and orchestration policies. The achieved results showed that the proposed hybrid cloud/edge architecture, with the use of two novel orchestration policies (network- and utilization-based), permits to improve the exploitation of resources, also granting low network latency and task failure rate in comparison with other standard scenarios (only-edge or only-cloud).\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/DS-RT52167.2021.9576131',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Cooperative Slack Management: Saving Energy of Multicore Processors by Trading Performance Slack Between QoS-Constrained Applications',\n",
       "  'authors': \"['Mehrzad Nejat', 'Madhavan Manivannan', 'Miquel Pericàs', 'Per Stenström']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Architecture and Code Optimization',\n",
       "  'abstract': 'Processor resources can be adapted at runtime according to the dynamic behavior of applications to reduce the energy consumption of multicore processors without affecting the Quality-of-Service (QoS). To achieve this, an online resource management scheme is needed to control processor configurations such as cache partitioning, dynamic voltage-frequency scaling, and dynamic adaptation of core resources.Prior State-of-the-art has shown the potential for reducing energy without any performance degradation by coordinating the control of different resources. However, in this article, we show that by allowing short-term variations in processing speed (e.g., instructions per second rate), in a controlled fashion, we can enable substantial improvements in energy savings while maintaining QoS. We keep track of such variations in the form of performance slack. Slack can be generated, at some energy cost, by processing faster than the performance target. On the other hand, it can be utilized to save energy by allowing a temporary relaxation in the performance target. Based on this insight, we present Cooperative Slack Management (CSM). During runtime, CSM finds opportunities to generate slack at low energy cost by estimating the performance and energy for different resource configurations using analytical models. This slack is used later when it enables larger energy savings. CSM performs such trade-offs across multiple applications, which means that the slack collected for one application can be used to reduce the energy consumption of another. This cooperative approach significantly increases the opportunities to reduce system energy compared with independent slack management for each application. For example, we show that CSM can potentially save up to 41% of system energy (on average, 25%) in a scenario in which both prior art and an extended version with local slack management for each core are ineffective.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3505559',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'G-GPU: a fully-automated generator of GPU-like ASIC accelerators',\n",
       "  'authors': \"['Tiago D. Perez', 'Márcio M. Gonçalves', 'Leonardo Gobatto', 'Marcelo Brandalero', 'José Rodrigo Azambuja', 'Samuel Pagliarini']\",\n",
       "  'date': 'March 2022',\n",
       "  'source': \"DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe\",\n",
       "  'abstract': 'Modern Systems on Chip (SoC), almost as a rule, require accelerators for achieving energy efficiency and high performance for specific tasks that are not necessarily well suited for execution in standard processing units. Considering the broad range of applications and necessity for specialization, the design of SoCs has thus become expressively more challenging. In this paper, we put forward the concept of G-GPU, a general-purpose GPU-like accelerator that is not application-specific but still gives benefits in energy efficiency and throughput. Furthermore, we have identified an existing gap for these accelerators in ASIC, for which no known automated generation platform/tool exists. Our solution, called GPUPlanner, is an open-source generator of accelerators, from RTL to GDSII, that addresses this gap. Our analysis results show that our automatically generated G-GPU designs are remarkably efficient when compared against the popular CPU architecture RISC-V, presenting speed-ups of up to 223 times in raw performance and up to 11 times when the metric is performance derated by area. These results are achieved by executing a design space exploration of the GPU-like accelerators, where the memory hierarchy is broken in a smart fashion and the logic is pipelined on demand. Finally, tapeout-ready layouts of the G-GPU in 65nm CMOS are presented.',\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3539845.3539972',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Microarchitectural Exploration of STT-MRAM Last-level Cache Parameters for Energy-efficient Devices',\n",
       "  'authors': \"['Tommaso Marinelli', 'José Ignacio Gómez Pérez', 'Christian Tenllado', 'Manu Komalan', 'Mohit Gupta', 'Francky Catthoor']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Embedded Computing Systems',\n",
       "  'abstract': 'As the technology scaling advances, limitations of traditional memories in terms of density and energy become more evident. Modern caches occupy a large part of a CPU physical size and high static leakage poses a limit to the overall efficiency of the systems, including IoT/edge devices. Several alternatives to CMOS SRAM memories have been studied during the past few decades, some of which already represent a viable replacement for different levels of the cache hierarchy. One of the most promising technologies is the spin-transfer torque magnetic RAM (STT-MRAM), due to its small basic cell design, almost absent static current and non-volatility as an added value. However, nothing comes for free, and designers will have to deal with other limitations, such as the higher latencies and dynamic energy consumption for write operations compared to reads. The goal of this work is to explore several microarchitectural parameters that may overcome some of those drawbacks when using STT-MRAM as last-level cache (LLC) in embedded devices. Such parameters include: number of cache banks, number of miss status handling registers (MSHRs) and write buffer entries, presence of hardware prefetchers. We show that an effective tuning of those parameters may virtually remove any performance loss while saving more than 60% of the LLC energy on average. The analysis is then extended comparing the energy results from calibrated technology models with data obtained with freely available tools, highlighting the importance of using accurate models for architectural exploration.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3490391',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A Case for Precise, Fine-Grained Pointer Synthesis in High-Level Synthesis',\n",
       "  'authors': \"['Nadesh Ramanathan', 'George A. Constantinides', 'John Wickerson']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Design Automation of Electronic Systems',\n",
       "  'abstract': 'This article combines two practical approaches to improve pointer synthesis within HLS tools. Both approaches focus on inefficiencies in how HLS tools treat the points-to graph—a mapping that connects each instruction to the memory locations that it might access at runtime. HLS pointer synthesis first computes the points-to graph via pointer analysis and then implements its connections in hardware, which gives rise to two inefficiencies. First, HLS tools typically favour pointer analysis that is fast, sacrificing precision. Second, they also favour centralising memory connections in hardware for instructions that can point to more than one location.In this article, we demonstrate that a more precise pointer analysis coupled with decentralised memory connections in hardware can substantially reduce the unnecessary sharing of memory resources. We implement both flow- and context-sensitive pointer analysis and fine-grained memory connections in two modern HLS tools, LegUp and Vitis HLS. An evaluation on three benchmark suites, ranging from non-trivial pointer use to standard HLS benchmarks, indicates that when we improve both precision and granularity of pointer synthesis, on average, we can reduce area and latency by around 42% and 37%, respectively.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3491430',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'PeQES: a platform for privacy-enhanced quantitative empirical studies',\n",
       "  'authors': \"['Dominik Meißner', 'Felix Engelmann', 'Frank Kargl', 'Benjamin Erb']\",\n",
       "  'date': 'March 2021',\n",
       "  'source': \"SAC '21: Proceedings of the 36th Annual ACM Symposium on Applied Computing\",\n",
       "  'abstract': \"Empirical sciences and in particular psychology suffer a methodological crisis due to the non-reproducibility of results, and in rare cases, questionable research practices. Pre-registered studies and the publication of raw data sets have emerged as effective countermeasures. However, this approach represents only a conceptual procedure and may in some cases exacerbate privacy issues associated with data publications. We establish a novel, privacy-enhanced workflow for pre-registered studies. We also introduce PeQES, a corresponding platform that technically enforces the appropriate execution while at the same time protecting the participants' data from unauthorized use or data repurposing. Our PeQES prototype proves the overall feasibility of our privacy-enhanced workflow while introducing only a negligible performance overhead for data acquisition and data analysis of an actual study. Using trusted computing mechanisms, PeQES is the first platform to enable privacy-enhanced studies, to ensure the integrity of study protocols, and to safeguard the confidentiality of participants' data at the same time.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3412841.3441997',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'LogStore: A Cloud-Native and Multi-Tenant Log Database',\n",
       "  'authors': \"['Wei Cao', 'Xiaojie Feng', 'Boyuan Liang', 'Tianyu Zhang', 'Yusong Gao', 'Yunyang Zhang', 'Feifei Li']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data\",\n",
       "  'abstract': 'With the prevalence of cloud computing, more and more enterprises are migrating applications to cloud infrastructures. Logs are the key to helping customers understand the status of their applications running on the cloud. They are vital for various scenarios, such as service stability assessment, root cause analysis and user activity profiling. Therefore, it is essential to manage the massive amount of logs collected on the cloud and tap their value. Although various log storages have been widely used in the past few decades, it is still a non-trivial problem to design a cost-effective log storage for cloud applications. It faces challenges of heavy write throughput of tens of millions of log records per second, retrieval on PB-level logs and massive hundreds of thousands of tenants. Traditional log processing systems cannot satisfy all these requirements. To address these challenges, we propose the cloud-native log database LogStore. It combines shared-nothing and shared-data architecture, and utilizes highly scalable and low-cost cloud object storage, while overcoming the bandwidth limitations and high latency of using remote storage when writing a large number of logs. We also propose a multi-tenant management method that physically isolates tenant data to ensure compliance and flexible data expiration policies, and uses a novel traffic scheduling algorithm to mitigate the impact of traffic skew and hotspots among tenants. In addition, we design an efficient column index structure LogBlock to support queries with full-text search, and combined several query optimization techniques to reduce query latency on cloud object storage. LogStore has been deployed in Alibaba Cloud on a large scale (more than 500 machines), processing logs of more than 100 GB per second, and has been running stably for more than two years.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3448016.3457565',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'FlexOS: towards flexible OS isolation',\n",
       "  'authors': \"['Hugo Lefeuvre', 'Vlad-Andrei Bădoiu', 'Alexander Jung', 'Stefan Lucian Teodorescu', 'Sebastian Rauch', 'Felipe Huici', 'Costin Raiciu', 'Pierre Olivier']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': \"At design time, modern operating systems are locked in a specific safety and isolation strategy that mixes one or more hardware/software protection mechanisms (e.g. user/kernel separation); revisiting these choices after deployment requires a major refactoring effort. This rigid approach shows its limits given the wide variety of modern applications' safety/performance requirements, when new hardware isolation mechanisms are rolled out, or when existing ones break.   We present FlexOS, a novel OS allowing users to easily specialize the safety and isolation strategy of an OS at compilation/deployment time instead of design time. This modular LibOS is composed of fine-grained components that can be isolated via a range of hardware protection mechanisms with various data sharing strategies and additional software hardening. The OS ships with an exploration technique helping the user navigate the vast safety/performance design space it unlocks. We implement a prototype of the system and demonstrate, for several applications (Redis/Nginx/SQLite), FlexOS' vast configuration space as well as the efficiency of the exploration technique: we evaluate 80 FlexOS configurations for Redis and show how that space can be probabilistically subset to the 5 safest ones under a given performance budget. We also show that, under equivalent configurations, FlexOS performs similarly or better than existing solutions which use fixed safety configurations.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503222.3507759',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'MIMHD: accurate and efficient hyperdimensional inference using multi-bit in-memory computing',\n",
       "  'authors': \"['Arman Kazemi', 'Mohammad Mehdi Sharifi', 'Zhuowen Zou', 'Michael Niemier', 'X. Sharon Hu', 'Mohsen Imani']\",\n",
       "  'date': 'July 2021',\n",
       "  'source': \"ISLPED '21: Proceedings of the ACM/IEEE International Symposium on Low Power Electronics and Design\",\n",
       "  'abstract': 'Hyperdimensional Computing (HDC) is an emerging computational framework that mimics important brain functions by operating over high-dimensional vectors, called hypervectors (HVs). In-memory computing implementations of HDC are desirable since they can significantly reduce data transfer overheads. All existing in-memory HDC platforms consider binary HVs where each dimension is represented with a single bit. However, utilizing multi-bit HVs allows HDC to achieve acceptable accuracies in lower dimensions which in turn leads to higher energy efficiencies. Thus, we propose a highly accurate and efficient multi-bit in-memory HDC inference platform called MIMHD. MIMHD supports multi-bit operations using ferroelectric field-effect transistor (FeFET) crossbar arrays for multiply-and-add and FeFET multi-bit content-addressable memories for associative search. We also introduce a novel hardware-aware retraining framework (HWART) that trains the HDC model to learn to work with MIMHD. For six popular datasets and 4000 dimension HVs, MIMHD using 3-bit (2-bit) precision HVs achieves (i) average accuracies of 92.6% (88.9%) which is 8.5% (4.8%) higher than binary implementations; (ii) 84.1x (78.6x) energy improvement over a GPU, and (iii) 38.4x (34.3x) speedup over a GPU, respectively. The 3-bit MIMHD is 4.3x and 13x faster and more energy-efficient than binary HDC accelerators while achieving similar accuracies.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISLPED52811.2021.9502498',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'LB Scalability: Achieving the Right Balance Between Being Stateful and Stateless',\n",
       "  'authors': \"['Reuven Cohen', 'Matty Kadosh', 'Alan Lo', 'Qasem Sayah']\",\n",
       "  'date': 'None',\n",
       "  'source': 'IEEE/ACM Transactions on Networking',\n",
       "  'abstract': 'A high performance Layer-4 load balancer (LB) is one of the most important components of a cloud service infrastructure. Such an LB uses network and transport layer information for deciding how to distribute client requests across a group of servers. A crucial requirement for a stateful LB is per connection consistency (PCC); namely, that all the packets of the same connection will be forwarded to the same server, as long as the server is alive, even if the pool of servers or the assignment function changes. The challenge is in designing a high throughput, low latency solution that is also scalable. This paper proposes a highly scalable LB, called Prism, implemented using a programmable switch ASIC. As far as we know, Prism is the first reported stateful LB that can process millions of connections per second and hundreds of millions connections in total, while ensuring PCC. This is due to the fact that Prism forwards all the packets in hardware, even during server pool changes, <bold>while avoiding the need to maintain a hardware state per every active connection</bold>. We implemented a prototype of the proposed architecture and showed that Prism can scale to 100 million simultaneous connections, and can accommodate more than one pool update per second.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/TNET.2021.3112517',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'JFMNet: Joint Fusion Multi-Networks for Image Dehazing and Denoising in The Port Environment',\n",
       "  'authors': \"['Guancheng Lin', 'Yijie Zheng', 'Zhihong Xu', 'Tianzhi Xia', 'Peng Yuan']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"ICMLC '22: Proceedings of the 2022 14th International Conference on Machine Learning and Computing\",\n",
       "  'abstract': \"The bad weather events, such as haze, in maritime traffic dramatically reduce the visibility, which can seriously affect the ship navigation especially in areas with intensive port traffic. Meanwhile, unwanted signals are inevitably introduced by the maritime imaging device during image capturing and transmission in hazy conditions. Therefore, the captured image is not only degraded by the haze, but also may contain unwanted noise. These low-quality images interfere with the subsequent image processing and increase the potential for maritime traffic accidents. It is therefore imperative to improve the image quality in hazy conditions. To reveal the information hidden in the haze while suppress noise, this paper proposes the joint fusion multi-networks (termed JFMNet) for Image dehazing and denoising in the port environment. The multi-networks use the dehazing module (DHNet) and the denoising module (DNNet) to suppress the noise and haze. Then use the information fusion module (FNet) to integrate the results of the DNNet and DHNet with the information of the original input images to achieve the goal of dehazing and denoising while preserving the details. The modules in multi-networks are based on an encoder-decoder structure. Experiments on a number of challenging hazy images with noise are present to reveal the efficacy of this structure. Meanwhile, experiments also show our JFMNet's superiority over several state-of-the-arts in terms of dehaze quality and efficiency.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3529836.3529923',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'VeGen: a vectorizer generator for SIMD and beyond',\n",
       "  'authors': \"['Yishen Chen', 'Charith Mendis', 'Michael Carbin', 'Saman Amarasinghe']\",\n",
       "  'date': 'April 2021',\n",
       "  'source': \"ASPLOS '21: Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'Vector instructions are ubiquitous in modern processors. Traditional compiler auto-vectorization techniques have focused on targeting single instruction multiple data (SIMD) instructions. However, these auto-vectorization techniques are not sufficiently powerful to model non-SIMD vector instructions, which can accelerate applications in domains such as image processing, digital signal processing, and machine learning. To target non-SIMD instruction, compiler developers have resorted to complicated, ad hoc peephole optimizations, expending significant development time while still coming up short. As vector instruction sets continue to rapidly evolve, compilers cannot keep up with these new hardware capabilities.   In this paper, we introduce Lane Level Parallelism (LLP), which captures the model of parallelism implemented by both SIMD and non-SIMD vector instructions. We present VeGen, a vectorizer generator that automatically generates a vectorization pass to uncover target-architecture-specific LLP in programs while using only instruction semantics as input. VeGen decouples, yet coordinates automatically generated target-specific vectorization utilities with its target-independent vectorization algorithm. This design enables us to systematically target non-SIMD vector instructions that until now require ad hoc coordination between different compiler stages. We show that VeGen can use non-SIMD vector instructions effectively, for example, getting speedup 3× (compared to LLVM’s vectorizer) on x265’s idct4 kernel.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3445814.3446692',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A low-cost methodology for EM fault emulation on FPGA',\n",
       "  'authors': \"['Paolo Maistri', 'Jiayun Po']\",\n",
       "  'date': 'March 2022',\n",
       "  'source': \"DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe\",\n",
       "  'abstract': 'In embedded systems, the presence of a security layer is now a well-established requirement. In order to guarantee the suitable level of performance and resistance against attacks, dedicated hardware implementations are often proposed to accelerate cryptographic computations in a controllable environment. On the other hand, these same implementations may be vulnerable to physical attacks, such as side channel analysis or fault injections. In this scenario, the designer must hence be able to assess the robustness of the implementation (and of the adopted countermeasures) as soon as possible in the design flow against several different threats. In this paper, we propose a methodology to characterize the robustness of a generic hardware design described at RTL against EM fault injections. Thanks to our framework, we are able to emulate the EM faults on FPGA platforms, without the need of expensive equipment or lengthy experimental campaigns. We present a tool supporting our methodology and the first validations tests done on several AES designs confirming the feasibility of the proposed approach.',\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3539845.3540127',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Q-VR: system-level design for future mobile collaborative virtual reality',\n",
       "  'authors': \"['Chenhao Xie', 'Xie Li', 'Yang Hu', 'Huwan Peng', 'Michael Taylor', 'Shuaiwen Leon Song']\",\n",
       "  'date': 'April 2021',\n",
       "  'source': \"ASPLOS '21: Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': \"High Quality Mobile Virtual Reality (VR) is what the incoming graphics technology era demands: users around the world, regardless of their hardware and network conditions, can all enjoy the immersive virtual experience. However, the state-of-the-art software-based mobile VR designs cannot fully satisfy the realtime performance requirements due to the highly interactive nature of user's actions and complex environmental constraints during VR execution. Inspired by the unique human visual system effects and the strong correlation between VR motion features and realtime hardware-level information, we propose Q-VR, a novel dynamic collaborative rendering solution via software-hardware co-design for enabling future low-latency high-quality mobile VR. At software-level, Q-VR provides flexible high-level tuning interface to reduce network latency while maintaining user perception. At hardware-level, Q-VR accommodates a wide spectrum of hardware and network conditions across users by effectively leveraging the computing capability of the increasingly powerful VR hardware. Extensive evaluation on real-world games demonstrates that Q-VR can achieve an average end-to-end performance speedup of 3.4x (up to 6.7x) over the traditional local rendering design in commercial VR devices, and a 4.1x frame rate improvement over the state-of-the-art static collaborative rendering.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3445814.3446715',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'ThundeRiNG: generating multiple independent random number sequences on FPGAs',\n",
       "  'authors': \"['Hongshi Tan', 'Xinyu Chen', 'Yao Chen', 'Bingsheng He', 'Weng-Fai Wong']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ICS '21: Proceedings of the ACM International Conference on Supercomputing\",\n",
       "  'abstract': 'In this paper, we propose ThundeRiNG, a resource-efficient and high-throughput system for generating multiple independent sequences of random numbers (MISRN) on FPGAs. Generating MISRN can be a time-consuming step in many applications such as numeric computation and approximate computing. Despite that decades of studies on generating a single sequence of random numbers on FPGAs have achieved very high throughput and high quality of randomness, existing MISRN approaches either suffer from heavy resource consumption or fail to achieve statistical independence among sequences. In contrast, ThundeRiNG resolves the dependence by using a resource-efficient decorrelator among multiple sequences, guaranteeing a high statistical quality of randomness. Moreover, ThundeRiNG develops a novel state sharing among a massive number of pseudo-random number generator instances on FPGAs. The experimental results show that ThundeRiNG successfully passes the widely used statistical test, TestU01, only consumes a constant number of DSPs (less than 1% of the FPGA resource capacity) for generating any number of sequences, and achieves a throughput of 655 billion random numbers per second. Compared to the state-of-the-art GPU library, ThundeRiNG demonstrates a 10.62x speedup on MISRN and delivers up to 9.15x performance and 26.63x power efficiency improvement on two applications (pi estimation and Monte Carlo option pricing). This work is open-sourced on Github at https://github.com/Xtra-Computing/ThundeRiNG.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3447818.3461664',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Tile size selection of affine programs for GPGPUs using polyhedral cross-compilation',\n",
       "  'authors': \"['Khaled Abdelaal', 'Martin Kong']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ICS '21: Proceedings of the ACM International Conference on Supercomputing\",\n",
       "  'abstract': 'Loop tiling is a key high-level transformation which is known to maximize locality in loop intensive programs. It has been successfully applied to a number of applications including tensor contractions, iterative stencils and machine learning. This technique has also been extended to a wide variety of computational domains and architectures. The performance achieved with this critical transformation largely depends on a set of inputs given, the tile sizes, due to the complex trade-off between locality and parallelism. This problem is exacerbated in GPGPU architectures due to limited hardware resources such as the available shared-memory. In this paper we present a new technique to compute resource conscious tile sizes for affine programs. We use Integer Linear Programming (ILP) constraints and objectives in a cross-compiler fashion to faithfully and effectively mimic the transformations applied in a polyhedral GPU compiler (PPCG). Our approach significantly reduces the need for experimental auto-tuning by generating only two tile size configurations that achieve strong out-of-the-box performance. We evaluate the effectiveness of our technique using the Polybench benchmark suite on two GPGPUs, an AMD Radeon VII and an NVIDIA Tesla V100, using OpenCL and CUDA programming models. Experimental validation reveals that our approach achieves nearly 75% of the best empirically found tile configuration across both architectures.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3447818.3460369',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Accurate and Accelerated Neuromorphic Network Design Leveraging A Bayesian Hyperparameter Pareto Optimization Approach',\n",
       "  'authors': \"['Maryam Parsa', 'Catherine Schuman', 'Nitin Rathi', 'Amir Ziabari', 'Derek Rose', 'J. Parker Mitchell', 'J. Travis Johnston', 'Bill Kay', 'Steven Young', 'Kaushik Roy']\",\n",
       "  'date': 'July 2021',\n",
       "  'source': 'ICONS 2021: International Conference on Neuromorphic Systems 2021',\n",
       "  'abstract': 'Neuromorphic systems allow for extremely efficient hardware implementations for neural networks (NNs). In recent years, several algorithms have been presented to train spiking NNs (SNNs) for neuromorphic hardware. However, SNNs often provide lower accuracy than their artificial NNs (ANNs) counterparts or require computationally expensive and slow training/inference methods. To close this gap, designers typically rely on reconfiguring SNNs through adjustments in the neuron/synapse model or training algorithm itself. Nevertheless, these steps incur significant design time, while still lacking the desired improvement in terms of training/inference times (latency). Designing SNNs that can mimic the accuracy of ANNs with reasonable training times is an exigent challenge in neuromorphic computing. In this work, we present an alternative approach that looks at such designs as an optimization problem rather than algorithm or architecture redesign. We develop a versatile multiobjective hyperparameter optimization (HPO) for automatically tuning HPs of two state-of-the-art SNN training algorithms, SLAYER and HYBRID. We emphasize that, to the best of our knowledge, this is the first work trying to improve SNNs’ computational efficiency, accuracy, and training time using an efficient HPO. We demonstrate significant performance improvements for SNNs on several datasets without the need to redesign or invent new training algorithms/architectures. Our approach results in more accurate networks with lower latency and, in turn, higher energy efficiency than previous implementations. In particular, we demonstrate improvement in accuracy and more than 5 × reduction in the training/inference time for the SLAYER algorithm on the DVS Gesture dataset. In the case of HYBRID, we demonstrate 30% reduction in timesteps while surpassing the accuracy of the state-of-the-art networks on CIFAR10. Further, our analysis suggests that even a seemingly minor change in HPs could change the accuracy by 5 − 6 ×.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3477145.3477160',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Benchmarking Quantum Computers and the Impact of Quantum Noise',\n",
       "  'authors': \"['Salonik Resch', 'Ulya R. Karpuzcu']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Computing Surveys',\n",
       "  'abstract': 'Benchmarking is how the performance of a computing system is determined. Surprisingly, even for classical computers this is not a straightforward process. One must choose the appropriate benchmark and metrics to extract meaningful results. Different benchmarks test the system in different ways, and each individual metric may or may not be of interest. Choosing the appropriate approach is tricky. The situation is even more open ended for quantum computers, where there is a wider range of hardware, fewer established guidelines, and additional complicating factors. Notably, quantum noise significantly impacts performance and is difficult to model accurately. Here, we discuss benchmarking of quantum computers from a computer architecture perspective and provide numerical simulations highlighting challenges that suggest caution.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3464420',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Blockchain-based root of trust management in security credential management system for vehicular communications',\n",
       "  'authors': \"['Arijet Sarker', 'SangHyun Byun', 'Wenjun Fan', 'Sang-Yoon Chang']\",\n",
       "  'date': 'March 2021',\n",
       "  'source': \"SAC '21: Proceedings of the 36th Annual ACM Symposium on Applied Computing\",\n",
       "  'abstract': \"Security Credential Management System (SCMS) provides the Public Key Infrastructure (PKI) for vehicular networking. SCMS builds the state-of-the-art distributed PKI to protect the vehicular networking privacy against an honest-but-curious authority (by the use of multiple PKI authorities) and to decentralize the PKI root of trust (by the Elector-Based Root Management or EBRM, having the distributed electors manage the Root Certificate Authority or RCA). We build on the EBRM architecture and construct a Blockchain-Based Root Management (BBRM) to provide even greater decentralization and security. More specifically, BBRM uses blockchain to i) replace the existing RCA and have the electors directly involved in the root certificate generation, ii) control the elector network membership including elector addition and revocation, and iii) provide greater accountability and transparency on the aforementioned functionalities. We implement BBRM on Hyperledger Fabric using smart contract for system experimentation and analyses. Our experiments show that BBRM is lightweight in processing, efficient in ledger size, and supports a bandwidth of multiple transactions per second. Our results show that the BBRM blockchain is appropriate for the root certificate generation and the elector membership control for EBRM within SCMS, which are significantly smaller in number and occurrences than the SCMS outputs of vehicle certificates. We also experiment to analyze how the BBRM distributed consensus protocol parameters, such as the number of electors and the number of required votes, affect the overall scheme's performances.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3412841.3441905',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'The Physical Design of Biological Systems - Insights from the Fly Brain',\n",
       "  'authors': \"['Louis K. Scheffer']\",\n",
       "  'date': 'March 2021',\n",
       "  'source': \"ISPD '21: Proceedings of the 2021 International Symposium on Physical Design\",\n",
       "  'abstract': \"Many different physical substrates can support complex computation. This is particularly apparent when considering human made and biological systems that perform similar functions, such as visually guided navigation. In common, however, is the need for good physical design, as such designs are smaller, faster, lighter, and lower power, factors in both the jungle and the marketplace. Although the physical design of man-made systems is relatively well understood, the physical design of biological computation has remained murky due to a lack of detailed information on their construction. The recent EM (electron microscope) reconstruction of the central brain of the fruit fly now allows us to start to examine these issues. Here we look at the physical design of the fly brain, including such factors as fan-in and fanout, logic depth, division into physical compartments and how this affects electrical response, pin to computation ratios (Rent's rule), and other physical characteristics of at least one biological computation substrate. From this we speculate on how physical design algorithms might change if the target implementation was a biological neural network.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3439706.3446898',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'We need kernel interposition over the network dataplane',\n",
       "  'authors': \"['Hugo Sadok', 'Zhipeng Zhao', 'Valerie Choung', 'Nirav Atre', 'Daniel S. Berger', 'James C. Hoe', 'Aurojit Panda', 'Justine Sherry']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"HotOS '21: Proceedings of the Workshop on Hot Topics in Operating Systems\",\n",
       "  'abstract': 'Kernel-bypass networking, which allows applications to circumvent the kernel and interface directly with NIC hardware, is one of the main tools for improving application network performance. However, allowing applications to circumvent the kernel makes it impossible to use tools (e.g., tcpdump) or impose policies (e.g., QoS and filters) that need to interpose on traffic sent by different applications running on a host. This makes maintainability and manageability a challenge for kernel-bypass applications. In response, we propose Kernel On-Path Interposition (KOPI), in which traditional kernel data-plane functionality is retained but implemented in a fully programmable SmartNIC. We hypothesize that KOPI can support the same tools and policies as the kernel stack while retaining the performance benefits of kernel bypass.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3458336.3465281',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Experimental Teaching Reform of Computer Graphics Oriented to GPU',\n",
       "  'authors': \"['Lin Wan', 'Ying Cheng']\",\n",
       "  'date': 'May 2021',\n",
       "  'source': 'ICMET 2021: 2021 3rd International Conference on Modern Educational Technology',\n",
       "  'abstract': 'By analyzing the current situation of \"computer graphics\" experimental teaching, the results point out that there are deficiencies in guiding students to recognize GPU (Graphics Processing Unit) in the current experimental teaching. Through the construction of GPU rendering architecture cognitive experiments, GPU computing performance cognitive experiments and GPU graphics interface consistency testing experiments, the progressive planning of each part, the introduction of shaders and shading language, the design of typical cases such as particle system and deferred rendering, a complete graphics experiment system oriented to GPU is obtained, which effectively supplements and improves the original course experiment.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3468978.3468983',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'SparseCore: stream ISA and processor specialization for sparse computation',\n",
       "  'authors': \"['Gengyu Rao', 'Jingji Chen', 'Jason Yik', 'Xuehai Qian']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'Computation on sparse data is becoming increasingly important for many applications. Recent sparse computation accelerators are designed for specific algorithm/application, making them inflexible with software optimizations. This paper proposes SparseCore, the first general-purpose processor extension for sparse computation that can flexibly accelerate complex code patterns and fast-evolving algorithms. We extend the instruction set architecture (ISA) to make stream or sparse vector first-class citizens, and develop efficient architectural components to support the stream ISA. The novel ISA extension intrinsically operates on streams, realizing both efficient data movement and computation. The simulation results show that SparseCore achieves significant speedups for sparse tensor computation and graph pattern computation.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503222.3507705',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Experience Paper: Danaus: isolation and efficiency of container I/O at the client side of network storage',\n",
       "  'authors': \"['Giorgos Kappes', 'Stergios V. Anastasiadis']\",\n",
       "  'date': 'December 2021',\n",
       "  'source': \"Middleware '21: Proceedings of the 22nd International Middleware Conference\",\n",
       "  'abstract': 'Containers are a mainstream virtualization technique commonly used to run stateful workloads over persistent storage. In multi-tenant hosts with high utilization, resource contention at the system kernel often leads to inefficient handling of the container I/O. Assuming a distributed storage architecture for scalability, resource sharing is particularly problematic at the client hosts serving the applications of competing tenants. Although increasing the scalability of a system kernel can improve resource efficiency, it is highly challenging to refactor the kernel for fair access to system services. As a realistic alternative, we isolate the storage I/O paths of different tenants by serving them with distinct clients running at user level. We introduce the Danaus client architecture to let each tenant access the container root and application filesystems over a private host path. We developed a Danaus prototype that integrates a union filesystem with a Ceph distributed filesystem client and a configurable shared cache. Across different host configurations, workloads and systems, Danaus achieves improved performance stability because it handles I/O with reserved per-tenant resources and avoids intensive kernel locking. Danaus offers up to 14.4x higher throughput than a popular kernel-based client under conditions of I/O contention. In comparison to a FUSE-based user-level client, Danaus also reduces by 14.2x the time to start 256 high-performance webservers. Based on our extensive experience from building and evaluating Danaus, we share several valuable lessons that we learned about resource contention, file management, service separation and performance stability.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3464298.3493390',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'ViK: practical mitigation of temporal memory safety violations through object ID inspection',\n",
       "  'authors': \"['Haehyun Cho', 'Jinbum Park', 'Adam Oest', 'Tiffany Bao', 'Ruoyu Wang', 'Yan Shoshitaishvili', 'Adam Doupé', 'Gail-Joon Ahn']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'Temporal memory safety violations, such as use-after-free (UAF) vulnerabilities, are a critical security issue for software written in memory-unsafe languages such as C and C++.   In this paper, we introduce ViK, a novel, lightweight, and widely applicable runtime defense that can protect both operating system (OS) kernels and user-space applications against temporal memory safety violations. ViK performs object ID inspection, where it assigns a random identifier to every allocated object and stores the identifier in the unused bits of the corresponding pointer. When the pointer is used, ViK inspects the value of a pointer before dereferencing, ensuring that the pointer still references the original object. To the best of our knowledge, this is the first mitigation against temporal memory safety violations that scales to OS kernels. We evaluated the software prototype of ViK on Android and Linux kernels and observed runtime overhead of around 20%. Also, we evaluated a hardware-assisted prototype of ViK on Android kernel, where the runtime overhead was as low as 2%.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503222.3507780',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'REVAMP: a systematic framework for heterogeneous CGRA realization',\n",
       "  'authors': \"['Thilini Kaushalya Bandara', 'Dhananjaya Wijerathne', 'Tulika Mitra', 'Li-Shiuan Peh']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'Coarse-Grained Reconfigurable Architectures (CGRAs) provide an excellent balance between performance, energy efficiency, and flexibility. However, increasingly sophisticated applications, especially on the edge devices, demand even better energy efficiency for longer battery life.   Most CGRAs adhere to a canonical structure where a homogeneous set of processing elements and memories communicate through a regular interconnect due to the simplicity of the design. Unfortunately, the homogeneity leads to substantial idle resources while mapping irregular applications and creates inefficiency. We plan to mitigate the inefficiency by systematically and judiciously introducing heterogeneity in CGRAs in tandem with appropriate compiler support.   We propose REVAMP, an automated design space exploration framework that helps architects uncover and add pertinent heterogeneity to a diverse range of originally homogeneous CGRAs when fed with a suite of target applications. REVAMP explores a comprehensive set of optimizations encompassing compute, network, and memory heterogeneity, thereby converting a uniform CGRA into a more irregular architecture with improved energy efficiency. As CGRAs are inherently software scheduled, any micro-architectural optimizations need to be partnered with corresponding compiler support, which is challenging with heterogeneity. The REVAMP framework extends compiler support for efficient mapping of loop kernels on the derived heterogeneous CGRA architectures.   We showcase REVAMP on three state-of-the-art homogeneous CGRAs, demonstrating how REVAMP derives a heterogeneous variant of each homogeneous architecture, with its corresponding compiler optimizations. Our results show that the derived heterogeneous architectures achieve up to 52.4% power reduction, 38.1% area reduction, and 36% average energy reduction over the corresponding homogeneous versions with minimal performance impact for the selected kernel suite.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503222.3507772',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'BPLight-CNN: A Photonics-Based Backpropagation Accelerator for Deep Learning',\n",
       "  'authors': \"['Dharanidhar Dang', 'Sai Vineel Reddy Chittamuru', 'Sudeep Pasricha', 'Rabi Mahapatra', 'Debashis Sahoo']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Journal on Emerging Technologies in Computing Systems',\n",
       "  'abstract': 'Training deep learning networks involves continuous weight updates across the various layers of the deep network while using a backpropagation (BP) algorithm. This results in expensive computation overheads during training. Consequently, most deep learning accelerators today employ pretrained weights and focus only on improving the design of the inference phase. The recent trend is to build a complete deep learning accelerator by incorporating the training module. Such efforts require an ultra-fast chip architecture for executing the BP algorithm. In this article, we propose a novel photonics-based backpropagation accelerator for high-performance deep learning training. We present the design for a convolutional neural network (CNN), BPLight-CNN, which incorporates the silicon photonics-based backpropagation accelerator. BPLight-CNN is a first-of-its-kind photonic and memristor-based CNN architecture for end-to-end training and prediction. We evaluate BPLight-CNN using a photonic CAD framework (IPKISS) on deep learning benchmark models, including LeNet and VGG-Net. The proposed design achieves (i) at least 34× speedup, 34× improvement in computational efficiency, and 38.5× energy savings during training; and (ii) 29× speedup, 31× improvement in computational efficiency, and 38.7× improvement in energy savings during inference compared with the state-of-the-art designs. All of these comparisons are done at a 16-bit resolution, and BPLight-CNN achieves these improvements at a cost of approximately 6% lower accuracy compared with the state-of-the-art.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3446212',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'TMO: transparent memory offloading in datacenters',\n",
       "  'authors': \"['Johannes Weiner', 'Niket Agarwal', 'Dan Schatzberg', 'Leon Yang', 'Hao Wang', 'Blaise Sanouillet', 'Bikash Sharma', 'Tejun Heo', 'Mayank Jain', 'Chunqiang Tang', 'Dimitrios Skarlatos']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'The unrelenting growth of the memory needs of emerging datacenter applications, along with ever increasing cost and volatility of DRAM prices, has led to DRAM being a major infrastructure expense. Alternative technologies, such as NVMe SSDs and upcoming NVM devices, offer higher capacity than DRAM at a fraction of the cost and power. One promising approach is to transparently offload colder memory to cheaper memory technologies via kernel or hypervisor techniques. The key challenge, however, is to develop a datacenter-scale solution that is robust in dealing with diverse workloads and large performance variance of different offload devices such as compressed memory, SSD, and NVM. This paper presents TMO, Meta’s transparent memory offloading solution for heterogeneous datacenter environments. TMO introduces a new Linux kernel mechanism that directly measures in realtime the lost work due to resource shortage across CPU, memory, and I/O. Guided by this information and without any prior application knowledge, TMO automatically adjusts how much memory to offload to heterogeneous devices (e.g., compressed memory or SSD) according to the device’s performance characteristics and the application’s sensitivity to memory-access slowdown. TMO holistically identifies offloading opportunities from not only the application containers but also the sidecar containers that provide infrastructure-level functions. To maximize memory savings, TMO targets both anonymous memory and file cache, and balances the swap-in rate of anonymous memory and the reload rate of file pages that were recently evicted from the file cache. TMO has been running in production for more than a year, and has saved between 20-32% of the total memory across millions of servers in our large datacenter fleet. We have successfully upstreamed TMO into the Linux kernel.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503222.3507731',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Distributed federated service chaining for heterogeneous network environments',\n",
       "  'authors': \"['Chen Chen', 'Lars Nagel', 'Lin Cui', 'Fung Po Tso']\",\n",
       "  'date': 'December 2021',\n",
       "  'source': \"UCC '21: Proceedings of the 14th IEEE/ACM International Conference on Utility and Cloud Computing\",\n",
       "  'abstract': 'Future networks are expected to support cross-domain, cost-aware and fine-grained services in an efficient and flexible manner. Service Function Chaining (SFC) has been introduced as a promising approach to deliver these services. In the literature, centralized resource orchestration is usually employed to process SFC requests and manage computing and network resources. However, centralized approaches inhibit the scalability and domain autonomy in multi-domain networks. They also neglect location and hardware dependencies of service chains. In this paper, we propose federated service chaining, a distributed framework which orchestrates and maintains the SFC placement while sharing a minimal amount of domain information and control. We first formulate a deployment cost minimization problem as an Integer Linear Programming (ILP) problem with fine-grained constraints for location and hardware dependencies, which is NP-hard. We then devise a Distributed Federated Service Chaining placement approach (DFSC) using inter-domain paths and border nodes information. Our extensive experiments demonstrate that DFSC efficiently optimizes the deployment cost, supports domain autonomy and enables faster decision-making. The results show that DFSC finds solutions within a factor 1.15 of the optimal solution. Compared to a centralized approach in the literature, DFSC reduces the deployment cost by 12% while being one order of magnitude faster.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3468737.3494091',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'HeteroFuzz: fuzz testing to detect platform dependent divergence for heterogeneous applications',\n",
       "  'authors': \"['Qian Zhang', 'Jiyuan Wang', 'Miryung Kim']\",\n",
       "  'date': 'August 2021',\n",
       "  'source': 'ESEC/FSE 2021: Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering',\n",
       "  'abstract': 'As specialized hardware accelerators like FPGAs become a prominent part of the current computing landscape, software applications are increasingly constructed to leverage heterogeneous architectures. Such a trend is already happening in the domain of machine learning and Internet-of-Things (IoT) systems built on edge devices. Yet, debugging and testing methods for heterogeneous applications are currently lacking. These applications may look similar to regular C/C++ code but include hardware synthesis details in terms of preprocessor directives. Therefore, their behavior under heterogeneous architectures may diverge significantly from CPU due to hardware synthesis details. Further, the compilation and hardware simulation cycle takes an enormous amount of time, prohibiting frequent invocations required for fuzz testing.   We propose a novel fuzz testing technique, called HeteroFuzz, designed to specifically target heterogeneous applications and to detect platform-dependent divergence. The key essence of HeteroFuzz is that it uses a three-pronged approach to reduce the long latency of repetitively invoking a hardware simulator on a heterogeneous application. First, in addition to monitoring code coverage as a fuzzing guidance mechanism, we analyze synthesis pragmas in kernel code and monitor accelerator-relevant value spectra. Second, we design dynamic probabilistic mutations to increase the chance of hitting divergent behavior under different platforms. Third, we memorize the boundaries of seen kernel inputs and skip HLS simulator invocation if it can expose only redundant divergent behavior. We evaluate HeteroFuzz on seven real-world heterogeneous applications with FPGA kernels. HeteroFuzz is 754X faster in exposing the same set of distinct divergence symptoms than naive fuzzing. Probabilistic mutations contribute to 17.5X speed up than the one without. Selective invocation of HLS simulation contributes to 8.8X speed up than the one without.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3468264.3468610',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'GREENHOME: A Household Energy Consumption and CO2 Footprint Metering Environment',\n",
       "  'authors': \"['Genoveva Vargas-Solar', 'Maysaa Khalil', 'Javier A. Espinosa-Oviedo', 'José-Luis Zechinelli-Martini']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Internet Technology',\n",
       "  'abstract': 'This article presents the GREENHOME environment, a toolkit providing several data analytical tools for metering household energy consumption and CO2 footprint under different perspectives. GREENHOME enables a multi-perspective analysis of household energy consumption and CO2 footprint using and combining several variables through various statistics and data mining algorithms. To test GREENHOME, the article reports on experiments conducted for modelling and forecasting energy consumption and CO2 footprint in the context of the Triple-A European project.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3505264',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Revizor: testing black-box CPUs against speculation contracts',\n",
       "  'authors': \"['Oleksii Oleksenko', 'Christof Fetzer', 'Boris Köpf', 'Mark Silberstein']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': \"Speculative vulnerabilities such as Spectre and Meltdown expose speculative execution state that can be exploited to leak information across security domains via side-channels. Such vulnerabilities often stay undetected for a long time as we lack the tools for systematic testing of CPUs to find them.   In this paper, we propose an approach to automatically detect microarchitectural information leakage in commercial black-box CPUs. We build on speculation contracts, which we employ to specify the permitted side effects of program execution on the CPU's microarchitectural state. We propose a Model-based Relational Testing (MRT) technique to empirically assess the CPU compliance with these specifications.   We implement MRT in a testing framework called Revizor, and showcase its effectiveness on real Intel x86 CPUs. Revizor automatically detects violations of a rich set of contracts, or indicates their absence. A highlight of our findings is that Revizor managed to automatically surface Spectre, MDS, and LVI, as well as several previously unknown variants.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503222.3507729',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Non-recurring engineering (NRE) best practices: a case study with the NERSC/NVIDIA OpenMP contract',\n",
       "  'authors': \"['Christopher S. Daley', 'Annemarie Southwell', 'Rahulkumar Gayatri', 'Scott Biersdorfff', 'Craig Toepfer', 'Güray Özen', 'Nicholas J. Wright']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"SC '21: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis\",\n",
       "  'abstract': 'The NERSC supercomputer, Perlmutter, consists of AMD CPUs and NVIDIA GPUs. NERSC users expect to be able to use OpenMP to take advantage of the highly capable GPUs. This paper describes how NERSC/NVIDIA constructed a Non-Recurring Engineering (NRE) contract to add OpenMP GPU-offload support to the NVIDIA HPC compilers. The paper describes how the contract incorporated the strengths of both parties and encouraged collaboration to improve the quality of the final deliverable. We include our best practices and how this particular contract took into account emerging OpenMP specifications, NERSC workload requirements, and how to use OpenMP most efficiently on GPU hardware. This paper includes OpenMP application performance results obtained with the NVIDIA compilers distributed in the NVIDIA HPC SDK.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3458817.3476213',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A Survey of Microarchitectural Side-channel Vulnerabilities, Attacks, and Defenses in Cryptography',\n",
       "  'authors': \"['Xiaoxuan Lou', 'Tianwei Zhang', 'Jun Jiang', 'Yinqian Zhang']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Computing Surveys',\n",
       "  'abstract': 'Side-channel attacks have become a severe threat to the confidentiality of computer applications and systems. One popular type of such attacks is the microarchitectural attack, where the adversary exploits the hardware features to break the protection enforced by the operating system and steal the secrets from the program. In this article, we systematize microarchitectural side channels with a focus on attacks and defenses in cryptographic applications. We make three contributions. (1) We survey past research literature to categorize microarchitectural side-channel attacks. Since these are hardware attacks targeting software, we summarize the vulnerable implementations in software, as well as flawed designs in hardware. (2) We identify common strategies to mitigate microarchitectural attacks, from the application, OS, and hardware levels. (3) We conduct a large-scale evaluation on popular cryptographic applications in the real world and analyze the severity, practicality, and impact of side-channel vulnerabilities. This survey is expected to inspire side-channel research community to discover new attacks, and more importantly, propose new defense solutions against them.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3456629',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'MC2-RAM: an in-8T-SRAM computing macro featuring multi-bit charge-domain computing and ADC-reduction weight encoding',\n",
       "  'authors': \"['Zhiyu Chen', 'Qing Jin', 'Jingyu Wang', 'Yanzhi Wang', 'Kaiyuan Yang']\",\n",
       "  'date': 'July 2021',\n",
       "  'source': \"ISLPED '21: Proceedings of the ACM/IEEE International Symposium on Low Power Electronics and Design\",\n",
       "  'abstract': 'In-memory computing (IMC) is a promising hardware architecture to circumvent the memory walls in data-intensive applications, like deep learning. Among various memory technologies, static random-access memory (SRAM) is promising thanks to its high computing accuracy, reliability, and scalability to advanced technology nodes. This paper presents a novel multi-bit capacitive convolution in-SRAM computing macro for high accuracy, high throughput and high efficiency deep learning inference. It realizes fully parallel charge-domain multiply-and-accumulate (MAC) within compact 8-transistor 1-capacitor (8T1C) SRAM arrays that is only 41% larger than the standard 6T cells. It performs MAC with multi-bit activations without conventional digital bit-serial shift-and-add schemes, leading to drastically improved throughput for high-precision CNN models. An ADC-reduction encoding scheme complements the compact sram design, by reducing the number of needed ADCs by half for energy and area savings. A 576x130 macro with 64 ADCs is evaluated in 65nm with post-layout simulations, showing 4.60 TOPS/mm2 compute density and 59.7 TOPS/W energy efficiency with 4/4-bit activations/weights. The MC2-RAM also achieves excellent linearity with only 0.14 mV (4.5% of the LSB) standard deviation of the output voltage in Monte Carlo simulations.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISLPED52811.2021.9502505',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Sentry-NoC: a statically-scheduled NoC for secure SoCs',\n",
       "  'authors': \"['Ahmed Shalaby', 'Yaswanth Tavva', 'Trevor E. Carlson', 'Li-Shiuan Peh']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"NOCS '21: Proceedings of the 15th IEEE/ACM International Symposium on Networks-on-Chip\",\n",
       "  'abstract': \"SoC security has become essential with devices now pervasive in critical infrastructure in homes and businesses. Today's embedded SoCs are becoming increasingly high-performance and complex, comprising multiple cores, accelerators, and IP blocks interconnected with a Network-on-Chip (NoC). As these IPs can originate from diverse sources, they cannot be trusted to form the root of trust in SoCs. However, the NoC itself, being the communication backbone linking all IPs, is naturally positioned to be the basis for a secure SoC. Therefore, there is a need for an efficient solution that both meets the stringent requirements of modern embedded SoC designs, while maintaining a high level of security. In this paper, we demonstrate how statically-scheduled NoCs inherently enforce traffic isolation and non-interference of communication. The time-division multiplexing (TDM) of NoC links across applications provably ensures that security properties are fulfilled. However, conventional TDM NoCs are still vulnerable to side-channel attacks. We thus propose temporal and data obfuscation schemes that can be embedded within static TDM NoCs, randomizing source-destination communication patterns and switching activity over the links. Our proposed statically-scheduled Sentry-NoC links up untrusted IP blocks to form a secure SoC. Sentry-NoC targets key security properties to effectively mitigate side-channel attacks with an extremely low overhead, reducing average temporal correlation by 81% and average data correlation by 91%.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3479876.3481595',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Accelerating Distributed Deep Learning using Multi-Path RDMA in Data Center Networks',\n",
       "  'authors': \"['Feng Tian', 'Yang Zhang', 'Wei Ye', 'Cheng Jin', 'Ziyan Wu', 'Zhi-Li Zhang']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"SOSR '21: Proceedings of the ACM SIGCOMM Symposium on SDN Research (SOSR)\",\n",
       "  'abstract': 'Data center networks (DCNs) have widely deployed RDMA to support data-intensive applications such as machine learning. While DCNs are designed with rich multi-path topology, current RDMA (hardware) technology does not support multi-path transport. In this paper we advance Maestro- a purely software-basedmulti-path RDMA solution - to effectively utilize the rich multi-path topology for load balancing and reliability. As a \"middleware\" operating at the user-space, Maestro is modulaR@and software-defined:Maestro decouples path selection and load balancing mechanisms from hardware features, and allows DCN operators and applications to make flexible decisions by employing the best mechanisms as needed. As such, Maestro can be readily deployed using existing RDMA hardware (NICs) to support distributed deep learning (DDL) applications. Our experiments show that Maestro is capable of fully utilizing multiple paths with negligible CPU overheads, thereby enhancing the performance of DDL applications.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3482898.3483363',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Vectorization for digital signal processors via equality saturation',\n",
       "  'authors': \"['Alexa VanHattum', 'Rachit Nigam', 'Vincent T. Lee', 'James Bornholt', 'Adrian Sampson']\",\n",
       "  'date': 'April 2021',\n",
       "  'source': \"ASPLOS '21: Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'Applications targeting digital signal processors (DSPs) benefit from fast implementations of small linear algebra kernels. While existing auto-vectorizing compilers are effective at extracting performance from large kernels, they struggle to invent the complex data movements necessary to optimize small kernels. To get the best performance, DSP engineers must hand-write and tune specialized small kernels for a wide spectrum of applications and architectures. We present Diospyros, a search-based compiler that automatically finds efficient vectorizations and data layouts for small linear algebra kernels. Diospyros combines symbolic evaluation and equality saturation to vectorize computations with irregular structure. We show that a collection of Diospyros-compiled kernels outperform implementations from existing DSP libraries by 3.1× on average, that Diospyros can generate kernels that are competitive with expert-tuned code, and that optimizing these small kernels offers end-to-end speedup for a DSP application.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3445814.3446707',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Design Patterns for Submission Evaluation within E-Assessment Systems',\n",
       "  'authors': \"['Michael Striewe']\",\n",
       "  'date': 'July 2021',\n",
       "  'source': \"EuroPLoP'21: 26th European Conference on Pattern Languages of Programs\",\n",
       "  'abstract': 'Many software systems in the area of educational technology can produce grades or other kind of feedback for students’ submissions automatically. Depending on the context of a particular system, there are different software engineering challenges regarding performance or flexibility of the submission evaluation process. During the experimental design of educational technology, these challenges and their consequences are often not considered appropriately, which leads to sub-optimal design decisions that limit productive use. This paper establishes a pattern catalogue that captures available design choices and their consequences in order to support developers and researchers in the domain of educational technology in making their design decisions. Two small case studies demonstrate the usefulness of the catalogue and the gains from applying appropriate patterns for each context.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3489449.3490010',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Flex: high-availability datacenters with zero reserved power',\n",
       "  'authors': \"['Chaojie Zhang', 'Alok Gautam Kumbhare', 'Ioannis Manousakis', 'Deli Zhang', 'Pulkit A. Misra', 'Rod Assis', 'Kyle Woolcock', 'Nithish Mahalingam', 'Brijesh Warrier', 'David Gauthier', 'Lalu Kunnath', 'Steve Solomon', 'Osvaldo Morales', 'Marcus Fontoura', 'Ricardo Bianchini']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture\",\n",
       "  'abstract': 'Cloud providers, like Amazon and Microsoft, must guarantee high availability for a large fraction of their workloads. For this reason, they build datacenters with redundant infrastructures for power delivery and cooling. Typically, the redundant resources are reserved for use only during infrastructure failure or maintenance events, so that workload performance and availability do not suffer. Unfortunately, the reserved resources also produce lower power utilization and, consequently, require more datacenters to be built. To address these problems, in this paper we propose \"zero-reserved-power\" datacenters and the Flex system to ensure that workloads still receive their desired performance and availability. Flex leverages the existence of software-redundant workloads that can tolerate lower infrastructure availability, while imposing minimal (if any) performance degradation for those that require high infrastructure availability. Flex mainly comprises (1) a new offline workload placement policy that reduces stranded power while ensuring safety during failure or maintenance events, and (2) a distributed system that monitors for failures and quickly reduces the power draw while respecting the workloads\\' requirements, when it detects a failure. Our evaluation shows that Flex produces less than 5% stranded power and increases the number of deployed servers by up to 33%, which translates to hundreds of millions of dollars in construction cost savings per datacenter site. We end the paper with lessons from our experience bringing Flex to production in Microsoft\\'s datacenters.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISCA52012.2021.00033',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'The case for adding privacy-related offloading to smart storage',\n",
       "  'authors': \"['Claudiu Mihali', 'Anca Hangan', 'Gheorghe Sebestyen', 'Zsolt István']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"SYSTOR '21: Proceedings of the 14th ACM International Conference on Systems and Storage\",\n",
       "  'abstract': 'It is important to ensure that personally identifiable information (PII) is protected within large distributed systems and is used only for intended purposes. Achieving this is challenging and several techniques have been proposed for privacy-preserving analytics, but they typically focus on the end hosts only. We argue that future storage solutions should include, in addition to emerging compute offload, also privacy-related operators. Since many privacy operators, such as perturbation and anonymization, take place as the very first step before other computations, query offload to a Smart Storage device might be only feasible in the future if privacy-related operators can also be offloaded. In this work we demonstrate that privacy-preserving operators can be implemented in hardware without reducing read bandwidths. We focus on perturbations and extend an FPGA-based network-attached Smart Storage solution to show that it is possible to provide these operations at 10Gbps line-rate while using only a small amount of additional FPGA real-estate. We also discuss how future faster smart storage nodes should look like in the light of these additional requirements.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3456727.3463769',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Research on Real-time Data Sharing Method of Power Information Acquisition System',\n",
       "  'authors': \"['Wei Huang', 'CaiXia Long', 'Xi Wang']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"ICITEE '21: Proceedings of the 4th International Conference on Information Technologies and Electrical Engineering\",\n",
       "  'abstract': 'The traditional data sharing architecture of the Power Information Acquisition System is based on Oracle database, with OGG synchronization and ETL extraction as the main methods. With the increasing amount of data collected, architecture limitations and other issues lead to the large amount of real-time power data in the system can not be quickly shared, which limits the development of real-time analysis business such as distribution network monitoring, power supply command. This paper puts forward a mass real-time data sharing architecture centered on distributed message queuing, which is based on Redis, Kafka and other components for integrated design and development. It implements two real-time data sharing modes: one transmitter and multiple receivers sharing, real-time data collecting service sharing, and effectively solves the problem that real-time data cannot be shared efficiently in Power Information Acquisition System. The research results of this paper have been deployed and applied in the Hunan Electric Power Company of China National Grid, and have achieved remarkable results in the application of big power data such as environmental protection monitoring.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3513142.3513213',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Every walk’s a hit: making page walks single-access cache hits',\n",
       "  'authors': \"['Chang Hyun Park', 'Ilias Vougioukas', 'Andreas Sandberg', 'David Black-Schaffer']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'As memory capacity has outstripped TLB coverage, large data applications suffer from frequent page table walks. We investigate two complementary techniques for addressing this cost: reducing the number of accesses required and reducing the latency of each access. The first approach is accomplished by opportunistically \"flattening\" the page table: merging two levels of traditional 4 KB page table nodes into a single 2 MB node, thereby reducing the table\\'s depth and the number of indirections required to traverse it. The second is accomplished by biasing the cache replacement algorithm to keep page table entries during periods of high TLB miss rates, as these periods also see high data miss rates and are therefore more likely to benefit from having the smaller page table in the cache than to suffer from increased data cache misses.   We evaluate these approaches for both native and virtualized systems and across a range of realistic memory fragmentation scenarios, describe the limited changes needed in our kernel implementation and hardware design, identify and address challenges related to self-referencing page tables and kernel memory allocation, and compare results across server and mobile systems using both academic and industrial simulators for robustness.   We find that flattening does reduce the number of accesses required on a page walk (to 1.0), but its performance impact (+2.3%) is small due to Page Walker Caches (already 1.5 accesses). Prioritizing caching has a larger effect (+6.8%), and the combination improves performance by +9.2%. Flattening is more effective on virtualized systems (4.4 to 2.8 accesses, +7.1% performance), due to 2D page walks. By combining the two techniques we demonstrate a state-of-the-art +14.0% performance gain and -8.7% dynamic cache energy and -4.7% dynamic DRAM energy for virtualized execution with very simple hardware and software changes.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503222.3507718',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Concentrated isolation for container networks toward application-aware sandbox tailoring',\n",
       "  'authors': \"['Yuki Nakata', 'Katsuya Matsubara', 'Ryosuke Matsumoto']\",\n",
       "  'date': 'December 2021',\n",
       "  'source': \"UCC '21: Proceedings of the 14th IEEE/ACM International Conference on Utility and Cloud Computing\",\n",
       "  'abstract': 'Containers provide a lightweight and fine-grained isolation for computational resources such as CPUs, memory, storage, and networks, but their weak isolation raises security concerns. As a result, research and development efforts have focused on redesigning truly sandboxed containers with system call intercept and hardware virtualization techniques such as gVisor and Kata Containers. However, such fully integrated sandboxing could overwhelm the lightweight and scalable nature of the containers. In this work, we propose a partially fortified sandboxing mechanism that concentratedly fortifies the network isolation, focusing on the fact that containerized clouds and the applications running on them require different isolation levels in accordance with their unique characteristics. We describe how to efficiently implement the mechanism to fortify network isolation for containers with a para-passthrough hypervisor and report evaluation results with benchmarks and real applications. Our findings demonstrate that this fortified network isolation has good potential to tailor sandboxes for containerized PaaS/FaaS clouds.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3468737.3494092',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Next-generation internet at terabit speed: SCION in P4',\n",
       "  'authors': \"['Joeri de Ruiter', 'Caspar Schutijser']\",\n",
       "  'date': 'December 2021',\n",
       "  'source': \"CoNEXT '21: Proceedings of the 17th International Conference on emerging Networking EXperiments and Technologies\",\n",
       "  'abstract': 'Regularly, new architectures are proposed to address shortcomings in the current internet. It is not always trivial to evaluate how these proposals would perform in practice. This situation is improved significantly with the introduction of the P4 programming language and programmable network equipment. In this paper we discuss our implementation of one particular future internet architecture, namely SCION. We implemented a SCION router in P4 for switches based on the Intel Tofino ASIC. Having an open source P4 implementation of SCION that runs on high-speed hardware can contribute to its adoption as well as support research in this area. Our work lead to several recommendations for and subsequent changes to the SCION protocol, as well as some generic guidelines when designing protocols. A first analysis of our implementation shows it can process SCION packets at high speeds.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3485983.3494839',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Development of a Neuromorphic Circuit in Reconfigurable Technology for the Simulation of Synapses',\n",
       "  'authors': \"['Christos Kompogiannis', 'Maria Sapounaki', 'Athanasios Kakarountas']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"PCI '21: Proceedings of the 25th Pan-Hellenic Conference on Informatics\",\n",
       "  'abstract': 'In last decades, neuromorphic circuits have received widespread attention across various scientific fields. Such circuits mathematically model the behaviour of biological neurons, synapses as well as their interaction. This work implements a neuromorphic synapse on an FPGA board and it improves previously proposed synapses in terms of performance and synchronization to novel neuron implementations. The proposed architecture is designed with the goal of being compatible with neuromorphic neurons based on the mathematical equations of the Izhikevich neuron model. The implementation consists of two computation cores; one core is responsible for computing the update of currents and the second core is computing the exponential decays of currents. Compared to similar neuromorphic synapses, the proposed retains low complexity and can calculate the needed synaptic currents of the connected neurons quickly and reliably. The speed of computation achieved by the parallel execution of instructions indicates that the system can function in real time.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503823.3503918',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'NURA: A Framework for Supporting Non-Uniform Resource Accesses in GPUs',\n",
       "  'authors': \"['Sina Darabi', 'Negin Mahani', 'Hazhir Baxishi', 'Ehsan Yousefzadeh-Asl-Miandoab', 'Mohammad Sadrosadati', 'Hamid Sarbazi-Azad']\",\n",
       "  'date': 'None',\n",
       "  'source': 'Proceedings of the ACM on Measurement and Analysis of Computing Systems',\n",
       "  'abstract': 'Multi-application execution in Graphics Processing Units (GPUs), a promising way to utilize GPU resources, is still challenging. Some pieces of prior work (e.g., spatial multitasking) have limited opportunity to improve resource utilization, while other works, e.g., simultaneous multi-kernel, provide fine-grained resource sharing at the price of unfair execution. This paper proposes a new multi-application paradigm for GPUs, called NURA, that provides high potential to improve resource utilization and ensures fairness and Quality-of-Service (QoS). The key idea is that each streaming multiprocessor (SM) executes Cooperative Thread Arrays (CTAs) belong to only one application (similar to the spatial multi-tasking) and shares its unused resources with the SMs running other applications demanding more resources. NURA handles resource sharing process mainly using a software approach to provide simplicity, low hardware cost, and flexibility. We also perform some hardware modifications as an architectural support for our software-based proposal. We conservatively analyze the hardware cost of our proposal, and observe less than 1.07% area overhead with respect to the whole GPU die. Our experimental results over various mixes of GPU workloads show that NURA improves GPU system throughput by 26% compared to state-of-the-art spatial multi-tasking, on average, while meeting the QoS target. In terms of fairness, NURA has almost similar results to spatial multitasking, while it outperforms simultaneous multi-kernel by an average of 76%.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3508036',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'SDN Enabled QoE and Security Framework for Multimedia Applications in 5G Networks',\n",
       "  'authors': \"['Prabhakar Krishnan', 'Kurunandan Jain', 'Pramod George Jose', 'Krishnashree Achuthan', 'Rajkumar Buyya']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Multimedia Computing, Communications, and Applications',\n",
       "  'abstract': 'The technologies for real-time multimedia transmission and immersive 3D gaming applications are rapidly emerging, posing challenges in terms of performance, security, authentication, data privacy, and encoding. The communication channel for these multimedia applications must be secure and reliable from network attack vectors and data-contents must employ strong encryption to preserve privacy and confidentiality. Towards delivering secure multimedia application environment for 5G networks, we propose an SDN/NFV (Software-Defined-Networking/Network-Function-Virtualization) framework called STREK, which attempts to deliver highly adaptable Quality-of-Experience (QoE), Security, and Authentication functions for multi-domain Cloud to Edge networks. The STREK architecture consists of a holistic SDNFV dataplane, NFV service-chaining and network slicing, a lightweight adaptable hybrid cipher scheme called TREK, and an open RESTful API for applications to deploy custom policies at runtime for multimedia services. For multi-domain/small-cell deployments, the key-generation scheme is dynamic at flow/session-level, and the handover authentication scheme uses a novel method to exchange security credentials with the Access Points (APs) of neighborhood cells. This scheme is designed to improve authentication function during handover with low overhead, delivering the 5G ultra-low latency requirements. We present the experiments with both software and hardware-based implementations and compare our solution with popular lightweight cryptographic solutions, standard open source software, and SDN-based research proposals for 5G multimedia. In the microbenchmarks, STREK achieves smaller hardware, low overhead, low computation, higher attack resistance, and offers better network performance for multimedia streaming applications. In real-time multimedia use-cases, STREK shows greater level of quality distortion for multimedia contents with minimal encryption bitrate overhead to deliver data confidentiality, immunity to common cryptanalysis, and significant resistance to communication channel attacks, in the context of low-latency 5G networks.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3377390',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'HAMMER: boosting fidelity of noisy Quantum circuits by exploiting Hamming behavior of erroneous outcomes',\n",
       "  'authors': \"['Swamit Tannu', 'Poulami Das', 'Ramin Ayanzadeh', 'Moinuddin Qureshi']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'Quantum computers with hundreds of qubits will be available soon. Unfortunately, high device error-rates pose a significant challenge in using these near-term quantum systems to power real-world applications. Executing a program on existing quantum systems generates both correct and incorrect outcomes, but often, the output distribution is too noisy to distinguish between them. In this paper, we show that erroneous outcomes are not arbitrary but exhibit a well-defined structure when represented in the Hamming space. Our experiments on IBM and Google quantum computers show that the most frequent erroneous outcomes are more likely to be close in the Hamming space to the correct outcome. We exploit this behavior to improve the ability to infer the correct outcome.  We propose Hamming Reconstruction (HAMMER), a post-processing technique that leverages the observation of Hamming behavior to reconstruct the noisy output distribution, such that the resulting distribution has higher fidelity. We evaluate HAMMER using experimental data from Google and IBM quantum computers with more than 500 unique quantum circuits and obtain an average improvement of 1.37x in the quality of solution. On Google’s publicly available QAOA datasets, we show that HAMMER sharpens the gradients on the cost function landscape.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503222.3507703',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Using Meta Reinforcement Learning to Bridge the Gap between Simulation and Experiment in Energy Demand Response',\n",
       "  'authors': \"['Doseok Jang', 'Lucas Spangher', 'Manan Khattar', 'Utkarsha Agwan', 'Costas Spanos']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"e-Energy '21: Proceedings of the Twelfth ACM International Conference on Future Energy Systems\",\n",
       "  'abstract': 'Our team is proposing to run a full-scale energy demand response experiment in an office building. Although this is an exciting endeavor which will provide value to the community, collecting training data for the reinforcement learning agent is costly and will be limited. In this work, we apply a meta-learning architecture to warm start the experiment with simulated tasks, to increase sample efficiency. We present results that demonstrate a similar a step up in complexity still corresponds with better learning.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3447555.3466589',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Impact of AVX-512 Instructions on Graph Partitioning Problems.',\n",
       "  'authors': \"['Md Maruf Hossain', 'Erik Saule']\",\n",
       "  'date': 'August 2021',\n",
       "  'source': \"ICPP Workshops '21: 50th International Conference on Parallel Processing Workshop\",\n",
       "  'abstract': 'Graph analysis now percolates society with applications ranging from advertising and transportation to medical research. The structure of graphs is becoming more complex every day while they are getting larger. The increasing size of graph networks has made many of the classical algorithms reasonably slow. Fortunately, CPU architectures have evolved to adjust to new and more complex problems in terms of core-level parallelism and vector-level parallelism (SIMD-level).  In this paper, we are exploring how the modern vector architecture of CPUs can help with community detection, partitioning, and coloring kernels by studying two representatives algorithms. We consider the Intel SkylakeX and Cascade Lake architectures, which support gather and scatter instructions on 512-bit vectors.  The existing vectorized graph algorithms of classic graph problems, such as BFS and PageRank, do not apply well to community detection; we show the support of gather and scatter are necessary. In particular for the implementation of the reduce-scatter patterns. We evaluate the performances achieved on the two architectures and conclude that good hardware support for scatter instructions is necessary to fully leverage the vector processing for graph partitioning problems.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3458744.3473362',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Cross-layer approximation for printed machine learning circuits',\n",
       "  'authors': \"['Giorgos Armeniakos', 'Georgios Zervakis', 'Dimitrios Soudris', 'Mehdi B. Tahoori', 'Jörg Henkel']\",\n",
       "  'date': 'March 2022',\n",
       "  'source': \"DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe\",\n",
       "  'abstract': 'Printed electronics (PE) feature low non-recurring engineering costs and low per unit-area fabrication costs, enabling thus extremely low-cost and on-demand hardware. Such low-cost fabrication allows for high customization that would be infeasible in silicon, and bespoke architectures prevail to improve the efficiency of emerging PE machine learning (ML) applications. However, even with bespoke architectures, the large feature sizes in PE constraint the complexity of the ML models that can be implemented. In this work, we bring together, for the first time, approximate computing and PE design targeting to enable complex ML models, such as Multi-Layer Perceptrons (MLPs) and Support Vector Machines (SVMs), in PE. To this end, we propose and implement a cross-layer approximation, tailored for bespoke ML architectures. At the algorithmic level we apply a hardware-driven coefficient approximation of the ML model and at the circuit level we apply a netlist pruning through a full search exploration. In our extensive experimental evaluation we consider 14 MLPs and SVMs and evaluate more than 4300 approximate and exact designs. Our results demonstrate that our cross approximation delivers Pareto optimal designs that, compared to the state-of-the-art exact designs, feature 47% and 44% average area and power reduction, respectively, and less than 1% accuracy loss.',\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3539845.3539898',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Securing Interruptible Enclaved Execution on Small Microprocessors',\n",
       "  'authors': \"['Matteo Busi', 'Job Noorman', 'Jo Van Bulck', 'Letterio Galletta', 'Pierpaolo Degano', 'Jan Tobias Mühlberg', 'Frank Piessens']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Programming Languages and Systems',\n",
       "  'abstract': 'Computer systems often provide hardware support for isolation mechanisms such as privilege levels, virtual memory, or enclaved execution. Over the past years, several successful software-based side-channel attacks have been developed that break, or at least significantly weaken, the isolation that these mechanisms offer. Extending a processor with new architectural or micro-architectural features brings a risk of introducing new software-based side-channel attacks.This article studies the problem of extending a processor with new features without weakening the security of the isolation mechanisms that the processor offers. Our solution is heavily based on techniques from research on programming languages. More specifically, we propose to use the programming language concept of full abstraction as a general formal criterion for the security of a processor extension. We instantiate the proposed criterion to the concrete case of extending a microprocessor that supports enclaved execution with secure interruptibility. This is a very relevant instantiation, as several recent papers have shown that interruptibility of enclaves leads to a variety of software-based side-channel attacks. We propose a design for interruptible enclaves and prove that it satisfies our security criterion. We also implement the design on an open-source enclave-enabled microprocessor and evaluate the cost of our design in terms of performance and hardware size.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3470534',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'CARAT CAKE: replacing paging via compiler/kernel cooperation',\n",
       "  'authors': \"['Brian Suchy', 'Souradip Ghosh', 'Drew Kersnar', 'Siyuan Chai', 'Zhen Huang', 'Aaron Nelson', 'Michael Cuevas', 'Alex Bernat', 'Gaurav Chaudhary', 'Nikos Hardavellas', 'Simone Campanoni', 'Peter Dinda']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'Virtual memory, specifically paging, is undergoing significant innovation due to being challenged by new demands from modern workloads. Recent work has demonstrated an alternative software only design that can result in simplified hardware requirements, even supporting purely physical addressing. While we have made the case for this Compiler- And Runtime-based Address Translation (CARAT) concept, its evaluation was based on a user-level prototype. We now report on incorporating CARAT into a kernel, forming Compiler- And Runtime-based Address Translation for CollAborative Kernel Environments (CARAT CAKE). In our implementation, a Linux-compatible x64 process abstraction can be based either on CARAT CAKE, or on a sophisticated paging implementation. Implementing CARAT CAKE involves kernel changes and compiler optimizations/transformations that must work on all code in the system, including kernel code. We evaluate CARAT CAKE in comparison with paging and find that CARAT CAKE is able to achieve the functionality of paging (protection, mapping, and movement properties) with minimal overhead. In turn, CARAT CAKE allows significant new benefits for systems including energy savings, larger L1 caches, and arbitrary granularity memory management.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503222.3507771',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Eavesdropping user credentials via GPU side channels on smartphones',\n",
       "  'authors': \"['Boyuan Yang', 'Ruirong Chen', 'Kai Huang', 'Jun Yang', 'Wei Gao']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': \"Graphics Processing Unit (GPU) on smartphones is an effective target for hardware attacks. In this paper, we present a new side channel attack on mobile GPUs of Android smartphones, allowing an unprivileged attacker to eavesdrop the user's credentials, such as login usernames and passwords, from their inputs through on-screen keyboard. Our attack targets on Qualcomm Adreno GPUs and investigate the amount of GPU overdraw when rendering the popups of user's key presses of inputs. Such GPU overdraw caused by each key press corresponds to unique variations of selected GPU performance counters, from which these key presses can be accurately inferred. Experiment results from practical use on multiple models of Android smartphones show that our attack can correctly infer more than 80% of user's credential inputs, but incur negligible amounts of computing overhead and network traffic on the victim device. To counter this attack, this paper suggests mitigations of access control on GPU performance counters, or applying obfuscations on the values of GPU performance counters.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503222.3507757',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Enabling Industry 4.0 Communication Protocol Interoperability: An OPC UA Case Study',\n",
       "  'authors': \"['Subash Kannoth', 'Frank Schnicke', 'Pablo Oliveira Antonino']\",\n",
       "  'date': 'May 2021',\n",
       "  'source': 'ECBS 2021: 7th Conference on the Engineering of Computer Based Systems',\n",
       "  'abstract': 'Rapid advances in digitalization are leading the automation and manufacturing sector towards the fourth industrial revolution also known as Industry 4.0, whose main goal is to realize the changeable production processes, which is currently expensive and effort-intensive. The Open Platform Communications Unified Architecture (OPC UA) is an established and well-known communication protocol in the industrial domain. The Reference Architecture Model Industry4.0 (RAMI 4.0) proposes OPC UA as the core communication protocol among assets such as machines, robots, and appliances. Despite the key role of OPC UA in Industry 4.0, there is still a lack of technical guidance on how to integrate OPC UA with other communication protocols, especially with legacy devices that communicate through proprietary protocols. To address this challenge, we propose a solution that is characterized by a set of communication primitives, a platform-independent type system and an intermediate language. We also evaluate the overhead created through integration in terms of the round-trip time and message size imposed by metadata for abstraction. We have implemented the proposed approach in reference Industry 4.0 projects, and in this paper, we report our experiences in integrating OPC UA in a homogeneous communication system comprised of Create, Read, Update, Delete and Invoke primitives which improves the protocol interoperability and reduces integration effort.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3459960.3459977',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'HyCSim: A rapid design space exploration tool for emerging hybrid last-level caches',\n",
       "  'authors': \"['Carlos Escuin', 'Asif Ali Khan', 'Pablo Ibañez', 'Teresa Monreal', 'Victor Viñals', 'Jeronimo Castrillon']\",\n",
       "  'date': 'January 2022',\n",
       "  'source': 'DroneSE and RAPIDO: System Engineering for constrained embedded systems',\n",
       "  'abstract': 'Recent years have seen a rising trend in the exploration of nonvolatile memory (NVM) technologies in the memory subsystem. Particularly in the cache hierarchy, hybrid last-level cache (LLC) solutions are proposed to meet the wide-ranging performance and energy requirements of modern days applications. These emerging hybrid solutions need simulation and detailed exploration to fully understand their capabilities before exploiting them. Existing simulation tools are either too slow or incapable of prototyping such systems and optimizing for NVM devices. To this end, we propose HyCSim1, a trace-driven simulation infrastructure that enables rapid comparison of various hybrid LLC configurations for different optimization objectives. Notably, HyCSim makes it possible to quickly estimate the impact of various hybrid LLC insertion and replacement policies, disabling of a cache region at byte or cache frame granularity for different fault maps. In addition, HyCSim allows to evaluate the impact of various compression schemes on the overall performance (hit and miss rate) and the number of writes to the LLC. Our evaluation on ten multi-program workloads from the SPEC 2006 benchmarks suite shows that HyCSim accelerates the simulation time by 24 ×, compared to the cycle-accurate Gem5 simulator, with high-fidelity.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3522784.3522801',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': \"Embedding an interactive art installation into a building for enhancing citizen's awareness on urban environmental conditions\",\n",
       "  'authors': \"['Penny Papageorgopoulou', 'Dimitris Delinikolas', 'Natalia Arsenopoulou', 'Louiza Katsarou', 'Charalampos Rizopoulos', 'Antonios Psaltis', 'Iouliani Theona', 'Alexandros Drymonitis', 'Antonios Korosidis', 'Dimitrios Charitos']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': 'MAB20: Media Architecture Biennale 20',\n",
       "  'abstract': 'The paper presents an artwork, embedded in a public-use building complex, comprising four interactive, sight specific installations. The artwork employs ubiquitous computing technologies and a variety of other components (projector, LED matrix displays, physical and custom-made objects) to sense, collect and translate urban, environmental data and human input into evolving multisensory representations, affording hybrid spatial experiences. The artwork aims at highlighting the impact of human activity upon the environment and the nonhuman entities that inhabit it, shifting the focus to a post-anthropocentric view of the world.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3469410.3469426',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'GPM: leveraging persistent memory from a GPU',\n",
       "  'authors': \"['Shweta Pandey', 'Aditya K Kamath', 'Arkaprava Basu']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'The GPU is a key computing platform for many application domains. While the new non-volatile memory technology has brought the promise of byte-addressable persistence (a.k.a., persistent memory, or PM) to CPU applications, the same, unfortunately, is beyond the reach of GPU programs.   We take three key steps toward enabling GPU programs to access PM directly. First, enable direct access to PM from within a GPU kernel without needing to modify the hardware. Next, we demonstrate three classes of GPU-accelerated applications that benefit from PM. In the process, we create a workload suite with nine such applications. We then create a GPU library, written in CUDA, to support logging, checkpointing, and primitives for native persistence for programmers to easily leverage PM.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503222.3507758',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'IOCost: block IO control for containers in datacenters',\n",
       "  'authors': \"['Tejun Heo', 'Dan Schatzberg', 'Andrew Newell', 'Song Liu', 'Saravanan Dhakshinamurthy', 'Iyswarya Narayanan', 'Josef Bacik', 'Chris Mason', 'Chunqiang Tang', 'Dimitrios Skarlatos']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'Resource isolation is a fundamental requirement in datacenter environments. However, our production experience in Meta’s large-scale datacenters shows that existing IO control mechanisms for block storage are inadequate in containerized environments. IO control needs to provide proportional resources to containers while taking into account the hardware heterogeneity of storage devices and the idiosyncrasies of the workloads deployed in datacenters. The speed of modern SSDs requires IO control to execute with low-overheads. Furthermore, IO control should strive for work conservation, take into account the interactions with the memory management subsystem, and avoid priority inversions that lead to isolation failures. To address these challenges, this paper presents IOCost, an IO control solution that is designed for containerized environments and provides scalable, work-conserving, and low-overhead IO control for heterogeneous storage devices and diverse workloads in datacenters. IOCost performs offline profiling to build a device model and uses it to estimate device occupancy of each IO request. To minimize runtime overhead, it separates IO control into a fast per-IO issue path and a slower periodic planning path. A novel work-conserving budget donation algorithm enables containers to dynamically share unused budget. We have deployed IOCost across the entirety of Meta’s datacenters comprised of millions of ma- chines, upstreamed IOCost to the Linux kernel, and open-sourced our device-profiling tools. IOCost has been running in production for two years, providing IO control for Meta’s fleet. We describe the design of IOCost and share our experience deploying it at scale.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503222.3507727',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'An Energy-Efficient Inference Method in Convolutional Neural Networks Based on Dynamic Adjustment of the Pruning Level',\n",
       "  'authors': \"['Mohammad-Ali Maleki', 'Alireza Nabipour-Meybodi', 'Mehdi Kamal', 'Ali Afzali-Kusha', 'Massoud Pedram']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Design Automation of Electronic Systems',\n",
       "  'abstract': 'In this article, we present a low-energy inference method for convolutional neural networks in image classification applications. The lower energy consumption is achieved by using a highly pruned (lower-energy) network if the resulting network can provide a correct output. More specifically, the proposed inference method makes use of two pruned neural networks (NNs), namely mildly and aggressively pruned networks, which are both designed offline. In the system, a third NN makes use of the input data for the online selection of the appropriate pruned network. The third network, for its feature extraction, employs the same convolutional layers as those of the aggressively pruned NN, thereby reducing the overhead of the online management. There is some accuracy loss induced by the proposed method where, for a given level of accuracy, the energy gain of the proposed method is considerably larger than the case of employing any one pruning level. The proposed method is independent of both the pruning method and the network architecture. The efficacy of the proposed inference method is assessed on Eyeriss hardware accelerator platform for some of the state-of-the-art NN architectures. Our studies show that this method may provide, on average, 70% energy reduction compared to the original NN at the cost of about 3% accuracy loss on the CIFAR-10 dataset.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3460972',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Research of Quantum Neural Networks and Development of a Single-qubit Model of Neuron',\n",
       "  'authors': \"['Sergey Gushanskiy', 'Viktor Potapov', 'Alexey Samoylov']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"PCI '21: Proceedings of the 25th Pan-Hellenic Conference on Informatics\",\n",
       "  'abstract': 'Recently, there has been a rapid increase interest in quantum computers. Their work is based on the use of quantum-mechanical phenomena such as superposition and entanglement for computing to transform input data into outputs, which can actually provide effective performance 3-4 orders of magnitude higher than any modern computing devices. In the past few decades, there has been an acute problem of creating a quantum computer that uses quantum mechanical effects for computations. Currently, there are prototypes of devices of this class, but it is not yet possible to achieve an effective solution to the planned tasks with their help due to a number of difficulties in implementation. However, there is a mathematical apparatus that describes the behavior of quantum particles using a wave function. Hence, it becomes possible to simulate quantum computations on the classical architecture and check the reliability of algorithms belonging to the NP class, which are theoretically executed on a quantum computer in polynomial time and which cannot be solved on classical computers within an acceptable time frame. Effectively simulate the operation of a quantum system on devices with classical architecture is impossible, it remains to propose methods for acceleration of quantum computing on classical systems.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503823.3503893',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Computing with time: microarchitectural weird machines',\n",
       "  'authors': \"['Dmitry Evtyushkin', 'Thomas Benjamin', 'Jesse Elwell', 'Jeffrey A. Eitel', 'Angelo Sapello', 'Abhrajit Ghosh']\",\n",
       "  'date': 'April 2021',\n",
       "  'source': \"ASPLOS '21: Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'Side-channel attacks such as Spectre rely on properties of modern CPUs that permit discovery of microarchitectural state via timing of various operations. The Weird Machine concept is an increasingly popular model for characterization of emergent execution that arises from side-effects of conventional computing constructs. In this work we introduce Microarchitectural Weird Machines (µWM): code constructions that allow performing computation through the means of side effects and conflicts between microarchitectual entities such as branch predictors and caches. The results of such computations are observed as timing variations. We demonstrate how µWMs can be used as a powerful obfuscation engine where computation operates based on events unobservable to conventional anti-obfuscation tools based on emulation, debugging, static and dynamic analysis techniques. We demonstrate that µWMs can be used to reliably perform arbitrary computation by implementing a SHA-1 hash function. We then present a practical example in which we use a µWM to obfuscate malware code such that its passive operation is invisible to an observer with full power to view the architectural state of the system until the code receives a trigger. When the trigger is received the malware decrypts and executes its payload. To show the effectiveness of obfuscation we demonstrate its use in the concealment and subsequent execution of a payload that exfiltrates a shadow password file, and a payload that creates a reverse shell.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3445814.3446729',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Towards failure correlation for improved cloud application service resilience',\n",
       "  'authors': \"['Dhanya R Mathews', 'Mudit Verma', 'Pooja Aggarwal', 'J. Lakshmi']\",\n",
       "  'date': 'December 2021',\n",
       "  'source': \"UCC '21: Proceedings of the 14th IEEE/ACM International Conference on Utility and Cloud Computing Companion\",\n",
       "  'abstract': 'Autonomously dealing with disruptions is necessary for maintaining the quality of a cloud application service. A fault, error, or failure in any component across the application service stack can potentially disrupt the service delivery. Fault localization and failure prediction are essential techniques in managing service failures. Emerging cloud computing paradigms are pushing application services to be built as loosely coupled distributed components for independent scaling. However, such architectures render existing approaches for fault localization and failure prediction to be limiting. Prevalent works on fault localization and failure prediction focus on a specific cloud service architecture layer or a subset of service components or specific fault types. These approaches restrict the view on the impact of the fault on the application service and obviate more intelligent methods for localizing faults or predicting failures, and thus efficiently dealing with service disruptions in an autonomous way. This paper contemplates the propagation of faults in multi-tiered architectures like clouds and uses a real-world disruption scenario to emphasize the need for correlating the faults across the service layers to acquire insights for end-to-end fault analysis for cloud application services.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3492323.3495586',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A Survey on Silicon Photonics for Deep Learning',\n",
       "  'authors': \"['Febin P. Sunny', 'Ebadollah Taheri', 'Mahdi Nikdast', 'Sudeep Pasricha']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Journal on Emerging Technologies in Computing Systems',\n",
       "  'abstract': 'Deep learning has led to unprecedented successes in solving some very difficult problems in domains such as computer vision, natural language processing, and general pattern recognition. These achievements are the culmination of decades-long research into better training techniques and deeper neural network models, as well as improvements in hardware platforms that are used to train and execute the deep neural network models. Many application-specific integrated circuit (ASIC) hardware accelerators for deep learning have garnered interest in recent years due to their improved performance and energy-efficiency over conventional CPU and GPU architectures. However, these accelerators are constrained by fundamental bottlenecks due to (1) the slowdown in CMOS scaling, which has limited computational and performance-per-watt capabilities of emerging electronic processors; and (2) the use of metallic interconnects for data movement, which do not scale well and are a major cause of bandwidth, latency, and energy inefficiencies in almost every contemporary processor. Silicon photonics has emerged as a promising CMOS-compatible alternative to realize a new generation of deep learning accelerators that can use light for both communication and computation. This article surveys the landscape of silicon photonics to accelerate deep learning, with a coverage of developments across design abstractions in a bottom-up manner, to convey both the capabilities and limitations of the silicon photonics paradigm in the context of deep learning acceleration.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3459009',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Suppressing ZZ crosstalk of Quantum computers through pulse and scheduling co-optimization',\n",
       "  'authors': \"['Lei Xie', 'Jidong Zhai', 'ZhenXing Zhang', 'Jonathan Allcock', 'Shengyu Zhang', 'Yi-Cong Zheng']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'Noise is a significant obstacle to quantum computing, and ZZ crosstalk is one of the most destructive types of noise affecting superconducting qubits. Previous approaches to suppressing ZZ crosstalk have mainly relied on specific chip design that can complicate chip fabrication and aggravate decoherence. To some extent, special chip design can be avoided by relying on pulse optimization to suppress ZZ crosstalk. However, existing approaches are non-scalable, as their required time and memory grow exponentially with the number of qubits involved.  To address the above problems, we propose a scalable approach by co-optimizing pulses and scheduling. We optimize pulses to offer an ability to suppress ZZ crosstalk surrounding a gate, and then design scheduling strategies to exploit this ability and achieve suppression across the whole circuit. A main advantage of such co-optimization is that it does not require special hardware support. Besides, we implement our approach as a general framework that is compatible with different pulse optimization methods. We have conducted extensive evaluations by simulation and on a real quantum computer. Simulation results show that our proposal can improve the fidelity of quantum computing on 4∼12 qubits by up to 81× (11× on average). Ramsey experiments on a real quantum computer also demonstrate that our method can eliminate the effect of ZZ crosstalk to a great extent.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503222.3507761',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'PUF based Secure and Lightweight Authentication and Key-Sharing Scheme for Wireless Sensor Network',\n",
       "  'authors': \"['Mahabub Hasan Mahalat', 'Dipankar Karmakar', 'Anindan Mondal', 'Bibhash Sen']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Journal on Emerging Technologies in Computing Systems',\n",
       "  'abstract': 'The deployment of wireless sensor networks (WSN) in an untended environment and the openness of the wireless channel bring various security threats to WSN. The resource limitations of the sensor nodes make the conventional security systems less attractive for WSN. Moreover, conventional cryptography alone cannot ensure the desired security against the physical attacks on sensor nodes. Physically unclonable function (PUF) is an emerging hardware security primitive that provides low-cost hardware security exploiting the unique inherent randomness of a device. In this article, we have proposed an authentication and key sharing scheme for the WSN integrating Pedersen’s verifiable secret sharing (Pedersen’s VSS) and Shamir’s secret sharing (Shamir’s SS) scheme with PUF which ensure the desired security with low overhead. The security analysis depicts the resilience of the proposed scheme against different active, passive and physical attacks. Also, the performance analysis shows that the proposed scheme possesses low computation, communication and storage overhead. The scheme only needs to store a polynomial number of PUF challenge-response pairs to the user node. The sink or senor nodes do not require storing any secret key. Finally, the comparison with the previous protocols establishes the dominance of the proposed scheme to use in WSN.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3466682',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Reducing the configuration overhead of the distributed two-level control system',\n",
       "  'authors': \"['Yu Yang', 'Dimitrios Stathis', 'Ahmed Hemani']\",\n",
       "  'date': 'March 2022',\n",
       "  'source': \"DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe\",\n",
       "  'abstract': 'With the growing demand for more efficient hardware accelerators for streaming applications, a novel Coarse-Grained Reconfigurable Architecture (CGRA) that uses a Distributed Two-Level Control (D2LC) system has been proposed in the literature. Even though the highly distributed and parallel structure makes it fast and energy-efficient, the single-issue instruction channel between the level-1 and level-2 controller in each D2LC cell becomes the bottleneck of its performance. In this paper, we improve its design to mimic a multi-issued architecture by inserting shadow instruction buffers between the level-1 and level-2 controllers. Together with a zero-overhead hardware loop, the improved D2LC architecture can enable efficient overlap between loop iterations. We also propose a complete constraint programming based instruction scheduling algorithm to support the above hardware features. The experiment result shows that the improved D2LC architecture can achieve up to 25% of reduction on the instruction execution cycles and 35% reduction on the energy-delay product.',\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3539845.3539877',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Eliminating Iterations of Iterative Methods: Solving Large-Scale Sparse Linear System in O(1) with RRAM-based In-Memory Accelerator',\n",
       "  'authors': \"['Tao Song', 'Xiaoming Chen', 'Yinhe Han']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"GLSVLSI '21: Proceedings of the 2021 on Great Lakes Symposium on VLSI\",\n",
       "  'abstract': 'The sparse linear solver is an important component in lots of scientific computing applications. For large-scale sparse linear systems, general-purpose processors such as CPUs and GPUs are facing challenges of high time complexity and massive data movements between processors and main memories. This work utilizes the ability of in-situ analog computing of RRAMs and builds an RRAMbased accelerator for iterative linear solvers.We first propose a basic principle of mapping iterative solvers onto RRAM-based crossbar arrays. The proposed principle eliminates not only the iterations but also the convergence condition. Based on the principle, we propose a scalable architecture that can solve large-scale sparse matrices in O(1) time complexity. Compared with a massively parallel iterative solver on GPU, our accelerator shows 100× higher performance and 1000× energy reduction. If the solution obtained by our accelerator is used as the seed for a further refinement on GPU, about 35% of the solving time and energy consumption can be saved compared with a pure GPU solving process.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3453688.3461510',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Serverless computing on heterogeneous computers',\n",
       "  'authors': \"['Dong Du', 'Qingyuan Liu', 'Xueqiang Jiang', 'Yubin Xia', 'Binyu Zang', 'Haibo Chen']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'Existing serverless computing platforms are built upon homogeneous computers, limiting the function density and restricting serverless computing to limited scenarios. We introduce Molecule, the first serverless computing system utilizing heterogeneous computers. Molecule enables both general-purpose devices (e.g., Nvidia DPU) and domain-specific accelerators (e.g., FPGA and GPU) for serverless applications that significantly improve function density (50% higher) and application performance (up to 34.6x). To achieve these results, we first propose XPU-Shim, a distributed shim to bridge the gap between underlying multi-OS systems (when using general-purpose devices) and our serverless runtime (i.e., Molecule). We further introduce vectorized sandbox, a sandbox abstraction to abstract hardware heterogeneity (when using domain-specific accelerators). Moreover, we also review state-of-the-art serverless optimizations on startup and communication latency and overcome the challenges to implement them on heterogeneous computers. We have implemented Molecule on real platforms with Nvidia DPUs and Xilinx FPGAs and evaluate it using benchmarks and real-world applications.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503222.3507732',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Understanding I/O Direct Cache Access Performance for End Host Networking',\n",
       "  'authors': \"['Minhu Wang', 'Mingwei Xu', 'Jianping Wu']\",\n",
       "  'date': 'None',\n",
       "  'source': 'Proceedings of the ACM on Measurement and Analysis of Computing Systems',\n",
       "  'abstract': \"Direct Cache Access (DCA) enables a network interface card (NIC) to load and store data directly on the processor cache, as conventional Direct Memory Access (DMA) is no longer suitable as the bridge between NIC and CPU in the era of 100 Gigabit Ethernet. As numerous I/O devices and cores compete for scarce cache resources, making the most of DCA for networking applications with varied objectives and constraints is a challenge, especially given the increasing complexity of modern cache hardware and I/O stacks. In this paper, we reverse engineer details of one commercial implementation of DCA, Intel's Data Direct I/O (DDIO), to explicate the importance of hardware-level investigation into DCA. Based on the learned knowledge of DCA and network I/O stacks, we (1) develop an analytical framework to predict the effectiveness of DCA (i.e., its hit rate) under certain hardware specifications, system configurations, and application properties; (2) measure penalties of the ineffective use of DCA (i.e., its miss penalty) to characterize its benefits; and (3) show that our reverse engineering, measurement, and model contribute to a deeper understanding of DCA, which in turn helps diagnose, optimize, and design end-host networking.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3508042',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A Critical Review of how Public Display Interfaces Facilitate Placemaking',\n",
       "  'authors': \"['Paul Biedermann', 'Andrew Vande Moere']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': 'MAB20: Media Architecture Biennale 20',\n",
       "  'abstract': 'Although public interfaces are promised to facilitate placemaking by offering a technological platform between citizens and decision makers, little is known about whether they actually bring these stakeholders closer together towards local transformative change. By systematically analysing the infrastructural concepts, methods and tools of 40 interface deployments, this review presents a relational model that describes how a public interface can afford the communication, reflection or inquiry of civic feedback. Our analysis also reveals how most public interfaces: are based on utilitarian motives rather than facilitating placemaking; provoke dialogues among citizens instead of between stakeholders; fail to upend the hierachical dependencies between stakeholders; make use of certain technological means that limit citizen agency; and are controlled by gatekeepers who operate covertly and without accountability. Based on these findings, we propose five ”middle-out” considerations that inform how the next generation of placemaking interfaces can facilitate more meaningful and democratic bilateral dialogues between citizens and decision makers.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3469410.3469427',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'GPUReplay: a 50-KB GPU stack for client ML',\n",
       "  'authors': \"['Heejin Park', 'Felix Xiaozhu Lin']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'GPUReplay (GR) is a novel way for deploying GPU-accelerated computation on mobile and embedded devices. It addresses high complexity of a modern GPU stack for deployment ease and security. The idea is to record GPU executions on the full GPU stack ahead of time and replay the executions on new input at run time. We address key challenges towards making GR feasible, sound, and practical to use. The resultant replayer is a drop-in replacement of the original GPU stack. It is tiny (50 KB of executable), robust (replaying long executions without divergence), portable (running in a commodity OS, in TEE, and baremetal), and quick to launch (speeding up startup by up to two orders of magnitude). We show that GPUReplay works with a variety of integrated GPU hardware, GPU APIs, ML frameworks, and 33 neural network (NN) implementations for inference or training. The code is available at https://github.com/bakhi/GPUReplay.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503222.3507754',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'High-Level Synthesis Implementation of an Embedded Real-Time HEVC Intra Encoder on FPGA for Media Applications',\n",
       "  'authors': \"['Panu Sjövall', 'Ari Lemmetti', 'Jarno Vanne', 'Sakari Lahti', 'Timo D. Hämäläinen']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Design Automation of Electronic Systems',\n",
       "  'abstract': 'High Efficiency Video Coding (HEVC) is the key enabling technology for numerous modern media applications. Overcoming its computational complexity and customizing its rich features for real-time HEVC encoder implementations, calls for automated design methodologies. This article introduces the first complete High-Level Synthesis (HLS) implementation for HEVC intra encoder on FPGA. The C source code of our open-source Kvazaar HEVC encoder is used as a design entry point for HLS that is applied throughout the whole encoder design process, from data-intensive coding tools like intra prediction and discrete transforms to more control-oriented tools such as context-adaptive binary arithmetic coding (CABAC). Our prototype is run on Nokia AirFrame Cloud Server equipped with 2.4 GHz dual 14-core Intel Xeon processors and two Intel Arria 10 PCIe FPGA accelerator cards with 40 Gigabit Ethernet. This proof-of-concept system is designed for hardware-accelerated HEVC encoding and it achieves real-time 4K coding speed up to 120 fps. The coding performance can be easily scaled up by adding practically any number of network-connected FPGA cards to the system. These results indicate that our HLS proposal not only boosts development time, but also provides previously unseen design scalability with competitive performance over the existing FPGA and ASIC encoder implementations.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3491215',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Improving Loop Parallelization by a Combination of Static and Dynamic Analyses in HLS',\n",
       "  'authors': \"['Florian Dewald', 'Johanna Rohde', 'Christian Hochberger', 'Heiko Mantel']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Reconfigurable Technology and Systems',\n",
       "  'abstract': 'High-level synthesis (HLS) can be used to create hardware accelerators for compute-intense software parts such as loop structures. Usually, this process requires significant amount of user interaction to steer kernel selection and optimizations. This can be tedious and time-consuming. In this article, we present an approach that fully autonomously finds independent loop iterations and reductions to create parallelized accelerators. We combine static analysis with information available only at runtime to maximize the parallelism exploited by the created accelerators. For loops where we see potential for parallelism, we create fully parallelized kernel implementations. If static information does not suffice to deduce independence, then we assume independence at compile time. We verify this assumption by statically created checks that are dynamically evaluated at runtime, before using the optimized kernel. Evaluating our approach, we can generate speedups for five out of seven benchmarks. With four loop iterations running in parallel, we achieve ideal speedups of up to 4× and on average speedups of 2.27×, both in comparison to an unoptimized accelerator.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3501801',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Pho$: a case for shared optical cache hierarchies',\n",
       "  'authors': \"['Haiyang Han', 'Theoni Alexoudi', 'Chris Vagionas', 'Nikos Pleros', 'Nikos Hardavellas']\",\n",
       "  'date': 'July 2021',\n",
       "  'source': \"ISLPED '21: Proceedings of the ACM/IEEE International Symposium on Low Power Electronics and Design\",\n",
       "  'abstract': 'Conventional electronic memory hierarchies are intrinsically limited in their ability to overcome the memory wall due to scaling constraints. Optical caches and interconnects can mitigate these constraints, and enable processors to reach performance and energy efficiency unattainable by purely electronic means. However, the promised benefits cannot be realized through a simple replacement process; to reach its full potential, the architecture needs to be holistically redesigned. This paper proposes Pho$, an opto-electronic memory hierarchy architecture for multicores. Pho$ replaces conventional core-private electronic caches with a large shared optical L1 built with optical SRAMs. A novel optical NoC provides low-latency and high-bandwidth communication between the electronic cores and the shared optical L1 at low optical loss. Our results show that Pho$ achieves on average 1.41x performance speedup (3.89x max) and 31% lower energy-delay product (90% max) against conventional designs. Moreover, the optical NoC for core-cache communication consumes 70% less power compared to directly applying previously-proposed optical NoC architectures.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISLPED52811.2021.9502487',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Real-time, High-resolution Depth Upsampling on Embedded Accelerators',\n",
       "  'authors': \"['David Langerman', 'Alan George']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Embedded Computing Systems',\n",
       "  'abstract': 'High-resolution, low-latency apps in computer vision are ubiquitous in today’s world of mixed-reality devices. These innovations provide a platform that can leverage the improving technology of depth sensors and embedded accelerators to enable higher-resolution, lower-latency processing for 3D scenes using depth-upsampling algorithms. This research demonstrates that filter-based upsampling algorithms are feasible for mixed-reality apps using low-power hardware accelerators. The authors parallelized and evaluated a depth-upsampling algorithm on two different devices: a reconfigurable-logic FPGA embedded within a low-power SoC; and a fixed-logic embedded graphics processing unit. We demonstrate that both accelerators can meet the real-time requirements of 11 ms latency for mixed-reality apps.1',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3436878',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'PSSM: achieving secure memory for GPUs with partitioned and sectored security metadata',\n",
       "  'authors': \"['Shougang Yuan', 'Yan Solihin', 'Huiyang Zhou']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ICS '21: Proceedings of the ACM International Conference on Supercomputing\",\n",
       "  'abstract': 'In this paper, we investigate the secure memory architecture for GPUs and point out that conventional CPU secure memory architecture can not be directly adopted to the GPUs. The key reasons include: (1) accessing the security metadata, including encryption counters, message authentication codes (MACs) and integrity trees, requires significant memory bandwidth, which may lead to severe bandwidth competition with normal data accesses and degrade the GPU performance; (2) contemporary GPUs use partitioned memory organization, which results in storage and coherence problems for encryption counters and integrity trees since different partitions may need to update the same counter/integrity tree blocks; and (3) the existing split-counter block organization is not friendly to sectored caches, which are commonly used in GPU for bandwidth savings. Based on these observations, we propose partitioned and sectored security metadata (PSSM), which has two components: (a) using the offset addresses (referred to as local addresses) within each partition, instead of the virtual or physical addresses, to generate the metadata so as to solve the counter or integrity tree storage and coherence problem and (b) reorganizing the security metadata to make them friendly to the sectored cache structure so as to reduce the memory bandwidth consumption of metadata accesses. With these proposed schemes, the performance overhead of secure GPU memory is reduced from 59.22% to 16.84% on average. If only memory encryption is required, the performance overhead is reduced from 29.53% to 5.18%.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3447818.3460374',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Mansard Roofline Model: Reinforcing the Accuracy of the Roofs',\n",
       "  'authors': \"['Diogo Marques', 'Aleksandar Ilic', 'Leonel Sousa']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Modeling and Performance Evaluation of Computing Systems',\n",
       "  'abstract': 'Continuous enhancements and diversity in modern multi-core hardware, such as wider and deeper core pipelines and memory subsystems, bring to practice a set of hard-to-solve challenges when modeling their upper-bound capabilities and identifying the main application bottlenecks. Insightful roofline models are widely used for this purpose, but the existing approaches overly abstract the micro-architecture complexity, thus providing unrealistic performance bounds that lead to a misleading characterization of real-world applications. To address this problem, the Mansard Roofline Model (MaRM), proposed in this work, uncovers a minimum set of architectural features that must be considered to provide insightful, but yet accurate and realistic, modeling of performance upper bounds for modern processors. By encapsulating the retirement constraints due to the amount of retirement slots, Reorder-Buffer and Physical Register File sizes, the proposed model accurately models the capabilities of a real platform (average rRMSE of 5.4%) and characterizes 12 application kernels from standard benchmark suites. By following a herein proposed MaRM interpretation methodology and guidelines, speed-ups of up to 5× are obtained when optimizing real-world bioinformatic application, as well as a super-linear speedup of 18.5× when parallelized.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3475866',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Beyond Cache Attacks: Exploiting the Bus-based Communication Structure for Powerful On-Chip Microarchitectural Attacks',\n",
       "  'authors': \"['Johanna Sepúlveda', 'Mathieu Gross', 'Andreas Zankl', 'Georg Sigl']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Embedded Computing Systems',\n",
       "  'abstract': 'System-on-Chips (SoCs) are a key enabling technology for the Internet-of-Things (IoT), a hyper-connected world where on- and inter-chip communication is ubiquitous. SoCs usually integrate cryptographic hardware cores for confidentiality and authentication services. However, these components are prone to implementation attacks. During the operation of a cryptographic core, the secret key may passively be inferred through cache observations. Access-driven attacks exploiting these observations are therefore a vital threat to SoCs operating in IoT environments. Previous works have shown the feasibility of these attacks in the SoC context. Yet, the SoC communication structure can be used to further improve access-based cache attacks. The communication attacks are not as well-understood as other micro-architectural attacks. It is important to raise the awareness of SoC designers of such a threat. To this end, we present four contributions. First, we demonstrate an improved Prime+Probe attack on four different AES-128 implementations (original transformation tables, T0-Only, T2KB, and S-Box). As a novelty, this attack exploits the collisions of the bus-based SoC communication to further increase its efficiency. Second, we explore the impact of preloading on the efficiency of our communication-optimized attack. Third, we integrate three countermeasures (shuffling, mini-tables, and Time-Division Multiple Access (TDMA) bus arbitration) and evaluate their impact on the attack. Although shuffling and mini-tables countermeasures were proposed in previous work, their application as countermeasures against the bus-based attack was not studied before. In addition, TDMA as a countermeasure for bus-based attacks is an original contribution of this work. Fourth, we further discuss the implications of our work in the SoC design and its perspective with the new cryptographic primitives proposed in the ongoing National Institute of Standard and Technology Lightweight Cryptography competition. The results show that our improved communication-optimized attack is efficient, speeding up full key recovery by up to 400 times when compared to the traditional Prime+Probe technique. Moreover, the protection techniques are feasible and effectively mitigate the proposed improved attack.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3433653',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Design of many-core big little μbrains for energy-efficient embedded neuromorphic computing',\n",
       "  'authors': \"['M. Lakshmi Varshika', 'Adarsha Balaji', 'Federico Corradi', 'Anup Das', 'Jan Stuijt', 'Francky Catthoor']\",\n",
       "  'date': 'March 2022',\n",
       "  'source': \"DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe\",\n",
       "  'abstract': 'As spiking-based deep learning inference applications are increasing in embedded systems, these systems tend to integrate neuromorphic accelerators such as μBrain to improve energy efficiency. We propose a μBrain-based scalable many-core neuromorphic hardware design to accelerate the computations of spiking deep convolutional neural networks (SDCNNs). To increase energy efficiency, cores are designed to be heterogeneous in terms of their neuron and synapse capacity (i.e., big vs. little cores), and they are interconnected using a parallel segmented bus interconnect, which leads to lower latency and energy compared to a traditional mesh-based Network-on-Chip (NoC). We propose a system software framework called SentryOS to map SDCNN inference applications to the proposed design. SentryOS consists of a compiler and a run-time manager. The compiler compiles an SDCNN application into sub-networks by exploiting the internal architecture of big and little μBrain cores. The run-time manager schedules these sub-networks onto cores and pipeline their execution to improve throughput. We evaluate the proposed big little many-core neuromorphic design and the system software framework with five commonly-used SDCNN inference applications and show that the proposed solution reduces energy (between 37% and 98%), reduces latency (between 9% and 25%), and increases application throughput (between 20% and 36%). We also show that SentryOS can be easily extended for other spiking neuromorphic accelerators such as Loihi and DYNAPs.',\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3539845.3540079',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A Pattern for Proof of Work Consensus Algorithm in Blockchain',\n",
       "  'authors': \"['Zain ul Abadin', 'Madiha Syed']\",\n",
       "  'date': 'July 2021',\n",
       "  'source': \"EuroPLoP'21: 26th European Conference on Pattern Languages of Programs\",\n",
       "  'abstract': 'Blockchain technology has gained immense popularity over the last decade. It has transformed several industries by enabling processes to work in a secure and decentralized manner. Blockchain consists of blocks that are linked together through cryptography. Blockchain is a distributed ledger that contains a set of sequenced blocks of data. Each block records transaction data in a transparent, immutable, and secure fashion. Consensus algorithm is the key part of this technology as it is used for the decision-making process among nodes in the blockchain network. Nodes in the verifying network use consensus algorithm to reach mutual agreement about state of the blockchain whenever a new block is added. This makes it possible to accept any transaction in the blockchain. Many consensus algorithms have been proposed in the literature each having its own performance and security characteristics. In this paper, we present a pattern for one of the most widely used blockchain consensus algorithms, which is the Proof of Work (PoW) algorithm. This paper is a part of our project on blockchain architecture and patterns.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3489449.3489994',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Toward Multi-FPGA Acceleration of the Neural Networks',\n",
       "  'authors': \"['Saman Biookaghazadeh', 'Pravin Kumar Ravi', 'Ming Zhao']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Journal on Emerging Technologies in Computing Systems',\n",
       "  'abstract': 'High-throughput and low-latency Convolutional Neural Network (CNN) inference is increasingly important for many cloud- and edge-computing applications. FPGA-based acceleration of CNN inference has demonstrated various benefits compared to other high-performance devices such as GPGPUs. Current FPGA CNN-acceleration solutions are based on a single FPGA design, which are limited by the available resources on an FPGA. In addition, they can only accelerate conventional 2D neural networks. To address these limitations, we present a generic multi-FPGA solution, written in OpenCL, which can accelerate more complex CNNs (e.g., C3D CNN) and achieve a near linear speedup with respect to the available single-FPGA solutions. The design is built upon the Intel Deep Learning Accelerator architecture, with three extensions. First, it includes updates for better area efficiency (up to 25%) and higher performance (up to 24%). Second, it supports 3D convolutions for more challenging applications such as video learning. Third, it supports multi-FPGA communication for higher inference throughput. The results show that utilizing multiple FPGAs can linearly increase the overall bandwidth while maintaining the same end-to-end latency. In addition, the design can outperform other FPGA 2D accelerators by up to 8.4 times and 3D accelerators by up to 1.7 times.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3432816',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'PLANAR: a programmable accelerator for near-memory data rearrangement',\n",
       "  'authors': \"['Adrián Barredo', 'Adrià Armejach', 'Jonathan Beard', 'Miquel Moreto']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ICS '21: Proceedings of the ACM International Conference on Supercomputing\",\n",
       "  'abstract': 'Many applications employ irregular and sparse memory accesses that cannot take advantage of existing cache hierarchies in high performance processors. To solve this problem, Data Layout Transformation (DLT) techniques rearrange sparse data into a dense representation, improving locality and cache utilization. However, prior proposals in this space fail to provide a design that (i) scales with multi-core systems, (ii) hides rearrangement latency, and (iii) provides the necessary interfaces to ease programmability. In this work we present PLANAR, a programmable near-memory accelerator that rearranges sparse data into dense. By placing PLANAR devices at the memory controller level we enable a design that scales well with multi-core systems, hides operation latency by performing non-blocking fine-grain data rearrangements, and eases programmability by supporting virtual memory and conventional memory allocation mechanisms. Our evaluation shows that PLANAR leads to significant reductions in data movement and dynamic energy, providing an average 4.58× speedup.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3447818.3460368',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'An Effective Forest Fire Detection Framework Using Heterogeneous Wireless Multimedia Sensor Networks',\n",
       "  'authors': \"['Burak Kizilkaya', 'Enver Ever', 'Hakan Yekta Yatbaz', 'Adnan Yazici']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Multimedia Computing, Communications, and Applications',\n",
       "  'abstract': 'With improvements in the area of Internet of Things (IoT), surveillance systems have recently become more accessible. At the same time, optimizing the energy requirements of smart sensors, especially for data transmission, has always been very important and the energy efficiency of IoT systems has been the subject of numerous studies. For environmental monitoring scenarios, it is possible to extract more accurate information using smart multimedia sensors. However, multimedia data transmission is an expensive operation. In this study, a novel hierarchical approach is presented for the detection of forest fires. The proposed framework introduces a new approach in which multimedia and scalar sensors are used hierarchically to minimize the transmission of visual data. A lightweight deep learning model is also developed for devices at the edge of the network to improve detection accuracy and reduce the traffic between the edge devices and the sink. The framework is evaluated using a real testbed, network simulations, and 10-fold cross-validation in terms of energy efficiency and detection accuracy. Based on the results of our experiments, the validation accuracy of the proposed system is 98.28%, and the energy saving is 29.94%. The proposed deep learning model’s validation accuracy is very close to the accuracy of the best performing architectures when the existing studies and lightweight architectures are considered. In terms of suitability for edge computing, the proposed approach is superior to the existing ones with reduced computational requirements and model size.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3473037',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Design of English Teaching Interactive System Based on Artificial Intelligence',\n",
       "  'authors': \"['Huimin Zhao']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"ICBDE '22: Proceedings of the 5th International Conference on Big Data and Education\",\n",
       "  'abstract': 'By exploring the characteristic of interaction of network teaching, we proposed to develop an English teaching interaction system based on artificial intelligence, which can solve the problems in current English teaching class. We analyzed the functional requirements of the system corresponding to different user. By buiding Ubuntu operating system as the computing platform and exploring the object-oriented dynamic teaching environment provided by artificial intelligence platform, we build the LAMP development environment with MySQL database and PHP script language. We constructed a three-tier separated architecture system by using B/S mode, and refined the design and implementation of each module function through the secondary development of platform module plug-ins. To exhibit the advantages of the artificial intelligence system, we take college English teaching as an example to illustrate its functions. The functions of curriculum creation, teaching resources and activity design can be realized in the system, which verifies the effectiveness of the artificial intelligence system in interactive teaching and learning.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3524383.3533247',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Ratatoskr: An Open-Source Framework for In-Depth Power, Performance, and Area Analysis and Optimization in 3D NoCs',\n",
       "  'authors': \"['Jan Moritz Joseph', 'Lennart Bamberg', 'Imad Hajjar', 'Behnam Razi Perjikolaei', 'Alberto García-Ortiz', 'Thilo Pionteck']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Modeling and Computer Simulation',\n",
       "  'abstract': 'We introduce Ratatoskr, an open-source framework for in-depth power, performance, and area (PPA) analysis in Networks-on-Chips (NoCs) for 3D-integrated and heterogeneous System-on-Chips (SoCs). It covers all layers of abstraction by providing an NoC hardware implementation on Register Transfer Level (RTL), an NoC simulator on cycle-accurate level and an application model on transaction level. By this comprehensive approach, Ratatoskr can provide the following specific PPA analyses: Dynamic power of links can be measured within 2.4% accuracy of bit-level simulations while maintaining cycle-accurate simulation speed. Router power is determined from RTL-to-gate-level synthesis combined with cycle-accurate simulations. The performance of the whole NoC can be measured both via cycle-accurate and RTL simulations. The performance (i.e., timing) of individual routers and the NoC area are obtained from RTL synthesis results. Despite these manifold features, Ratatoskr offers easy two-step user interaction: (1) A single point-of-entry allows setting design parameters. (2) PPA reports are generated automatically. For both the input and the output, different levels of abstraction can be chosen for high-level rapid network analysis or low-level improvement of architectural details. The synthesizable NoC-RTL model shows improved total router power and area in comparison to a conventional standard router. As a forward-thinking and unique feature not found in other NoC PPA-measurement tools, Ratatoskr supports heterogeneous 3D integration that is one of the most promising integration paradigms for upcoming SoCs. Thereby, Ratatoskr lays the groundwork to design their communication architectures. The framework is publicly available at  https://github.com/ratatoskr-project.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3472754',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Binary Precision Neural Network Manycore Accelerator',\n",
       "  'authors': \"['Morteza Hosseini', 'Tinoosh Mohsenin']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Journal on Emerging Technologies in Computing Systems',\n",
       "  'abstract': 'This article presents a low-power, programmable, domain-specific manycore accelerator, Binarized neural Network Manycore Accelerator (BiNMAC), which adopts and efficiently executes binary precision weight/activation neural network models. Such networks have compact models in which weights are constrained to only 1 bit and can be packed several in one memory entry that minimizes memory footprint to its finest. Packing weights also facilitates executing single instruction, multiple data with simple circuitry that allows maximizing performance and efficiency. The proposed BiNMAC has light-weight cores that support domain-specific instructions, and a router-based memory access architecture that helps with efficient implementation of layers in binary precision weight/activation neural networks of proper size. With only 3.73% and 1.98% area and average power overhead, respectively, novel instructions such as Combined Population-Count-XNOR, Patch-Select, and Bit-based Accumulation are added to the instruction set architecture of the BiNMAC, each of which replaces execution cycles of frequently used functions with 1 clock cycle that otherwise would have taken 54, 4, and 3 clock cycles, respectively. Additionally, customized logic is added to every core to transpose 16×16-bit blocks of memory on a bit-level basis, that expedites reshaping intermediate data to be well-aligned for bitwise operations. A 64-cluster architecture of the BiNMAC is fully placed and routed in 65-nm TSMC CMOS technology, where a single cluster occupies an area of 0.53 mm2 with an average power of 232\\xa0mW at 1-GHz clock frequency and 1.1\\xa0V. The 64-cluster architecture takes 36.5\\xa0mm2 area and, if fully exploited, consumes a total power of 16.4\\xa0W and can perform 1,360 Giga Operations Per Second (GOPS) while providing full programmability. To demonstrate its scalability, four binarized case studies including ResNet-20 and LeNet-5 for high-performance image classification, as well as a ConvNet and a multilayer perceptron for low-power physiological applications were implemented on BiNMAC. The implementation results indicate that the population-count instruction alone can expedite the performance by approximately 5×. When other new instructions are added to a RISC machine with existing population-count instruction, the performance is increased by 58% on average. To compare the performance of the BiNMAC with other commercial-off-the-shelf platforms, the case studies with their double-precision floating-point models are also implemented on the NVIDIA Jetson TX2 SoC (CPU+GPU). The results indicate that, within a margin of ∼2.1%--9.5% accuracy loss, BiNMAC on average outperforms the TX2 GPU by approximately 1.9× (or 7.5× with fabrication technology scaled) in energy consumption for image classification applications. On low power settings and within a margin of ∼3.7%--5.5% accuracy loss compared to ARM Cortex-A57 CPU implementation, BiNMAC is roughly ∼9.7×--17.2× (or 38.8×--68.8× with fabrication technology scaled) more energy efficient for physiological applications while meeting the application deadline.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3423136',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Zerializer: towards zero-copy serialization',\n",
       "  'authors': \"['Adam Wolnikowski', 'Stephen Ibanez', 'Jonathan Stone', 'Changhoon Kim', 'Rajit Manohar', 'Robert Soulé']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"HotOS '21: Proceedings of the Workshop on Hot Topics in Operating Systems\",\n",
       "  'abstract': 'Achieving zero-copy I/O has long been an important goal in the networking community. However, data serialization obviates the benefits of zero-copy I/O, because it requires the CPU to read, transform, and write message data, resulting in additional memory copies between the real object instances and the contiguous socket buffer. Therefore, we argue for offloading serialization logic to the DMA path via specialized hardware. We propose an initial hardware design for such an accelerator, and give preliminary evidence of its feasibility and expected benefits.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3458336.3465283',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'An Efficient Video Prediction Recurrent Network using Focal Loss and Decomposed Tensor Train for Imbalance Dataset',\n",
       "  'authors': \"['Mingshuo Liu', 'Kevin Han', 'Shiyi Luo', 'Mingze Pan', 'Mousam Hossain', 'Bo Yuan', 'Ronald F. DeMara', 'Yu Bai']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"GLSVLSI '21: Proceedings of the 2021 on Great Lakes Symposium on VLSI\",\n",
       "  'abstract': 'Nowadays, from companies to academics, researchers across the world are interested in developing recurrent neural networks due to their incredible feats in various applications, such as speech recognition, video detection, predictions, and machine translation. However, the advantages of recurrent neural networks accompanied by high computational and power demands, which are a major design constraint for electronic devices with limited resources used in such network implementations. Optimizing the recurrent neural networks, such as model compression, is crucial to ensure the broad deployment of recurrent neural networks and promote recurrent neural networks for implementing most resource-constrained scenarios. Among many techniques, tensor train (TT) decomposition is considered an up-and-coming technology. Although our previous efforts have achieved 1) expanding limits of many multiplications within eliminating all redundant computations; and 2) decomposing into multi-stage processing to reduce memory traffic, this work still faces some limitations. In particular, current TT decomposition on recurrent neural networks leads to a complex computation sensitive to the quality of training datasets. In this paper, we investigate a new method for TT decomposition on recurrent neural networks for constructing an efficient model within imbalance datasets to overcome this issue. Experimental results show that the proposed new training method can achieve significant improvements in accuracy, precision, recall, F1-score, False Negative Rate (FNR), and False Omission Rate (FOR).',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3453688.3461748',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Electromyography Data Transmission via Galvanic Coupling Intra-body Communication Link',\n",
       "  'authors': \"['Anna Vizziello', 'Pietro Savazzi', 'Giovanni Magenes']\",\n",
       "  'date': 'September 2021',\n",
       "  'source': \"NANOCOM '21: Proceedings of the Eight Annual ACM International Conference on Nanoscale Computing and Communication\",\n",
       "  'abstract': \"Intra-body communication (IBC) is a novel research filed that will promote personalized medicine by allowing real time and in situ monitoring in daily life. In this work, the energy efficient galvanic coupling (GC) technology is used to send electromyography (EMG) data through intra-body links. Real EMG data are first acquired and recorded with needle electrodes inserted in the muscle of a person's forearm. Then, the data are transferred via GC intra-body communication employing a GC sound card-based testbed. The experiments are conducted by transferring EMG data in both ex-vivo and in-vivo tissue with different electrodes placements. Almost error free performance is achieved with a robust and reliable communication, a valuable result in medical applications.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3477206.3477450',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'NVAlloc: rethinking heap metadata management in persistent memory allocators',\n",
       "  'authors': \"['Zheng Dang', 'Shuibing He', 'Peiyi Hong', 'Zhenxin Li', 'Xuechen Zhang', 'Xian-He Sun', 'Gang Chen']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'Persistent memory allocation is a fundamental building block for developing high-performance and in-memory applications. Existing persistent memory allocators suffer from suboptimal heap organizations that introduce repeated cache line flushes and small random accesses in persistent memory. Worse, many allocators use static slab segregation resulting in a dramatic increase in memory consumption when allocation request size is changed. In this paper, we design a novel allocator, named NVAlloc, to solve the above issues simultaneously. First, NVAlloc eliminates cache line reflushes by mapping contiguous data blocks in slabs to interleaved metadata entries stored in different cache lines. Second, it writes small metadata units to a persistent bookkeeping log in a sequential pattern to remove random heap metadata accesses in persistent memory. Third, instead of using static slab segregation, it supports slab morphing, which allows slabs to be transformed between size classes to significantly improve slab usage. NVAlloc is complementary to the existing consistency models. Results on 6 benchmarks demonstrate that NVAlloc improves the performance of state-of-the-art persistent memory allocators by up to 6.4x and 57x for small and large allocations, respectively. Using NVAlloc reduces memory usage by up to 57.8%. Besides, we integrate NVAlloc in a persistent FPTree. Compared to the state-of-the-art allocators, NVAlloc improves the performance of this application by up to 3.1x.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503222.3507743',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'RecSSD: near data processing for solid state drive based recommendation inference',\n",
       "  'authors': \"['Mark Wilkening', 'Udit Gupta', 'Samuel Hsia', 'Caroline Trippel', 'Carole-Jean Wu', 'David Brooks', 'Gu-Yeon Wei']\",\n",
       "  'date': 'April 2021',\n",
       "  'source': \"ASPLOS '21: Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'Neural personalized recommendation models are used across a wide variety of datacenter applications including search, social media, and entertainment. State-of-the-art models comprise large embedding tables that have billions of parameters requiring large memory capacities. Unfortunately, large and fast DRAM-based memories levy high infrastructure costs. Conventional SSD-based storage solutions offer an order of magnitude larger capacity, but have worse read latency and bandwidth, degrading inference performance. RecSSD is a near data processing based SSD memory system customized for neural recommendation inference that reduces end-to-end model inference latency by 2× compared to using COTS SSDs across eight industry-representative models.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3445814.3446763',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Hipernetch: High-Performance FPGA Network Switch',\n",
       "  'authors': \"['Philippos Papaphilippou', 'Jiuxi Meng', 'Nadeen Gebara', 'Wayne Luk']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Reconfigurable Technology and Systems',\n",
       "  'abstract': 'We present Hipernetch, a novel FPGA-based design for performing high-bandwidth network switching. FPGAs have recently become more popular in data centers due to their promising capabilities for a wide range of applications. With the recent surge in transceiver bandwidth, they could further benefit the implementation and refinement of network switches used in data centers. Hipernetch replaces the crossbar with a “combined parallel round-robin arbiter”. Unlike a crossbar, the combined parallel round-robin arbiter is easy to pipeline, and does not require centralised iterative scheduling algorithms that try to fit too many steps in a single or a few FPGA cycles. The result is a network switch implementation on FPGAs operating at a high frequency and with a low port-to-port latency. Our proposed Hipernetch architecture additionally provides a competitive switching performance approaching output-queued crossbar switches. Our implemented Hipernetch designs exhibit a throughput that exceeds 100 Gbps per port for switches of up to 16 ports, reaching an aggregate throughput of around 1.7 Tbps.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3477054',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Client layer becomes bottleneck: workload analysis of an ultra-large-scale cloud storage system',\n",
       "  'authors': \"['Xiaoyi Sun', 'Kaishi Li', 'Yaodanjun Ren', 'Jiale Lin', 'Zhenyu Ren', 'Shuzhi Feng', 'Yin Jian', 'Zhengwei Qi']\",\n",
       "  'date': 'December 2021',\n",
       "  'source': \"UCC '21: Proceedings of the 14th IEEE/ACM International Conference on Utility and Cloud Computing Companion\",\n",
       "  'abstract': \"Recent years have witnessed the fast development of file and storage systems. Many improvements of file and storage systems are inspired by Workload analysis, which reveals the characteristics of I/O behavior. Although cloud storage systems are becoming increasingly prominent, few real-world and large-scale cloud storage workload studies are presented. Alibaba Cloud is one of the world's largest cloud providers, and we have collected and analyzed workloads from Alibaba for an extended period. We observe that modern cloud network architecture can easily handle the peak load during busy festivals. However, the client layer is the system bottleneck during the peak period, which calls for further optimization. We also find that the workload is heavily skewed toward a small percentage of virtual disks, and its distribution conforms 80/20 rule. In summary, the characteristics of such a large-scale cloud storage system in production environments are important for future cloud storage system modifications.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3492323.3495625',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'The Strong Scaling Advantage of FPGAs in HPC for N-body Simulations',\n",
       "  'authors': \"['Johannes Menzel', 'Christian Plessl', 'Tobias Kenter']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Reconfigurable Technology and Systems',\n",
       "  'abstract': 'N-body methods are one of the essential algorithmic building blocks of high-performance and parallel computing. Previous research has shown promising performance for implementing n-body simulations with pairwise force calculations on FPGAs. However, to avoid challenges with accumulation and memory access patterns, the presented designs calculate each pair of forces twice, along with both force sums of the involved particles. Also, they require large problem instances with hundreds of thousands of particles to reach their respective peak performance, limiting the applicability for strong scaling scenarios. This work addresses both issues by presenting a novel FPGA design that uses each calculated force twice and overlaps data transfers and computations in a way that allows to reach peak performance even for small problem instances, outperforming previous single precision results even in double precision, and scaling linearly over multiple interconnected FPGAs. For a comparison across architectures, we provide an equally optimized CPU reference, which for large problems actually achieves higher peak performance per device, however, given the strong scaling advantages of the FPGA design, in parallel setups with few thousand particles per device, the FPGA platform achieves highest performance and power efficiency.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3491235',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Vector instruction selection for digital signal processors using program synthesis',\n",
       "  'authors': \"['Maaz Bin Safeer Ahmad', 'Alexander J. Root', 'Andrew Adams', 'Shoaib Kamil', 'Alvin Cheung']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'Instruction selection, whereby input code represented in an intermediate representation is translated into executable instructions from the target platform, is often the most target-dependent component in optimizing compilers. Current approaches include pattern matching, which is brittle and tedious to design, or search-based methods, which are limited by scalability of the search algorithm. In this paper, we propose a new algorithm that first abstracts the target platform instructions into high-level uber-instructions, with each uber-instruction unifying multiple concrete instructions from the target platform. Program synthesis is used to lift input code sequences into semantically equivalent sequences of uber-instructions and then to lower from uber-instructions to machine code. Using 21 real-world benchmarks, we show that our synthesis-based instruction selection algorithm can generate instruction sequences for a hardware target, with the synthesized code performing up to 2.1x faster as compared to code generated by a professionally-developed optimizing compiler for the same platform.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503222.3507714',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Protecting Network-on-Chip Intellectual Property Using Timing Channel Fingerprinting',\n",
       "  'authors': \"['Arnab Kumar Biswas', 'Biplab Sikdar']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Embedded Computing Systems',\n",
       "  'abstract': 'The theft of Intellectual property (IP) is a serious security threat for all businesses that are involved in the creation of IP. In this article, we consider such attacks against IP for Network-on-Chip (NoC) that are commonly used as a popular on-chip scalable communication medium for Multiprocessor System-on-Chip. As a protection mechanism, we propose a timing channel fingerprinting method and show its effectiveness by implementing five different solutions using this method. We also provide a formal proof of security of the proposed method. We show that the proposed technique provides better security and requires much lower hardware overhead (64%–74% less) compared to an existing NoC IP security solution without affecting the normal packet latency or degrading the NoC performance.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3495565',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': \"The end of Moore's law and the rise of the data processor\",\n",
       "  'authors': \"['Niv Dayan', 'Moshe Twitto', 'Yuval Rochman', 'Uri Beitler', 'Itai Ben Zion', 'Edward Bortnikov', 'Shmuel Dashevsky', 'Ofer Frishman', 'Evgeni Ginzburg', 'Igal Maly', 'Avraham (Poza) Meir', 'Mark Mokryn', 'Iddo Naiss', 'Noam Rabinovich']\",\n",
       "  'date': 'None',\n",
       "  'source': 'Proceedings of the VLDB Endowment',\n",
       "  'abstract': \"With the end of Moore's Law, database architects are turning to hardware accelerators to offload computationally intensive tasks from the CPU. In this paper, we show that accelerators can facilitate far more than just computation: they enable algorithms and data structures that lavishly expand computation in order to optimize for disparate cost metrics. We introduce the Pliops Extreme Data Processor (XDP), a novel storage engine implemented from the ground up using customized hardware. At its core, XDP consists of an accelerated hash table to index the data in storage using less memory and fewer storage accesses for queries than the best alternative. XDP also employs an accelerated compressor, a capacitor, and a lock-free RAID sub-system to minimize storage space and recovery time while minimizing performance penalties. As a result, XDP overcomes cost contentions that have so far been inescapable.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.14778/3476311.3476373',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A Logarithmic Floating-Point Multiplier for the Efficient Training of Neural Networks',\n",
       "  'authors': \"['Zijing Niu', 'Honglan Jiang', 'Mohammad Saeed Ansari', 'Bruce F. Cockburn', 'Leibo Liu', 'Jie Han']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"GLSVLSI '21: Proceedings of the 2021 on Great Lakes Symposium on VLSI\",\n",
       "  'abstract': 'The development of important applications of increasingly large neural networks (NNs) is spurring research that aims to increase the power efficiency of the arithmetic circuits that perform the huge amount of computation in NNs. The floating-point (FP) representation with a large dynamic range is usually used for training. In this paper, it is shown that the FP representation is naturally suited for the binary logarithm of numbers. Thus, it favors a design based on logarithmic arithmetic. Specifically, we propose an efficient hardware implementation of logarithmic FP multiplication that uses simpler operations to replace complex multipliers for the training of NNs. This design produces a double-sided error distribution that mitigates the accumulative effect of errors in iterative operations, so it is up to 45% more accurate than a recent logarithmic FP design. The proposed multiplier also consumes up to 23.5x less energy and 10.7x smaller area compared to exact FP multipliers. Benchmark NN applications, including a 922-neuron model for the MNIST dataset, show that the classification accuracy can be slightly improved using the proposed multiplier, while achieving up to 2.4x less energy and 2.8x smaller area with a better performance.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3453688.3461509',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Challenging the Security of Logic Locking Schemes in the Era of Deep Learning: A Neuroevolutionary Approach',\n",
       "  'authors': \"['Dominik Sisejkovic', 'Farhad Merchant', 'Lennart M. Reimann', 'Harshit Srivastava', 'Ahmed Hallawa', 'Rainer Leupers']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Journal on Emerging Technologies in Computing Systems',\n",
       "  'abstract': 'Logic locking is a prominent technique to protect the integrity of hardware designs throughout the integrated circuit design and fabrication flow. However, in recent years, the security of locking schemes has been thoroughly challenged by the introduction of various deobfuscation attacks. As in most research branches, deep learning is being introduced in the domain of logic locking as well. Therefore, in this article we present SnapShot, a novel attack on logic locking that is the first of its kind to utilize artificial neural networks to directly predict a key bit value from a locked synthesized gate-level netlist without using a golden reference. Hereby, the attack uses a simpler yet more flexible learning model compared to existing work. Two different approaches are evaluated. The first approach is based on a simple feedforward fully connected neural network. The second approach utilizes genetic algorithms to evolve more complex convolutional neural network architectures specialized for the given task. The attack flow offers a generic and customizable framework for attacking locking schemes using machine learning techniques. We perform an extensive evaluation of SnapShot for two realistic attack scenarios, comprising both reference combinational and sequential benchmark circuits as well as silicon-proven RISC-V core modules. The evaluation results show that SnapShot achieves an average key prediction accuracy of 82.60% for the selected attack scenario, with a significant performance increase of 10.49 percentage points compared to the state of the art. Moreover, SnapShot outperforms the existing technique on all evaluated benchmarks. The results indicate that the security foundation of common logic locking schemes is built on questionable assumptions. Based on the lessons learned, we discuss the vulnerabilities and potentials of logic locking uncovered by SnapShot. The conclusions offer insights into the challenges of designing future logic locking schemes that are resilient to machine learning attacks.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3431389',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Hybrid, scalable, trace-driven performance modeling of GPGPUs',\n",
       "  'authors': \"['Yehia Arafa', 'Abdel-Hameed Badawy', 'Ammar ElWazir', 'Atanu Barai', 'Ali Eker', 'Gopinath Chennupati', 'Nandakishore Santhi', 'Stephan Eidenbenz']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"SC '21: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis\",\n",
       "  'abstract': 'In this paper, we present PPT-GPU, a scalable performance prediction toolkit for GPUs. PPT-GPU achieves scalability through a hybrid high-level modeling approach where some computations are extrapolated and multiple parts of the model are parallelized. The tool primary prediction models use pre-collected memory and instructions traces of the workloads to accurately capture the dynamic behavior of the kernels. PPT-GPU reports an extensive array of GPU performance metrics accurately while being easily extensible. We use a broad set of benchmarks to verify predictions accuracy. We compare the results against hardware metrics collected using vendor profiling tools and cycle-accurate simulators. The results show that the performance predictions are highly correlated to the actual hardware (MAPE: < 16% and Correlation: > 0.98). Moreover, PPT-GPU is orders of magnitude faster than cycle-accurate simulators. This comprehensiveness of the collected metrics can guide architects and developers to perform design space explorations. Moreover, the scalability of the tool enables conducting efficient and fast sensitivity analyses for performance-critical applications.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3458817.3476221',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'One size does not fit all: security hardening of MIPS embedded systems via static binary debloating for shared libraries',\n",
       "  'authors': \"['Haotian Zhang', 'Mengfei Ren', 'Yu Lei', 'Jiang Ming']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'Embedded systems have become prominent targets for cyberattacks. To exploit firmware’s memory corruption vulnerabilities, cybercriminals harvest reusable code gadgets from the large shared library codebase (e.g., uClibc). Unfortunately, unlike their desktop counterparts, embedded systems lack essential computing resources to enforce security hardening techniques. Recently, we have witnessed a surge of software debloating as a new defense mechanism against code-reuse attacks; it erases unused code to significantly diminish the possibilities of constructing reusable gadgets. Because of the single firmware image update style, static library debloating shows promise to fortify embedded systems without compromising performance and forward compatibility. However, static library debloating on stripped binaries (e.g., firmware’s shared libraries) is still an enormous challenge. In this paper, we show that this challenge is not insurmountable for MIPS firmware. We develop a novel system, named uTrimmer, to identify and wipe out unused basic blocks from shared libraries’ binary code, without causing additional runtime overhead or memory consumption. We propose a new method to identify address-taken blocks/functions, which further help us maintain an inter-procedural control flow graph to conservatively include library code that could be potentially used by firmware. By capturing address access patterns for position-independent code, we circumvent the challenge of determining code-pointer targets and safely eliminate unused code. We run uTrimmer to debloat shared libraries for SPEC CPU2017 benchmarks, popular firmware applications (e.g., Apache, BusyBox, and OpenSSL), and a real-world wireless router firmware image. Our experiments show that not only does uTrimmer deliver functional programs, but also it can cut the exposed code surface and eliminate various reusable code gadgets remarkably. uTrimmer’s debloating capability can compete with the static linking results.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503222.3507768',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A Design Methodology for Energy-Aware Processing in Unmanned Aerial Vehicles',\n",
       "  'authors': \"['Jingyu He', 'Yao Xiao', 'Corina Bogdan', 'Shahin Nazarian', 'Paul Bogdan']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Design Automation of Electronic Systems',\n",
       "  'abstract': 'Unmanned Aerial Vehicles (UAVs) have rapidly become popular for monitoring, delivery, and actuation in many application domains such as environmental management, disaster mitigation, homeland security, energy, transportation, and manufacturing. However, the UAV perception and navigation intelligence (PNI) designs are still in their infancy and demand fundamental performance and energy optimizations to be eligible for mass adoption. In this article, we present a generalizable three-stage optimization framework for PNI systems that (i) abstracts the high-level programs representing the perception, mining, processing, and decision making of UAVs into complex weighted networks tracking the interdependencies between universal low-level intermediate representations; (ii) exploits a differential geometry approach to schedule and map the discovered PNI tasks onto an underlying manycore architecture. To mine the complexity of optimal parallelization of perception and decision modules in UAVs, this proposed design methodology relies on an Ollivier-Ricci curvature-based load-balancing strategy that detects the parallel communities of the PNI applications for maximum parallel execution, while minimizing the inter-core communication; and (iii) relies on an energy-aware mapping scheme to minimize the energy dissipation when assigning the communities onto tile-based networks-on-chip. We validate this approach based on various drone PNI designs including flight controller, path planning, and visual navigation. The experimental results confirm that the proposed framework achieves 23% flight time reduction and up to 34% energy savings for the flight controller application. In addition, the optimization on a 16-core platform improves the on-time visit rate of the path planning algorithm by 14% while reducing 81% of run time for ConvNet visual navigation.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3470451',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'The Android Platform Security Model',\n",
       "  'authors': \"['René Mayrhofer', 'Jeffrey Vander Stoep', 'Chad Brubaker', 'Nick Kralevich']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Privacy and Security',\n",
       "  'abstract': 'Android is the most widely deployed end-user focused operating system. With its growing set of use cases encompassing communication, navigation, media consumption, entertainment, finance, health, and access to sensors, actuators, cameras, or microphones, its underlying security model needs to address a host of practical threats in a wide variety of scenarios while being useful to non-security experts. The model needs to strike a difficult balance between security, privacy, and usability for end users, assurances for app developers, and system performance under tight hardware constraints. While many of the underlying design principles have implicitly informed the overall system architecture, access control mechanisms, and mitigation techniques, the Android security model has previously not been formally published. This article aims to both document the abstract model and discuss its implications. Based on a definition of the threat model and Android ecosystem context in which it operates, we analyze how the different security measures in past and current Android implementations work together to mitigate these threats. There are some special cases in applying the security model, and we discuss such deliberate deviations from the abstract model.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3448609',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Defensive approximation: securing CNNs using approximate computing',\n",
       "  'authors': \"['Amira Guesmi', 'Ihsen Alouani', 'Khaled N. Khasawneh', 'Mouna Baklouti', 'Tarek Frikha', 'Mohamed Abid', 'Nael Abu-Ghazaleh']\",\n",
       "  'date': 'April 2021',\n",
       "  'source': \"ASPLOS '21: Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'In the past few years, an increasing number of machine-learning and deep learning structures, such as Convolutional Neural Networks (CNNs), have been applied to solving a wide range of real-life problems. However, these architectures are vulnerable to adversarial attacks: inputs crafted carefully to force the system output to a wrong label. Since machine-learning is being deployed in safety-critical and security-sensitive domains, such attacks may have catastrophic security and safety consequences. In this paper, we propose for the first time to use hardware-supported approximate computing to improve the robustness of machine learning classifiers. We show that our approximate computing implementation achieves robustness across a wide range of attack scenarios. Specifically, we show that successful adversarial attacks against the exact classifier have poor transferability to the approximate implementation. The transferability is even poorer for the black-box attack scenarios, where adversarial attacks are generated using a proxy model. Surprisingly, the robustness advantages also apply to white-box attacks where the attacker has unrestricted access to the approximate classifier implementation: in this case, we show that substantially higher levels of adversarial noise are needed to produce adversarial examples. Furthermore, our approximate computing model maintains the same level in terms of classification accuracy, does not require retraining, and reduces resource utilization and energy consumption of the CNN. We conducted extensive experiments on a set of strong adversarial attacks; We empirically show that the proposed implementation increases the robustness of a LeNet-5 and an Alexnet CNNs by up to 99% and 87%, respectively for strong transferability-based attacks along with up to 50% saving in energy consumption due to the simpler nature of the approximate logic. We also show that a white-box attack requires a remarkably higher noise budget to fool the approximate classifier, causing an average of 4\\xa0dB degradation of the PSNR of the input image relative to the images that succeed in fooling the exact classifier.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3445814.3446747',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'FlyNet: a platform to support scientific workflows from the edge to the core for UAV applications',\n",
       "  'authors': \"['Eric Lyons', 'Hakan Saplakoglu', 'Michael Zink', 'Komal Thareja', 'Anirban Mandal', 'Chengyi Qu', 'Songjie Wang', 'Prasad Calyam', 'George Papadimitriou', 'Ryan Tanaka', 'Ewa Deelman']\",\n",
       "  'date': 'December 2021',\n",
       "  'source': \"UCC '21: Proceedings of the 14th IEEE/ACM International Conference on Utility and Cloud Computing\",\n",
       "  'abstract': 'Many Internet of Things (IoT) applications require compute resources that cannot be provided by the devices themselves. At the same time, processing of the data generated by IoT devices often has to be performed in real- or near real-time. Examples of such scenarios are autonomous vehicles in the form of cars and drones where the processing of observational data (e.g., video feeds) needs to be performed expeditiously to allow for safe operation. To support the computational needs and timeliness requirements of such applications it is essential to include suitable edge resources to execute these applications. In this paper, we present our FlyNet architecture which has the goal to provide a new platform to support workflows that include applications executing at the network edge, at the computing core, and leverage deeply programmable networks. We discuss the challenges associated with provisioning such networking and compute infrastructure on demand, tailored to IoT application workflows. We describe a strategy to leverage the end-to-end integrated infrastructure that covers all points in the spectrum of response latency for application processing. We present our prototype implementation of the architecture and evaluate its performance for the case of drone video analytics workflows with varying computational requirements.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3468737.3494098',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Fast ReRoute on Programmable Switches',\n",
       "  'authors': \"['Marco Chiesa', 'Roshan Sedar', 'Gianni Antichi', 'Michael Borokhovich', 'Andrzej Kamisiński', 'Georgios Nikolaidis', 'Stefan Schmid']\",\n",
       "  'date': 'None',\n",
       "  'source': 'IEEE/ACM Transactions on Networking',\n",
       "  'abstract': 'Highly dependable communication networks usually rely on some kind of Fast Re-Route (FRR) mechanism which allows to quickly re-route traffic upon failures, entirely in the data plane. This paper studies the design of FRR mechanisms for emerging reconfigurable switches. Our main contribution is an FRR primitive for <italic>programmable</italic> data planes, PURR, which provides low failover latency and high switch throughput, by <italic>avoiding packet recirculation</italic>. PURR tolerates multiple concurrent failures and comes with minimal memory requirements, ensuring <italic>compact</italic> forwarding tables, by unveiling an intriguing connection to classic &#x201C;string theory&#x201D; (<italic>i.e.</italic>, stringology), and in particular, the shortest common supersequence problem. PURR is well-suited for high-speed match-action forwarding architectures (<italic>e.g.</italic>, PISA) and supports the implementation of a broad variety of FRR mechanisms. Our simulations and prototype implementation (on an FPGA and a Tofino switch) show that PURR improves TCAM memory occupancy by a factor of <inline-formula> <tex-math notation=\"LaTeX\">$1.5 \\\\times$ </tex-math></inline-formula>&#x2013;<inline-formula> <tex-math notation=\"LaTeX\">$10.8 \\\\times$ </tex-math></inline-formula> compared to a na&#x00EF;ve encoding when implementing state-of-the-art FRR mechanisms. PURR also improves the latency and throughput of datacenter traffic up to a factor of <inline-formula> <tex-math notation=\"LaTeX\">$2.8 \\\\times$ </tex-math></inline-formula>&#x2013;<inline-formula> <tex-math notation=\"LaTeX\">$5.5 \\\\times$ </tex-math></inline-formula> and <inline-formula> <tex-math notation=\"LaTeX\">$1.2 \\\\times$ </tex-math></inline-formula>&#x2013;<inline-formula> <tex-math notation=\"LaTeX\">$2 \\\\times$ </tex-math></inline-formula>, respectively, compared to approaches based on recirculating packets.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/TNET.2020.3045293',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Design Exploration and Scalability Analysis of a CMOS-Integrated, Polymorphic, Nanophotonic Arithmetic-Logic Unit',\n",
       "  'authors': \"['Venkata Sai Praneeth Karempudi', 'Shreyan Datta', 'Ishan G Thakkar']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"SenSys '21: Proceedings of the 19th ACM Conference on Embedded Networked Sensor Systems\",\n",
       "  'abstract': \"Over the past two decades, the clock speed, and hence, the singlecore performance of microprocessors has already stagnated. Following this, the recent faltering of Moore's law due to the CMOS fabrication technology reaching its unavoidable physical limit has presaged daunting challenges for designing power-efficient and ultrafast microprocessors. To overcome these challenges, vigorous efforts have been made to develop new more-than-Moore technologies and architectures for computing. Among these, nanophotonic integrated circuits based computing architectures have shown revolutionary potential. Among recent demonstrations of nanophotonic circuits for computing, a polymorphic, nanophotonic ALU (PoN-ALU) carries a notable importance since it has shown very high flexibility, high speed, and low power consumption for computing. In this paper, we carry out a design space exploration of this PoN-ALU to derive new design guidelines that can help scale the speed and energy efficiency of PoNALU even further.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3485730.3494042',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Energy-aware scheduling of multi-version tasks on heterogeneous real-time systems',\n",
       "  'authors': \"['Julius Roeder', 'Benjamin Rouxel', 'Sebastian Altmeyer', 'Clemens Grelck']\",\n",
       "  'date': 'March 2021',\n",
       "  'source': \"SAC '21: Proceedings of the 36th Annual ACM Symposium on Applied Computing\",\n",
       "  'abstract': 'The emergence of battery-powered devices has led to an increase of interest in the energy consumption of computing devices. For embedded systems, dispatching the workload on different computing units enables the optimisation of the overall energy consumption on high-performance heterogeneous platforms. However, to use the full power of heterogeneity, architecture specific binary blocks are required, each with different energy/time trade-offs. Finding a scheduling strategy that minimises the energy consumption, while guaranteeing timing constraints creates new challenges. These challenges can only be met by using the full heterogeneous capacity of the platform (e.g. heterogeneous CPU, GPU, DVFS, dynamic frequency changes from within an application). We propose an off-line scheduling algorithm for dependent multiversion tasks based on Forward List Scheduling to minimise the overall energy consumption. Our heuristic accounts for Dynamic Voltage and Frequency Scaling (DVFS) and enables applications to dynamically adapt voltage and frequency during run time. We demonstrate the benefits of multi-version task models coupled with an energy-aware scheduler. We observe that selecting the most energy efficient version for each task does not lead to the lowest energy consumption for the whole application. Then we show that our approach produces schedules that are on average 45.6% more energy efficient than schedules produced by a state-of-the-art scheduling algorithm. Next we compare our heuristic against an optimal solution derived by an Integer Linear Programming (ILP) formulation (deviation of 1.6% on average). Lastly, we empirically show that the energy consumption predicted by our scheduler is close to the actual measured energy consumption on a Odroid-XU4 board (at most-15.8%).',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3412841.3441930',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Automated Orchestration of Online Educational Collaboration in Cloud-based Environments',\n",
       "  'authors': \"['Łukasz Czekierda', 'Krzysztof Zieliński', 'Sławomir Zieliński']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Multimedia Computing, Communications, and Applications',\n",
       "  'abstract': 'Integrated collaboration environments (ICEs) are widely used by corporations to increase productivity by fostering groupwide and interpersonal collaboration. In this article, we discuss the enhancements of such environment needed to build an educational ICE (E-ICE) that addresses the specific needs of educational users. The motivation for the research was the Małopolska Educational Cloud (MEC) project conducted by AGH University and its partners.The E-ICE developed by MEC project fosters collaboration between universities and high schools by creating an immersive virtual collaboration space. MEC is a unique project due to its scale and usage domain. Multiple online collaboration events are organized weekly between over 150 geographically scattered institutions. Such events, aside from videoconferencing, require various services. The MEC E-ICE is a complex composition of a significant number of services and various terminals that require very specific configuration and management.In this article, we focus on a model-driven approach to automating the organization of online meetings in their preparation, execution, and conclusion phases. We present a conceptual model of E-ICE-supported educational courses, introduce a taxonomy of online educational services, identify planes and modes of their operation, as well as discuss the most common collaboration patterns.The MEC E-ICE, which we present as a case study, is built in accordance with the presented, model-driven approach. MEC educational services are described in a way that allows for converting the declarative specification of E-ICE application models into platform-independent models, platform-specific models, and, finally, working sets of orchestrated service instances. Such approach both reduces the level of technical knowledge required from the end-users and considerably speeds up the construction of online educational collaboration environments.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3412381',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Localizing Acoustic Objects on a Single Phone',\n",
       "  'authors': \"['Hongzi Zhu', 'Yuxiao Zhang', 'Zifan Liu', 'Xiao Wang', 'Shan Chang', 'Yingying Chen']\",\n",
       "  'date': 'None',\n",
       "  'source': 'IEEE/ACM Transactions on Networking',\n",
       "  'abstract': 'Finding a small object (e.g., earbuds, keys or a wallet) in an indoor environment (e.g., in a house or an office) can be frustrating. In this paper, we propose an innovative system, called <italic>HyperEar</italic>, to localize such an object using only a single smartphone, based on enhanced time-difference-of-arrival (TDoA) measurements over acoustic signals issued from the object. One major challenge is the hardware limitations of a Commercial-Off-The-Shelf (COTS) phone with a short separation between the two microphones and the low sampling rate of such microphones. HyperEar enhances the accuracy of TDoA measurements by virtually increasing distances between microphones through sliding the phone in the air. HyperEar requires no communication for synchronization between the phone and the object and is a low-cost and easy-to-use system. We evaluate the performance of HyperEar via extensive experiments in various indoor conditions and the results demonstrate that, for an object of 7 m away, HyperEar can achieve a mean localization accuracy of about 15 cm when the object in normal indoor environments.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/TNET.2021.3080820',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'VIoLET: An Emulation Environment for Validating IoT Deployments at Large Scales',\n",
       "  'authors': \"['Shrey Baheti', 'Shreyas Badiger', 'Yogesh Simmhan']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Cyber-Physical Systems',\n",
       "  'abstract': 'Internet of Things (IoT) deployments have been growing manifold, encompassing sensors, networks, edge, fog, and cloud resources. Despite the intense interest from researchers and practitioners, most do not have access to large-scale IoT testbeds for validation. Simulation environments that allow analytical modeling are a poor substitute for evaluating software platforms or application workloads in realistic computing environments. Here, we propose a virtual environment for validating Internet of Things at large scales (VIoLET), an emulator for defining and launching large-scale IoT deployments within cloud VMs. It allows users to declaratively specify container-based compute resources that match the performance of native IoT compute devices using Docker. These can be inter-connected by complex topologies on which bandwidth and latency rules are enforced. Users can configure synthetic sensors for data generation as well. We also incorporate models for CPU resource dynamism, and for failure and recovery of the underlying devices. We offer a detailed comparison of VIoLET’s compute and network performance between the virtual and physical deployments, evaluate its scaling with deployments with up to 1, 000 devices and 4, 000 device-cores, and validate its ability to model resource dynamism. Our extensive experiments show that the performance of the virtual IoT environment accurately matches the expected behavior, with deviations levels within what is seen in actual physical devices. It also scales to 1, 000s of devices and at a modest cloud computing costs of under 0.15% of the actual hardware cost, per hour of use, with minimal management effort. This IoT emulation environment fills an essential gap between IoT simulators and real deployments.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3446346',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Astraea: towards QoS-aware and resource-efficient multi-stage GPU services',\n",
       "  'authors': \"['Wei Zhang', 'Quan Chen', 'Kaihua Fu', 'Ningxin Zheng', 'Zhiyi Huang', 'Jingwen Leng', 'Minyi Guo']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'Multi-stage user-facing applications on GPUs are widely-used nowa- days, and are often implemented to be microservices. Prior re- search works are not applicable to ensuring QoS of GPU-based microservices due to the different communication patterns and shared resource contentions. We propose Astraea to manage GPU microservices considering the above factors. In Astraea, a microser- vice deployment policy is used to maximize the supported peak service load while ensuring the required QoS. To adaptively switch the communication methods between microservices according to different deployments, we propose an auto-scaling GPU communi- cation framework. The framework automatically scales based on the currently used hardware topology and microservice location, and adopts global memory-based techniques to reduce intra-GPU communication. Astraea increases the supported peak load by up to 82.3% while achieving the desired 99%-ile latency target compared with state-of-the-art solutions.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503222.3507721',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Paulihedral: a generalized block-wise compiler optimization framework for Quantum simulation kernels',\n",
       "  'authors': \"['Gushu Li', 'Anbang Wu', 'Yunong Shi', 'Ali Javadi-Abhari', 'Yufei Ding', 'Yuan Xie']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'The quantum simulation kernel is an important subroutine appearing as a very long gate sequence in many quantum programs. In this paper, we propose Paulihedral, a block-wise compiler framework that can deeply optimize this subroutine by exploiting high-level program structure and optimization opportunities. Paulihedral first employs a new Pauli intermediate representation that can maintain the high-level semantics and constraints in quantum simulation kernels. This naturally enables new large-scale optimizations that are hard to implement at the low gate-level. In particular, we propose two technology-independent instruction scheduling passes, and two technology-dependent code optimization passes which reconcile the circuit synthesis, gate cancellation, and qubit mapping stages of the compiler. Experimental results show that Paulihedral can outperform state-of-the-art compiler infrastructures in a wide-range of applications on both near-term superconducting quantum processors and future fault-tolerant quantum computers.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503222.3507715',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'ALPINE: An Agile Processing-in-Memory Macro Compilation Framework',\n",
       "  'authors': \"['Jinshan Zhang', 'Bo Jiao', 'Yunzhengmao Wang', 'Haozhe Zhu', 'Lihua Zhang', 'Chixiao Chen']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"GLSVLSI '21: Proceedings of the 2021 on Great Lakes Symposium on VLSI\",\n",
       "  'abstract': 'Processing-in-Memory architectures and circuit designs are playing significant roles in the recent energy-efficient machine learning chips. This paper proposes a PIM macro compilation framework called ALPINE to speed up previously tedious and error-prone PIM design flow, paving the way towards open-source and process-portable PIM chips. Relying on an extensible PIM standard cell library, ALPINE can generate the corresponding topology according to the specification, and process placement and routing. The proposed PIM macro is compatible with different storage devices such as SRAM and RRAM, and can support various quantization bit-widths and dataflows. To verify the effectiveness, a 128×128 SRAM-based PIM macro instance is implemented, and the simulation results show that it can achieve an energy efficiency of 19.05TOPS/W under 65nm CMOS technology. The macro performance is not inferior to the state-of-the-art custom PIM designs.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3453688.3461532',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Betalogger: Smartphone Sensor-based Side-channel Attack Detection and Text Inference Using Language Modeling and Dense MultiLayer Neural Network',\n",
       "  'authors': \"['Abdul Rehman Javed', 'Saif Ur Rehman', 'Mohib Ullah Khan', 'Mamoun Alazab', 'Habib Ullah Khan']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Asian and Low-Resource Language Information Processing',\n",
       "  'abstract': 'With the recent advancement of smartphone technology in the past few years, smartphone usage has increased on a tremendous scale due to its portability and ability to perform many daily life tasks. As a result, smartphones have become one of the most valuable targets for hackers to perform cyberattacks, since the smartphone can contain individuals’ sensitive data. Smartphones are embedded with highly accurate sensors. This article proposes BetaLogger, an Android-based application that highlights the issue of leaking smartphone users’ privacy using smartphone hardware sensors (accelerometer, magnetometer, and gyroscope). BetaLogger efficiently infers the typed text (long or short) on a smartphone keyboard using Language Modeling and a Dense Multi-layer Neural Network (DMNN). BetaLogger is composed of two major phases: In the first phase, Text Inference Vector is given as input to the DMNN model to predict the target labels comprising the alphabet, and in the second phase, sequence generator module generate the output sequence in the shape of a continuous sentence. The outcomes demonstrate that BetaLogger generates highly accurate short and long sentences, and it effectively enhances the inference rate in comparison with conventional machine learning algorithms and state-of-the-art studies.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3460392',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Formal verification of high-level synthesis',\n",
       "  'authors': \"['Yann Herklotz', 'James D. Pollard', 'Nadesh Ramanathan', 'John Wickerson']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': 'Proceedings of the ACM on Programming Languages',\n",
       "  'abstract': 'High-level synthesis (HLS), which refers to the automatic compilation of software into hardware, is rapidly gaining popularity. In a world increasingly reliant on application-specific hardware accelerators, HLS promises hardware designs of comparable performance and energy efficiency to those coded by hand in a hardware description language such as Verilog, while maintaining the convenience and the rich ecosystem of software development. However, current HLS tools cannot always guarantee that the hardware designs they produce are equivalent to the software they were given, thus undermining any reasoning conducted at the software level. Furthermore, there is mounting evidence that existing HLS tools are quite unreliable, sometimes generating wrong hardware or crashing when given valid inputs. To address this problem, we present the first HLS tool that is mechanically verified to preserve the behaviour of its input software. Our tool, called Vericert, extends the CompCert verified C compiler with a new hardware-oriented intermediate language and a Verilog back end, and has been proven correct in Coq. Vericert supports most C constructs, including all integer operations, function calls, local arrays, structs, unions, and general control-flow statements. An evaluation on the PolyBench/C benchmark suite indicates that Vericert generates hardware that is around an order of magnitude slower (only around 2× slower in the absence of division) and about the same size as hardware generated by an existing, optimising (but unverified) HLS tool.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3485494',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Power Side-Channel Analysis of RNS GLV ECC Using Machine and Deep Learning Algorithms',\n",
       "  'authors': \"['Mohamad Ali Mehrabi', 'Naila Mukhtar', 'Alireza Jolfaei']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Internet Technology',\n",
       "  'abstract': 'Many Internet of Things applications in smart cities use elliptic-curve cryptosystems due to their efficiency compared to other well-known public-key cryptosystems such as RSA. One of the important components of an elliptic-curve-based cryptosystem is the elliptic-curve point multiplication which has been shown to be vulnerable to various types of side-channel attacks. Recently, substantial progress has been made in applying deep learning to side-channel attacks. Conceptually, the idea is to monitor a core while it is running encryption for information leakage of a certain kind, for example, power consumption. The knowledge of the underlying encryption algorithm can be used to train a model to recognise the key used for encryption. The model is then applied to traces gathered from the crypto core in order to recover the encryption key. In this article, we propose an RNS GLV elliptic curve cryptography core which is immune to machine learning and deep learning based side-channel attacks. The experimental analysis confirms the proposed crypto core does not leak any information about the private key and therefore it is suitable for hardware implementations.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3423555',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Increasing the Channel Capacity: Parallel Data Transmission in a Testbed for Molecular Communication',\n",
       "  'authors': \"['Max Bartunik', 'Matthias Streb', 'Harald Unterweger', 'Jakob Haller', 'Jens Kirchner']\",\n",
       "  'date': 'September 2021',\n",
       "  'source': \"NANOCOM '21: Proceedings of the Eight Annual ACM International Conference on Nanoscale Computing and Communication\",\n",
       "  'abstract': 'In the field of molecular communication, molecules or other particles in the nanoscale are used to transmit information. This rather new communication paradigm has a large application potential, ranging from medicine to industrial systems. As testbeds are essential for evaluating the capabilities of this new form of communication, we improve an existing water-based testbed to achieve parallel data transmission within one communication channel in this work. To this end, two different information carriers, superparamagnetic iron oxide nanoparticles and fluorescent particles, are used to simultaneously transmit independent data streams. A detection system for fluorescent particles was specifically developed for the fluid molecular communication testbed. Using parallel data transmission in the modified testbed the channel capacity was nearly doubled in comparison to a transmission using only a single type of information particles.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3477206.3477449',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Paging and the Address-Translation Problem',\n",
       "  'authors': \"['Michael A. Bender', 'Abhishek Bhattacharjee', 'Alex Conway', 'Martín Farach-Colton', 'Rob Johnson', 'Sudarsun Kannan', 'William Kuszmaul', 'Nirjhar Mukherjee', 'Don Porter', 'Guido Tagliavini', 'Janet Vorobyeva', 'Evan West']\",\n",
       "  'date': 'July 2021',\n",
       "  'source': \"SPAA '21: Proceedings of the 33rd ACM Symposium on Parallelism in Algorithms and Architectures\",\n",
       "  'abstract': 'The classical paging problem, introduced by Sleator and Tarjan in 1985, formalizes the problem of caching pages in RAM in order to minimize IOs. Their online formulation ignores the cost of address translation: programs refer to data via virtual addresses, and these must be translated into physical locations in RAM. Although the cost of an individual address translation is much smaller than that of an IO, every memory access involves an address translation, whereas IOs can be infrequent. In practice, one can spend money to avoid paging by over-provisioning RAM; in contrast, address translation is effectively unavoidable. Thus address-translation costs can sometimes dominate paging costs, and systems must simultaneously optimize both.   To mitigate the cost of address translation, all modern CPUs have translation lookaside buffers (TLBs), which are hardware caches of common address translations. What makes TLBs interesting is that a single TLB entry can potentially encode the address translation for many addresses. This is typically achieved via the use of huge pages, which translate runs of contiguous virtual addresses to runs of contiguous physical addresses. Huge pages reduce TLB misses at the cost of increasing the IOs needed to maintain contiguity in RAM. This tradeoff between TLB misses and IOs suggests that the classical paging problem does not tell the full story.   This paper introduces the Address-Translation Problem, which formalizes the problem of maintaining a TLB, a page table, and RAM in order to minimize the total cost of both TLB misses and IOs. We present an algorithm that achieves the benefits of huge pages for TLB misses without the downsides of huge pages for IOs.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3409964.3461814',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'ANT-UNet: Accurate and Noise-Tolerant Segmentation for Pathology Image Processing',\n",
       "  'authors': \"['Yufei Chen', 'Tingtao Li', 'Qinming Zhang', 'Wei Mao', 'Nan Guan', 'Mei Tian', 'Hao Yu', 'Cheng Zhuo']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Journal on Emerging Technologies in Computing Systems',\n",
       "  'abstract': 'Pathology image segmentation is an essential step in early detection and diagnosis for various diseases. Due to its complex nature, precise segmentation is not a trivial task. Recently, deep learning has been proved as an effective option for pathology image processing. However, its efficiency is highly restricted by inconsistent annotation quality. In this article, we propose an accurate and noise-tolerant segmentation approach to overcome the aforementioned issues. This approach consists of two main parts: a preprocessing module for data augmentation and a new neural network architecture, ANT-UNet. Experimental results demonstrate that, even on a noisy dataset, the proposed approach can achieve more accurate segmentation with 6% to 35% accuracy improvement versus other commonly used segmentation methods. In addition, the proposed architecture is hardware friendly, which can reduce the amount of parameters to one-tenth of the original and achieve 1.7× speed-up.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3451213',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'T-Cache: Efficient Policy-Based Forwarding Using Small TCAM',\n",
       "  'authors': \"['Ying Wan', 'Haoyu Song', 'Yang Xu', 'Yilun Wang', 'Tian Pan', 'Chuwen Zhang', 'Yi Wang', 'Bin Liu']\",\n",
       "  'date': 'None',\n",
       "  'source': 'IEEE/ACM Transactions on Networking',\n",
       "  'abstract': 'Ternary Content Addressable Memory (TCAM) is widely used by modern routers and switches to support policy-based forwarding due to its incomparable lookup speed and flexible matching patterns. However, the limited TCAM capacity does not scale with the ever-increasing rule table size due to the high hardware cost and high power consumption. At present, using TCAM just as a rule cache is an appealing solution, but one must resolve several tricky issues including the rule dependency and the associated TCAM updates. In this paper, we propose a new approach which can generate dependency-free rules to cache. By removing the rule dependency, the complex TCAM update problem also disappears. We provide the complete T-cache system design including slow path processing and cache replacement, and implement a T-cache prototype on Barefoot Tofino switches. We conduct comprehensive software simulations and hardware experiments based on real-world and synthesized rule tables and packet traces to show that T-cache is efficient and robust for network traffic in various scenarios.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/TNET.2021.3098320',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Ghost Thread: Effective User-Space Cache Side Channel Protection',\n",
       "  'authors': \"['Robert Brotzman', 'Danfeng Zhang', 'Mahmut Kandemir', 'Gang Tan']\",\n",
       "  'date': 'April 2021',\n",
       "  'source': \"CODASPY '21: Proceedings of the Eleventh ACM Conference on Data and Application Security and Privacy\",\n",
       "  'abstract': 'Cache-based side channel attacks pose a serious threat to computer security. Numerous cache attacks have been demonstrated, highlighting the need for effective and efficient defense mechanisms to shield systems from this threat. In this paper, we propose a novel application-level protection mechanism, called Ghost Thread. Ghost Thread is a flexible library that allows a user to protect cache accesses to a requested sensitive region to mitigate cache-based side channel attacks. This is accomplished by injecting random cache accesses to the sensitive cache region by separate threads. Compared with prior work that injects noise in a modified OS and hardware, our novel approach is applicable to commodity OS and hardware. Compared with other user-space mitigation mechanisms, our novel approach does not require any special hardware support, and it only requires slight code changes in the protected application making it readily deployable. Evaluation results on an Apache server show that Ghost Thread provides both strong protection and negligible overhead on real-world applications where only a fragment requires protection. In the worst-case scenario where the entire application requires protection, Ghost Thread still incurs negligible overhead when a system is under utilized, and moderate overhead when a system is fully utilized.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3422337.3447846',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'NASCENT2: Generic Near-Storage Sort Accelerator for Data Analytics on SmartSSD',\n",
       "  'authors': \"['Sahand Salamat', 'Hui Zhang', 'Yang Seok Ki', 'Tajana Rosing']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Reconfigurable Technology and Systems',\n",
       "  'abstract': 'As the size of data generated every day grows dramatically, the computational bottleneck of computer systems has shifted toward storage devices. The interface between the storage and the computational platforms has become the main limitation due to its limited bandwidth, which does not scale when the number of storage devices increases. Interconnect networks do not provide simultaneous access to all storage devices and thus limit the performance of the system when executing independent operations on different storage devices. Offloading the computations to the storage devices eliminates the burden of data transfer from the interconnects. Near-storage computing offloads a portion of computations to the storage devices to accelerate big data applications. In this article, we propose a generic near-storage sort accelerator for data analytics, NASCENT2, which utilizes Samsung SmartSSD, an NVMe flash drive with an on-board FPGA chip that processes data in situ.NASCENT2 consists of dictionary decoder, sort, and shuffle FPGA-based accelerators to support sorting database tables based on a key column with any arbitrary data type. It exploits data partitioning applied by data processing management systems, such as SparkSQL, to breakdown the sort operations on colossal tables to multiple sort operations on smaller tables. NASCENT2 generic sort provides 2 × speedup and 15.2 × energy efficiency improvement as compared to the CPU baseline. It moreover considers the specifications of the SmartSSD (e.g., the FPGA resources, interconnect network, and solid-state drive bandwidth) to increase the scalability of computer systems as the number of storage devices increases. With 12 SmartSSDs, NASCENT2 is 9.9× (137.2 ×) faster and 7.3 × (119.2 ×) more energy efficient in sorting the largest tables of TPCC and TPCH benchmarks than the FPGA (CPU) baseline.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3472769',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Randomized row-swap: mitigating Row Hammer by breaking spatial correlation between aggressor and victim rows',\n",
       "  'authors': \"['Gururaj Saileshwar', 'Bolin Wang', 'Moinuddin Qureshi', 'Prashant J. Nair']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'Row Hammer is a fault-injection attack in which rapid activations to a single DRAM row causes bit-flips in nearby rows. Several recent defenses propose tracking aggressor-rows and applying mitigating action on neighboring victim rows by refreshing them. However, all such proposals using victim-focused mitigation preserve the spatial connection between victim and aggressor rows. Therefore, these proposals are susceptible to access patterns causing bit-flips in rows beyond the immediate neighbor. For example, the Half-Double attack causes bit-flips in the presence of victim-focused mitigation.   We propose Randomized Row-Swap (RRS), a novel mitigation action that breaks the spatial connection between the aggressor and victim DRAM rows. This enables RRS to provide robust defense against even complex Row Hammer access patterns. RRS is an aggressor-focused mitigation that periodically swaps aggressor-rows with other randomly selected rows in memory. This limits the possible damage in any one locality within the DRAM memory. While RRS can be used with any tracking mechanism, we implement it with a Misra-Gries tracker and target a Row Hammer Threshold of 4.8K activations (similar to the state-of-the-art attacks). Our evaluations show that RRS has negligible slowdown (0.4% on average) and provides strong security guarantees for avoiding Row Hammer bit flips even under several years of continuous attack.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503222.3507716',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Discrete samplers for approximate inference in probabilistic machine learning',\n",
       "  'authors': \"['Shirui Zhao', 'Nimish Shah', 'Wannes Meert', 'Marian Verhelst']\",\n",
       "  'date': 'March 2022',\n",
       "  'source': \"DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe\",\n",
       "  'abstract': 'Probabilistic reasoning models (PMs) and probabilistic inference bring advantages when dealing with small datasets or uncertainty on the observed data, and allow to integrate expert knowledge and create interpretable models. The main challenge of using these PMs in practice is that their inference is very compute-intensive. Therefore, custom hardware architectures for the exact and approximate inference of PMs have been proposed in the SotA. The throughput, energy and area efficiency of approximate PM inference accelerators are strongly dominated by the sampler blocks required to sample arbitrary discrete distributions. This paper proposes and studies novel discrete sampler architectures towards efficient and flexible hardware implementations for PM accelerators. Both cumulative distribution table (CDT) and Knuth-Yao (KY) based sampling algorithms are assessed, based on which different sampler hardware architectures were implemented. Innovation is brought in terms of a reconfigurable CDT sampling architecture with a flexible range and a reconfigurable Knuth-Yao sampling architecture that supports both flexible range and dynamic precision. All architectures are benchmarked on real-world Bayesian Networks, demonstrating up to 13× energy efficiency benefits and 11× area efficiency improvement of the optimized reconfigurable Knuth-Yao sampler over the traditional linear CDT-based samplers used in the PM SotA.',\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3539845.3540135',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'STM-multifrontal QR: streaming task mapping multifrontal QR factorization empowered by GCN',\n",
       "  'authors': \"['Shengle Lin', 'Wangdong Yang', 'Haotian Wang', 'Qinyun Tsai', 'Kenli Li']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"SC '21: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis\",\n",
       "  'abstract': 'Multifrontal QR algorithm, which consists of symbolic analysis and numerical factorization, is a high-performance algorithm for orthogonal factorizing sparse matrix. In this work, a graph convolutional network (GCN) for adaptively selecting the optimal reordering algorithm is proposed in symbolic analysis. Using our GCN adaptive classifier, the average numerical factorization time is reduced by 20.78% compared with the default approach, and the additional memory overhead is approximately 4% higher than that of prior work. Moreover, for numerical factorization, an optimized tasks stream parallel processing strategy is proposed and a more efficient computing task mapping framework for NUMA architecture is adopted in this paper, which called STM-Multifrontal QR factorization. Numerical experiments on the TaiShan Server show average 1.22x performance gains over the original SuiteSparseQR. Nearly 80% of datasets have achieved better performance compared with the MKL sparse QR on Intel Xeon 6248.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3458817.3476199',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'SGXoMeter: Open and Modular Benchmarking for Intel SGX',\n",
       "  'authors': \"['Mohammad Mahhouk', 'Nico Weichbrodt', 'Rüdiger Kapitza']\",\n",
       "  'date': 'April 2021',\n",
       "  'source': \"EuroSec '21: Proceedings of the 14th European Workshop on Systems Security\",\n",
       "  'abstract': \"Intel's Software Guard Extensions (SGX) are currently the most wide-spread commodity trusted execution environment, which provides integrity and confidentiality of sensitive code and data. Thereby, it offers protection even against privileged attackers and various forms of physical attacks. As a technology that only became available in late 2015, it has received massive interest and undergone a rapid evolution. Despite first ad-hoc attempts, there is so far no standardised approach to benchmark the SGX hardware, its associated environment, and techniques that were designed to harden SGX-based applications. In this paper, we present SGXoMeter, an open and modular framework designed to benchmark different SGX-aware CPUs, μcode revisions, SDK versions and extensions to mitigate side-channel attacks. SGXoMeter provides a set of practical SGX test case scenarios and eases the development of custom benchmarks. Furthermore, we compare it to sgx-nbench, the only other SGX application benchmark tool we are aware of, and evaluate their differences. Through our benchmark results, we identified a performance overhead of up to ã10 times induced between two different SGX-SDK versions for certain workload scenarios.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3447852.3458722',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'FlowAcc: real-time high-accuracy DNN-based optical flow accelerator in FPGA',\n",
       "  'authors': \"['Yehua Ling', 'Yuanxing Yan', 'Kai Huang', 'Gang Chen']\",\n",
       "  'date': 'March 2022',\n",
       "  'source': \"DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe\",\n",
       "  'abstract': 'Recently, accelerator architectures have been designed to use deep neural networks (DNNs) to accelerate computer vision tasks, possessing the advantages of both accuracy and speed. Optical flow accelerator is however not among these architectures that DNNs have been successfully deployed. Existing hardware accelerators for optical flow estimation are all designed for classic methods and generally perform poorly in estimated accuracy. In this paper, we present FlowAcc, a dedicated hardware accelerator for DNN-based optical flow estimation, adopting a pipelined hardware design for real-time processing of image streams. We design an efficient multiplexing binary neural network (BNN) architecture for pyramidal feature extraction to significantly reduce the hardware cost and make it independent of the pyramid level number. Furthermore, efficient hamming distance calculation and competent flow regularization are utilized for hierarchical optical flow estimation to greatly improve the system efficiency. Comprehensive experimental results demonstrate that FlowAcc achieves state-of-the-art estimation accuracy and real-time performance on the Middlebury dataset when compared with the existing optical flow accelerators.',\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3539845.3539880',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'FaaSFlow: enable efficient workflow execution for function-as-a-service',\n",
       "  'authors': \"['Zijun Li', 'Yushi Liu', 'Linsong Guo', 'Quan Chen', 'Jiagan Cheng', 'Wenli Zheng', 'Minyi Guo']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'Serverless computing (Function-as-a-Service) provides fine-grain resource sharing by running functions (or Lambdas) in containers. Data-dependent functions are required to be invoked following a pre-defined logic, which is known as serverless workflows. However, our investigation shows that the traditional master-worker based workflow execution architecture performs poorly in serverless context. One significant overhead results from the master-side workflow schedule pattern, with which the functions are triggered in the master node and assigned to worker nodes for execution. Besides, the data movement between workers also reduces the throughput.  To this end, we present a worker-side workflow schedule pattern for serverless workflow execution. Following the design, we implement FaaSFlow to enable efficient workflow execution in the serverless context. Besides, we propose an adaptive storage library FaaStore that enables fast data transfer between functions on the same node without through the database. Experiment results show that FaaSFlow effectively mitigates the workflow scheduling overhead by 74.6% on average and data transmission overhead by 95% at most. When the network bandwidth fluctuates, FaaSFlow-FaaStore reduces the throughput degradation by 23.0%, and is able to multiply the utilization of network bandwidth by 1.5X-4X.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503222.3507717',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'The Processing-in-Memory Model',\n",
       "  'authors': \"['Hongbo Kang', 'Phillip B. Gibbons', 'Guy E. Blelloch', 'Laxman Dhulipala', 'Yan Gu', 'Charles McGuffey']\",\n",
       "  'date': 'July 2021',\n",
       "  'source': \"SPAA '21: Proceedings of the 33rd ACM Symposium on Parallelism in Algorithms and Architectures\",\n",
       "  'abstract': 'As computational resources become more efficient and data sizes grow, data movement is fast becoming the dominant cost in computing. Processing-in-Memory is emerging as a key technique for reducing costly data movement, by enabling computation to be executed on compute resources embedded in the memory modules themselves.   This paper presents the Processing-in-Memory (PIM) model, for the design and analysis of parallel algorithms on systems providing processing-in-memory modules. The PIM model focuses on keys aspects of such systems, while abstracting the rest. Namely, the model combines (i) a CPU-side consisting of parallel cores with fast access to a small shared memory of size M words (as in traditional parallel computing), (ii) a PIM-side consisting of P PIM modules, each with a core and a local memory of size Θ(n/P) words for an input of size n (as in traditional distributed computing), and (iii) a network between the two sides. The model combines standard parallel complexity metrics for both shared memory (work and depth) and distributed memory (local work, communication time) computing. A key algorithmic challenge is to achieve load balance among the PIM modules in both their communication and their local work, while minimizing the communication time. We demonstrate how to overcome this challenge for an ordered search structure, presenting a parallel PIM-skiplist data structure that efficiently supports a wide range of batch-parallel queries and updates.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3409964.3461816',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'NOVIA: A Framework for Discovering Non-Conventional Inline Accelerators',\n",
       "  'authors': \"['David Trilla', 'John-David Wellman', 'Alper Buyuktosunoglu', 'Pradip Bose']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"MICRO '21: MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture\",\n",
       "  'abstract': 'Accelerators provide an increasingly valuable source of performance in modern computing systems. In most cases, accelerators are implemented as stand-alone, offload engines to which the processor can send large computation tasks. For many edge devices, as performance needs increase accelerators become essential, but the tight constraints on these devices limit the extent to which offload engines can be incorporated. An alternative is inline accelerators, which can be integrated as part of the core and provide performance with much smaller start-up times and area overheads. While inline accelerators allow greater flexibility in the interface and acceleration of finer grain code, determining good inline candidate accelerators is non-trivial. In this paper, we present NOVIA, a framework to derive inline accelerators by examining the workload source code and identifying inline accelerator candidates that provide benefits across many different regions of the workload. These NOVIA-derived accelerators are then integrated into an embedded core. For this core, NOVIA produces inline accelerators that improve the performance of various benchmark suites like EEMBC Autobench 2.0 and Mediabench by 1.37x with only a 3% core area increase.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3466752.3480094',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Application Threats to Exploit Northbound Interface Vulnerabilities in Software Defined Networks',\n",
       "  'authors': \"['Bilal Rauf', 'Haider Abbas', 'Muhammad Usman', 'Tanveer A. Zia', 'Waseem Iqbal', 'Yawar Abbas', 'Hammad Afzal']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Computing Surveys',\n",
       "  'abstract': 'Software Defined Networking (SDN) is an evolving technology that decouples the control functionality from the underlying hardware managed by the control plane. The application plane supports programmers to develop numerous applications (such as networking, management, security, etc.) that can even be executed from remote locations. Northbound interface (NBI) bridges the control and application planes to execute the third-party applications business logic. Due to the software bugs in applications and existing vulnerabilities such as illegal function calling, resource exhaustion, lack of trust, and so on, NBIs are susceptible to different attacks. Based on the extensive literature review, we have identified that the researchers and academia have mainly focused on the security of the control plane, data plane, and southbound interface (SBI). NBI, in comparison, has received far less attention. In this article, the security of the least explored, but a critical component of the SDN architecture, i.e., NBI, is analyzed. The article provides a brief overview of SDN, followed by a detailed discussion on the categories of NBI, vulnerabilities of NBI, and threats posed by malicious applications to NBI. Efforts of the researchers to counter malicious applications and NBI issues are then discussed in detail. The standardization efforts for the single acceptable NBI and security requirements of SDN by Open Networking Foundation (ONF) are also presented. The article concludes with the future research directions for the security of a single acceptable NBI.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3453648',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'DAGguise: mitigating memory timing side channels',\n",
       "  'authors': \"['Peter W. Deutsch', 'Yuheng Yang', 'Thomas Bourgeat', 'Jules Drean', 'Joel S. Emer', 'Mengjia Yan']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'This paper studies the mitigation of memory timing side channels, where attackers utilize contention within DRAM controllers to infer a victim’s secrets. Already practical, this class of channels poses an important challenge to secure computing in shared memory environments.  Existing state-of-the-art memory timing side channel mitigations have several key performance and security limitations. Prior schemes require onerous static bandwidth partitioning, extensive profiling phases, or simply fail to protect against attacks which exploit fine-grained timing and bank information. We present DAGguise, a defense mechanism which fully protects against memory timing side channels while allowing for dynamic traffic contention in order to achieve good performance. DAGguise utilizes a novel abstract memory access representation, the Directed Acyclic Request Graph (rDAG for short), to model memory access patterns which experience contention. DAGguise shapes a victim’s memory access patterns according to a publicly known rDAG obtained through a lightweight profiling stage, completely eliminating information leakage.  We formally verify the security of DAGguise, proving that it maintains strong security guarantees. Moreover, by allowing dynamic traffic contention, DAGguise achieves a 12% overall system speedup relative to Fixed Service, which is the state-of-the-art mitigation mechanism, with up to a 20% relative speedup for co-located applications which do not require protection. We further claim that the principles of DAGguise can be generalized to protect against other types of scheduler-based timing side channels, such as those targeting on-chip networks, or functional units in SMT cores.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503222.3507747',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Network intelligence in 6G: challenges and opportunities',\n",
       "  'authors': \"['Albert Banchs', 'Marco Fiore', 'Andres Garcia-Saavedra', 'Marco Gramaglia']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"MobiArch '21: Proceedings of the 16th ACM Workshop on Mobility in the Evolving Internet Architecture\",\n",
       "  'abstract': 'The success of the upcoming 6G systems will largely depend on the quality of the Network Intelligence (NI) that will fully automate network management. Artificial Intelligence (AI) models are commonly regarded as the cornerstone for NI design, as they have proven extremely successful at solving hard problems that require inferring complex relationships from entangled, massive (network traffic) data. However, the common approach of plugging ‘vanilla’ AI models into controllers and orchestrators does not fulfil the potential of the technology. Instead, AI models should be tailored to the specific network level and respond to the specific needs of network functions, eventually coordinated by an end-to-end NI-native architecture for 6G. In this paper, we discuss these challenges and provide results for a candidate NI-driven functionality that is properly integrated into the proposed architecture: network capacity forecasting.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3477091.3482761',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Predictive auto-scaling with OpenStack Monasca',\n",
       "  'authors': \"['Giacomo Lanciano', 'Filippo Galli', 'Tommaso Cucinotta', 'Davide Bacciu', 'Andrea Passarella']\",\n",
       "  'date': 'December 2021',\n",
       "  'source': \"UCC '21: Proceedings of the 14th IEEE/ACM International Conference on Utility and Cloud Computing\",\n",
       "  'abstract': 'Cloud auto-scaling mechanisms are typically based on reactive automation rules that scale a cluster whenever some metric, e.g., the average CPU usage among instances, exceeds a predefined threshold. Tuning these rules becomes particularly cumbersome when scaling-up a cluster involves non-negligible times to bootstrap new instances, as it happens frequently in production cloud services. To deal with this problem, we propose an architecture for auto-scaling cloud services based on the status in which the system is expected to evolve in the near future. Our approach leverages on time-series forecasting techniques, like those based on machine learning and artificial neural networks, to predict the future dynamics of key metrics, e.g., resource consumption metrics, and apply a threshold-based scaling policy on them. The result is a predictive automation policy that is able, for instance, to automatically anticipate peaks in the load of a cloud application and trigger ahead of time appropriate scaling actions to accommodate the expected increase in traffic. We prototyped our approach as an open-source OpenStack component, which relies on, and extends, the monitoring capabilities offered by Monasca, resulting in the addition of predictive metrics that can be leveraged by orchestration components like Heat or Senlin. We show experimental results using a recurrent neural network and a multi-layer perceptron as predictor, which are compared with a simple linear regression and a traditional non-predictive auto-scaling policy. However, the proposed framework allows for the easy customization of the prediction policy as needed.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3468737.3494104',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Power and energy efficient routing for Mach-Zehnder interferometer based photonic switches',\n",
       "  'authors': \"['Markos Kynigos', 'Jose A. Pascual', 'Javier Navaridas', 'John Goodacre', 'Mikel Lujan']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ICS '21: Proceedings of the ACM International Conference on Supercomputing\",\n",
       "  'abstract': 'Silicon Photonic top-of-rack (ToR) switches are highly desirable for the datacenter (DC) and high-performance computing (HPC) domains for their potential high-bandwidth and energy efficiency. Recently, photonic Beneš switching fabrics based on Mach-Zehnder Interferometers (MZIs) have been proposed as a promising candidate for the internals of high-performance switches. However, state-of-the-art routing algorithms that control these switching fabrics are either computationally complex or unable to provide non-blocking, energy efficient routing permutations.To address this, we propose for the first time a combination of energy efficient routing algorithms and time-division multiplexing (TDM). We evaluate this approach by conducting a simulation-based performance evaluation of a 16x16 Beneš fabric, deployed as a ToR switch, when handling a set of 8 representative workloads from the DC and HPC domains. Our results show that state-of-the-art approaches (circuit switched energy efficient routing algorithms) introduce up to 23% contention in the switching fabric for some workloads, thereby increasing communication time. We show that augmenting the algorithms with TDM can ameliorate switch fabric contention by segmenting communication data and gracefully interleaving the segments, thus reducing communication time by up to 20% in the best case. We also discuss the impact of the TDM segment size, finding that although a 10KB segment size is the most beneficial in reducing communication time, a 100KB segment size offers similar performance while requiring a less stringent path-computation time window. Finally, we assess the impact of TDM on path-dependent insertion loss and switching energy consumption, finding it to be minimal in all cases.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3447818.3460363',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Software-Defined Vector Processing on Manycore Fabrics',\n",
       "  'authors': \"['Philip Bedoukian', 'Neil Adit', 'Edwin Peguero', 'Adrian Sampson']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"MICRO '21: MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture\",\n",
       "  'abstract': 'We describe a tiled architecture that can fluidly transition between manycore (MIMD) and vector (SIMD) execution. The hardware provides a software-defined vector programming model that lets applications aggregate groups of manycore tiles into logical vector engines. In manycore mode, the machine behaves as a standard parallel processor. In vector mode, groups of tiles repurpose their functional units as vector execution lanes and scratchpads as vector memory banks. The key mechanism is an instruction forwarding network: a single tile fetches instructions and sends them to other trailing cores. Most cores disable their frontends and instruction caches, so vector groups amortize the intrinsic hardware costs of von\\xa0Neumann control. Vector groups also use a decoupled access/execute scheme to centralize their memory requests and issue coalesced, wide loads.  We augment an existing RISC-V manycore design with a minimal hardware extension to implement software-defined vectors. Cycle-level simulation results show that software-defined vectors improve performance by an average of 1.7 × over standard MIMD execution while saving 22% of the energy. Compared to a similarly configured GPU, the architecture improves performance by 1.9 ×.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3466752.3480099',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'CeMux: Maximizing the Accuracy of Stochastic Mux Adders and an Application to Filter Design',\n",
       "  'authors': \"['Timothy J. Baker', 'John P. Hayes']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Design Automation of Electronic Systems',\n",
       "  'abstract': \"Stochastic computing (SC) is a low-cost computational paradigm that has promising applications in digital filter design, image processing, and neural networks. Fundamental to these applications is the weighted addition operation, which is most often implemented by a multiplexer (mux) tree. Mux-based adders have very low area but typically require long bitstreams to reach practical accuracy thresholds when the number of summands is large. In this work, we first identify the main contributors to mux adder error. We then demonstrate with analysis and experiment that two new techniques, precise sampling and full correlation, can target and mitigate these error sources. Implementing these techniques in hardware leads to the design of CeMux (Correlation-enhanced Multiplexer), a stochastic mux adder that is significantly more accurate and uses much less area than traditional weighted adders. We compare CeMux to other SC and hybrid designs for an electrocardiogram filtering case study that employs a large digital filter. One major result is that CeMux is shown to be accurate even for large input sizes. CeMux's higher accuracy leads to a latency reduction of 4× to 16× over other designs. Furthermore, CeMux uses about 35% less area than existing designs, and we demonstrate that a small amount of accuracy can be traded for a further 50% reduction in area. Finally, we compare CeMux to a conventional binary design and we show that CeMux can achieve a 50% to 73% area reduction for similar power and latency as the conventional design but at a slightly higher level of error.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3491213',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Prolonging 3D NAND SSD lifetime via read latency relaxation',\n",
       "  'authors': \"['Chun-Yi Liu', 'Yunju Lee', 'Myoungsoo Jung', 'Mahmut Taylan Kandemir', 'Wonil Choi']\",\n",
       "  'date': 'April 2021',\n",
       "  'source': \"ASPLOS '21: Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'The adoption of 3D NAND has significantly increased the SSD density; however, 3D NAND density-increasing techniques, such as extensive stacking of cell layers, can amplify read disturbances and shorten SSD lifetime. From our lifetime-impact characterization on 8 state-of-the-art SSDs, we observe that the 3D TLC/QLC SSDs can be worn-out by low read-only workloads within their warranty period since a huge amount of read disturbance-induced rewrites are performed in the background. To understand alternative read disturbance mitigation opportunities, we also conducted read-latency characterizations on 2 other SSDs without the background rewrite mechanism. The collected results indicate that, without the background rewriting, the read latencies of the majority of data become higher, as the number of reads on the data increases. Motivated by these two characterizations, in this paper, we propose to relax the short read latency constraint on the high-density 3D SSDs. Specifically, our proposal relies on the hint information passed from applications to SSDs that specifies the expected read performance. By doing so, the lifetime consumption caused by the read-induced writes can be reduced, thereby prolonging the SSD lifetime. The detailed experimental evaluations show that our proposal can reduce up to 56% of the rewrite-induced spent-lifetime with only 2% lower performance, under a file-server application.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3445814.3446733',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Parallel application power and performance prediction modeling using simulation',\n",
       "  'authors': \"['Kishwar Ahmed', 'Kazutomo Yoshii', 'Samia Tasnim']\",\n",
       "  'date': 'December 2021',\n",
       "  'source': \"WSC '21: Proceedings of the Winter Simulation Conference\",\n",
       "  'abstract': 'High performance computing (HPC) system runs compute-intensive parallel applications requiring large number of nodes. An HPC system consists of heterogeneous computer architecture nodes, including CPUs, GPUs, field programmable gate arrays (FPGAs), etc. Power capping is a method to improve parallel application performance subject to variable power constraints. In this paper, we propose a parallel application power and performance prediction simulator. We present prediction model to predict application power and performance for unknown power-capping values considering heterogeneous computing architecture. We develop a job scheduling simulator based on parallel discrete-event simulation engine. The simulator includes a power and performance prediction model, as well as a resource allocation model. Based on real-life measurements and trace data, we show the applicability of our proposed prediction model and simulator.',\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3522802.3522980',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'APUF-BNN: An Automated Framework for Efficient Combinational Logic Based Implementation of Arbiter PUF through Binarized Neural Network',\n",
       "  'authors': \"['Pranesh Santikellur', 'Rijoy Mukherjee', 'Rajat Subhra Chakraborty']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"GLSVLSI '21: Proceedings of the 2021 on Great Lakes Symposium on VLSI\",\n",
       "  'abstract': 'Analysis of Physically Unclocnable Functions (PUFs) from a Boolean function perspective, and the efficient hardware implementation of such Boolean representations, can potentially lead to interesting insights about their behavior and robustness. Such a circuit implementation can also be a convenient substitute for the machine learning model of a PUF instance in PUF-based security protocols. In this paper, we present APUF-BN, a novel computer-aided design (CAD) framework to efficiently generate a combinational circuit representation of an Arbiter PUF (APUF) instance, which accurately mimics its input-output behavior. This representation is derived from an optimized fully-connected Binarized Neural Network (BNN) model of the APUF. Our fully-automated CAD framework takes challenge-response pairs (CRPs) of an APUF instance as input, and generates Verilog description corresponding to the optimized combinational circuit representation as output. The optimized Boolean logic representation achieves more than 24% reduction in area overhead compared to the unoptimized BNN representation, while achieving close to 98% modeling accuracy. We also validate the derived combinational circuit representation on Xilinx Artix-7 FPGA platform.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3453688.3461484',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'SortCache: Intelligent Cache Management for Accelerating Sparse Data Workloads',\n",
       "  'authors': \"['Sriseshan Srikanth', 'Anirudh Jain', 'Thomas M. Conte', 'Erik P. Debenedictis', 'Jeanine Cook']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Architecture and Code Optimization',\n",
       "  'abstract': 'Sparse data applications have irregular access patterns that stymie modern memory architectures. Although hyper-sparse workloads have received considerable attention in the past, moderately-sparse workloads prevalent in machine learning applications, graph processing and HPC have not. Where the former can bypass the cache hierarchy, the latter fit in the cache. This article makes the observation that intelligent, near-processor cache management can improve bandwidth utilization for data-irregular accesses, thereby accelerating moderately-sparse workloads. We propose SortCache, a processor-centric approach to accelerating sparse workloads by introducing accelerators that leverage the on-chip cache subsystem, with minimal programmer intervention.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3473332',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Using Auto-Encoder Neural Networks for Memory Fault Tolerance in Gesture Recognition System',\n",
       "  'authors': \"['Bo ZHANG', 'Lei ZHANG', 'Mojun WU', 'Yan WANG']\",\n",
       "  'date': 'March 2021',\n",
       "  'source': \"ICMAI '21: Proceedings of the 2021 6th International Conference on Mathematics and Artificial Intelligence\",\n",
       "  'abstract': 'Data faults and retention characteristics in memories induce inaccuracy and failure in conventional electronic systems. While intelligent hardware which using AI algorithms can tolerate these faults with the advantage of neural network. This paper proposes using auto-encoder (AE) neural networks for memory fault tolerance in gesture recognition system based on RF sensors. This paper models the data faults of memories with defects or operating in ultra-low power state by a binary-type noise distribution. Then the model is used to test the effect of AE neural network in gesture recognition algorithm. Experimental results show AE neural network compress and extract useful features from noisy RF images, and higher gesture recognition accuracy is achieved based on these features. The algorithm achieves a recognition accuracy of 93% considering 20% bit level faults in RF images. The purpose of this method is to reduce the power consumption and improve the yield of the embedded RF based gesture recognition chip.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3460569.3460571',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'PLD: fast FPGA compilation to make reconfigurable acceleration compatible with modern incremental refinement software development',\n",
       "  'authors': \"['Yuanlong Xiao', 'Eric Micallef', 'Andrew Butt', 'Matthew Hofmann', 'Marc Alston', 'Matthew Goldsmith', 'Andrew Merczynski-Hait', 'André DeHon']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'FPGA-based accelerators are demonstrating significant absolute performance and energy efficiency compared with general-purpose CPUs. While FPGA computations can now be described in standard, programming languages, like C, development for FPGAs accelerators remains tedious and inaccessible to modern software engineers. Slow compiles (potentially taking tens of hours) inhibit the rapid, incremental refinement of designs that is the hallmark of modern software engineering. To address this issue, we introduce separate compilation and linkage into the FPGA design flow, providing faster design turns more familiar to software development. To realize this flow, we provide abstractions, compiler options, and compiler flow that allow the same C source code to be compiled to processor cores in seconds and to FPGA regions in minutes, providing the missing -O0 and -O1 options familiar in software development. This raises the FPGA programming level and standardizes the programming experience, bringing FPGA-based accelerators into a more familiar software platform ecosystem for software engineers.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503222.3507740',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'BenQ: <u>ben</u>chmarking automated quantization on deep neural network accelerators',\n",
       "  'authors': \"['Zheng Wei', 'Xingjun Zhang', 'Jingbo Li', 'Zeyu Ji', 'Jia Wei']\",\n",
       "  'date': 'March 2022',\n",
       "  'source': \"DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe\",\n",
       "  'abstract': 'Hardware-aware automated quantization promises to unlock an entirely new algorithm-hardware co-design paradigm for efficiently accelerating deep neural network (DNN) inference by incorporating the hardware cost into the reinforcement learning (RL) -based quantization strategy search process. Existing works usually design an automated quantization algorithm targeting one hardware accelerator with a device-specific performance model or pre-collected data. However, determining the hardware cost is non-trivial for algorithm experts due to their lack of cross-disciplinary knowledge in computer architecture, compiler, and physical chip design. Such a barrier limits reproducibility and fair comparison. Moreover, it is notoriously challenging to interpret the results due to the lack of quantitative metrics. To this end, we first propose BenQ, which includes various RL-based automated quantization algorithms with aligned settings and encapsulates two off-the-shelf performance predictors with standard OpenAI Gym API. Then, we leverage cosine similarity and manhattan distance to interpret the similarity between the searched policies. The experiments show that different automated quantization algorithms can achieve near equivalent optimal trade-offs because of the high similarity between the searched policies, which provides insights for revisiting the innovations in automated quantization algorithms.',\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3539845.3540187',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Achieving crash consistency by employing persistent L1 cache',\n",
       "  'authors': \"['Akshay Krishna Ramanathan', 'Sara Mahdizadeh Shahri', 'Yi Xiao', 'Vijaykrishnan Narayanan']\",\n",
       "  'date': 'March 2022',\n",
       "  'source': \"DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe\",\n",
       "  'abstract': 'Emerging non-volatile memory technologies promise the opportunity for maintaining persistent data in memory. However, providing crash-consistency in such systems can be costly as any update to the persistent data has to reach the persistent domain in a specific order, imposing high overhead. Prior works, proposed solutions both in software (SW) and hardware (HW) to address this problem but fall short to remove this overhead completely. In this work, we propose Non-Volatile Cache (NVC) architecture design that employs a hybrid volatile, non-volatile memory cell employing monolithic 3D and Ferroelectric technology in L1 data cache to guarantee crash consistency with almost no performance overhead. We show that NVC achieves up to 5.1x speedup over state-of-the-art (SOTA) SW undo logging and 11% improvement over SOTA HW solution without yielding the conventional architecture, while incurring 7% hardware overhead.',\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3539845.3540172',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Reconfigurable Framework for Resilient Semantic Segmentation for Space Applications',\n",
       "  'authors': \"['Sebastian Sabogal', 'Alan George', 'Gary Crum']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Reconfigurable Technology and Systems',\n",
       "  'abstract': 'Deep learning (DL) presents new opportunities for enabling spacecraft autonomy, onboard analysis, and intelligent applications for space missions. However, DL applications are computationally intensive and often infeasible to deploy on radiation-hardened (rad-hard) processors, which traditionally harness a fraction of the computational capability of their commercial-off-the-shelf counterparts. Commercial FPGAs and system-on-chips present numerous architectural advantages and provide the computation capabilities to enable onboard DL applications; however, these devices are highly susceptible to radiation-induced single-event effects (SEEs) that can degrade the dependability of DL applications. In this article, we propose Reconfigurable ConvNet (RECON), a reconfigurable acceleration framework for dependable, high-performance semantic segmentation for space applications. In RECON, we propose both selective and adaptive approaches to enable efficient SEE mitigation. In our selective approach, control-flow parts are selectively protected by triple-modular redundancy to minimize SEE-induced hangs, and in our adaptive approach, partial reconfiguration is used to adapt the mitigation of dataflow parts in response to a dynamic radiation environment. Combined, both approaches enable RECON to maximize system performability subject to mission availability constraints. We perform fault injection and neutron irradiation to observe the susceptibility of RECON and use dependability modeling to evaluate RECON in various orbital case studies to demonstrate a 1.5–3.0× performability improvement in both performance and energy efficiency compared to static approaches.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3472770',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Microphone array backscatter: an application-driven design for lightweight spatial sound recording over the air',\n",
       "  'authors': \"['Jia Zhao', 'Wei Gong', 'Jiangchuan Liu']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"MobiCom '21: Proceedings of the 27th Annual International Conference on Mobile Computing and Networking\",\n",
       "  'abstract': 'Modern acoustic wearables with microphone arrays are promising to offer rich experience (e.g., 360° sound and acoustic imaging) to consumers. Realtime multi-track audio streaming with precise synchronization however poses significant challenges to the existing wireless microphone array designs that depend on complex digital synchronization as well as bulky and power-hungry hardware. This paper presents a novel microphone array sensor architecture that enables synchronous concurrent transmission of multitrack audio signals using analog backscatter communication. We develop novel Pulse Position Modulation (PPM) and Differential Pulse Position Modulation (DPPM) baseband circuits that can generate a spectral-efficient, time-multiplexing, and multi-track-synchronous baseband signal for backscattering. Its lightweight analog synchronization supports parallel multimedia signals without using any ADCs, DSPs, codecs and RF transceivers, hence largely reducing the complexity, latency, and power consumption. To further enhance self-sustainability, we also design an energy harvester that can extract energy from both sound and RF. We have built a microphone array backscatter sensor prototype using an FPGA, discrete components, and analog devices. Our experiments demonstrate a communication range (sensor-to-reader) of up to 28 meters for 8 audio tracks, and an equivalent throughput of up to 6.4 Mbps with a sample rate over 48KHz. Our sensor achieves 87.4μs of streaming latency for 4 tracks, which is 650x improvement as compared with digital solutions. ASIC design results show that it consumes as low as 175.2μW of power. Three sample applications including an acoustic imaging system, a beamform filter, and a voice control system, all built with our phased-array microphone, further demonstrate the applicability of our design.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3447993.3483265',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'LIBSHALOM: optimizing small and irregular-shaped matrix multiplications on ARMv8 multi-cores',\n",
       "  'authors': \"['Weiling Yang', 'Jianbin Fang', 'Dezun Dong', 'Xing Su', 'Zheng Wang']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"SC '21: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis\",\n",
       "  'abstract': 'General Matrix Multiplication (GEMM) is a key subroutine in highperformance computing. While the mainstream linear algebra libraries can deliver high performance on large and regular-shaped GEMM, they are inadequate for optimizing small and irregular-shaped GEMMs, which are commonly seen in new HPC applications. Some of the recent works in this direction have made promising progress on x86 architectures and GPUs but still leave much room for improvement on emerging HPC hardware built upon the ARMv8 architecture. We present LibShalom, an open-source library for optimizing small and irregular-shaped GEMMs, explicitly targeting the ARMv8 architecture. LibShalom builds upon the classical Goto algorithm but tailors it to minimize the expensive memory accessing overhead for data packing and processing small matrices. It uses analytic methods to determine GEMM kernel optimization parameters, enhancing the computation and parallelization efficiency of the GEMM kernels. We evaluate LibShalom by applying it to three ARMv8 multi-core architectures and comparing it against five mainstream linear algebra libraries. Experimental results show that LibShalom can consistently outperform existing solutions across GEMM workloads and hardware architectures.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3458817.3476217',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Research on Mixed-Precision Quantization and Fault-Tolerant of Deep Neural Networks',\n",
       "  'authors': \"['Zhaoxin Wang', 'Jing Wang', 'Kun Qian']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"CSAE '21: Proceedings of the 5th International Conference on Computer Science and Application Engineering\",\n",
       "  'abstract': 'As deep neural networks become more and more common in mission-critical applications, such as smart medical care, drones, and autonomous driving, ensuring their reliable operation becomes critical. The data in the hardware memory is susceptible to bit-flip due to external factors, which leads to a decrease in the inference accuracy of the deep neural network deployed on the hardware. We solve this problem from the perspective of the deep neural network itself, We use a reinforcement learning algorithm to search for the optimal bit width for the weights of each layer of the deep neural network. According to this bit width strategy, the deep neural network is quantified, which maximizes the limitation of data fluctuations caused by bit-flip and improves the fault-tolerance of the neural network. The fault-tolerance of the network model compared with the original model, the solution proposed in this paper improves the fault-tolerance of LeNet5 model by 8.5x , the fault tolerance of MobileNetV2 model by 15.6x , the fault tolerance of VGG16 model by 14.5x , and the accuracy decreases negligibly.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3487075.3487135',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'An Efficient Parallelized Huffman Decoding PE',\n",
       "  'authors': \"['Yan Wu', 'Ruizhen Wu', 'Heng Ma', 'Lin Wang', 'Jianlong Su']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ITCC '21: Proceedings of the 2021 3rd International Conference on Information Technology and Computer Communications\",\n",
       "  'abstract': 'An efficient parallelized Huffman decoding PE is proposed in this paper. The classic Huffman decoding algorithm is optimized first in parallel processing and then four hardware modules are designed based on that. The proposed PE can decode different numbers of streams with various compression ratios. The hardware architecture is fully verified at RTL level and synthesized with GF 12nm technology lib. The simulation shows a better decoding performance especially with wide data width and high compression ratio settings.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3473465.3473485',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'BEACON: BEst Approximations for Complete BehaviOral HeterogeNeous SoCs',\n",
       "  'authors': \"['Prattay Chowdhury', 'Benjamin Carrion Schafer']\",\n",
       "  'date': 'July 2021',\n",
       "  'source': \"ISLPED '21: Proceedings of the ACM/IEEE International Symposium on Low Power Electronics and Design\",\n",
       "  'abstract': \"Approximate computing has shown to be an effective approach to generate smaller and more power-efficient circuits by trading the accuracy of the circuit vs. area/power. So far, most work on approximate computing has focused on specific components within a system. This severely limits the approximation potential as most Integrated Circuits (ICs) are now complex heterogeneous systems. This paper investigates if lower-power designs can be found through mixing approximations across the different components in the SoC as opposed to only aggressively approximating a single component. The main hypothesis is that some approximations amplify across the system, while others tend to cancel each other out, thus, allowing to maximize the power savings while meeting the given maximum error threshold. In this work, we consider the Analog-to-Digital Converter (ADC), CPU, hardware accelerators, and interconnect between all these components. Moreover, to quickly measure the effect of different approximation mixes, we have developed a framework that allows generating complete SoCs at the behavioral level through a bus generator and a library of synthesizable bus interfaces. This enables the use of fast simulation models (transaction and cycle-accurate) to accurately measure the error at the system's output while measuring the benefit in terms of area or energy reduction of different mixes of approximations. Experimental results show that taking into account the entire system as oppose to only individual components leads to an additional average energy savings of 14% to 17% for different maximum error thresholds and the best case up to 39%.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ISLPED52811.2021.9502479',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Jamais vu: thwarting microarchitectural replay attacks',\n",
       "  'authors': \"['Dimitrios Skarlatos', 'Zirui Neil Zhao', 'Riccardo Paccagnella', 'Christopher W. Fletcher', 'Josep Torrellas']\",\n",
       "  'date': 'April 2021',\n",
       "  'source': \"ASPLOS '21: Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'Microarchitectural Replay Attacks (MRAs) enable an attacker to eliminate the measurement variation in potentially any microarchitectural side channel—even if the victim instruction is supposed to execute only once. In an MRA, the attacker forces pipeline flushes in order to repeatedly re-execute the victim instruction and denoise the channel. MRAs are not limited to transient execution attacks: the replayed victim can be an instruction that will eventually retire. This paper presents the first technique to thwart MRAs. The technique, called Jamais Vu, detects when an instruction is squashed. Then, as the instruction is re-inserted into the pipeline, Jamais Vu automatically places a fence before it to prevent the attacker from squashing it again. This paper presents several Jamais Vu designs that offer different trade-offs between security, execution overhead, and implementation complexity. One design, called Epoch-Loop-Rem, effectively mitigates MRAs, has an average execution time overhead of 13.8% in benign executions, and only needs counting Bloom filters. An even simpler design, called Clear-on-Retire, has an average execution time overhead of only 2.9%, although it is less secure.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3445814.3446716',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Accelerating Deep Neuroevolution on Distributed FPGAs for Reinforcement Learning Problems',\n",
       "  'authors': \"['Alexis Asseman', 'Nicolas Antoine', 'Ahmet S. Ozcan']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Journal on Emerging Technologies in Computing Systems',\n",
       "  'abstract': 'Reinforcement learning, augmented by the representational power of deep neural networks, has shown promising results on high-dimensional problems, such as game playing and robotic control. However, the sequential nature of these problems poses a fundamental challenge for computational efficiency. Recently, alternative approaches such as evolutionary strategies and deep neuroevolution demonstrated competitive results with faster training time on distributed CPU cores. Here we report record training times (running at about 1 million frames per second) for Atari 2600 games using deep neuroevolution implemented on distributed FPGAs. Combined hardware implementation of the game console, image preprocessing and the neural network in an optimized pipeline, multiplied with the system level parallelism enabled the acceleration. These results are the first application demonstration on the IBM Neural Computer, which is a custom designed system that consists of 432 Xilinx FPGAs interconnected in a 3D mesh network topology. In addition to high performance, experiments also showed improvement in accuracy for all games compared to the CPU implementation of the same algorithm.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3425500',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Automatic Sublining for Efficient Sparse Memory Accesses',\n",
       "  'authors': \"['Wim Heirman', 'Stijn Eyerman', 'Kristof Du Bois', 'Ibrahim Hur']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Architecture and Code Optimization',\n",
       "  'abstract': 'Sparse memory accesses, which are scattered accesses to single elements of a large data structure, are a challenge for current processor architectures. Their lack of spatial and temporal locality and their irregularity makes caches and traditional stream prefetchers useless. Furthermore, performing standard caching and prefetching on sparse accesses wastes precious memory bandwidth and thrashes caches, deteriorating performance for regular accesses. Bypassing prefetchers and caches for sparse accesses, and fetching only a single element (e.g., 8\\xa0B) from main memory (subline access), can solve these issues.Deciding which accesses to handle as sparse accesses and which as regular cached accesses, is a challenging task, with a large potential impact on performance. Not only is performance reduced by treating sparse accesses as regular accesses, not caching accesses that do have locality also negatively impacts performance by significantly increasing their latency and bandwidth consumption. Furthermore, this decision depends on the dynamic environment, such as input set characteristics and system load, making a static decision by the programmer or compiler suboptimal.We propose the Instruction Spatial Locality Estimator (ISLE), a hardware detector that finds instructions that access isolated words in a sea of unused data. These sparse accesses are dynamically converted into uncached subline accesses, while keeping regular accesses cached. ISLE does not require modifying source code or binaries, and adapts automatically to a changing environment (input data, available bandwidth, etc.). We apply ISLE to a graph analytics processor running sparse graph workloads, and show that ISLE outperforms the performance of no subline accesses, manual sublining, and prior work on detecting sparse accesses.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3452141',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Terminator: A Secure Coprocessor to Accelerate Real-Time AntiViruses Using Inspection Breakpoints',\n",
       "  'authors': \"['Marcus Botacin', 'Francis B. Moreira', 'Philippe O. A. Navaux', 'André Grégio', 'Marco A. Z. Alves']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Privacy and Security',\n",
       "  'abstract': 'AntiViruses (AVs) are essential to face the myriad of malware threatening Internet users. AVs operate in two modes: on-demand checks and real-time verification. Software-based real-time AVs intercept system and function calls to execute AV’s inspection routines, resulting in significant performance penalties as the monitoring code runs among the suspicious code. Simultaneously, dark silicon problems push the industry to add more specialized accelerators inside the processor to mitigate these integration problems. In this article, we propose Terminator, an AV-specific coprocessor to assist software AVs by outsourcing their matching procedures to the hardware, thus saving CPU cycles and mitigating performance degradation. We designed Terminator \\xa0 to be flexible and compatible with existing AVs by using YARA and ClamAVrules. Our experiments show that our approach can save up to 70 million CPU cycles per rule when outsourcing on-demand checks for matching typical, unmodified YARA rules against a dataset of 30 thousand in-the-wild malware samples. Our proposal eliminates the AV’s need for blocking the CPU to perform full system checks, which can now occur in parallel. We also designed a new inspection breakpoint mechanism that signals to the coprocessor the beginning of a monitored region, allowing it to scan the regions in parallel with their execution. Overall, our mechanism mitigated up to 44% of the overhead imposed to execute and monitor the SPEC benchmark applications in the most challenging scenario.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3494535',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Software-Managed Read and Write Wear-Leveling for Non-Volatile Main Memory',\n",
       "  'authors': \"['Christian Hakert', 'Kuan-Hsun Chen', 'Horst Schirmeier', 'Lars Bauer', 'Paul R. Genssler', 'Georg von der Brüggen', 'Hussam Amrouch', 'Jörg Henkel', 'Jian-Jia Chen']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Embedded Computing Systems',\n",
       "  'abstract': 'In-memory wear-leveling has become an important research field for emerging non-volatile main memories over the past years. Many approaches in the literature perform wear-leveling by making use of special hardware. Since most non-volatile memories only wear out from write accesses, the proposed approaches in the literature also usually try to spread write accesses widely over the entire memory space. Some non-volatile memories, however, also wear out from read accesses, because every read causes a consecutive write access. Software-based solutions only operate from the application or kernel level, where read and write accesses are realized with different instructions and semantics. Therefore different mechanisms are required to handle reads and writes on the software level. First, we design a method to approximate read and write accesses to the memory to allow aging aware coarse-grained wear-leveling in the absence of special hardware, providing the age information. Second, we provide specific solutions to resolve access hot-spots within the compiled program code (text segment) and on the application stack. In our evaluation, we estimate the cell age by counting the total amount of accesses per cell. The results show that employing all our methods improves the memory lifetime by up to a factor of 955×.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3483839',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'HyperTester: High-Performance Network Testing Driven by Programmable Switches',\n",
       "  'authors': \"['Dai Zhang', 'Yu Zhou', 'Zhaowei Xi', 'Yangyang Wang', 'Mingwei Xu', 'Jianping Wu']\",\n",
       "  'date': 'None',\n",
       "  'source': 'IEEE/ACM Transactions on Networking',\n",
       "  'abstract': 'Modern network devices and systems are raising higher requirements on network testers that are regularly used to evaluate performance and assess correctness. These requirements include high scale, high accuracy, flexibility and low cost, which existing testers cannot fulfill at the same time. In this paper, we propose HyperTester, a network tester leveraging new-generation programmable switches and achieving all of the above goals simultaneously. Programmable switches are born with features like high throughput and linerate, deterministic processing pipelines and nanosecond-level hardware timestamps, the P4 programming model as well as comparable pricing with commodity servers, but they come with limited programmability and memory resources. HyperTester uses template-based packet generation to overcome the limitations of the switch ASIC in programmability and designs a stateless connection mechanism as well as counter-based state compression algorithms to overcome the memory resource constraints in the data plane. We have implemented HyperTester on Tofino, and the evaluations on the hardware testbed show that HyperTester supports high-scale packet generation (more than 1.6Tbps) and achieves highly accurate rate control and timestamping. We demonstrate that programmable switches can be potential and attractive targets for realizing network testers.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/TNET.2021.3077652',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'AnaCoNGA: analytical HW-CNN co-design using nested genetic algorithms',\n",
       "  'authors': \"['Nael Fasfous', 'Manoj Rohit Vemparala', 'Alexander Frickenstein', 'Emanuele Valpreda', 'Driton Salihu', 'Julian Höfer', 'Anmol Singh', 'Naveen-Shankar Nagaraja', 'Hans-Joerg Voegel', 'Nguyen Anh Vu Doan', 'Maurizio Martina', 'Juergen Becker', 'Walter Stechele']\",\n",
       "  'date': 'March 2022',\n",
       "  'source': \"DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe\",\n",
       "  'abstract': 'We present AnaCoNGA, an analytical co-design methodology, which enables two genetic algorithms to evaluate the fitness of design decisions on layer-wise quantization of a neural network and hardware (HW) resource allocation. We embed a hardware architecture search (HAS) algorithm into a quantization strategy search (QSS) algorithm to evaluate the hardware design Pareto-front of each considered quantization strategy. We harness the speed and flexibility of analytical HW-modeling to enable parallel HW-CNN co-design. With this approach, the QSS is focused on seeking high-accuracy quantization strategies which are guaranteed to have efficient hardware designs at the end of the search. Through AnaCoNGA, we improve the accuracy by 2.88 p.p. with respect to a uniform 2-bit ResNet20 on CIFAR-10, and achieve a 35% and 37% improvement in latency and DRAM accesses, while reducing LUT and BRAM resources by 9% and 59% respectively, when compared to a standard edge variant of the accelerator. The nested genetic algorithm formulation also reduces the search time by 51% compared to an equivalent, sequential co-design formulation.',\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3539845.3539907',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Ookami: Deployment and Initial Experiences',\n",
       "  'authors': \"['Andrew Burford', 'Alan Calder', 'David Carlson', 'Barbara Chapman', 'Firat Coskun', 'Tony Curtis', 'Catherine Feldman', 'Robert Harrison', 'Yan Kang', 'Benjamin Michalowicz', 'Eric Raut', 'Eva Siegmann', 'Daniel Wood', 'Robert DeLeon', 'Mathew Jones', 'Nikolay Simakov', 'Joseph White', 'Dossay Oryspayev']\",\n",
       "  'date': 'July 2021',\n",
       "  'source': \"PEARC '21: Practice and Experience in Advanced Research Computing\",\n",
       "  'abstract': 'Ookami\\xa0[3] is a computer technology testbed supported by the United States National Science Foundation. It provides researchers with access to the A64FX processor developed by Fujitsu\\xa0[17] in collaboration with RIKΞN\\xa0\\xa0[35, 37] for the Japanese path to exascale computing, as deployed in Fugaku\\xa0[36], the fastest computer in the world [34]. By focusing on crucial architectural details, the ARM-based, multi-core, 512-bit SIMD-vector processor with ultrahigh-bandwidth memory promises to retain familiar and successful programming models while achieving very high performance for a wide range of applications. We review relevant technology and system details, and the main body of the paper focuses on initial experiences with the hardware and software ecosystem for micro-benchmarks, mini-apps, and full applications, and starts to answer questions about where such technologies fit into the NSF ecosystem.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3437359.3465578',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Interpreting Intermediate Convolutional Layers of Generative CNNs Trained on Waveforms',\n",
       "  'authors': \"['Gašper Beguš', 'Alan Zhou']\",\n",
       "  'date': 'None',\n",
       "  'source': 'IEEE/ACM Transactions on Audio, Speech and Language Processing',\n",
       "  'abstract': 'This paper presents a technique to interpret and visualize intermediate layers in generative CNNs trained on raw speech data in an unsupervised manner. We argue that averaging over feature maps after ReLU activation in each transpose convolutional layer yields interpretable time-series data. This technique allows for acoustic analysis of intermediate layers that parallels the acoustic analysis of human speech data: we can extract F0, intensity, duration, formants, and other acoustic properties from intermediate layers in order to test where and how CNNs encode various types of information. We further combine this technique with linear interpolation of a model&#x0027;s latent space to show a causal relationship between individual variables in the latent space and activations in a model&#x0027;s intermediate convolutional layers. In particular, observing the causal effect between linear interpolation and the resulting changes in intermediate layers can reveal how individual latent variables get transformed into spikes in activation in intermediate layers. We train and probe internal representations of two models &#x2014; a bare WaveGAN architecture and a ciwGAN extension which forces the Generator to output informative data and results in the emergence of linguistically meaningful representations. Interpretation and visualization is performed for three basic acoustic properties of speech: periodic vibration (corresponding to vowels), aperiodic noise vibration (corresponding to fricatives), and silence (corresponding to stops). The proposal also allows testing of higher-level morphophonological alternations such as reduplication (copying). In short, using the proposed technique, we can analyze how linguistically meaningful units in speech get encoded in each convolutional layer of a generative neural network.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/TASLP.2022.3209938',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'The Hera framework for fault-tolerant sensor fusion with Erlang and GRiSP on an IoT network',\n",
       "  'authors': \"['Sébastien Kalbusch', 'Vincent Verpoten', 'Peter Van Roy']\",\n",
       "  'date': 'August 2021',\n",
       "  'source': 'Erlang 2021: Proceedings of the 20th ACM SIGPLAN International Workshop on Erlang',\n",
       "  'abstract': 'Classical sensor fusion approaches require to work directly with the hardware and involve a lot of low-level programming, which is not suited for reliable and user-friendly sensor fusion for Internet of Things (IoT) applications. In this paper, we propose and analyze Hera, a Kalman filter-based sensor fusion framework for Erlang. Hera offers a high-level approach for asynchronous and fault-tolerant sensor fusion directly at the edge of an IoT network. We use the GRiSP-Base board, a low-cost platform specially designed for Erlang and to avoid soldering or dropping down to C. We emphasize on the importance of performing all the computations directly at the sensor-equipped devices themselves, completely removing the cloud necessity. We show that we can perform sensor fusion for position and orientation tracking at a high level of abstraction and with the strong guarantee that the system will keep running as long as one GRiSP board is alive. With Hera, the implementation effort is significantly reduced which makes it an excellent candidate for IoT prototyping and education in the field of sensor fusion.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3471871.3472962',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Best Approximate Quantum Compiling Problems',\n",
       "  'authors': \"['Liam Madden', 'Andrea Simonetto']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Quantum Computing',\n",
       "  'abstract': 'We study the problem of finding the best approximate circuit that is the closest (in some pertinent metric) to a target circuit, and which satisfies a number of hardware constraints, like gate alphabet and connectivity. We look at the problem in the CNOT+rotation gate set from a mathematical programming standpoint, offering contributions both in terms of understanding the mathematics of the problem and its efficient solution. Among the results that we present, we are able to derive a 14-CNOT 4-qubit Toffoli decomposition from scratch, and show that the Quantum Shannon Decomposition can be compressed by a factor of two without practical loss of fidelity.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3505181',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Sense Your Power: The ECO Approach to Energy Awareness for IoT Devices',\n",
       "  'authors': \"['Michel Rottleuthner', 'Thomas C. Schmidt', 'Matthias Wählisch']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Embedded Computing Systems',\n",
       "  'abstract': 'Energy-constrained sensor nodes can adaptively optimize their energy consumption if a continuous measurement is provided. This is of particular importance in scenarios of high dynamics such as with energy harvesting. Still, self-measuring of power consumption at reasonable cost and complexity is unavailable as a generic system service.In this article, we present ECO, a hardware-software co-design that adds autonomous energy management capabilities to a large class of low-end IoT devices. ECO consists of a highly portable hardware shield built from inexpensive commodity components and software integrated into the RIOT operating system. RIOT supports more than 200 popular microcontrollers. Leveraging this flexibility, we assembled a variety of sensor nodes to evaluate key performance properties for different device classes. An overview and comparison with related work shows how ECO fills the gap of in situ power attribution transparently for consumers and how it improves over existing solutions. We also report about two different real-world field trials, which validate our solution for long-term production use.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3441643',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Zero-Shot Normalization Driven Multi-Speaker Text to Speech Synthesis',\n",
       "  'authors': \"['Neeraj Kumar', 'Ankur Narang', 'Brejesh Lall']\",\n",
       "  'date': 'None',\n",
       "  'source': 'IEEE/ACM Transactions on Audio, Speech and Language Processing',\n",
       "  'abstract': 'Text-to-speech (TTS) systems are designed to synthesize natural and expressive speech, adapt to an unseen voice, and capture the speaking style of an unseen speaker by converting text into speech. The introduction of an unseen speaker&#x2019;s speaking style into a TTS system offers a wide range of application scenarios, including personal assistant, news broadcast, and audio navigation, among others. The style of the speech varies from person to person and every person exhibits his or her style of speaking that is determined by the language, demography, culture and other factors. Style is best captured by the prosody of a signal. It is an ongoing research area with numerous real-world applications that produces high-quality multi-speaker voice synthesis while taking into account prosody and in a zero-shot manner. Despite the fact that several efforts have been made in this area, it continues to be an interesting and difficult topic to solve. In this paper, we present a novel zero-shot multi-speaker speech synthesis approach (ZSM-SS) that leverages the normalization architecture and speaker encoder with non-autoregressive multi-head attention driven encoder-decoder architecture. Given an input text and a reference speech sample of an unseen person, ZSM-SS can generate speech in that person&#x2019;s style in a zero-shot manner. Additionally, we demonstrate how the affine parameters of normalization help in capturing the prosodic features such as energy and fundamental frequency in a disentangled fashion and can be used to generate morphed speech output. We generate the 256 dimensional speaker embedding using a speaker encoder based on wav2vec2.0 based architecture. We demonstrate the efficacy of our proposed architecture on multi-speaker VCTK [1] and LibriTTS [2] datasets, using visualization of hessian of proposed model, multiple quantitative metrics that measure generated speech distortion and MOS, along with speaker embedding analysis of the proposed speaker encoder model.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/TASLP.2022.3169634',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Integrated Power Signature Generation Circuit for IoT Abnormality Detection',\n",
       "  'authors': \"['David Thompson', 'Haibo Wang']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Journal on Emerging Technologies in Computing Systems',\n",
       "  'abstract': 'This work presents a methodology to monitor the power signature of IoT devices for detecting operation abnormality. It does not require bulky measurement equipment thanks to the proposed power signature generation circuit which can be integrated into LDO voltage regulators. The proposed circuit is implemented using a 130 nm CMOS technology and simulated with power trace measured from a wireless sensor. It shows the generated power signature accurately reflects the power consumption and can be used to distinguish different operation conditions, such as wireless transmission levels, data sampling rates and microcontroller UART communications.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3460476',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Parallelizing Intra-Window Join on Multicores: An Experimental Study',\n",
       "  'authors': \"['Shuhao Zhang', 'Yancan Mao', 'Jiong He', 'Philipp M. Grulich', 'Steffen Zeuch', 'Bingsheng He', 'Richard T. B. Ma', 'Volker Markl']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data\",\n",
       "  'abstract': 'The intra-window join (IaWJ), i.e., joining two input streams over a single window, is a core operation in modern stream processing applications. This paper presents the first comprehensive study on parallelizing the IaWJ on modern multicore architectures. In particular, we classify IaWJ algorithms into lazy and eager execution approaches. For each approach, there are further design aspects to consider, including different join methods and partitioning schemes, leading to a large design space. Our results show that none of the algorithms always performs the best, and the choice of the most performant algorithm depends on: (i) workload characteristics, (ii) application requirements, and (iii) hardware architectures. Based on the evaluation results, we propose a decision tree that can guide the selection of an appropriate algorithm.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3448016.3452793',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Quantum Annealing versus Digital Computing: An Experimental Comparison',\n",
       "  'authors': \"['Michael Jünger', 'Elisabeth Lobe', 'Petra Mutzel', 'Gerhard Reinelt', 'Franz Rendl', 'Giovanni Rinaldi', 'Tobias Stollenwerk']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Journal of Experimental Algorithmics',\n",
       "  'abstract': 'Quantum annealing is getting increasing attention in combinatorial optimization. The quantum processing unit by D-Wave is constructed to approximately solve Ising models on so-called Chimera graphs. Ising models are equivalent to quadratic unconstrained binary optimization (QUBO) problems and maximum cut problems on the associated graphs. We have tailored branch-and-cut as well as semidefinite programming algorithms for solving Ising models for Chimera graphs to provable optimality and use the strength of these approaches for comparing our solution values to those obtained on the current quantum annealing machine, D-Wave 2000Q. This allows for the assessment of the quality of solutions produced by the D-Wave hardware. In addition, we also evaluate the performance of a heuristic by Selby. It has been a matter of discussion in the literature how well the D-Wave hardware performs at its native task, and our experiments shed some more light on this issue. In particular, we examine how reliably the D-Wave computer can deliver true optimum solutions and present some surprising results.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3459606',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Deep Optimization of Parametric IIR Filters for Audio Equalization',\n",
       "  'authors': \"['Giovanni Pepe', 'Leonardo Gabrielli', 'Stefano Squartini', 'Carlo Tripodi', 'Nicolò Strozzi']\",\n",
       "  'date': 'None',\n",
       "  'source': 'IEEE/ACM Transactions on Audio, Speech and Language Processing',\n",
       "  'abstract': 'This paper describes a novel Deep Learning method for the design of IIR parametric filters for automatic multipoint audio equalization, that is the task of improving the sound quality of a listening environment at multiple listening points employing multiple loudspeakers. The filters are designed to approximate the inverse of the RIR and achieve almost flat magnitude response. A simple and effective neural architecture, named BiasNet, is proposed to determine the IIR equalizer parameters. This novel architecture is conceived for optimization and, as such, is able to produce optimal IIR equalizer parameters at its output, after training, with no input required. In absence of input, the presence of learnable non-zero bias terms ensures that the network works properly. An output scaling method is used to obtain accurate tuning of the IIR filters center frequency, quality factor and gain. All layers involved in the proposed method are shown to be differentiable, allowing backpropagation to optimize the network weights and achieve, after a number of training iterations, the optimal output according to a given RIR. The parameters are optimized with respect to a loss function based on a spectral distance between the measured and desired magnitude response, and a regularization term is used to keep the same microphone-loudspeaker energy balance after equalization. Two experimental scenarios are employed, a room and a car cabin, with several loudspeakers. The performance of the proposed method improves over the baseline techniques and achieves an almost flat band at a lower computational cost.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/TASLP.2022.3155289',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'SOURCE: a Freesound Community Music Sampler',\n",
       "  'authors': \"['Frederic Font']\",\n",
       "  'date': 'September 2021',\n",
       "  'source': \"AM '21: Proceedings of the 16th International Audio Mostly Conference\",\n",
       "  'abstract': 'SOURCE is an open-source hardware music sampler powered by Freesound’s collection of near 500k Creative Commons sounds contributed by a community of thousands of people around the world. SOURCE provides a hardware interface with Freesound and implements different methods to search and load sounds into the sampler. We see SOURCE as a proof of concept device that can be used as and extendable base system on top of which further research on the interaction between hardware devices and online large sound collections can be carried out. This paper describes the architecture of SOURCE and the different ways in which it interacts with Freesound. Even though we have not carried out a formal evaluation of SOURCE our informal tests show that SOURCE can be successfully integrated into a live music performance setup, and influence the creative process in interesting ways by being able to quickly generate new rich sound palettes that would otherwise be difficult to create with traditional hardware music samplers.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3478384.3478388',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A Deep Learning Framework to Predict Routability for FPGA Circuit Placement',\n",
       "  'authors': \"['Abeer Al-Hyari', 'Hannah Szentimrey', 'Ahmed Shamli', 'Timothy Martin', 'Gary Gréwal', 'Shawki Areibi']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Reconfigurable Technology and Systems',\n",
       "  'abstract': 'The ability to accurately and efficiently estimate the routability of a circuit based on its placement is one of the most challenging and difficult tasks in the Field Programmable Gate Array (FPGA) flow. In this article, we present a novel, deep learning framework based on a Convolutional Neural Network (CNN) model for predicting the routability of a placement. Since the performance of the CNN model is strongly dependent on the hyper-parameters selected for the model, we perform an exhaustive parameter tuning that significantly improves the model’s performance and we also avoid overfitting the model. We also incorporate the deep learning model into a state-of-the-art placement tool and show how the model can be used to (1) avoid costly, but futile, place-and-route iterations, and (2) improve the placer’s ability to produce routable placements for hard-to-route circuits using feedback based on routability estimates generated by the proposed model. The model is trained and evaluated using over 26K placement images derived from 372 benchmarks supplied by Xilinx Inc. We also explore several opportunities to further improve the reliability of the predictions made by the proposed DLRoute technique by splitting the model into two separate deep learning models for (a) global and (b) detailed placement during the optimization process. Experimental results show that the proposed framework achieves a routability prediction accuracy of 97% while exhibiting runtimes of only a few milliseconds.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3465373',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'The Case for SIMDified Analytical Query Processing on GPUs',\n",
       "  'authors': \"['Johannes Fett', 'Annett Ungethüm', 'Dirk Habich', 'Wolfgang Lehner']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"DAMON '21: Proceedings of the 17th International Workshop on Data Management on New Hardware\",\n",
       "  'abstract': 'Data-level parallelism (DLP) is a heavily used hardware-driven parallelization technique to optimize the analytical query processing, especially in in-memory column stores. This kind of parallelism is characterized by executing essentially the same operation on different data elements simultaneously. Besides Single Instruction Multiple Data (SIMD) extensions on common x86-processors, GPUs also provide DLP but with a different execution model called Single Instruction Multiple Threads (SIMT), where multiple scalar threads are executed in a SIMD manner. Unfortunately, a complete GPU-specific implementation of all query operators has to be set up, since the state of the vectorized implementations cannot be ported from x86-processors to GPUs right now. To avoid this implementation effort, we present our vision to virtualize GPUs as virtual vector engines with software-defined SIMD instructions and to specialize hardware-oblivious vectorized operators to GPUs using our Template Vector Library (TVL) in this paper.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3465998.3466015',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'When wearable technology meets computing in future networks: a road ahead',\n",
       "  'authors': \"['Aleksandr Ometov', 'Olga Chukhno', 'Nadezhda Chukhno', 'Jari Nurmi', 'Elena Simona Lohan']\",\n",
       "  'date': 'May 2021',\n",
       "  'source': \"CF '21: Proceedings of the 18th ACM International Conference on Computing Frontiers\",\n",
       "  'abstract': \"Rapid technology advancement, economic growth, and industrialization have paved the way for developing a new niche of small body-worn personal devices, gathered together under a wearable-technology title. The triggers stimulated by end-users interest have introduced the first generation of mass-consumer wearables in just the past decade. Evidently, the trailblazing ones were not designed with strict energy-consumption restrictions in mind. Thus, wearable-computing-related research remained fragmented. Advanced and sophisticated batteries and communication technologies could be already procurable on devices. Additional solutions for efficient utilization of processing power are still a white spot on the wearable technology roadmap. A-WEAR EU project aims to enhance the understanding of how the superimposition of those technologies would improve wearable devices' energy efficiency, with the research area being far from saturation. We foresee enormous room for research as the Edge computing paradigm is emerging towards hand-held devices.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3457388.3458614',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'CO2\\xa0Meter: A do-it-yourself carbon dioxide measuring device for the classroom',\n",
       "  'authors': \"['Thomas Dey', 'Ingo Elsen', 'Alexander Ferrein', 'Tobias Frauenrath', 'Michael Reke', 'Stefan Schiffer']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"PETRA '21: Proceedings of the 14th PErvasive Technologies Related to Assistive Environments Conference\",\n",
       "  'abstract': 'In this paper we report on CO2\\xa0Meter, a do-it-yourself carbon dioxide measuring device for the classroom. Part of the current measures for dealing with the SARS-CoV-2 pandemic is proper ventilation in indoor settings. This is especially important in schools with students coming back to the classroom even with high incidents rates. Static ventilation patterns do not consider the individual situation for a particular class. Influencing factors like the type of activity, the physical structure or the room occupancy are not incorporated. Also, existing devices are rather expensive and often provide only limited information and only locally without any networking. This leaves the potential of analysing the situation across different settings untapped. Carbon dioxide level can be used as an indicator of air quality, in general, and of aerosol load in particular. Since, according to the latest findings, SARS-CoV-2 can be transmitted primarily in the form of aerosols, carbon dioxide may be used as a proxy for the risk of a virus infection. Hence, schools could improve the indoor air quality and potentially reduce the infection risk if they actually had measuring devices available in the classroom. Our device supports schools in ventilation and it allows for collecting data over the Internet to enable a detailed data analysis and model generation. First deployments in schools at different levels were received very positively. A pilot installation with a larger data collection and analysis is underway.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3453892.3462697',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Writing P4 compiler backend for packet processing engines',\n",
       "  'authors': \"['Balachandher Sambasivam', 'Maheswari Subramanian', 'Deb Chatterjee', 'Mallikarjuna Gouda', 'Sosutha Sethuramapandian', 'Yogender Singh Saroha']\",\n",
       "  'date': 'December 2021',\n",
       "  'source': \"ANCS '21: Proceedings of the Symposium on Architectures for Networking and Communications Systems\",\n",
       "  'abstract': 'The advent of P4 as a protocol-independent and platform-independent network packet processing language has revolutionized the way networks are designed and the way networking devices are programmed. There are few programmable devices, whether ASICs or FPGA-based devices, that are designed with P4 programmability as the end goal right from the beginning. As a consequence, although these packet processing engines are programmable, writing a P4 compiler for these targets requires overcoming some technical challenges. Our team has worked on a variety of packet processing pipelines in recent years, in this article, we are presenting some of these challenges as well as the solutions we found to work around them.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3493425.3502769',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Research on Full-Chip Programming for Sunway Heterogeneous Many-core Processor',\n",
       "  'authors': \"['Wei Wu', 'Hong Qian', 'Qi Zhu', 'Jue Wang', 'XingJian Fan']\",\n",
       "  'date': 'September 2021',\n",
       "  'source': \"WSSE '21: Proceedings of the 3rd World Symposium on Software Engineering\",\n",
       "  'abstract': \"Programming on many-core processors is a challenging task. It's a difficult topic to program and compile on heterogeneous many-core architectures in high-performance computing area. The bottom-level programming support on Sunway many-core processors is insufficient and can hardly satisfy the growing need in applications. This paper conducts an thread accelerated programming model and a multi-mode accelerating thread library on Sunway architectures. A fast thread group mode is proposed, which can reduce the consumption of thread spawn and join according to hardware's features. Compared to GPU, it achieves a speedup of 24.88. We also designs a globally sharing executing mode, which supports many-core acceleration programming from the view of full-chip. Evaluation results on the Parboil benchmark shows that, the average performance is 1.16 times as NVIDIA V100 GPU.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3488838.3488868',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'MIND: In-Network Memory Management for Disaggregated Data Centers',\n",
       "  'authors': \"['Seung-seob Lee', 'Yanpeng Yu', 'Yupeng Tang', 'Anurag Khandelwal', 'Lin Zhong', 'Abhishek Bhattacharjee']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"SOSP '21: Proceedings of the ACM SIGOPS 28th Symposium on Operating Systems Principles\",\n",
       "  'abstract': 'Memory disaggregation promises transparent elasticity, high resource utilization and hardware heterogeneity in data centers by physically separating memory and compute into network-attached resource \"blades\". However, existing designs achieve performance at the cost of resource elasticity, restricting memory sharing to a single compute blade to avoid costly memory coherence traffic over the network. In this work, we show that emerging programmable network switches can enable an efficient shared memory abstraction for disaggregated architectures by placing memory management logic in the network fabric. We find that centralizing memory management in the network permits bandwidth and latency-efficient realization of in-network cache coherence protocols, while programmable switch ASICs support other memory management logic at line-rate. We realize these insights into MIND1, an in-network memory management unit for rack-scale disaggregation. MIND enables transparent resource elasticity while matching the performance of prior memory disaggregation proposals for real-world workloads.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3477132.3483561',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Finding and Finessing Static Islands in Dynamically Scheduled Circuits',\n",
       "  'authors': \"['Jianyi Cheng', 'John Wickerson', 'George A. Constantinides']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"FPGA '22: Proceedings of the 2022 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays\",\n",
       "  'abstract': \"In high-level synthesis, scheduling is the process that determines the start time of each operation in hardware. A hardware design can be scheduled either at compile time (static), run time (dynamic), or both. Recent research has shown that combining dynamic and static scheduling can achieve high performance and small area. However, there is still a challenge to determine which part to schedule statically and which part dynamically. An inappropriate choice can lead to suboptimal design quality. This paper proposes a heuristic-driven approach to automatically determine 'static islands' - i.e., code regions that are amenable for static scheduling. Over a set of benchmarks where our approach is applicable, we show that our tool can achieve on average a 3.8-fold reduction in area combined with a 13% performance boost through automatic identification and synthesis of static islands from fully dynamically scheduled circuits. The performance of the resulting hardware is close to optimum (as determined by an exhaustive enumeration of all possible static islands).\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3490422.3502362',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'SumMerge: an efficient algorithm and implementation for weight repetition-aware DNN inference',\n",
       "  'authors': \"['Rohan Baskar Prabhakar', 'Sachit Kuhar', 'Rohit Agrawal', 'Christopher J. Hughes', 'Christopher W. Fletcher']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ICS '21: Proceedings of the ACM International Conference on Supercomputing\",\n",
       "  'abstract': \"Deep Neural Network (DNN) inference efficiency is a key concern across the myriad of domains now relying on Deep Learning. A recent promising direction to speed-up inference is to exploit \\\\emph{weight repetition}. The key observation is that due to DNN quantization schemes---which attempt to reduce DNN storage requirements by reducing the number of bits needed to represent each weight---the same weight is bound to repeat many times within and across filters. This enables a weight-repetition aware inference kernel to factorize and memoize out common sub-computations, reducing arithmetic per inference while still maintaining the compression benefits of quantization. Yet, significant challenges remain. For instance, weight repetition introduces significant irregularity in the inference operation and hence (up to this point) has required custom hardware accelerators to derive net benefit. This paper proposes SumMerge: a new algorithm and set of implementation techniques to make weight repetition practical on general-purpose devices such as CPUs. The key idea is to formulate inference as traversing a sequence of data-flow graphs \\\\emph{with weight-dependent structure}. We develop an offline heuristic to select a data-flow graph structure that minimizes arithmetic operations per inference (given trained weight values) and use an efficient online procedure to traverse each data-flow graph and compute the inference result given DNN inputs. We implement the above as an optimized C++ routine that runs on a commercial multicore processor with vector extensions and evaluate performance relative to Intel's optimized library oneDNN and the prior-art weight repetition algorithm (AGR). When applied on top of six different quantization schemes, SumMerge achieves a speedup of between 1.09x-2.05x and 1.04x-1.51x relative to oneDNN and AGR, respectively, while simultaneously compressing the DNN model by 8.7x to 15.4x.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3447818.3460375',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Built-in Self-Test and Fault Localization for Inter-Layer Vias in Monolithic 3D ICs',\n",
       "  'authors': \"['Arjun Chaudhuri', 'Sanmitra Banerjee', 'Jinwoo Kim', 'Heechun Park', 'Bon Woong Ku', 'Sukeshwar Kannan', 'Krishnendu Chakrabarty', 'Sung Kyu Lim']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Journal on Emerging Technologies in Computing Systems',\n",
       "  'abstract': 'Monolithic 3D (M3D) integration provides massive vertical integration through the use of nanoscale inter-layer vias (ILVs). However, high integration density and aggressive scaling of the inter-layer dielectric make ILVs especially prone to defects. We present a low-cost built-in self-test (BIST) method that requires only two test patterns to detect opens, stuck-at faults, and bridging faults (shorts) in ILVs. We also propose an extended BIST architecture for fault detection, called Dual-BIST, to guarantee zero ILV fault masking due to single BIST faults and negligible ILV fault masking due to multiple BIST faults. We analyze the impact of coupling between adjacent ILVs arranged in a 1D array in block-level partitioned designs. Based on this analysis, we present a novel test architecture called Shared-BIST with the added functionality of localizing single and multiple faults, including coupling-induced faults. We introduce a systematic clustering-based method for designing and integrating a delay bank with the Shared-BIST architecture for testing small-delay defects in ILVs with minimal yield loss. Simulation results for four two-tier M3D benchmark designs highlight the effectiveness of the proposed BIST framework.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3464430',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Optimizing large-scale plasma simulations on persistent memory-based heterogeneous memory with effective data placement across memory hierarchy',\n",
       "  'authors': \"['Jie Ren', 'Jiaolin Luo', 'Ivy Peng', 'Kai Wu', 'Dong Li']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ICS '21: Proceedings of the ACM International Conference on Supercomputing\",\n",
       "  'abstract': 'Particle simulations of plasma are important for understanding plasma dynamics in space weather and fusion devices. However, production simulations that use billions and even trillions of computational particles require high memory capacity. In this work, we explore the latest persistent memory (PM) hardware to enable large-scale plasma simulations at unprecedented scales on a single machine. We use WarpX, an advanced plasma simulation code which is mission-critical and targets future exascale systems. We analyze the performance of WarpX on PM-based heterogeneous memory systems and propose to make the best use of memory hierarchy to avoid the impact of inferior performance of PM. We introduce a combination of static and dynamic data placement, and processor-cache prefetch mechanism for performance optimization. We develop a performance model to enable efficient data migration between PM and DRAM in the background, without reducing available bandwidth and parallelism to the application threads. We also build an analytical model to decide when to prefetch for the best use of caches. Our design achieves 66.4% performance improvement over the PM-only baseline and outperforms DRAM-cached, NUMA first-touch, and a state-of-the-art software solution by 38.8%, 45.1% and 83.3%, respectively.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3447818.3460356',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Elk Audio OS: An Open Source Operating System for the Internet of Musical Things',\n",
       "  'authors': \"['Luca Turchet', 'Carlo Fischione']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Internet of Things',\n",
       "  'abstract': 'As the Internet of Musical Things (IoMusT) emerges, audio-specific operating systems (OSs) are required on embedded hardware to ease development and portability of IoMusT applications. Despite the increasing importance of IoMusT applications, in this article, we show that there is no OS able to fulfill the diverse requirements of IoMusT systems. To address such a gap, we propose the Elk Audio OS as a novel and open source OS in this space. It is a Linux-based OS optimized for ultra-low-latency and high-performance audio and sensor processing on embedded hardware, as well as for handling wireless connectivity to local and remote networks. Elk Audio OS uses the Xenomai real-time kernel extension, which makes it suitable for the most demanding of low-latency audio tasks. We provide the first comprehensive overview of Elk Audio OS, describing its architecture and the key components of interest to potential developers and users. We explain operational aspects like the configuration of the architecture and the control mechanisms of the internal sound engine, as well as the tools that enable an easier and faster development of connected musical devices. Finally, we discuss the implications of Elk Audio OS, including the development of an open source community around it.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3446393',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Stream-AI-MD: streaming AI-driven adaptive molecular simulations for heterogeneous computing platforms',\n",
       "  'authors': \"['Alexander Brace', 'Michael Salim', 'Vishal Subbiah', 'Heng Ma', 'Murali Emani', 'Anda Trifa', 'Austin R. Clyde', 'Corey Adams', 'Thomas Uram', 'Hyunseung Yoo', 'Andew Hock', 'Jessica Liu', 'Venkatram Vishwanath', 'Arvind Ramanathan']\",\n",
       "  'date': 'July 2021',\n",
       "  'source': \"PASC '21: Proceedings of the Platform for Advanced Scientific Computing Conference\",\n",
       "  'abstract': 'Emerging hardware tailored for artificial intelligence (AI) and machine learning (ML) methods provide novel means to couple them with traditional high performance computing (HPC) workflows involving molecular dynamics (MD) simulations. We propose Stream-AI-MD, a novel instance of applying deep learning methods to drive adaptive MD simulation campaigns in a streaming manner. We leverage the ability to run ensemble MD simulations on GPU clusters, while the data from atomistic MD simulations are streamed continuously to AI/ML approaches to guide the conformational search in a biophysically meaningful manner on a wafer-scale AI accelerator. We demonstrate the efficacy of Stream-AI-MD simulations for two scientific use-cases: (1) folding a small prototypical protein, namely ββα-fold (BBA) FSD-EY and (2) understanding protein-protein interaction (PPI) within the SARS-CoV-2 proteome between two proteins, nsp16 and nsp10. We show that Stream-AI-MD simulations can improve time-to-solution by ~50X for BBA protein folding. Further, we also discuss performance trade-offs involved in implementing AI-coupled HPC workflows on heterogeneous computing architectures.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3468267.3470578',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'ShuffleFL: gradient-preserving federated learning using trusted execution environment',\n",
       "  'authors': \"['Yuhui Zhang', 'Zhiwei Wang', 'Jiangfeng Cao', 'Rui Hou', 'Dan Meng']\",\n",
       "  'date': 'May 2021',\n",
       "  'source': \"CF '21: Proceedings of the 18th ACM International Conference on Computing Frontiers\",\n",
       "  'abstract': \"Federated Learning (FL) is a promising approach to privacy-preserving machine learning. However, recent works reveal that gradients can leak private data. Using trusted SGX-processors for this task yields gradient-preserving but requires to prevent exploitation of any side-channel attacks. In this work, we present ShuffleFL, a gradient-preserving system using trusted SGX, which combines random group structure and intra-group gradient segment aggregation for combating any side-channel attacks. We analyze the security of our system against semi-honest adversaries. ShuffleFL effectively guarantees the participants' gradient privacy. We demonstrate the performance of ShuffleFL and show its applicability in the federated learning system.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3457388.3458665',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Building blocks for redundancy-free vector integer multiplication',\n",
       "  'authors': '[\\'James You\\', \\'Christopher W. Schankula\\', \"Bill O\\'Farrell\", \\'Christopher K. Anand\\']',\n",
       "  'date': 'November 2021',\n",
       "  'source': \"CASCON '21: Proceedings of the 31st Annual International Conference on Computer Science and Software Engineering\",\n",
       "  'abstract': 'Commercial applications of cryptography require arithmetic in prime fields with primes larger than the sizes of architected registers, and over time there will be pressure to use even larger fields to keep up with the increasing resources available for brute-force attacks and the threat that quantum computers will reach the power required for unconventional attacks. Integer multiplication is the bottleneck for most computations, and most algorithm innovations revolve around strategic composition of efficient hardware multipliers for smaller integers into algorithms for larger integer multiplication. In this paper we present an novel vector instruction which would allow hardware multipliers to be used optimally for school-book multiplication by flexibly grouping multiplications to avoid empty slots in vector instructions resulting in unused hardware capacity. We give general conditions for optimality, consider latency/throughput tradeoffs and optionally pair the new instruction, mammma, with a novel shift-and-sum instruction.',\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3507788.3507819',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Precise Cache Profiling for Studying Radiation Effects',\n",
       "  'authors': \"['James Marshall', 'Robert Gifford', 'Gedare Bloom', 'Gabriel Parmer', 'Rahul Simha']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Embedded Computing Systems',\n",
       "  'abstract': 'Increased access to space has led to an increase in the usage of commodity processors in radiation environments. These processors are vulnerable to transient faults such as single event upsets that may cause bit-flips in processor components. Caches in particular are vulnerable due to their relatively large area, yet are often omitted from fault injection testing because many processors do not provide direct access to cache contents and they are often not fully modeled by simulators. The performance benefits of caches make disabling them undesirable, and the presence of error correcting codes is insufficient to correct for increasingly common multiple bit upsets.This work explores building a program’s cache profile by collecting cache usage information at an instruction granularity via commonly available on-chip debugging interfaces. The profile provides a tighter bound than cache utilization for cache vulnerability estimates (50% for several benchmarks). This can be applied to reduce the number of fault injections required to characterize behavior by at least two-thirds for the benchmarks we examine. The profile enables future work in hardware fault injection for caches that avoids the biases of existing techniques.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3442339',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Enhancing Privacy in PUF-Cash through Multiple Trusted Third Parties and Reinforcement Learning',\n",
       "  'authors': \"['Georgios Fragkos', 'Cyrus Minwalla', 'Eirini Eleni Tsiropoulou', 'Jim Plusquellic']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Journal on Emerging Technologies in Computing Systems',\n",
       "  'abstract': 'Electronic cash (e-Cash) is a digital alternative to physical currency such as coins and bank notes. Suitably constructed, e-Cash has the ability to offer an anonymous offline experience much akin to cash, and in direct contrast to traditional forms of payment such as credit and debit cards. Implementing security and privacy within e-Cash, i.e., preserving user anonymity while preventing counterfeiting, fraud, and double spending, is a non-trivial challenge. In this article, we propose major improvements to an e-Cash protocol, termed PUF-Cash, based on physical unclonable functions (PUFs). PUF-Cash was created as an offline-first, secure e-Cash scheme that preserved user anonymity in payments. In addition, PUF-Cash supports remote payments; an improvement over traditional currency. In this work, a novel multi-trusted-third-party exchange scheme is introduced, which is responsible for “blinding” Alice’s e-Cash tokens; a feature at the heart of preserving her anonymity. The exchange operations are governed by machine learning techniques which are uniquely applied to optimize user privacy, while remaining resistant to identity-revealing attacks by adversaries and trusted authorities. Federation of the single trusted third party into multiple entities distributes the workload, thereby improving performance and resiliency within the e-Cash system architecture. Experimental results indicate that improvements to PUF-Cash enhance user privacy and scalability.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3441139',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'SoK: Enabling Security Analyses of Embedded Systems via Rehosting',\n",
       "  'authors': \"['Andrew Fasano', 'Tiemoko Ballo', 'Marius Muench', 'Tim Leek', 'Alexander Bulekov', 'Brendan Dolan-Gavitt', 'Manuel Egele', 'Aurélien Francillon', 'Long Lu', 'Nick Gregory', 'Davide Balzarotti', 'William Robertson']\",\n",
       "  'date': 'May 2021',\n",
       "  'source': \"ASIA CCS '21: Proceedings of the 2021 ACM Asia Conference on Computer and Communications Security\",\n",
       "  'abstract': 'Closely monitoring the behavior of a software system during its execution enables developers and analysts to observe, and ultimately understand, how it works. This kind of dynamic analysis can be instrumental to reverse engineering, vulnerability discovery, exploit development, and debugging. While these analyses are typically well-supported for homogeneous desktop platforms (e.g., x86 desktop PCs), they can rarely be applied in the heterogeneous world of embedded systems. One approach to enable dynamic analyses of embedded systems is to move software stacks from physical systems into virtual environments that sufficiently model hardware behavior. This process which we call \"rehosting\" poses a significant research challenge with major implications for security analyses. Although rehosting has traditionally been an unscientific and ad-hoc endeavor undertaken by domain experts with varying time and resources at their disposal, researchers are beginning to address rehosting challenges systematically and in earnest. In this paper, we establish that emulation is insufficient to conduct large-scale dynamic analysis of real-world hardware systems and present rehosting as a firmware-centric alternative. Furthermore, we taxonomize preliminary rehosting efforts, identify the fundamental components of the rehosting process, and propose directions for future research.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3433210.3453093',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Adaptive brokerage framework for the cloud with functional testing',\n",
       "  'authors': \"['Sheriffo Ceesay', 'Yuhui Lin', 'Adam Barker']\",\n",
       "  'date': 'December 2021',\n",
       "  'source': \"UCC '21: Proceedings of the 14th IEEE/ACM International Conference on Utility and Cloud Computing Companion\",\n",
       "  'abstract': 'In this paper, we present an Adaptive Brokerage for the Cloud (ABC) that can be used to simplify application deployment, monitoring and management processes in the cloud. The broker uses modern cloud infrastructure automation tools to test, deploy, monitor and optimise cloud resources. We used an e-commerce application to evaluate the entire functionality of the broker, we found out that different deployment options such as single-tier vs two-tier lead to interesting hardware and application performance insights. These insights are used to make effective infrastructure optimisation decisions.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3492323.3495624',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'S-Vectors and TESA: Speaker Embeddings and a Speaker Authenticator Based on Transformer Encoder',\n",
       "  'authors': \"['Narla John Metilda Sagaya Mary', 'Srinivasan Umesh', 'Sandesh Varadaraju Katta']\",\n",
       "  'date': 'None',\n",
       "  'source': 'IEEE/ACM Transactions on Audio, Speech and Language Processing',\n",
       "  'abstract': 'One of the most popular speaker embeddings is x-vectors, which are obtained from an architecture that gradually builds a larger temporal context with layers. In this paper, we propose to derive speaker embeddings from Transformer&#x2019;s encoder trained for speaker classification. Self-attention, on which Transformer&#x2019;s encoder is built, attends to all the features over the entire utterance and might be more suitable in capturing the speaker characteristics in an utterance. We refer to the speaker embeddings obtained from the proposed speaker classification model as s-vectors to emphasize that they are obtained from an architecture that heavily relies on self-attention. Through experiments, we demonstrate that s-vectors perform better than x-vectors. In addition to the s-vectors, we also propose a new architecture based on Transformer&#x2019;s encoder for speaker verification as a replacement for speaker verification based on conventional probabilistic linear discriminant analysis (PLDA). This architecture is inspired by the next sentence prediction task of bidirectional encoder representations from Transformers (BERT), and we feed the s-vectors of two utterances to verify whether they belong to the same speaker. We name this architecture the Transformer encoder speaker authenticator (TESA). Our experiments show that the performance of s-vectors with TESA is better than s-vectors with conventional PLDA-based speaker verification.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/TASLP.2021.3134566',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Lane Compression: A Lightweight Lossless Compression Method for Machine Learning on Embedded Systems',\n",
       "  'authors': \"['Yousun Ko', 'Alex Chadwick', 'Daniel Bates', 'Robert Mullins']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Embedded Computing Systems',\n",
       "  'abstract': 'This article presents Lane Compression, a lightweight lossless compression technique for machine learning that is based on a detailed study of the statistical properties of machine learning data. The proposed technique profiles machine learning data gathered ahead of run-time and partitions values bit-wise into different lanes with more distinctive statistical characteristics. Then the most appropriate compression technique is chosen for each lane out of a small number of low-cost compression techniques. Lane Compression’s compute and memory requirements are very low and yet it achieves a compression rate comparable to or better than Huffman coding. We evaluate and analyse Lane Compression on a wide range of machine learning networks for both inference and re-training. We also demonstrate the profiling prior to run-time and the ability to configure the hardware based on the profiling guarantee robust performance across different models and datasets. Hardware implementations are described and the scheme’s simplicity makes it suitable for compressing both on-chip and off-chip traffic.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3431815',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Reducing solid-state drive read latency by optimizing read-retry',\n",
       "  'authors': \"['Jisung Park', 'Myungsuk Kim', 'Myoungjun Chun', 'Lois Orosa', 'Jihong Kim', 'Onur Mutlu']\",\n",
       "  'date': 'April 2021',\n",
       "  'source': \"ASPLOS '21: Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': '3D NAND flash memory with advanced multi-level cell techniques provides high storage density, but suffers from significant performance degradation due to a large number of read-retry operations. Although the read-retry mechanism is essential to ensuring the reliability of modern NAND flash memory, it can significantly in-crease the read latency of an SSD by introducing multiple retry steps that read the target page again with adjusted read-reference voltage values. Through a detailed analysis of the read mechanism and rigorous characterization of 160 real 3D NAND flash memory chips, we find new opportunities to reduce the read-retry latency by exploiting two advanced features widely adopted in modern NAND flash-based SSDs: 1) the CACHE READ command and 2) strong ECC engine. First, we can reduce the read-retry latency using the advanced CACHE READ command that allows a NAND flash chip to perform consecutive reads in a pipelined manner. Second, there exists a large ECC-capability margin in the final retry step that can be used for reducing the chip-level read latency. Based on our new findings, we develop two new techniques that effectively reduce the read-retry latency: 1) Pipelined Read-Retry (PR²) and 2) Adaptive Read-Retry (AR²). PR² reduces the latency of a read-retry operation by pipelining consecutive retry steps using the CACHE READ command. AR² shortens the latency of each retry step by dynamically reducing the chip-level read latency depending on the current operating conditions that determine the ECC-capability margin. Our evaluation using twelve real-world workloads shows that our proposal improves SSD response time by up to 31.5% (17% on average)over a state-of-the-art baseline with only small changes to the SSD controller.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3445814.3446719',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Sensor Virtualization for Efficient Sharing of Mobile and Wearable Sensors',\n",
       "  'authors': \"['Jian Xu', 'Arani Bhattacharya', 'Aruna Balasubramanian', 'Donald E. Porter']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"SenSys '21: Proceedings of the 19th ACM Conference on Embedded Networked Sensor Systems\",\n",
       "  'abstract': \"Users are surrounded by sensors that are available through various devices beyond their smartphones. However, these sensors are not fully utilized by current end-user applications. A key reason sensor use is so limited is that application developers must exactly identify how the sensor data can be used by smartphone apps. To mitigate this problem, we present SenseWear, a sensor-sharing platform that extends the functionality of a smartphone to use remote sensors with limited additional developer effort. Sensor sharing has several uses, including augmenting the hardware in smartphones, creating new gestural interactions with smartphone applications, and improving application's Quality of Experience via higher-quality sensors from other devices, such as wearables. We developed and present six use cases that use remote sensors in various smartphone applications. Each extension requires adding fewer than 20 lines of code on average. Furthermore, using remote sensors did not introduce a perceptible increase in latency, and creates more convenient interaction options for smartphone apps.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3485730.3493451',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': \"Cores that don't count\",\n",
       "  'authors': \"['Peter H. Hochschild', 'Paul Turner', 'Jeffrey C. Mogul', 'Rama Govindaraju', 'Parthasarathy Ranganathan', 'David E. Culler', 'Amin Vahdat']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"HotOS '21: Proceedings of the Workshop on Hot Topics in Operating Systems\",\n",
       "  'abstract': 'We are accustomed to thinking of computers as fail-stop, especially the cores that execute instructions, and most system software implicitly relies on that assumption. During most of the VLSI era, processors that passed manufacturing tests and were operated within specifications have insulated us from this fiction. As fabrication pushes towards smaller feature sizes and more elaborate computational structures, and as increasingly specialized instruction-silicon pairings are introduced to improve performance, we have observed ephemeral computational errors that were not detected during manufacturing tests. These defects cannot always be mitigated by techniques such as microcode updates, and may be correlated to specific components within the processor, allowing small code changes to effect large shifts in reliability. Worse, these failures are often \"silent\" - the only symptom is an erroneous computation. We refer to a core that develops such behavior as \"mercurial.\" Mercurial cores are extremely rare, but in a large fleet of servers we can observe the disruption they cause, often enough to see them as a distinct problem - one that will require collaboration between hardware designers, processor vendors, and systems software architects. This paper is a call-to-action for a new focus in systems research; we speculate about several software-based approaches to mercurial cores, ranging from better detection and isolating mechanisms, to methods for tolerating the silent data corruption they cause.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3458336.3465297',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Request, Coalesce, Serve, and Forget: Miss-Optimized Memory Systems for Bandwidth-Bound Cache-Unfriendly Applications on FPGAs',\n",
       "  'authors': \"['Mikhail Asiatici', 'Paolo Ienne']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Reconfigurable Technology and Systems',\n",
       "  'abstract': 'Applications such as large-scale sparse linear algebra and graph analytics are challenging to accelerate on FPGAs due to the short irregular memory accesses, resulting in low cache hit rates. Nonblocking caches reduce the bandwidth required by misses by requesting each cache line only once, even when there are multiple misses corresponding to it. However, such reuse mechanism is traditionally implemented using an associative lookup. This limits the number of misses that are considered for reuse to a few tens, at most. In this article, we present an efficient pipeline that can process and store thousands of outstanding misses in cuckoo hash tables in on-chip SRAM with minimal stalls. This brings the same bandwidth advantage as a larger cache for a fraction of the area budget, because outstanding misses do not need a data array, which can significantly speed up irregular memory-bound latency-insensitive applications. In addition, we extend nonblocking caches to generate variable-length bursts to memory, which increases the bandwidth delivered by DRAMs and their controllers. The resulting miss-optimized memory system provides up to 25% speedup with 24× area reduction on 15 large sparse matrix-vector multiplication benchmarks evaluated on an embedded and a datacenter FPGA system.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3466823',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Improving railway track coverage with mmWave bridges: A Measurement Campaign',\n",
       "  'authors': \"['Adrian Schumacher', 'Nima Jamaly', 'Ruben Merz', 'Andreas Burg']\",\n",
       "  'date': 'August 2021',\n",
       "  'source': \"5G-MeMU '21: Proceedings of the 1st Workshop on 5G Measurements, Modeling, and Use Cases\",\n",
       "  'abstract': 'Bringing cellular capacity into modern trains is challenging because they act as Faraday cages. Building a radio frequency (RF) corridor along the railway tracks ensures a high signal-to-noise ratio and limits handovers. However, building such RF corridors is difficult because of the administrative burden of excessive formalities to obtain construction permissions and costly because of the sheer number of base stations. Our contribution in this paper is an unconventional solution of mmWave fronthauled low-power out-of-band repeater nodes deployed in short intervals on existing masts between high-power macro cell sites. The paper demonstrates the feasibility of the concept with an extensive measurement campaign on a commercial railway line. The benefit of using many low-power nodes with low-gain antennas compared to a baseline with only high-gain macro antennas is discussed, and the coverage improvement is evaluated. Based on the measurement results, a simple path loss model is calibrated. This model allows evaluation of the potential of the mmWave repeater architecture to increase the macro cell inter-site distance and reduce deployment costs.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3472771.3472774',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'ProMT: optimizing integrity tree updates for write-intensive pages in secure NVMs',\n",
       "  'authors': \"['Mazen Alwadi', 'Aziz Mohaisen', 'Amro Awad']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ICS '21: Proceedings of the ACM International Conference on Supercomputing\",\n",
       "  'abstract': \"Current computer systems are vulnerable to a wide range of attacks caused by the proliferation of accelerators, and the fact that current system comprise multiple SoCs provided from different vendors. Thus, major processor vendors are moving towards limiting the trust boundary to the processor chip only as in Intel's SGX, AMD's SME, and ARM's TrustZone. This secure boundary limitation requires protecting the memory content against data remanence attacks, which were performed against DRAM in the form of cold-boot attack and are more successful against NVM due to NVM's data persistency feature. However, implementing secure memory features, such as memory encryption and integrity verification has a non-trivial performance overhead, and can significantly reduce the emerging NVM's expected lifetime. Previous work looked at reducing the overheads of the secure memory implementation by packing more counters into a cache line, increasing the cacheability of security metadata, slightly reducing the size of the integrity tree, or using the ECC chip to store the MAC values. However, the root update process is barely studied, which requires a sequential update of the MAC values in all the integrity tree levels. In this paper, we propose ProMT, a novel memory controller design that ensures a persistently secure system with minimal overheads. ProMT protects the data confidentiality and ensures the data integrity with minimal overheads. ProMT reduces the performance overhead of secure memory implementation to 11.7%, extends the NVM's life time by 3.59x, and enables the system recovery in a fraction of a second.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3447818.3460377',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'TAS: <u>t</u>ernarized neural <u>a</u>rchitecture <u>s</u>earch for resource-constrained edge devices',\n",
       "  'authors': \"['Mohammad Loni', 'Hamid Mousavi', 'Mohammad Riazati', 'Masoud Daneshtalab', 'Mikael Sjödin']\",\n",
       "  'date': 'March 2022',\n",
       "  'source': \"DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe\",\n",
       "  'abstract': 'Ternary Neural Networks (TNNs) compress network weights and activation functions into 2-bit representation resulting in remarkable network compression and energy efficiency. However, there remains a significant gap in accuracy between TNNs and full-precision counterparts. Recent advances in Neural Architectures Search (NAS) promise opportunities in automated optimization for various deep learning tasks. Unfortunately, this area is unexplored for optimizing TNNs. This paper proposes TAS, a framework that drastically reduces the accuracy gap between TNNs and their full-precision counterparts by integrating quantization into the network design. We experienced that directly applying NAS to the ternary domain provides accuracy degradation as the search settings are customized for full-precision networks. To address this problem, we propose (i) a new cell template for ternary networks with maximum gradient propagation; and (ii) a novel learnable quantizer that adaptively relaxes the ternarization mechanism from the distribution of the weights and activation functions. Experimental results reveal that TAS delivers 2.64% higher accuracy and ≈2.8× memory saving over competing methods with the same bit-width resolution on the CIFAR-10 dataset. These results suggest that TAS is an effective method that paves the way for the efficient design of the next generation of quantized neural networks.',\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3539845.3540104',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Algorithms for Right-Sizing Heterogeneous Data Centers',\n",
       "  'authors': \"['Susanne Albers', 'Jens Quedenfeld']\",\n",
       "  'date': 'July 2021',\n",
       "  'source': \"SPAA '21: Proceedings of the 33rd ACM Symposium on Parallelism in Algorithms and Architectures\",\n",
       "  'abstract': 'Power consumption is a dominant and still growing cost factor in data centers. In time periods with low load, the energy consumption can be reduced by powering down unused servers. We resort to a model introduced by Lin, Wierman, Andrew and Thereska (23,24) that considers data centers with identical machines, and generalize it to heterogeneous data centers with d different server types. The operating cost of a server depends on its load and is modeled by an increasing, convex function for each server type. In contrast to earlier work, we consider the discrete setting, where the number of active servers must be integral. Thereby, we seek truly feasible solutions. For homogeneous data centers (d=1), both the offline and the online problem were solved optimally in (3,4)  In this paper, we study heterogeneous data centers with general time-dependent operating cost functions. We develop an online algorithm based on a work function approach which achieves a competitive ratio of 2d + 1 + ε for any ε > 0. For time-independent operating cost functions, the competitive ratio can be reduced to 2d + 1. There is a lower bound of 2d shown in (5), so our algorithm is nearly optimal. For the offline version, we give a graph-based (1+ε)-approximation algorithm. Additionally, our offline algorithm is able to handle time-variable data-center sizes.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3409964.3461789',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Wideband Full-Duplex Phased Array With Joint Transmit and Receive Beamforming: Optimization and Rate Gains',\n",
       "  'authors': \"['Tingjun Chen', 'Mahmood Baraani Dastjerdi', 'Harish Krishnaswamy', 'Gil Zussman']\",\n",
       "  'date': 'None',\n",
       "  'source': 'IEEE/ACM Transactions on Networking',\n",
       "  'abstract': 'Full-duplex (FD) wireless and phased arrays are both promising techniques that can significantly improve data rates in future wireless networks. However, integrating FD with transmit (Tx) and receive (Rx) phased arrays is extremely challenging, due to the large number of self-interference (SI) channels. Previous work relies on either RF canceller hardware or on analog/digital Tx beamforming (TxBF) to achieve SI cancellation (SIC). However, Rx beamforming (RxBF) and the data rate gain introduced by FD nodes employing beamforming have not been considered yet. We study FD phased arrays with joint TxBF and RxBF with the objective of achieving improved FD data rates. The key idea is to carefully select the TxBF and RxBF weights to achieve wideband RF SIC in the spatial domain with minimal TxBF and RxBF gain losses. Essentially, TxBF and RxBF are <italic>repurposed</italic>, thereby not requiring specialized RF canceller circuitry. We formulate the corresponding optimization problem and develop an iterative algorithm to obtain an approximate solution with provable performance guarantees. Using SI channel measurements and datasets, we extensively evaluate the performance of the proposed approach in different use cases under various network settings. The results show that an FD phased array with 9/36/72 elements can cancel the total SI power to below the noise floor with sum TxBF and RxBF gain losses of 10.6/7.2/6.9dB, even at Tx power level of 30dBm. Moreover, the corresponding FD rate gains are at least 1.33/1.66/1.68<inline-formula> <tex-math notation=\"LaTeX\">$\\\\times $ </tex-math></inline-formula>.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/TNET.2021.3069125',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Automated Testing of Graphics Units by Deep-Learning Detection of Visual Anomalies',\n",
       "  'authors': \"['Lev Faivishevsky', 'Adi Szeskin', 'Ashwin K. Muppalla', 'Ravid Shwartz-Ziv', 'Itamar Ben Ari', 'Ronen Laperdon', 'Benjamin Melloul', 'Tahi Hollander', 'Tom Hope', 'Amitai Armon']\",\n",
       "  'date': 'August 2021',\n",
       "  'source': \"KDD '21: Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining\",\n",
       "  'abstract': 'We present a novel system for performing real-time detection of diverse visual corruptions in videos, for validating the quality of graphics units in our company. The system is used for several types of content, including movies and 3D graphics, with strict constraints on low false alert rates and real-time processing of millions of video frames per day. These constraints required novel solutions involving both hardware and software, including new supervised and weakly-supervised methods we developed. Our deployed system has enabled a ~20X reduction of human effort and discovering new corruptions missed by humans and existing approaches.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3447548.3467116',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'SALAD: Static Analyzer for Loop Acceleration by Exploiting DLP',\n",
       "  'authors': \"['Yang Li', 'Xianfeng Li', 'Mingtao Chen', 'Weikang Zhou', 'Fan Deng']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"HP3C '21: Proceedings of the 5th International Conference on High Performance Compilation, Computing and Communications\",\n",
       "  'abstract': 'Data-intensive applications are becoming increasingly popular. However, only a few of them with high volume can afford dedicated hardware acceleration (such as Neural Network Processor, or NPU) or platform-specific software implementation (such as Tensorflow running on GPU). In this paper, we propose a hardware and software transparent framework for the acceleration of general-purpose data-intensive applications. Our framework is based on a key insight that most data-intensive applications spend the vast majority of their execution time on some inner loops with abundant opportunities for Data-Level Parallelism (DLP). In particular, we propose SALAD, a static analyzer for loop acceleration by exploiting DLP in hot loops under the LLVM (LLVM compiler infrastructure) framework. In contrast to traditional DLP exploration techniques, SALAD is both software and architectural transparent, without the need to change either the source code or binary code, and does not need vectorized instruction set architecture (ISA) extensions. Instead, it directly works on the program binary code and generates a profile for DLP opportunities in the binary. This profile will be fed to the hardware accelerator transparently to speed up execution. With the experiments result, we estimate that the DLP information provided by SALAD could result in 3.6x-60.2x speedups on a set of benchmarks, depending on their inherent DLP.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3471274.3471279',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Red Alert for Power Leakage: Exploiting Intel RAPL-Induced Side Channels',\n",
       "  'authors': \"['Zhenkai Zhang', 'Sisheng Liang', 'Fan Yao', 'Xing Gao']\",\n",
       "  'date': 'May 2021',\n",
       "  'source': \"ASIA CCS '21: Proceedings of the 2021 ACM Asia Conference on Computer and Communications Security\",\n",
       "  'abstract': 'RAPL (Running Average Power Limit) is a hardware feature introduced by Intel to facilitate power management. Even though RAPL and its supporting software interfaces can benefit power management significantly, they are unfortunately designed without taking certain security issues into careful consideration. In this paper, we demonstrate that information leaked through RAPL-induced side channels can be exploited to mount realistic attacks. Specifically, we have constructed a new RAPL-based covert channel using a single AVX instruction, which can exfiltrate data across different boundaries (e.g., those established by containers in software or even CPUs in hardware); and, we have investigated the first RAPL-based website fingerprinting technique that can identify visited webpages with a high accuracy (up to 99% in the case of the regular network using a browser like Chrome or Safari, and up to 81% in the case of the anonymity network using Tor). These two studies form a preliminary examination into RAPL-imposed security implications. In addition, we discuss some possible countermeasures.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3433210.3437517',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Declarative Power Sequencing',\n",
       "  'authors': \"['Jasmin Schult', 'Daniel Schwyn', 'Michael Giardino', 'David Cock', 'Reto Achermann', 'Timothy Roscoe']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Embedded Computing Systems',\n",
       "  'abstract': 'Modern computer server systems are increasingly managed at a low level by baseboard management controllers (BMCs). BMCs are processors with access to the most critical parts of the platform, below the level of OS or hypervisor, including control over power delivery to every system component. Buggy or poorly designed BMC software not only poses a security threat to a machine, it can permanently render the hardware inoperative. Despite this, there is little published work on how to rigorously engineer the power management functionality of BMCs so as to prevent this happening.This article takes a first step toward putting BMC software on a sound footing by specifying the hardware environment and the constraints necessary for safe and correct operation. This is best accomplished through automation: correct-by-construction power control sequences can be efficiently generated from a simple, trustworthy model of the platform’s power tree that incorporates the sequencing requirements and safe voltage ranges of all components.We present both a modeling language for complex power-delivery networks and a tool to automatically generate safe, efficient power sequences for complex modern platforms. This not only increases the trustworthiness of a hitherto opaque yet critical element of platform firmware: regulator and chip power models are significantly simpler to produce than hand-written power sequences. This, combined with model reuse for common components, reduces both time and cost associated with platform bring-up for new hardware.We evaluate our tool using a new high-performance 2-socket server platform with >100W per socket TDP, tight voltage limits and 25 distinct power regulators needing configuration, showing both fast (<10s) tool runtime, and correct power sequencing of a live system.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3477039',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Archytas: A Framework for Synthesizing and Dynamically Optimizing Accelerators for Robotic Localization',\n",
       "  'authors': \"['Weizhuang Liu', 'Bo Yu', 'Yiming Gan', 'Qiang Liu', 'Jie Tang', 'Shaoshan Liu', 'Yuhao Zhu']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"MICRO '21: MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture\",\n",
       "  'abstract': 'Despite many recent efforts, accelerating robotic computing is still fundamentally challenging for two reasons. First, robotics software stack is extremely complicated. Manually designing an accelerator while meeting the latency, power, and resource specifications is unscalable. Second, the environment in which an autonomous machine operates constantly changes; a static accelerator design leads to wasteful computation.  This paper takes a first step in tackling these two challenges using localization as a case study. We describe, a framework that automatically generates a synthesizable accelerator from the high-level algorithm description while meeting design constraints. The accelerator continuously optimizes itself at run time according to the operating environment to save power while sustaining performance and accuracy. is able to generate FPGA-based accelerator designs that cover large a design space and achieve orders of magnitude performance improvement and/or energy savings compared to state-of-the-art baselines.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3466752.3480077',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Macchiato: Importing Cache Side Channels to SDNs',\n",
       "  'authors': \"['Amir Sabzi', 'Liron Schiff', 'Kashyap Thimmaraju', 'Andreas Blenk', 'Stefan Schmid']\",\n",
       "  'date': 'December 2021',\n",
       "  'source': \"ANCS '21: Proceedings of the Symposium on Architectures for Networking and Communications Systems\",\n",
       "  'abstract': \"Since caches are shared and coherent, a memory access of one process may evict from the cache another process' memory block with an address mapped to the same cache line. This property is exploited by several attacks to form side channels. We show that MAC learning in Software Defined Networks (SDNs) has a similar property in the sense that a MAC address discovered by one network device may be revoked by the discovery of the same address at another switch. This allows us to implement Macchiato, a covert channel for SDNs between any two network devices (including hosts); prior SDN covert channels required at least one malicious switch. We evaluate a prototype implementation of Macchiato and discuss how methods to improve the performance of cache side channels (such as deep neural networks) can also be used in Macchiato.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3493425.3502758',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Distance learning of electronic engineering based on Arduino platform',\n",
       "  'authors': \"['Alipbay Mansur-Matritdinovich Dairbayev', 'Bahytzhan Sergeevich Baikenov', 'Yevgeniya Alexandrovna Daineko', 'Madina Tolegenovna Ipalakova']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"ICEMIS'21: The 7th International Conference on Engineering &amp; MIS 2021\",\n",
       "  'abstract': 'The principles of learning based on the Arduino platform, its features and development prospects are considered. Examples of using distance learning methods based on the Arduino platform, advantages and disadvantages of teaching methods are given. An effective, hybrid method of distance learning of electronic equipment on this platform is proposed. The laboratory work developed according to the proposed method of distance learning is presented.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3492547.3492644',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'The last CPU',\n",
       "  'authors': \"['Joel Nider', 'Alexandra (Sasha) Fedorova']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"HotOS '21: Proceedings of the Workshop on Hot Topics in Operating Systems\",\n",
       "  'abstract': \"Since the end of Dennard scaling and Moore's Law have been foreseen, specialized hardware has become the focus for continued scaling of application performance. Programmable accelerators such as smart memory, smart disks, and smart NICs are now being integrated into our systems. Many accelerators can be programmed to process their data autonomously and require little or no intervention during normal operation. In this way, entire applications are offloaded, leaving the CPU with the minimal responsibilities of initialization, coordination and error handling. We claim that these responsibilities can also be handled in simple hardware other than the CPU and that it is wasteful to use a CPU for these purposes. We explore the role and the structure of the OS in a system that has no CPU and demonstrate that all necessary functionality can be moved to other hardware. We show that almost all of the pieces for such a system design are already available today. The responsibilities of the operating system must be split between self-managing devices and a system bus that handles privileged operations.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3458336.3465291',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Fortifying Vehicular Security through Low Overhead Physically Unclonable Functions',\n",
       "  'authors': \"['Carson Labrado', 'Himanshu Thapliyal', 'Saraju P. Mohanty']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Journal on Emerging Technologies in Computing Systems',\n",
       "  'abstract': 'Within vehicles, the Controller Area Network (CAN) allows efficient communication between the electronic control units (ECUs) responsible for controlling the various subsystems. The CAN protocol was not designed to include much support for secure communication. The fact that so many critical systems can be accessed through an insecure communication network presents a major security concern. Adding security features to CAN is difficult due to the limited resources available to the individual ECUs and the costs that would be associated with adding the necessary hardware to support any additional security operations without overly degrading the performance of standard communication. Replacing the protocol is another option, but it is subject to many of the same problems. The lack of security becomes even more concerning as vehicles continue to adopt smart features. Smart vehicles have a multitude of communication interfaces an attacker could exploit to gain access to the networks. In this work, we propose a security framework that is based on physically unclonable functions (PUFs) and lightweight cryptography (LWC). The framework does not require any modification to the standard CAN protocol while also minimizing the amount of additional message overhead required for its operation. The improvements in our proposed framework result in major reduction in the number of CAN frames that must be sent during operation. For a system with 20 ECUs, for example, our proposed framework only requires 6.5% of the number of CAN frames that is required by the existing approach to successfully authenticate every ECU.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3442443',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Octopus+: An RDMA-Enabled Distributed Persistent Memory File System',\n",
       "  'authors': \"['Bohong Zhu', 'Youmin Chen', 'Qing Wang', 'Youyou Lu', 'Jiwu Shu']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Storage',\n",
       "  'abstract': 'Non-volatile memory and remote direct memory access (RDMA) provide extremely high performance in storage and network hardware. However, existing distributed file systems strictly isolate file system and network layers, and the heavy layered software designs leave high-speed hardware under-exploited. In this article, we propose an RDMA-enabled distributed persistent memory file system, Octopus+, to redesign file system internal mechanisms by closely coupling non-volatile memory and RDMA features. For data operations, Octopus+ directly accesses a shared persistent memory pool to reduce memory copying overhead, and actively fetches and pushes data all in clients to rebalance the load between the server and network. For metadata operations, Octopus+ introduces self-identified remote procedure calls for immediate notification between file systems and networking, and an efficient distributed transaction mechanism for consistency. Octopus+ is enabled with replication feature to provide better availability. Evaluations on Intel Optane DC Persistent Memory Modules show that Octopus+ achieves nearly the raw bandwidth for large I/Os and orders of magnitude better performance than existing distributed file systems.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3448418',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Probabilistic Risk-Aware Scheduling with Deadline Constraint for Heterogeneous SoCs',\n",
       "  'authors': \"['Xing Chen', 'Umit Ogras', 'Chaitali Chakrabarti']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Embedded Computing Systems',\n",
       "  'abstract': 'Hardware Trojans can compromise System-on-Chip (SoC) performance. Protection schemes implemented to combat these threats cannot guarantee 100% detection rate and may also introduce performance overhead. This paper defines the risk of running a job on an SoC as a function of the misdetection rate of the hardware Trojan detection methods implemented on the cores in the SoC. Given the user-defined deadlines of each job, our goal is to minimize the job-level risk as well as the deadline violation rate for both static and dynamic scheduling scenarios. We assume that there is no relationship between the execution time and risk of a task executed on a core. Our risk-aware scheduling algorithm first calculates the probability of possible task allocations and then uses it to derive the task-level deadlines. Each task is then allocated to the core with minimum risk that satisfies the task-level deadline. In addition, in dynamic scheduling, where multiple jobs are injected randomly, we propose to explicitly operate with a reduced virtual deadline to avoid possible future deadline violations. Simulations on randomly generated graphs show that our static scheduler has no deadline violations and achieves 5.1%–17.2% lower job-level risk than the popular Earliest Time First (ETF) algorithm when the deadline constraint is 1.2×–3.0× the makespan of ETF. In the dynamic case, the proposed algorithm achieves a violation rate comparable to that of Earliest Deadline First (EDF), an algorithm optimized for dynamic scenarios. Even when the injection rate is high, it outperforms EDF with 8.4%–10% lower risk when the deadline is 1.5×–3.0× the makespan of ETF.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3489409',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Construction and Realisation of a Virtual Simulation Experiment Platform',\n",
       "  'authors': \"['Dong Liu', 'Jing Chang']\",\n",
       "  'date': 'February 2022',\n",
       "  'source': \"ICBDE '22: Proceedings of the 5th International Conference on Big Data and Education\",\n",
       "  'abstract': 'This paper examines the problems existing in traditional teaching, proposes the application of virtual simulation technology in teaching as a response and summarises the current state of use of virtual simulation technology in modern teaching. It describes the application of virtual simulation experiment technology and the construction of an experimental platform, as well as introducing the system architecture, composition and advantages of the virtual experiment platform. In accordance with the design goal of the virtual simulation experiment platform, this paper examines the architecture, functional modules and software and hardware facilities of the virtual simulation experiment platform, and it analyses the application of the computer virtual simulation experiment platform using specific examples.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3524383.3524407',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A POWER MANAGEMENT CIRCUIT COMPATIBLE with Ti SmartReflex and Xilinx Ultrascale+ MPSoC DUAL VOLTAGE MECHANISM',\n",
       "  'authors': \"['Chenyun Li', 'Xuwen Li', 'Changjiang Tang', 'Qiang Wu']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"ICCPR '21: Proceedings of the 2021 10th International Conference on Computing and Pattern Recognition\",\n",
       "  'abstract': 'Since the semiconductor technology has entered the age of 90 nm, the static power consumption of the chip has increased rapidly. For edge computing devices, the waste of power can not be ignored. This paper uses the relationship between power consumption and power supply voltage of FPGA and DSP, analyzes the characteristics of the traditional scheme, and designs a hardware circuit according to the different mechanisms of reducing static power consumption of FPGA and DSP According to the design of FPGA and DSP core power supply voltage compatibility, the system realizes the requirement of FPGA and DSP for power supply voltage amplitude and realizes dynamic control of power supply voltage. After verification and comparison, FPGA and DSP can realize the adjustment of performance power consumption and stable operation under the power system.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3497623.3497681',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Scale-down experiments on TPCx-HS',\n",
       "  'authors': \"['Maximilian Böther', 'Tilmann Rabl']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"BiDEDE '21: Proceedings of the International Workshop on Big Data in Emergent Distributed Environments\",\n",
       "  'abstract': \"The Transaction Processing Performance Council's (TPC) benchmarks are the standard for evaluating data processing performance and are extensively used in academia and industry. Official TPC results are usually produced on high-end deployments, making transferability to commodity hardware difficult. Recent performance improvements on low-power ARM CPUs have made low-end computers, such as the Raspberry Pi, a candidate platform for distributed, low-scale data processing. In this paper, we conduct a feasibility study of executing scaled-down big data workloads on low-power ARM clusters. To this end, we run the TPCx-HS benchmark on two Raspberry Pi clusters. TPCx-HS is the ideal candidate for hardware comparisons and understanding hardware characteristics for data processing workloads because TPCx-HS results do not depend on specific software implementations and the benchmark has limited options for workload-specific tuning. Our evaluation shows that Pis exhibit similar behavior to large-scale big data systems in terms of price performance and relative throughput to performance results. Current generation Pi clusters are becoming a reasonable choice for GB-scale data processing due to the increasing amount of available memory, while older versions struggle with stable execution of high-load scenarios.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3460866.3461774',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A performance portability framework for Python',\n",
       "  'authors': \"['Nader Al Awar', 'Steven Zhu', 'George Biros', 'Milos Gligoric']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ICS '21: Proceedings of the ACM International Conference on Supercomputing\",\n",
       "  'abstract': 'Kokkos is a programming model for writing performance portable applications for all major high performance computing platforms. It provides abstractions for data management and common parallel operations, allowing developers to write portable high performance code with minimal knowledge of architecture-specific details. Kokkos is implemented as a heavily-templated C++ library. However, C++ is not ideal for rapid prototyping and quick algorithmic exploration. An increasing number of developers use Python for scientific computing, machine learning, and data analytics. In this paper, we present a new Python framework, dubbed PyKokkos, for writing performance portable applications entirely in Python. PyKokkos provides Kokkos-like abstractions that are easier to use and more concise than the C++ interface. We implemented PyKokkos by building a translator from a subset of Python to C++ Kokkos and bridging necessary function calls via automatically generated Python bindings. PyKokkos is also compatible with NumPy, a widely-used high performance Python library. By porting several existing Kokkos applications to PyKokkos, including ExaMiniMD (∼3k lines of code in C++), we show that the latter can achieve efficient execution with low performance overhead.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3447818.3460376',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'On-Demand SIMO Channel Impulse Response Shaping in Smart On-Chip Electromagnetic Environments',\n",
       "  'authors': \"['Mohammadreza F. Imani', 'Sergi Abadal', 'Philipp del Hougne']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"SenSys '21: Proceedings of the 19th ACM Conference on Embedded Networked Sensor Systems\",\n",
       "  'abstract': 'We recently introduced the concept of reconfigurable Wireless Networks on Chips (r-WNoCs) for which an on-chip reconfigurable intelligent surface (RIS) endows the wireless on-chip propagation environment with programmability. In this work-in-progress report, we apply this idea to a single-input multiple-output (SIMO) context. Specifically, we demonstrate that using an on-chip RIS we can simultaneously shape multiple channel impulse responses (CIRs) such that they become essentially pulse-like despite rich scattering inside the chip enclosure. Pulse-like CIRs are essential to enable high-speed information exchange between different processors on the same chip with the simple on-off-keying modulation schemes envisaged for WNoCs.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3485730.3494043',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Guardian: Symbolic Validation of Orderliness in SGX Enclaves',\n",
       "  'authors': \"['Pedro Antonino', 'Wojciech Aleksander Woloszyn', 'A. W. Roscoe']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"CCSW '21: Proceedings of the 2021 on Cloud Computing Security Workshop\",\n",
       "  'abstract': \"Modern processors can offer hardware primitives that allow a process to run in isolation. These primitives implement a trusted execution environment (TEE) in which a program can run such that the integrity and confidentiality of its execution are guaranteed. Intel's Software Guard eXtensions (SGX) is an example of such primitives and its isolated processes are called enclaves. These guarantees, however, can be easily thwarted if the enclave has not been properly designed. Its interface with the untrusted software stack is a perhaps the largest attack surface that adversaries can exploit; unintended interactions with untrusted code can expose the enclave to memory corruption attacks, for instance. In this paper, we propose a notion of an orderly enclave which splits its behaviour into the following execution phases: entry, secure, ocall, and exit. Each of them imposes a set of restrictions that enforce a particular policy of access to untrusted memory and, in some cases, sanitisation conditions. A violation of these policies and conditions might indicate an undesired interaction with untrusted data/code or a lack of sanitisation, both of which can be harnessed to perpetrate attacks against the enclave. We also introduce Guardian: an open-source tool that uses symbolic execution to carry out the validation of an enclave against our notion of an orderly enclave; in this process, it also looks for some other typical attack primitives. We discuss how our approach can prevent and flag enclave vulnerabilities that have been identified in the literature. Moreover, we have evaluated how our approach fares in the analysis of some enclave samples. In this process, Guardian identified some security issues previously undetected in some of these samples that were acknowledged and fixed by the corresponding maintainers.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3474123.3486755',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'In-situ Programmable Switching using rP4: Towards Runtime Data Plane Programmability',\n",
       "  'authors': \"['Yong Feng', 'Haoyu Song', 'Jiahao Li', 'Zhikang Chen', 'Wenquan Xu', 'Bin Liu']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"HotNets '21: Proceedings of the 20th ACM Workshop on Hot Topics in Networks\",\n",
       "  'abstract': 'The existing chip architecture and programming language are incapable of supporting in-service updates by loading or offloading on-demand protocols and functions at runtime. We examine the fundamental reasons for the inflexibility and design a new In-situ Programmable Switch Architecture (IPSA) as a fix. We further design rP4, a P4 extension, for programming IPSA-based devices. To manifest the in-situ programming feasibility, we develop an rP4 compiler and demonstrate several use cases on both a software switch, ipbm, and an FPGA-based prototype. Our preliminary experiments and analysis show that, compared to PISA, IPSA provides higher flexibility in enabling runtime functional update with limited performance and gate-count penalty. The in-situ programming capability enabled by IPSA and rP4 opens a promising design space for programmable networks.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3484266.3487367',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'AsTAR: Sustainable Energy Harvesting for the Internet of Things through Adaptive Task Scheduling',\n",
       "  'authors': \"['Fan Yang', 'Ashok Samraj Thangarajan', 'Gowri Sankar Ramachandran', 'Wouter Joosen', 'Danny Hughes']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Sensor Networks',\n",
       "  'abstract': 'Battery-free Internet-of-Things devices equipped with energy harvesting hold the promise of extended operational lifetime, reduced maintenance costs, and lower environmental impact. Despite this clear potential, it remains complex to develop applications that deliver sustainable operation in the face of variable energy availability and dynamic energy demands. This article aims to reduce this complexity by introducing AsTAR, an energy-aware task scheduler that automatically adapts task execution rates to match available environmental energy. AsTAR enables the developer to prioritize tasks based upon their importance, energy consumption, or a weighted combination thereof. In contrast to prior approaches, AsTAR is autonomous and self-adaptive, requiring no a priori modeling of the environment or hardware platforms. We evaluate AsTAR based on its capability to efficiently deliver sustainable operation for multiple tasks on heterogeneous platforms under dynamic environmental conditions. Our evaluation shows that (1) comparing to conventional approaches, AsTAR guarantees Sustainability by maintaining a user-defined optimum level of charge, and (2) AsTAR reacts quickly to environmental and platform changes, and achieves Efficiency by allocating all the surplus resources following the developer-specified task priorities. (3) Last, the benefits of AsTAR are achieved with minimal performance overhead in terms of memory, computation, and energy.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3467894',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Processing-in-Memory Acceleration of MAC-based Applications Using Residue Number System: A Comparative Study',\n",
       "  'authors': \"['Shaahin Angizi', 'Arman Roohi', 'MohammadReza Taheri', 'Deliang Fan']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"GLSVLSI '21: Proceedings of the 2021 on Great Lakes Symposium on VLSI\",\n",
       "  'abstract': 'Processing-in-memory (PIM) has raised as a viable solution for the memory wall crisis and has attracted great interest in accelerating computationally intensive AI applications ranging from filtering to complex neural networks. In this paper, we try to take advantage of both PIM and the residue number system (RNS) as an alternative for the conventional binary number representation to accelerate multiplication-and-accumulations (MACs), primary operations of target applications. The PIM architecture utilizes the maximum internal bandwidth of memory chips to realize a local and parallel computation to eliminates the off-chip data transfer. Moreover, RNS limits inter-digit carry propagation by performing arithmetic operations on small residues independently and in parallel. Thus, we develop a PIM-RNS, entitled PRIMS, and analyze the potential of intertwining PIM architecture with the inherent parallelism of the RNS arithmetic to delineate the opportunities and challenges. To this end, we build a comprehensive device-to-architecture evaluation framework to quantitatively study this problem considering the impact of PIM technology for a well-known three-moduli set as a case study.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3453688.3461529',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Secure Video Transmission System for UAV Applications',\n",
       "  'authors': \"['Dimitrios Psilias', 'Athanasios Milidonis', 'Ioannis Voyiatzis']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"PCI '21: Proceedings of the 25th Pan-Hellenic Conference on Informatics\",\n",
       "  'abstract': 'UAV applications are starting to increase nowadays. Their data demanding applications are implemented mostly using software platforms. These applications have critical requirements on high performance, power consumption and there should be security at their transmissions, especially for video data. In this paper, an architecture is proposed for secure video transmission for UAV applications. The system consists of a digital camera a transmission board to transmit the video and a FPGA for implementing the security encryption tasks. The camera sends the video data to the FPGA and the inside circuit encrypts the video data. The transmission module transmits the encrypted data to the Ground Station (GS). Measurements taken concerning the execution time and power consumption, reveal the benefits of the proposed architecture in comparison with well-known software platforms.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503823.3503882',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'cREAtIve: reconfigurable embedded artificial intelligence',\n",
       "  'authors': \"['Poona Bahrebar', 'Leon Denis', 'Maxim Bonnaerens', 'Kristof Coddens', 'Joni Dambre', 'Wouter Favoreel', 'Illia Khvastunov', 'Adrian Munteanu', 'Hung Nguyen-Duc', 'Stefan Schulte', 'Dirk Stroobandt', 'Ramses Valvekens', 'Nick Van den Broeck', 'Geert Verbruggen']\",\n",
       "  'date': 'May 2021',\n",
       "  'source': \"CF '21: Proceedings of the 18th ACM International Conference on Computing Frontiers\",\n",
       "  'abstract': 'cREAtIve targets the development of novel highly-adaptable embedded deep learning solutions for automotive and traffic monitoring applications, including position sensor processing, scene interpretation based on LiDAR, and object detection and classification in thermal images for traffic camera systems. These applications share the need for deep learning solutions tailored for deployment on embedded devices with limited resources and featuring high adaptability and robustness to changing environmental conditions. cREAtIve develops knowledge, tools and methods that enable hardware-efficient, adaptable, and robust deep learning.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3457388.3458857',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A case against (most) context switches',\n",
       "  'authors': \"['Jack Tigar Humphries', 'Kostis Kaffes', 'David Mazières', 'Christos Kozyrakis']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"HotOS '21: Proceedings of the Workshop on Hot Topics in Operating Systems\",\n",
       "  'abstract': 'Multiplexing software threads onto hardware threads and serving interrupts, VM-exits, and system calls require frequent context switches, causing high overheads and significant kernel and application complexity. We argue that context switching is an idea whose time has come and gone, and propose eliminating it through a radically different hardware threading model targeted to solve software rather than hardware problems. The new model adds a large number of hardware threads to each physical core - making thread multiplexing unnecessary - and lets software manage them. The only state change directly triggered in hardware by system calls, exceptions, and asynchronous hardware events will be blocking and unblocking hardware threads. We also present ISA extensions to allow kernel and user software to exploit this new threading model. Developers can use these extensions to eliminate interrupts and implement fast I/O without polling, exception-less system and hypervisor calls, practical microkernels, simple distributed programming models, and untrusted but fast hypervisors. Finally, we suggest practical hardware implementations and discuss the hardware and software challenges toward realizing this novel approach.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3458336.3465274',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A ReRAM Memory Compiler for Monolithic 3D Integrated Circuits in a Carbon Nanotube Process',\n",
       "  'authors': \"['Edward Lee', 'Daehyun Kim', 'Jinwoo Kim', 'Sung Kyu Lim', 'Saibal Mukhopadhyay']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Journal on Emerging Technologies in Computing Systems',\n",
       "  'abstract': 'We present a ReRAM memory compiler for monolithic 3D (M3D) integrated circuits (IC). We develop ReRAM architectures for M3D ICs using 1T-1R bit cells and single and multiple tiers of transistors for access and peripheral circuits. The compiler includes an automated flow for generation of subarrays of different dimensions and larger arrays of a target capacity by integrating multiple subarrays. The compiler is demonstrated using an M3D process design kit (PDK) based on a Carbon Nanotube Transistor technology. The PDK includes multiple layers of transistors and back-end-of-the-line integrated ReRAM. Simulations show the compiled ReRAM macros with multiple tiers of transistors reduces footprint and improves performance over the macros with single-tier transistors. The compiler creates layout views that are exported into library exchange format or graphic data system for full-array assembly and schematic/symbol views to extract per-bit read/write energy and read latency. Comparison of the proposed M3D subarray architectures with baseline 2D subarrays, generated with a custom-designed set of bit cells and peripherals, demonstrate up to 48% area reduction and 13% latency improvement.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3466681',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Low-precision Logarithmic Number Systems: Beyond Base-2',\n",
       "  'authors': \"['Syed Asad Alam', 'James Garland', 'David Gregg']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Architecture and Code Optimization',\n",
       "  'abstract': 'Logarithmic number systems (LNS) are used to represent real numbers in many applications using a constant base raised to a fixed-point exponent making its distribution exponential. This greatly simplifies hardware multiply, divide, and square root. LNS with base-2 is most common, but in this article, we show that for low-precision LNS the choice of base has a significant impact.We make four main contributions. First, LNS is not closed under addition and subtraction, so the result is approximate. We show that choosing a suitable base can manipulate the distribution to reduce the average error. Second, we show that low-precision LNS addition and subtraction can be implemented efficiently in logic rather than commonly used ROM lookup tables, the complexity of which can be reduced by an appropriate choice of base. A similar effect is shown where the result of arithmetic has greater precision than the input. Third, where input data from external sources is not expected to be in LNS, we can reduce the conversion error by selecting a LNS base to match the expected distribution of the input. Thus, there is no one base that gives the global optimum, and base selection is a trade-off between different factors. Fourth, we show that circuits realized in LNS require lower area and power consumption for short word lengths.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3461699',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'UNIQ: Uniform Noise Injection for Non-Uniform Quantization of Neural Networks',\n",
       "  'authors': \"['Chaim Baskin', 'Natan Liss', 'Eli Schwartz', 'Evgenii Zheltonozhskii', 'Raja Giryes', 'Alex M. Bronstein', 'Avi Mendelson']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Computer Systems',\n",
       "  'abstract': 'We present a novel method for neural network quantization. Our method, named UNIQ, emulates a non-uniform k-quantile quantizer and adapts the model to perform well with quantized weights by injecting noise to the weights at training time. As a by-product of injecting noise to weights, we find that activations can also be quantized to as low as 8-bit with only a minor accuracy degradation. Our non-uniform quantization approach provides a novel alternative to the existing uniform quantization techniques for neural networks. We further propose a novel complexity metric of number of bit operations performed (BOPs), and we show that this metric has a linear relation with logic utilization and power. We suggest evaluating the trade-off of accuracy vs. complexity (BOPs). The proposed method, when evaluated on ResNet18/34/50 and MobileNet on ImageNet, outperforms the prior state of the art both in the low-complexity regime and the high accuracy regime. We demonstrate the practical applicability of this approach, by implementing our non-uniformly quantized CNN on FPGA.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3444943',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Deep Learning for Sensor-based Human Activity Recognition: Overview, Challenges, and Opportunities',\n",
       "  'authors': \"['Kaixuan Chen', 'Dalin Zhang', 'Lina Yao', 'Bin Guo', 'Zhiwen Yu', 'Yunhao Liu']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Computing Surveys',\n",
       "  'abstract': 'The vast proliferation of sensor devices and Internet of Things enables the applications of sensor-based activity recognition. However, there exist substantial challenges that could influence the performance of the recognition system in practical scenarios. Recently, as deep learning has demonstrated its effectiveness in many areas, plenty of deep methods have been investigated to address the challenges in activity recognition. In this study, we present a survey of the state-of-the-art deep learning methods for sensor-based human activity recognition. We first introduce the multi-modality of the sensory data and provide information for public datasets that can be used for evaluation in different challenge tasks. We then propose a new taxonomy to structure the deep methods by challenges. Challenges and challenge-related deep methods are summarized and analyzed to form an overview of the current research progress. At the end of this work, we discuss the open issues and provide some insights for future directions.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3447744',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Judging a type by its pointer: optimizing GPU virtual functions',\n",
       "  'authors': \"['Mengchi Zhang', 'Ahmad Alawneh', 'Timothy G. Rogers']\",\n",
       "  'date': 'April 2021',\n",
       "  'source': \"ASPLOS '21: Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'Programmable accelerators aim to provide the flexibility of traditional CPUs with significantly improved performance. A well-known impediment to the widespread adoption of programmable accelerators, like GPUs, is the software engineering overhead involved in porting the code. Existing support for C++ on GPUs allows programmers to port polymorphic code with little effort. However, the overhead from the virtual functions introduced by polymorphic code has not been well studied or mitigated on GPUs.  To alleviate the performance cost of virtual functions, we propose two novel techniques that determine an object’s type based only on the object’s address, without accessing the object’s embedded virtual table pointer. The first technique, Coordinated Object Allocation and function Lookup (COAL), is a software-only solution that allocates objects by type and uses the compiler and runtime to find the object’s vTable without accessing an embedded pointer. COAL improves performance by 80%, 47%, and 6% over contemporary CUDA, prior research, and our newly-proposed type-based allocator, respectively. The second solution, TypePointer, introduces a hardware modification that allows unused bits in the object pointer to encode the object’s type, improving performance by 90%, 56%, and 12% over CUDA, prior work, and our new allocator. TypePointer can also be used with the default CUDA allocator to achieve an 18% performance improvement without modifying object allocation.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3445814.3446734',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Generalizable coordination of large multiscale workflows: challenges and learnings at scale',\n",
       "  'authors': \"['Harsh Bhatia', 'Francesco Di Natale', 'Joseph Y. Moon', 'Xiaohua Zhang', 'Joseph R. Chavez', 'Fikret Aydin', 'Chris Stanley', 'Tomas Oppelstrup', 'Chris Neale', 'Sara Kokkila Schumacher', 'Dong H. Ahn', 'Stephen Herbein', 'Timothy S. Carpenter', 'Sandrasegaram Gnanakaran', 'Peer-Timo Bremer', 'James N. Glosli', 'Felice C. Lightstone', 'Helgi I. Ingólfsson']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"SC '21: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis\",\n",
       "  'abstract': 'The advancement of machine learning techniques and the heterogeneous architectures of most current supercomputers are propelling the demand for large multiscale simulations that can automatically and autonomously couple diverse components and map them to relevant resources to solve complex problems at multiple scales. Nevertheless, despite the recent progress in workflow technologies, current capabilities are limited to coupling two scales. In the first-ever demonstration of using three scales of resolution, we present a scalable and generalizable framework that couples pairs of models using machine learning and in situ feedback. We expand upon the massively parallel Multiscale Machine-Learned Modeling Infrastructure (MuMMI), a recent, award-winning workflow, and generalize the framework beyond its original design. We discuss the challenges and learnings in executing a massive multiscale simulation campaign that utilized over 600,000 node hours on Summit and achieved more than 98% GPU occupancy for more than 83% of the time. We present innovations to enable several orders of magnitude scaling, including simultaneously coordinating 24,000 jobs, and managing several TBs of new data per day and over a billion files in total. Finally, we describe the generalizability of our framework and, with an upcoming open-source release, discuss how the presented framework may be used for new applications.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3458817.3476210',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Trust assessment in 32 KiB of RAM: multi-application trust-based task offloading for resource-constrained IoT nodes',\n",
       "  'authors': \"['Matthew Bradbury', 'Arshad Jhumka', 'Tim Watson']\",\n",
       "  'date': 'March 2021',\n",
       "  'source': \"SAC '21: Proceedings of the 36th Annual ACM Symposium on Applied Computing\",\n",
       "  'abstract': 'There is an increasing demand for Internet of Things (IoT) systems comprised of resource-constrained sensor and actuator nodes executing increasingly complex applications, possibly simultaneously. IoT devices will not be able to execute computationally expensive tasks and will require more powerful computing nodes, called edge nodes, for such execution, in a process called computation offloading. When multiple powerful nodes are available, a selection problem arises: which edge node should a task be submitted to? This problem is even more acute when the system is subjected to attacks, such as DoS, or network perturbations such as system overload. In this paper, we present a trust model-based system architecture for computation offloading, based on behavioural evidence. The system architecture provides confidentiality, authentication and non-repudiation of messages in required scenarios and will operate within the resource constraints of embedded IoT nodes. We demonstrate the viability of the architecture with an example deployment of Beta Reputation System trust model on real hardware.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3412841.3441898',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'To Partition, or Not to Partition, That is the Join Question in a Real System',\n",
       "  'authors': \"['Maximilian Bandle', 'Jana Giceva', 'Thomas Neumann']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data\",\n",
       "  'abstract': 'An efficient implementation of a hash join has been a highly researched problem for decades. Recently, the radix join has been shown to have superior performance over the alternatives (e.g., the non-partitioned hash join), albeit on synthetic microbenchmarks. Therefore, it is unclear whether one can simply replace the hash join in an RDBMS or use the radix join as a performance booster for selected queries. If the latter, it is still unknown when one should rely on the radix join to improve performance. In this paper, we address these questions, show how to integrate the radix join in Umbra, a code-generating DBMS, and make it competitive for selective queries by introducing a Bloom-filter based semi-join reducer. We have evaluated how well it runs when used in queries from more representative workloads like TPC-H. Surprisingly, the radix join brings a noticeable improvement in only one out of all 59 joins in TPC-H. Thus, with an extensive range of microbenchmarks, we have isolated the effects of the most important workload factors and synthesized the range of values where partitioning the data for the radix join pays off. Our analysis shows that the benefit of data partitioning quickly diminishes as soon as we deviate from the optimal parameters, and even late materialization rarely helps in real workloads. We thus, conclude that integrating the radix join within a code-generating database rarely justifies the increase in code and optimizer complexity and advise against it for processing real-world workloads.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3448016.3452831',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Spatial Dependency Analysis to Extract Information from Side-Channel Mixtures',\n",
       "  'authors': \"['Aurélien Vasselle', 'Hugues Thiebeauld', 'Philippe Maurine']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"ASHES '21: Proceedings of the 5th Workshop on Attacks and Solutions in Hardware Security\",\n",
       "  'abstract': \"Practical side-channel attacks on recent devices may be challenging due to the poor quality of acquired signals. It can originate from different factors, such as the growing architecture complexity, especially in System-on-Chips, creating unpredictable and concurrent operation of multiple signal sources in the device. This work makes use of mixture distributions to formalize this complexity, allowing us to explain the benefit of using a technique like Scatter, where different samples of the traces are aggregated into the same distribution. Some observations of the conditional mixture distributions are made in order to model the leakage in such context. From this, we infer local coherency of information held in the distribution as a general expression of the leakage in mixture distributions. This leads us to introduce how spatial analysis tools, such as Moran's Index, can be used to significantly improve non-profiled attacks compared to other techniques from the state-of-the-art. Exploitation of this technique is experimentally shown very promising, as demonstrated by its application to ASCAD dataset.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3474376.3487280',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Judging a type by its pointer: optimizing GPU virtual functions',\n",
       "  'authors': \"['Mengchi Zhang', 'Ahmad Alawneh', 'Timothy G. Rogers']\",\n",
       "  'date': 'April 2021',\n",
       "  'source': \"ASPLOS '21: Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'Programmable accelerators aim to provide the flexibility of traditional CPUs with significantly improved performance. A well-known impediment to the widespread adoption of programmable accelerators, like GPUs, is the software engineering overhead involved in porting the code. Existing support for C++ on GPUs allows programmers to port polymorphic code with little effort. However, the overhead from the virtual functions introduced by polymorphic code has not been well studied or mitigated on GPUs.  To alleviate the performance cost of virtual functions, we propose two novel techniques that determine an object’s type based only on the object’s address, without accessing the object’s embedded virtual table pointer. The first technique, Coordinated Object Allocation and function Lookup (COAL), is a software-only solution that allocates objects by type and uses the compiler and runtime to find the object’s vTable without accessing an embedded pointer. COAL improves performance by 80%, 47%, and 6% over contemporary CUDA, prior research, and our newly-proposed type-based allocator, respectively. The second solution, TypePointer, introduces a hardware modification that allows unused bits in the object pointer to encode the object’s type, improving performance by 90%, 56%, and 12% over CUDA, prior work, and our new allocator. TypePointer can also be used with the default CUDA allocator to achieve an 18% performance improvement without modifying object allocation.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3445814.3446734',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'To Partition, or Not to Partition, That is the Join Question in a Real System',\n",
       "  'authors': \"['Maximilian Bandle', 'Jana Giceva', 'Thomas Neumann']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data\",\n",
       "  'abstract': 'An efficient implementation of a hash join has been a highly researched problem for decades. Recently, the radix join has been shown to have superior performance over the alternatives (e.g., the non-partitioned hash join), albeit on synthetic microbenchmarks. Therefore, it is unclear whether one can simply replace the hash join in an RDBMS or use the radix join as a performance booster for selected queries. If the latter, it is still unknown when one should rely on the radix join to improve performance. In this paper, we address these questions, show how to integrate the radix join in Umbra, a code-generating DBMS, and make it competitive for selective queries by introducing a Bloom-filter based semi-join reducer. We have evaluated how well it runs when used in queries from more representative workloads like TPC-H. Surprisingly, the radix join brings a noticeable improvement in only one out of all 59 joins in TPC-H. Thus, with an extensive range of microbenchmarks, we have isolated the effects of the most important workload factors and synthesized the range of values where partitioning the data for the radix join pays off. Our analysis shows that the benefit of data partitioning quickly diminishes as soon as we deviate from the optimal parameters, and even late materialization rarely helps in real workloads. We thus, conclude that integrating the radix join within a code-generating database rarely justifies the increase in code and optimizer complexity and advise against it for processing real-world workloads.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3448016.3452831',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Accelerating applications using edge tensor processing units',\n",
       "  'authors': \"['Kuan-Chieh Hsu', 'Hung-Wei Tseng']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"SC '21: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis\",\n",
       "  'abstract': 'Neural network (NN) accelerators have been integrated into a wide-spectrum of computer systems to accommodate the rapidly growing demands for artificial intelligence (AI) and machine learning (ML) applications. NN accelerators share the idea of providing native hardware support for operations on multidimensional tensor data. Therefore, NN accelerators are theoretically tensor processors that can improve system performance for any problem that uses tensors as inputs/outputs. Unfortunately, commercially available NN accelerators only expose computation capabilities through AI/ML-specific interfaces. Furthermore, NN accelerators reveal very few hardware design details, so applications cannot easily leverage the tensor operations NN accelerators provide. This paper introduces General-Purpose Computing on Tensor Processing Units (GPTPU), an open-source, open-architecture framework that allows the developer and research communities to discover opportunities that NN accelerators enable for applications. GPTPU includes a powerful programming interface with efficient runtime system-level support---similar to that of CUDA/OpenCL in GPGPU computing---to bridge the gap between application demands and mismatched hardware/software interfaces. We built GPTPU machine uses Edge Tensor Processing Units (Edge TPUs), which are widely available and representative of many commercial NN accelerators. We identified several novel use cases and revisited the algorithms. By leveraging the underlying Edge TPUs to perform tensor-algorithm-based compute kernels, our results reveal that GPTPU can achieve a 2.46× speedup over high-end CPUs and reduce energy consumption by 40%.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3458817.3476177',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Faulty Point Unit: ABI Poisoning Attacks on Trusted Execution Environments',\n",
       "  'authors': \"['Fritz Alder', 'Jo Van Bulck', 'Jesse Spielman', 'David Oswald', 'Frank Piessens']\",\n",
       "  'date': 'None',\n",
       "  'source': 'Digital Threats: Research and Practice',\n",
       "  'abstract': 'This article analyzes a previously overlooked attack surface that allows unprivileged adversaries to impact floating-point computations in enclaves through the Application Binary Interface (ABI). In a comprehensive study across 7 industry-standard and research enclave shielding runtimes for Intel Software Guard Extensions (SGX), we show that control and state registers of the x87 Floating-Point Unit (FPU) and Intel Streaming SIMD Extensions are not always properly sanitized on enclave entry. We furthermore show that this attack goes beyond the x86 architecture and can also affect RISC-V enclaves. Focusing on SGX, we abuse the adversary’s control over precision and rounding modes as an ABI fault injection primitive to corrupt enclaved floating-point operations. Our analysis reveals that this is especially relevant for applications that use the older x87 FPU, which is still under certain conditions used by modern compilers. We exemplify the potential impact of ABI quality-degradation attacks for enclaved machine learning and for the SPEC benchmarks. We then explore the impact on confidentiality, showing that control over exception masks can be abused as a controlled channel to recover enclaved multiplication operands. Our findings, affecting 5 of 7 studied SGX runtimes and one RISC-V runtime, demonstrate the challenges of implementing high-assurance trusted execution across computing architectures.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3491264',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'BCD deduplication: effective memory compression using partial cache-line deduplication',\n",
       "  'authors': \"['Sungbo Park', 'Ingab Kang', 'Yaebin Moon', 'Jung Ho Ahn', 'G. Edward Suh']\",\n",
       "  'date': 'April 2021',\n",
       "  'source': \"ASPLOS '21: Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'In this paper, we identify new partial data redundancy among multiple cache lines that are not exploited by traditional memory compression or memory deduplication. We propose Base and Compressed Difference (BCD) deduplication that effectively utilizes the partial matches among cache lines through a novel combination of compression and deduplication to increase the effective capacity of main memory. Experimental results show that BCD achieves the average compression ratio of 1.94× for SPEC2017, DaCapo, TPC-DS, and TPC-H, which is 48.4% higher than the best prior work. We also present an efficient implementation of BCD in a modern memory hierarchy, which compresses data in both the last-level cache (LLC) and main memory with modest area overhead. Even with additional meta-data accesses and compression/deduplication operations, cycle-level simulations show that BCD improves the performance of the SPEC2017 benchmarks by 2.7% on average because it increases the effective capacity of the LLC. Overall, the results show that BCD can significantly increase the capacity of main memory with little performance overhead.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3445814.3446722',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Generalizable coordination of large multiscale workflows: challenges and learnings at scale',\n",
       "  'authors': \"['Harsh Bhatia', 'Francesco Di Natale', 'Joseph Y. Moon', 'Xiaohua Zhang', 'Joseph R. Chavez', 'Fikret Aydin', 'Chris Stanley', 'Tomas Oppelstrup', 'Chris Neale', 'Sara Kokkila Schumacher', 'Dong H. Ahn', 'Stephen Herbein', 'Timothy S. Carpenter', 'Sandrasegaram Gnanakaran', 'Peer-Timo Bremer', 'James N. Glosli', 'Felice C. Lightstone', 'Helgi I. Ingólfsson']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"SC '21: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis\",\n",
       "  'abstract': 'The advancement of machine learning techniques and the heterogeneous architectures of most current supercomputers are propelling the demand for large multiscale simulations that can automatically and autonomously couple diverse components and map them to relevant resources to solve complex problems at multiple scales. Nevertheless, despite the recent progress in workflow technologies, current capabilities are limited to coupling two scales. In the first-ever demonstration of using three scales of resolution, we present a scalable and generalizable framework that couples pairs of models using machine learning and in situ feedback. We expand upon the massively parallel Multiscale Machine-Learned Modeling Infrastructure (MuMMI), a recent, award-winning workflow, and generalize the framework beyond its original design. We discuss the challenges and learnings in executing a massive multiscale simulation campaign that utilized over 600,000 node hours on Summit and achieved more than 98% GPU occupancy for more than 83% of the time. We present innovations to enable several orders of magnitude scaling, including simultaneously coordinating 24,000 jobs, and managing several TBs of new data per day and over a billion files in total. Finally, we describe the generalizability of our framework and, with an upcoming open-source release, discuss how the presented framework may be used for new applications.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3458817.3476210',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Co-Exploration of Graph Neural Network and Network-on-Chip Design Using AutoML',\n",
       "  'authors': \"['Daniel Manu', 'Shaoyi Huang', 'Caiwen Ding', 'Lei Yang']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"GLSVLSI '21: Proceedings of the 2021 on Great Lakes Symposium on VLSI\",\n",
       "  'abstract': 'Recently, Graph Neural Networks (GNNs) have exhibited high efficiency in several graph-based machine learning tasks. Compared with the neural networks for computer vision or speech tasks (e.g., Convolutional Neural Networks), GNNs have much higher requirements on communication due to the complicated graph structures; however, when applying GNNs for real-world applications, say in recommender systems (e.g. Uber Eats), it commonly has the real-time requirements. To deal with the tradeoff between the complicated architecture and the high-demand timing performance, both GNN architecture and hardware accelerator need to be optimized. Network-on-Chip (NoC), derived for efficiently managing the high-volume of communications, naturally becomes one of the top candidates to accelerate GNNs. However, there is a missing link between the optimize of GNN architecture and the NoC design. In this work, we present an AutoML-based framework GN-NAS, aiming at searching for the optimum GNN architecture, which can be suitable for the NoC accelerator. We devise a robust reinforcement learning based controller to validate the retained best GNN architectures, coupled with a parameter sharing approach, namely ParamShare, to improve search efficiency. Experimental results on four graph-based benchmark datasets, Cora, Citeseer, Pubmed and Protein-Protein Interaction show that the GNN architectures obtained by our framework outperform that of the state-of-the-art and baseline models, whilst reducing model size which makes them easy to deploy onto the NoC platform.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3453688.3461741',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A Convolutional Neural Network Accelerator Based on NVDLA',\n",
       "  'authors': \"['Kangjin Zhao', 'Jing Wang', 'Di Zang']\",\n",
       "  'date': 'September 2021',\n",
       "  'source': \"ICACS '21: Proceedings of the 5th International Conference on Algorithms, Computing and Systems\",\n",
       "  'abstract': 'In recent years, Convolutional Neural Network (CNN) has been successfully applied to a wider range of fields, such as image recognition and natural language processing. With the application of CNN to solve more complex problems, their computing and storage requirements are also greatly increased. Traditionally, CNN is executed on CPU and GPU, but their low throughput and energy efficiency are the bottleneck of using them. Field Programmable Gate Array (FPGA) has many characteristics suitable for acceleration, it has become an ideal platform for hardware acceleration of CNN. We design and implement a convolutional neural network accelerator based on NVIDIA deep learning accelerator (NVDLA) on FPGA platform. We give the detailed structure of NVDLA, design the hardware system and software system. The neural networks that NVDLA can support are limited, but our architecture can realize the high bandwidth data communication between NVDLA and CPU. CPU handle the operations that NVDLA does not support. The accelerator will support more and more complex networks in the future.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3490700.3490708',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'RAS: Continuously Optimized Region-Wide Datacenter Resource Allocation',\n",
       "  'authors': \"['Andrew Newell', 'Dimitrios Skarlatos', 'Jingyuan Fan', 'Pavan Kumar', 'Maxim Khutornenko', 'Mayank Pundir', 'Yirui Zhang', 'Mingjun Zhang', 'Yuanlai Liu', 'Linh Le', 'Brendon Daugherty', 'Apurva Samudra', 'Prashasti Baid', 'James Kneeland', 'Igor Kabiljo', 'Dmitry Shchukin', 'Andre Rodrigues', 'Scott Michelson', 'Ben Christensen', 'Kaushik Veeraraghavan', 'Chunqiang Tang']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"SOSP '21: Proceedings of the ACM SIGOPS 28th Symposium on Operating Systems Principles\",\n",
       "  'abstract': \"Capacity reservation is a common offering in public clouds and on-premise infrastructure. However, no prior work provides capacity reservation with SLO guarantees that takes into account random and correlated hardware failures, datacenter maintenance, and heterogeneous hardware. In this paper, we describe how Facebook's region-scale Resource Allowance System (RAS) addresses these issues and provides guaranteed capacity. RAS uses a capacity abstraction called reservation to represent a set of servers dynamically assigned to a logical cluster. We take a two-level approach to scale resource allocation to all datacenters in a region, where a mixed-integer-programming solver continuously optimizes server-to-reservation assignments off the critical path, and a traditional container allocator does real-time placement of containers on servers in a reservation. As a relatively new component of Facebook's 10-year old cluster manager Twine, RAS has been running in production for almost two years, continuously optimizing the allocation of millions of servers to thousands of reservations. We describe the design of RAS and share our experience of deploying it at scale.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3477132.3483578',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Grand Challenges in Immersive Analytics',\n",
       "  'authors': \"['Barrett Ens', 'Benjamin Bach', 'Maxime Cordeil', 'Ulrich Engelke', 'Marcos Serrano', 'Wesley Willett', 'Arnaud Prouzeau', 'Christoph Anthes', 'Wolfgang Büschel', 'Cody Dunne', 'Tim Dwyer', 'Jens Grubert', 'Jason H. Haga', 'Nurit Kirshenbaum', 'Dylan Kobayashi', 'Tica Lin', 'Monsurat Olaosebikan', 'Fabian Pointecker', 'David Saffo', 'Nazmus Saquib', 'Dieter Schmalstieg', 'Danielle Albers Szafir', 'Matt Whitlock', 'Yalong Yang']\",\n",
       "  'date': 'May 2021',\n",
       "  'source': \"CHI '21: Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems\",\n",
       "  'abstract': 'Immersive Analytics is a quickly evolving field that unites several areas such as visualisation, immersive environments, and human-computer interaction to support human data analysis with emerging technologies. This research has thrived over the past years with multiple workshops, seminars, and a growing body of publications, spanning several conferences. Given the rapid advancement of interaction technologies and novel application domains, this paper aims toward a broader research agenda to enable widespread adoption. We present 17 key research challenges developed over multiple sessions by a diverse group of 24 international experts, initiated from a virtual scientific workshop at ACM CHI 2020. These challenges aim to coordinate future work by providing a systematic roadmap of current directions and impending hurdles to facilitate productive and effective applications for Immersive Analytics.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3411764.3446866',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'WasmAndroid: a cross-platform runtime for native programming languages on Android (WIP paper)',\n",
       "  'authors': \"['Elliott Wen', 'Gerald Weber', 'Suranga Nanayakkara']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': 'LCTES 2021: Proceedings of the 22nd ACM SIGPLAN/SIGBED International Conference on Languages, Compilers, and Tools for Embedded Systems',\n",
       "  'abstract': \"Open-source hardware such as RISC-V has been gaining substantial momentum. Recently, they have begun to embrace Google's Android operating system to leverage its software ecosystem. Despite the encouraging progress, a challenging issue arises: a majority of Android applications are written in native languages and need to be recompiled to target new hardware platforms. Unfortunately, this recompilation process is not scalable because of the explosion of new hardware platforms. To address this issue, we present WasmAndroid, a high-performance cross-platform runtime for native programming languages on Android. WasmAndroid only requires developers to compile their source code to WebAssembly, an efficient and portable bytecode format that can be executed everywhere without additional reconfiguration. WasmAndroid can also trans-pile existing application binaries to WebAssembly when source code is not available. WebAssembly's language model is very different from C/C++ and this mismatch leads to many unique implementation challenges. In this paper, we provide workable solutions and conduct a preliminary system evaluation. We show that WasmAndroid provides acceptable performance to execute native applications in a cross-platform manner.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3461648.3463849',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Switches for HIRE: resource scheduling for data center in-network computing',\n",
       "  'authors': \"['Marcel Blöcher', 'Lin Wang', 'Patrick Eugster', 'Max Schmidt']\",\n",
       "  'date': 'April 2021',\n",
       "  'source': \"ASPLOS '21: Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'The recent trend towards more programmable switching hardware in data centers opens up new possibilities for distributed applications to leverage in-network computing (INC). Literature so far has largely focused on individual application scenarios of INC, leaving aside the problem of coordinating usage of potentially scarce and heterogeneous switch resources among multiple INC scenarios, applications, and users. The traditional model of resource pools of isolated compute containers does not fit an INC-enabled data center.   This paper describes HIRE, a Holistic INC-aware Resource managEr which allows for server-local and INC resources to be coordinated in a unified manner. HIRE introduces a novel flexible resource (meta-)model to address heterogeneity, resource interchangeability, and non-linear resource requirements, and integrates dependencies between resources and locations in a unified cost model, cast as a min-cost max-flow problem. In absence of prior work, we compare HIRE against variants of state-of-the-art schedulers retrofitted to handle INC requests. Experiments with a workload trace of a 4000 machine cluster show that HIRE makes better use of INC resources by serving 8-30% more INC requests, while at the same time reducing network detours by 20%, and reducing tail placement latency by 50%.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3445814.3446760',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Sunflower: an environment for standardized communication of IoMusT',\n",
       "  'authors': \"['Rômulo Vieira', 'Flávio Schiavoni']\",\n",
       "  'date': 'September 2021',\n",
       "  'source': \"AM '21: Proceedings of the 16th International Audio Mostly Conference\",\n",
       "  'abstract': 'The Internet of Musical Things (IoMusT) area, although recent, has well-defined aspects concerning musical practice via the network. However, several challenges are also present, from those related to musical and artistic practice, even those dealing with environmental and social issues. From a computational point of view, the main dilemmas revolve around the lack of resources to deal with heterogeneity and the lack of standard in the communication of the devices that make up this scenario. Therefore, this paper presents Sunflower, a tool inspired by the Pipes-and-Filters architecture that allows communication between different objects, and focuses on its usage protocol. Its layered structure is also presented, showing the types of data, messages, and musical things present in each one of them. After all, the tests and results that certify to the functionality of this environment are demonstrated.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3478384.3478414',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Llama: A Heterogeneous & Serverless Framework for Auto-Tuning Video Analytics Pipelines',\n",
       "  'authors': \"['Francisco Romero', 'Mark Zhao', 'Neeraja J. Yadwadkar', 'Christos Kozyrakis']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"SoCC '21: Proceedings of the ACM Symposium on Cloud Computing\",\n",
       "  'abstract': 'The proliferation of camera-enabled devices and large video repositories has led to a diverse set of video analytics applications. These applications rely on video pipelines, represented as DAGs of operations, to transform videos, process extracted metadata, and answer questions like, \"Is this intersection congested?\" The latency and resource efficiency of pipelines can be optimized using configurable knobs for each operation (e.g., sampling rate, batch size, or type of hardware used). However, determining efficient configurations is challenging because (a) the configuration search space is exponentially large, and (b) the optimal configuration depends on users\\' desired latency and cost targets, (c) input video contents may exercise different paths in the DAG and produce a variable amount intermediate results. Existing video analytics and processing systems leave it to the users to manually configure operations and select hardware resources. We present Llama: a heterogeneous and serverless framework for auto-tuning video pipelines. Given an end-to-end latency target, Llama optimizes for cost efficiency by (a) calculating a latency target for each operation invocation, and (b) dynamically running a cost-based optimizer to assign configurations across heterogeneous hardware that best meet the calculated per-invocation latency target. This makes the problem of auto-tuning large video pipelines tractable and allows us to handle input-dependent behavior, conditional branches in the DAG, and execution variability. We describe the algorithms in Llama and evaluate it on a cloud platform using serverless CPU and GPU resources. We show that compared to state-of-the-art cluster and serverless video analytics and processing systems, Llama achieves 7.8x lower latency and 16x cost reduction on average.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3472883.3486972',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'MultiScatter: Multistatic Backscatter Networking for Battery-Free Sensors',\n",
       "  'authors': \"['Mohamad Katanbaf', 'Ali Saffari', 'Joshua R. Smith']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"SenSys '21: Proceedings of the 19th ACM Conference on Embedded Networked Sensor Systems\",\n",
       "  'abstract': 'Realizing the vision of ubiquitous battery-free sensing has proven to be challenging, mainly due to the practical energy and range limitations of current wireless communication systems. To address this, we design the first wide-area and scalable backscatter network with multiple receivers (RX) and transmitters (TX) base units to communicate with battery-free sensor nodes. Our system circumvents the inherent limitations of backscatter systems -including the limited coverage area, frequency-dependent operability, and sensor node limitations in handling network tasks- by introducing several coordination techniques between the base units starting from a single RX-TX pair to networks with many RX and TX units. We build low-cost RX and TX base units and battery-free sensor nodes with multiple sensing modalities and evaluate the performance of the MultiScatter system in various deployments. Our evaluation shows that we can successfully communicate with battery-free sensor nodes across 23400 ft2 of a two-floor educational complex using 5 RX and 20 TX units, costing $569. Also, we show that the aggregated throughput of the backscatter network increases linearly as the number of RX units and the network coverage grows.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3485730.3485939',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'On Efficient Oblivious Wavelength Assignments for Programmable Wide-Area Topologies',\n",
       "  'authors': \"['Thomas Fenz', 'Klaus-Tycho Foerster', 'Stefan Schmid']\",\n",
       "  'date': 'December 2021',\n",
       "  'source': \"ANCS '21: Proceedings of the Symposium on Architectures for Networking and Communications Systems\",\n",
       "  'abstract': 'Given the explosively growing traffic related to data-centric applications and AI, especially to and from the cloud, it is crucial to make the best use of the given resources of wide-area backbone networks (WANs). An intriguing approach to improve both efficiency and performance of WANs is to render networks more adaptive and \"demand-aware\", on the physical layer: innovative programmable wide-area topologies support dynamic wavelength assignments. This is enabled by the application of colorless and directionless Reconfigurable Optical Add/Drop Multiplexers (CD ROADM), and by leveraging the capabilities of software-defined controllers. This paper investigates the benefit of such fully dynamic wavelength assignments in programmable WAN topologies, compared to an oblivious wavelength assignment. To this end, we also propose a new demand-oblivious strategy to optimize the capacity of a WAN. Considering both real and synthetic scenarios, we find that our proposed demand-oblivious strategy can perform close to dynamic approaches with respect to throughput, without entailing reconfiguration costs.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3493425.3502753',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'WNOS: Enabling Principled Software-Defined Wireless Networking',\n",
       "  'authors': \"['Zhangyu Guan', 'Lorenzo Bertizzolo', 'Emrecan Demirors', 'Tommaso Melodia']\",\n",
       "  'date': 'None',\n",
       "  'source': 'IEEE/ACM Transactions on Networking',\n",
       "  'abstract': 'This article investigates the basic design principles for a new <italic>Wireless Network Operating System (WNOS), a radically different approach to software-defined</italic> networking (SDN) for <italic>infrastructure-less</italic> wireless networks. Departing from well-understood approaches inspired by OpenFlow, WNOS provides the network designer with an abstraction hiding (i) the lower-level details of the wireless protocol stack and (ii) the distributed nature of the network operations. Based on this abstract representation, the WNOS takes network control programs written on a centralized, high-level view of the network and automatically generates distributed cross-layer control programs based on distributed optimization theory that are executed by each individual node on an abstract representation of the radio hardware. We first discuss the main architectural principles of WNOS. Then, we discuss a new approach to automatically generate solution algorithms for each of the resulting subproblems in an automated fashion. Finally, we illustrate a prototype implementation of WNOS on software-defined radio devices and test its effectiveness by considering specific cross-layer control problems. Experimental results indicate that, based on the automatically generated distributed control programs, WNOS achieves 18&#x0025;, 56&#x0025; and 80.4&#x0025; utility gain in networks with low, medium and high levels of interference; maybe more importantly, we illustrate how the global network behavior can be controlled by modifying a few lines of code on a centralized abstraction.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/TNET.2021.3064824',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Towards practical quantum applications using hybrid problem solving techniques',\n",
       "  'authors': \"['Mehdi Bozzo-Rey', 'Robert Loredo', 'Hausi A. Müller', 'Sean Wagner', 'Ulrike Stege', 'Prashanti Priya Angara']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"CASCON '21: Proceedings of the 31st Annual International Conference on Computer Science and Software Engineering\",\n",
       "  'abstract': 'Quantum computing is transitioning from scientific research to a quantum technology industry. Much progress is still needed to solve industry relevant problems and achieve quantum advantage. Moving towards quantum computing readiness is now an urgent goal shared across many industry verticals, academic institutions and governments. Hybrid models, algorithms and architectures is now seen as way to overcome the limitations of Noisy Intermediate Scale Quantum (NISQ) devices for near-term quantum computation. CASCON 2021 features a 2-day quantum computing workshop, which discusses using hybrid problem solving techniques as a way to move towards practical quantum applications, featuring speakers from, or related to the Canadian ecosystem, including start-up companies and academic research programs. The key goal of this workshop is to discuss the state of these hybrid problem solving techniques, their potential but also challenges.',\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3507788.3507854',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Octopus: a practical and versatile wideband MIMO sensing platform',\n",
       "  'authors': \"['Zhe Chen', 'Tianyue Zheng', 'Jun Luo']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"MobiCom '21: Proceedings of the 27th Annual International Conference on Mobile Computing and Networking\",\n",
       "  'abstract': 'Radio frequency (RF) technologies have achieved a great success in data communication. In recent years, pervasive RF signals are further exploited for sensing; RF sensing has since attracted attentions from both academia and industry. Existing developments mainly employ commodity Wi-Fi hardware or rely on sophisticated SDR platforms. While promising in many aspects, there still remains a gap between lab prototypes and real-life deployments. On one hand, due to its narrow bandwidth and communication-oriented design, Wi-Fi sensing offers a coarse sensing granularity and its performance is very unstable in harsh real-world environments. On the other hand, SDR-based designs may hardly be adopted in practice due to its large size and high cost. To this end, we propose, design, and implement Octopus, a compact and flexible wideband MIMO sensing platform, built using commercial-grade low-power impulse radio. Octopus provides a standalone and fully programmable RF sensing solution; it allows for quick algorithm design and application development, and it specifically leverages the wideband radio to achieve a competent and robust performance in practice. We evaluate the performance of Octopus via micro-benchmarking, and further demonstrate its applicability using representative RF sensing applications, including passive localization, vibration sensing, and human/object imaging.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3447993.3483267',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Characterising the Role of Pre-Processing Parameters in Audio-based Embedded Machine Learning',\n",
       "  'authors': \"['Wiebke Toussaint', 'Akhil Mathur', 'Aaron Yi Ding', 'Fahim Kawsar']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"SenSys '21: Proceedings of the 19th ACM Conference on Embedded Networked Sensor Systems\",\n",
       "  'abstract': \"When deploying machine learning (ML) models on embedded and IoT devices, performance encompasses more than an accuracy metric: inference latency, energy consumption, and model fairness are necessary to ensure reliable performance under heterogeneous and resource-constrained operating conditions. To this end, prior research has studied model-centric approaches, such as tuning the hyperparameters of the model during training and later applying model compression techniques to tailor the model to the resource needs of an embedded device. In this paper, we take a data-centric view of embedded ML and study the role that pre-processing parameters in the data pipeline can play in balancing the various performance metrics of an embedded ML system. Through an in-depth case study with audio-based keyword spotting (KWS) models, we show that pre-processing parameter tuning is a remarkable tool that model developers can adopt to trade-off between a model's accuracy, fairness, and system efficiency, as well as to make an embedded ML model resilient to unseen deployment conditions.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3485730.3493448',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Design and Implementation of English Online Translation Software Based on Intelligent Proofreading System',\n",
       "  'authors': \"['Jing Qiu']\",\n",
       "  'date': 'May 2021',\n",
       "  'source': 'ICAIIS 2021: 2021 2nd International Conference on Artificial Intelligence and Information Systems',\n",
       "  'abstract': 'With the continuous development of modern information technology, the ways and means of learning English have become more diversified. Especially, the emergence of English electronic dictionary makes English learning more and more simple. This paper puts forward the design idea of English online translation software based on intelligent proofreading system. Firstly, the semantic ontology model is created by creating semantic ontology model and phrase translation combination translation algorithm. In addition, the overall system is designed. The hardware and software of the system are designed through the overall architecture of the system, and the hardware design is mainly the search module, so as to improve the search accuracy. In the process of software module, it mainly includes information transmission model, system network topology design and system function design, and can support translation and proofreading of multiple languages. Finally, the designed system is tested. The test results show that this method is accurate and intelligent in English automatic translation.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3469213.3471370',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': '3U-EdgeAI: Ultra-Low Memory Training, Ultra-Low Bitwidth Quantization, and Ultra-Low Latency Acceleration',\n",
       "  'authors': \"['Yao Chen', 'Cole Hawkins', 'Kaiqi Zhang', 'Zheng Zhang', 'Cong Hao']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"GLSVLSI '21: Proceedings of the 2021 on Great Lakes Symposium on VLSI\",\n",
       "  'abstract': 'The deep neural network (DNN) based AI applications on the edge require both low-cost computing platforms and high-quality services. However, the limited memory, computing resources, and power budget of the edge devices constrain the effectiveness of the DNN algorithms. Developing edge-oriented AI algorithms and implementations (e.g., accelerators) is challenging. In this paper, we summarize our recent efforts for efficient on-device AI development from three aspects, including both training and inference. First, we present on-device training with ultra-low memory usage. We propose a novel rank-adaptive tensor-based tensorized neural network model, which offers orders-of-magnitude memory reduction during training. Second, we introduce an ultra-low bitwidth quantization method for DNN model compression, achieving the state-of-the-art accuracy under the same compression ratio. Third, we introduce an ultra-low latency DNN accelerator design, practicing the software/hardware co-design methodology. This paper emphasizes the importance and efficacy of training, quantization and accelerator design, and calls for more research breakthroughs in the area for AI on the edge.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3453688.3461738',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Game-theoretic modeling of DDoS attacks in cloud computing',\n",
       "  'authors': \"['Kaho Wan', 'Joel Coffman']\",\n",
       "  'date': 'December 2021',\n",
       "  'source': \"UCC '21: Proceedings of the 14th IEEE/ACM International Conference on Utility and Cloud Computing\",\n",
       "  'abstract': \"The benefits of cloud computing have attracted many organizations to migrate their IT infrastructures into the cloud. In an infrastructure as a service (IaaS) model, the cloud service provider offers services to multiple consumers using shared physical hardware resources. However, by sharing a cloud environment with other consumers, organizations may also share security risks with their cotenants. Distributed denial of service (DDoS) attacks are considered one of the major security threats in cloud computing. Without a proper defense mechanism, an attack against one tenant can also affect the availability of cotenants. This work uses a game-theoretic approach to analyze the interactions between various entities when the cloud is under attack. The resulting Nash equilibrium shows that collateral damage to cotenants is unlikely if the cloud service provider is unbiased and chooses a rational strategy, but the Nash equilibrium can change when the cloud service provider does not treat cloud consumers equally. The cloud service provider's bias can influence its strategy selection and create a situation where untargeted users suffer unnecessary collateral damage from DDoS attacks.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3468737.3494093',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Adapting Surprise Minimizing Reinforcement Learning Techniques for Transactive Control',\n",
       "  'authors': \"['William Arnold', 'Tarang Srivastava', 'Lucas Spangher', 'Utkarsha Agwan', 'Costas Spanos']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"e-Energy '21: Proceedings of the Twelfth ACM International Conference on Future Energy Systems\",\n",
       "  'abstract': \"Optimizing prices for energy demand response requires a flexible controller with ability to navigate complex environments. We propose a reinforcement learning controller with surprise minimizing modifications in its architecture. We suggest that surprise minimization can be used to improve learning speed, taking advantage of predictability in peoples' energy usage. Our architecture performs well in a simulation of energy demand response. We propose this modification to improve functionality and savin a large-scale experiment.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3447555.3466590',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A Holistic Solution for Reliability of 3D Parallel Systems',\n",
       "  'authors': \"['Javad Bagherzadeh', 'Aporva Amarnath', 'Jielun Tan', 'Subhankar Pal', 'Ronald G. Dreslinski']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Journal on Emerging Technologies in Computing Systems',\n",
       "  'abstract': 'Monolithic 3D technology is emerging as a promising solution that can bring massive opportunities, but the gains can be hindered due to the reliability issues exaggerated by high temperature. Conventional reliability solutions focus on one specific feature and assume that the other required features would be provided by different solutions. Hence, this assumption has resulted in solutions that are proposed in isolation of each other and fail to consider the overall compatibility and the implied overheads of multiple isolated solutions for one system.This article proposes a holistic reliability management engine, R2D3, for post-Moore’s M3D parallel systems that have low yield and high failure rate. The proposed engine, comprising a controller, reconfigurable crossbars, and detection circuitry, provides concurrent single-replay detection and diagnosis, fault-mitigating repair, and aging-aware lifetime management at runtime. This holistic view enables us to create a solution that is highly effective while achieving a low overhead. Our solution achieves 96% coverage of defect; reduces Vth degradation by 53%, leading to a 78% performance improvement on average over 8 years for an eight-core system; and ultimately yields a 2.16× longer mean-time-to-failure (MTTF) while incurring an overhead of 7.4% in area, 6.5% in power, and an 8.2% decrease in frequency.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3488900',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A data-driven approach to optimize building energy performance and thermal comfort using machine learning models',\n",
       "  'authors': \"['Ziqi Chen', 'Zhuoang Tao', 'Aiwei Chang']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"ICCIR '21: Proceedings of the 2021 1st International Conference on Control and Intelligent Robotics\",\n",
       "  'abstract': \"Buildings account for 30% of the world's total energy consumption, and heating, ventilation and air conditioning systems account for more than 70% of the world's total energy consumption. People pay more and more attention to the energy efficiency improvement of building energy saving system, especially HVAC system. Open-plan office is one of the most popular office types in recent decades, which can not only improve communication efficiency, but also save considerable construction costs. But it can't satisfy everyone's comfort requirements, especially indoor air temperature and relative humidity. In this paper, a data-driven thermal comfort model is established based on ASHRAE Global Thermal Comfort Database II. Two machine learning algorithms for building thermal comfort models are studied: support vector machine (SVM) and random forest. The model optimizes energy consumption while ensuring thermal comfort of commercial buildings. Under the thermal comfort condition, the purpose of energy saving is achieved by controlling the indoor temperature setting value. We also set up a co-simulation model of building energy consumption, compared the benchmark control strategy with the optimal control strategy by using the data-driven thermal comfort model, and analyzed the economic benefits of the enterprise by using the whole-life cycle cost analysis method. The results have shown the optimized control strategy outperforms the baseline owing to better thermal comfort prediction performances with machine learning. Therefore, this paper contributes to intelligent human building interaction areas with artificial intelligence.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3473714.3473794',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Super-efficient super resolution for fast adversarial defense at the edge',\n",
       "  'authors': \"['Kartikeya Bhardwaj', 'Dibakar Gope', 'James Ward', 'Paul Whatmough', 'Danny Loh']\",\n",
       "  'date': 'March 2022',\n",
       "  'source': \"DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe\",\n",
       "  'abstract': 'Autonomous systems are highly vulnerable to a variety of adversarial attacks on Deep Neural Networks (DNNs). Training-free model-agnostic defenses have recently gained popularity due to their speed, ease of deployment, and ability to work across many DNNs. To this end, a new technique has emerged for mitigating attacks on image classification DNNs, namely, preprocessing adversarial images using super resolution - upscaling low-quality inputs into high-resolution images. This defense requires running both image classifiers and super resolution models on constrained autonomous systems. However, super resolution incurs a heavy computational cost. Therefore, in this paper, we investigate the following question: Does the robustness of image classifiers suffer if we use tiny super resolution models? To answer this, we first review a recent work called Super-Efficient Super Resolution (SESR) [1] that achieves similar or better image quality than prior art while requiring 2X to 330X fewer Multiply-Accumulate (MAC) operations. We demonstrate that despite being orders of magnitude smaller than existing models, SESR achieves the same level of robustness as significantly larger networks. Finally, we estimate end-to-end performance of super resolution-based defenses on a commercial Arm Ethos-U55 micro-NPU. Our findings show that SESR achieves nearly 3X higher FPS than a baseline while achieving similar robustness.',\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3539845.3539945',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Exploring the potential of context-aware dynamic CPU undervolting',\n",
       "  'authors': \"['Emmanouil Maroudas', 'Spyros Lalis', 'Nikolaos Bellas', 'Christos D. Antonopoulos']\",\n",
       "  'date': 'May 2021',\n",
       "  'source': \"CF '21: Proceedings of the 18th ACM International Conference on Computing Frontiers\",\n",
       "  'abstract': 'CPU operation at sub-nominal voltage levels has been researched to reduce the power and energy consumption of computer systems. While it is possible to determine a safe undervolting level for each application, typically only the most conservative setting is applied statically across all workloads. In this paper, we go a step further and investigate the gains that can be achieved by dynamically and transparently changing the level of CPU undervolting at runtime. To enable this functionality, we design and implement a novel, OS-level, context-aware dynamic undervolting mechanism, able to decide and apply voltage levels according to the specific tolerance of each workload that executes on a multicore CPU at a particular time. Our mechanism can further differentiate between the user- and kernel-level code executed within the same application thread, enabling the exploitation of differences in their undervolting potential. User- and kernel-level code have inherently different characteristics, yet in previous work have never been characterized individually. Our experiments, on an Intel x86-64 multicore show that the proposed approach can reduce the average CPU power consumption by 5.58%/30.05% compared to static undervolting and the nominal voltage level, respectively. Finally, we provide indicative estimates for the gains that could be achieved in future CPU architectures with multiple, per-core voltage domains.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3457388.3458658',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Exploring the cost and performance benefits of AWS step functions using a data processing pipeline',\n",
       "  'authors': \"['Anil Mathew', 'Vasilios Andrikopoulos', 'Frank J. Blaauw']\",\n",
       "  'date': 'December 2021',\n",
       "  'source': \"UCC '21: Proceedings of the 14th IEEE/ACM International Conference on Utility and Cloud Computing\",\n",
       "  'abstract': 'In traditional cloud computing, dedicated hardware is substituted by dynamically allocated, utility-oriented resources such as virtualized servers. While cloud services are following the pay-as-you-go pricing model, resources are billed based on instance allocation and not on the actual usage, leading the customers to be charged needlessly. In serverless computing, as exemplified by the Function-as-a-Service (FaaS) model where functions are the basic resources, functions are typically not allocated or charged until invoked or triggered. Functions are not applications, however, and to build compelling serverless applications they frequently need to be orchestrated with some kind of application logic. A major issue emerging by the use of orchestration is that it complicates further the already complex billing model used by FaaS providers, which in combination with the lack of granular billing and execution details offered by the providers makes the development and evaluation of serverless applications challenging. Towards shedding some light into this matter, in this work we extensively evaluate the state-of-the-art function orchestrator AWS Step Functions (ASF) with respect to its performance and cost. For this purpose we conduct a series of experiments using a serverless data processing pipeline application developed as both ASF Standard and Express workflows. Our results show that Step Functions using Express workflows are economical when running short-lived tasks with many state transitions. In contrast, Standard workflows are better suited for long-running tasks, offering in addition detailed debugging and logging information. However, even if the behavior of the orchestrated AWS Lambda functions influences both types of workflows, Step Functions realized as Express workflows get impacted the most by the phenomena affecting Lambda functions.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3468737.3494084',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A distributed, decoupled system for losslessly streaming dynamic light probes to thin clients',\n",
       "  'authors': \"['Michael Stengel', 'Zander Majercik', 'Benjamin Boudaoud', 'Morgan McGuire']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"MMSys '21: Proceedings of the 12th ACM Multimedia Systems Conference\",\n",
       "  'abstract': 'We present a networked, high-performance graphics system that combines dynamic, high-quality, ray traced global illumination computed on a server with direct illumination and primary visibility computed on a client. This approach provides many of the image quality benefits of real-time ray tracing on low-power and legacy hardware, while maintaining a low latency response and mobile form factor. As opposed to streaming full frames from rendering servers to end clients, our system distributes the graphics pipeline over a network by computing diffuse global illumination on a remote machine. Diffuse global illumination is computed using a recent irradiance volume representation combined with a new lossless, HEVC-based, hardware-accelerated encoding, and a perceptually-motivated update scheme. Our experimental implementation streams thousands of irradiance probes per second and requires less than 50 Mbps of throughput, reducing the consumed bandwidth by 99.4% when streaming at 60 Hz compared to traditional lossless texture compression. The bandwidth reduction achieved with our approach allows higher quality and lower latency graphics than state-of-the-art remote rendering via video streaming. In addition, our split-rendering solution decouples remote computation from local rendering and so does not limit local display update rate or display resolution.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3458305.3463379',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'QDiff: differential testing of quantum software stacks',\n",
       "  'authors': \"['Jiyuan Wang', 'Qian Zhang', 'Guoqing Harry Xu', 'Miryung Kim']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"ASE '21: Proceedings of the 36th IEEE/ACM International Conference on Automated Software Engineering\",\n",
       "  'abstract': \"Over the past few years, several quantum software stacks (QSS) have been developed in response to rapid hardware advances in quantum computing. A QSS includes a quantum programming language, an optimizing compiler that translates a quantum algorithm written in a high-level language into quantum gate instructions, a quantum simulator that emulates these instructions on a classical device, and a software controller that sends analog signals to a very expensive quantum hardware based on quantum circuits. In comparison to traditional compilers and architecture simulators, QSSes are difficult to tests due to the probabilistic nature of results, the lack of clear hardware specifications, and quantum programming complexity. This work devises a novel differential testing approach for QSSes, named QDIFF with three major innovations: (1) We generate input programs to be tested via semantics-preserving, source to source transformation to explore program variants. (2) We speed up differential testing by filtering out quantum circuits that are not worthwhile to execute on quantum hardware by analyzing static characteristics such as a circuit depth, 2-gate operations, gate error rates, and T1 relaxation time. (3) We design an extensible equivalence checking mechanism via distribution comparison functions such as Kolmogorov-Smirnov test and cross entropy. We evaluate QDiff with three widely-used open source QSSes: Qiskit from IBM, Cirq from Google, and Pyquil from Rigetti. By running QDiff on both real hardware and quantum simulators, we found several critical bugs revealing potential instabilities in these platforms. QDiff's source transformation is effective in producing semantically equivalent yet not-identical circuits (i.e., 34% of trials), and its filtering mechanism can speed up differential testing by 66%.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/ASE51524.2021.9678792',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Cache-aware Sparse Patterns for the Factorized Sparse Approximate Inverse Preconditioner',\n",
       "  'authors': \"['Sergi Laut', 'Ricard Borrell', 'Marc Casas']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"HPDC '21: Proceedings of the 30th International Symposium on High-Performance Parallel and Distributed Computing\",\n",
       "  'abstract': 'Conjugate Gradient is a widely used iterative method to solve linear systems Ax=b with matrix A being symmetric and positive definite. Part of its effectiveness relies on finding a suitable preconditioner that accelerates its convergence. Factorized Sparse Approximate Inverse (FSAI) preconditioners are a prominent and easily parallelizable option. An essential element of a FSAI preconditioner is the definition of its sparse pattern, which constraints the approximation of the inverse A-1. This definition is generally based on numerical criteria. In this paper we introduce complementary architecture-aware criteria to increase the numerical effectiveness of the preconditioner without incurring in significant performance costs. In particular, we define cache-aware pattern extensions that do not trigger additional cache misses when accessing vector x in the y=Ax Sparse Matrix-Vector (SpMV) kernel. As a result, we obtain very significant reductions in terms of average solution time ranging between 12.94% and 22.85% on three different architectures - Intel Skylake, POWER9 and A64FX - over a set of 72 test matrices.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3431379.3460642',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'SoundStream: An End-to-End Neural Audio Codec',\n",
       "  'authors': \"['Neil Zeghidour', 'Alejandro Luebs', 'Ahmed Omran', 'Jan Skoglund', 'Marco Tagliasacchi']\",\n",
       "  'date': 'None',\n",
       "  'source': 'IEEE/ACM Transactions on Audio, Speech and Language Processing',\n",
       "  'abstract': 'We present <italic>SoundStream</italic>, a novel neural audio codec that can efficiently compress speech, music and general audio at bitrates normally targeted by speech-tailored codecs. <italic>SoundStream</italic> relies on a model architecture composed by a fully convolutional encoder/decoder network and a residual vector quantizer, which are trained jointly end-to-end. Training leverages recent advances in text-to-speech and speech enhancement, which combine adversarial and reconstruction losses to allow the generation of high-quality audio content from quantized embeddings. By training with structured dropout applied to quantizer layers, a single model can operate across variable bitrates from 3 kbps to 18 kbps, with a negligible quality loss when compared with models trained at fixed bitrates. In addition, the model is amenable to a low latency implementation, which supports streamable inference and runs in real time on a smartphone CPU. In subjective evaluations using audio at 24 kHz sampling rate, <italic>SoundStream</italic> at 3 kbps outperforms Opus at 12 kbps and approaches EVS at 9.6 kbps. Moreover, we are able to perform joint compression and enhancement either at the encoder or at the decoder side with no additional latency, which we demonstrate through background noise suppression for speech.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/TASLP.2021.3129994',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Threat-modeling-guided Trust-based Task Offloading for Resource-constrained Internet of Things',\n",
       "  'authors': \"['Matthew Bradbury', 'Arshad Jhumka', 'Tim Watson', 'Denys Flores', 'Jonathan Burton', 'Matthew Butler']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Sensor Networks',\n",
       "  'abstract': 'There is an increasing demand for Internet of Things (IoT) networks consisting of resource-constrained devices executing increasingly complex applications. Due to these resource constraints, IoT devices will not be able to execute expensive tasks. One solution is to offload expensive tasks to resource-rich edge nodes, which requires a framework that facilitates the selection of suitable edge nodes to perform task offloading. Therefore, in this article, we present a novel trust-model-driven system architecture, based on behavioral evidence, that is suitable for resource-constrained IoT devices and supports computation offloading. We demonstrate the viability of the proposed architecture with an example deployment of the Beta Reputation System trust model on real hardware to capture node behaviors. The open environment of edge-based IoT networks means that threats against edge nodes can lead to deviation from expected behavior. Hence, we perform a threat modeling to identify such threats. The proposed system architecture includes threat handling mechanisms that provide security properties such as confidentiality, authentication, and non-repudiation of messages in required scenarios and operate within the resource constraints. We evaluate the efficacy of the threat handling mechanisms and identify future work for the standards used.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3510424',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Integration of parallel I/O library and flash native accelerators: an evaluation of SIONlib with IME',\n",
       "  'authors': \"['Konstantinos Chasapis', 'Jean-Thomas Acquaviva', 'Sebastian Lührs']\",\n",
       "  'date': 'April 2021',\n",
       "  'source': \"CHEOPS '21: Proceedings of the Workshop on Challenges and Opportunities of Efficient and Performant Storage Systems\",\n",
       "  'abstract': \"As illustrated by the IO500 ranking [9] performance and scalability of storage systems have improved dramatically in the recent years. Most of these progresses are due to novel hardware and architectures. The resulting solutions tend to be more complex and harder to manage from an end-user perspective. This work proposes to address simultaneously the user experience and the performance issue. The solution is based on DDN's Infinite Memory Engine (IME) [12], an all Flash distributed cache that sits between the compute nodes and the parallel file system. IME delivers extreme performance for bulk data and complex I/O patterns but does not accelerate metadata traffic. To shield the end-user from the additional complexity of dealing with an extra storage-tier, IME accesses are embedded within SIONlib. SIONlib is a parallel I/O library, which has the ability to provide a task local like access scheme for a single file in the underlying file system that allows to reduce the number of metadata operations. This combined construction with IME and SIONlib allows end-users to obtain the Flash level of performance transparently for their applications and also to receive a performance boost on metadata operations. In respect of traditional file systems, according to the partest benchmark suite, the solution improves performance over 1.8 times on a single node and up to 4 times for multiple nodes. Moreover, experimenting with SIONlib parameters we observe that the resulting performances are more robust towards different data layout patterns and transfer sizes than with traditional file systems.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3439839.3458736',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'In-depth analyses of unified virtual memory system for GPU accelerated computing',\n",
       "  'authors': \"['Tyler Allen', 'Rong Ge']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"SC '21: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis\",\n",
       "  'abstract': 'The abstraction of a shared memory space over separate CPU and GPU memory domains has eased the burden of portability for many HPC codebases. However, users pay for the ease of use provided by systems-managed memory space with a moderate-to-high performance overhead. NVIDIA Unified Virtual Memory (UVM) is presently the primary real-world implementation of such abstraction and offers a functionally equivalent testbed for a novel in-depth performance study for both UVM and future Linux Heterogeneous Memory Management (HMM) compatible systems. The continued advocation for UVM and HMM motivates the improvement of the underlying system. We focus on a UVM-based system and investigate the root causes of the UVM overhead, which is a non-trivial task due to the complex interactions of multiple hardware and software constituents and the requirement of targeted analysis methodology. In this paper, we take a deep dive into the UVM system architecture and the internal behaviors of page fault generation and servicing. We reveal specific GPU hardware limitations using targeted benchmarks to uncover driver functionality as a real-time system when processing the resultant workload. We further provide a quantitative evaluation of fault handling for various applications under different scenarios, including prefetching and oversubscription. We find that the driver workload is dependent on the interactions among application access patterns, GPU hardware constraints, and Host OS components. We determine that the cost of host OS components is significant and present across implementations, warranting close attention. This study serves as a proxy for future shared memory systems such as those that interface with HMM.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3458817.3480855',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Improved Lite Audio-Visual Speech Enhancement',\n",
       "  'authors': \"['Shang-Yi Chuang', 'Hsin-Min Wang', 'Yu Tsao']\",\n",
       "  'date': 'None',\n",
       "  'source': 'IEEE/ACM Transactions on Audio, Speech and Language Processing',\n",
       "  'abstract': 'Numerous studies have investigated the effectiveness of audio-visual multimodal learning for speech enhancement (AVSE) tasks, seeking a solution that uses visual data as auxiliary and complementary input to reduce the noise of noisy speech signals. Recently, we proposed a lite audio-visual speech enhancement (LAVSE) algorithm for a car-driving scenario. Compared to conventional AVSE systems, LAVSE requires less online computation and to some extent solves the user privacy problem on facial data. In this study, we extend LAVSE to improve its ability to address three practical issues often encountered in implementing AVSE systems, namely, the additional cost of processing visual data, audio-visual asynchronization, and low-quality visual data. The proposed system is termed improved LAVSE (iLAVSE), which uses a convolutional recurrent neural network architecture as the core AVSE model. We evaluate iLAVSE on the Taiwan Mandarin speech with video dataset. Experimental results confirm that compared to conventional AVSE systems, iLAVSE can effectively overcome the aforementioned three practical issues and can improve enhancement performance. The results also confirm that iLAVSE is suitable for real-world scenarios, where high-quality audio-visual sensors may not always be available.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/TASLP.2022.3153265',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A First Look at Energy Consumption of NB-IoT in the Wild: Tools and Large-Scale Measurement',\n",
       "  'authors': \"['Deliang Yang', 'Xuan Huang', 'Jun Huang', 'Xiangmao Chang', 'Guoliang Xing', 'Yang Yang']\",\n",
       "  'date': 'None',\n",
       "  'source': 'IEEE/ACM Transactions on Networking',\n",
       "  'abstract': 'Recent years have seen a widespread deployment of NB-IoT networks for massive machine-to-machine communication in the emerging 5G era. Unfortunately, the key aspects of NB-IoT networks, such as radio access performance and power consumption have not been well-understood due to lack of effective tools and closed nature of operational cellular infrastructure. In this paper, we develop NB-Scope - the first hardware NB-IoT diagnostic tool that supports fine-grained fusion of power and protocol traces. We then conduct a large-scale field measurement study consisting of 30 nodes deployed at over 1,200 locations in 4 regions during a period of three months. Our in-depth analysis of the collected 49 GB traces showed that NB-IoT nodes yield significantly imbalanced energy consumption in the wild, up to a ratio of 75:1, which may lead to short battery lifetime and frequent network partition. Such a high performance variance can be attributed to several key factors including diverse network coverage levels, long tail power profile, and excessive control message repetitions. We then explore the optimization of NB-IoT base station settings on a software-defined eNodeB testbed, and suggest several important design aspects that can be considered by future NB-IoT specifications and chipsets.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/TNET.2021.3096656',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'NEUROTEC I: neuro-inspired artificial intelligence technologies for the electronics of the future',\n",
       "  'authors': \"['Melvin Galicia', 'Stephan Menzel', 'Farhad Merchant', 'Maximilian Müller', 'Hsin-Yu Chen', 'Qing-Tai Zhao', 'Felix Cüppers', 'Abdur R. Jalil', 'Qi Shu', 'Peter Schüffelgen', 'Gregor Mussler', 'Carsten Funck', 'Christian Lanius', 'Stefan Wiefels', 'Moritz von Witzleben', 'Christopher Bengel', 'Nils Kopperberg', 'Tobias Ziegler', 'R. Walied Ahmad', 'Alexander Krüger', 'Leticia Pöhls', 'Regina Dittmann', 'Susanne Hoffmann-Eifert', 'Vikas Rana', 'Detlev Grützmacher', 'Matthias Wuttig', 'Dirk Wouters', 'Andrei Vescan', 'Tobias Gemmeke', 'Joachim Knoch', 'Max Lemme', 'Rainer Leupers', 'Rainer Waser']\",\n",
       "  'date': 'March 2022',\n",
       "  'source': \"DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe\",\n",
       "  'abstract': 'The field of neuromorphic computing is approaching an era of rapid adoption driven by the urgent need of a substitute for the von Neumann computing architecture. NEUROTEC I: \"Neuro-inspired Artificial Intelligence Technologies for the Electronics of the Future\" project is an initiative sponsored by the German Federal Ministry of Education and Research (BMBF for its initials in German), that aims to effectively advance the foundations for the utilization and exploitation of neuromorphic computing. NEUROTEC I stands at its successful \"final stage\" driven by the collaboration from more than 8 institutes from the Jülich Research Center and the RWTH Aachen University, as well as collaboration from several high-tech industry partners. The NEUROTEC I project considers the field interplay among materials, circuits, design and simulation tools. This paper provides an overview of the project\\'s overall structure and discusses the scientific achievements of its individual activities.',\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3539845.3540068',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Tap: an app framework for dynamically composable mobile systems',\n",
       "  'authors': \"['Naser AlDuaij', 'Jason Nieh']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"MobiSys '21: Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services\",\n",
       "  'abstract': 'As smartphones and tablets have become ubiquitous, there is a growing demand for apps that can enable users to collaboratively use multiple mobile systems. We present Tap, a framework that makes it easy for users to dynamically compose collections of mobile systems and developers to write apps that make use of those impromptu collections. Tap users control the composition by simply tapping systems together for discovery and authentication. The physical interaction mimics and supports ephemeral user interactions without the need for tediously exchanging user contact information such as phone numbers or email addresses. Tapping triggers a simple NFC-based mechanism to exchange connectivity information and security credentials that works across heterogeneous networks and requires no user accounts or cloud infrastructure support. Tap makes it possible for apps to use existing mobile platform APIs across multiple mobile systems by virtualizing data sources so that local and remote data sources can be combined together upon tapping. Virtualized data sources can be hardware or software features, including media, clipboard, calendar events, and devices such as cameras and microphones. Leveraging existing mobile platform APIs makes it easy for developers to write apps that use hardware and software features across dynamically composed collections of mobile systems. We have implemented a Tap prototype that allows apps to make use of both unmodified Android and iOS systems. We have modified and implemented various apps using Tap to demonstrate that it is easy to use and can enable apps to provide powerful new functionality by leveraging multiple mobile systems. Our results show that Tap has good performance, even for high-bandwidth features, and is user and developer friendly.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3458864.3467678',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A study of persistent memory bugs in the Linux kernel',\n",
       "  'authors': \"['Duo Zhang', 'Om Rameshwar Gatla', 'Wei Xu', 'Mai Zheng']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"SYSTOR '21: Proceedings of the 14th ACM International Conference on Systems and Storage\",\n",
       "  'abstract': 'Persistent memory (PM) technologies have inspired a wide range of PM-based system optimizations. However, building correct PM-based systems is difficult due to the unique characteristics of PM hardware. To better understand the challenges as well as the opportunities to address them, this paper presents a comprehensive study of PM-related bugs in the Linux kernel. By analyzing 1,350 PM-related kernel patches in depth, we derive multiple insights in terms of PM patch categories, PM bug patterns, consequences, and fix strategies. We hope our results could contribute to the development of effective PM bug detectors and robust PM-based systems.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3456727.3463783',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Emerging ExG-based NUI Inputs in Extended Realities: A\\xa0Bottom-up\\xa0Survey',\n",
       "  'authors': \"['Kirill A. Shatilov', 'Dimitris Chatzopoulos', 'Lik-Hang Lee', 'Pan Hui']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Interactive Intelligent Systems',\n",
       "  'abstract': 'Incremental and quantitative improvements of two-way interactions with extended realities (XR) are contributing toward a qualitative leap into a state of XR ecosystems being efficient, user-friendly, and widely adopted. However, there are multiple barriers on the way toward the omnipresence of XR; among them are the following: computational and power limitations of portable hardware, social acceptance of novel interaction protocols, and usability and efficiency of interfaces. In this article, we overview and analyse novel natural user interfaces based on sensing electrical bio-signals that can be leveraged to tackle the challenges of XR input interactions. Electroencephalography-based brain-machine interfaces that enable thought-only hands-free interaction, myoelectric input methods that track body gestures employing electromyography, and gaze-tracking electrooculography input interfaces are the examples of electrical bio-signal sensing technologies united under a collective concept of ExG. ExG signal acquisition modalities provide a way to interact with computing systems using natural intuitive actions enriching interactions with XR. This survey will provide a bottom-up overview starting from (i) underlying biological aspects and signal acquisition techniques, (ii) ExG hardware solutions, (iii) ExG-enabled applications, (iv) discussion on social acceptance of such applications and technologies, as well as (v) research challenges, application directions, and open problems; evidencing the benefits that ExG-based Natural User Interfaces inputs can introduce to the area of XR.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3457950',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Lucid: a language for control in the data plane',\n",
       "  'authors': \"['John Sonchack', 'Devon Loehr', 'Jennifer Rexford', 'David Walker']\",\n",
       "  'date': 'August 2021',\n",
       "  'source': \"SIGCOMM '21: Proceedings of the 2021 ACM SIGCOMM 2021 Conference\",\n",
       "  'abstract': \"Programmable switch hardware makes it possible to move fine-grained control logic inside the network data plane, improving performance for a wide range of applications. However, applications with integrated control are inherently hard to write in existing data-plane programming languages such as P4. This paper presents Lucid, a language that raises the level of abstraction for putting control functionality in the data plane. Lucid introduces abstractions that make it easy to write sophisticated data-plane applications with interleaved packet-handling and control logic, specialized type and syntax systems that prevent programmer bugs related to data-plane state, and an open-sourced compiler that translates Lucid programs into P4 optimized for the Intel Tofino. These features make Lucid general and easy to use, as we demonstrate by writing a suite of ten different data-plane applications in Lucid. Working prototypes take well under an hour to write, even for a programmer without prior Tofino experience, have around 10x fewer lines of code compared to P4, and compile efficiently to real hardware. In a stateful firewall written in Lucid, we find that moving control from a switch's CPU to its data-plane processor using Lucid reduces the latency of performance-sensitive operations by over 300X.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3452296.3472903',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'UltraDepth: Exposing High-Resolution Texture from Depth Cameras',\n",
       "  'authors': \"['Zhiyuan Xie', 'Xiaomin Ouyang', 'Xiaoming Liu', 'Guoliang Xing']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"SenSys '21: Proceedings of the 19th ACM Conference on Embedded Networked Sensor Systems\",\n",
       "  'abstract': 'Time-of-flight (ToF) depth cameras have been increasingly adopted in various real-world applications, e.g., used with RGB cameras for advanced computer vision tasks like 3-D mapping or deployed alone in privacy-sensitive applications such as sleep monitoring. In this paper, we propose UltraDepth, the first system that can expose high-resolution texture from depth maps captured by off-the-shelf ToF cameras, simply by introducing a distorting IR source. The exposed texture information can significantly augment depth-based applications. Moreover, such a capability can be used to launch privacy attacks, which poses a major concern due to the prominence of ToF cameras. To design UltraDepth, we present an in-depth analysis on the impact of the distorting IR light on the distance measurement. We further show that, the reflection properties (reflectivity and incidence angle) of the objects will be encoded in the distorted depth map and hence can be leveraged to reveal texture of objects in UltraDepth. We then propose two practical implementations of UltraDepth, i.e., reflection-based and external IR-based implementations. Our extensive real-world experiments show that, the depth maps output by UltraDepth achieve 89.06%, 99.33%, 81.25% mean accuracy in object detection, face recognition and character recognition, respectively, which offers over 10x improvement over the ordinary depth maps and even approaches the performance of RGB and IR images in a number of scenarios. The findings of this work provide key insights for new research on depth-related computer vision and security of depth sensing devices.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3485730.3485927',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Toward confidential cloud computing',\n",
       "  'authors': \"['Mark Russinovich', 'Manuel Costa', 'Cédric Fournet', 'David Chisnall', 'Antoine Delignat-Lavaud', 'Sylvan Clebsch', 'Kapil Vaswani', 'Vikas Bhatia']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': 'Communications of the ACM',\n",
       "  'abstract': 'Extending hardware-enforced cryptographic protection to data while in use.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3453930',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Pantomime: Mid-Air Gesture Recognition with Sparse Millimeter-Wave Radar Point Clouds',\n",
       "  'authors': \"['Sameera Palipana', 'Dariush Salami', 'Luis A. Leiva', 'Stephan Sigg']\",\n",
       "  'date': 'None',\n",
       "  'source': 'Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies',\n",
       "  'abstract': 'We introduce Pantomime, a novel mid-air gesture recognition system exploiting spatio-temporal properties of millimeter-wave radio frequency (RF) signals. Pantomime is positioned in a unique region of the RF landscape: mid-resolution mid-range high-frequency sensing, which makes it ideal for motion gesture interaction. We configure a commercial frequency-modulated continuous-wave radar device to promote spatial information over the temporal resolution by means of sparse 3D point clouds and contribute a deep learning architecture that directly consumes the point cloud, enabling real-time performance with low computational demands. Pantomime achieves 95% accuracy and 99% AUC in a challenging set of 21 gestures articulated by 41 participants in two indoor environments, outperforming four state-of-the-art 3D point cloud recognizers. We further analyze the effect of the environment in 5 different indoor environments, the effect of articulation speed, angle, and the distance of the person up to 5m. We have publicly made available the collected mmWave gesture dataset consisting of nearly 22,000 gesture instances along with our radar sensor configuration, trained models, and source code for reproducibility. We conclude that pantomime is resilient to various input conditions and that it may enable novel applications in industrial, vehicular, and smart home scenarios.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3448110',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Get The Best of the Three Worlds: Real-Time Neural Image Compression in a Non-GPU Environment',\n",
       "  'authors': \"['Zekun Zheng', 'Xiaodong Wang', 'Xinye Lin', 'Shaohe Lv']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"MM '21: Proceedings of the 29th ACM International Conference on Multimedia\",\n",
       "  'abstract': \"Lossy image compression always faces a tradeoff between rate-distortion performance and compression/decompression speed. With the advent of neural image compression, hardware (GPU) becomes the new vertex in the tradeoff triangle. By resolving the high GPU dependency and improving the low speed of neural models, this paper proposes two non-GPU models that get the best of the three worlds. First, the CPU-friendly Independent Separable Down-Sampling (ISD) and Up-Sampling (ISU) modules are proposed to lighten the network while ensuring a large receptive field. Second, an asymmetric autoencoder architecture is adopted to boost the decoding speed. At last, the Inverse Quantization Residual (IQR) module is proposed to reduce the error caused by quantization. In terms of rate-distortion performance, our network surpasses the state-of-the-art real-time GPU neural compression work at medium and high bit rates. In terms of speed, our model's compression and decompression speeds surpass all other traditional compression methods except JPEG, using only CPUs. In terms of hardware, the proposed models are CPU friendly and perform stably well in a non-GPU environment. The code is publicly available at https://github.com/kengchikengchi/FasiNet.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3474085.3475667',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Full-credit flow control: a novel technique to implement deadlock-free adaptive routing',\n",
       "  'authors': \"['Yi Dai', 'Kai Lu', 'Sheng Ma', 'Junsheng Chang']\",\n",
       "  'date': 'March 2022',\n",
       "  'source': \"DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe\",\n",
       "  'abstract': 'Deadlock-free adaptive routing is extensively adopted in interconnection networks to improve communication bandwidth and reduce latency. However, existing deadlock-free flow control schemes either underutilize memory resources due to inefficient buffer management for simple hardware implementations, or rely on complicated coordination and synchronization mechanisms with high hardware complexity. In this work, we solve the deadlock problem from a different perspective by considering the deadlock as a lack of credit. With minor modifications of the credit accumulation procedure, our proposed full-credit flow control (FFC) ensures atomic buffer usage only based on local credit status while making full use of the buffer space. FFC can be easily integrated in the industrial router to achieve deadlock freedom with less area and power consumption, but 112% higher throughput, compared to the critical bubble scheme (CBS). We further propose a credit reservation strategy to eliminate the escape virtual channel (VC) cost for fully adaptive routing implementation. The synthesizing results demonstrate that FFC along with credit reservation (FFC-CR) can reduce the area by 29% and power consumption by 26% compared with CBS.',\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3539845.3540085',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'The Impact of Terrestrial Radiation on FPGAs in Data Centers',\n",
       "  'authors': \"['Andrew M. Keller', 'Michael J. Wirthlin']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Reconfigurable Technology and Systems',\n",
       "  'abstract': 'Field programmable gate arrays (FPGAs) are used in large numbers in data centers around the world. They are used for cloud computing and computer networking. The most common type of FPGA used in data centers are re-programmable SRAM-based FPGAs. These devices offer potential performance and power consumption savings. A single device also carries a small susceptibility to radiation-induced soft errors, which can lead to unexpected behavior. This article examines the impact of terrestrial radiation on FPGAs in data centers. Results from artificial fault injection and accelerated radiation testing on several data-center-like FPGA applications are compared. A new fault injection scheme provides results that are more similar to radiation testing. Silent data corruption (SDC) is the most commonly observed failure mode followed by FPGA unavailable and host unresponsive. A hypothetical deployment of 100,000 FPGAs in Denver, Colorado, will experience upsets in configuration memory every half-hour on average and SDC failures every 0.5–11 days on average.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3457198',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'CIB-HIER: Centralized Input Buffer Design in Hierarchical High-radix Routers',\n",
       "  'authors': \"['Cunlu Li', 'Dezun Dong', 'Shazhou Yang', 'Xiangke Liao', 'Guangyu Sun', 'Yongheng Liu']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Architecture and Code Optimization',\n",
       "  'abstract': 'Hierarchical organization is widely used in high-radix routers to enable efficient scaling to higher switch port count. A general-purpose hierarchical router must be symmetrically designed with the same input buffer depth, resulting in a large amount of unused input buffers due to the different link lengths. Sharing input buffers between different input ports can improve buffer utilization, but the implementation overhead also increases with the number of shared ports. Previous work allowed input buffers to be shared among all router ports, which maximizes the buffer utilization but also introduces higher implementation complexity. Moreover, such design can impair performance when faced with long packets, due to the head-of-line blocking in intermediate buffers.In this work, we explain that sharing unused buffers between a subset of router ports is a more efficient design. Based on this observation, we propose Centralized Input Buffer Design in Hierarchical High-radix Routers (CIB-HIER), a novel centralized input buffer design for hierarchical high-radix routers. CIB-HIER integrates multiple input ports onto a single tile and organizes all unused input buffers in the tile as a centralized input buffer. CIB-HIER only allows the centralized input buffer to be shared between ports on the same tile, without introducing additional intermediate virtual channels or global scheduling circuits. Going beyond the basic design of CIB-HIER, the centralized input buffer can be used to relieve the head-of-line blocking caused by shallow intermediate buffers, by stashing long packets in the centralized input buffer. Experimental results show that CIB-HIER is highly effective and can significantly increase the throughput of high-radix routers.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3468062',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'SoK: Remote Power Analysis',\n",
       "  'authors': \"['Macarena C. Martínez-Rodríguez', 'Ignacio M. Delgado-Lozano', 'Billy Bob Brumley']\",\n",
       "  'date': 'August 2021',\n",
       "  'source': \"ARES '21: Proceedings of the 16th International Conference on Availability, Reliability and Security\",\n",
       "  'abstract': 'In recent years, numerous attacks have appeared that aim to steal secret information from their victim using the power side-channel vector, yet without direct physical access. These attacks are called Remote Power Attacks or Remote Power Analysis, utilizing resources that are natively present inside the victim environment. However, there is no unified definition about the limitations that a power attack requires to be defined as remote. This paper aims to propose a unified definition and concrete threat models to clearly differentiate remote power attacks from non-remote ones. Additionally, we collect the main remote power attacks performed so far from the literature, and the principal proposed countermeasures to avoid them. The search of such countermeasures denoted a clear gap in preventing remote power attacks at the technical level. Thus, the academic community must face an important challenge to avoid this emerging threat, given the clear room for improvement that should be addressed in terms of defense and security of devices that work with private information.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3465481.3465773',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'CIB-HIER: Centralized Input Buffer Design in Hierarchical High-radix Routers',\n",
       "  'authors': \"['Cunlu Li', 'Dezun Dong', 'Shazhou Yang', 'Xiangke Liao', 'Guangyu Sun', 'Yongheng Liu']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Architecture and Code Optimization',\n",
       "  'abstract': 'Hierarchical organization is widely used in high-radix routers to enable efficient scaling to higher switch port count. A general-purpose hierarchical router must be symmetrically designed with the same input buffer depth, resulting in a large amount of unused input buffers due to the different link lengths. Sharing input buffers between different input ports can improve buffer utilization, but the implementation overhead also increases with the number of shared ports. Previous work allowed input buffers to be shared among all router ports, which maximizes the buffer utilization but also introduces higher implementation complexity. Moreover, such design can impair performance when faced with long packets, due to the head-of-line blocking in intermediate buffers.In this work, we explain that sharing unused buffers between a subset of router ports is a more efficient design. Based on this observation, we propose Centralized Input Buffer Design in Hierarchical High-radix Routers (CIB-HIER), a novel centralized input buffer design for hierarchical high-radix routers. CIB-HIER integrates multiple input ports onto a single tile and organizes all unused input buffers in the tile as a centralized input buffer. CIB-HIER only allows the centralized input buffer to be shared between ports on the same tile, without introducing additional intermediate virtual channels or global scheduling circuits. Going beyond the basic design of CIB-HIER, the centralized input buffer can be used to relieve the head-of-line blocking caused by shallow intermediate buffers, by stashing long packets in the centralized input buffer. Experimental results show that CIB-HIER is highly effective and can significantly increase the throughput of high-radix routers.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3468062',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A Design of Zynq-based Medical Image Edge Detection Accelerator',\n",
       "  'authors': \"['Bin Li', 'Jingxian Chen', 'Xuejun Zhang', 'Xianfu Xu', 'Yini Wei', 'Deyu Kong']\",\n",
       "  'date': 'August 2021',\n",
       "  'source': \"ICBIP '21: Proceedings of the 6th International Conference on Biomedical Signal and Image Processing\",\n",
       "  'abstract': 'Edge detection technology plays an important role in medical image processing. Sobel operator edge detection is one of the commonly used edge detection operators. At present, most of the solutions using Sobel operator for edge detection of medical images are based on CPU and GPU. Processing speed can become a serious problem as image data increases. The acceleration effect of FPGA on edge detection is quite significant. However, the traditional Sobel edge detection scheme based on FPGA is developed by hardware description language, which has high requirements for developers and is very unfavorable to debugging. Using the Zynq series of C/C++ programming for acceleration can perfectly solve the above problems. However, the current Zynq-based Sobel operator edge detection research, only horizontal edge and vertical edge detection. In order to extract more edge details from different angles, we proposed an improved Sobel operator based on Zynq to detect edges. The performance of the proposed improved Sobel algorithm and the conventional Sobel algorithm on CPU and Zynq platform is compared and evaluated in detail. Experimental results show that the proposed scheme can extract more edge details and achieve satisfactory acceleration effect.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3484424.3484434',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Unsupervised Digit Recognition Using Cosine Similarity In A Neuromemristive Competitive Learning System',\n",
       "  'authors': \"['Bon Woong Ku', 'Catherine D. Schuman', 'Md Musabbir Adnan', 'Tiffany M. Mintz', 'Raphael Pooser', 'Kathleen E. Hamilton', 'Garrett S. Rose', 'Sung Kyu Lim']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Journal on Emerging Technologies in Computing Systems',\n",
       "  'abstract': 'This work addresses how to naturally adopt the l2-norm cosine similarity in the neuromemristive system and studies the unsupervised learning performance on handwritten digit image recognition. Proposed architecture is a two-layer fully connected neural network with a hard winner-take-all (WTA) learning module. For input layer, we propose single-spike temporal code that transforms input stimuli into the set of single spikes with different latencies and voltage levels. For a synapse model, we employ a compound memristor where stochastically switching binary-state memristors connected in parallel, which offers a reliable and scalable multi-state solution for synaptic weight storage. Hardware-friendly synaptic adaptation mechanism is proposed to realize spike-timing-dependent plasticity learning. Input spikes are sent out through those memristive synapses to each and every integrate-and-fire neuron in the fully connected output layer, where the hard WTA network motif introduces the competition based on cosine similarity for the given input stimuli. Finally, we present 92.64% accuracy performance on unsupervised digit recognition with only single-epoch MNIST dataset training via high-level simulations, including extensive analysis on the impact of system parameters.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3473036',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'The semantics of shared memory in Intel CPU/FPGA systems',\n",
       "  'authors': \"['Dan Iorga', 'Alastair F. Donaldson', 'Tyler Sorensen', 'John Wickerson']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': 'Proceedings of the ACM on Programming Languages',\n",
       "  'abstract': \"Heterogeneous CPU/FPGA devices, in which a CPU and an FPGA can execute together while sharing memory, are becoming popular in several computing sectors. In this paper, we study the shared-memory semantics of these devices, with a view to providing a firm foundation for reasoning about the programs that run on them. Our focus is on Intel platforms that combine an Intel FPGA with a multicore Xeon CPU. We describe the weak-memory behaviours that are allowed (and observable) on these devices when CPU threads and an FPGA thread access common memory locations in a fine-grained manner through multiple channels. Some of these behaviours are familiar from well-studied CPU and GPU concurrency; others are weaker still. We encode these behaviours in two formal memory models: one operational, one axiomatic. We develop executable implementations of both models, using the CBMC bounded model-checking tool for our operational model and the Alloy modelling language for our axiomatic model. Using these, we cross-check our models against each other via a translator that converts Alloy-generated executions into queries for the CBMC model. We also validate our models against actual hardware by translating 583 Alloy-generated executions into litmus tests that we run on CPU/FPGA devices; when doing this, we avoid the prohibitive cost of synthesising a hardware design per litmus test by creating our own 'litmus-test processor' in hardware. We expect that our models will be useful for low-level programmers, compiler writers, and designers of analysis tools. Indeed, as a demonstration of the utility of our work, we use our operational model to reason about a producer/consumer buffer implemented across the CPU and the FPGA. When the buffer uses insufficient synchronisation -- a situation that our model is able to detect -- we observe that its performance improves at the cost of occasional data corruption.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3485497',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'RedMulE: a compact FP16 matrix-multiplication accelerator for adaptive deep learning on RISC-V-based ultra-low-power SoCs',\n",
       "  'authors': \"['Yvan Tortorella', 'Luca Bertaccini', 'Davide Rossi', 'Luca Benini', 'Francesco Conti']\",\n",
       "  'date': 'March 2022',\n",
       "  'source': \"DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe\",\n",
       "  'abstract': \"The fast proliferation of extreme-edge applications using Deep Learning (DL) based algorithms required dedicated hardware to satisfy extreme-edge applications' latency, throughput, and precision requirements. While inference is achievable in practical cases, online finetuning and adaptation of general DL models are still highly challenging. One of the key stumbling stones is the need for parallel floating-point operations, which are considered unaffordable on sub-100mW extreme-edge SoCs. We tackle this problem with RedMulE (Reduced-precision matrix Multiplication Engine), a parametric low-power hardware accelerator for FP16 matrix multiplications - the main kernel of DL training and inference - conceived for tight integration within a cluster of tiny RISC-V cores based on the PULP (Parallel Ultra-Low-Power) architecture. In 22 nm technology, a 32-FMA RedMulE instance occupies just 0.07 mm2 (14% of an 8-core RISC-V cluster) and achieves up to 666MHz maximum operating frequency, for a throughput of 31.6 MAC/cycle (98.8% utilization). We reach a cluster-level power consumption of 43.5mW and a full-cluster energy efficiency of 688 16-bit GFLOPS/W. Overall, RedMulE features up to 4.65× higher energy efficiency and 22× speedup over SW execution on 8RISC-V cores.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3539845.3540099',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Low-power Near-data Instruction Execution Leveraging Opcode-based Timing Analysis',\n",
       "  'authors': \"['Tziouvaras Athanasios', 'Dimitriou Georgios', 'Stamoulis Georgios']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Architecture and Code Optimization',\n",
       "  'abstract': 'Traditional processor architectures utilize an external DRAM for data storage, while they also operate under worst-case timing constraints. Such designs are heavily constrained by the delay costs of the data transfer between the core pipeline and the DRAM, and they are incapable of exploiting the timing variations of their pipeline stages. In this work, we focus on a near-data processing methodology combined with a novel timing analysis technique that enables the adaptive frequency scaling of the core clock and boosts the performance of low-power designs. We propose a near-data processing and better-than-worst-case co-design methodology to efficiently move the instruction execution to the DRAM side and, at the same time, to allow the pipeline to operate at higher clock frequencies compared to the worst-case approach. To this end, we develop a timing analysis technique, which evaluates the timing requirements of individual instructions and we dynamically scale the clock frequency, according to the instructions types that currently occupy the pipeline. We evaluate the proposed methodology on six different RISC-V post-layout implementations using an HMC DRAM to enable the processing-in-memory (PIM) process. Results indicate an average speedup factor of 1.96× with a 1.6× reduction in energy consumption compared to a standard RISC-V PIM baseline implementation.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3504005',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Dual-Path Modeling With Memory Embedding Model for Continuous Speech Separation',\n",
       "  'authors': \"['Chenda Li', 'Zhuo Chen', 'Yanmin Qian']\",\n",
       "  'date': 'None',\n",
       "  'source': 'IEEE/ACM Transactions on Audio, Speech and Language Processing',\n",
       "  'abstract': 'Continuous speech separation (CSS) aims at separating overlap-free targets from a long, partially-overlapped recording. Though it has shown promising results, the origin CSS framework does not consider cross-window information and long-span dependency. To alleviate these limitations, this work introduces two novel methods to implicitly and explicitly capture the long-span knowledge, respectively. We firstly apply the dual-path (DP) modeling architecture for the CSS framework, where the within and across window information are jointly modeled by alternating stacked local-global processing modules. Secondly, to further capture the long-span dependency, we introduce a memory-based model for CSS. An additional memory pool is designed to extract embedding from each small window, and the inter-window commutation is established above the memory embedding pool through an attention mechanism. This memory-based model can precisely control what information needs to be transferred across the windows, thus leading to both improved modeling capacity and interpretability. The experimental results on the LibriCSS dataset show that both strategies can well capture the long-span information of the continuous speech and significantly improve system performance. Moreover, further improvements are observed with the integration of these two methods.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/TASLP.2022.3165712',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Two Methods to Describe New Shift Registers',\n",
       "  'authors': \"['Tao Wu']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"CSAE '21: Proceedings of the 5th International Conference on Computer Science and Application Engineering\",\n",
       "  'abstract': 'Shift registers are typical digital logic structures for integrated circuits and are widely used in hardware descriptions. They can be used as early units before memories and be replaced by them later. In multi-precision shift registers, the control signal has heavy loads, and the critical path appears if the input is complex. In this paper, two techniques for synthesis are proposed to reduce either the critical path or power consumption. The first method divides the shift register into 2 or 3 parts, while the second method applies word-based registers to act as the shift register. Both techniques localize the control signals and reduce the path delay.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3487075.3487111',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'ASIC Design Principle Course with Combination of Online-MOOC and Offline-Inexpensive FPGA Board',\n",
       "  'authors': \"['Zhixiong Di', 'Yongming Tang', 'Jiahua Lu', 'Zhaoyang Lv']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"GLSVLSI '21: Proceedings of the 2021 on Great Lakes Symposium on VLSI\",\n",
       "  'abstract': 'ASIC Design Principle (ASICDP) is a compulsory course for undergraduate majors in microelectronics and integrated circuits, and the focus of this paper is the teaching methods of online theoretical teaching and offline experimental teaching of this course. As is well known, in order to prevent and control COVID-19, the use of online platforms to carry out online teaching has attracted worldwide attention. In this paper, the teaching strategy \"Online-MOOC + Offline Inexpensive FPGA Board\" in ASICDP in the Spring 2020 semester is demonstrated, in where MOOC means Massive Open Online Course. The theoretical teaching content of ASICDP is entirely replicated from Hardware Acceleration Design Methodology (HADM) released by the present authors on \"China University MOOC,\" the largest MOOC platform in China. Meanwhile, with the support of the \"Xilinx & Ministry of Education University-Industry Collaborative Education Program,\" an FPGA development board called the \"Spartan Edge Accelerator Board\" (SEA Board) designed by the authors was used in the experimental teaching of the ASICDP. This method can be used to establish the linkage between online courses and offline experiments, and cultivate students\\' practical VLSI design and FPGA prototype verification skills. It is believed that for educators that want to improve courses related to ASIC design and FPGA prototype verification, Online-MOOC + Offline-Inexpensive FPGA Board is an effective method with lower cost that is easily promotable and replicated.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3453688.3461502',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Fast k-NN Graph Construction by GPU based NN-Descent',\n",
       "  'authors': \"['Hui Wang', 'Wan-Lei Zhao', 'Xiangxiang Zeng', 'Jianye Yang']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"CIKM '21: Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management\",\n",
       "  'abstract': 'NN-Descent is a classic k-NN graph construction approach. It is still widely employed in machine learning, computer vision, and information retrieval tasks due to its efficiency and genericness. However, the current design only works well on CPU. In this paper, NN-Descent has been redesigned to adapt to the GPU architecture. A new graph update strategy called selective update is proposed. It reduces the data exchange between GPU cores and GPU global memory significantly, which is the processing bottleneck under GPU computation architecture. This redesign leads to full exploitation of the parallelism of the GPU hardware. In the meantime, the genericness, as well as the simplicity of NN-Descent, are well-preserved. Moreover, a procedure that allows to k-NN graph to be merged efficiently on GPU is proposed. It makes the construction of high-quality k-NN graphs for out-of-GPU-memory datasets tractable. Our approach is 100-250× faster than the single-thread NN-Descent and is 2.5-5× faster than the existing GPU-based approaches as we tested on million as well as billion scale datasets.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3459637.3482344',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Robust and Attack Resilient Logic Locking with a High Application-Level Impact',\n",
       "  'authors': \"['Yuntao Liu', 'Michael Zuzak', 'Yang Xie', 'Abhishek Chakraborty', 'Ankur Srivastava']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Journal on Emerging Technologies in Computing Systems',\n",
       "  'abstract': 'Logic locking is a hardware security technique aimed at protecting intellectual property against security threats in the IC supply chain, especially those posed by untrusted fabrication facilities. Such techniques incorporate additional locking circuitry within an integrated circuit (IC) that induces incorrect digital functionality when an incorrect verification key is provided by a user. The amount of error induced by an incorrect key is known as the effectiveness of the locking technique. A family of attacks known as “SAT attacks” provide a strong mathematical formulation to find the correct key of locked circuits. To achieve high SAT resilience (i.e., complexity of SAT attacks), many conventional logic locking schemes fail to inject sufficient error into the circuit when the key is incorrect. For example, in the case of SARLock and Anti-SAT, there are usually very few (or only one) input minterms that cause any error at the circuit output. The state-of-the-art stripped functionality logic locking (SFLL) technique provides a wide spectrum of configurations that introduced a tradeoff between SAT resilience and effectiveness. In this work, we prove that such a tradeoff is universal among all logic locking techniques. To attain high effectiveness of locking without compromising SAT resilience, we propose a novel logic locking scheme, called Strong Anti-SAT (SAS). In addition to SAT attacks, removal-based attacks are another popular kind of attack formulation against logic locking where the attacker tries to identify and remove the locking structure. Based on SAS, we also propose Robust SAS (RSAS) that is resilient to removal attacks and maintains the same SAT resilience and effectiveness as SAS. SAS and RSAS have the following significant improvements over existing techniques. (1) We prove that the SAT resilience of SAS and RSAS against SAT attack is not compromised by increase in effectiveness. (2) In contrast to prior work that focused solely on the circuit-level locking impact, we integrate SAS-locked modules into an 80386 processor and show that SAS has a high application-level impact. (3) Our experiments show that SAS and RSAS exhibit better SAT resilience than SFLL and their effectiveness is similar to SFLL.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3446215',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Novel Approximate Multiplier Designs for Edge Detection Application',\n",
       "  'authors': \"['Yashaswi Mannepalli', 'Viraj Bharadwaj Korede', 'Madhav Rao']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"GLSVLSI '21: Proceedings of the 2021 on Great Lakes Symposium on VLSI\",\n",
       "  'abstract': 'Approximate computing in general has garnered much needed attention in the design community owing to high power saving benefits, and at the same time quick generation of results. Approximate computing as a design technique continues to offer design advantages which is recently ceased by the ever decreasing technology scaling. Approximate computing is mostly applied to arithmetic designs, that has resulted in significant research interests. The paper proposes a reliable and efficient approximate multiplier design, that uses optimized lower part constant OR adder (OLOCA) design and hardware optimized approximate adder with normal error distribution (HOAANED) separately as two variants. The two approximate multipliers derived from OLOCA adder and HOAANED adder were found to be highly power and footprint efficient, and in addition offers performance improvement over other approximate multipliers. The error characteristics for the proposed multiplier designs were evaluated and compared with the existing approximate multiplier design. The proposed multiplier design along with the existing ones were synthesized using 45 nm CMOS technology and results were analyzed. The proposed approximate multipliers were further explored for canny edge detection application, and results for different standard images were found to be highly acceptable showing 99.9% of outcome similar to exact multiplier design.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3453688.3461482',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Toward Polymorphic Internet of Things Receivers Through Real-Time Waveform-Level Deep Learning',\n",
       "  'authors': \"['Francesco Restuccia', 'Tommaso Melodia']\",\n",
       "  'date': 'September 2021',\n",
       "  'source': 'GetMobile: Mobile Computing and Communications',\n",
       "  'abstract': \"Wireless systems such as the Internet of Things (IoT) are changing the way we interact with the cyber and the physical world. As IoT systems become more and more pervasive, it is imperative to design wireless protocols that can effectively and efficiently support IoT devices and operations. On the other hand, today's IoT wireless systems are based on inflexible designs, which makes them inefficient and prone to a variety of wireless attacks. In this paper, we introduce the new notion of a deep learning-based polymorphic IoT receiver, able to reconfigure its waveform demodulation strategy itself in real time, based on the inferred waveform parameters. Our key innovation is the introduction of a novel embedded deep learning architecture that enables the solution of waveform inference problems, which is then integrated into a generalized hardware/software architecture with radio components and signal processing. Our polymorphic wireless receiver is prototyped on a custom-made software-defined radio platform. We show through extensive over-the-air experiments that the system achieves throughput within 87% of a perfect-knowledge Oracle system, thus demonstrating for the first time that polymorphic receivers are feasible.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3511285.3511294',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A community-driven approach to democratize access to satellite ground stations',\n",
       "  'authors': \"['Vaibhav Singh', 'Akarsh Prabhakara', 'Diana Zhang', 'Osman Yağan', 'Swarun Kumar']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"MobiCom '21: Proceedings of the 27th Annual International Conference on Mobile Computing and Networking\",\n",
       "  'abstract': 'Should you decide to launch a nano-satellite today in Low-Earth Orbit (LEO), the cost of renting ground station communication infrastructure is likely to significantly exceed your launch costs. While space launch costs have lowered significantly with innovative launch vehicles, private players, and smaller payloads, access to ground infrastructure remains a luxury. This is especially true for smaller LEO satellites that are only visible at any location for a few tens of minutes a day and whose signals are extremely weak, necessitating bulky and expensive ground station infrastructure. In this paper, we present a community-driven distributed reception paradigm for LEO satellite signals where signals received on many tiny handheld receivers (not necessarily deployed on rooftops but also indoors) are coherently combined to recover the desired signal. This is made possible by employing new synchronization and receiver orientation techniques that study satellite trajectories and leverage the presence of other ambient signals. We compare our results with a large commercial receiver deployed on a rooftop and show a 8 dB SNR increase both indoors and outdoors using 8 receivers, costing $38 per RF frontend.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3447993.3448630',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'LED-to-LED based VLC systems: developments and open problems',\n",
       "  'authors': \"['Muhammad Sarmad Mir', 'Behnaz Majlesein', 'Borja Genoves Guzman', 'Julio Rufo', 'Domenico Giustiniano']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"IoL '21: Proceedings of the Workshop on Internet of Lights\",\n",
       "  'abstract': 'Visible light communication (VLC) is an emerging short-range wireless communication technology using the unlicensed light spectrum. Light Emitting Diode (LED) is used as VLC transmitter, while photodiodes or image sensors are used as receiver, depending on the applications and hardware constraints. However, LEDs can be used not only as a transmitter, but also as a receiver in applications where cost is of primary concern. LED as a receiver is sensitive to a narrow band of wavelengths, it is robust to sunlight interference and widely available as compared to other light detectors. This paper surveys the potential and limitations of LED-to-LED communication. It also contributes to identifying the challenges and potential research directions in this rising area of interest.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3469264.3469805',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A golden age for computing frontiers, a dark age for computing education?',\n",
       "  'authors': \"['Christof Teuscher']\",\n",
       "  'date': 'May 2021',\n",
       "  'source': \"CF '21: Proceedings of the 18th ACM International Conference on Computing Frontiers\",\n",
       "  'abstract': 'There is no doubt that the body of knowledge spanned by the computing disciplines has gone through an unprecedented expansion, both in depth and breadth, over the last century. In this position paper, we argue that this expansion has led to a crisis in computing education: quite literally the vast majority of the topics of interest of this conference are not taught at the undergraduate level and most graduate courses will only scratch the surface of a few selected topics. But alas, industry is increasingly expecting students to be familiar with emerging topics, such as neuromorphic, probabilistic, and quantum computing, AI, and deep learning. We provide evidence for the rapid growth of emerging topics, highlight the decline of traditional areas, muse about the failure of higher education to adapt quickly, and delineate possible ways to avert the crisis by looking at how the field of physics dealt with significant expansions over the last centuries.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3457388.3458673',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Exploiting parallelism with vertex-clustering in processing-in-memory-based GCN accelerators',\n",
       "  'authors': \"['Yu Zhu', 'Zhenhua Zhu', 'Guohao Dai', 'Kai Zhong', 'Huazhong Yang', 'Yu Wang']\",\n",
       "  'date': 'March 2022',\n",
       "  'source': \"DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe\",\n",
       "  'abstract': 'Recently, Graph Convolutional Networks (GCNs) have shown powerful learning capabilities in graph processing tasks. Computing GCNs with conventional von Neumann architectures usually suffers from limited memory bandwidth due to the irregular memory access. Recent work has proposed Processing-In-Memory (PIM) architectures to overcome the bandwidth bottleneck in Convolutional Neural Networks (CNNs) by performing in-situ matrix-vector multiplication. However, the performance improvement and computation parallelism of existing CNN-oriented PIM architectures is hindered when performing GCNs because of the large scale and sparsity of graphs. To tackle these problems, this paper presents a parallelism enhancement framework for PIM-based GCN architectures. At the software level, we propose a fixed-point quantization method for GCNs, which reduces the PIM computation overhead with little accuracy loss. We also introduce the vertex clustering algorithm to the graph, minimizing the inter-cluster links and realizing cluster-level parallel computing on multi-core systems. At the hardware level, we design a Resistive Random Access Memory (RRAM) based multi-core PIM architecture for GCN, which supports the cluster-level parallelism. Besides, we propose a coarse-grained pipeline dataflow to cover the RRAM write costs and improve the GCN computation throughput. At the software/hardware interface level, we propose a PIM-aware GCN mapping strategy to achieve the optimal tradeoff between resource utilization and computation performance. We also propose edge dropping methods to reduce the inter-core communications with little accuracy loss. We evaluate our framework on typical datasets with multiple widely-used GCN models. Experimental results show that the proposed framework achieves 698×, 89×, and 41× speedup with 7108×, 255×, and 31× energy efficiency enhancement compared with CPUs, GPUs, and ASICs, respectively.',\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3539845.3540006',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Verification: can wifi backscatter replace RFID?',\n",
       "  'authors': \"['Farzan Dehbashi', 'Ali Abedi', 'Tim Brecht', 'Omid Abari']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"MobiCom '21: Proceedings of the 27th Annual International Conference on Mobile Computing and Networking\",\n",
       "  'abstract': 'WiFi backscatter communication has been proposed to enable battery-free sensors to transmit data using WiFi networks. The main advantage of WiFi backscatter technologies over RFID is that data from their tags can be read using existing WiFi infrastructures instead of specialized readers. This can potentially reduce the complexity and cost of deploying battery-free sensors. Despite extensive work in this area, none of the existing systems are in widespread use today. We hypothesize that this is because WiFi-based backscatter tags do not scale well and their range and capabilities are limited when compared with RFID. To test this hypothesis we conduct several real-world experiments. We compare WiFi backscatter and RFID technologies in terms of RF harvesting capabilities, throughput, range and scalability. Our results show that existing WiFi backscatter tags cannot rely on RF harvesting (as opposed to RFID tags) due to their high power consumption. We find that WiFi backscatter tags must be quite close to a WiFi device to work robustly in non-line-of-sight scenarios, limiting their operating range. Furthermore, our results show that some WiFi backscatter systems can cause significant interference for existing WiFi traffic and be affected by them since they do not perform carrier sensing.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3447993.3448622',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Design and Implementation of the Processing Circuit of the Miniaturized Multi-channel Ultrasonic Signal',\n",
       "  'authors': \"['YouDi Kong', 'YuHua Wu', 'GuangJie Wang', 'JianRui Zhao', 'HaiTao Wang', 'WeiXing Zhang']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"EITCE '21: Proceedings of the 2021 5th International Conference on Electronic Information Technology and Computer Engineering\",\n",
       "  'abstract': 'With the rapid development of the measurement technology and the sensor technology, the processing circuit of the multi-channel ultrasonic signal is more and more widely used in many fields. At present, the miniaturization technology of the circuit is a hot research problem, especially in the academia and industry. In the study, the transmitting and receiving circuit of the miniaturized multi-channel ultrasonic signal based on the integrated transceiver ultrasonic transducer is innovatively designed by combining the phase difference signal of the microcontroller with the new mode of the conventional multiplexer. The method has been tested and validated. It is concluded that the function of the transmission and reception of the ultrasonic signal can be completed through the design. The complexity of the processing circuit of the multi-channel ultrasonic signal is reduced. The miniaturization of the signal processing circuit is realized, and the multi-channel signal transmission of the ultrasonic transducer is ensured. The overall system which has some features such as simple architecture, small volume and light weight has wide prospect in the future market.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3501409.3501485',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Clobber-NVM: log less, re-execute more',\n",
       "  'authors': \"['Yi Xu', 'Joseph Izraelevitz', 'Steven Swanson']\",\n",
       "  'date': 'April 2021',\n",
       "  'source': \"ASPLOS '21: Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'Non-volatile memory allows direct access to persistent storage via a load/store interface. However, because the cache is volatile, cached updates to persistent state will be dropped after a power loss. Failure-atomicity NVM libraries provide the means to apply sets of writes to persistent state atomically. Unfortunately, most of these libraries impose significant overhead.  This work proposes Clobber-NVM, a failure-atomicity library that ensures data consistency by reexecution. Clobber-NVM’s novel logging strategy, clobber logging, records only those transaction inputs that are overwritten during transaction execution. Then, after a failure, it recovers to a consistent state by restoring overwritten inputs and reexecuting any interrupted transactions. Clobber-NVM utilizes a clobber logging compiler pass for identifying the minimal set of writes that need to be logged. Based on our experiments, classical undo logging logs up to 42.6X more bytes than Clobber-NVM, and requires 2.4X to 4.7X more expensive ordering instructions (e.g., clflush and sfence). Less logging leads to better performance: Relative to prior art, Clobber-NVM provides up to 2.5X performance improvement over Mnemosyne, 2.6X over Intel’s PMDK, and up to 8.1X over HP’s Atlas.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3445814.3446730',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'SNR: Squeezing Numerical Range Defuses Bit Error Vulnerability Surface in Deep Neural Networks',\n",
       "  'authors': \"['Elbruz Ozen', 'Alex Orailoglu']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Embedded Computing Systems',\n",
       "  'abstract': 'As deep learning algorithms are widely adopted, an increasing number of them are positioned in embedded application domains with strict reliability constraints. The expenditure of significant resources to satisfy performance requirements in deep neural network accelerators has thinned out the margins for delivering safety in embedded deep learning applications, thus precluding the adoption of conventional fault tolerance methods. The potential of exploiting the inherent resilience characteristics of deep neural networks remains though unexplored, offering a promising low-cost path towards safety in embedded deep learning applications. This work demonstrates the possibility of such exploitation by juxtaposing the reduction of the vulnerability surface through the proper design of the quantization schemes with shaping the parameter distributions at each layer through the guidance offered by appropriate training methods, thus delivering deep neural networks of high resilience merely through algorithmic modifications. Unequaled error resilience characteristics can be thus injected into safety-critical deep learning applications to tolerate bit error rates of up to \\\\(\\\\) at absolutely zero hardware, energy, and performance costs while improving the error-free model accuracy even further.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3477007',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A Distributed Real-time Scheduling System for Industrial Wireless Networks',\n",
       "  'authors': \"['Venkata P. Modekurthy', 'Abusayeed Saifullah', 'Sanjay Madria']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Embedded Computing Systems',\n",
       "  'abstract': 'The concept of Industry 4.0 introduces the unification of industrial Internet-of-Things (IoT), cyber physical systems, and data-driven business modeling to improve production efficiency of the factories. To ensure high production efficiency, Industry 4.0 requires industrial IoT to be adaptable, scalable, real-time, and reliable. Recent successful industrial wireless standards such as WirelessHART appeared as a feasible approach for such industrial IoT. For reliable and real-time communication in highly unreliable environments, they adopt a high degree of redundancy. While a high degree of redundancy is crucial to real-time control, it causes a huge waste of energy, bandwidth, and time under a centralized approach and are therefore less suitable for scalability and handling network dynamics. To address these challenges, we propose DistributedHART—a distributed real-time scheduling system for WirelessHART networks. The essence of our approach is to adopt local (node-level) scheduling through a time window allocation among the nodes that allows each node to schedule its transmissions using a real-time scheduling policy locally and online. DistributedHART obviates the need of creating and disseminating a central global schedule in our approach, thereby significantly reducing resource usage and enhancing the scalability. To our knowledge, it is the first distributed real-time multi-channel scheduler for WirelessHART. We have implemented DistributedHART and experimented on a 130-node testbed. Our testbed experiments as well as simulations show at least 85% less energy consumption in DistributedHART compared to existing centralized approach while ensuring similar schedulability.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3464429',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Research on Machine Translation and Computer Aided Translation Based on Cloud Computing',\n",
       "  'authors': \"['Xiaohui Zhang']\",\n",
       "  'date': 'September 2021',\n",
       "  'source': 'ICISCAE 2021: 2021 4th International Conference on Information Systems and Computer Aided Education',\n",
       "  'abstract': 'New breakthroughs have not been made in the principle and technology of machine translation, and high-quality automatic translation has not yet been realized. The core technology of computer-aided translation is translation memory technology. Although computer-aided translation has made considerable progress, the translation memory technology has not yet made a decisive breakthrough. Aiming at the problem that the translation time of machine-aided translation system is relatively long at present, the design of machine-aided translation system based on cloud computing is proposed. A new machine-aided translation system is designed by referring to the cloud computing model. The hardware of the system is divided into four layers: user layer, service layer, computing layer and storage layer. The storage structure, translation structure and retrieval structure are designed respectively. After that, the software and hardware of the system are designed. The test results show that the translation time of the machine-assisted translation system based on cloud computing is shorter than that of the traditional machine translation system.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3482632.3484009',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Development of an online resource integration system for computer aided aesthetic education by big data technology',\n",
       "  'authors': \"['Xinyun Du', 'Tsungshun Hsieh']\",\n",
       "  'date': 'September 2021',\n",
       "  'source': 'ICISCAE 2021: 2021 4th International Conference on Information Systems and Computer Aided Education',\n",
       "  'abstract': 'Big data technology is a currently emerging technology that plays an important role in driving online education in depth. The article discusses the development of an online resource integration system for university general aesthetic education based on big data technology, firstly, it introduces the system design and the main functional modules, including online resource management module, information information management module, and system maintenance module, introduces the main functions of the system, such as resource retrieval service, information interaction service, and resource utilization service, and finally, from the perspectives of hardware configuration, system architecture system integration and cost control are proposed.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3482632.3484076',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A flash-based current-mode IC to realize quantized neural networks',\n",
       "  'authors': \"['Kyler R. Scott', 'Cheng-Yen Lee', 'Sunil P. Khatri', 'Sarma Vrudhula']\",\n",
       "  'date': 'March 2022',\n",
       "  'source': \"DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe\",\n",
       "  'abstract': 'This paper presents a mixed-signal architecture for implementing Quantized Neural Networks (QNNs) using flash transistors to achieve extremely high throughput with extremely low power, energy and memory requirements. Its low resource consumption makes our design especially suited for use in edge devices. The network weights are stored in-memory using flash transistors, and nodes perform operations in the analog current domain. Our design can be programmed with any QNN whose hyperparameters (the number of layers, filters, or filter size, etc) do not exceed the maximum provisioned. Once the flash devices are programmed with a trained model and the IC is given an input, our architecture performs inference with zero access to off-chip memory. We demonstrate the robustness of our design under current-mode non-linearities arising from process and voltage variations. We test validation accuracy on the ImageNet dataset, and show that our IC suffers only 0.6% and 1.0% reduction in classification accuracy for Top-1 and Top-5 outputs, respectively. Our implementation results in a ~50× reduction in latency and energy when compared to a recently published mixed-signal ASIC implementation, with similar power characteristics. Our approach provides layer partitioning and node sharing possibilities, which allow us to trade off latency, power, and area amongst each other.',\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3539845.3540082',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Exploring planning and operations design space for EV charging stations',\n",
       "  'authors': \"['Sangyoung Park', 'Alma Pröbstl', 'Wanli Chang', 'Anuradha Annaswamy', 'Samarjit Chakraborty']\",\n",
       "  'date': 'March 2021',\n",
       "  'source': \"SAC '21: Proceedings of the 36th Annual ACM Symposium on Applied Computing\",\n",
       "  'abstract': 'EVs suffer from long charging times and short-drive ranges, limiting EV usage to daily short-range commuting rather than general purpose use. Among the candidates for EV charging infrastructures, the public EV charging station architecture has benefits in that it allows an efficient investment of costly equipments, and a long-range travel with multiple charging cycles. This paper focuses on an EC charging station architecture comprising PV panels, an energy storage system (ESS) and multiple fast-DC charging posts. Systematically deriving the optimal planning, i.e., determining the optimal sizes of these components, is a complicated problem as the EV charging station operations and planning are intertwined. In this paper, we derive EV charging station operation policies by formulating an average reward Markov decision process (MDP) maximization problem to synthesize controllers that maximize the operating income. Then, these controllers are used to evaluate the operating income, for the purpose of EV charging station planning. For efficient exploration of the design space, we perform a mixed search-based technique combining sequential quadratic programming (SQP) with a greedy algorithm. There will be significant gain in terms of long-term operating cost when the costs of ESS and PV panels continue to reduce in the future. Our solution framework is a helpful tool for such reasoning, and for identifying optimal planning and operation policies for public EV charging stations.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3412841.3441896',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Extending Intel-x86 consistency and persistency: formalising the semantics of Intel-x86 memory types and non-temporal stores',\n",
       "  'authors': \"['Azalea Raad', 'Luc Maranget', 'Viktor Vafeiadis']\",\n",
       "  'date': 'January 2022',\n",
       "  'source': 'Proceedings of the ACM on Programming Languages',\n",
       "  'abstract': 'Existing semantic formalisations of the Intel-x86 architecture cover only a small fragment of its available features that are relevant for the consistency semantics of multi-threaded programs as well as the persistency semantics of programs interfacing with non-volatile memory. We extend these formalisations to cover: (1) non-temporal writes, which provide higher performance and are used to ensure that updates are flushed to memory; (2) reads and writes to other Intel-x86 memory types, namely uncacheable, write-combined, and write-through; as well as (3) the interaction between these features. We develop our formal model in both operational and declarative styles, and prove that the two characterisations are equivalent. We have empirically validated our formalisation of the consistency semantics of these additional features and their subtle interactions by extensive testing on different Intel-x86 implementations.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3498683',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'EmSBoTScript: A Tiny Virtual Machine-Based Embedded Software Framework',\n",
       "  'authors': \"['Long Peng', 'Hao Xu', 'Jie Yu', 'Xiaodong Liu', 'Fei Guan']\",\n",
       "  'date': 'December 2021',\n",
       "  'source': \"CSAI '21: Proceedings of the 2021 5th International Conference on Computer Science and Artificial Intelligence\",\n",
       "  'abstract': 'Modern swarm and modular robotic systems can be composed of diverse and miniature hardware components. To deal with heterogeneity, researchers adopt a virtual machine (VM)-based approach to ease software programming and updating for robotic systems. However, current VM-based solutions neither consider resource-constrained devices, nor have limited capabilities. This paper introduces EmSBoTScript, a tiny VM-based robotic software framework that is tailored for heterogeneous and miniature platforms. We endow EmSBoTScript with features of CPU independence, low memory footprint, concurrency and synchronization. We elaborate its programming model, script language and VM architecture to show its novelty in this paper. Implementation details and benchmark results are also provided.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3507548.3507592',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'SecNVM: An Efficient and Write-Friendly Metadata Crash Consistency Scheme for Secure NVM',\n",
       "  'authors': \"['Mengya Lei', 'Fan Li', 'Fang Wang', 'Dan Feng', 'Xiaomin Zou', 'Renzhi Xiao']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Architecture and Code Optimization',\n",
       "  'abstract': 'Data security is an indispensable part of non-volatile memory (NVM) systems. However, implementing data security efficiently on NVM is challenging, since we have to guarantee the consistency of user data and the related security metadata. Existing consistency schemes ignore the recoverability of the SGX style integrity tree (SIT) and the access correlation between metadata blocks, thereby generating unnecessary NVM write traffic. In this article, we propose SecNVM, an efficient and write-friendly metadata crash consistency scheme for secure NVM. SecNVM utilizes the observation that for a lazily updated SIT, the lost tree nodes after a crash can be recovered by the corresponding child nodes in NVM. It reduces the SIT persistency overhead through a restrained write-back metadata cache and exploits the SIT inter-layer dependency for recovery. Next, leveraging the strong access correlation between the counter and DMAC, SecNVM improves the efficiency of security metadata access through a novel collaborative counter-DMAC scheme. In addition, it adopts a lightweight address tracker to reduce the cost of address tracking for fast recovery. Experiments show that compared to the state-of-the-art schemes, SecNVM improves the performance and decreases write traffic a lot, and achieves an acceptable recovery time.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3488724',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Sherlock: A Multi-Objective Design Space Exploration Framework',\n",
       "  'authors': \"['Quentin Gautier', 'Alric Althoff', 'Christopher L. Crutchfield', 'Ryan Kastner']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Design Automation of Electronic Systems',\n",
       "  'abstract': 'Design space exploration (DSE) provides intelligent methods to tune the large number of optimization parameters present in modern FPGA high-level synthesis tools. High-level synthesis parameter tuning is a time-consuming process due to lengthy hardware compilation times—synthesizing an FPGA design can take tens of hours. DSE helps find an optimal solution faster than brute-force methods without relying on designer intuition to achieve high-quality results. Sherlock is a DSE framework that can handle multiple conflicting optimization objectives and aggressively focuses on finding Pareto-optimal solutions. Sherlock integrates a model selection process to choose the regression model that helps reach the optimal solution faster. Sherlock designs a strategy based around the multi-armed bandit problem, opting to balance exploration and exploitation based on the learned and expected results. Sherlock can decrease the importance of models that do not provide correct estimates, reaching the optimal design faster. Sherlock is capable of tailoring its choice of regression models to the problem at hand, leading to a model that best reflects the application design space. We have tested the framework on a large dataset of FPGA design problems and found that Sherlock converges toward the set of optimal designs faster than similar frameworks.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3511472',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'The Control System Design for 600kV High Voltage Platform of HIAF Electron Cooler',\n",
       "  'authors': \"['Yunbin Zhou', 'Lijun Mao', 'Wei Zhang', 'Kaiming Yan', 'Xiaoming Ma', 'Mingrui Li']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"CSAE '21: Proceedings of the 5th International Conference on Computer Science and Application Engineering\",\n",
       "  'abstract': 'HIAF is the next generation heavy ion accelerator in China, it contains lots of sub facilities. The electron cooler is one of the most significant facilities in HIAF of Spectrometer Ring (SRing). With the electron cooler, SRing could generate high quality and high intensity ion beam for the experiments. The electron cooler in HIAF was designed to produce 600KeV electron beam. To achieve this design target, high voltage needs to reach 600kV maximum and ripple wave less than 1*10-4. A new control system was designed for the high voltage platform which uses cascaded transformer construction. The system makes use of XGS-PON network as the main means of communication. Zynq 7015 as the CPU of the embedded controller. The controller integrated 2 DAC ports with 100KS/S and 4 ADC ports with 200KS/S for high voltage modules setting and high voltage divider monitor, 8 low-speed ADC ports for auxiliary power supplies, and the environment sensors. The embedded Ubuntu Linux and the EPICS frameworks were programmed in the controller, all of the control parameters were sent through channel access protocol. No need for host computer to participate in control logic, only used for display. The hardware mentioned above interacts with the operating system through the FPGA part of Zynq. The FPGA is programed as a coprocessor for communicating, data processing and interlock control. Corresponding drivers are integrated in the Linux system at the same time.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3487075.3487085',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Separation of Powers in Federated Learning (Poster Paper)',\n",
       "  'authors': \"['Pau-Chen Cheng', 'Kevin Eykholt', 'Zhongshu Gu', 'Hani Jamjoom', 'K. R. Jayaram', 'Enriquillo Valdez', 'Ashish Verma']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"ResilientFL '21: Proceedings of the First Workshop on Systems Challenges in Reliable and Secure Federated Learning\",\n",
       "  'abstract': 'In federated learning (FL), model updates from mutually distrusting parties are aggregated in a centralized fusion server. The concentration of model updates simplifies FL\\'s model building process, but might lead to unforeseeable information leakage. This problem has become acute due to recent FL attacks that can reconstruct large fractions of training data from ostensibly \"sanitized\" model updates. In this paper, we re-examine the current design of FL systems under the new security model of reconstruction attacks. To break down information concentration, we build TRUDA, a new cross-silo FL system, employing a trustworthy and decentralized aggregation architecture. Based on the unique computational properties of model-fusion algorithms, we disassemble all exchanged model updates at the parameter-granularity and re-stitch them to form random partitions designated for multiple hardware-protected aggregators. Thus, each aggregator only has a fragmentary and shuffled view of model updates and is oblivious to the model architecture. The deployed security mechanisms in TRUDA can effectively mitigate training data reconstruction attacks, while still preserving the accuracy of trained models and keeping performance overheads low.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3477114.3488765',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'WaFFLe: Gated Cache-<underline>Wa</underline>ys with Per-Core <underline>F</underline>ine-Grained DV<underline>F</underline>S for Reduced On-Chip Temperature and <underline>Le</underline>akage Consumption',\n",
       "  'authors': \"['Shounak Chakraborty', 'Magnus Själander']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Architecture and Code Optimization',\n",
       "  'abstract': 'Managing thermal imbalance in contemporary chip multi-processors (CMPs) is crucial in assuring functional correctness of modern mobile as well as server systems. Localized regions with high activity, e.g., register files, ALUs, FPUs, and so on, experience higher temperatures than the average across the chip and are commonly referred to as hotspots. Hotspots affect functional correctness of the underlying circuitry and a noticeable increase in leakage power, which in turn generates heat in a self-reinforced cycle. Techniques that reduce the severity of or completely eliminate hotspots can maintain functional correctness along with improving performance of CMPs. Conventional dynamic thermal management targets the cores to reduce hotspots but often ignores caches, which are known for their high leakage power consumption.This article presents WaFFLe, an approach that targets the leakage power of the last-level cache (LLC) and hotspots occurring at the cores. WaFFLe turns off LLC-ways to reduce leakage power and to generate on-chip thermal buffers. In addition, fine-grained DVFS is applied during long LLC miss induced stalls to reduce core temperature. Our results show that WaFFLe reduces peak and average temperature of a 16-core based homogeneous tiled CMP with up to 8.4 ֯ C and 6.2 ֯ C, respectively, with an average performance degradation of only 2.5 %. We also show that WaFFLe outperforms a state-of-the-art cache-based technique and a greedy DVFS policy.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3471908',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Generic change detection (almost entirely) in the dataplane',\n",
       "  'authors': \"['Gonçalo Matos', 'Salvatore Signorello', 'Fernando M. V. Ramos']\",\n",
       "  'date': 'December 2021',\n",
       "  'source': \"ANCS '21: Proceedings of the Symposium on Architectures for Networking and Communications Systems\",\n",
       "  'abstract': 'Identifying traffic changes accurately sits at the core of many network tasks, from congestion analysis to intrusion detection. Modern systems leverage sketch-based structures that achieve favourable memory-accuracy tradeoffs by maintaining compact summaries of traffic data. Mainly used to detect heavy-hitters (usually the major source of network congestion), some can be adapted to detect traffic changes, but they fail on generality. As their core data structures track elephant flows, they miss to identify mice traffic that may be the main cause of change (e.g., microbursts or low-volume attacks). We present k-meleon, an in-network online change detection system that identifies heavy-changes - instead of changes amongst heavy-hitters only, a subtle but crucial difference. Our main contribution is a variant of the k-ary sketch (a well-known heavy-change detector) that runs on the data plane of a switch. The challenge was the batch-based design of the original. To address it, k-meleon features a new stream-based design that matches the pipeline computation model and fits its tough constraints. A preliminary evaluation shows that k-meleon achieves the same level of accuracy for online detection as the offline k-ary, detecting changes for any type of flow: be it an elephant, or a mouse.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3493425.3502767',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Comparative Evaluation of Machine Learning Inference Machines on Edge-class Devices',\n",
       "  'authors': \"['Petros Amanatidis', 'George Iosifidis', 'Dimitris Karampatzakis']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"PCI '21: Proceedings of the 25th Pan-Hellenic Conference on Informatics\",\n",
       "  'abstract': 'Computer science and engineering have evolved rapidly over the last decade offering innovative Machine Learning frameworks and high-performance hardware devices. Executing data analytics at the edge promises to transform the mobile computing paradigm by bringing intelligence next to the end user. However, it remains an open question to explore if, and to what extent, today’s Edge-class devices can support ML frameworks and which is the best configuration for efficient task execution. This paper provides a comparative evaluation of Machine Learning inference machines on Edge-class compute engines. The testbed consists of two hardware compute engines (i.e., CPU-based Raspberry Pi 4 and Google Edge TPU accelerator) and two inference machines (i.e., TensorFlow-Lite and Arm NN). Through an extensive set of experiments in our bespoke testbed, we compared three setups using TensorFlow-Lite ML framework, in terms of accuracy, execution time, and energy efficiency. Based on the results, an optimized configuration of the workload parameters can increase accuracy by 10%, and in addition, the class of the Edge compute engine in combination with the inference machine affects execution time by 86% and power consumption by almost 145%.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3503823.3503843',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'FlexPushdownDB: hybrid pushdown and caching in a cloud DBMS',\n",
       "  'authors': \"['Yifei Yang', 'Matt Youill', 'Matthew Woicik', 'Yizhou Liu', 'Xiangyao Yu', 'Marco Serafini', 'Ashraf Aboulnaga', 'Michael Stonebraker']\",\n",
       "  'date': 'None',\n",
       "  'source': 'Proceedings of the VLDB Endowment',\n",
       "  'abstract': 'Modern cloud databases adopt a storage-disaggregation architecture that separates the management of computation and storage. A major bottleneck in such an architecture is the network connecting the computation and storage layers. Two solutions have been explored to mitigate the bottleneck: caching and computation pushdown. While both techniques can significantly reduce network traffic, existing DBMSs consider them as orthogonal techniques and support only one or the other, leaving potential performance benefits unexploited.In this paper we present FlexPushdownDB (FPDB), an OLAP cloud DBMS prototype that supports fine-grained hybrid query execution to combine the benefits of caching and computation pushdown in a storage-disaggregation architecture. We build a hybrid query executor based on a new concept called separable operators to combine the data from the cache and results from the pushdown processing. We also propose a novel Weighted-LFU cache replacement policy that takes into account the cost of pushdown computation. Our experimental evaluation on the Star Schema Benchmark shows that the hybrid execution outperforms both the conventional caching-only architecture and pushdown-only architecture by 2.2X. In the hybrid architecture, our experiments show that Weighted-LFU can outperform the baseline LFU by 37%.',\n",
       "  'link': 'https://dl.acm.org/doi/10.14778/3476249.3476265',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Machine Learning and Soil Humidity Sensing: Signal Strength Approach',\n",
       "  'authors': \"['Lea Dujić Rodić', 'Tomislav Županović', 'Toni Perković', 'Petar Šolić', 'Joel J. P. C. Rodrigues']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Internet Technology',\n",
       "  'abstract': 'The Internet-of-Things vision of ubiquitous and pervasive computing gives rise to future smart irrigation systems comprising the physical and digital worlds. A smart irrigation ecosystem combined with Machine Learning can provide solutions that successfully solve the soil humidity sensing task in order to ensure optimal water usage. Existing solutions are based on data received from the power hungry/expensive sensors that are transmitting the sensed data over the wireless channel. Over time, the systems become difficult to maintain, especially in remote areas due to the battery replacement issues with a large number of devices. Therefore, a novel solution must provide an alternative, cost- and energy-effective device that has unique advantage over the existing solutions. This work explores the concept of a novel, low-power, LoRa-based, cost-effective system that achieves humidity sensing using Deep Learning techniques that can be employed to sense soil humidity with high accuracy simply by measuring the signal strength of the given underground beacon device.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3418207',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'An embedded controller for the hydraulic walking robot WLBOT',\n",
       "  'authors': \"['Ziqi Liu', 'Bo Jin', 'Shuo Zhai', 'Junkui Dong']\",\n",
       "  'date': 'April 2021',\n",
       "  'source': \"ICRSA '21: Proceedings of the 2021 4th International Conference on Robot Systems and Applications\",\n",
       "  'abstract': \"This paper presents an embedded controller for the quadruped hydraulic robot WLBOT. First, we give an overview of a WLBOT system. Second, the hardware design and the software architecture of the embedded controller are introduced. The embedded controller takes charge of multi-sensor information processing and signal output of the servo valve, as well as receiving control command and sending processed information via Control Area Network (CAN) bus. What's more, the realization of the 2kHz high-speed control of the embedded controller is illustrated. Finally, the platform is constructed, in which the feasibility of the design and the validity of the control algorithm is verified. It shows that WLBOT can walk properly in a PID controller as expected.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3467691.3467703',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A Methodology for Simulating Multi-chiplet Systems Using Open-source Simulators',\n",
       "  'authors': \"['Haocong Zhi', 'Xianuo Xu', 'Weijian Han', 'Zhilin Gao', 'Xiaohang Wang', 'Maurizio Palesi', 'Amit Kumar Singh', 'Letian Huang']\",\n",
       "  'date': 'September 2021',\n",
       "  'source': \"NANOCOM '21: Proceedings of the Eight Annual ACM International Conference on Nanoscale Computing and Communication\",\n",
       "  'abstract': 'Multi-chiplet systems are a new design paradigm to mitigate the chip design cost and improve yield for complex SoCs. The design space of multi-chiplet systems is much larger compared to a single chip SoC system. To support early stage design space exploration, simulators are of paramount importance. However, existing open-source multi-/many-core simulators are not suitable for simulating large-scale multi-chiplet systems due to the following reasons: 1) lack of accurate inter-chiplet interconnection model, and 2) incapable of supporting large-scale parallel simulation with accurate interconnection modelling. Therefore, we propose a methodology for building up a simulator for multi-chiplet systems using open-source simulators like gem5, sniper, gpgpu-sim, etc. This simulation methodology mimics the reuse and integration idea of chiplets, that is, these existing open-source simulators are reused to simulate individual chiplets, and an inter-simulator-process communication and synchronization protocol is proposed to simulate inter-chiplet communication. The proposed simulation methodology has the following features: 1) Parallel simulation for large-scale systems is supported with inter- and intra-chiplet interconnection accurately modelled. 2) Both distributed and shared memory models are supported for multi-chiplet systems. We also provide a method to modify the code of the open-source simulators like gem5, sniper, gpgpu-sim, etc. for multi-chiplet simulation, and we have released the source code of multi-chiplet simulators based on gem5, sniper, gpgpu-sim at https://github.com/FCAS-SCUT/chiplet_simulators. In the future we will port more applications/benchmarks and integrate more open-source simulators.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3477206.3477459',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'SpiderWeb: Enabling Through-Screen Visible Light Communication',\n",
       "  'authors': \"['Hanting Ye', 'Qing Wang']\",\n",
       "  'date': 'November 2021',\n",
       "  'source': \"SenSys '21: Proceedings of the 19th ACM Conference on Embedded Networked Sensor Systems\",\n",
       "  'abstract': \"We are now witnessing a trend of realizing full-screen on electronic devices such as smartphones to maximize their screen-to-body ratio for a better user experience. Thus the bezel/narrow-bezel on today's devices to host various line-of-sight sensors would disappear. This trend not only is forcing sensors like the front cameras to be placed under the screen of devices, but also will challenge the deployment of the emerging Visible Light Communication (VLC) technology, a paradigm for the next-generation wireless communication. In this work, we propose the concept of through-screen VLC with photosensors placed under Organic Light-Emitting Diode (OLED) screen. Though being transparent, an OLED screen greatly attenuates the intensity of passing-through light, degrading the efficiency of intensity-based VLC systems. In this paper, we instead exploit the color domain to build SpiderWeb, a through-screen VLC system. For the first time, we observe that an OLED screen introduces a color-pulling effect at photosensors, affecting the decoding of color-based VLC signals. Motivated by this observation and by the structure of spider's web, we design the SWebCSK Color-Shift Keying modulation scheme and a slope-based demodulation method, which can eliminate the color-pulling effect. We prototype SpiderWeb with off-the-shelf hardware and evaluate its performance thoroughly under various scenarios. The results show that compared to existing solutions, our solutions can reduce the bit error rate by two orders of magnitude and can achieve a 3.4x data rate.\",\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3485730.3485948',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Kard: lightweight data race detection with per-thread memory protection',\n",
       "  'authors': \"['Adil Ahmad', 'Sangho Lee', 'Pedro Fonseca', 'Byoungyoung Lee']\",\n",
       "  'date': 'April 2021',\n",
       "  'source': \"ASPLOS '21: Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems\",\n",
       "  'abstract': 'Finding data race bugs in multi-threaded programs has proven challenging. A promising direction is to use dynamic detectors that monitor the program’s execution for data races. However, despite extensive work on dynamic data race detection, most proposed systems for commodity hardware incur prohibitive overheads due to expensive compiler instrumentation of memory accesses; hence, they are not efficient enough to be used in all development and testing settings.   KARD is a lightweight system that dynamically detects data races caused by inconsistent lock usage—when a program concurrently accesses the same memory object using different locks or only some of the concurrent accesses are synchronized using a common lock. Unlike existing detectors, KARD does not monitor memory accesses using expensive compiler instrumentation. Instead, KARD leverages commodity per-thread memory protection, Intel Memory Protection Keys (MPK). Using MPK, KARD ensures that a shared object is only accessible to a single thread in its critical section, and captures all violating accesses from other concurrent threads. KARD overcomes various limitations of MPK by introducing key-enforced race detection, employing consolidated unique page allocation, carefully managing protection keys, and automatically pruning out non-racy or redundant violations. Our evaluation shows that KARD detects all data races caused by inconsistent lock usage and has a low geometric mean execution time overhead: 7.0% on PARSEC and SPLASH-2x benchmarks and 5.3% on a set of real-world applications (NGINX, memcached, pigz, and Aget).',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3445814.3446727',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Realization of Multi Parameter Measuring Device Based on ATT7022E and STM32',\n",
       "  'authors': \"['Zhaoling Gao', 'Guang Yu']\",\n",
       "  'date': 'October 2021',\n",
       "  'source': \"ICCPR '21: Proceedings of the 2021 10th International Conference on Computing and Pattern Recognition\",\n",
       "  'abstract': 'This article introduces in detail a multi-parameter measurement device that can measure six-channel voltage, current and other information with the three-phase multi-function measurement chip ATT7022E, and can transmit the measured data to the high-performance ARM_Cortex_M4 microcontroller through the serial port of the STM32 single-chip microcomputer. This controller is equipped with an industrial Ethernet interface to connect to the Internet. In addition to the data acquisition part, it also realizes the remote configuration and precise control of the entire device. This article mainly introduces the content of the data acquisition part, including the acquisition of three-phase input and output Terminal voltage, current, active power, reactive power, apparent power, power factor and other parameters. Describes the design of this part of the hardware schematic diagram: including voltage sampling circuit, current sampling circuit, ATT7022E basic circuit, STM32F103RBT6 microcontroller basic circuit, serial communication interface and other circuit designs. It also describes the software calibration method, the realization of serial communication with the host computer ARM_Cortex_M4 microcontroller and the analysis of the measurement results. After experiments, the voltage is calibrated near the grid voltage, and the voltage value can reach the accuracy requirement within 0.5 level in the technical indicators of the power monitor.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3497623.3497662',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Reprogramming 3D TLC Flash Memory based Solid State Drives',\n",
       "  'authors': \"['Congming Gao', 'Min Ye', 'Chun Jason Xue', 'Youtao Zhang', 'Liang Shi', 'Jiwu Shu', 'Jun Yang']\",\n",
       "  'date': 'None',\n",
       "  'source': 'ACM Transactions on Storage',\n",
       "  'abstract': 'NAND flash memory-based SSDs have been widely adopted. The scaling of SSD has evolved from plannar (2D) to 3D stacking. For reliability and other reasons, the technology node in 3D NAND SSD is larger than in 2D, but data density can be increased via increasing bit-per-cell. In this work, we develop a novel reprogramming scheme for TLCs in 3D NAND SSD, such that a cell can be programmed and reprogrammed several times before it is erased. Such reprogramming can improve the endurance of a cell and the speed of programming, and increase the amount of bits written in a cell per program/erase cycle, i.e., effective capacity. Our work is the first to perform a real 3D NAND SSD test to validate the feasibility of the reprogram operation. From the collected data, we derive the restrictions of performing reprogramming due to reliability challenges. Furthermore, a reprogrammable SSD (ReSSD) is designed to structure reprogram operations. ReSSD is evaluated in a case study in RAID 5 system (RSS-RAID). Experimental results show that RSS-RAID can improve the endurance by 35.7%, boost write performance by 15.9%, and increase effective capacity by 7.71%, with negligible overhead compared with conventional 3D SSD-based RAID 5 system.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3487064',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'User-defined cloud',\n",
       "  'authors': \"['Yiying Zhang', 'Ardalan Amiri Sani', 'Guoqing Harry Xu']\",\n",
       "  'date': 'June 2021',\n",
       "  'source': \"HotOS '21: Proceedings of the Workshop on Hot Topics in Operating Systems\",\n",
       "  'abstract': 'Since its creation, cloud computing has always taken a provider-dictated approach, where cloud providers define and manage the cloud to accommodate the user needs they deem important. We propose \"User-Defined Cloud\", or UDC, a new cloud scheme that allows users to define their own \"clouds\", by defining hardware resource needs, system software features, and security requirements of their applications, and to do so without the need to build or manage low-level systems.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3458336.3465304',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A cross-platform cache timing attack framework via deep learning',\n",
       "  'authors': \"['Ruyi Ding', 'Ziyue Zhang', 'Xiang Zhang', 'Cheng Gongye', 'Yunsi Fei', 'Aidong A. Ding']\",\n",
       "  'date': 'March 2022',\n",
       "  'source': \"DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe\",\n",
       "  'abstract': 'While deep learning methods have been adopted in power side-channel analysis, they have not been applied to cache timing attacks due to the limited dimension of cache timing data. This paper proposes a persistent cache monitor based on cache line flushing instructions, which runs concurrently to a victim execution and captures detailed memory access patterns in high-dimensional timing traces. We discover a new cache timing side-channel across both inclusive and non-inclusive caches, different from the traditional \"Flush+Flush\" timing leakage. We then propose a non-profiling differential deep learning analysis strategy to exploit the cache timing traces for key recovery. We further propose a framework for cross-platform cache timing attack via deep learning. Knowledge learned from profiling a common reference device can be transferred to build models to attack many other victim devices, even in different processor families. We take the OpenSSL AES-128 encryption algorithm as an example victim and deploy an asynchronous cache attack. We target three different devices from Intel, AMD, and ARM processors. We examine various scenarios for assigning the teacher role to one device and the student role to other devices, and evaluate the cross-platform deep-learning attack framework. Experimental results show that this new attack is easily extendable to victim devices and is more effective than attacks without any prior knowledge.',\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3539845.3540011',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Use of Speaker Recognition Approaches for Learning and Evaluating Embedding Representations of Musical Instrument Sounds',\n",
       "  'authors': \"['Xuan Shi', 'Erica Cooper', 'Junichi Yamagishi']\",\n",
       "  'date': 'None',\n",
       "  'source': 'IEEE/ACM Transactions on Audio, Speech and Language Processing',\n",
       "  'abstract': 'Constructing an embedding space for musical instrument sounds that can meaningfully represent new and unseen instruments is important for downstream music generation tasks such as multi-instrument synthesis and timbre transfer. The framework of Automatic Speaker Verification (ASV) provides us with architectures and evaluation methodologies for verifying the identities of unseen speakers, and these can be repurposed for the task of learning and evaluating a musical instrument sound embedding space that can support unseen instruments. Borrowing from state-of-the-art ASV techniques, we construct a musical instrument recognition model that uses a SincNet front-end, a ResNet architecture, and an angular softmax objective function. Experiments on the NSynth and RWC datasets show our model&#x2019;s effectiveness in terms of equal error rate (EER) for unseen instruments, and ablation studies show the importance of data augmentation and the angular softmax objective. Experiments also show the benefit of using a CQT-based filterbank for initializing SincNet over a Mel filterbank initialization. Further complementary analysis of the learned embedding space is conducted with t-SNE visualizations and probing classification tasks, which show that including instrument family labels as a multi-task learning target can help to regularize the embedding space and incorporate useful structure, and that meaningful information such as playing style, which was not included during training, is contained in the embeddings of unseen instruments.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/TASLP.2022.3140549',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'Towards energy-efficient CGRAs via stochastic computing',\n",
       "  'authors': \"['Bo Wang', 'Rong Zhu', 'Jiaxing Shang', 'Dajiang Liu']\",\n",
       "  'date': 'March 2022',\n",
       "  'source': \"DATE '22: Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe\",\n",
       "  'abstract': 'Stochastic computing (SC) is a promising computing paradigm for low-power and low-cost applications with the added benefit of high error tolerance. Meanwhile, Coarse-Grained Re-configurable Architecture (CGRA) is also a promising platform for domain-specific applications for its combination of energy efficiency and flexibility. Intuitively, introducing SC to CGRA would synergistically reinforce the strengths of both paradigms. Accordingly, this paper proposes an SC-based CGRA by replacing the exact multiplication in traditional CGRA with an SC-based multiplication, where the problem of accuracy and latency are both improved using parallel stochastic sequence generators and leading zero shifters. In addition, with the flexible connections among PEs, the high-accuracy operation can be easily achieved by combing neighbor PEs without switching costs like power-gating. Compared to the state-of-the-art approximate computing design of CGRA, our proposed CGRA has 16% more energy reduction and 34% energy efficiency improvement while keeping high configuration flexibility.',\n",
       "  'link': 'https://dl.acm.org/doi/10.5555/3539845.3539900',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'End-to-End Dereverberation, Beamforming, and Speech Recognition in a Cocktail Party',\n",
       "  'authors': \"['Wangyou Zhang', 'Xuankai Chang', 'Christoph Boeddeker', 'Tomohiro Nakatani', 'Shinji Watanabe', 'Yanmin Qian']\",\n",
       "  'date': 'None',\n",
       "  'source': 'IEEE/ACM Transactions on Audio, Speech and Language Processing',\n",
       "  'abstract': 'Far-field multi-speaker automatic speech recognition (ASR) has drawn increasing attention in recent years. Most existing methods feature a signal processing frontend and an ASR backend. In realistic scenarios, these modules are usually trained separately or progressively, which suffers from either inter-module mismatch or a complicated training process. In this paper, we propose an end-to-end multi-channel model that jointly optimizes the speech enhancement (including speech dereverberation, denoising, and separation) frontend and the ASR backend as a single system. To the best of our knowledge, this is the first work that proposes to optimize dereverberation, beamforming, and multi-speaker ASR in a fully end-to-end manner. The frontend module consists of a weighted prediction error (WPE) based submodule for dereverberation and a neural beamformer for denoising and speech separation. For the backend, we adopt a widely used end-to-end (E2E) ASR architecture. It is worth noting that the entire model is differentiable and can be optimized in a fully end-to-end manner using only the ASR criterion, without the need of parallel signal-level labels. We evaluate the proposed model on several multi-speaker benchmark datasets, and experimental results show that the fully E2E ASR model can achieve competitive performance on both noisy and reverberant conditions, with over 30&#x0025; relative word error rate (WER) reduction over the single-channel baseline systems.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1109/TASLP.2022.3209942',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " {'title': 'A Method for Accelerating YOLO by Hybrid Computing Based on ARM and FPGA',\n",
       "  'authors': \"['Qilin Xiong', 'Chun Liao', 'Zhenhong Yang', 'Wanlin Gao']\",\n",
       "  'date': 'December 2021',\n",
       "  'source': \"ACAI '21: Proceedings of the 2021 4th International Conference on Algorithms, Computing and Artificial Intelligence\",\n",
       "  'abstract': 'CNN has promoted the rapid development of target recognition and detection technology. By comparison with machine learning, it has faster detection speed and higher robustness.\\xa0\\xa0However, the deployment of the CNN network model often needs more computing resources, which hinders the application of artificial intelligence technology.\\xa0\\xa0In this paper, the authors use the hybrid architecture of ARM and FPGA to deploy a You Only Look Once (YOLO) model on the FPGA to improve the efficiency of target recognition and detection under condition of low resources consumption and low power consumption.\\xa0YOLO is a one-stage real-time detection model and it has high detection speed and remarkable accuracy.\\xa0High-level Synthesis (HLS) is a fast development and verification technology of FPGA based on C/C++. We use HLS to implement the pipeline mechanism and complete the parallel calculation of convolution, thereby constructing a forward reasoning model of YOLOv3-tiny.\\xa0\\xa0In order to accelerate the forward inference process of YOLO, we combine convolution with batch normalization.\\xa0\\xa0The FPGA we use in the paper is Xilinx Zynq-7035 containing system on chip (SoC). We build the software and hardware co-architecture of ARM and FPGA on Zynq-7035, which makes full use of the logic control advantages of ARM and the logic computing advantages of FPGA.\\xa0\\xa0In the end, we achieve 28.99 GOP/S speed with only 3.715W power consumption.\\xa0\\xa0Finally, compared with the Ryzen 5 3600, we achieve 41.3inference speed at a lower clock rate.',\n",
       "  'link': 'https://dl.acm.org/doi/10.1145/3508546.3508576',\n",
       "  'category': 'Hardware AND Architecture'},\n",
       " ...]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('not_tmccmldm_categories.json', 'r', encoding='utf-8') as json_file:\n",
    "    data_no_category0 = json.load(json_file)\n",
    "data_no_category0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "3f111130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5709"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_no_category0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "8a4ca923",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_category = pd.DataFrame(data_no_category0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "b593ae92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "586"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df_no_category.link.value_counts() == 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "0c5d8109",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_no_category = []\n",
    "unique_links_1 = set()\n",
    "TARGET_COUNT = 4987\n",
    "\n",
    "while len(data_no_category) < TARGET_COUNT:\n",
    "    random_item = random.choice(data_no_category0)\n",
    "    link = random_item['link']\n",
    "\n",
    "    if link not in unique_links_1 and link not in unique_links and random_item['abstract']:\n",
    "        unique_links_1.add(link)\n",
    "        data_no_category.append(random_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "93b167a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4987"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_no_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "0250ea93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "Databases                    0.185282\n",
       "Software AND Engineering     0.176459\n",
       "Hardware AND Architecture    0.174654\n",
       "Systems AND Control          0.173852\n",
       "Computer AND Security        0.167836\n",
       "Fuzzy AND Logic              0.121917\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data_no_category).category.value_counts()/len(data_no_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "5294d09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('category.json', 'w') as json_file:\n",
    "    json.dump(data_category, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "40ea5077",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('no_category.json', 'w') as json_file:\n",
    "    json.dump(data_no_category, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "d61d9e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('category.json', 'r') as json_file:\n",
    "    data_category = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "53f4c16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('no_category.json', 'r') as json_file:\n",
    "    data_no_category = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "42c89ee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': \"Application Data Mining Technology in Monitoring Strategies of College Students' English Learning Motivation\",\n",
       " 'authors': \"['Shuang Liu']\",\n",
       " 'date': 'September 2021',\n",
       " 'source': 'ICISCAE 2021: 2021 4th International Conference on Information Systems and Computer Aided Education',\n",
       " 'abstract': \"With the advent of the era of big data, how to apply big data technology to English learning has become a problem that people care about. This research mainly discusses the monitoring strategies of college students' English learning motivation based on data mining. After the collection of Web logs, an information matrix of online learning motivation is formed. Frequent user groups are retrieved through frequently visited page sets, and similar user groups are obtained through association rule analysis, and then related page sets are obtained from frequently visited page sets according to the set distance threshold between pages. Finally, perform cluster analysis on user information to obtain their preferences. Using gender as a categorical variable, the test found that there are significant differences in the intrinsic interest motivation and overall learning motivation dimensions of male and female students. The score of boys (2.6) and the score of girls in this item (3.873) are higher than those of boys. Therefore, the students' learning interest and motivation can be cultivated in a targeted manner, thereby effectively improving the students' academic performance.\",\n",
       " 'link': 'https://dl.acm.org/doi/10.1145/3482632.3484150',\n",
       " 'category': 'Text AND Mining OR Clustering OR Classification OR Machine AND Learning OR Data AND Mining'}"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_category[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "5f5c4741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Tangled: A Conventional Processor Integrating A Quantum-Inspired Coprocessor',\n",
       " 'authors': \"['Henry Dietz']\",\n",
       " 'date': 'August 2021',\n",
       " 'source': \"ICPP Workshops '21: 50th International Conference on Parallel Processing Workshop\",\n",
       " 'abstract': 'Quantum computers use quantum physics phenomena to create specialized hardware that can efficiently execute algorithms operating on entangled superposed data. That hardware must be attached to and controlled by a conventional host computer. However, it can be argued that the main benefit thus far has been from reformulating problems to make use of entangled superpositions rather than from use of exotic physics mechanisms to perform the computation – such reformulations often have produced more efficient algorithms for conventional computers. Parallel bit pattern computing does not simulate quantum computing, but provides a way to use non-quantum, bit-level, massively-parallel, SIMD hardware to efficiently execute a broad class of algorithms leveraging superposition and entanglement.  Just as quantum hardware needs a conventional host, so to does parallel bit pattern hardware. Thus, the current work presents Tangled: a simple proof-of-concept conventional processor design incorporating a tightly-coupled interface to an integrated parallel bit pattern co-processor (Qat). The feasibility of this type of interface between conventional and quantum-inspired computation was investigated by construction of an instruction set, building complete Verilog designs for pipelined implementations, and by observing the effectiveness of the interface in executing simple quantum-inspired algorithms involving operations on entangled, superposed, values.',\n",
       " 'link': 'https://dl.acm.org/doi/10.1145/3458744.3474044',\n",
       " 'category': 'Hardware AND Architecture'}"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_no_category[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "43339561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "977"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "158a5441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4987"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_no_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "6e9dba19",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = data_category + data_no_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "ec5e500b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_category = pd.DataFrame(data_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "53348def",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "977"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df_category.link.value_counts() == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "d2404bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_category = pd.DataFrame(data_no_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "0e033b06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4987"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df_no_category.link.value_counts() == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "8e6334d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "ca882a33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5274"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df.link.value_counts() == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "dd695fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_words_category = create_total_bag(data_category)\n",
    "bag_words_no_category = create_total_bag(data_no_category)\n",
    "bag_words = create_total_bag(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00180d42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
