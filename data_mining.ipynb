{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6951c59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import copy\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from wordcloud import WordCloud\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate, StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from wordcloud import WordCloud\n",
    "random.seed = 456"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bf4e6bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordcloud in c:\\users\\linag\\anaconda3\\lib\\site-packages (1.9.2)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\linag\\anaconda3\\lib\\site-packages (from wordcloud) (1.24.3)\n",
      "Requirement already satisfied: pillow in c:\\users\\linag\\anaconda3\\lib\\site-packages (from wordcloud) (9.4.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\linag\\anaconda3\\lib\\site-packages (from wordcloud) (3.7.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\linag\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\linag\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\linag\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\linag\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\linag\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (23.1)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\users\\linag\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\linag\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\linag\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b0d6b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_keywords_list(list_of_keyword_dicts):\n",
    "    normalized_list = []\n",
    "    for keyword_dict in list_of_keyword_dicts:\n",
    "        total_value = sum(keyword_dict.values())\n",
    "        normalized_keywords = keyword_dict.copy()\n",
    "        for keyword in normalized_keywords:\n",
    "            normalized_keywords[keyword] /= total_value\n",
    "        normalized_list.append(normalized_keywords)\n",
    "    return normalized_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95b3ef20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tfidf(list_of_keyword_dicts):\n",
    "    idf_values = dict.fromkeys(list_of_keyword_dicts[0].keys(), 0.0)\n",
    "\n",
    "    for keyword_dict in list_of_keyword_dicts:\n",
    "        for word, frequency in filter(lambda x: x[1] > 0, keyword_dict.items()):\n",
    "            idf_values[word] += 1.0\n",
    "\n",
    "    for word in idf_values:\n",
    "        if idf_values[word]:\n",
    "            idf_values[word] = math.log(len(list_of_keyword_dicts) / idf_values[word])\n",
    "\n",
    "    tfidf_results = []\n",
    "    for keyword_dict in list_of_keyword_dicts:\n",
    "        keyword_dict = keyword_dict.copy()\n",
    "        total_frequency = sum(keyword_dict.values())\n",
    "        for word, frequency in filter(lambda x: x[1] > 0, keyword_dict.items()):\n",
    "            keyword_dict[word] *= idf_values[word] / total_frequency\n",
    "        tfidf_results.append(keyword_dict)\n",
    "    return tfidf_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31adebf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_tfidf(list_of_keyword_dicts):\n",
    "    list_of_keyword_dicts = calculate_tfidf(list_of_keyword_dicts)\n",
    "    keywords = list_of_keyword_dicts[0].keys()\n",
    "    normalized_list = []\n",
    "\n",
    "    for index, value_dict in enumerate(list_of_keyword_dicts):\n",
    "        norm = 0\n",
    "\n",
    "        for v in value_dict.values():\n",
    "            norm += v**2\n",
    "\n",
    "        norm = math.sqrt(norm)\n",
    "\n",
    "        if norm == 0:\n",
    "            normalized_list.append(dict(zip(keywords, value_dict.values())))\n",
    "            continue\n",
    "\n",
    "        normalized_values = map(lambda x: x / norm, value_dict.values())\n",
    "        normalized_list.append(dict(zip(keywords, normalized_values)))\n",
    "\n",
    "    return normalized_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be2cf9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_word_cloud(word_frequency_dict, title, axis=None):\n",
    "\n",
    "    wordcloud = WordCloud(background_color=\"white\", height=500, width=1000, random_state=random.seed)\n",
    "    wordcloud.generate_from_frequencies(word_frequency_dict)\n",
    "\n",
    "    if axis is not None:\n",
    "        axis.imshow(wordcloud, interpolation='bilinear')\n",
    "        axis.set_title(title)\n",
    "        axis.axis(False)\n",
    "    else:\n",
    "        plt.subplots(num=None, figsize=(15, 10), dpi=80)\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.title(title)\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ab9fa68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\linag\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\linag\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\linag\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9934c450",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(language='english')\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "stopwords_set = set(stopwords.words('english'))\n",
    "punctuation = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "394457e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8913e4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_stem_nltk(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    return [stemmer.stem(word) for word in tokens if word not in stopwords_set and word not in punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6df4bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_lemmatize_nltk(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    return [lemmatizer.lemmatize(word) for word in tokens if word not in stopwords_set and word not in punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "065e03af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_total_bag(results):\n",
    "    word_frequency_dict = dict()\n",
    "    for result in results:\n",
    "        tokens = preprocess_and_lemmatize_nltk(result['abstract'])\n",
    "        for token in tokens:\n",
    "            if token not in word_frequency_dict:\n",
    "                word_frequency_dict[token] = 1\n",
    "            else:\n",
    "                word_frequency_dict[token] += 1\n",
    "    return word_frequency_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc6d6f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_word_frequency(abstract, word_frequency_dict):\n",
    "    tokens = preprocess_and_lemmatize_nltk(abstract)\n",
    "    for token in tokens:\n",
    "        if token in word_frequency_dict:\n",
    "            word_frequency_dict[token] += 1\n",
    "    return word_frequency_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf47d8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_low_frequency_words(word_frequency_dict, threshold=0):\n",
    "    filtered_dict = dict()\n",
    "    for word, frequency in word_frequency_dict.items():\n",
    "        if frequency > threshold:\n",
    "            filtered_dict[word] = frequency\n",
    "    return filtered_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "569ba227",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_list_keywords(word_frequency_dict, articles):\n",
    "    list_keywords = []\n",
    "    temp_bag = dict.fromkeys(word_frequency_dict, 0)\n",
    "    for article in articles:\n",
    "        list_keywords.append(update_word_frequency(article['abstract'], temp_bag.copy()))\n",
    "    del temp_bag\n",
    "    return list_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38291d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_list_keywords_weights(list_of_keyword_weights):\n",
    "    list_keywords_weight_items = []\n",
    "    for item in list_of_keyword_weights:\n",
    "        list_keywords_weight_items.append(list(item.values()))\n",
    "    return list_keywords_weight_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc43cb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_precision_positive_class(y_true, y_pred):\n",
    "    return precision_score(y_true, y_pred, pos_label=0)\n",
    "\n",
    "def binary_recall_positive_class(y_true, y_pred):\n",
    "    return recall_score(y_true, y_pred, pos_label=0)\n",
    "\n",
    "def binary_f1_positive_class(y_true, y_pred):\n",
    "    return f1_score(y_true, y_pred, pos_label=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a4c5d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_binary_positive = make_scorer(binary_precision_positive_class, greater_is_better=True)\n",
    "recall_binary_positive = make_scorer(binary_recall_positive_class, greater_is_better=True)\n",
    "f1_binary_positive = make_scorer(binary_f1_positive_class, greater_is_better=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1dc673c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classifier(model, X_train, y_train, X_test=None, y_test=None):\n",
    "    cross_validator = StratifiedKFold(n_splits=5, random_state=0, shuffle=True)\n",
    "    scoring_metrics = {'accuracy': 'accuracy', 'precision_binary_positive': precision_binary_positive, 'recall_binary_positive': recall_binary_positive, 'f1_binary_positive': f1_binary_positive}\n",
    "    scores = cross_validate(model, X_train, y_train, cv=cross_validator, return_estimator=True, scoring=scoring_metrics)\n",
    "    return cross_validator, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f164c4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_classifier_roc(list_of_cross_validators, list_of_scores, x_train, y_train, x_test=None, y_test=None):\n",
    "\n",
    "    list_num_best_models = []\n",
    "    for i in range(len(list_of_scores)):\n",
    "        list_num_best_models.append(list_of_scores[i]['test_f1_binary_positive'].argmax())\n",
    "\n",
    "    best_models = []\n",
    "    for i in range(len(list_of_scores)):\n",
    "        best_models.append(list_of_scores[i]['estimator'][list_num_best_models[i]])\n",
    "\n",
    "    list_x_test = []\n",
    "    list_y_test = []\n",
    "    for j in range(len(list_num_best_models)):\n",
    "        if x_test is None and y_test is None:\n",
    "            _, test_num = list(list_of_cross_validators[j].split(x_train, y_train))[list_num_best_models[j]]\n",
    "            x_test, y_test = [], []\n",
    "            for i in test_num:\n",
    "                x_test.append(x_train[i])\n",
    "                y_test.append(y_train[i])\n",
    "            list_x_test.append(x_test)\n",
    "            list_y_test.append(y_test)\n",
    "            x_test = None\n",
    "            y_test = None\n",
    "\n",
    "    list_y_proba = []\n",
    "    for i in range(len(best_models)):\n",
    "        list_y_proba.append(best_models[i].predict_proba(list_x_test[i])[:, 1])\n",
    "\n",
    "    list_fpr = [0] * len(list_y_proba)\n",
    "    list_tpr = [0] * len(list_y_proba)\n",
    "    for i in range(len(list_y_proba)):\n",
    "        list_fpr[i], list_tpr[i], _ = roc_curve(list_y_test[i], list_y_proba[i])\n",
    "\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    lw = 2\n",
    "    for i in range(len(list_tpr)):\n",
    "        plt.plot(\n",
    "            list_fpr[i],\n",
    "            list_tpr[i],\n",
    "            lw=lw,\n",
    "            label=f'{str(best_models[i]).split(\"(\")[0]}\\nAUC = {round(roc_auc_score(list_y_test[i], list_y_proba[i]), 5)}'\n",
    "        )\n",
    "    plt.plot([0, 1], [0, 1], color=\"navy\", lw=lw, linestyle=\"--\")\n",
    "    plt.xlim([-0.1, 1.1])\n",
    "    plt.ylim([-0.1, 1.1])\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title('ROC curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decd39f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
